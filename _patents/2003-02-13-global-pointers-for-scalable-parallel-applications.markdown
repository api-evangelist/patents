---
title: Global pointers for scalable parallel applications
abstract: Mapping of cacheable memory pages from other processes in a parallel job provides a very efficient mechanism for inter-process communication. A trivial address computation can then be used to look up a virtual address that allows the use of cacheable loads and stores to directly access or update the memory of other processes in the job for communication purposes. When an interconnection network permits the cacheable access of one host's memory from another host in the cluster, kernel and library software can map memory from processes on other hosts, in addition to the memory on the same host. This mapping can be done at the start of a parallel job using a system library interface. A function in an application programming interface provides a user-level, fast lookup of a virtual address that references data regions residing on all of the processes in a parallel job running across multiple hosts.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07451278&OS=07451278&RS=07451278
owner: Silicon Graphics, Inc.
number: 07451278
owner_city: Mountain View
owner_country: US
publication_date: 20030213
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS"],"p":["1. Field of the Invention","The present invention is directed to sharing memory by parallel executing processes and, more particularly, to accessing cacheable memory across partitions in a multi-host parallel processing system.","2. Description of the Related Art","There are a number of parallel programming models currently or recently in use. These include the Message Passing Interface (both MPI-1 and MPI-2) as described in, e.g., by Snir, et al. published in 1996 by the Massachusetts Institute of Technology; Multi-Level Parallelism (MLP) as used by the National Aeronautics and Space Administration and described in the Dec. 15, 2000 journal published by Elsevier; OpenMP; CoArray Fortran; and High Performance Fortran (HPF); as well as proprietary multi-processing programming models, such as SHMEM which originated from Cray Research, Inc. and is currently available from Silicon Graphics, Inc. (SGI) of Mountain View, Calif.","Another example is the Global Arrays (GA) toolkit which is in the public domain and is available from the William R. Wiley Environmental Molecular Sciences Laboratory at the Pacific Northwest National Laboratory (PNNL) in Richland, Wash. The GA toolkit is compatible with MPI and provides a portable \u201cshared-memory\u201d programming interface for distributed-memory computers. Using GAs, each process in a multiple instruction\/multiple data (MIMD) parallel program can asynchronously access logical blocks of physically distributed dense multi-dimensional arrays using library calls. The GA model exposes to the programmer the non-uniform memory access (NUMA) characteristics of high performance computers and acknowledges that access to a remote portion of the shared data is slower than to the local portion.","Some of these multi-processing programming models, such as MLP and OpenMP have only been implemented on hardware with a single system image (single-host). The others have been implemented on clusters (multi-host) with between 100 to 6,000 or more processors per cluster to provide massively parallel processing capability. All of these multi-processing programming models utilize alternating computation and communication phases. The more time required for communication, the less efficient and less scalable the model.","The multi-processing programming models described above utilize one or more of the following types of communication. In generally increasing efficiency these types are send\/receive, put\/get and load\/store. Conventionally, the most efficient, load\/store, has only been available in single-host systems using MLP, OpenMP or SHMEM\/XPMEM. The MPI-1 multi-processing programming model utilizes only send\/receive which requires hand-shaking between processes. Library calls and hand-shaking are avoided in load\/store data transfers, thereby minimizing overhead. However, load\/store data transfers require access to shared memory that has previously been difficult to implement in a multi-host system.","It is an aspect of the present invention to provide faster communication between hosts in a parallel processing computer system.","It is another aspect of the present invention to provide communication between partitions of cacheable memory in a parallel processing computer system with low latency and high bandwidth performance.","It is a further aspect of the present invention to provide communication between partitions of cacheable memory in a parallel processing computer system without a subroutine call and without the use of memory mapped registers.","It is yet another aspect of the present invention to provide a mechanism for memory mapping across partitions of cacheable memory in a parallel processing computer system.","It is a yet further aspect of the present invention to permit generated code from existing compilers to access remote memory efficiently.","It is yet another aspect of the present invention to enable existing compilers to generate efficient code for accessing small-strided transfers by built-in processor queues that allow multiple loads or stores to be operating concurrently.","It is a yet further aspect of the present invention to provide data access flexibility to facilitate load balancing.","The above aspects can be attained by a method of implementing a multi-processing programming model that provides access to memory cached by cooperating processes executing on multiple hosts. The method makes all memory assigned to any of the cooperating processes accessible to any of the processes by using pointers that are unique across all hosts. This is accomplished by mapping portions of the memory, including both segments of virtual memory fixed in size and changeable in size, upon initial start up of each process for subsequent inter-host access. The method also includes transferring messages through mapped memory regions via a single inter-host copy request from a user process. Preferably the areas of memory are transferred via user-level loop load\/store operations on cacheable memory.","These together with other aspects and advantages which will be subsequently apparent, reside in the details of construction and operation as more fully hereinafter described and claimed, reference being had to the accompanying drawings forming a part hereof, wherein like numerals refer to like parts throughout.","Following is a glossary of abbreviations used herein.\n\n","An example of a processing node  in a scalable computing system to which the present invention can be applied is illustrated in . In this example, four processors -, including cache memory, are coupled to bedrock components . The bedrock components  include four interfaces - and a crossbar  connecting the interfaces. CPU interface  is coupled to the processors -. Input\/output control interface  is connected to input\/output control logic and peripherals . Memory control interface  is connected to main and directory memory . System control logic  provides control of the bedrock components . Remote node interface  provides a connection to other nodes , e.g., via a NUMAlink\u2122 (not shown).","Memory control interface  provides a hardware firewall. Physical memory  of a scalable node system is divided up such that, for X nodes, each node contains (1\/X) of the total system's physical memory . Thus, each node, like node  illustrated in , maintains the memory directory  for its (1\/X) of the system's memory using memory protection masks (not shown separately). The memory protection masks control access to cache lines of memory, where, for example, there may be 128 cache lines per page of memory. It does not matter from where the accessers of this (1\/X) memory are located (on this node, or on a different node on this or on another partition) or how many there are in a partition. Once the hardware firewall for an area of memory has been \u201copened\u201d in the memory directory for access from another partition, it is accessible to all processes on that partition that have been informed how to access it, as discussed below.","Conventionally, cross partition communication between processes operating under a multi-processing programming model executing on a cluster of nodes requires send\/receive hand-shaking or put\/get communications, both of which would use I\/O control interface  and logic peripherals , e.g., including an Ethernet link (not shown) of node  illustrated in . However, according to the present invention inter-host communication uses remote node interface  to communicate with other nodes , e.g., via a NUMAlink\u2122 connection.","The hardware illustrated in  may be operated to permit a process in one partition to access memory in the virtual address space of a process in a different partition with low latency and high bandwidth performance. The invention is not limited to the illustrated architecture, but can be applied to many different hardware architectures executing user applications in different partitions and is particularly relevant to scalable node architectures. It is common for scalable node architectures to have the capability of being partitioned into multiple independent systems. In the most secure systems, the partitions are separated by hardware firewalls, where the address space for each host is defined by hardware registers used solely for this purpose, e.g., in memory control interface . Hardware and operating system software can set the limits of the partitions and permit communication across partitions, but application software has no direct access to the hardware registers forming the firewall.","A block diagram illustrating the relationship between user applications and partitioning drivers is illustrated in . Scalable node architecture, such as computer systems available from SGI include hardware that provides a block transfer engine (BTE) . In the processing node illustrated in , BTE hardware is included in I\/O control interface . Block transfer engines can be viewed as cache-coherent direct memory access (DMA) engines. Once initiated, BTE data transfers do not require processor resources and they can be performed across partition boundaries.","The present invention makes use of block transfer engines (BTEs) to copy areas of memory in a separate partition almost as easily as memory within the same partition of a scalable node system, via fast system calls. However, unlike load\/store operations, BTE transfers require cache line alignment and a system call. In addition the present invention makes it possible to share memory accessed via load\/store operations in generated code.","The capabilities described above is accomplished by providing a set of library functions and other operating system software to access memory belonging to processes executing on a scalable node system. The relationship between this software and user processes is illustrated in  where a horizontal line designates a direct relationship and dependency between objects on either side of the horizontal line (the vertical borders in  are not indicative of any relationship). A BTE driver (not shown) is provided in system space for BTE hardware . Cross partition communication (XP\/XPC) driver  provides cross-partition communication using BTE hardware in I\/O control interface . XP\/XPC driver  is available in conventional SGI scalable node systems and similar drivers exist in similar architectures from other computer system manufacturers. However, no known operating system provides XPMEM driver  or function library (libxpmem)  and MPI\/SHMEM memory mapping library  in user space to provide easy access to a cross partition driver for user processes in the manner described below.","To enable cross-partition or inter-host communication, XPMEM driver\/library ,  has to know where to find the memory area in the virtual address space of a process. To provide the most efficient way to allow a multi-processing program model to access cacheable memory across a multi-host system, a sequence of memory mapping operations are preferably performed at the user process initialization time using MPI\/SHMEM memory mapping library . Thereafter, user processes have access to the Message Passing Interface (MPI) and SHMEM parallel programming models by calling functions in libmpi  and libsma , respectively. To allow the most flexible cross-partition communication using the MPI and SHMEM parallel programming models, five segments of memory are mapped during user process initialization. These segments are the static area, private heap, symmetric heap, stack area and internal MPI buffers.","Since several of these segments (the private heap, symmetric heap, and stack area) can grow dynamically based on user application design, the exact sizes of these regions to map at initialization time is not known. To accommodate this, each process requests a large potential address region for the mapped heaps and stack. These large regions are not faulted in at initialization time, as described below with respect to , to prevent excess memory consumption if the user application never accesses these regions. The size of the static region and internal MPI buffers can be determined empirically at initialization time.","As each MPI or SHMEM process starts up, it communicates the starting address and size of each of the five memory segments to be mapped, to all other processes in the multi-processing program model. Each process then maps (or attaches) the five memory segments of all the remote processes onto the current process' virtual memory region. This mapping sequence is performed by making calls to functions in the XPMEM library.","These initialization operations create the complete layout of the memory mapping for cross-partition or inter-host communication. Given this layout, each process can compute the virtual address of a corresponding memory location on a remote process by a simple formula or table look up. A formula is preferred for faster execution. Generally speaking, the formula takes into consideration the base address of the local memory mapping, the type of memory, and the remote process rank.",{"@attributes":{"id":"p-0036","num":"0046"},"figref":"FIG. 3A","b":["1","3","1"]},"A simplified example of memory mapping in a partitioned scalable node system for two hosts  is illustrated in . Hosts A and B are separated by firewall . Each host  is illustrated as executing two processes . Process  and process  are running on host A and process  and process  are running on host B. Each process has virtual address space assigned in a manner like that of the virtual address space  for process .  shows how a symmetric data segment, Array A, has been mapped into the other processes' virtual memory for later lookup by global pointer function.","In , a symmetric data segment (Array A) is mapped into virtual address space  of each process for later lookup by a user-level library function called shmem_ptr. This particular library function provides a fast address lookup for \u201csymmetric\u201d data segments only. Either a table look up, or a formula could be used, but a formula is preferred for faster execution. A symmetric data segment is one that exists with the same size and relative offset for all processes. The two symmetric data segments in UNIX systems, such as IRIX, are static memory and symmetric heap.","To perform a copy operation via XPMEM as illustrated in , a handle and identifier must be created for the copy destination in addition to the handle and identifier obtained for the copy source as described above. Process  permits  access to a range of virtual address space by calling a function (xpmem_make) in XPMEM library . XPMEM driver  stores  addresses and permissions defined by process  in the xpmem_make function call and returns a handle for the address range that can be used in subsequent calls to XPMEM. Process  then sends  the handle to other process(s) using conventional inter-host communication channels, such as a TCP\/IP Ethernet-based socket connection. Process , for example, receives  the handle from process .","To establish access to the address space of process , process  requests  access via function call xpmem_get in XPMEM library  in user space  of the host on which process  is running. In response, XPMEM driver  checks  the permissions stored  previously and returns an identifier for access to the address range by process . By separating the permissions check from access operations, it is unnecessary to check credentials on each access. If another process wants access to the virtual address space of process , library function xpmem_get is called using the same handle as an argument, but a different identifier is returned. XPMEM keeps track of which processes access which address spaces using the identifiers.","If process  receives permission to access the virtual address space of process , process  can request a copy of the data using BTE hardware as described below, or can request  that the address space be mapped for sharing by calling the xpmem_attach function in XPMEM library  using the identifier returned by XPMEM driver . In response to an attach request , XPMEM driver  associates  a kernel page fault handler with a portion of process 's virtual address space.","To perform a copy operation via XPMEM, a handle and identifier must be created for the copy destination in addition to handle obtained for the copy source as described above. The handle is created via xpmem_make( ) and the identifier is created via xpmem_get( ) both using the destination process' address space to obtain the handle and request permission to access the memory. After the identifiers for the source and destination are known, one process calls xpmem_copy( ) specifying both identifiers and a length as arguments. XPMEM driver  translates these identifiers to blocks of physical memory to be copied from source to destination, thereby copying a portion of the virtual address space for the source process to the virtual address space of the destination process.","The result of the association  is illustrated in . Virtual address space  for process  includes mapping of array A in each of processes -. However, at this point in time only the virtual address space of process  contains page table mappings to array A. The first reference to the virtual address space of process  associated with array A generates  a fault. Due to the association  performed by XPMEM, the kernel fault handler, is initiated to locate  the source page. The kernel fault handler therefore calls XPMEM driver  to pin the source page and update the page table of process  to access the source page.","Subsequent accesses to the virtual address space of process  may be performed by load\/store operations. This makes the process of accessing the memory area in the virtual address space of process  very efficient for compiled programs written in, e.g., C or Fortran. Load\/store operations are normal reads\/writes to memory through any dereference or assignment methods available in any programming language. In C for example, a variable x could be defined to be a pointer to a location in a process' own virtual address space. This virtual address space could actually be XPMEM-attach memory which maps to another process' address space. Reading from that process' address space (a load) would be as simple as dereferencing the variable x itself. Writing to that process' address space (a store) would be as simple as storing a new value to the location which the variable x represents.","If process  is to discontinue sharing the range of virtual address space previously permitted , e.g., when process  is being terminated or process  performs an xpmem_remove( ) operation, the procedure illustrated in  is performed. Process  revokes  access to the range of virtual address space by calling function xpmem_remove in XPMEM library . In response, XPMEM driver  sends  a message to all processes to detach and release the address space. The xpmem_remove( ) operation results in an XPC message being sent to all partitions which have processes attached to the source address space range. Upon receipt of this message on the remote partition, a separate XPMEM kernel thread is created which effectively does the xpmem_detach( ) and xpmem_release( ) operations on behalf of the process which originally did the xpmem_get( ) and xpmem_attach( ) in the first place. If the process should then try to use that previously attached memory, it will fail as a valid physical translation is no longer available for that virtual address range. Once all processes attached to the virtual address segment being removed have completed these steps, XPMEM driver  unpins  the underlying physical page(s) of the source process to permit swapping.","As illustrated in , process  (or any other process that has established access to the virtual address space) requests  detachment of the address space by calling xpmem_detach using as an argument the virtual address in the virtual space of process .","The process of releasing is illustrated in . Process  (or any other process that has detached the previously detached address space of processor ) requests  release of the address space by calling xpmem_release in XPMEM library  using its identifier for the virtual address space. In response, XPMEM frees  data structures and relationship information created by the function call xpmem_get.","The present invention has been described primarily with respect to a CC-NUMA system running IRIX or LINUX and using a proprietary shared memory programming model for multi-host parallel processing. However, the present invention is not limited to this operating environment and can be advantageously applied to many others, including MPI-1, MPI-2, Open MP, MLP, CoArray Fortran, HPF and other parallel programming models.","By providing access to cacheable memory in the manner described above, generated code from existing compilers will access the remote memory efficiently since ordinary cacheable loads and stores are used to reference remote memory. Also, the invention provides more flexibility in writing algorithms that balance load between cooperating parallel processes, because work stealing can be done in-place rather than by copying the work, operating on it, and putting it back. In addition, existing compilers can generate efficient code for accessing small-strided transfers by built-in processor queues that allow multiple loads or stores to be operating concurrently, where each stride is a gap between consecutive memory words that are accessed.","The many features and advantages of the invention are apparent from the detailed specification and, thus, it is intended by the appended claims to cover all such features and advantages of the invention that fall within the true spirit and scope of the invention. Further, since numerous modifications and changes will readily occur to those skilled in the art, it is not desired to limit the invention to the exact construction and operation illustrated and described, and accordingly all suitable modifications and equivalents may be resorted to, falling within the scope of the invention."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 3A"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 3B"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIGS. 5A-5C"}]},"DETDESC":[{},{}]}
