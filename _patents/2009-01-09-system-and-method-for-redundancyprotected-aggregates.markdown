---
title: System and method for redundancy-protected aggregates
abstract: The present invention provides a system and a method for utilizing a parity protection module to back up data on striped aggregates. Specifically, the system computes parity data for data stored at a particular location of each of a plurality of constituent aggregates, and stores the parity on one of the constituent aggregates that is a parity owner for that particular location of data. In the event one of the constituent aggregates fails, new data may still be accessed by the system (the striped aggregates), both to write new data, and to read data stored on the failed aggregate. In particular, the parity protection module allows clients to read data from a failed aggregate by running a reverse parity computation, which may also be used to restore the data to the failed aggregate.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08495417&OS=08495417&RS=08495417
owner: NetApp, Inc.
number: 08495417
owner_city: Sunnyvale
owner_country: US
publication_date: 20090109
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF AN ILLUSTRATIVE EMBODIMENT"],"p":["The present invention is directed to storage systems, and, in particular, to redundancy-protected aggregates on one or more storage systems.","A storage system typically comprises one or more storage devices into which information may be entered, and from which information may be obtained, as desired. The storage system includes a storage operating system that functionally organizes the system by, inter alia, invoking storage operations in support of a storage service implemented by the system. The storage system may be implemented in accordance with a variety of storage architectures including, but not limited to, a network-attached storage environment, a storage area network and a disk assembly directly attached to a client or host computer. The storage devices may be persistent electronic storage devices, such as flash memories, but are typically disk drives organized as a disk array, wherein the term \u201cdisk\u201d commonly describes a self-contained rotating magnetic media storage device. The term disk in this context is synonymous with hard disk drive (HDD) or direct access storage device (DASD).","Storage of information on the disk array is illustratively implemented on one or more storage volumes of physical disks, defining an overall logical arrangement of storage space. The storage operating system of the storage system may implement a high-level module, such as a file system, to logically organize the information stored on the volumes as a hierarchical structure of data containers, such as files and logical units. For example, each \u201con-disk\u201d file may be implemented as set of data structures, i.e., disk blocks, configured to store information, such as the actual data for the file. These data blocks are organized within a volume block number (vbn) space that is maintained by the file system.","The storage system may be further configured to operate according to a client\/server model of information delivery to thereby allow many clients to access data containers stored on the system. In this model, the client may comprise an application, such as a database application, executing on a computer that \u201cconnects\u201d to the storage system over a computer network, such as a point-to-point link, shared local area network (LAN), wide area network (WAN), or virtual private network (VPN) implemented over a public network such as the Internet. Each client may request the services of the storage system by issuing file-based and block-based protocol messages (in the form of packets) to the system over the network.","A plurality of storage systems or nodes may be interconnected to provide a storage system cluster configured to service many clients. Each storage system may be configured to service one or more aggregates, wherein each aggregate contains one or more volumes of disks. Aggregates are further described in commonly owned, U.S. Patent Publication No. 2005\/0246401, entitled EXTENSION OF WRITE ANYWHERE FILE SYSTEM LAYOUT, by John K. Edwards et al., now issued as U.S. Pat. No. 7,409,494 on Aug. 5, 2008, the contents of which are hereby incorporated by reference. Aggregates can fail for a number of reasons, including lost connectivity, failure of a significant number of disks within a volume and\/or aggregate, etc. When an aggregate fails, clients may be unable to access the data contained on the failed aggregate.","Typically, the disks of a volume\/aggregate are organized into Redundant Arrays of Independent (or Inexpensive) Disk (RAID) groups. Most RAID implementations enhance the reliability\/integrity of the data storage through the redundant writing of data \u201cstripes\u201d across a given number of physical disks in the RAID group and by storing redundancy information (e.g., parity) with respect to the striped data. The use of a RAID group thus protects data locally stored in the group of the aggregate. That is, RAID groups generally provide protection against the loss of data on one or more disks within the group of an aggregate, which is served by a particular storage system. If the storage system itself fails, however, then the data stored on the served aggregate is no longer accessible to the client, thus resulting in aggregate failure.","One solution to such aggregate failure has been to create a mirrored image (\u201cmirror\u201d) of the data contained on the aggregate and service that mirror on another storage system. Mirroring of an aggregate typically requires complete duplication of storage system resources, including storage devices, resulting in an inefficient use of storage space (for example by utilizing half of the overall space consumed on a storage system) and substantial operating costs. Additionally, the response time in some mirrored systems, e.g., a mirrored synchronous storage system, may be especially slow because such systems store data in both mirrors before the systems can respond to clients that the data has been persistently stored.","The present invention overcomes the disadvantages of the prior art by providing a storage architecture that implements redundancy-protected aggregates across a plurality of nodes interconnected as a cluster. Each node is embodied as a storage system that is primarily responsible for servicing a locally attached aggregate. Moreover, each storage system is associated with a designated \u201cpartner\u201d storage system in the cluster that is configured to service the aggregate in the event of a failure. That is, redundancy-protected aggregates are configured so that if a storage system (e.g., its attached aggregate) fails, the storage system (or its partner) can reconstruct the data which would be otherwise in-accessible from the failed aggregate.","To that end, a plurality of aggregates of the cluster is illustratively organized as \u201cstriped aggregates.\u201d The striped aggregates illustratively comprise a plurality of constituent aggregates where each constituent aggregate comprises a plurality of disks, e.g., organized into one or more RAID groups. Specifically, when data is written to the disks on a particular aggregate, the data is written to (e.g., striped across) each of the disks of that aggregate. The written data is then compared with data of the remaining constituent aggregates to compute corresponding redundancy information, e.g., parity, which is stored on one of the aggregates at a corresponding location (a \u201cparity owner\u201d aggregate). For instance, a logical, e.g., an exclusive OR, (XOR) value may be computed to determine whether the related parity should be changed on the particular parity owner.","Illustratively, a block range of the storage space of each aggregate is divided into arbitrary fixed-size \u201cparity regions\u201d wherein within each region only one constituent aggregate is assigned as the parity owner. Ownership of a parity region, however, may be distributed evenly across the cluster so that no constituent aggregate is designated to serve as the parity owner of a region more often than any other constituent aggregate. Therefore, for any given block in a plurality of N aggregates, N-1 of the aggregates is a group of data\/consumer aggregates, and an Nth aggregate is a parity owner aggregate at a particular offset within the block storage space. The consumer aggregates store their own data at an offset of the storage space while the parity owner aggregates store parity of the consumer aggregates at that same offset. For example, in order to maintain and store the parity protected data, each constituent aggregate may illustratively reserve 1\/Nth of its own storage space capacity for storing the parity of data corresponding to the other constituent aggregates, wherein N is the number of aggregates in the striped aggregates.","According to one or more embodiments described herein, redundancy-protected aggregates are configured so that if one aggregate of a clustered storage system fails, the storage system (or its partner) can reconstruct the data stored on the failed aggregate, which would be otherwise inaccessible by the other storage systems of the cluster. For instance, striped aggregates illustratively comprise a plurality of constituent aggregates. Redundancy information, e.g., parity, is distributed among the constituent aggregates based upon the number of constituent aggregates in the cluster and arbitrary fixed-size \u201cparity regions,\u201d wherein within each region only one constituent aggregate is assigned as a parity owner. During a \u201cnormal\u201d mode of operation, data is written to an aggregate and parity computed for the data is written to a corresponding parity owner, e.g., based on the parity region of the written data and the constituent aggregates of the striped aggregates. Upon failure of an aggregate, a \u201cdegraded\u201d mode is entered where the storage system utilizes the parity of the distributed parity regions to determine and serve the data of the failed aggregate. Once the failed aggregate is restored or replaced, a \u201crebuild\u201d mode may provide any updates (or the entire data set) to the restored aggregate.","A. Cluster Environment",{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 1","b":["200","100","200","100","200","310","350","310","200","180","140","350","130","120","200","150"]},"An exemplary distributed file system architecture is generally described in U.S. Patent Application Publication No. US 2002\/0116593 titled METHOD AND SYSTEM FOR RESPONDING TO FILE SYSTEM REQUESTS, by M. Kazar et al. published Aug. 22, 2002, now issued as U.S. Pat. No. 6,671,773 on Dec. 30, 2003. It should be noted that while there is shown an equal number of N and D-modules in the illustrative cluster , there may be differing numbers of N and\/or D-modules in accordance with various embodiments of the present invention. For example, there may be a plurality of N-modules and\/or D-modules interconnected in a cluster configuration  that does not reflect a one-to-one correspondence between the N and D-modules. As such, the description of a node  comprising one N-module and one D-module should be taken as illustrative only.","The clients  may be general-purpose computers configured to interact with the node  in accordance with a client\/server model of information delivery. That is, each client may request the services of the node, and the node may return the results of the services requested by the client, by exchanging packets over the network . The client may issue packets including file-based access protocols, such as the Common Internet File System (CIFS) protocol or Network File System (NFS) protocol, over the Transmission Control Protocol\/Internet Protocol (TCP\/IP) when accessing information in the form of files and directories. Alternatively, the client may issue packets including block-based access protocols, such as the Small Computer Systems Interface (SCSI) protocol encapsulated over TCP (iSCSI) and SCSI encapsulated over Fibre Channel (FCP), when accessing information in the form of blocks.","B. Storage System Node",{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 2","b":["200","222","224","225","226","228","232","230","223","230","235","226","200","100","226","100"],"i":["a","b, "]},"Each node  is illustratively embodied as a dual processor storage system executing a storage operating system  that preferably implements a high-level module, such as a file system, to logically organize the information as a hierarchical structure of named directories, files and special types of files called virtual disks (hereinafter generally \u201cblocks\u201d) on the disks. However, it will be apparent to those of ordinary skill in the art that the node  may alternatively comprise a single or more than two processor system. Illustratively, one processor a executes the functions of the N-module  on the node, while the other processor b executes the functions of the D-module .","The memory  illustratively comprises storage locations that are addressable by the processors and adapters for storing software program code and data structures associated with the present invention. The processors and adapters may, in turn, comprise processing elements and\/or logic circuitry configured to execute the software code and manipulate the data structures. The storage operating system , portions of which is typically resident in memory and executed by the processing elements, functionally organizes the node  by, inter alia, invoking storage operations in support of the storage service implemented by the node. It will be apparent to those skilled in the art that other processing and memory means, including various computer readable media, may be used for storing and executing program instructions pertaining to the invention described herein.","The network adapter  comprises a plurality of ports adapted to couple the node  to one or more clients  over point-to-point links, wide area networks, virtual private networks implemented over a public network (e.g., Internet) or a shared local area network. The network adapter  thus may comprise the mechanical, electrical and signaling circuitry needed to connect the node to the network. Illustratively, the computer network  may be embodied as an Ethernet network or a Fibre Channel (FC) network. Each client  may communicate with the node over network  by exchanging discrete frames or packets of data according to pre-defined protocols, such as TCP\/IP.","The storage adapter  cooperates with the storage operating system  executing on the node  to access information requested by the clients. The information may be stored on any type of attached array of writable storage device media such as video tape, optical, DVD, magnetic tape, bubble memory, electronic random access memory (e.g., flash memory), micro-electro mechanical and any other similar media adapted to store information, including data and redundancy (e.g., parity) information. However, as illustratively described herein, the information is preferably stored on the disks  of array . The storage adapter comprises a plurality of ports having input\/output (I\/O) interface circuitry that couples to the disks over an I\/O interconnect arrangement, such as a conventional high-performance, FC link topology.","Storage of information on each array  is preferably implemented as one or more storage \u201cvolumes\u201d that comprise a collection of physical storage disks  cooperating to define an overall logical arrangement of volume block number (vbn) space on the volume(s). Each volume is generally, although not necessarily, associated with its own file system. The disks within a volume may be further organized as an aggregate comprising one or more groups of disks, wherein each group may be operated as a Redundant Array of Independent (or Inexpensive) Disks (RAID). Most RAID implementations, such as a RAID-4 level implementation, enhance the reliability\/integrity of data storage through the redundant writing of data \u201cstripes\u201d across a given number of physical disks in the RAID group, and the appropriate storing of parity information with respect to the striped data.","Parity protection is used in the storage system to protect against loss of data on a storage device, such as a disk. A parity value may be computed by summing (usually modulo 2) data of a particular word size (usually one bit) across a number of similar disks holding different data and then storing the results on an additional similar disk. That is, parity may be computed on vectors 1-bit wide, composed of bits in corresponding positions on each of the disks. When computed on vectors 1-bit wide, the parity can be either the computed sum or its complement; these are referred to as even and odd parity respectively. Addition and subtraction are on -bit vectors equivalent to an exclusive-OR (XOR) logical operation and accordingly, the addition and subtraction operations are replaced by XOR operations. The data is then protected against the loss of any of the disks. If the disk storing the parity is lost, the parity can be regenerated from the data. If one of the data disks is lost, the data can be regenerated by adding the contents of the surviving data disks together and then subtracting the result from the stored parity.","Typically, the disks are divided into parity groups, each of which comprises one or more data disks and a parity disk. The disk storage space is divided into stripes, with each stripe containing one block from each disk. The blocks of a stripe are usually at the same locations on each disk in the group. Within a stripe, all but one of the blocks contain data (\u201cdata blocks\u201d) and one of the blocks contains parity (\u201cparity block\u201d) computed by the XOR of all the data. If the parity blocks are all stored on one disk, thereby providing a single disk that contains all (and only) parity information, a RAID-4 implementation is provided. If the parity blocks are contained within different disks in each stripe, usually in a rotating pattern, then the implementation is referred to as RAID-5. While illustrative examples of RAID implementations are a RAID-4 or RAID-5 level implementation, it should be understood that other types and levels of RAID implementations may be used in accordance with the inventive principles described herein.","The NVRAM  may be embodied as a solid state random access memory array having either a back-up battery, or other built-in last-state-retention capabilities (e.g., flash memory), that holds the last state of the memory in the event of any power loss to the array. A portion of the NVRAM  is organized as a Non-Volatile Log (NVLOG ) configured to provide a temporary, yet persistent, storage space capable of maintaining write requests, including write data (updates), directed to data containers served by the node (storage system), particularly in light of a failure to the system. To that end, the NVLOG  stores write data prior to the data being stored on disk, thereby improving responsiveness to client requests.","Illustratively, the NVRAM  (e.g., the NVLOG ) may be organized into a plurality of areas, including, e.g., a message area , a \u201cSend-WIF\u201d area , a \u201cReceive-WIF\u201d area  and one or more Rebuild Bitmaps , each as described herein. In particular, the message area  is utilized to store write data received from client operations directed to a data container that is being serviced by the storage system. The Send-WIF area  is utilized to provide atomicity between writing a block locally (e.g., to a locally-attached aggregate of the storage system) and sending a parity update request to a remote node, as described further below by storing a record of the transactions. Once the block has been written to a local disk of the aggregate and the parity has been updated, the records are removed from the Send WIF area. In alternative embodiments, the Send-WIF area  may be mirrored to a failover partner. The NVLOG  also implements the Receive-WIF area  to ensure that there is only one set of semantics written to parity. For example, when a record and an associated transaction identifier (ID) are written to this area in accordance with a first request, the NVLOG  will detect a second duplicate request that attempts to perform the same parity write, and thus delete the second request. In accordance with an illustrative embodiment of the present invention, the Receive-WIF area may mirror its records with a failover partner. Finally, at least one Rebuild Bitmap area  is allocated in the NVLOG  for each aggregate. Initially these bitmap areas are clear (empty), and are only populated when the cluster is in degraded mode to indicate which regions of an aggregate have been dirtied and is rebuilt during recovery.","C. Storage Operating System","To facilitate access to the disks , the storage operating system  may illustratively implement a write-anywhere file system that cooperates with one or more virtualization modules to \u201cvirtualize\u201d the storage space provided by disks . The file system logically organizes the information as a hierarchical structure of named data containers, such as directories and files on the disks. Each \u201con-disk\u201d file may be implemented as set of disk blocks configured to store information, such as data, whereas the directory may be implemented as a specially formatted file in which names and links to other files and directories are stored. The virtualization module(s) allow the file system to further logically organize information as a hierarchical structure of data containers, such as blocks on the disks that are exported as named logical unit numbers (luns).","In the illustrative embodiment, the storage operating system is preferably the NetApp\u00ae Data ONTAP\u00ae operating system available from NetApp, Inc., of Sunnyvale, Calif. that implements a Write Anywhere File Layout (WAFL\u00ae) file system. However, it is expressly contemplated that any appropriate storage operating system may be enhanced for use in accordance with the inventive principles described herein. As such, where the term Data ONTAP\u00ae is employed, it should be taken broadly to refer to any storage operating system that is otherwise adaptable to the teachings of this invention.",{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIG. 3","b":["300","325","312","314","316","315","318","320","322","324","326","318","328","330","200"]},"In addition, the storage operating system includes a series of software layers organized to form a storage server  that provides data paths for accessing information stored on the disks  of the node . To that end, the storage server  includes a file system module  in cooperating relation with a parity protection module , a RAID system module  and a disk driver system module . The RAID system  manages the storage and retrieval of information to and from the volumes\/disks in accordance with I\/O operations, while the disk driver system  implements a disk access protocol such as, e.g., the SCSI protocol. The parity protection module  implements striped aggregates in accordance with an illustrative embodiment of the present invention as described herein. It should be noted that while the parity protection module  is shown interposed between the file system  and the RAID system , the functionality of the parity protection module  may be alternatively integrated into other modules e.g., the RAID system and\/or the file system . As such, the description of a separate parity protection module  should be taken as illustrative only.","The file system  implements a virtualization system of the storage operating system  through the interaction with one or more virtualization modules illustratively embodied as, e.g., a virtual disk (vdisk) module (not shown) and a SCSI target module . The vdisk module enables access by administrative interfaces, such as a user interface of a management framework, in response to a user (system administrator) issuing commands to the node . The SCSI target module  is generally disposed between the FC and iSCSI drivers ,  respectively and the file system  to provide a translation layer of the virtualization system between the block (lun) space and the file system space, where luns are represented as blocks.","The file system  is illustratively a message-based system that provides logical volume management capabilities for use in access to the information stored on the storage devices, such as disks. That is, in addition to providing file system semantics, the file system  provides functions normally associated with a volume manager. These functions include (i) aggregation of the disks, (ii) aggregation of storage bandwidth of the disks, and (iii) reliability guarantees, such as mirroring and\/or parity (RAID). The file system  illustratively implements the WAFL file system (hereinafter generally the \u201cwrite-anywhere file system\u201d) having an on-disk format representation that is block-based using, e.g., 4 kilobyte (kB) blocks and using index nodes (\u201cinodes\u201d) to identify files and file attributes (such as creation time, access permissions, size and block location). The file system uses files to store meta-data describing the layout of its file system; these meta-data files include, among others, an inode file. A file handle, i.e., an identifier that includes an inode number, is used to retrieve an inode from disk.","Broadly stated, all inodes of the write-anywhere file system are organized into the inode file. A file system (fs) info block specifies the layout of information in the file system and includes an inode of a file that includes all other inodes of the file system. Each logical volume (file system) has an fsinfo block that is preferably stored at a fixed location within, e.g., a RAID group. The inode of the inode file may directly reference (point to) data blocks of the inode file or may reference indirect blocks of the inode file that, in turn, reference data blocks of the inode file. Within each data block of the inode file are embedded inodes, each of which may reference indirect blocks that, in turn, reference data blocks of a file.","Operationally, a request from the client  is forwarded as a packet over the computer network  and onto the node  where it is received at the network adapter . A network driver (of layer  or layer ) processes the packet and, if appropriate, passes it on to a network protocol and file access layer for additional processing prior to forwarding to the write-anywhere file system . Here (e.g., for a read request), the file system generates operations to load (retrieve) the requested data from disk  if it is not resident \u201cin core\u201d, i.e., in memory . If the information is not in memory, the file system  indexes into the inode file using the inode number to access an appropriate entry and retrieve a logical vbn. The file system then passes a message structure including the logical vbn to the RAID system ; the logical vbn is mapped to a disk identifier and disk block number (disk, dbn) and sent to an appropriate driver (e.g., SCSI) of the disk driver system . The disk driver accesses the dbn from the specified disk  and loads the requested data block(s) in memory for processing by the node. Upon completion of the request, the node (and operating system) returns a reply to the client  over the network .","It should be noted that the software \u201cpath\u201d through the storage operating system layers described above needed to perform data storage access for the client request received at the node may alternatively be implemented in hardware. That is, in an alternate embodiment of the invention, a storage access request data path may be implemented as logic circuitry embodied within a field programmable gate array (FPGA) or an application specific integrated circuit (ASIC). This type of hardware implementation increases the performance of the storage service provided by node  in response to a request issued by client . Moreover, in another alternate embodiment of the invention, the processing elements of adapters  and\/or  may be configured to offload some or all of the packet processing and storage access operations, respectively, from processor , to thereby increase the performance of the storage service provided by the node. It is expressly contemplated that the various processes, architectures and procedures described herein can be implemented in hardware, firmware or software.","As used herein, the term \u201cstorage operating system\u201d generally refers to the computer-executable code operable on a computer to perform a storage function that manages data access and may, in the case of a node , implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel, an application program operating over a general-purpose operating system, such as UNIX\u00ae or Windows NT\u00ae, or as a general-purpose operating system with configurable functionality, which is configured for storage applications as described herein.","In addition, it will be understood to those skilled in the art that the invention described herein may apply to any type of special-purpose (e.g., file server, filer or storage serving appliance) or general-purpose computer, including a standalone computer or portion thereof, embodied as or including a storage system. Moreover, the teachings of this invention can be adapted to a variety of storage system architectures including, but not limited to, a network-attached storage environment, a storage area network and disk assembly directly-attached to a client or host computer. The term \u201cstorage system\u201d should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems. It should be noted that while this description is written in terms of a write any where file system, the teachings of the present invention may be utilized with any suitable file system, including a write in place file system.","D. CF Protocol","In the illustrative embodiment, the storage server  is embodied as D-module  of the storage operating system  to service one or more volumes of array . In addition, the multi-protocol engine  is embodied as N-module  to (i) perform protocol termination with respect to a client issuing incoming data access request packets over the network , as well as (ii) redirect those data access requests to any storage server  of the cluster . Moreover, the N-module  and D-module  cooperate to provide a highly-scalable, distributed storage system architecture of the cluster . To that end, each module includes a cluster fabric (CF) interface module adapted to implement intra-cluster communication among the modules, including D-module-to-D-module communication for data container striping operations described herein.","The protocol layers, e.g., the NFS\/CIFS layers and the iSCSI\/FC layers, of the N-module  function as protocol servers that translate file-based and block based data access requests from clients into CF protocol messages used for communication with the D-module . That is, the N-module servers convert the incoming data access requests into file system primitive operations (commands) that are embedded within CF messages by the CF interface module  for transmission to the D-modules  of the cluster . Notably, the CF interface modules  cooperate to provide a single file system image across all D-modules  in the cluster . Thus, any network port of an N-module that receives a client request can access any data container within the single file system image located on any D-module  of the cluster.","Further to the illustrative embodiment, the N-module  and D-module  are implemented as separately-scheduled processes of storage operating system ; however, in an alternate embodiment, the modules may be implemented as pieces of code within a single operating system process. Communication between an N-module and D-module is thus illustratively effected through the use of message passing between the modules although, in the case of remote communication between an N-module and D-module of different nodes, such message passing occurs over the cluster switching fabric . A known message-passing mechanism provided by the storage operating system to transfer information between modules (processes) is the Inter Process Communication (IPC) mechanism. The protocol used with the IPC mechanism is illustratively a generic file and\/or block-based \u201cagnostic\u201d CF protocol that comprises a collection of methods\/functions constituting a CF application programming interface (API). Examples of such an agnostic protocol are the SpinFS and CF protocols available from NetApp, Inc. The SpinFS protocol is described in the above-referenced METHOD AND SYSTEM FOR RESPONDING TO FILE SYSTEM REQUEST, U.S. Patent Publication No. US 2002\/0116593, by Michael Kazar et al., now issued as U.S. Pat. No. 6,671,773 on Dec. 30, 2003, the contents of which are hereby incorporated by reference.","The CF interface module  implements the CF protocol for communicating file system commands among the modules of cluster . Communication is illustratively effected by the D-module exposing the CF API to which an N-module (or another D-module) issues calls. To that end, the CF interface module  is organized as a CF encoder and CF decoder. The CF encoder of, e.g., CF interface on N-module , encapsulates a CF message as (i) a local procedure call (LPC) when communicating a file system command to a D-module  residing on the same node  or (ii) a remote procedure call (RPC) when communicating the command to a D-module residing on a remote node of the cluster . In either case, the CF decoder of CF interface on D-module  decapsulates the CF message and processes the file system command.",{"@attributes":{"id":"p-0062","num":"0061"},"figref":"FIG. 4","b":["400","400","150","100","400","402","404","406","408","410","100","410","400","310","350","408","406"]},"E. File System Organization",{"@attributes":{"id":"p-0064","num":"0063"},"figref":"FIG. 5","b":["500","500","500","502","504","506","508","510","500","500","550","550","560","530"]},"Whereas the aggregate  is analogous to a physical volume of a conventional storage system, a flexible volume is analogous to a file within that physical volume. That is, the aggregate  may include one or more files, wherein each file contains a flexible volume  and wherein the sum of the storage space consumed by the flexible volumes is physically smaller than (or equal to) the size of the overall physical volume. The aggregate utilizes a physical PVBN space that defines a storage space of blocks provided by the disks of the physical volume, while each embedded flexible volume (within a file) utilizes a logical VVBN space to organize those blocks, e.g., as files. Each VVBN space is an independent set of numbers that corresponds to locations within the file, which locations are then translated to dbns on disks.","F. VLDB",{"@attributes":{"id":"p-0067","num":"0066"},"figref":"FIG. 6","b":["600","300","610","630","650","610","670","200","100"]},"The VLDB  is a database process that tracks the locations of various storage components (e.g., aggregates) within the cluster  to thereby facilitate routing of requests throughout the cluster. The VLDB includes a plurality of entries which, in turn, provide the contents of entries in the configuration table ; among other things, these VLDB entries keep track of the locations of the flexible volumes (hereinafter generally \u201cvolumes \u201d) and aggregates  within the cluster.","The VLDB illustratively implements a RPC interface, e.g., a Sun RPC interface, which allows the N-module  to query the VLDB . When encountering contents that are not stored in its configuration table, the N-module sends an RPC to the VLDB process. In response, the VLDB  returns to the N-module the appropriate mapping information, including an identifier (ID) of the D-module that owns the data container. The N-module caches the information in its configuration table  and uses the D-module ID to forward the incoming request to the appropriate data container. All functions and interactions between the N-module  and D-module  are coordinated on a cluster-wide basis through the collection of management processes and the RDB library user mode applications .","To that end, the management processes have interfaces to (are closely coupled to) RDB . The RDB comprises a library that provides a persistent object store (storing of objects) for the management data processed by the management processes. Notably, the RDB  replicates and synchronizes the management data object store access across all nodes  of the cluster  to thereby ensure that the RDB database image is identical on all of the nodes . At system startup, each node  records the status\/state of its interfaces and IP addresses (those IP addresses it \u201cowns\u201d) into the RDB database.","F. Striped Aggregates","As noted, according to one or more embodiments described herein, redundancy-protected aggregates are configured so that if one aggregate of the cluster fails, a storage system of the cluster can reconstruct the data which would be otherwise inaccessible by the cluster. For instance, striped aggregates illustratively comprise a plurality of constituent aggregates. Parity is distributed among the constituent aggregates based upon the number of constituent aggregates in the cluster and the arbitrary fixed-size \u201cparity regions,\u201d wherein within each region only one constituent aggregate is assigned as the parity owner.",{"@attributes":{"id":"p-0073","num":"0072"},"figref":"FIG. 7","b":["700","700","714","724","734","744","712","722","732","742","710","720","730","740","703","703"]},"Striped aggregates  as described herein are made up of N aggregates, each of which is illustratively built of local RAID shelves operatively interconnected within storage systems A-D. In accordance with an illustrative embodiment, data for constituent aggregate A is stored on storage devices locally attached to storage system A. Additionally, operations that affect aggregate A cause parity to be updated elsewhere in striped aggregates . Furthermore, constituent aggregate A () is illustratively designated as a failover partner of storage system B . A failover partner is utilized to obtain redundancy information (e.g., parity) in the case of failure by an aggregate in the striped aggregates . This means that if aggregate A () were to fail, the client  would still be able to access the information using partner aggregate B () based on the parity stored on constituent aggregates (e.g., B-D). For instance, if storage system A fails, storage system B is responsible for taking over storage system A's role in cluster by, e.g., simulating access to the disks in aggregate A () to perform reverse parity computations from the remaining data available in the other constituent aggregates (described further herein). Furthermore, NVRAM mirroring () is applied between an aggregate and the aggregate's failover partner. For example, if aggregate B is aggregate A's failover partner, information stored in the NVRAM  on aggregate A is also stored on the NVRAM in aggregate B.",{"@attributes":{"id":"p-0075","num":"0074"},"figref":"FIG. 8","b":["800","800","801","804"]},"Furthermore, in order to prevent the cluster from introducing a performance bottleneck, the parity ownership role is distributed. The PVBN range of striped aggregates  (See , , and ) is divided into arbitrary fixed size \u201cparity regions\u201d -. Within regions -, one constituent aggregate is assigned as the parity owner. In the exemplary embodiment, aggregate A () is responsible for parity within parity region  () and region  (). At any given time, for any PVBN within either region  or , the corresponding block in aggregate A stores redundancy information for data contained in the equivalent PVBNs in aggregates B (), C () and D (). Ownership of the parity region may be distributed using the process as described in U.S. Patent Publication DATA ALLOCATION WITHIN A STORAGE SYSTEM ARCHITECTURE, by Richard Jernigan et al., 2006\/0248273, now issued as U.S. Pat. No. 7,617,370 on Nov. 10, 2009, the contents of which are hereby incorporated by reference. The algorithm described therein, as applied to aggregates, prevents a constituent aggregate (i.e., its associated parity protection module ) from being designated as a parity owner more often than any other constituent aggregate. In addition to balanced consumption, the algorithm also prevents the cluster from \u201chotspotting,\u201d meaning that the parity protection module may not assume parity ownership for multiple consecutive regions in an aggregate. Finally, the algorithm renders addition or subtraction of aggregates easier, for example, as described below (e.g.,  and ).",{"@attributes":{"id":"p-0077","num":"0076"},"figref":["FIG. 9","FIG. 8"],"b":["900","910","920","930","903","906","909","1","903","900","1","2","2","903","906","909"]},"Notably, redundancy protected aggregates may be expanded by either adding a storage device to each constituent aggregate in the cluster or by adding a new constituent aggregate to the cluster.  is a schematic block diagram illustrating addition of storage devices, e.g., a fourth storage device to existing storage devices (storages devices , , and , respectively) of striped aggregates  in accordance with an illustrative embodiment of the present invention. Note that the ranges with no shading in the added storage devices represent zero-filled ranges, i.e., PVBN ranges that have yet to be populated. The PVBN ranges that are shaded in the newly added storage devices are parity owner regions which are only populated with parity on that particular aggregate. By adding the same geometry to every constituent aggregate a new parity owner region is an XOR of PVBN blocks from the corresponding previously populated parity owner regions.",{"@attributes":{"id":"p-0079","num":"0078"},"figref":["FIG. 11","FIG. 9"],"b":["1140","900","5","0","8","910","920","930","5","0","8","1140"]},"H. Striped Aggregates Operation and Operational Modes","Operationally, parity protection may illustratively utilize storage space in a persistent storage device of the storage system , such as NVRAM . As noted, each storage system may organize and allocate a \u201cwrite-in-flight\u201d (WIF) area in the NVRAM. Data stored in the WIF area coincides with the data being written to a block locally, such as data stored in a Non-Volatile Log (NVLOG)  (e.g., in response to a write request received at the \u201clocal\u201d storage system). A parity update request is sent from a Send-WIF area , located in the WIF area of the NVRAM , to a remote storage system of the cluster (e.g., the parity owner aggregate). Records of both sending and receiving the requests may be stored in the Send-WIF area (on sending storage system) and Receive WIF area (of the receiving storage system). When the two requests have completed for a particular block of data, these records (entries) are removed from the NVRAM . As noted, the local storage system also organizes and allocates at least one Receive-WIF area  in the WIF area of the NVRAM . The Receive-WIF area  records data configured to ensure that the cluster can detect duplicate requests that attempt to perform the same parity update. In addition, the storage system organizes and allocates a Rebuild area\/bitmap  of the NVRAM  for each aggregate owned by the system. The Rebuild area is initially clear and then populated during a degraded mode (described below) to indicate which parity regions of the aggregate have been \u201cdirtied\u201d (updated) and are rebuilt during recovery from a failed aggregate. These areas associated with NVRAM  reflect the number of parity updates that are currently outstanding at any given time in the cluster. Therefore, only when local data and parity updates have completed may the associated records be removed (flushed) from the NVRAM . In an illustrative embodiment of the present invention, the cluster (e.g., striped aggregates) is capable of running in various operational modes. For example, depending upon various states of the storage system, the striped aggregates may operate in a normal mode, or in a degrade mode, suspended mode, or rebuild mode, as described herein.","Certain information is temporarily and persistently stored in the NVLOG. The NVLOG  temporarily stores such information to increase the reliability and performance of the storage system. Write data received and processed by the storage system is illustratively written to disk during a consistency model event, e.g., a Consistency Point (CP). Illustratively, the CP may be initiated when a predetermined portion (e.g., one half) of the NVLOG is filled. In response, the storage system flushes the write data to both its local aggregate and the owner of the corresponding parity block in the cluster. This process is known as normal mode. Upon completion of the CP, the storage system can then discard\/remove the information from the NVRAM .","In particular, in normal mode, the parity protection module  of the local storage system responds to a write request for new data by reading the data currently in the block to be overwritten, computing an XOR for the new data and current data, and creating a new Send-WIF record to store both the new data and the redundancy information (parity) in the NVLOG . As soon as the write request is acknowledged by the parity storage system (parity owner), the storage system sends a write response back to the client. In parallel, the storage system writes the information (e.g., write data) stored by the NVLOG  to its local aggregate and sends a request to the owner of the parity block (the parity owner) for an XOR update. The parity owner then creates its own Receive-WIF record by writing a transaction ID and a computed XOR in the parity owner's NVLOG. The parity owner then returns a success response to the local parity protection module . Thereafter, the parity owner writes the computed XOR to disk, thereby allowing the XOR to be deleted from the NVLOG.","At the same time, the data stored on the local NVLOG  is removed once the data has been written to the local aggregate. The XOR-value, however, may be removed once the parity owner responds that the parity has been successfully stored on the parity owner's NVLOG. When both the local data and parity updates have been removed from the NVLOG, the parity protection module of the locally attached aggregate discards the transaction ID. Finally, at the end of the CP, the storage system makes one last purge call to the NVLOG  to ensure that all transactions have been removed.",{"@attributes":{"id":"p-0085","num":"0084"},"figref":"FIG. 12","b":["1202","1242","1206","241","233","1208"]},"Next in step , an XOR update request is sent from the parity protection module of the data aggregate to the parity owner (i.e., parity aggregate ). Any data that has been modified is written (in parallel) to disk (i.e., cached dirty data) in step  on the data aggregate . Once the data has been written to disk, the data is flushed\/deleted from the NVLOG  in step . The redundancy information and transaction ID, however are not deleted until an XOR response is received from the parity owner indicating that the redundancy information has been recorded on the parity owner's NVLOG  in step . While steps  and  are processing, the parity aggregate  illustratively operates in parallel. In step , the parity aggregate writes a transaction ID and the redundancy information to the NVLOG and sends back a response to the parity protection module indicating that the XOR update has completed in step . Then, the parity protection module of data aggregate can delete the redundancy information and transaction ID which were recorded on its NVLOG (steps  and ). At the same time, the parity owner\/aggregate  writes the updated XOR to disk (step ), and deletes the XOR from its NVLOG, keeping the transaction ID until the parity protection module of data aggregate  sends an \u201cXOR complete\u201d response back to the parity owner  indicating the process is complete. At this time, the parity owner  deletes the transaction ID (step ) from the NVLOG and the process repeats. In alternative embodiments, the data aggregate may \u201cpiggyback\u201d requests to the parity owner in order to increase efficiency. Therefore, when sending the \u201cXOR complete\u201d request (step ), the data aggregate also sends a new XOR update request (step ) to the parity owner. (Notably, when receiving a read request while in normal mode, the data aggregate reads files in a conventional manner from their originally stored location on local disks of the data aggregate , as will be understood by those skilled in the art.)","In particular, each aggregate not only stores its own data, but also stores some type of redundancy information (e.g., parity) for another constituent aggregate in the striped aggregate. As noted, when another storage system (the requestor) requests that parity be written to a remote storage system (i.e., the parity aggregate), the requester provides a target aggregate ID, a PVBN within the aggregate being written, an XOR indicating the difference between the old and new block data, and a unique transaction identifier (ID). The transaction ID is used to provide a set of semantics for the target.",{"@attributes":{"id":"p-0088","num":"0087"},"figref":"FIG. 13","b":["1300","1300","1302","1304","1306","243","1308","1310","1312","243","1320","1304","1322"]},"The Send-WIF area  associated with a particular aggregate also keeps track of any write requests for which an aggregate transmits a parity-write, illustratively in the form of records. In an illustrative embodiment, these records store additional information. That is, not only do the records store the transaction ID and XOR data, but the NVLOG  also stores the new (write) data which is being written to the data aggregate. Write data may also be stored in order to ensure that in the event of a failover, a constituent aggregate can complete the local write request exactly as the failed aggregate would have done.","Furthermore, transaction IDs may accumulate in the Receive-WIF over time. For example, upon restart, a storage system may not \u201cremember\u201d that a transaction ID needs to be \u201ccleaned up\u201d and therefore does not notify the storage system associated with the Receive WIF, that the transaction has been committed. Thus, each storage system will send periodic requests to other constituent storage systems that it holds transaction IDs, asking the constituent storage systems whether the transaction IDs are still valid, and thereby allowing the system to clean up old transactions when the response indicates that the transaction IDs are no longer in use.","Once an aggregate fails, the parity protected module transitions the aggregate\/cluster to a degraded mode to retrieve the data requested by the client. In degraded mode, a write request is sent to an aggregate in the cluster while the primary storage for that aggregate is offline (i.e., a failed aggregate has been identified). The parity protection module first receives a write request on the failed aggregate. Note that a failed aggregate may be any aggregate in which the storage devices containing the data cannot be accessed; yet, the D module (i.e. its parity protection module) connected to the failed aggregate may be fully operationally in order to send and receive files from the client. The failed aggregate then sends a read request from its parity protection module to each remote aggregate hosting data for a plurality of target data blocks. Each read request results in locking of the target data blocks and retrieval of the data from the disk. Once the lock is in place on the remote aggregates, all updates to parity for the data blocks are fenced (i.e., no other storage system can modify the parity for the blocks at this time). With this fence in place, the failed aggregate's parity protection module may compute an XOR from all of the remote data blocks as well as the block to which the parity protection module wishes to write. Then the parity protection module on the failed aggregate writes the computed XOR and a transaction ID to the NVLOG  on the failed aggregate. The resulting computation is sent (via a write request) directly to the parity-owning aggregate's parity protection module and a write response is sent back to the client.",{"@attributes":{"id":"p-0092","num":"0091"},"figref":"FIG. 14","b":["1400","1402","1420","1404","1440","1406","1408","1440","1412","1414","233","1420","1418","1460","1424","1426","1432","1440","1436","1438"]},"Notably, when a storage system issues a read request when the cluster is in degraded mode, its parity protection module performs a reverse parity computation to obtain the requested data. A reverse parity computation is performed by using the appropriate stored parity to determine the missing data value. In particular, when a read request is received by the failed aggregate while the cluster is in degraded mode, its parity protection module sends a read request to the other constituent data aggregates. The data is then read from a corresponding disk (i.e., each corresponding PVBN) located on each data aggregate. A read response is thereafter sent back to the parity protection module on the failed aggregate. The parity protection module (of the failed aggregate) computes the reverse parity XOR for all of the remote data blocks in order to determine the current read request. The reverse parity computation result may then be returned to the requesting client from the parity protection module of the failed aggregate, accordingly.","In an illustrative embodiment, the parity protection module of the failed aggregate proceeds to rebuild mode once the failed aggregate comes back online. First, the parity protection module of the failed aggregate sends a write request to the parity owner. This request locks each appropriate data block and writes the incoming data as an updated parity block on the parity owner. The parity owner's parity protection module then unlocks the data block and sends a write response back to the parity protection module of the failed aggregate. The failed aggregate's parity protection module then deletes the XOR and the transaction ID from the NVLOG  and sends an unlock request back to the remote data aggregates to allow the target data blocks to be unlocked for access by the D modules of other constituent aggregates. Finally, the process completes when all of the parity protection modules of the constituent aggregates have sent an unlock response back to the parity protection module on the failed aggregate indicating that it is safe to return to normal mode.","Rebuilding is performed utilizing a rolling fence directed to one parity region at a time. Parity regions \u201cbehind\u201d the fence (e.g., already traversed regions) have been rebuilt and are accessed in accordance with normal mode operation. In contrast, parity regions \u201cahead\u201d of the fence are accessed in accordance with degraded mode operation. The parity region being actively rebuilt is fenced, and all access to that region stalls while the region is being rebuilt.",{"@attributes":{"id":"p-0096","num":"0095"},"figref":"FIG. 15","b":["1505","245","1510","1515"]},"In step , a parity region is rebuilt by splitting the parity region blocks into segments, and transferring the rebuild job for different segments to different constituent aggregates (step ). To rebuild a segment, a previously failed aggregate's parity protection module reads data from its own NVLOG  and all other constituent aggregates (step ), computes the missing complement piece (step ), such as through a reverse parity computation (mentioned above), and sends that piece ( e.g., a summary) back to the previously-failed aggregate, which then writes the data to disk (). The rebuild process may be throttled, to ensure that no more than N rebuild-segment requests are in flight simultaneously. Note that, the larger the value of N, the faster the rebuild completes, but the less responsive the cluster will be for all other traffic during rebuilding. Furthermore, a parity region is illustratively expected to be a predetermined size (e.g., 100 MB), and the size of a single rebuild segment will be influenced by the amount of data that can be sent or received on a single CF call.",{"@attributes":{"id":"p-0098","num":"0097"},"figref":"FIG. 16","b":["1600","630","1605","1615","1613"]},"If data cannot be written to exactly one aggregate in the cluster, then a broadcasting node determines that the parity protection modules of the cluster are in degraded mode (step ) when an aggregate cannot be reached. Degraded mode is further qualified by which aggregate in the cluster is lost due to failure. Furthermore, in addition to qualifying the failed aggregate, the degraded mode also qualifies which aggregate will be simulating the failed aggregate (i.e., the failover partner of the failed aggregate). When the striped aggregates enter degraded mode, the VLDB  records the striped aggregate's current state. If there is more than one failed aggregate (or if the striped aggregates have trouble reaching two or more aggregates) then the parity protection modules in the cluster transitions to suspended mode in step . The decision to remain in suspended mode is complicated by the current state of the striped aggregates. This means that if an aggregate is dirty (e.g., fails while degraded mode is running), then any second failure requires the striped aggregate to enter suspended mode.","A storage system may examine\/analyze its health status to decide on an operational mode that does not match the current mode of the striped aggregate. When it renders a decision to change modes, the storage system (e.g., a broadcasting node configured as such by a system administrator) sends a request to all the other storage systems of the striped aggregates to change to normal (healthy) mode, degraded mode, etc. Each of the storage systems then receives the request and tests itself to determine if the request is compatible with its own health. Once the test is complete, each of the other storage systems responds to the request with a success or a failure. After collecting all of the responses from all of the storage systems, the broadcasting node evaluates the response to determine whether a quorum of the storage systems wish to proceed to replay. Therefore, the striped aggregates remain in suspended mode until a quorum of storage systems agree that the striped aggregates no longer needs to remain in suspended mode in step . Once there is a quorum of storage systems in agreement, the striped aggregate\/cluster proceeds to replay mode (step ), where it waits for a replay of the data to begin. Here, every node\/storage system of the cluster replays any WIF records recorded on the NVLOG in order to synchronize\/change the striped aggregates (step ). In step , any NVLOG records stored while the cluster was in suspended mode are replayed. After a storage system has completed its replay procedure in step , the completed aggregate translates to \u201creplay done\u201d mode (step ), and awaits a message from the broadcasting node that all of the aggregates in the cluster have completed replay in step .","Thereafter, the broadcasting node once again begins sending and receiving requests and responses from the storage systems to determine the operating mode of the striped aggregates (cluster). At this point, the cluster can proceed to degraded mode, normal mode, or rebuild mode (step ). If the broadcasting node decides to first enter degraded mode, however, the cluster transitions to rebuild mode before entering healthy\/normal mode, thereby finally completing in step . (Notably completion in step  may imply a restart (step ) to update the status of whether the cluster is to remain in normal mode or not based on failure of one or more aggregates.)","To again summarize, the present invention provides a system and a method for utilizing a parity protection module to back up data on striped aggregates. Specifically, the parity protection module computes parity for data stored at a particular location of each of a plurality of constituent aggregates, and stores the parity on one of the constituent aggregates that is a parity owner for that particular location of data. In the event one of the constituent aggregates fails, data may still be accessed by the striped aggregates, both to write data, and to read data stored on the failed aggregate. In particular, the parity protection module allows clients to read data from a failed aggregate by performing a reverse parity computation, which may also be used to restore the data to the failed aggregate.","The foregoing description has been directed to specific embodiments of this invention. It will be apparent that other variations and modifications may be made to the described embodiments, with the attainment of some or all of their advantages. For example, it is expressly contemplated that the teachings of this invention can be implemented, including a computer readable medium having program instructions executing on a computer, hardware, firmware, or a combination thereof. Additionally, while this description is written in terms of striped aggregates over parity protected modules, it should be noted that other data container implementations may be utilized. As such, the use of redundancy information (e.g., parity) to support the parity protected modules should be taken as exemplary only. Accordingly this description is to be taken only by way of example and not otherwise limit the scope of the invention. It is thus the object of the appended claims to cover all such variations and modifications as come within the true spirit and scope of the invention."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The above and further advantages of invention may be better understood by referring to the following description in conjunction with the accompanying drawings in which like reference numerals indicate identical or functionally similar elements:",{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 16"}]},"DETDESC":[{},{}]}
