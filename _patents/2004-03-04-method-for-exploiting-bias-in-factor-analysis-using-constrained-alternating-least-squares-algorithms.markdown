---
title: Method for exploiting bias in factor analysis using constrained alternating least squares algorithms
abstract: Bias plays an important role in factor analysis and is often implicitly made use of, for example, to constrain solutions to factors that conform to physical reality. However, when components are collinear, a large range of solutions may exist that satisfy the basic constraints and fit the data equally well. In such cases, the introduction of mathematical bias through the application of constraints may select solutions that are less than optimal. The biased alternating least squares algorithm of the present invention can offset mathematical bias introduced by constraints in the standard alternating least squares analysis to achieve factor solutions that are most consistent with physical reality. In addition, these methods can be used to explicitly exploit bias to provide alternative views and provide additional insights into spectral data sets.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07472153&OS=07472153&RS=07472153
owner: Sandia Corporation
number: 07472153
owner_city: Albuquerque
owner_country: US
publication_date: 20040304
---

{"@attributes":{"id":"description"},"GOVINT":[{},{}],"heading":["STATEMENT OF GOVERNMENT INTEREST","FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION","Exploiting Bias","Connections with Other Work","General Linear Equality Constraints"],"p":["This invention was made with Government support under contract no. DE-AC04-94AL85000 awarded by the U.S. Department of Energy to Sandia Corporation. The Government has certain rights in the invention.","The present invention relates to multivariate statistical techniques and, in particular, to a method for exploiting bias in factor analysis using constrained alternating least squares algorithms.","Bias plays an important role in factor analysis and we often implicitly make use of it. The application of non-negativity constraints in alternating least squares (ALS) procedures, for instance, is nothing more than a statement that we are biased toward factor solutions that conform to physical reality. We are willing to accept a model that provides a poorer mathematical fit to the data in return for model that is easier to interpret in physical terms. When components are collinear, a large range of solutions may exist that satisfy the basic constraints and fit the data equally well. In such cases, the introduction of mathematical bias through the application of constraints may select solutions that are less than optimal. Peak shapes may be distorted, for example, or physically discrete chemical phases may appear to be mixed.","The biased ALS (BALS) algorithms of the present invention are designed to offset mathematical bias introduced by constraints in ALS with the goal of achieving factor solutions that are most consistent with physical reality. Methods are also described that are explicitly intended to exploit bias to provide alternative views and provide additional insights into spectral data sets.","The present invention is directed to a biased alternating least squares algorithm for factoring a data matrix into a product of a first factor matrix and a second factor matrix. An initial feasible estimate is made for a first factor matrix. The first factor matrix is normalized. A first bias parameter is selected and a least squares estimate is obtained for the first factor matrix, subject to constraints on the first factor matrix. A second bias parameter is selected and a least squares estimate is obtained for the second factor matrix, subject to constraints on the second factor matrix. The process of obtaining biased alternating least squares estimates is repeated at least once, or until the estimates converge. Finally, an unbiased estimate of the first factor matrix is computed. The biased alternating least squares algorithm is particularly useful for the analysis of spectroscopic data, wherein the data matrix is factored into a factor matrix comprising estimated pure component spectra and a factor matrix comprising estimated component concentrations.","The data matrix can be preprocessed (e.g., weighted for heteroscedastic noise, spatially and\/or spectrally compressed, etc.). The bias parameters preferably are selected to be between the smallest and largest eigenvalue of the crossproduct of the respective factor matrix, in order to increase or decrease the component contrast. The constraints on the factor matrices can comprise non-negativity, general linear equality constraints, general linear inequality constraints, or the like.","Factor analysis comprises a family of multivariate statistical techniques that attempt to distill high-dimensional data into a limited number of components that retain the essential information pertinent to an underlying model. Factor analysis has a long history in the social sciences and has also proven effective for solving a variety of chemical problems. See, e.g., H. H. Harman, , The University of Chicago Press, Chicago (1976) and E. R. Malinowski, , John Wiley & Sons, New York (2002).","In mathematical terms, factor analysis methods seek to decompose a data matrix D into a product of matrix factors that describe all of the relevant information contained in D in physically meaningful terms. For example, in a spectroscopic experiment which adheres to Beer's Law, D is an m\u00d7n matrix containing spectroscopic data (e.g. an n-channel spectrum at each of m pixels in a spectrum image) and we wish to estimate factor matrices C and S, which describe the concentrations and spectra, respectively, of the p pure components that comprise the sample. In this case, the data are said to follow a bilinear model:\n\nD=CS\u2003\u2003(1)\n","Higher order, multilinear models, have also found increasing use in recent years. See R. Bro, \u201cPARAFAC. Tutorial and applications\u201d, 38(2), 149 (1997). In this case, a multidimensional data array  may be factored into a product of several matrices. A general multilinear model, for instance, might be written:\n\n(, . . . )\u2003\u2003(2)\n\nwhere A, B, C, etc., are conformable matrices. The solution of Equation (2), however, is typically accomplished by reducing the problem to one of solving a series of bilinear models. Thus, methods for computing solutions to Equation (1) are of fundamental importance and will be the focus of this description.\n","It is well known, that factor-based methods suffer from \u201crotational ambiguity\u201d and \u201cintensity ambiguity\u201d, that is, there are an infinite number of factor models that will describe the data equally well. See Tauler et al., \u201cMultivariate Curve Resolution Applied to Spectral Data from Multiple Runs of an Industrial process,\u201d 65(15), 2040 (1993). In practice, relatively unique solutions are obtained by applying constraints on the admissible solutions.","Principal Component Analysis (PCA), used either by itself or to preprocess data, is the most ubiquitous tool of factor analysis. In the spectroscopic context, the constraints imposed by PCA are that the concentration and spectral factors must contain orthogonal components and that the components serially maximize the variance in the data that each accounts for. Neither constraint has any physical basis; thus, the factors obtained via PCA are abstract and not easily interpreted. The PCA solution does have the useful property, however, that for a given number of factors p, it provides the best p-factor model for describing the data in a least squares sense.","The key to deriving easily interpretable components is to constrain the factor solutions to conform to physical reality. While we will not, in general, know either C or S in Equation (1), we may know some of their properties. Physically plausible concentrations and spectra, for instance, may be known a priori to be non-negative. A common way to impose such constraints is to solve the factor model in an iterative fashion as a series of constrained, conditional least squares problems. For the applications in multivariate image analysis, this approach has been described in detail. See J. J. Andrew et al., \u201cRapid Analysis of Raman Image Data Using Two-Way Multivariate Curve Resolution,\u201d 52(6), 797 (1998); P. Kotula et al., \u201cAutomated Analysis of SEM X-Ray Spectrum Images: A Powerful New Microanalysis Tool,\u201d 9(1), 1 (2003); U.S. Pat. No. 6,584,413 to Keenan and Kotula; and U.S. Pat. No. 6,675,106 to Keenan and Kotula, which are incorporated herein by reference.","There are many details that are specific to the particular problems at hand. At a basic level, however, the Multivariate Curve Resolution-Alternating Least Squares (MCR-ALS) procedure reduces to the algorithm:",{"@attributes":{"id":"p-0035","num":"0034"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Make an initial guess for S"]},{"entry":[{},"Do until converged:"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"175pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"14pt","align":"right"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\u20091. normalize S","(3)"]},{"entry":[{},{}]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mn":"2.","mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mtext":"solve\u00a0\u00a0"}],"mrow":{"munder":{"mi":["min","c"]},"mo":"\u2062","mrow":{"msub":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mo":["\uf603","\uf604"],"mrow":{"msup":[{"mi":["D","T"]},{"mi":["SC","T"]}],"mo":"-"}}},"mi":"F"},"mo":["\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mtext":" subject\u00a0\u00a0to\u00a0\u00a0constraints\u00a0\u00a0on\u00a0\u00a0C"}]}}}}}}]},{"entry":[{},{}]},{"entry":[{},{"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mn":"3.","mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mtext":"solve\u00a0\u00a0"}],"mrow":{"munder":{"mi":["min","s"]},"mo":"\u2062","mrow":{"msub":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mi":"D","mo":"-","msup":{"mi":["CS","T"]}}}},"mi":"F"},"mo":["\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mtext":"subject\u00a0\u00a0to\u00a0\u00a0constraints\u00a0\u00a0on\u00a0\u00a0S"}]}}}}}}]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}},"br":{}},"In the absence of constraints, the classical least squares (CLS) solution provides an unbiased estimate of the unknown parameters. Once constraints are applied, however, bias is introduced into the solution. Taking non-negativity constrained least squares (NNLS) as an example, the inequality constrained least squares problem with multiple observation vectors [bb. . . b]=B and coefficient matrix A can be written in generic terms as:",{"@attributes":{"id":"p-0037","num":"0036"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mover":{"mi":"x","mo":"^"},"mo":"=","mrow":{"mrow":[{"munder":{"mi":["min","x"]},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mi":["AX","B"],"mo":"-"}}},"mi":"F","mn":"2"}},{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mi":"min","msub":{"mi":["x","j"]}},"mo":"\u2062","mrow":{"msubsup":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mo":["\uf603","\uf604"],"mrow":{"msub":[{"mi":["Ax","j"]},{"mi":["b","j"]}],"mo":"-"}}},"mn":["2","2"]},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":["subject","to"],"msub":{"mi":["x","j"]}}}},"mo":"\u2265","mn":"0"}],"mo":"="}}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}}}},"br":{},"i":["Solving Least Squares Problems, Classics in applied mathematics ","J. of Chemometrics "]},"In brief, the inequality constraints define a feasible region in multidimensional space. At the least squares minimum, variables will either reside in the interior of the feasible region or will be found on the boundary. The passive set comprises those variables that are interior to the feasible region, and the active set, those on the boundary. If we knew, a priori, the active set for the optimal solution, the solution to the inequality constrained least squares problem could be obtained as the solution to the equality constrained problem with all active-set variables being constrained to be equal to their boundary values. The active\/passive set strategy for solving inequality constrained least squares problems, then, is to iteratively move variables between the active and passive sets in such a way that the sum of the squared residuals steadily decreases and feasibility of the solution is maintained. Since there are a limited number of ways to partition variables between the active and passive sets, this process is guaranteed to converge.","The fact that the constraints introduce bias into the least squares solution can be seen by extending the arguments of Chapter 20 of the C. L. Lawson et al. reference to show that the constrained estimate {circumflex over (x)}is a linear transformation of the classical least squares estimate {circumflex over (x)}:\n\n()\u2003\u2003(5)\n\nK is a p\u00d7k selection matrix where k is the number of the p total variables that are in the passive set, and the superscript + represents a pseudoinverse. The two estimates coincide only for the case that K is the identity matrix, namely, when no variables are actively constrained.\n","The manifestation of bias is best illustrated by a simple example.  shows the results of a CLS fit of the concentrations for an energy dispersive x-ray spectral image of a copper\/nickel diffusion couple. Details of the sample and experiment are given in Kotula et al. In this case, regions of the sample at the far left and far right of the image are known to be pure nickel and pure copper, respectively. The pure component spectra were then obtained by simply averaging the measured spectra over the leftmost 16 columns of the image for nickel and over the rightmost 16 columns for copper. In the absence of noise, the concentrations of both nickel and copper should vary over the range from 0-1.",{"@attributes":{"id":"p-0041","num":"0040"},"figref":"FIG. 2"},"Applying non-negativity constraints, on the other hand, yields the results shown in . All concentrations are strictly non-negative but, on average, the estimated concentration of nickel is slightly positive in the region known to be pure copper and the estimated concentration of copper is slightly positive in the region known to be pure nickel. It is also evident that the concentrations of both nickel and copper are less than one in regions known to consist of the respective pure elements. Hence, the application of constraints is shown to introduce bias in the resulting estimates, and the specific effect is to narrow the range of the mean concentration estimates.","While the bias is not too severe for a single NNLS solution, the effects are amplified in iterative algorithms, such as Equation (3).  shows the estimated pure-component spectra and concentration images after complete convergence of MCR-ALS for the copper\/nickel data set. The known, true pure-component spectra were used to initialize the iteration, and both concentrations and spectra were constrained thereafter to be non-negative. Note, particularly in the case of copper, that the estimated pure-component spectra (labeled \u201cMCR\u201d) have diverged from what is known to be true (labeled \u201cTrue\u201d).","A more telling look at the effect of bias on concentrations is shown in . Regions known to be pure copper are estimated to be about 16% nickel! Interestingly, only a single concentration in this region is being actively constrained. Thus, lacking additional information, one would naturally assume that this portion of the sample is, in fact, a copper\/nickel alloy rather than pure copper. The bias induced by constrained ALS into the pure-component spectral estimates has an opposing effect. Rather than showing apparent component mixing, the spectra are, in some sense, de-mixed.  compares the true nickel (labeled \u201cNi True\u201d) and copper (labeled \u201cCu True\u201d) spectra together with the bias in each estimate resulting from ALS (labeled \u201cNi Bias\u201d and \u201cCu Bias\u201d, respectively). In spectral regions having relatively high nickel intensity, the ALS-estimated nickel component is enhanced and the copper component suppressed as compared to the true values. Conversely, copper intensity is enhanced and nickel intensity is suppressed in the channels having relatively high overall copper intensity. The maximum magnitude of the bias is about 15%.","The origination and amplification of bias during constrained alternating least squares arises fundamentally from the asymmetric treatment of noise, and collinearity (i.e. non-orthogonality) of the pure components, either spectral, compositional, or both. In the foregoing example, positive biases in the zero-concentration regions result from the fact that apparent negative concentrations are suppressed, whereas, apparent positive concentrations are simply ignored. Consequently, known zero-concentration noise appears to have, on average, a positive concentration. The effect of positive concentration bias on fitting the data can be offset, in part, by biasing the corresponding pure spectral components in an opposite sense. These newly biased spectra then induce additional bias in the new concentration estimates, and so on. Since each iteration changes the bias only slightly, convergence to the constrained least squares minimum can be exceedingly slow.  shows the apparent mean copper concentration in the pure copper region as the ALS iteration proceeds. About 10,000 iterations are needed to achieve convergence; the process is slow, indeed.","Having established that constrained alternating least squares yields a biased estimate of the pure spectral and compositional components, we seek an alternative least squares solution process that enables us to offset the bias induced by the constraints. One approach is to employ biased least squares estimators at each step of the iteration. Two such estimators come to mind, Total Least Squares (TLS) and Ridge Regression (RR). See S. Van Huffel et al., , Society for Industrial and Applied Mathematics, Philadelphia (1991); G. H. Golub et al., , The John Hopkins University Press, Baltimore (1996); A. Bjorck, , Society for Industrial and Applied Mathematics, Philadelphia (1996); A. E. Hoerl et al., \u201cRidge Regression: Biased Estimation for Nonorthogonal Problems,\u201d 12(1), 55 (1970); D. W. Marquardt et al., \u201cGeneralized Inverses, Ridge Regression, Biased Linear Estimation, and Nonlinear Estimation,\u201d 12(3), 591 (1970); and N. R. Draper et al., , John Wiley & Sons, New York (1998), which are incorporated herein by reference.","The original motivation for developing these two techniques is quite different. TLS was developed to solve the \u201cerror-in-variables\u201d problem. Classical least squares assumes that error is confined to the observations and that the least squares model itself (i.e. the data matrix A in Equation (4)) is error-free. TLS solves the problem for the case that the data as well as the observations are subject to error. RR, on the other hand, is intended to deal with difficulties associated with performing least squares estimates with highly collinear data (i.e. correlations among the columns of A). Clearly, both of these issues impact the ability of ALS to yield useful information. Under the assumptions of classical least squares, for instance, S is considered to be error-free while estimating C, and the resulting estimate of C will reflect any error present in the observations. In the following step, however, C is considered to be error-free when obtaining a new estimate for S. These assumptions are obviously inconsistent and, in fact, there is error in both S and C. Typical spectroscopic data can also exhibit a high degree of collinearity. Samples may consist of chemical mixtures having compositional collinearity and pure-component spectra often have overlapping spectral features giving rise to spectral collinearity.","As evidenced in the references, extensive theoretical work surrounds both TLS and RR. It is perhaps more fruitful, however, to simply consider the phenomenology of TLS and RR. The solution vectors obtained from the regression techniques considered here satisfy a set of normal equations that is unique to each:\n\nTLS: ()\n\nCLS: AAx=Ab\n\nRR: ())\u2003\u2003(6)\n\nwhere I is the identity matrix. For TLS, the parameter \u03c3 is the smallest singular value of the augmented matrix [A, b]. By the interlacing property of singular values, 0<\u03c3<\u03c3, where \u03c3is the smallest singular value of A (or, equivalently, \u03c3in is the smallest eigenvalue of AA). The key observation is that a small positive value is being subtracted from each element of the diagonal of AA. For RR, on the other hand, the ridge parameter \u03bb>0, and a (typically) small positive value is being added to the diagonal of AA. It is the modification of the diagonal that induces bias in the TLS and RR solutions relative to CLS. The exact relationships can be derived by substituting the CLS result into the other two expressions in Equation (6), then\n\nTLS: =(())\n\nRR: =(+\u03bb())\u2003\u2003(7)\n\nOne particular consequence of Equation (7) is that:\n\n\u2225\u2225\u2225\u2225\u2003\u2003(8)\n\nfor non-zero values of the respective parameters. Thus, for an overall size fixed by the magnitude of the observations, RR would tend to decrease the spread of the parameters and TLS would increase the spread relative to the CLS result. Putting it in other terms, RR would tend to decrease the contrast among the components whereas TLS would tend to increase it. Given this insight, Equations (6) are seen to arise from a single least squares model that enables contrast bias to be introduced into the result. The normal equations formulation of the contrast-biased solution is:\n\n()\u2003\u2003(9)\n\nThe restriction on \u03b3 ensures that the biased cross product matrix AA remains symmetric positive definite. Taking this restriction together with claims from the ridge regression literature suggests that useful values of the bias parameter \u03b3 will fall in the range:\n\n\u2212\u03c3<\u03b3<\u03c3\n\nhigher contrastlower contrast\u2003\u2003(10)\n\nNote that \u03b3=0 corresponds to the unbiased least squares solution. Equation (9) provides a biased least squares estimation of x given a single vector of observations b. Much like the unbiased estimator in Equation (4), however, it can be used to build up a biased ALS algorithm (BALS) that parallels Equation (3):\n",{"@attributes":{"id":"p-0049","num":"0048"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"Make an initial guess for S"},{"entry":"Do until converged:"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"182pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"right"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\u20091. normalize S","(11)"]},{"entry":[{},"\u20092. select a bias parameter \u03b3"]},{"entry":[{},{}]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mn":"3.","mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mtext":"solve"},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mrow":{"munder":{"mi":["min","c"]},"mo":"\u2062","mrow":{"msub":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mrow":[{"msup":[{"mi":["S","T"]},{"mi":["D","T"]}],"mo":"\u2062"},{"mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msup":{"mi":["S","T"]},"mo":"\u2062","mi":"S"},{"msub":{"mi":["\u03b3","C"]},"mo":"\u2062","mi":"I"}],"mo":"+"}},"mo":"\u2062","msup":{"mi":["C","T"]}}],"mo":"-"}}},"mi":"F"},"mo":["\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mtext":"subject\u00a0\u00a0to\u00a0\u00a0constraints\u00a0\u00a0on\u00a0\u00a0C"}]}}}}}}]},{"entry":[{},{}]},{"entry":[{},"\u20094. select a bias parameter \u03b3"]},{"entry":[{},{}]},{"entry":[{},{"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mn":"5.","mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mtext":"solve\u00a0\u00a0"}],"mrow":{"munder":{"mi":["min","s"]},"mo":"\u2062","mrow":{"msub":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mrow":[{"msup":{"mi":["C","T"]},"mo":"\u2062","mi":"D"},{"mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msup":{"mi":["C","T"]},"mo":"\u2062","mi":"C"},{"msub":{"mi":["\u03b3","S"]},"mo":"\u2062","mi":"I"}],"mo":"+"}},"mo":"\u2062","msup":{"mi":["S","T"]}}],"mo":"-"}}},"mi":"F"},"mo":["\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mtext":"subject\u00a0\u00a0to\u00a0\u00a0constraints\u00a0\u00a0on\u00a0\u00a0S"}]}}}}}}]},{"entry":[{},{}]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"Compute S = (I + \u03b3(CC))S"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"Of course the roles of C and S can be interchanged whereby an initial guess is made for C and the order of the least squares solution steps 2 and 3 can be exchanged with steps 4 and 5. The subscript on \u03b3 indicates the matrix for which we will be obtaining a biased estimate. As with Equation (3), the normalization step is important for stabilizing the iteration process. The final computation step is also important. It yields a final unbiased solution to ensure that the predicted signal intensity is comparable to the observed intensity.","The data matrix D can be preprocessed prior to the BALS analysis. For example, the data matrix can be a two-dimensional data matrix obtained by unfolding a measured multidimensional spectral data set. Furthermore, the data matrix can be weighted for heteroscedastic noise, as described in the above-referenced patents to Keenan and Kotula. Or, it can be a spatially compressed and\/or spectrally compressed (e.g., a PCA-factored representation) data matrix, as described in U.S. patent application Ser. No. 10\/772,548, filed Feb. 4, 2004 now U.S. Pat. No. 7,283,684, and U.S. patent application Ser. No. 10\/772,805, filed Feb. 4, 2004 now U.S. Pat. No. 7,400,722, which are incorporated herein by reference.","Note also that, in general, the BALS algorithm of the present invention can be applied to the factorization of any arbitrary data matrix. That is, the data matrix D need not represent spectroscopic data and factor matrices C and S need not describe the concentrations and spectra of the spectroscopic data, respectively.","Just as it is the case with standard ridge regression, there are a variety of ways that the bias parameters might be selected in Equation (11). In fact, analogous to generalized ridge regression, there is no fundamental reason why \u03b3 needs to be a scalar constant. \u03b3 could be a diagonal matrix such that each variable is individually adjusted or biased. We might want to bias only a subset of variables, for instance. Such applications would certainly be problem dependent. Equation (10), however, suggests a generally useful way to select \u03b3. Let f be a fractional number in the interval [0, 1) that expresses, in a qualitative way, the extent to which we wish to bias the results. f=0 would represent no bias with increasing f indicating increasing levels of bias. Then, in the language of component contrast, we would select \u03b3 according to:\n\n","Returning to the Cu\/Ni example, the constrained ALS analysis reduced compositional contrast and increased spectral contrast relative to the known true values. To correct for this, we can (1) increase compositional contrast by solving for the concentrations with \u03b3<0; (2) decrease the spectral contrast by solving for the pure-component spectra using \u03b3>0; or (3) do both.  show the pure-component concentration and spectral estimates, respectively, after taking approach (1) and solving for the concentrations with (\u03b3=0, \u03b3=\u22120.05\u03c3) and subject to non-negativity constraints. Bias in the pure spectral components is essentially eliminated and there is a residual bias of less than 1% in the estimated composition of the pure nickel and about 2% in the composition of pure copper. Solving for the pure component spectra according to approach (2) with (\u03b3=0.11\u03c3, \u03b3=0) and subject to non-negativity constraints gave the results shown in . These results are essentially equivalent to those in .","It should also be noted that the BALS algorithm is extremely general and the constraints in Equation (11) are not limited to non-negativity. For example, the constraints can comprise non-negativity constraints, general linear equality constraints, general linear inequality constraints, combined linear equality and inequality constraints, constraints imposed by the method of weighting, constraints imposed as \u201cpreferences\u201d using the method of weighting, bounded variable constraints, simple variable equality constraints, closure constraints, or combinations of the above constraints.  show the results obtained for the Cu\/Ni example when the respective concentrations are limited to the range of 0-1 using, for instance, the Bounded Variables Least Squares (BVLS) algorithm in the C. L. Lawson et al. reference and approach (3) with (\u03b3=0.09\u03c3, \u03b3=\u22120.03\u03c3). Again, the results are quite similar to the other two approaches.",{"@attributes":{"id":"p-0056","num":"0060"},"figref":"FIG. 10","i":"Analytica Chimica Acta "},"A second example problem is illustrated in . The sample represents an interface between a copper\/silver braze and an iron-nickel-cobalt alloy, with titanium added to promote adhesion. The mean spectrum obtained by energy dispersive x-ray analysis is also shown in . In contrast with the Cu\/Ni example, element-specific spectral features are not collinear; rather, the spectral features are collinear with a broad, non-specific background.",{"@attributes":{"id":"p-0058","num":"0062"},"figref":"FIG. 12","sub":["S","C","S","C","min"],"sup":"2"},"The bias parameter needed to correct the braze example is larger than the corresponding parameter required for the Cu\/Ni example, and the question arises as to how the parameters should be selected. One approach to selecting appropriate parameters for a given problem is simply trial and error. In general, the more ALS-induced bias that needs to be offset, the larger the bias parameter needs to be. Once the induced bias has been fully offset, however, further increases in the magnitude of the bias parameter have relatively little effect.","This observation suggests another parameter selection approach as illustrated in  for the braze example. Given a metric describing how bias is changing, we simply look for the point at which increasing the bias parameter has little effect. In the present case, the metric is the Frobenius norm of the spectral difference between the ALS-derived pure components and those obtained from BALS with a given bias parameter. The break in the curve (\u03b3\u2248\u22120.40\u03c3) indicates the optimum parameter. Clearly, many other metrics might be used in a similar way depending on the particular problem at hand.","A fundamental problem that arises from collinearity is that there will be a wide range of solutions that fit the data equally well. While these solutions may be equivalent mathematically, they may not have the same explanatory power for a given analytical problem. Introduction of bias selects some of the solutions as being preferable to others. Up to this point we have been concerned with offsetting bias introduced by the constraints in ALS. However, these methods can provide not only a bias adjustment, but a general way to control contrast in the extracted pure components.","One example of when it would be useful to exaggerate contrast is when it is desired to segment an image into regions having some characteristics in common. Typically, thresholding techniques based on image intensity histograms are used for this purpose. See R. C. Gonzales et al., , Prentice-Hall, Upper Saddle River, N.J. (2002).  compares concentration histograms for the four components of the braze example as obtained by ALS (\u03b3=0, \u03b3=0), BALS with the \u201coptimal\u201d bias offset parameter (\u03b3=0, \u03b3=\u22120.40\u03c3), and BALS with bias parameters selected to exaggerate spatial contrast (\u03b3=0.99\u03c3, \u03b3=\u22120.99\u03c3). Clearly, the increase sharpness of the latter histograms will lead to a more successful segmentation of the spectral image by component.","The example shown in  presents another interesting scenario where contrast adjustment can be used to good effect. The sample consists of 6 different alloy wires embedded in an epoxy block. The individual wires are composed from a palette of 6 different elements and the sample was imaged by energy dispersive x-ray analysis. We would expect the wires in this sample to be comprised of 6 pure components, but the presumed nature of the components may be ambiguous. It seems that there are two equally valid ways of viewing the components, either as alloys or as pure elements. Depending on the particular analytical question, one or the other of these views might be preferable.","The left-hand panel in  shows the results of an ALS analysis (\u03b3=0, \u03b3=0). The results are, by and large, separated by element. The three wires containing copper are broken out in one component, the two containing iron in another component, and so on. This particular view is one that has high spectral contrast, i.e., the elemental spectra are highly orthogonal, and have relatively low spatial contrast, e.g. three different sets of wires show appreciable intensity in the copper component. The results of BALS analysis, where the bias parameters (\u03b3=0, \u03b3=0.10\u03c3) have been selected to decrease spectral contrast, are shown in the right-hand panel in . The wires are now cleanly distinguished by alloy. Thus, using BALS enables the data to be understood from a variety of perspectives to aid in interpretation and analytical problem solving.","For a positive bias parameter (i.e., ridge regression), the least squares problem that is being solved can be formulated as:",{"@attributes":{"id":"p-0066","num":"0070"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"munder":{"mi":["min","x"]},"mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mrow":[{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mi":"A"}},{"mtd":{"mrow":{"mi":"\u03bb","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"1"}}}]}},"mo":"\u2062","mi":"x"},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mi":"b"}},{"mtd":{"mn":"0"}}]}}],"mo":"-"}}}}},{"mrow":{"mo":["(",")"],"mn":"13"}}]}}}},"br":{}},"Recently, work has been published in which such preferences were used to make non-negativity a \u201csoft\u201d constraint. See P. J. Gemperline et al., \u201cAdvantages of soft versus hard constraints in self-modeling curve resolution problems,\u201d 75(16), 4236 (2003). The basic idea is that by making non-negativity a preference, noisy components are allowed to go slightly negative. This is accomplished by using the weighting method to solve for the actively constrained variables in an active\/passive set solution strategy. Mathematically, this is equivalent to replacing the identity matrix in Equation (13) by a selection matrix that selects the actively constrained variables. One difficulty with this approach is that the noise is still being treated asymmetrically. \u201cNegative\u201d noise will be damped but \u201cpositive\u201d noise is still ignored. The BALS algorithm of the present invention, when combined with an active\/passive set NNLS algorithm, effectively selects the passive variables in Equation (13). In other words, negative noise is completely eliminated and positive noise is damped.","Of course, the two approaches can be combined by giving different weights (i.e. bias parameters) to the active and passive variables. Given that Kis the p\u00d7k selection matrix that selects the k active variables and Kis the p\u00d7(p\u2212k) selection matrix for the (p\u2212k) passively constrained variables, then the biased, preference solution at each step in the iteration satisfies the normal equations:\n\n()0, \u03b3>\u2212\u03c3\n\n\u2003\u2003(14)\n\nBALS analysis results for the copper\/nickel data set when the non-negativity constraints are imposed as preferences are shown in . Spectral bias is largely eliminated, as is concentration bias. In the latter case, some concentrations are seen to be negative and the concentration histograms for the two components have maxima near the known pure element compositions.\n","A second related work was recently published in J.-H. Wang et al, \u201cApplication of modified alternating least squares regression to spectroscopic image analysis,\u201d 476, 93 (2003). The authors employed ridge regression in a modified ALS algorithm (MALS) for the purpose of improving solution speed, stability and \u201ccomponent resolution ability.\u201d The improved convergence rate and stability is also observed with the BALS algorithm of the present invention, as described supra. A difference with the BALS algorithm is that the MALS algorithm takes explicit steps to avoid introducing any bias into the results, whereas, the BALS algorithm intentionally introduces bias. Additional differences are that positive ridge parameters are used in the computation of both the spectral and compositional parameters. This will tend to have a canceling effect. For instance, a positive ridge parameter when computing the spectra will tend to decrease the spectral contrast, a positive ridge parameter when computing the concentrations, on the other hand will tend to increase the spectral contrast.","The discussion surrounding Equation (4) considers the case where constrained least squares problems involving matrices of observations can be reformulated as a series of one-column-at-a-time problems. The specific example of the non-negativity constraint was considered there. A more generic notation to describe the general linearly constrained least squares model for a single vector of observations b is given by:\n\nminimize \u2225Ax\u2212b\u2225subject to the constraints:\n\nCx\u2267d\n\nEx=f\u2003\u2003(15)\n\nIn this formulation, for example, the non-negativity constraint can be expressed with C=I, and d=f=E=0. The unconstrained solution x can be obtained by solving the normal equations:\n\nAAx=Ab\u2003\u2003(16)\n\nAs shown by Equation (9), the present invention introduces bias into the solution vector by adding a diagonal matrix to the symmetric positive definite matrix AA. For the general linearly constrained least squares problem, the constraints can be applied through a symmetric modification of AA, yielding a new problem whose solution satisfies the constraints and which can be biased in a similar way. Returning, again, to the non-negativity constraint, each iteration of the solution process computes an unconstrained solution for the variables in the passive set. Given the selection matrix K introduced in Equation (5), the passive variables {tilde over (x)}=Kx satisfy the normal equations:\n\n()()()=()()()=()\n\nor\n\n\u00c3\u00c3{tilde over (x)}=\u00c3b\u2003\u2003(17)\n\nClearly, the latter expression has the same form as Equation (16) and can be biased in a similar fashion. Exactly the same procedure can be used for other inequality constrained problems that are solved by the active\/passive set strategy, for instance, the example shown in . where the constraints comprise both upper and lower bounds on the concentrations. For general linear inequality constraints, C. L. Lawson et al. shows how the solution procedure can be reduced to solving a non-negativity constrained problem. Thus, the algorithm outlined above is generally useful for solving inequality constrained problems.\n","One approach to solving general equality constrained least squares problems is the method of weighting which is outlined in Chapter 22 of C. L. Lawson et al. In brief, the unconstrained solution to the problem",{"@attributes":{"id":"p-0072","num":"0076"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"munder":{"mi":["min","x"]},"mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mrow":[{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mi":"A"}},{"mtd":{"mrow":{"mi":["\u03bb","E"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}}]}},"mo":"\u2062","mi":"x"},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mi":"b"}},{"mtd":{"mrow":{"mi":["\u03bb","f"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}}]}}],"mo":"-"}}}}},{"mrow":{"mo":["(",")"],"mn":"18"}}]}}}},"br":[{},{},{},{},{}],"in-line-formulae":[{},{},{},{}],"i":["A","A+\u03bb","E","E","x=A","b+\u03bb","E","f, \u03bb\u2267","A","A+\u03bb","E","E+\u03b3I","x=A","b+\u03bb","E","f, \u03bb\u2267"],"sup":["T","2","T","T","2","T","T","2","T","T","2","T","2","2 ","T","2","T","T "],"sub":["min","min"]},"Finally, it should be clear that the active\/passive set strategy outlined in Equation (17) can be used in conjunction with weighting to solve least squares problems containing both equality and inequality constraints using BALS. As a final example, consider the copper\/nickel data set. In principle, the concentrations of copper and nickel should sum to one. Owing to the presence of voids in the sample, however, this constraint is not satisfied for all pixels. To demonstrate the BALS solution, then, a simulated data set created from the pure component spectra shown in  and using concentrations that rigorously sum to one was used. Noise was then added to the simulated spectra using a Poisson random number generator.  show the results for an ALS analysis that incorporated both closure and non-negativity of the concentrations. Comparison with  suggests that the bias in the result has increased owing to the additional constraint. The results of the BALS analysis using the same simulated data and constraint sets are shown in . Once again, both spectral and compositional biases have been effectively eliminated by the BALS analysis.","The present invention has been described as a method for exploiting bias in factor analysis using constrained alternating least squares algorithms. It will be understood that the above description is merely illustrative of the applications of the principles of the present invention, the scope of which is to be determined by the claims viewed in light of the specification. Other variants and modifications of the invention will be apparent to those of skill in the art."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The accompanying drawings, which are incorporated in and form part of the specification, illustrate the present invention and, together with the description, describe the invention. In the drawings, like elements are referred to by like numbers.",{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":["FIG. 5A","FIG. 5B"]},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":["FIG. 7A","FIG. 7B"],"sub":["S","C","min"],"sup":"2 "},{"@attributes":{"id":"p-0016","num":"0015"},"figref":["FIG. 8A","FIG. 8B"],"sub":["S","max","C"],"sup":"2"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":["FIG. 9A","FIG. 9B"],"sub":["S","max","C","max"],"sup":["2","2"]},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 12","sub":["S","C","S","C","min"],"sup":"2"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 13","sub":"C"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 14","sub":["S","C","S","C","min","S","max","C","min"],"sup":["2","2","2"]},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 16","sub":["C","S","C","S","max"],"sup":"2"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":["FIG. 18A","FIG. 18B"]},{"@attributes":{"id":"p-0027","num":"0026"},"figref":["FIG. 19A","FIG. 19B"],"sub":["S","C","min"],"sup":"2"}]},"DETDESC":[{},{}]}
