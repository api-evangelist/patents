---
title: Interfaces for digital media processing
abstract: APIs discussed herein promote efficient and timely interoperability between hardware and software components within the media processing pipelines of media content players. A PhysMemDataStructure API facilitates a hardware component's direct access to information within a memory used by a software component, to enable the hardware component to use direct memory access techniques to obtain the contents of the memory, instead of using processor cycles to execute copy commands. The PhysMemDataStructure API exposes one or more fields of data structures associated with units of media content stored in a memory used by a software component, and the exposed fields store information about the physical properties of the memory locations of the units of media content. SyncHelper APIs are used for obtaining information from, and passing information to, hardware components, which information is used to adjust the hardware components' timing for preparing media samples of synchronously-presentable media content streams.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09043504&OS=09043504&RS=09043504
owner: MICROSOFT TECHNOLOGY LICENSING, LLC
number: 09043504
owner_city: Redmond
owner_country: US
publication_date: 20131216
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["This application is a continuation of U.S. Ser. No. 11\/824,720, filed Jun. 30, 2007, entitled, \u201cINTERFACES FOR DIGITAL MEDIA PROCESSING\u201d, now U.S. Pat. No. 8,612,643, issued Dec. 17, 2013, which is incorporated herein by reference in its entirety.","Digital media presentations are composed of sequenced sets of media content such as video, audio, images, text, and\/or graphics. When media content players render and\/or present such sequenced sets of media content to users, they are referred to as streams of media content. Some media content players are configured to concurrently render and present more than one independently-controlled stream of media content (for example, a main movie along with features such as a director's commentary, actor biographies, or advertising). Such media content players may also be capable of rendering and presenting user-selectable visible or audible objects (for example, various menus, games, special effects, or other options) concurrently with one or more streams of media content.","Any type of device in the form of software, hardware, firmware, or any combination thereof may be a media content player. Devices such as optical media players (for example, DVD players), computers, and other electronic devices that provide access to large amounts of relatively inexpensive, portable or otherwise accessible data storage are particularly well positioned to meet consumer demand for digital media presentations having significant play durations.","It is common for various entities to supply different software and hardware components of media content players, and such components are expected to successfully interoperate in environments having limited processing and memory resources. It is therefore desirable to provide techniques for ensuring resource-efficient, relatively glitch-free play of digital media presentations, including the accurate synchronization of concurrently presentable streams of media content, on all types of media content players and architectures thereof.","Digital media processing techniques and interfaces (such as application programming interfaces (\u201cAPIs\u201d)) discussed herein promote efficient, consistent interoperability between hardware and software components within a media processing pipeline associated with a media content player.","Generally, a media processing pipeline is responsible for receiving sets of media content from media sources such as optical disks, hard drives, network locations, and other possible sources, and performing processing tasks to prepare the sets of media content for presentation to a user as one or more media content streams of a digital media presentation such as a movie, television program, audio program, or other presentation. Sets of media content are referred to as \u201cclips,\u201d with one clip generally received from one media source. Discrete portions of clips read from a particular media source are referred to herein as media content units, which are generally demultiplexed, decompressed, decoded, and\/or decrypted. After being demultiplexed, such media content units are referred to herein as media samples. It will be appreciated, however, that the naming convention(s) used herein is\/are for illustrative purposes only, and that any desired naming conventions may be used.","A media processing pipeline includes components such as media source readers, demultiplexers, decoders, decrypters, and the like, which are implemented in hardware or software or a combination thereof. Frameworks such as the Microsoft\u00ae DirectShow\u2122 multimedia framework may be used to implement a media processing pipeline. It will be appreciated, however, that any now known or later developed framework may be used to implement a media processing pipeline.","Information (such as information about the media content itself and\/or presentation of the media content to a user) is exchanged at boundaries between software components and hardware components in a media processing pipeline. In one information exchange scenario, information within a memory (the term memory can encompass any type of computer-readable storage medium) used by a software component is usable by a hardware component. In another information exchange scenario, a hardware component modifies its operation based on information ascertained by a software component, or vice-versa.","One exemplary technique and interface discussed herein\u2014referred to for discussion purposes as the \u201cPhysMemDataStructure\u201d interface\u2014is configured for operation at a boundary between a software component and a hardware component of a media processing pipeline to facilitate the hardware component's direct access of information from a memory used by the software component, instead of using instructions\/processor cycles to copy the information. The PhysMemDataStructure interface exposes to the hardware component one or more fields of data structures associated with units of media content (which are to be processed by the hardware component) stored in a memory used by the software component. The fields of the data structures store information about the physical properties of the memory where individual units of media content are located. Examples of such physical properties include but are not limited to type of memory, memory block size, locations of read\/write pointers to memory locations, and offset locations of media content units with respect to such memory pointers. To further enhance the efficient use of memory resources, the software component may store units of media content in a ring buffer. To achieve still further memory and processing efficiencies, virtual memory may be used to duplicate the beginning portion of the ring buffer at the ending portion of the physical memory ring buffer.","Other exemplary techniques and interfaces discussed herein\u2014referred to for discussion purposes as the \u201cSyncHelper\u201d interfaces\u2014are configured to facilitate information exchange between hardware components and software components, which may be used to adjust timing (to maintain perceived synchronization between two media content streams, for example) or other operational aspects of the hardware or software components. One SyncHelper interface discussed herein\u2014referred to as the \u201cGetDecodeTimes\u201d interface\u2014provides information about a particular media content unit or media sample being rendered by a hardware component (such as a demultiplexer, decoder, or renderer) at a particular point in time. The provided information includes the elapsed amount of the play duration of the digital media presentation at the particular point in time, as well as the elapsed amount of the play duration of the clip from which the media sample was derived. Another SynchHelper interface\u2014referred to as the \u201cSyncToSTC\u201d interface\u2014facilitates synchronization of various concurrently presentable media content streams. In an exemplary scenario, the SyncToSTC interface ascertains (that is, either requests\/receives or calculates) a difference between two values of the elapsed amount of the play duration of the digital media presentation returned by the GetDecodeTimes interface, and instructs one or more hardware components to adjust timing (for example, adjust the rate of a timing signal or adjust which media sample is being decoded or both) based on the ascertained difference.","This Summary is provided to introduce a selection of concepts in a simplified form. The concepts are further described in the Detailed Description section. Elements or steps other than those described in this Summary are possible, and no element or step is necessarily required. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended for use as an aid in determining the scope of the claimed subject matter. The claimed subject matter is not limited to implementations that solve any or all disadvantages noted in any part of this disclosure.","The predictable and relatively glitch-free play of a digital media presentation is often dependent on the efficient use of limited computing resources of the media content player, especially memory and processor resources. Glitches and inefficiencies can arise in various situations, especially when information is transferred between hardware components and software components operating in a media processing pipeline. In one scenario, inefficiencies may arise when information is transferred between a memory used by a software component and a memory used by a hardware component\u2014it is desirable to minimize the processing and\/or memory resources used in memory access transactions. In another scenario, glitches in the play of the media content stream(s) and\/or user-perceived loss of synchronization may occur when multiple media content streams are prepared by separate hardware components for concurrent presentation to a user and appropriate information is not available to the hardware components to ensure operational synchronization\u2014it is desirable to provide information to the hardware components for use in adjusting the timing for performing certain processing tasks.","Various techniques and application programming interfaces (\u201cAPIs\u201d) are discussed herein that operate at a boundary between a software component and a hardware component, to expose information usable by the hardware component to enhance the efficiency, accuracy and interoperability of the components operating in the media processing pipeline of a media content player.","Turning now to the drawings, where like numerals designate like components,  is a simplified functional block diagram of an exemplary media content player  (hereinafter referred to as \u201cPresentation System\u201d ) that renders media content. Media content is composed of sequences (generally, time-ordered) of video, audio, images, text, and\/or graphics. Presentation System may be any system that renders media content, including but not limited to an optical media player, a computing device or operating system thereof, an audio player, a set-top box, a telecommunication device, a personal digital assistant, an image or video capture device, and the like. For purposes of discussion, it is assumed that Presentation System is an interactive multimedia presentation system used to play media content such as movies or other types of presentations in concurrently with user-selectable visible or audible interactive objects (for example, menus, games, special effects, or other options).","As shown, Presentation System  includes a media content manager , an interactive content (\u201cIC\u201d) manager , a presentation manager , a timing signal management block , and a mixer\/renderer . In general, design choices dictate how specific functions of Presentation System  are implemented. Such functions may be implemented using hardware, software, or firmware, or combinations thereof.","In operation, Presentation System  handles interactive multimedia presentation content (\u201cPresentation Content\u201d) . Presentation Content  includes a media content component (\u201cmedia component\u201d)  and an interactive content component (\u201cIC component\u201d) . Media component  and IC component  are generally, but need not be, handled separately streams, by media content manager  and IC manager , respectively.","Presentation System  facilitates presentation of Presentation Content  to a user (not shown) as played presentation . Played presentation  represents the visible and\/or audible information associated with Presentation Content  that is produced by mixer\/renderer  and receivable by the user via devices such as displays or speakers (not shown). For discussion purposes, it is assumed that Presentation Content  and played presentation  represent aspects of high-definition DVD movie content, in any format. It will be appreciated, however, that Presentation Content  and Played Presentation  may be configured for presenting any type of presentation of media content now known or later developed.","Media component  represents one or more sequences (generally, time-ordered) of video, audio, images, text, and\/or graphics presentable to users as media content streams (media content streams  and  are shown and discussed further below, in connection with ) within played presentation . More than one independently-controlled media content stream may be concurrently presented (for example, a main movie along with features such as a director's commentary, actor biographies, or advertising).","A movie generally has one or more versions (a version for mature audiences, and a version for younger audiences, for example); one or more titles  with one or more chapters (not shown) associated with each title (titles are discussed further below, in connection with presentation manager ); one or more audio tracks (for example, the movie may be played in one or more languages, with or without subtitles); and extra features such as director's commentary, additional footage, actor biographies, advertising, trailers, and the like. It will be appreciated that distinctions between titles and chapters are purely logical distinctions. For example, a single perceived media segment could be part of a single title\/chapter, or could be made up of multiple titles\/chapters. It is up to the content authoring source to determine the applicable logical distinctions.","Sets of sequences of video, audio, images, text, and\/or graphics that form aspects of media component  are commonly referred to as clips  (clips  are shown within media component  and playlist , and are also referred to in  and discussed further below). It will be appreciated, however, that sets of data sequences that form media component  may be grouped and\/or referred to in any desirable manner and the actual data may be arranged into and represented by any desired units, for example, bits, frames, samples, data packets, groups of pictures, enhanced video object units, etc. The digital contents of a particular unit of data (and also the size of a unit of data) may be based on several factors, such as the characteristics of the video, audio, or data content comprising the unit, or one or more parameters associated with the media source from which the sample is derived (for example, media source identity and\/or location, encoder\/decoder parameters or settings, or encryption parameters or settings). Media sources are discussed further below, in connection with .","Media data  is data associated with media component  that has been prepared for rendering by media content manager  and transmitted to mixer\/renderer . Media data  generally includes, for each active clip , a rendering of a portion of the clip.","Referring again to Presentation Content , IC component  includes interactive objects , which are user-selectable visible or audible objects, optionally presentable concurrently with media component , along with any instructions (shown as applications ) for presenting the visible or audible objects. Examples of interactive objects include, among other things, video samples or clips, audio samples or clips, images, graphics, text, and combinations thereof.","Applications  provide the mechanism by which Presentation System  presents interactive objects  to a user. Applications  represent any signal processing method or stored instruction(s) that electronically control predetermined operations on data.","IC manager  includes one or more instruction handling engines , which receive, interpret, and arrange for execution of commands associated with applications . As execution of applications  progresses and user input  is received, behavior within played presentation  may be triggered. Execution of certain instructions of application , labeled as \u201cinput from ICM\u201d , may facilitate communication or interoperability with other functionality or components within Presentation System . As shown, input  is received by media content manager  (discussed further below, in connection with ), but other components or functions within Presentation System  may also be responsive to input .","Interactive content data (\u201cIC data\u201d)  is data associated with IC component  that has been prepared for rendering by IC manager  and transmitted to mixer\/renderer .","Timing signal management block  (discussed further below, in connection with ) produces various timing signals , which are used to control the timing for preparation and production of media data  and IC data  by media content manager  and IC manager , respectively. For example, timing signal management block  is generally responsible for determining rates at which media data  (\u201cmedia data presentation rate ,\u201d shown and discussed in connection with ) and IC data  are presented to a user. In another example, timing signals  are used to achieve approximate synchronization of media data  and\/or IC data  (for example, timing\/synchronization on a per-frame basis or on another time basis).","Mixer\/renderer renders media data  in a video plane (not shown), and renders IC data  in a graphics plane (not shown). The graphics plane is generally, but not necessarily, overlayed onto the video plane to produce played presentation  for the user.","Presentation manager , which is configured for communication with media content manager , IC manager , mixer\/renderer , and timing signal management block , facilitates handling of Presentation Content  and presentation of played presentation  to the user. Presentation manager  has access to a playlist . Playlist  includes, among other things, a time-ordered sequence of clips  and applications  (including interactive objects ) that are presentable to a user. The clips  and applications \/interactive objects  may be arranged to form one or more titles . As discussed above, it is possible for more than one independently-controlled title\/media content stream to be concurrently played to a user. Such concurrently played streams may be indicated on playlist , or serendipitous user input may cause concurrent play of media content streams.","Presentation manager  uses playlist  to ascertain a presentation timeline  for a particular media presentation (a title  in the case of a movie), which generally has a predetermined play duration representing the particular amount of time in which the title is presentable to a user. Representations of amounts of specific elapsed times within the play duration are often referred to as \u201ctitle times\u201d. Because a title may be played once or more than once (in a looping fashion, for example), the play duration is determined based on one iteration of the title. Conceptually, presentation timeline  indicates the title times when specific clips  and applications  are presentable to a user (although as indicated, it is not generally known when user inputs starting and stopping the play of some specific clips may occur). Specific clips  also generally have predetermined play durations representing the particular amounts of time for presenting the clip. Representations of amounts of specific elapsed times within the clip play durations are often referred to as \u201cpresentation times\u201d. Each individually-presentable portion of a clip (which may for discussion purposes be referred to as a \u201cmedia sample,\u201d although any desired naming convention may be used) has an associated, pre-determined presentation time within the play duration of the clip. To avoid user-perceptible glitches in the presentation of media content, one or more upcoming media samples are prepared for presentation in advance of the scheduled\/pre-determined presentation time.","To better illustrate the play of a particular clip and timing\/times associated therewith, it is useful to use playlist  and\/or presentation timeline  to ascertain one or more media content timelines (\u201cmedia timeline(s)\u201d) . With continuing reference to ,  is an exemplary media timeline  for a particular clip . Various media sample presentation times  are indicated on media timeline . Media sample presentation times  represent times within the play duration of a particular clip at which one or more media samples are presentable as media data . As shown, media sample presentation times  occur at a rate based on a predetermined media data presentation rate , which may vary from clip-to-clip. Note that it is not necessary for media data presentation rate  to be the same as the rate at which a particular clip  was encoded, although the media data presentation rate may change based on the encoding rate for a particular clip. Certain user input  can also affect the speed of media sample retrieval from media sources and thus affect the rate at which media sample presentation times  occur. For example, played presentation  may proceed in a forward direction at a normal speed, and may also proceed in both forward and reverse directions at speeds faster or slower than the normal speed. It will appreciated that normal speed is a relative term, and that normal speed may vary from presentation to presentation, and from clip-to-clip. During fast-reverse and fast-forward operations, the playing of certain media content (as shown, media samples ) is often skipped. Other user input may cause the playing of certain content to be skipped, such as when the user jumps from one part of the movie to another.","A current elapsed play time  (that is, the title time of the digital media presentation with which the clip is associated) is shown on media timeline . Media sample  is being presented to a user at current elapsed play time . As shown, current elapsed play time  coincides with a particular media sample presentation time , although such coinciding is not necessary. A next presentable media sample presentation time  is also shown. Next presentable media sample presentation time  is used to determine the next media sample, and\/or the next media sample presentation time, that should be next prepared for presentation to a user (as shown, next processable media sample  is to be prepared for presentation). It will be appreciated that the next presentable media sample\/presentation time may be the next consecutive media sample\/presentation time based on playlist , or may be a media sample\/presentation time one or more media samples\/presentation times  away from the media sample\/presentation time associated with current elapsed play time . There are various ways to ascertain the next presentable media sample\/media sample presentation time, which are not discussed in detail herein. Generally, however, a predicted elapsed play time  (that is, predicted title time of the play duration of the digital media presentation) and the corresponding next presentable media sample\/presentation time are ascertained. Information such as the play speed, media frame rate , and other information may be used to determine the predicted elapsed play time and\/or locate the particular media sample presentation time\/media sample.","Referring again to , in operation, presentation manager  provides information, including but not limited to information about presentation timeline  and\/or media timeline  to media content manager  and IC manager . Based on input from presentation manager , IC manager  prepares IC data  for rendering, and media content manager  prepares media data  for rendering.","With continuing reference to ,  is a simplified functional block diagram illustrating aspects of media content manager  in more detail. Media content manager  includes one or more media processing pipelines. Two media processing pipelines are shown, media processing pipeline  and media processing pipeline , although any number of media processing pipelines is possible. Generally, media processing pipeline  and media processing pipeline  are used to prepare independently-controlled media content streams  and , respectively, for presentation to a user. One media processing pipeline is usually responsible for preparing a primary media content stream, such as a movie, with reference to a first timing signal  , and other media processing pipelines are responsible for preparing one or more secondary media content streams, such as director's commentary, actor biographies, advertising, etc., with reference to a second timing signal  . Timing signals represent the rate(s) at which samples of media content are retrieved from media sources and\/or prepared for presentation to a user (such rate(s) may change dynamically, however, based on user input, encoding\/encryption\/compression formats, and other factors), and are generally derived from clocks sources (not shown), such as clock sources associated with Presentation System  and\/or special-purpose devices such as hardware components within media processing pipelines.","Media content manager  is responsible for preparing upcoming individually-presentable portions of clips, such as next processable media sample(s)  shown in , for presentation. Such preparation often involves multiple steps, including but not limited to reading the upcoming portion of the clip from a particular media source (media sources  and  shown, which are any devices, locations, or data from which media content is derived or obtained), and using hardware- and software-based media processing components (media processing components  and  are shown and discussed further below in connection with ) such as readers, demultiplers, decoders, renderers, and\/or decrypters to obtain playable media content streams ,  from the information read from the media source(s).","It will be appreciated that media content manager  may have a dynamic processing load based on the identity and scheduling (pre-determined or based on serendipitous user input ) of the various clips  comprising media component  and\/or IC component . Generally, it is desirable for media processing pipelines to consume no more than 10-15% of the processing resources (for example, CPU cycles) of Presentation System .","Large amounts of processing resources can be consumed when information is transferred between memory locations using traditional copy transactions such as memory-to-memory copies, and the over-use of processing resources for copy transactions has the potential to cause glitches in the play of a digital media presentation. Yet, it is often desirable to transfer information between memories used by different components of media processing pipelines, especially between memories used by software components and memories used by hardware components. Hardware components are used, among other reasons, to accelerate media content processing.","Contemporaneously preparing for presentation upcoming portions of two or more clips can also consume large amounts of computing resources such as memory and processor cycles in a manner that is not easily predictable, and can further exacerbate the potential for glitches in the play of digital media content. Moreover, memory and\/or processing resources required to prepare a particular portion of a clip for presentation (and thus times for such preparation) are not always constant from sample-to-sample or clip-to-clip. Some factors that affect required resources and preparation times are associated with the media content itself (including but not limited to factors such as media unit\/sample size, media source\/location, encoding or decoding parameters, and encryption parameters). Other factors that affect required resources are associated with the media content player (for example, media processing pipeline architecture, dynamic processing loads, and other features of media content player architecture), while still other factors that affect required resources are associated with user input (user-selected media content, content formats, or play speeds, for example).","With continuing reference to ,  is a simplified functional block diagram illustrating architectural and operational aspects of media processing component blocks  and  in more detail. In one possible implementation, the Microsoft\u00ae DirectShow\u2122 multimedia framework is used to divide media processing tasks into groups of steps known as filters, with each filter having a number of input pins and a number of output pins that connect filters together. It will be appreciated, however, that any now known or later developed framework may be used to implement a media processing pipeline.","As shown, a software-hardware boundary  is indicated by a dashed line\u2014components on the left side of boundary  are primarily software-based components (or portions of components implemented using software), and components on the right side of boundary  are primarily hardware-based components (or portions of components implemented using hardware or firmware or a combination thereof). An exemplary architecture includes a software-based media source reader  having access to a first memory  from which a hardware-based component can directly read from; a hardware-based demultiplexer (\u201cdemux\u201d)  generally having access to one or more blocks of memory (shown and referred to as a second memory  for discussion purposes); one or more hardware-based decoders\/renderers  also generally having access to one or more blocks of memory (shown and referred to as second memory  for discussion purposes); one or more hardware-based decoders\/renderers  also generally having access to one or more blocks of memory (shown and referred to as second memory  for discussion purposes); and application programming interfaces , which include a PhysMemDataStructure API , Sniffer\/Callback APIs , and SyncHelper APIs  including GetDecodeTimes API  and SyncToSTC API .","Media source reader  is responsible for receiving (via data push or pull techniques) individually-presentable portions of clips (referred to for discussion purposes as media units ) from a particular media source, storing the received media units  in memory , and for passing data regarding the stored media units  downstream (to demux  or decoders\/renderers , for example). In one possible implementation, data is passed downstream to demux  using data structures. In the context of a Microsoft\u00ae DirectShow\u2122 framework, for example, media units  are wrapped in data structures referred to as IMediaSample objects (IMediaSample references an interface the objects implement, the objects may be referred to as Media Samples). Often IMediaSample objects are constrained to a fixed size allocation at initialization time, and depending on sizes of media content units, may not be used to their full extent. Using a ring buffer  as discussed below enables more efficient use of memory.","Memory  represents any computer-readable medium (computer-readable media are discussed further below, in connection with ) accessible via the operating system of Presentation System , including but not limited to physically contiguous and scatter-gathered memory, virtually cached and uncached memory, physically locked and unlocked (for scatter-gather type) memory, and virtual memory mapping optimized for usage by a ring buffer  (discussed further below). Hardware-allocated memory block  is an abstract representation of an amount or area (of any size or configuration) of memory  that can be viewed as having blocks that may be separately allocated, via media source reader , for access by demux  (or other components of media processing pipelines  or ) in accordance with certain algorithms (an exemplary algorithm is shown and discussed below, in connection with ) and via use of certain APIs , such as PhysMemDataStructure API . In one exemplary implementation, hardware-allocated memory block  is ring buffer  having blocks in which individual media content units  obtained from media source reader  are stored. An advantage of using ring buffer  is that some computer-readable media can be read more efficiently when data is read via tracks, especially when ring buffer  does not put any packet constraints on the read operation (for example, from an optical device). Another advantage of using ring buffer  is for trick modes, when data is read faster than usual (from an optical drive, for example). Skipping parts of media content units that are not required to perform a full decode is more easily achieved with the chunking mechanism built into the ring buffer reader. Other details of ring buffer  and the benefits and operation thereof are discussed further below, in connection with .","Demux  is responsive to receive media units  (such as next processable media sample(s) , shown in ) and\/or data structures associated with media units  at input pin  from output pin  of media source reader , and to separate two or more signals (such as decoded streams of media content) that were previously combined by a compatible multiplexer. Memory(ies)  represent(s) one or more computer-readable media (computer-readable media are discussed further below, in connection with ), such as buffers or registers, usable by demux  or other hardware components. Demux  provides demultiplexed media samples  associated with individual media content streams (such as media content streams  or ) on output pin  to an input pin  of decoders\/renderers .","Decoders\/renderers  are responsible for receiving demultiplexed media units, referred to for discussion purposes as media samples  (MPEG-2 samples, for example), and for using generally well-known techniques for unscrambling\/unencrypting the demultiplexed media samples to produce media data  associated with a particular media content stream , . Although a one-to-one relationship between media sources, demultiplexers, and decoders\/renderers is shown, it will be appreciated that any arrangement of any number of such components (along with additional components) is possible, and that such components may be shared between media processing pipeline  and media processing pipeline .","APIs  are provided to enhance the interoperability of software components and hardware components within a media processing pipeline, and to promote the efficient use of memory and processing resources of Presentation System . In one possible implementation, APIs  are sets of computer-executable instructions encoded on computer-readable storage media that may be either executed during operation of Presentation System  and\/or accessed by authors of instructions for media processing components  and . Generally, APIs  are configured to perform aspects of the method(s) shown and discussed further below in connection with .","PhysMemDataStructure API  is configured to generalize the support of memory  that can be directly consumed by hardware components such as demux  and decoders\/renderers . In one possible implementation (in the context of a media processing pipeline having a DirectShow\u2122 framework, for example) media units  wrapped in IMediaSample objects are allocated (by means of an implementation of an IMemAllocator object (using input pin  of demux , for example\u2014output pin  would query input pin , so demux  can provide memory with properties usable\/needed by the hardware) to storage locations within hardware-allocated memory block , and information about such storage locations (such as the type memory; a size of a memory block; a location of a pointer to the memory; and an offset location of a storage location of a particular media unit with respect to a pointer to the memory) is exposed to hardware components such as demux  and decoders\/renderers  by PhysMemDataStructureAPI . Hardware components are thereby able to directly access\/retrieve information within hardware-allocated memory block  (via direct memory access techniques, for example), instead of using instructions and processor cycles to copy the information.","Exemplary pseudo-code usable for implementing PhysMemDataStructureAPI  in the context of media processing pipelines ,  and\/or media processing components ,  is shown below.",{"@attributes":{"id":"p-0056","num":"0055"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"175pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"interface IPhysMemMediaSample : IUnknown"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"{"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\/\/ S_OK==contigous physical memory, S_FALSE scatter\/gather"]},{"entry":[{},"method required"]},{"entry":[{},"HRESULT IsContigous( );"]},{"entry":[{},"\/\/ S_OK==cached memory, S_FALSE uncached"]},{"entry":[{},"HRESULT IsCached( );"]},{"entry":[{},"\/\/ S_OK==memory is ready for dma operation"]},{"entry":[{},"\/\/ S_FALSE==memory needs PageLock\/Unlock for DMA"]},{"entry":[{},"HRESULT IsPageLocked( );"]},{"entry":[{},"\/\/ S_OK==virtual memory pointer wraps around to start of"]},{"entry":[{},"\/\/ physical buffer when exceeding ring buffer size,"]},{"entry":[{},"\/\/ S_FALSE no wrap"]},{"entry":[{},"HRESULT IsAutoWrap( );"]},{"entry":[{},"\/\/ page size for scatter \/ gather table"]},{"entry":[{},"HRESULT PageSize("]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"126pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"[out] DWORD","*pPageLength"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},");"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\/\/ allow to build scatter\/gather table after memory is locked"]},{"entry":[{},"HRESULT GetPhysicalPointer("]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"126pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"[in] DWORD","offset,"]},{"entry":[{},"[out] DWORD","*pPhysAddress,"]},{"entry":[{},"[out] DWORD","*pLength"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},");"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\/\/ allow to build scatter\/gather table after memory is locked"]},{"entry":[{},"\/\/"]},{"entry":[{},"HRESULT GetScatterGatherTable("]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"126pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"[in] DWORD","size,"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"[out, size_is(size)] DWORD *pTable,"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"126pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"[out] DWORD","*pEntries"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},");"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\/\/ size in bytes for DWORD array required to build"]},{"entry":[{},"\/\/ scatter gather table"]},{"entry":[{},"HRESULT GetScatterGatherTableSize("]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"126pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"[out] DWORD","*pSize"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},");"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\/\/ lock \/ unlock physical pages in buffer"]},{"entry":[{},"HRESULT PageLock( );"]},{"entry":[{},"HRESULT PageUnlock( );"]},{"entry":[{},"\/\/ cache writeback & discard"]},{"entry":[{},"\/\/ use before dma from media sample buffer to hardware"]},{"entry":[{},"HRESULT CacheSyncWriteback("]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"[in] DWORD offset,"]},{"entry":[{},"[in] DWORD length"]},{"entry":[{},");"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\/\/ cache discard \/ invalidate"]},{"entry":[{},"\/\/ use before dma from hardware to media sample buffer"]},{"entry":[{},"HRESULT CacheSyncDiscard("]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"[in] DWORD offset,"]},{"entry":[{},"[in] DWORD length"]},{"entry":[{},");"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"};"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"Sniffer\/Callback APIs  are used to provide access by software-based elements of Presentation System  to certain media samples  (for example, \u201cHLI,\u201d \u201cADV,\u201d and \u201cNAV\u201d packets multiplexed in a high-definition DVD program stream) that have been parsed by demux  and\/or media data  that has been decoded\/rendered by decoders\/renderers . In one possible implementation, a DirectShow\u2122 framework filter is connected to output pin  of demux  or an output pin (not shown) of decoders\/renderers , and this filter is used to support the Sniffer\/Callback APIs .","Exemplary pseudo-code usable for implementing a Sniffer\/Callback API that will detect certain types of media samples  or media data  in the context of media processing pipelines ,  and\/or media processing components ,  is shown below.",{"@attributes":{"id":"p-0059","num":"0058"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"161pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"HRESULT RegisterCallback("]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"105pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"70pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"[in] IRendererDataCallBack*","pCallBack"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"175pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},");"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}},"br":{}},{"@attributes":{"id":"p-0060","num":"0059"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"168pt","align":"left"}}],"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"\u2003","HRESULT Callback("]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"77pt","align":"left"}},{"@attributes":{"colname":"4","colwidth":"91pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},"\u2003[in] IMediaSample*","pMediaSample"]},{"entry":[{},{},"\u2003\u2003) ;"]},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}}]}}]}}},"SyncHelper APIs  are configured to facilitate information exchange usable to maintain perceived synchronization between media content streams  and . GetDecodeTimes API  is configured to provide status notifications about certain times (such as title times  and media sample presentation times ) associated with times at which certain media samples (for example, media units  or media samples  deemed to be next processable media samples ) are being prepared for presentation by a hardware component (such as demux  or one or more decoders\/renderers ). Information provided via the SyncToSTC API  may be used, among other things, to adjust timing signals  and\/or  based on differences in title times  returned by GetDecodeTimes API  from different decoders\/renderers (or other hardware components) processing synchronously presentable media samples.","Exemplary pseudo-code usable for implementing SyncHelper APIs  is shown below.",{"@attributes":{"id":"p-0063","num":"0062"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"\/*++"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Implemented on the renderes"]},{"entry":[{},"Helper functions to synchronize streams and provide information"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"about PTS and STC times"},{"entry":"--*\/"},{"entry":"interface ISyncHelper : IUnknown"},{"entry":"{"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"\/\/ Returns the current STC time and the PTS of the sample currently"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"being decoded in"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\/\/ PTS_TIME_BASE ticks"]},{"entry":[{},"\/\/"]},{"entry":[{},"HRESULT"]},{"entry":[{},"GetDecodeTimes ("]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"91pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"98pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"[out] PTS_TIME* ptSTC,","\/\/ current STC time (global)"]},{"entry":[{},"[out] PTS_TIME* ptPTS","\/\/ current PTS time"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},");"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\/\/"]},{"entry":[{},"\/\/ Synchonize two sessions. Provides delta off the STC time to render"]},{"entry":[{},"samples,"]},{"entry":[{},"\/\/"]},{"entry":[{},"HRESULT"]},{"entry":[{},"SyncToSTC ("]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"91pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"98pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"[in] STC_IDENTIFIER","\/\/ the clock to sync to"]},{"entry":[{},"stcToSyncTo,"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"98pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"[in] PTS_TIME","tDelta","\/\/\u2003delta off the STC to render"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"samples at, can be \u2212ve"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},");"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"};"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"With continuing reference to ,  are flowcharts of methods for preparing media content (such as portions of one or more clips ) for presentation by one or more media processing pipelines (such as media processing pipeline  or media processing pipeline ) using the functionality provided by one or more APIs . The method shown in  is useful to minimize the processing and\/or memory resources used when information is transferred between a memory (such as memory ) used by a software component (such as media source reader  or another software component) and a memory (such as memory ) used by a hardware component (such as demux  or decoders\/renderers  or other hardware components). The method shown in  is useful to maintain perceived synchronization when portions of multiple concurrently playable media content streams are prepared for presentation by separate hardware components.","The processes illustrated in  may be implemented in one or more general, multi-purpose, or single-purpose processors, such as processor  discussed below in connection with . Unless specifically stated, the methods described herein are not constrained to a particular order or sequence. In addition, some of the described methods or elements thereof can occur or be performed concurrently.","Referring to the method shown in the flowchart of , the method begins at block  and continues at block , where a portion of a first memory, such as hardware-allocated memory block , is identified for storing media content units such as individually-playable portions of a clip  (such as media units  received from a particular media source ). A particular media content unit and a storage location for the media content unit in the first memory are identified at blocks  and , respectively.","In the context of media processing components ,  of media processing pipelines , , respectively, hardware-allocated memory block  may be implemented as ring buffer  to enhance the efficient use of memory and processing resources. Ring buffer  can be viewed as having blocks that may be separately allocated, via media source reader  (or other components of media processing pipelines  or ), for storing media units . The offset of each media unit  stored in ring buffer  is known, and can be expressed relative to the values of one or more pointers to locations within ring buffer , such as a beginning of memory (\u201cBOM\u201d) pointer , an end of memory (\u201cEOM\u201d) pointer , a beginning of used memory pointer (\u201cBUMP\u201d) , and\/or an end of used memory pointer (\u201cEUMP\u201d) . As demux  or another hardware component obtains representations of media units  from ring buffer , BUMP  and\/or EUMP  may be moved accordingly. Because media units  may be obtained and released out of order, a list of offsets of media units  within ring buffer  may be maintained to ensure that BUMP  and EUMP  are not permitted to bypass each other.","To further enhance memory use and processing efficiencies, virtual memory may be used to duplicate one or more memory blocks from the beginning of ring buffer  to the end of ring buffer . As shown, duplicate BOM block  (which is a duplicate of beginning-of-memory \u201cBOM\u201d block ) is implemented using virtual memory, and is logically located after end-of-memory \u201cEOM\u201d block . This use of virtual memory is referred to as the \u201cauto-wrap\u201d function, because it is especially useful when breaking up a larger block of memory to be used in a ring buffer fashion with read and write pointers. Use of the auto-wrap function is optional\u2014generally the provider of demux  can choose to provide memory that does not map twice and the media processing pipeline will still work, but may make less efficient use of memory. In such a ring buffer implementation there is the special case that the piece of memory that \u201cwraps around\u201d to the beginning of the buffer may require special treatment. For example, copying or otherwise obtaining the information in the portion of memory that wraps around may require two transactions\u2014one transaction to retrieve the information in the end of the buffer, and another transaction to retrieve the information in the beginning of the buffer. Thus, it is usually difficult to take full advantage of the ring buffer size. Use of virtual memory as described above avoids the need to either allocate extra memory or skip to the end of the ring buffer (both result in inefficient use of memory) when the information size is too large to fit at the end of the ring buffer.","Exemplary code usable (for Microsoft\u00ae Windows CE 6.0 operating system software, although any operating system using virtual memory may be used) for implementing an \u201cauto-wrap\u201d feature that maps a physical piece of memory twice to a double-sized virtual memory region is shown below.",{"@attributes":{"id":"p-0070","num":"0069"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"\/\/ map physical memory twice to double sized virtual memory region"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"CallerProc = GetCallerProcess( );"]},{"entry":[{},"VirtualAddress = VirtualAllocEx(CallerProc, 0, Size*2,"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"MEM_RESERVE, PAGE_NOACCESS);"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"VirtualCopyEx(CallerProc , VirtualAddress,"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"GetCurrentProcess( ), (PVOID)( PhysicalAddress>> 8), Size,"]},{"entry":[{},"PAGE_PHYSICAL | PAGE_READ WRITE | (Cached ? 0 :"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"PAGE_NOCACHE)) )"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"VirtualCopyEx(CallerProc , (PBYTE) VirtualAddress+Size,"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"GetCurrentProcess( ), (PVOID)( PhysicalAddress>> 8), Size,"]},{"entry":[{},"PAGE_PHYSICAL | PAGE_READWRITE | (Cached ? 0 :"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"PAGE_NOCACHE))"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}}},"Referring again to the flowchart of , block  illustrates the step of forming a data structure associated with the particular media content unit, the data structure having a field for storing information about the storage location of the media content unit in the first memory. Next, at block , the data structure is exposed to a hardware component (such as demux  or decoders\/renderers ) that has a second memory. At block , it can be seen that the hardware component can use the information in the data structure about the storage location of the media content unit to directly transfer the media content unit from the first memory to the second memory, without a central processing unit.","In the context of media processing components ,  implemented using DirectShow\u2122 frameworks, media source reader  uses data structures such as IMediaSampleObjects to provide all or some of the following information to downstream hardware components: pointers to memory  and\/or hardware-allocated memory block ; size of memory  and\/or hardware-allocated memory block ; start and stop times of media units ; flag(s); and any other desired information. Advantageously, information regarding properties of memory blocks of ring buffer  allocated by media source reader  for access by demux  (and other hardware components) are exposed via PhysMemDataStructure API , which may also be provided by a data structure (or fields thereof) such as the IMediaSampleObject. Physical memory information derived by demux  and other hardware components from the PhysMemDataStructure API  are used to directly access storage location of individual media content units  within ring buffer , largely obviating the need for processor-intensive copy transactions such as \u201cmemcopy\u201d transactions. Information regarding properties of hardware-allocated memory block  that is exposed via the PhysMemDataStructure API  include but is not limited to: the type of memory ; a size of a memory block of the memory; a location of one or more pointers , , , or  to the memory; and an offset location of a particular media unit  with respect to one or more pointers to the memory.","Referring to the method shown in the flowchart of , the method begins at block  and continues at blocks  and , where, respectively, a play duration of a multimedia presentation is identified, and two clips (each having their own play durations) playable as separate media content streams, such as media content stream  and media content stream , are identified. Next, two synchronously presentable media samples, one from the first clip and one from the second clip, are identified, at blocks  and , respectively.","Generally, software-based components of Presentation System (such as aspects of presentation manager ) are aware of currently playable clips . In the context of media processing components ,  of media processing pipelines , , respectively, it is possible to use Sniffer\/Callback APIs  to identify specific media units  and\/or media samples  being processed by demux  and\/or decoders\/renderers .","As indicated at block , certain information is ascertained at a first time\u2014the first time associated with when the media sample from the first clip is undergoing preparation for presentation by a first hardware component, such as demux  or decoder\/renderer  within media processing pipeline . The following information is ascertained at block : an elapsed amount of the play duration of the digital media presentation, and an elapsed amount of the play duration of the first clip.","As indicated at block , certain information is ascertained at a second time\u2014the second time associated with when the media sample from the second clip is undergoing preparation for presentation by a second hardware component, such as demux  or decoder\/renderer  within media processing pipeline . The following information is ascertained at block : an elapsed amount of the play duration of the digital media presentation, and an elapsed amount of the play duration of the second clip.","As discussed above in connection with the media exemplary media timeline shown in , the elapsed amount of the play duration is often referred to the title time (or the global system time), and an elapsed amount of the play duration of the a particular clip generally corresponds to a particular pre-determined media sample presentation time  associated with a particular media sample. The GetDecodeTimes API  is configured to examine media samples and\/or media timelines  of both the first and second clips, and to return the information indicated at blocks  and .","At block , the difference between the elapsed amount of the play duration of the digital media presentation calculated at block  and the elapsed amount of the play duration of the digital media presentation calculated at block  is ascertained, and, as indicated at block , is usable to adjust timing of the hardware components for preparing and\/or presenting media samples.","In the context of media processing components ,  of media processing pipelines , , respectively, the SyncToSTC API  is configured to use information obtained via the GetDecodeTimesAPI  to synchronize various media content streams from different hardware components, by applying deltas (based on the difference between the elapsed amount of the play duration ascertained at block ) to processing times and\/or timing signals, such as timing signals  and . It will be appreciated that the SyncToSTC API  can also be used to synchronize media content streams with other playback constraints (for example, as defined by a playlist).","With continued reference to ,  is a block diagram of an exemplary configuration of an operating environment  in which all or part of Presentation System  may be implemented or used. Operating environment  is generally indicative of a wide variety of general-purpose or special-purpose computing environments. Operating environment  is only one example of a suitable operating environment and is not intended to suggest any limitation as to the scope of use or functionality of the system(s) and methods described herein. For example, operating environment  may be a type of computer, such as a personal computer, a workstation, a server, a portable device, a laptop, a tablet, or any other type of electronic device, such as an optical media player or another type of media player, now known or later developed, or any aspect thereof. Operating environment  may also be a distributed computing network or a Web service, for example. A specific example of operating environment  is an environment, such as a DVD player or an operating system associated therewith, which facilitates playing high-definition DVD movies.","As shown, operating environment  includes or accesses components of a computing unit, including one or more processors , computer-readable media , and computer programs . Processor(s)  is\/are responsive to computer-readable media  and to computer programs . Processor(s)  may be physical or virtual processors, and may execute instructions at the assembly, compiled, or machine-level to perform a particular process. Such instructions may be created using source code or any other known computer program design tool.","Computer-readable media  represent any number and combination of local or remote devices, in any form, now known or later developed, capable of recording, storing, or transmitting computer-readable data, such as the instructions executable by processor . In particular, computer-readable media  may be, or may include, a semiconductor memory (such as a read only memory (\u201cROM\u201d), any type of programmable ROM (\u201cPROM\u201d), a random access memory (\u201cRAM\u201d), or a flash memory, for example); a magnetic storage device (such as a floppy disk drive, a hard disk drive, a magnetic drum, a magnetic tape, or a magneto-optical disk); an optical storage device (such as any type of compact disk or digital versatile disk); a bubble memory; a cache memory; a core memory; a holographic memory; a memory stick; a paper tape; a punch card; or any combination thereof. Computer-readable media  may also include transmission media and data associated therewith. Examples of transmission media\/data include, but are not limited to, data embodied in any form of wire line or wireless transmission, such as packetized or non-packetized data carried by a modulated carrier signal. The above notwithstanding, computer-readable media  does not include any form of propagated data signal.","Computer programs  represent any signal processing methods or stored instructions that electronically control predetermined operations on data. In general, computer programs  are computer-executable instructions implemented as software components according to well-known practices for component-based software development, and encoded in computer-readable media (such as computer-readable media ). Computer programs may be combined or distributed in various ways.","Storage  includes additional or different computer-readable media associated specifically with operating environment , such as an optical disc or other portable (optical discs are handled by optional optical disc drive ). One or more internal buses , which are well-known and widely available elements, may be used to carry data, addresses, control signals and other information within, to, or from operating environment  or elements thereof.","Input interface(s)  provide input to computing environment . Input may be collected using any type of now known or later-developed interface, such as a user interface. User interfaces may be touch-input devices such as remote controls, displays, mice, pens, styluses, trackballs, keyboards, microphones, scanning devices, and all types of devices that are used input data.","Output interface(s)  provide output from operating environment . Examples of output interface(s)  include displays, printers, speakers, drives (such as optical disc drive  and other disc drives or storage media), and the like.","External communication interface(s)  are available to enhance the ability of operating environment  to receive information from, or to transmit information to, another entity via a communication medium such as a channel signal, a data signal, or a computer-readable medium. External communication interface(s)  may be, or may include, elements such as cable modems, data terminal equipment, media players, data storage devices, personal digital assistants, or any other device or component\/combination thereof, along with associated network support devices and\/or software or interfaces.",{"@attributes":{"id":"p-0088","num":"0087"},"figref":"FIG. 8","b":["800","100","700","100","700","802","800","804","800","803","802","804"]},"On client-side , one or more clients , which may be implemented in hardware, software, firmware, or any combination thereof, are responsive to client data stores . Client data stores  may be computer-readable media , employed to store information local to clients . On server-side , one or more servers  are responsive to server data stores . Like client data stores , server data stores  may include one or more computer-readable media , employed to store information local to servers .","Various aspects of a presentation system that is used to present interactive content to a user synchronously with media content have been described. It will be understood, however, that all of the described components of the presentation system need not be used, nor must the components, when used, be present concurrently. Functions\/components described in the context of Presentation System  as being computer programs are not limited to implementation by any specific embodiments of computer programs. Rather, functions are processes that convey or transform data, and may generally be implemented by, or executed in, hardware, software, firmware, or any combination thereof.","Although the subject matter herein has been described in language specific to structural features and\/or methodological acts, it is also to be understood that the subject matter defined in the claims is not necessarily limited to the specific features or acts described above. Rather, the specific features and acts described above are disclosed as example forms of implementing the claims.","It will further be understood that when one element is indicated as being responsive to another element, the elements may be directly or indirectly coupled. Connections depicted herein may be logical or physical in practice to achieve a coupling or communicative interface between elements. Connections may be implemented, among other ways, as inter-process communications among software processes, or inter-machine communications among networked computers.","The word \u201cexemplary\u201d is used herein to mean serving as an example, instance, or illustration. Any implementation or aspect thereof described herein as \u201cexemplary\u201d is not necessarily to be constructed as preferred or advantageous over other implementations or aspects thereof.","As it is understood that embodiments other than the specific embodiments described above may be devised without departing from the spirit and scope of the appended claims, it is intended that the scope of the subject matter herein will be governed by the following claims."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":["FIG. 2","FIG. 1"]},{"@attributes":{"id":"p-0015","num":"0014"},"figref":["FIG. 3","FIG. 1"]},{"@attributes":{"id":"p-0016","num":"0015"},"figref":["FIG. 4","FIG. 3"]},{"@attributes":{"id":"p-0017","num":"0016"},"figref":["FIG. 5","FIG. 1","FIG. 3","FIG. 4"]},{"@attributes":{"id":"p-0018","num":"0017"},"figref":["FIG. 6","FIG. 1","FIG. 3","FIG. 4"]},{"@attributes":{"id":"p-0019","num":"0018"},"figref":["FIG. 7","FIG. 1","FIGS. 5 and 6"]},{"@attributes":{"id":"p-0020","num":"0019"},"figref":["FIG. 8","FIG. 7"]}]},"DETDESC":[{},{}]}
