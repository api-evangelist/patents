---
title: Quality-driven optimization of sensor stream processing
abstract: A system and method to perform data quality driven optimization of data are described. In one embodiment, a method is presented to iteratively test configurations of a data processing path until a configuration that processes data to predefined quality requirements is identified. In one embodiment, a system is presented. The system includes a data quality initialization module, a primary data stream processing module and an optimization module that is incorporated in a memory chip on a computer processor.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08238231&OS=08238231&RS=08238231
owner: SAP AG
number: 08238231
owner_city: Walldorf
owner_country: DE
publication_date: 20090528
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION"],"p":["The invention relates generally to data stream processing, and, more specifically, to quality-driven optimization of sensor stream processing.","Smart monitoring systems use sensor data to guide automatic manufacturing processes and complex business decisions. The restricted quality of sensor data due to limited sensor precision and sensor failures poses a crucial problem, which is very often ignored by the application owners. If not handled carefully, these data quality deficiencies result in misguided or even incorrect decisions.","Data stream management systems have been developed to process continuous data flows of high data rate and volume. For example, data streams are recorded in sensors networks to control manufacturing processes or maintenance activities. Further, turnover values or sales volume may be streamed from distributed affiliations to the central controlling department to derive management strategies. Data stream systems encounter limitations such as, limited memory capacity, data transfer capability and computational power. One way to address these constraints is to reduce the data stream volume, which can result in loss of information. For instance, data processing results such as aggregations may have to be approximated resulting in incorrect or incomplete outcomes. Moreover, this may exacerbate the already existing problem of limited data quality in multiple dimensions of data stream sources. Inaccuracies may result, for example, by sensor imprecision, by typos in text elements, or by RFID readers failing to scan an item properly. The completeness of a data stream is reduced whenever a true world event is missed due to sensor or system malfunction.","A system and method to perform data quality driven optimization of a data processing path are described. The method includes iteratively testing configurations of a data processing path until a configuration that processes data to predefined quality requirements is identified.","Embodiments of systems and methods for optimizing data quality processing are described herein. In the following description, numerous specific details are set forth to provide a thorough understanding of embodiments of the invention. One skilled in the relevant art will recognize, however, that the invention can be practiced without one or more of the specific details, or with other methods, components, materials, etc. In other instances, well-known structures, materials, or operations are not shown or described in detail to avoid obscuring aspects of the invention.","Reference throughout this specification to \u201cone embodiment\u201d or \u201cthis embodiment\u201d means that a particular feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment of the present invention. Thus, the appearances of the phrases \u201cin one embodiment\u201d or \u201cin this embodiment\u201d in various places throughout this specification are not necessarily all referring to the same embodiment. Furthermore, the particular features, structures, or characteristics may be combined in any suitable manner in one or more embodiments.","Embodiments of the invention may be used in systems where data is collected and processed to allow for automatic process control and to guide business decisions, for example, in manufacturing, or in analysis of qualitative data. Based on the outcome of the processing of the data, decisions may be made for the process providing the data. For the processing to produce reliable output, the collected data may need to satisfy quality requirements. Data quality represents how good data is for a specific application. A data quality dimension may be defined as an aspect describing the utility of data. Different data quality dimensions may be applicable in different applications or different scenarios. Examples of data quality dimensions are timeliness, completeness, confidence, accessibility, readability, and so on. For example, the processing of data may estimate that the dimension timeliness is not of the expected quality, that is, the dimension timeliness does not fulfill the requirements set forth for that particular dimension for that particular scenario. Thus, a system may include instructions to be applied on the process generating the data in case timeliness is estimated to be of undesirable quality. An operator is typically an operation used in the processing of data. In other words, data processing is the process of applying a set of operators to data in a certain sequence (thus producing output to guide business decisions). Some examples of operators are selection, projection, join, interpolation, aggregation, and so on. Some operators may have attributes and some may not. Attributes may also be referred to as \u201cparameters\u201d. Some operators may influence the quality of data and some may not influence the quality of data. For example, projection does not influence the quality of data, while aggregation influences the quality of data. Table 1 below lists exemplary data quality operators with their parameters:",{"@attributes":{"id":"p-0015","num":"0014"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"9"},"colspec":[{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"8","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"9","colwidth":"42pt","align":"center"}}],"thead":{"row":[{"entry":"TABLE 1"},{"entry":{"@attributes":{"namest":"1","nameend":"9","align":"center","rowsep":"1"}}},{"entry":[{},{},{},{},{},"Amount of",{},{},{}]},{"entry":["Operator","Parameter","Accuracy","Confidence","Completeness","Data","Timeliness","Stream Volume","Granularity"]},{"entry":{"@attributes":{"namest":"1","nameend":"9","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Projection","\u2014",{},{},{},{},{},{},{}]},{"entry":["Selection","\u2014"]},{"entry":["Join","\u2014"]},{"entry":["Aggregation","Group size",{},{},{},"+",{},"\u2212","\u2212"]},{"entry":["Sampling","Rate",{},"\u2212","\u2212",{},"+","+"]},{"entry":["Frequency","Group size",{},{},{},"+",{},"\u2212","\u2212"]},{"entry":"Analysis"},{"entry":["Filter","Group size"]},{"entry":["Algebra","\u2014"]},{"entry":["Threshold","\u2014"]},{"entry":{"@attributes":{"namest":"1","nameend":"9","align":"center","rowsep":"1"}}}]}}}}},"Embodiments described herein provide an algorithm and architecture to define an optimal setting for at least some of the parameters for the processing of data according to a scenario. Parameters, in the processing, may have desirable optimum values for the processing of data in a given scenario (for example, based on user requirements). Moreover, some parameters may influence other parameters, that is, the change of the setting of one parameter may influence another parameter. That is why, it may be necessary to find compromises between parameter settings. The finding of these compromises is known as multi-objective optimization. Thus, an embodiment of the invention may define all optimum compromises of parameter settings for the processing of data in a scenario so that there is a combination of the best possible compromises for the quality of data processing in a given scenario. The combination of the best possible compromises of parameter settings is relevant within a specific scenario, that is, each scenario may require different quality of data processing based on user requirements or on the nature of data itself, thus, best possible compromises for one scenario may not be relevant for another scenario.",{"@attributes":{"id":"p-0017","num":"0016"},"figref":["FIG. 1","FIG. 1"],"b":["100","154","100","102","112","100","152","100","100","100","100"]},{"@attributes":{"id":"p-0018","num":"0017"},"figref":["FIG. 2","FIG. 2"],"b":["202","204","206"]},"However if the parameters do meet the quality requirements, at process block , the identified configuration is applied to the primary processing path. The identified configuration is such that, when applied, the resulting processed data will meet the data quality requirements of data quality dimensions as required by the user. Applying this configuration to the primary processing path optimizes the primary processing path so that the data processed by the primary processing path is of quality that is better suited to the scenario and the user requirements. For example, if a configuration that is of a certain quality of readability and timeliness of data is desired then a configuration of the processing path that yields such a result may be identified.",{"@attributes":{"id":"p-0020","num":"0019"},"figref":["FIG. 3","FIG. 3"],"b":["302","304","306","308","310","308","312"]},{"@attributes":{"id":"p-0021","num":"0020"},"figref":["FIG. 4","FIG. 4"],"b":["402","402","404","402","406","402","408","410","412","414","410","406","410","406","402","408","410","410","412","406","410","408"]},"In one embodiment, the optimization module  may use an optimization algorithm that applies a heuristic optimization technique called Evolution Strategy (ES) that is based on ideas of biological adaptation and evolution and supports multi-objective as well as single-objective optimization problems. There are various strategies to solve multi-objective optimization problems. Pareto optimization allows the finding of a set of optimal compromises. The Pareto set is defined as the set of optimization solutions that dominates all other possible solutions. That is, the improvement of one sub-objective is only possible with a decline of one or more of the other sub-objectives. Moreover, the different objectives can be summarized in one function, which will be minimized or maximized. Priorities can be defined for each objective determining the order of optimization. This may be done prior to the algorithm execution, for example by weighing the sub-objectives or by a lexicographical ordering. Another possibility is the selection or prioritization of the sub-objectives after the execution of the optimization algorithm. The main advantage of the multi-objective optimization is the automatic optimization of different aims by determining optimal compromises. No user-defined weighing or ordering is necessary, which restrict the search space and may lead to minor optimization solutions. Instead, the user is supported by a set of Pareto optimal compromises and can decide after the algorithm execution which solution fits best to the set of requirements of the current optimization.","In one embodiment, the method as described in  is performed by components as described in . Referring to , at process block , the static checker module  performs a static check. The static check estimates if user defined requirements in the quality of processing can be achieved. For example, there may be external constraints that make it impossible for the data processing to be optimized as per the quality requirements of the user. Such constraints may be the precision of the data sources  that obtain the data or the rate of data provision. If conflicts are found, they are reported to the user. At process block , the optimization module  receives a copy of the primary data processing path of the primary processing module  to copy to a secondary processing path in the secondary processing module . The secondary processing path allows the optimization to be performed simultaneously with the primary processing, thus saving time and resources. At process block , the optimization module  iteratively tests processing path configurations to estimate the best possible configuration for the processing path so that the quality requirements are met. At each iteration, the secondary processing path is run with a specific configuration of a partition of data, a set of sampling operators, a set of interpolation operators, and a set of data group sizes. Running the processing path with the specific configuration produces average values for the data quality parameters required by the user quality requirements. If these parameter values do not meet the user's quality requirements, iterations may follow until a configuration of the secondary processing path derives average data quality parameter values meeting the quality requirements. At process block , the optimization module  applies the identified configuration to the primary processing path. The identified configuration is such that, when applied, the resulting processed data will meet the data quality requirements of data quality dimensions as required by the user. Applying this configuration to the primary processing path optimizes the primary processing path so that the data processed by the primary processing path is of quality better suited to the scenario and user requirements. For example, a configuration may be desired so that the readability and timeliness of data is of a certain quality. Thus, a configuration of the processing path may be identified so that when applied, the processed data has the desired average readability and timeliness as required.","In one embodiment, the method as described in  is performed by components as described in . Referring to , at process block , the optimization module  selects a combination of configuration parameters for the secondary processing path in the secondary processing module . These may include a partition of data to process, a set of sampling operators, group sizes, and interpolation rates. At processing block , secondary processing module  processes the selected partition. Processing the partition yields average values for monitored data quality dimensions. The monitored data quality dimensions are set forth in the user defined quality requirements or are imposed by the nature of the scenario, such as the type of manufacture or industry. At process block , the averages are used by the fitness module  to compute the fitness of the averages. The fitness of the averages represents how well, if at all, the averages meet the quality requirements. At process block , the optimization module  checks if the computed fitness meets quality requirements. If the computed fitness does not meet the quality requirements, at process block , the configuration module  derives another configuration for the secondary processing path using the data quality dimension averages. As each following configuration of the secondary processing path is derived from averages of data quality dimensions from the previous configuration of the secondary processing path, with each iteration, the secondary processing path is fine-tuned closer and closer to the quality requirements. The iterations and testing of configurations of the processing path may be performed until at some iteration, at process block , the optimization module  estimates that the fitness of the computed averages of data quality dimensions meets the quality requirements. At process block , the identified configuration is sent to the primary processing path in the data quality initialization module  and the primary processing module . As the primary processing module  runs the primary processing path with the identified optimized configuration, the processed data that is the result of the primary processing path will have data quality dimensions as required in the quality requirements.","Embodiments of the present invention may be implemented for various purposes in various industries where data quality monitoring and optimization is of importance. For example embodiments of the present invention may include data streams to control manufacturing processes, to guide automatic production lines, for decision support at high-level management (customer, financial, and economic data streams), to monitor telecommunication networks, and to monitor patients in a hospital (for instance heart rate).","In one embodiment, a system of the embodiment of the invention performs data quality driven optimization of a data stream. As used hereinafter, a \u201cdata stream\u201d may comprise a continuous stream of m tuples \u03c4, consisting of n attribute values A(1\u2266i\u2266n) and the represented time interval [t,t]. To allow for the efficient management of data quality in data streams, a data quality window approach is adopted. In one embodiment, a data quality window may be used as a partition in the secondary processing path in the process described in . Data quality information is not forwarded for each single data item, but aggregated over \u03c9data items independent for each stream attribute A. The stream is partitioned into \u03baconsecutive, non-overlapping jumping data quality windows w(k) 1\u2266k\u2266\u03ba, each of which is identified by its starting point tw, its end point tw, the window size \u03c9and the corresponding attribute Aas illustrated in the exemplary Table 2 below.",{"@attributes":{"id":"p-0027","num":"0026"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"13"},"colspec":[{"@attributes":{"colname":"1","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"8","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"9","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"10","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"11","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"12","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"13","colwidth":"14pt","align":"center"}}],"thead":{"row":[{"entry":"TABLE 2"},{"entry":{"@attributes":{"namest":"1","nameend":"13","align":"center","rowsep":"1"}}},{"entry":["Timestamp",". . .","210","211","212","213","214","215","216","217","218","219",". . ."]},{"entry":{"@attributes":{"namest":"1","nameend":"13","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":["Pressure",". . .","180","178","177","175","176","181","189","201","204","190",". . ."]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"112pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"112pt","align":"char","char":"."}},{"@attributes":{"colname":"5","colwidth":"14pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Accuracy",". . .","3.0","3.9",". . ."]},{"entry":["Confidence",". . .","4.4","21.7",". . ."]},{"entry":["Completeness",". . .","0.9","0.8",". . ."]},{"entry":["DataBasics",". . .","1","1",". . ."]},{"entry":[{},{},"t= 210","t= 215"]},{"entry":[{},{},"t= 214","t= 219"]},{"entry":[{},{},"\u03c9 = 5","\u03c9 = 5"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}}]}}},"Beyond the data stream items x(j)(tw\u2266j\u2266tw), the window may contain |Q| data quality information q, each obtained by averaging the tuple-wise data quality information over the window.","The data quality Q of a data stream D may be defined by a set of five data quality dimensions: accuracy a, confidence \u03b5, completeness c, data amount d and timeliness u. In one embodiment, these data quality dimensions and their average values are computed by a secondary processing module such as the one described in  in a secondary processing path such as the secondary processing path described in .","The data quality-driven optimization configures the data stream processing to maximize the above defined stream data quality while guaranteeing the compliance to the restricted system resources. In one embodiment, an optimization module, such as the one described in , performs this configuring of the data stream processing. The data stream volume V is used as a metric for the resource load. From a user's point of view, a high data volume has positive effects, because a high data volume provides more details, and with more details in the data stream, a better decision may be made. To measure data granularity, a timeframe T represented by one data stream tuple is selected. While raw data depicts one point in time, the result of a data stream aggregation represents a larger time interval. The wider the timeframe, the lower is the granularity. To support the detailed evaluation of streaming data, the granularity has to be maximized.","With respect to the five data quality dimensions given above, accuracy describes defects or imprecision of data stream sources; thus, the accuracy may not be improved by any operator configuration. The objectives determined above span different value domains. For example, the window completeness constitutes values in the range 0\u2266c\u22661, while the absolute systematic error in the dimension accuracy is unlimited: 0\u2266a\u2266\u221e. To allow for the quantitative comparison of different objectives, the objective functions are normalized to the range [0,1].","The confidence illustrates the statistical error \u03b5 due to random environmental interferences (e.g. vibrations and shocks) defining the interval [\u03bd\u2212\u03b5; \u03bd+\u03b5] around the data stream value \u03bd containing the true value with the confidence probability p. In one embodiment, confidence \u03b5 may be defined by (1\u2212p\/2)-quantil \u03b1 and the data variance \u03c3of each data quality window w. For example, for p=99%, the initial confidence of a data quality window including the pressure measurement values {181, 189, 201, 204, 190} is set to \u03b5=\u03b1\u00b7\u03c3=2.58\u02dc8.4 bar.","The average statistical error over all data stream attributes has to be minimized to maximize the data quality confidence. The objective function is normalized by division with the maximal statistical confidence error \u03b5in the stream. The objective \u0192 is defined as follows:",{"@attributes":{"id":"p-0034","num":"0033"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"msub":{"mi":["f","\u025b"]},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mtext":":"},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"min","mfrac":{"mn":"1","mrow":{"mi":"n","mo":"\u00b7","msub":{"mi":["\u025b","max"]}}},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mfrac":{"mn":"1","msub":{"mi":["k","i"]}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"1"},"msub":{"mi":["k","i"]}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":["\u025b","w"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"k"}}}}}},"mo":"\u2062","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}}},"The completeness addresses the problem of missing values due to stream source failures or malfunctions. As a compromise between the quality of value estimation and computational capacity, linear interpolation is applied.","The data quality dimension completeness c may be defined as the ratio of originally measured, not interpolated values compared to the size of the analyzed data quality window. For example, if a temperature sensor in a monitoring system fails briefly two times during a data quality window (r=1\/min, \u03c9=1 h). The completeness for this window is set to c=0.967. To conform with the objective above, the objective of maximal completeness is transformed to the minimizing problem \u0192, which minimizes the ratio of interpolated data items. It is normalized using the maximal incompleteness 1\u2212cas follows:",{"@attributes":{"id":"p-0037","num":"0036"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":["f","c"]},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mtext":":"},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"min","mfrac":{"mn":"1","mrow":{"mi":"n","mo":"\u00b7","mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","msub":{"mi":["c","min"]}}}}},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mfrac":{"mn":"1","msub":{"mi":["k","i"]}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"1"},"msub":{"mi":["k","i"]}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mo":["(",")"],"mrow":{"msub":{"mn":"1","mrow":{"mo":"-","msub":{"mi":["c","w"]}}},"mo":"\u2062","mrow":{"mo":["(",")"],"mi":"k"}}}}}}}}}},"The data amount determines the set of raw data x used to derive a data stream tuple y=f(x). The higher the amount of data, the more reliable is the processed information. To transform the objective to a minimization problem, the difference to the highest possible data amount d=m that comprises the complete data stream whose maximum at the same time serves as normalization weight, is calculated as follows:",{"@attributes":{"id":"p-0039","num":"0038"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":["f","d"]},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mtext":":"},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"min","mfrac":{"mn":"1","mrow":{"mi":"n","mo":"\u00b7","mrow":{"mo":["(",")"],"mrow":{"mi":"m","mo":"-","msub":{"mi":["d","min"]}}}}},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mfrac":{"mn":"1","msub":{"mi":["k","i"]}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"1"},"msub":{"mi":["k","i"]}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"m","mo":"-"},"mo":"\u2062","mrow":{"msub":{"mi":["d","w"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"k"}}}}}}}}}}},"To maximize the data quality dimension timeliness, the average tuple age has to be minimized. Therefore, the age is computed as difference between tuple timestamp t(j) and correct system time clock, normalized by the maximum age uas follows:",{"@attributes":{"id":"p-0041","num":"0040"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msub":{"mi":["f","u"]},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mtext":":"},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"min","mfrac":{"mn":"1","mrow":{"mi":"m","mo":"\u00b7","msub":{"mi":["u","max"]}}},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"u","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"j"}}}},{"mfrac":{"mn":"1","mrow":{"mi":"clock","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"msub":{"mi":["t","min"]}}},"mo":"\u00b7","mrow":{"mo":["[","]"],"mrow":{"mi":"clock","mo":"-","mrow":{"mfrac":{"mn":"1","mi":"m"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"t","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"j"}}}}}}}],"mo":"="}}}},"The data stream volume V defines the number of transferred data values in n stream attributes and m data tuples: V=m\u00b7n. The transferred data quality information is incorporated in the calculation. The additional volume is computed based on the number of transferred data quality dimensions Qper attribute Aand the average data quality window size .",{"@attributes":{"id":"p-0043","num":"0042"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"V","mo":"=","mrow":{"mrow":[{"mi":"m","mo":"\u00b7","mrow":{"mo":["(",")"],"mrow":{"mi":"n","mo":"+","mn":"1"}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mfrac":{"mi":"m","msub":{"mi":["\u03d6","i"]}},"mo":"\u00b7","mrow":{"mo":["\uf603","\uf604"],"msub":{"mi":["Q","i"]}}}}],"mo":"+"}}}},"br":{},"sub":["max","max","max ","max"]},{"@attributes":{"id":"p-0044","num":"0043"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":["V","max"]},"mo":"=","mrow":{"mrow":[{"msub":{"mi":["m","max"]},"mo":"\u00b7","mrow":{"mo":["(",")"],"mrow":{"mi":"n","mo":"+","mn":"1"}}},{"msub":{"mi":["m","max"]},"mo":"\u00b7","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":{"mo":["\uf603","\uf604"],"msub":{"mi":["Q","i"]}},"mo":"."}}}],"mo":"+"}}}},"br":{}},{"@attributes":{"id":"p-0045","num":"0044"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":["f","V"]},"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mtext":":"},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"min","mfrac":{"mi":"V","msub":{"mi":["V","max"]}}}}}},"The data stream granularity T may be measured as the average time frame [t\u2212t] represented by the m data stream tuples. For raw data items describing one point in time, the granularity equals 0, as t=t. To maximize the granularity, the average timeframe normalized by its maximum Thas to be minimized.",{"@attributes":{"id":"p-0047","num":"0046"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msub":{"mi":["f","T"]},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mtext":":"},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"min","mfrac":{"mn":"1","mrow":{"mi":"m","mo":"\u00b7","msub":{"mi":["T","max"]}}},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":["t","e"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"j"}}}},{"msub":{"mi":["t","b"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"j"}}],"mo":"-"}}}},"To optimize the data quality in a data stream, in one embodiment, a set of configuration parameters are configured, which may include, but are not limited to, aggregation, sampling, interpolation, frequency analysis, and filtering. In one embodiment, these configuration parameters may be configured by a configuration module as described in . In one embodiment, these configuration parameters may be iteratively combined and tested to derive a configuration that when applied on a data stream processing may process a data stream and produce data quality dimension values in line with a set of quality requirements.","In one embodiment, a system is implemented to monitor the pressure of hydraulic systems to detect leaks or blocks. A set of sensors (e.g. oil viscosity, particle contamination, pressure, and temperature) is used to control the ageing of hydraulic oil, for example, to predict efficient maintenance dates for oil change. In this context, the sensor data quality has to be monitored, for example, to prevent from undetected contaminated oil corroding system components like filters or sealing. If sensor data quality is not properly monitored, this may lead to high maintenance costs or even a system breakdown. In one embodiment, sensor data quality may be monitored using the process described in .","In another embodiment, a system is implemented to monitor changes in climate for a given region. Using a real-world weather dataset that consists of cloud and weather measurements over several years recorded at land stations and ships all over the globe. Each tuple is identified by time (year, month, day, and hour) and place (latitude, longitude) of recording and contains information about present weather, cloud cover, solar attitude, etc. In an exemplary processing path, the embodiment of the invention evaluates weather measurements taken at land stations in the month of June 1990. For example, the query",{"@attributes":{"id":"p-0051","num":"0050"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"182pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"SELECT AVG(total_cloud_cover)"]},{"entry":[{},"FROM june90"]},{"entry":[{},"WHERE latitude > 37470 AND latitude < 37480"]},{"entry":[{},"AND longitude > 12220 AND longitude < 12230"]},{"entry":[{},"GROUP BY day"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}},"br":{}},"Some example embodiments of the invention may include the above-illustrated modules and methods being written as one or more software components. These components, and the functionality associated with each, may be used by client, server, or peer computer systems. These components may be written in any computer programming languages including object-oriented computer languages such as C++, and Java. The functionality described herein may be distributed among different components and may be linked to each other via application programming interfaces and compiled into one complete server and\/or client application. Furthermore, these components may be linked together via distributed programming protocols. Some example embodiments of the invention may include remote procedure calls being used to implement one or more of these components across a distributed programming environment. For example, a logic level may reside on a first computer system that is remotely located from a second computer system containing an interface level (e.g., a graphical user interface). These first and second computer systems can be configured in a server-client, peer-to-peer, or other configurations.","Software components described above are tangibly stored on a machine readable medium including a computer readable medium. The term \u201ccomputer readable medium\u201d should be taken to include a single medium or multiple media that stores one or more sets of instructions. The term \u201ccomputer readable medium\u201d should also be taken to include medium that is capable of tangibly storing or encoding instructions for execution by a computer system and that causes the computer system to perform any of the methods described herein.",{"@attributes":{"id":"p-0054","num":"0053"},"figref":"FIG. 5","b":["500","500","505","555","500","540","555","510","515","510","515","505","515","500","525","530","500","520","500","535","500","550","500","545"]},"The above description of illustrated embodiments of the invention, including what is described in the Abstract, is not intended to be exhaustive or to limit the invention to the precise forms disclosed. While specific embodiments of, and examples for, the invention are described herein for illustrative purposes, various equivalent modifications are possible within the scope of the invention, as those skilled in the relevant art will recognize.","These modifications can be made to the invention in light of the above detailed description. The terms used in the following claims should not be construed to limit the invention to the specific embodiments disclosed in the specification and the claims. Rather, the scope of the invention is to be determined entirely by the following claims, which are to be construed in accordance with established doctrines of claim interpretation."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The invention is illustrated by way of example and not by way of limitation in the figures of the accompanying drawings in which like references indicate similar elements. It should be noted that references to \u201can\u201d or \u201cone\u201d embodiment in this disclosure are not necessarily to the same embodiment, and such references mean at least one.",{"@attributes":{"id":"p-0007","num":"0006"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 5"}]},"DETDESC":[{},{}]}
