---
title: Method utilizing image correlation to determine position measurements in a machine vision system
abstract: A method utilizing image correlation to determine position measurements in a machine vision system. In a first operating state, the machine vision system utilizes traditional scale-based techniques to determine position measurements, while in a second operating state, image correlation displacement sensing techniques are utilized to determine position measurements. The image correlation techniques provide for higher accuracy for measuring distances between features that are separated by more than one field of view. The user may toggle between the operating states through a selection on the user interface, and guidance may be provided regarding when the image correlation mode is likely to provide higher accuracy, depending on factors such as the distance to be measured and the characteristics of the surface being measured.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09080855&OS=09080855&RS=09080855
owner: Mitutoyo Corporation
number: 09080855
owner_city: Kawasaki-Shi, Kanagawa-Ken
owner_country: JP
publication_date: 20110923
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT"],"p":["The invention relates generally to machine vision inspection systems, and more particularly to methods for determining position measurements in machine vision inspection systems.","Precision machine vision inspection systems (or \u201cvision systems\u201d for short) can be utilized to obtain precise dimensional measurements of inspected objects and to inspect various other object characteristics. Such systems may include a computer, a camera and optical system, and a precision stage that is movable in multiple directions to allow workpiece inspection. One exemplary prior art system, that can be characterized as a general-purpose \u201coff-line\u201d precision vision system is the commercially available QUICK VISION\u00ae series of PC-based vision systems and QVPAK\u00ae software available from Mitutoyo America Corporation (MAC), located in Aurora, Ill. The features and operation of the QUICK VISION\u00ae series of vision systems and the QVPAK\u00ae software are generally described, for example, in the 3, published January 2003, and the 3, published September 1996, each of which is hereby incorporated by reference in their entirety. This type of system is able to use a microscope-type optical system and move the stage so as to provide inspection images of either small or relatively large workpieces at various magnifications.","Machine vision inspection systems generally utilize automated video inspection. U.S. Pat. No. 6,542,180 (the '180 patent) teaches various aspects of such automated video inspection and is incorporated herein by reference in its entirety. As taught in the '180 patent, automated video inspection metrology instruments generally have a programming capability that allows an automatic inspection event sequence to be defined by the user for each particular workpiece configuration. This can be implemented by text-based programming, for example, or through a recording mode which progressively \u201clearns\u201d the inspection event sequence by storing a sequence of machine control instructions corresponding to a sequence of inspection operations performed by a user with the aid of a graphical user interface, or through a combination of both methods. Such a recording mode is often referred to as \u201clearn mode\u201d or \u201ctraining mode.\u201d Once the inspection event sequence is defined in \u201clearn mode,\u201d such a sequence can then be used to automatically acquire (and additionally analyze or inspect) images of a workpiece during \u201crun mode.\u201d","The machine control instructions including the specific inspection event sequence (i.e., how to acquire each image and how to analyze\/inspect each acquired image) are generally stored as a \u201cpart program\u201d or \u201cworkpiece program\u201d that is specific to the particular workpiece configuration. For example, a part program defines how to acquire each image, such as how to position the camera relative to the workpiece, at what lighting level, at what magnification level, etc. Further, the part program defines how to analyze\/inspect an acquired image, for example, by using one or more video tools such as edge\/boundary detection video tools.","Video tools (or \u201ctools\u201d for short) and other graphical user interface features may be used manually to accomplish manual inspection and\/or machine control operations (in \u201cmanual mode\u201d). Their set-up parameters and operation can also be recorded during learn mode, in order to create automatic inspection programs, or \u201cpart programs.\u201d Video tools may include, for example, edge\/boundary detection tools, autofocus tools, shape or pattern matching tools, dimension measuring tools, and the like.","Measurement accuracy and repeatability in the micron or submicron range are routinely obtained along the X and Y axes (that is, axes parallel to the plane of the inspection images used by precision machine vision inspection systems), in particular when the features being measured are within a single field of view. However, the level of accuracy for the measurement of the distance between features that are separated by more than one field of view tends to be lower than that for features within a single field of view. In particular, when features are being measured that are separated by more than one field of view, the overall distance measurement between the features is typically determined by the difference in the stage positions between the respective images that contain the features to be analyzed, plus the difference in the feature positions in the respective images. The stage positions are typically determined by position encoders (e.g., scale-based encoders), for which the potential position errors may be greater than what is desired for certain applications and\/or displacements. It would be desirable for a machine vision inspection system to operate with improved accuracy for measuring distances between features that are separated by more than one field of view.","This summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This summary is not intended to identify key features of the claimed subject matter, nor is it intended to be used as an aid in determining the scope of the claimed subject matter.","A precision machine vision inspection system for producing position measurements is provided. The machine vision inspection system comprises a stage that receives a workpiece, an imaging system that images the workpiece, a scale-based measurement portion that provides position measurements indicative of the stage position relative to the imaging system, a control system, a display and a user interface. In accordance with one aspect of the invention, the machine vision inspection system is configured to provide at least first and second operating states used to provide the position measurements. The first operating state comprises determining the position of an image based on a scale-based position measurement. The second operating state comprises providing the position of at least one second-state image based on enhanced second-state position measurement provided by using image correlation.","In accordance with another aspect of the invention, the second operating state using image correlation comprises referencing a first image captured at a first image position, the first image position characterized by a first position measurement. In many cases, the first position measurement may be a scale-based position measurement (e.g., as determined in the first operating state). In other cases, the first position measurement may be an enhanced second-state position measurement determined during a period of operation of the second operating state. In either case, the stage is moved from the first image position toward a second-state image position and overlapping images are captured between the first position and the second-state image position. The second-state image is then captured at the second-state image position. The enhanced second-state position measurement is then provided based on the first position measurement and a set of image displacements determined based on using image correlation applied to the first image, the overlapping images, and the second-state image.","In accordance with another aspect of the invention, The user interface is displayed on the display at least during a learn mode of operation of the machine vision inspection system, and the user interface comprises a second-state indicator that indicates when the second operating state that provides the enhanced second-state position measurement is active. In some embodiments, the user interface comprises a second-state activation element that may be operated by a user to activate and\/or deactivate the second operating state, such that the user may control the machine vision system to operate in the first operating state at a first time during the learn mode of operation and in the second operating state at a second time during the learn mode of operation. In some embodiments, the second-state activation element and the second-state indicator are provided by a single user interface element. In some embodiments, when a second-state activation element is operated by a user to start the second operating state, the control system automatically defines the image position of a current image as the first image position. In various embodiments, the enhanced second-state position measurement provided in the second operating state may be used in conjunction with a dimensional measuring video tool, that allows a user to select a second feature in the second-state image for performing a distance measurement between the selected second feature in the second-state image and a first feature in the first image.","The machine vision inspection system may comprise a motion control element operated by a user to define the stage position relative to the imaging system during the learn mode of operation. In some embodiments, in the second operating state moving from the first image position toward the second-state image position may comprise the user using the motion control element to define the nominal second-state image position; and the control system may automatically determine the spacing between the overlapping images along a motion path between the first image position and the second-state image position. In some embodiments, the user may further use the motion control element to define intermediate positions that define the motion path between the first image position and the second-state image position","In some embodiments, the machine vision inspection system comprises an enhanced position measurement limit parameter which is indicative of a maximum recommended displacement limit referenced to the first image, for using the second operating state to provide an enhanced second state position measurement. The user interface may comprise an enhanced position measurement displacement limit status indicator which indicates at least one of (a) a relationship between a current displacement referenced to the first image and the maximum recommended displacement limit, and (b) a warning when a current displacement referenced to the first image is greater than the maximum recommended displacement limit. In some embodiments, the enhanced position measurement displacement limit status indicator and the second-state indicator (e.g., as outlined above) are provided by a single user interface element. In one specific example embodiment, the enhanced position measurement displacement limit may correspond to a defined number times a dimension of a field of view (e.g., 40 times a FOV dimension). In some embodiments, the machine vision inspection system comprises a plurality of enhanced position measurement limit parameters which correspond to a plurality of respective optical configurations of the machine vision inspection system.","In some embodiments, during the learn mode of operations the second operating state comprises evaluating an image correlation quality metric for image correlations performed to determine the set of image displacements during the second operating state, and the user interface comprises an indicator of poor image correlation that is displayed based on the image correlation quality metric (e.g., a warning is displayed when the image correlation quality metric indicates poor image quality and\/or low correlation quality).","In some embodiments, the user interface further comprises an element operated by a user during the learn mode of operation to define a distance measurement between a first feature in the first image and a second feature in the second-state image, wherein the distance measurement comprises determining a difference between the first position measurement and the second-state position measurement plus a difference of the respective feature positions relative to their respective images.","In some embodiments, when the second operating state using image correlation is active, the scale-based measurement portion is also used for indicating approximate scale-based image positions for at least one of (a) the overlapping images and (b) the second state image, such that a pixel offset search range for a correlation algorithm used in the second state is defined based on the approximate scale-based image positions. For example, the approximate scale-based image positions provided by the scale-based measurement portion allow the pixel offset search range for an image correlation algorithm to be safely limited to fewer pixels than would be required if the scale-based measurement portion were not utilized, which speeds up the image correlation calculations.","In accordance with another aspect of the invention, when the second operating state using image correlation is active, an error checking process is performed which comprises comparing the distance indicated by the image correlation to that indicated by the scale-based measurement portion. In one embodiment, if the difference between the distance indicated by the image correlation is different from that indicated by the scale-based measurement portion by an amount that is greater than an expected error range for the scale-based measurement portion with a defined safety margin, then a warning is provided to the user, and\/or the condition is logged, and\/or the measurement provided by the scale-based measurement portion is indicated as the current measurement.",{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 1","b":["10","10","12","14","14","16","18","22","24","26","16","10"]},"The vision measuring machine  includes a moveable workpiece stage  and an optical imaging system  which may include a zoom lens or interchangeable lenses. The zoom lens or interchangeable lenses generally provide various magnifications for the images provided by the optical imaging system . The machine vision inspection system  is generally comparable to the QUICK VISION\u00ae series of vision systems and the QVPAK\u00ae software discussed above, and similar state-of-the-art commercially available precision machine vision inspection systems. The machine vision inspection system  is also described in commonly assigned U.S. Pat. Nos. 7,454,053, and 7,324,682, U.S. patent application Ser. No. 12\/343,383, filed Dec. 23, 2008, and Ser. No. 12\/608,943, filed Oct. 29, 2009, which are each incorporated herein by reference in their entireties.",{"@attributes":{"id":"p-0029","num":"0028"},"figref":["FIG. 2","FIG. 1"],"b":["120","200","100","120","200","200","205","220","230","230","240","210","212","210","20","205","260","250","280","286","288"]},"The optical assembly portion  is controllably movable along a Z-axis that is generally orthogonal to the X and Y axes, by using a controllable motor  that drives an actuator to move the optical assembly portion  along the Z-axis to change the focus of the image of the workpiece . The controllable motor  is connected to the input\/output interface  via a signal line .","A workpiece , or a tray or fixture holding a plurality of workpieces , which is to be imaged using the machine vision inspection system  is placed on the workpiece stage . The workpiece stage  may be controlled to move relative to the optical assembly portion , such that the interchangeable objective lens  moves between locations on a workpiece , and\/or among a plurality of workpieces . One or more of a stage light , a coaxial light , and a surface light  (e.g., a ring light) may emit source light , , and\/or , respectively, to illuminate the workpiece or workpieces . The light source  may emit light  along a path including a mirror . The source light is reflected or transmitted as workpiece light , and the workpiece light used for imaging passes through the interchangeable objective lens  the turret lens assembly  and is gathered by the camera system . The image of the workpiece(s) , captured by the camera system , is output on a signal line  to the control system portion . The light sources , , and  may be connected to the control system portion  through signal lines or busses , , and , respectively. To alter the image magnification, the control system portion  may rotate the turret lens assembly  along axis  to select a turret lens, through a signal line or bus .","As shown in , in various exemplary embodiments, the control system portion  includes a controller , the input\/output interface , a memory , a workpiece program generator and executor , and a power supply portion . Each of these components, as well as the additional components described below, may be interconnected by one or more data\/control buses and\/or application programming interfaces, or by direct connections between the various elements.","The input\/output interface  includes an imaging control interface , a motion control interface , a lighting control interface , and a lens control interface . The motion control interface  may include a position control element , and a speed\/acceleration control element although such elements may be merged and\/or indistinguishable. The lighting control interface  includes lighting control elements -and which control, for example, the selection, power, on\/off switch, and strobe pulse timing, if applicable, for the various corresponding light sources of the machine vision inspection system .","The memory  may include an image file memory portion , an enhanced limited range position memory portion , described in greater detail below, a workpiece program memory portion  that may include one or more part programs, or the like, and a video tool portion . The video tool portion  includes video tool portion and other video tool portions (e.g., ), which determine the GUI, image processing operation, etc., for each of the corresponding video tools, and a region of interest (ROI) generator that supports automatic, semi-automatic and\/or manual operations that define various ROIs that are operable in various video tools included in the video tool portion .","In the context of this description, and as known by one of ordinary skill in the art, the term video tool generally refers to a relatively complex set of automatic or programmed operations that a machine vision user can implement through a relatively simple user interface (e.g., a graphical user interface, editable parameter windows, menus, and the like), without creating the step-by-step sequence of operations included in the video tool or resorting to a generalized text-based programming language, or the like. For example, a video tool may include a complex pre-programmed set of image processing operations and computations which are applied and customized in a particular instance by adjusting a few variables or parameters that govern the operations and computations. In addition to the underlying operations and computations, the video tool comprises the user interface that allows the user to adjust those parameters for a particular instance of the video tool. For example, many machine vision video tools allow a user to configure a graphical region of interest (ROI) indicator through simple \u201chandle dragging\u201d operations using a mouse, in order to define the location parameters of a subset of an image that is to be analyzed by the image procession operations of a particular instance of a video tool. It should be noted that the visible user interface features are sometimes referred to as the video tool, with the underlying operations being included implicitly.","In common with many video tools, the enhanced limited range position subject matter described herein includes both user interface features and underlying image processing operations, and the like, and the related features may be characterized as features of an enhanced limited range position mode included in the video tool portion . The majority of video tools are implemented for a particular instance of analysis in relation to a particular feature or region of interest, perform their function, and then cease operation. In contrast, it will be appreciated that in some embodiments the enhanced limited range position mode features disclosed herein may be applied globally to enhanced limited range position determinations, and may generally persist and continue to operate, until they are explicitly terminated by a user. While a user may experience the features of the enhanced limited range position mode , described below primarily as an operating mode, alternative implementations and\/or user interface features may also be provided (e.g., an enhanced limited range distance measuring video tool, etc.). Thus, it should be appreciated that characterizing the enhanced limited range position subject matter of this description as an operating mode in the following description is a matter of choice for description, and it is not intended to be limiting with regard to its appearance to the user, or its manner of implementation. One of ordinary skill in the art will appreciate that the circuits and routines underlying the enhanced limited range position features disclosed herein may implemented as distinct elements, in some embodiments.","Briefly, as will be described in more detail below, in one embodiment the enhanced limited range position mode may be selected by a user to enhance the accuracy of certain short range measurements in a machine vision system . In certain implementations, the enhanced limited range position mode may be an alternative to a more typical measurement mode, in which a scale-based measurement portion (e.g., an encoder) is utilized for the position measurements. In contrast, in the enhanced limited range position mode , image correlation techniques may be utilized as part of the measurement process (e.g., to determine a second image position relative to a first image position), in such a way as to increase the accuracy of the measurements, in particular for distances between features that are separated by more than one field of view.","In one embodiment, the enhanced limited range position mode may include a portion that provides enhanced limited range position operations\/mode control , with a portion for enhancement range parameters and operations and a portion that provides an enhanced limited range user interface . Features and operations associated with these elements are described in greater detail below. Briefly, the enhanced limited range position operations\/mode control may perform operations (e.g., image analysis operations, memory management, etc.), to configure and support operation of the enhanced limited range position mode as described in greater detail below. In one embodiment, the enhanced limited range position mode may also be linked or otherwise act in conjunction with certain known position measurement operations or tools.","Alternative configurations are possible for the enhanced limited range position mode . In general, it will be appreciated that the enhanced limited range position techniques described herein may be implemented in any now known or later-developed form that is operable in conjunction with the machine vision inspection system  to provide the features disclosed herein in relation to the measurement operations.","In general, the memory portion  stores data usable to operate the vision system components portion  to capture or acquire an image of the workpiece  such that the acquired image of the workpiece  has desired image characteristics. The enhanced limited range position memory portion may be controlled by the enhanced limited range position operations\/mode control to store and\/or recall the various data used by the enhanced limited range position mode . The memory portion  may also contain data defining a graphical user interface operable through the input\/output interface . The memory portion  may also store inspection result data, may further store data usable to operate the machine vision inspection system  to perform various inspection and measurement operations on the acquired images (e.g., implemented, in part, as video tools), either manually or automatically, and to output the results through the input\/output interface .","The signal lines or busses , , and  of the stage light , the coaxial lights  and \u2032, and the surface light , respectively, are all connected to the input\/output interface . The signal line  from the camera system  and the signal line  from the controllable motor  are connected to the input\/output interface . In addition to carrying image data, the signal line  may carry a signal from the controller  that initiates image acquisition.","One or more display devices  (e.g., the display  of ) and one or more input devices  (e.g., the joystick , keyboard , and mouse  of ) can also be connected to the input\/output interface . The display devices  and input devices  can be used to display a user interface, which may include various graphical user interface (GUI) features that are usable to perform inspection operations, and\/or to create and\/or modify part programs, to view the images captured by the camera system , and\/or to directly control the vision system components portion . The display devices  may display user interface features associated with the enhanced limited range position user interface , described in greater detail below.","In various exemplary embodiments, when a user utilizes the machine vision inspection system  to create a part program for the workpiece , the user generates part program instructions by operating the machine vision inspection system  in a learn mode to provide a desired image acquisition training sequence. For example a training sequence may comprise positioning a particular workpiece feature of a representative workpiece in the field of view (FOV), setting light levels, focusing or autofocusing, acquiring an image, and providing an inspection training sequence applied to the image (e.g., using an instance of one of the video tools on that workpiece feature). The learn mode operates such that the sequence(s) are captured or recorded and converted to corresponding part program instructions. These instructions, when the part program is executed, will cause the machine vision inspection system to reproduce the trained image acquisition and inspection operations to automatically inspect that particular workpiece feature (that is, the corresponding feature in the corresponding location) on a run mode workpiece or workpieces which matches the representative workpiece used when creating the part program.","These analysis and inspection methods that are used to inspect features in a workpiece image are typically embodied in the various video tools (e.g., video tools , , etc.) included in the video tool portion  of the memory , as outlined above. Many known video tools, or \u201ctools\u201d for short, are included in commercially available machine vision inspection systems, such as the QUICK VISION\u00ae series of vision systems and the associated QVPAK\u00ae software, discussed above.","As noted above, the enhanced limited range position mode is distinct from the more traditional scale-based measurement mode of a typical machine vision inspection system. In a typical machine vision inspection system, a measurement of the distance between two features that are in different fields of view is traditionally determined by the difference in the stage positions plus the difference in the feature positions in the respective images, wherein the stage (i.e., image) positions are traditionally determined by position encoders (e.g., scale-based encoders). In contrast, in the enhanced limited range position mode , while the feature positions in the respective images may still be determined in the same manner, the difference in the stage (i.e., image) positions is alternatively determined through image correlation techniques, in accordance with the teachings herein. In one embodiment, for the purpose of determining the difference in the stage positions, the machine vision inspection system is essentially operated as a pseudo correlation encoder, although with certain differences from traditional correlation encoders, as will be described in more detail below.","As will be described in more detail below with respect to , various known traditional correlation encoders use images acquired by a sensor array, and correlation between images acquired by the sensor array, to determine deformations and\/or displacements of an object. For example, one class of such devices is described in U.S. Pat. No. 6,873,422 (the '422 patent), U.S. Pat. No. 6,990,254 (the '254 patent), U.S. Pat. No. 6,996,291 (the '291 patent), and U.S. Pat. No. 7,065,258 (the '258 patent), all to Nahum, each of which is hereby incorporated by reference in its entirety. In general, in such devices, prior to displacing or deforming the object, a first or reference image arising from the object is captured and stored. Then, after displacing or deforming the object, a second or subsequent image arising from the object is captured and stored. The first and second images are then quantitatively compared, e.g., by correlation operations, on a pixel-by-pixel basis. In general, a plurality of respective comparisons are performed with the first and second images offset, or spatially translated, relative to each other by different respective amounts (e.g., by varying the offset in one pixel increments between the various comparisons). Then the resulting quantitative comparison, such as a correlation function value, is plotted against its corresponding offset amount, or spatial translation position, to determine a correlation function value point. The offsets having the strongest correlations between the second and first images will generate a peak or a trough (depending on how the pixel-by-pixel comparison is performed) in the plot of correlation function value points. The offset amount corresponding to the peak or trough represents the amount of displacement or deformation between the first and second images.",{"@attributes":{"id":"p-0047","num":"0046"},"figref":["FIG. 3","FIG. 3","FIG. 3"],"b":["300","305","304","301","302","304","307","306","302","301","303","305","305","302","303","303","305"]},"The '422 patent describes how a sub-pixel error that is spatially periodic at the pixel pitch of the sensor array may arise in the correlation peak location estimates provided by various methods that use curve fitting. The '422 patent teaches various methods for reducing such errors when estimating a correlation peak location. However, some level of periodic sub-pixel error may remain after the methods of the '422 patent are applied. U.S. Pat. No. 7,085,431 to Jones (the '431 patent), which is hereby incorporated herein by reference in its entirety, teaches a method wherein the previously indicated periodic sub-pixel errors, and other errors, are characterized and compensated. U.S. Pat. No. 7,885,480 to Bryll et al. (the '480 patent), which is hereby incorporated herein by reference in its entirety, additionally teaches a method wherein systematic sub-pixel errors, and other errors, are characterized and compensated. It will be appreciated that the sub-pixel accuracy achieved by such techniques may correspond to accuracy and\/or resolution on the order of 10 nm or less in various applications.","As will be described in more detail below with respect to , the image correlation techniques of the present invention differ from a traditional correlation encoder, in that rather than determining deformations or displacements of an object, a distance between two different features in two different images is being determined. Furthermore, in one embodiment, through the use of the image correlation techniques in combination with the more traditional scale-based measurements, the search range for the correlation algorithm may be made to only require a few pixels. In other words, because the scale-based portion may provide a relatively accurate indication of how far and in what direction the stage has moved, the pixel offset search range for a correlation algorithm may be defined based on the approximate scale-based image positions, and the search range (XY) for the correlation algorithm may only need to be directed to relatively few pixels in order to determine the relevant correlation peak or valley between the common pixels of a prior image and a current correlation image.",{"@attributes":{"id":"p-0050","num":"0049"},"figref":["FIG. 4","FIG. 4"],"b":["400","2","2","1","1","1","1","2","2","1","1","1","1","2","2","2","2","1","2","1","2"],"sub":"FE "},"As shown in , a set of overlapping images CIA, CIB, and CIC are captured as the stage is moved from the position of the first image F toward the position of the second-state image F. The location of each of the images F, CIA, CIB, CIC, and F is arbitrarily designated as being referenced by the pixel location in the upper right corner of each of the images. In an alternative embodiment, the movement from the first image F to the second-state image F may be done in a straight line, rather than as a series of different angled line segments, so as to simplify the image correlation processing and calculations.","Each of the overlapping images CIA, CIB, and CIC is shown to include an overlap portion which is sufficient for the search range processing of the correlation algorithm. More specifically, the first image F and the overlapping image CIA are shown to include a common overlap portion OV, while the overlapping images CIA and CIB have a common overlap portion OV, and the overlapping images CIB and CIC have a common overlap portion OV, and the overlapping image CIC and the second-state image F have a common overlap portion OV.","It will be appreciated that in contrast to more traditional image correlation encoder techniques wherein a majority of the pixels in an image may be required for a desired level of accuracy of the image correlation function, in accordance with the techniques described herein, the search range (XY) for the correlation algorithm can be restricted to relatively few pixels from the respective overlap portions. In one embodiment, this is because the scale-based measurement portion of the machine vision inspection system provides a relatively accurate indication of the approximate distance and direction of the movement of the stage, such that only a few pixels are required for the search range of the correlation algorithm to accurately determine the position of the subsequent image. The end result is that the coordinates of the second-state image F may be determined with reference to the coordinates of the first image F through this image correlation process, from which the distance Dbetween the images F and F can be determined. As described above, the total distance Dbetween the features FE and FE can then be determined as the distance Dplus the difference in the feature positions within their respective images, as indicated by the internal coordinates X(I), Y(I), and X(I), Y(I), as described above.","It will be appreciated that the enhanced accuracy of the determination of the distance Dmay be useful for various types of applications. As an example, in one specific implementation, the first feature FE may correspond to a first edge of an object that is to be measured. The object to be measured may have an approximate width (e.g., 5 microns), with a second edge (not shown) which is not easily accessible (e.g., such as being obscured under a covering substance). However, the second obscured edge may be fabricated so as to be a precise known distance Dfrom a remote fiducial, such as the second feature FE (which for various reasons may be located more than one field of view away from the object). Since the object has relatively small dimensions (e.g., 5 microns across), it may be desirable for the measurement of the distance between the first and second edges to be relatively precise (e.g., desired accuracy of 0.1 to 0.3 microns). By measuring the position of the first edge (e.g., as represented by the first feature FE) and then utilizing the image correlation techniques to measure the relative position of the remote fiducial (e.g., the second feature FE), the width of the object may be determined with a high degree of accuracy as being the difference between the distance Dand the distance D.",{"@attributes":{"id":"p-0055","num":"0054"},"figref":"FIG. 5","b":["500","500","510","510","510"]},"In contrast, as will be described in more detail below, the image correlation techniques of the present invention may provide higher measurement accuracy over short ranges. As shown in , an error line  indicates an expected error range for a second system utilizing the image correlation techniques of the present invention. It will be understood that the error line  applies nominally to an accumulated displacement (e.g., a displacement that totals all included displacement steps as absolute displacements, regardless of their direction) along an axis. In this specific example embodiment, the estimated correlation error indicated by the error line  is of the form SREc+Kc*(L)^0.5. More specifically, this approximating formula (which it will be appreciated is a simplification of a complex system) includes the short range correlation error SREc (e.g., 0.1 microns in this example) plus the long range correlation error coefficient Kc (e.g., 0.2 in this example) times the square root of the actual position change (L) expressed in millimeters, where the result is expressed in microns. The estimated correlation error is applicable when the first and second features are located more than one field of view apart (i.e., if the first and second features are within the same field of view, then the estimated correlation error need not apply because the stage need not be moved to make a distance measurement within the field of view).","In one specific example embodiment, this estimated correlation error may correspond to the following types of considerations, which may be supplemented by information gained from analysis or experiment. From statistical principles, random correlation errors may accumulate approximately in proportion to the square root of the number of correlations included in a displacement estimate, which accounts for the square root of the distance factor L appearing in the approximation of errors. For an approximation of accuracy, assuming 10 micron pixels on VGA (640\u00d7480) CCD, and 10\u00d7 magnification, based on experimental results and\/or analysis, an expected error based on image correlation may be on the order of SREc=100 nm of error per instance of image correlation (e.g., per image compared to the previous image), assuming a target with reasonable texture along the correlation image path. This accounts for the value of the short range correlation error SREc in this particular example. It may be desirable to limit the moves between correlation to something on the order of one half to one quarter of the field of view, to ensure a good amount of image area for correlation between images. With field of view size at 10\u00d7 being approximately 0.64\u00d70.48 mm for the CCD array outlined above, it may be desirable to limit the move between correlation images to approximately DM=0.25 mm. Using these values, we may approximate correlation errors as 0.1+0.1(L\/M)^0.5, which may alternatively be expressed in terms of the formula outlined above with SREc=0.1 and Kc=(SREc\/(DM^0.5))=(0.1\/0.5)=0.2. The estimated error line  corresponds to these values. In various embodiments, increasing the magnification may decrease the short range error. It may also affect the long range error coefficient in that the error per correlation will become smaller and M (the desirable move between correlation images) will also become smaller. Of course, decreasing the magnification will have the opposite effect. For example, the estimated error line \u2032 corresponds to a higher magnification system, which is characterized by the values SREc\u2032=0.05 and Kc\u2032=0.14.","It will be appreciated that, as indicated by , for certain implementations the image correlation techniques of the present invention may provide for higher accuracy than the previously described scale-based measurement techniques for measurement, within a certain range wherein the estimated position error for the image correlation techniques (e.g., as indicated by the lines  and \u2032) is less than the estimated position error for the scale-based techniques (e.g., as indicated by the line ). We may refer to this range as an enhanced position measurement limit or range  (for the estimated error line ) or \u2032 (for the estimated error line \u2032). As shown in , an enhanced position measurement limit line  illustrates a limit of the first desirable enhanced position measurement limit or range  which is indicative of a maximum recommended displacement limit for using the second operating state to provide an enhanced second state position measurement (e.g., a range where the image correlation technique may be preferred over the more traditional scale-based measurement technique, for a corresponding first optical system, due to its potential for providing lower errors). An enhanced position measurement limit line \u2032 illustrates a limit of the second desirable enhanced position measurement limit or range \u2032 in which the image correlation technique may be preferred over the more traditional scale-based measurement technique, for a corresponding second optical system. As previously outlined, the error line  and an enhanced position measurement limit or range apply based thereon, apply nominally to an accumulated displacement referenced to the location of a first correlation image (e.g., a displacement that totals all included displacement steps as absolute displacements, regardless of their direction) along an axis. However, in some embodiments, it may be quite unlikely that accumulated displacements will include a reversed direction, or significant deviations from a straight line, and an enhanced position measurement limit or range may be implemented as a simple distance limit relative to a first correlation limit in such embodiments.","In some embodiments, an enhanced position measurement limit or range may be indicated by an enhanced position measurement limit parameter included in a machine vision control system. The enhanced position measurement limit parameter is indicative of a maximum recommended displacement limit referenced to the first image, for using the second operating state to provide an enhanced second state position measurement. In one specific example implementation, the enhanced position measurement limit parameter corresponds to a maximum recommended displacement limit that is a specific number times a dimension of a field of view of the machine vision inspection system (e.g., 40 times a FOV dimension), since the image correlation technique has an error which depends partially on the accumulated number of images correlated. However, the enhancement range for any particular optical system may be expressed in absolute displacement terms, or any other convenient form, if desired.","As outlined above, while the scale-based measurement error estimate may be constant regardless of the magnification of an optical configuration, the image correlation measurement error estimate will generally depend on a particular optical configuration. Therefore, in general, the error relationships illustrated in , may be different for different optical configurations (e.g., different magnifications, or the like). It will be appreciated that the enhanced position measurement limit or range may be set more conservatively than the \u201cequivalent error\u201d limit or range shown in , such that an image correlation measurement is relatively certain to provide lower error than a scale-based measurement within that limit or range.","In various embodiments, error estimates and desirable enhanced position measurement limits or ranges, analogous to those illustrated in , may be determined for each desired optical configuration used in a machine vision system, either by experiment or analysis. It will be appreciated that a machine vision inspection system may comprise a plurality of enhanced position measurement limit parameters which correspond to a plurality of respective optical configurations of the machine vision inspection system. The desirable enhanced position measurement limits ranges, and\/or related relationships to a present displacement or position, may be displayed or otherwise used to guide a user in choosing when to use the image correlation enhanced measurement techniques disclosed herein for an enhanced relative position measurement. For example, if a displacement or relative position falls within an enhanced position measurement range, determined as outlined above, and the scale-based error estimate is larger than a desired error limit, then the image correlation enhanced position measurement technique is preferred. Conversely, if the displacement or relative position falls outside an enhanced position measurement range determined as outlined above, then this may be indicated to the user, and\/or the image correlation enhanced measurement techniques should not be used. Related user interface features and operations are outlined further below. Furthermore, if the scale-based error estimate is within a desired error limit, then the scale-based measurement may be preferred in some embodiments (even if it produces larger, but acceptable, errors), since it is a faster mode of position measurement.",{"@attributes":{"id":"p-0062","num":"0061"},"figref":"FIG. 6","b":["600","610"]},"At a decision block , a determination is made as to whether the system is in the second operating state rather than the first operating state (e.g., a second operating state provided by an enhanced limited range position mode of , employing principles outlined in relation to , , and ). In some embodiments, the system may be placed in the second operating state by the user operating a second-state activation element included in the user interface during learn mode operations. In some embodiments, the second operating state may be automatically implemented for certain tool conditions and\/or measurement situations (e.g., when a small tolerance is set for distance measurements between closely spaced features). In some embodiments, the user may terminate such automatic implementations and place the machine in a first operating state by using a second state activation or deactivation element (e.g., an activation element may toggle between the first and second states). During run mode, the first and second operating states are governed by the part program (e.g., as recorded during the learn mode of operations and\/or as hand-written by an expert user). In any case, if the system is in the second operating state, then the routine continues to a block , where the position of at least one second-state image is provided based on enhanced second-state position measurement provided by using image correlation. One embodiment of a more detailed implementation of this function at block  will be described in more detail below with respect to .","If at the decision block  it is determined that the system is in the first operating state rather than the second operating state, then the position of an image is determined based on a scale-based position measurement provided by the more standard scale-based measurement portion. As outlined above, in some embodiments, the machine vision inspection system may comprise an enhanced position measurement limit parameter which is indicative of a maximum recommended displacement limit referenced to a first image, for using the second operating state to provide an enhanced second state position measurement. If the location of the first image is defined, then in some embodiments the system may automatically revert to the first operating state for any image location outside the maximum recommended displacement limit.",{"@attributes":{"id":"p-0065","num":"0064"},"figref":["FIG. 7","FIG. 6","FIG. 4","FIG. 7","FIG. 4","FIG. 8"],"b":["700","630","710","1"]},"At a block , the system is moved toward a second-state image position and overlapping images (e.g., overlapping images CIA, CIB, and CIC of ) are captured between the first position and the second-state image position. In some instances, a motion control element of the system may be operated by a user during the learn mode of operation, to move from the first image position toward the second-state image position and define a nominal second-state image position (e.g., by stopping at that position). In some instances, the user may define intermediate positions that define the motion path between the first image position and the second-state image position (e.g., using a \u201cMove To\u201d command, or the like), in order to navigate around height changes or poor correlation image locations (e.g., large holes). In some embodiments, during the learn mode of operations, the second operating state comprises evaluating an image correlation quality metric for image correlations performed to determine a set of image displacements during the second operating state, and the user interface comprises an indicator of image correlation quality that is displayed based on the image correlation quality metric. In such embodiments, the user may navigate or edit the selection of intermediate positions based on the indicated image correlation quality. In some instances, the user may indicate a nominal second-state image position based on CAD data, and\/or text entry, or the like. Movement to the nominal second-state image position may be based on using the scale-based measurement portion for motion control, while using image correlation to determine the measurement positions associated with the overlapping correlation images and the second-state image, as outlined above. In some embodiments, the control system may automatically determine the spacing between the overlapping images (e.g., one quarter or one half of a dimension of a field of view, or the like) along a motion path between the first image position and the second-state image position.","At a block , the second-state image (e.g., second-state image F of ) is captured at the second-state image position. At a block , the enhanced second-state position measurement is provided based on the first position measurement and a set of image displacements determined based on using image correlation applied to the first image, the overlapping images, and the second-state image.",{"@attributes":{"id":"p-0068","num":"0067"},"figref":["FIG. 8","FIG. 8","FIG. 8","FIG. 8"],"b":["800","800","803","810","800","820","840","830","850","860","861","860","862","861","830","861","861","861","861"]},"In one embodiment, when the enhanced position measurement state is active, the user interface may automatically display an enhanced position measurement mode dialog box , for displaying and configuring the various parameters of the selected enhanced position mode. As shown in the enhanced position measurement mode parameter dialog box , a first image position box  may indicate the X-Y-Z coordinates of the first image that is referenced during the enhanced position measurement state. An enhanced position measurement limit parameter box  may display a value which is indicative of a maximum recommended displacement limit referenced to the first image, for using the enhanced position measurement operating state to provide an enhanced position measurement for a current optical configuration.","In various embodiments, the user interface may comprise an element operated by a user during the learn mode of operation (e.g., the distance measurement tool activated by distance tool button ) to define a distance measurement between a first feature in the first image and a second feature in a second image acquired during the enhanced position measurement operating state. When the enhanced position measurement is used for the second image, the distance measurement comprises determining a difference between the first position measurement (regardless of whether it is a scale-based position measurement or an enhanced position measurement) and the enhanced position measurement of the second image, plus a difference of the respective feature positions relative to their respective images.","It will be appreciated that not all of the user interface elements outlined herein need to be used in all embodiments. Various features of the elements may be combined or omitted in various embodiments. Other alternative user interface embodiments will be apparent to one of ordinary skill in the art, based on the teachings of this disclosure.","As an example of the operation of the user interface display , in one specific example implementation, the user may initially capture a workpiece image  in the field of view window  and make a position measurement of a first feature in the image (e.g., using a known edge detection video tool). The user may then select an enhanced position mode button  to activate the enhanced position measurement operating state, at which point the system records the XYZcoordinates of the first image (i.e., the stage position), such as may be displayed in the enhanced position mode parameter dialog box . The user then manually moves the stage in a direction toward a second image position, and during the movement overlapping images are captured according to previously outlined principles. Once the desired second image position is reached, the user stops moving the stage and captures the second image. The system records the XYZlocation of the second image as being the XYZcoordinates of the first image plus the offset as determined by image correlation between the first image, the overlapping images, and the second image. In the second image, the user may make a position measurement of a second feature (e.g., using a known edge detection video tool). The user may also implement a distance measurement tool using the distance tool button , and determine the distance between the first and second feature, as outlined above. If the system is in a learn mode, the system records the relevant instructions, such as the sequence for capturing the first image, activating the enhanced position measurement operating state, recording the relative coordinates, capturing overlapping images during movement to the second image position, capturing the second image and recording the enhanced position measurement coordinates of the second image (e.g., relative to the coordinates of the first image), and any other parameters that are needed, and so on. The user may deactivate the enhanced position measurement operating state (thus returning to the default scale-based position measurement state) at any convenient time. During a run mode, the instructions recorded in the part program recreate the operations that were performed by the user during the learn mode, including the transitions to and from the enhanced position measurement operating state.","In some embodiments, when the enhanced position measurement operating state is active, an error checking process is performed which comprises comparing the position indicated by the enhanced position measurement based on image correlation to that indicated by the scale-based measurement portion. In one embodiment, if the difference between the distance indicated by the enhanced position measurement is different from that indicated by the scale-based measurement portion by an amount that is greater than an expected error range for the scale-based measurement portion (possibly including a defined safety margin), then a warning is provided to the user, and\/or the condition is logged, and\/or the measurement provided by the scale-based measurement portion is indicated as the current measurement. This reduces the risks of using the enhanced position measurement state.","While the preferred embodiment of the invention has been illustrated and described, numerous variations in the illustrated and described arrangements of features and sequences of operations will be apparent to one skilled in the art based on this disclosure. Thus, it will be appreciated that various changes can be made therein without departing from the spirit and scope of the invention."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The foregoing aspects and many of the attendant advantages of this invention will become more readily appreciated as the same become better understood by reference to the following detailed description, when taken in conjunction with the accompanying drawings, wherein:",{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":["FIG. 2","FIG. 1"]},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":["FIG. 7","FIG. 6"]},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 8"}]},"DETDESC":[{},{}]}
