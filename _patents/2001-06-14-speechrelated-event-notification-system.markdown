---
title: Speech-related event notification system
abstract: The present invention is directed to a system and method of notifying a speech related application of events generated by a speech related engine. A middleware layer receives a notification selection from the application. The notification selection is indicative of a selected notification mechanism for notifying the application of the events. The middleware component receives an event indication from the engine. The event indication is indicative of an event generated by the engine. The event indication is transferred to the application according to the selected notification mechanism.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=06931376&OS=06931376&RS=06931376
owner: Microsoft Corporation
number: 06931376
owner_city: Redmond
owner_country: US
publication_date: 20010614
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"p":["The present application is based on and claims the benefit of U.S. provisional patent application Ser. No. 60\/219,861, filed Jul. 20, 2000, the content of which is hereby incorporated by reference in its entirety.","The following Patent Application is hereby fully incorporated by reference, and priority is claimed therefrom; MIDDLEWARE LAYER BETWEEN SPEECH RELATED APPLICATIONS AND ENGINES, filed on Dec. 29, 2000, Ser. No. 09\/751,836.","The Present invention deals with services for enabling speech recognition and speech synthesis technology. In particular, the present invention relates to an event notification system in a middleware layer which lies between speech related applications and speech related engines.","Speech synthesis engines typically include a decoder which receives textual information and converts it to audio information which can be synthesized into speech on an audio device. Speech recognition engines typically include a decoder which receives audio information in the form of a speech signal and identifies a sequence of words from the speech signal.","In the past, applications which invoked these engines communicated directly with the engines. Because the engines from each vendor interacted with applications directly, the behavior of that interaction was unpredictable and inconsistent. This made it virtually impossible to change synthesis or recognition engines without inducing errors in the application. It is believed that, because of these difficulties, speech recognition technology and speech synthesis technology have not quickly gained wide acceptance.","In an effort to make such technology more readily available, an interface between engines and applications was specified by a set of application programming interfaces (API's) referred to as the Microsoft Speech API version 4.0 (SAPI4). Though the set of API's in SAPI4 specified direct interaction between applications and engines, and although this was a significant step forward in making speech recognition and speech synthesis technology more widely available, some of these API's were cumbersome to use, required the application to be apartment threaded, and did not support all languages.","The process of making speech recognition and speech synthesis more widely available has encountered other obstacles as well. For example, the vendors of applications and engines have been required to write an enormous amount of code simply to implement the different interfaces for the different applications and engines that can be used together. In such systems, event notification is very cumbersome. The engines are required to notify the applications directly of events, such as word boundaries, visemes, bookmarks, etc. This has required engines to know exactly how the application wished to be notified of such events. Similarly, output devices (such as audio devices in a text-to-speech system) have also been required to know when events are occurring and how an application wishes to be notified of the events. Since applications traditionally can be notified of events in one of a number of different ways, this required specific code to be written to interface to specific applications.","The present invention is directed to a system and method of notifying a speech related application of events generated by a speech related engine. A middleware layer receives a notification selection from the application. The notification selection is indicative of a selected notification mechanism for notifying the application of the events. The middleware component receives an event indication from the engine. The event indication is indicative of an event generated by the engine. The event indication is transferred to the application according to the selected notification mechanism.","In one embodiment, the event indication is first transferred to an output device, from the middleware component, which notifies the middleware component when it has reached the event in an output data stream. The middleware component then retrieves the event indication from the output device and transmits it to the application.","In another embodiment, prior to transferring the event indication to the application, the middleware component receives an interest indication from the application indicative of events that the application is interested in. When the middleware component receives an event notification from the output device, it first determines whether the application is interested in the event by comparing the event to the interest indication received from the application.","In another embodiment, the middleware component initializes a notification assistance component based on the notification selection made by the application. The notification assistance component notifies the application of the event according to the selected notification mechanism.","The present invention can also be embodied as a method and apparatus for synchronizing a speech related output with processing within an application. A speech related data stream to be processed is received at a middleware component between the engine and the application. The speech related data stream is transferred from the application to the speech related engine and is processed by the engine. Event indications from the engine are received at the middleware component. The event indications contain information that identifies an event and a position of the event in the data stream. The middleware component transmits the event and position to the application.",{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 1","b":["100","100","100","100"]},"The invention is operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of well known computing systems, environments, and\/or configurations that may be suitable for use with the invention include, but are not limited to, personal computers, server computers, hand-held or laptop devices, multiprocessor systems, microprocessor-based systems, set top boxes, programmable consumer electronics, network PCs, minicomputers, mainframe computers, distributed computing environments that include any of the above systems or devices, and the like.","The invention may be described in the general context of computer-executable instructions, such as program modules, being executed by a computer. Generally, program modules include routines, programs, objects, components, data structures, etc. that perform particular tasks or implement particular abstract data types. The invention may also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment, program modules may be located in both local and remote computer storage media including memory storage devices.","With reference to , an exemplary system for implementing the invention includes a general purpose computing device in the form of a computer . Components of computer  may include, but are not limited to, a processing unit , a system memory , and a system bus  that couples various system components including the system memory to the processing unit . The system bus  may be any of several types of bus structures including a memory bus or memory controller, a peripheral bus, and a local bus using any of a variety of bus architectures. By way of example, and not limitation, such architectures include Industry Standard Architecture (ISA) bus, Micro Channel Architecture (MCA) bus, Enhanced ISA (EISA) bus, Video Electronics Standards Association (VESA) local bus, and Peripheral Component Interconnect (PCI) bus also known as Mezzanine bus.","Computer  typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by computer  and includes both volatile and nonvolatile media, removable and non-removable media. By way of example, and not limitation, computer readable media may comprise computer storage media and communication media. Computer storage media includes both volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information such as computer readable instructions, data structures, program modules or other data. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by computer . Communication media typically embodies computer readable instructions, data structures, program modules or other data in a modulated data signal such as a carrier WAV or other transport mechanism and includes any information delivery media. The term \u201cmodulated data signal\u201d means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media includes wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, FR, infrared and other wireless media. Combinations of any of the above should also be included within the scope of computer readable media.","The system memory  includes computer storage media in the form of volatile and\/or nonvolatile memory such as read only memory (ROM)  and random access memory (RAM) . A basic input\/output system  (BIOS), containing the basic routines that help to transfer information between elements within computer , such as during start-up, is typically stored in ROM . RAM  typically contains data and\/or program modules that are immediately accessible to and\/or presently being operated on by processing unit . By way o example, and not limitation,  illustrates operating system , application programs , other program modules , and program data .","The computer  may also include other removable\/non-removable volatile\/nonvolatile computer storage media. By way of example only,  illustrates a hard disk drive  that reads from or writes to non-removable, nonvolatile magnetic media, a magnetic disk drive  that reads from or writes to a removable, nonvolatile magnetic disk , and an optical disk drive  that reads from or writes to a removable, nonvolatile optical disk  such as a CD ROM or other optical media. Other removable\/non-removable, volatile\/nonvolatile computer storage media that can be used in the exemplary operating environment include, but are not limited to, magnetic tape cassettes, flash memory cards, digital versatile disks, digital video tape, solid state RAM, solid state ROM, and the like. The hard disk drive  is typically connected to the system bus  through a non-removable memory interface such as interface , and magnetic disk drive  and optical disk drive  are typically connected to the system bus  by a removable memory interface, such as interface .","The drives and their associated computer storage media discussed above and illustrated in , provide storage of computer readable instructions, data structures, program modules and other data for the computer . In , for example, hard disk drive  is illustrated as storing operating system , application programs , other program modules , and program data . Note that these components can either be the same as or different from operating system , application programs , other program modules , and program data . Operating system , application programs , other program modules , and program data  are given different numbers here to illustrate that, at a minimum, they are different copies.","A user may enter commands and information into the computer  through input devices such as a keyboard , a microphone , and a pointing device , such as a mouse, trackball or touch pad. Other input devices (not shown) may include a joystick, game pad, satellite dish, scanner, or the like. These and other input devices are often connected to the processing unit  through a user input interface  that is coupled to the system bus, but may be connected by other interface and bus structures, such as a parallel port, game port or a universal serial bus (USB). A monitor  or other type of display device is also connected to the system bus  via an interface, such as a video interface . In addition to the monitor, computers may also include other peripheral output devices such as speakers  and printer , which may be connected through an output peripheral interface .","The computer  may operate in a networked environment using logical connections to one or more remote computers, such as a remote computer . The remote computer  may be a personal computer, a hand-held device, a server, a router, a network PC, a peer device or other common network node, and typically includes many or all of the elements described above relative to the computer . The logical connections depicted in  include a local area network (LAN)  and a wide area network (WAN) , but may also include other networks. Such networking environments are commonplace in offices, enterprise-wide computer networks, intranets and the Internet.","When used in a LAN networking environment, the computer  is connected to the LAN  through a network interface or adapter . When used in a WAN networking environment, the computer  typically includes a modem  or other means for establishing communications over the WAN , such as the Internet. The modem , which may be internal or external, may be connected to the system bus  via the user input interface , or other appropriate mechanism. In a networked environment, program modules depicted relative to the computer , or portions thereof, may be stored in the remote memory storage device. By way of example, and not limitation,  illustrates remote application programs  as residing on remote computer . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.",{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 2","b":["200","200","1","200","202","204","206","208"]},"In one illustrative embodiment, speech middleware component  is implemented in the operating system  illustrated in FIG. . Speech middleware component , as shown in , includes speech recognition middleware component , context free grammar (CFG) engine  and text-to-speech middleware component .","Briefly, in operation, speech middleware component  resides between applications  and engines  and . Applications  can be speech recognition and speech synthesis applications which desire to invoke engines  and . In doing so, applications  make calls to speech middleware component  which, in turn, makes calls to the appropriate engines  and  in order to have speech recognized or synthesized.","For example, applications  may provide the source of audio data for speech recognition. Speech middleware component  passes that information to speech recognition engine  which simply recognizes the speech and returns a recognition result to speech recognition middleware component . Speech recognition middleware component  places the result in a desired format and returns it to the application  which requested it or to another desired location specified by the application .","Similarly, an application  can provide a source of textual data to be synthesized. TTS middleware component  assembles that data, and provides it to TTS engine , for synthesis. TTS engine  simply synthesizes the data and returns audio information, along with associated event information, to TTS middleware component , which handles spooling of that information to an audio device, writing that information to memory, or placing that information in any other desired location, as specified by the application  which requested it.","CFG engine , briefly, assembles and maintains grammars that are to be used by speech recognition engine . This allows multiple applications and multiple grammars to be used with a single speech recognition engine .",{"@attributes":{"id":"p-0040","num":"0039"},"figref":["FIG. 3","FIG. 3"],"b":["200","2","214","214","216","218","220","214","224","214","202","214","208"]},"A general discussion of the operation of TTS middleware component , with applications  and engine , is illustrated by the flow diagram in FIG. . Initially, application  opens an instance of the SpVoice object . In one illustrative embodiment, the application calls the COM CoCreateInstance for the component CLSID_SpVoice to get a pointer to the interface ISpVoice of the SpVoice object. SpVoice object  then creates lexicon container object  and an XML parser object . This is indicated by blocks ,  and  in FIG. .","Next, application  can specify the attributes of TTS engine , such as whether the engine which is the synthesizer exhibits male or female voice qualities, the language of the synthesis, etc. This is done, for example, by calling the SetVoice method on the SpVoice object . This is indicated by optional block  in FIG. . In addition, the application can optionally specify the particular audio output object  which is desired. This is indicated by optional block  in FIG. .","The application  can set other attributes associated with the voice speaking, such as the rate and volume of speech, using for example, the SetRate and the SetVolume methods exposed by the SpVoice object . These are optional as well.","It should be noted that specifying the attributes of the engine  and audio output object  are optional. If the application does not specify these items, the first call to the SpVoice object  requiring synthesis results in the SpVoice object  choosing and initializing the default voice (i.e., the default TTS engine ) and the default audio output object .","Application  must then indicated to SpVoice object  which particular events it is interested in, and how it wishes to be notified of those events. This will be discussed in greater detail below. Suffice it to say that application  will likely be interested in a variety of different events and will wish to be notified when those events are occurring (or prior to the occurrence) at audio output object . Similarly, application  may wish to be notified of the events in one of a variety of different manners. Therefore application  provides this information to SpVoice object . This is indicated by block .","Once these items are configured properly, application  can call the SpVoice object  and request that textual information be synthesized. This can be done, for example, by calling the Speak or the SpeakStream methods on the SpVoice object . This is indicated by block .","The SpVoice object  then performs format negotiation. This does not form part of the present invention and is only optional. Briefly, the SpVoice object  attempts to optimize the format of data created by TTS engine  and that accepted by audio output object  for optimal synthesis. Format negotiation is indicated by block  in FIG. .","The SpVoice object  then breaks the textual information provided by application  into text fragments. For example, if the textual information is in XML, the SpVoice object  invokes the XML parser  to parse the XML input into text fragments. While the textual information can come from a variety of sources (such as a text buffer, straight textual information, XML, etc.) that information is broken into text fragments by SpVoice object , as indicated by block  in FIG. .","The SpVoice object  then calls a speak method on TTS engine , passing in the information to be synthesized. This is indicated by block . In doing this, the SpVoice object  also specifies a Site object  to be used by the TTS engine for returning the synthesized information.","TTS engine  receives the text fragments, synthesizes the text into PCM (pulse code modulation) data (or other suitable audio data) and provides an indication of where events occur in the PCM data. For example, TTS engine  can illustratively provide an indication of where word and phoneme boundaries occur in the PCM data. This information is all provided from TTS engine  to SpVoice object  through the Site object .","It should be noted that, in performing the synthesis, TTS engine  can access the lexicon object  contained in TTS middleware component . The lexicon container object also forms no part of the present invention and is mentioned only for the sake of completeness. Briefly, the lexicon container object  contains all lexicons of interest and the TTS engine  simply needs to access object  as if it were a single lexicon.","Synthesizing the actual fragments and writing them and the events to the Site object are indicated by blocks  and  in FIG. .","During the format negotiation step , the SpVoice object  determines whether the format of the audio output object  or the format of the information provided by TTS engine  need to be converted. If conversion is required, information is provided to a format converter object, such as through the ISpAudio or ISpStream interfaces, where the information is converted into a desired format for the audio output object . The format converter object then manages the process of spooling out the audio information to audio output object  and also manages returning events noticed by the audio output object  to the Site object  and the SpVoice object  for transmission back to the application . Where no format conversion is desired, the information from the Site object  is spooled out to the audio output object  by the SpVoice object , through a suitable interface such as the ISpStream interface, and the audio output object  returns events to the SpVoice object. This is indicated by blocks  and .","Of course, it should also be noted that rather than providing the information directly to an audio output object , the information can be written to memory, or provided at some other specified output or location.","In notifying the SpVoice object  of events, audio output object  can do one of any number of things. For example, audio output object  can provide a notification that an event has occurred, or is occurring, as audio output object  is playing the data containing the event. However, it may well be that application  wishes to be notified of the event prior to the occurrence of the event at the audio object. For example, if the event is a viseme, application  may desire to be notified of the viseme so that it can animate a character on a computer screen, as the audio information is being played. In that instance, or in any instance where application  desires to be notified of the event prior to it occurring, audio output object  can be configured to notify the SpVoice object  that an event is about to occur at any predetermined offset prior to the event in the data stream. This simply requires audio output object  to look forward in the data stream by the offset amount and notify the SpVoice object  when an event is encountered.","As will be described later with respect to ,  and , the SpVoice object  (or its counterpart in SR middleware component ) can initialize a notification assistance component whose primary job is to notify the application  of events. Therefore, when the SpVoice object  receives the event from audio output object , it first determines whether application  is even interested in the event. This was indicated at block  when the application indicated to SpVoice object  the particular types of events it was interested in.","Assuming that application  is interested in the event which has been notified, then SpVoice object  may notify application  directly of the event, using the specific notification mechanism selected by the application in step  of FIG. . However, when the notification assistance object has been initialized, the SpVoice object  notifies the notification assistance object that an event which the application is interested in has occurred. The notification assistance object then notifies the application  of the event. This is indicated by block .",{"@attributes":{"id":"p-0058","num":"0057"},"figref":["FIG. 5","FIG. 3","FIG. 5","FIG. 5","FIG. 5"],"b":["202","3","204","300","302","204","300","204","302","204","216"]},{"@attributes":{"id":"p-0059","num":"0058"},"figref":["FIG. 6","FIG. 5","FIG. 6"],"b":"304"},{"@attributes":{"id":"p-0060","num":"0059"},"figref":"FIG. 7","b":["304","202","216","202","307","216","202"]},"For example, if the application  calls SetNotifySink that indicates that the application is implementing an ISpNotifySink which exposes a Notify method that can simply be called by notification assistance object  to notify application  of an event.","If the application invokes SetNotifyCallBackFunction that indicates that application  simply wishes to be notified using a callback function, such as those common in the C or C++ programming languages.","If application  invokes SetNotifyWindowMessage that indicates that application  wishes to be notified using a window callback function to receive notifications by having a window message posted.","If the application calls SetNotifyWin32Event this simply sets up a Win32 event object and indicates that the application supports Win32 event notification and can be notified according to that mechanism.","These are but a few of the possible notification mechanisms that can be selected. The application  notifying SpVoice  of its selected notification mechanism is indicated by block .","Application  then provides an indication as to the particular types of events it is interested in to SpVoice Object . This is indicated by block . In doing so, the application may illustratively invoke the SetInterests method on the ISpEventSource interface  on SpVoice object . This method allows the application  to specify the different event types which it wishes to be notified of.","Once SpVoice object  has received the selected notification mechanism from application , it initializes notification assistance object  according to the notification mechanism selected by the application. This is indicated by block . This can be done, for example, by initializing assistance object  by invoking the Initxxxx method on the ISpNotifyTranslator interface. Notification assistance object  is thus initialized such that, when SpVoice object  invokes the Notify method on the ISpNotifySink interface  supported by notification assistance object  notifies application  according to the selected method. In one illustrative embodiment, notification assistance object  supports an interface  which makes the events available for retrieval and examination by application . In another embodiment, notification assistance object  simply notifies application  of the event and application  retrieves or examines the event from the SpVoice object .",{"@attributes":{"id":"p-0068","num":"0067"},"figref":["FIG. 8","FIGS. 5 and 6","FIG. 8"],"b":["208","208","350"]},"TTS engine  invokes the AddEvents method on interface  of the Site object . This adds the event identified at block  by the TTS engine to the Site object . This is indicated by block  in FIG. .","The SpVoice object  then transfers the event and its offset in the audio stream to audio object . The event and its offset are placed in event queue  in audio object . Of course, transferring of this event can be accomplished through appropriate interfaces as well. Transferring the event and its offset to event queue  in audio object  is indicated by block  in FIG. .","Audio object  begins playing the data. As it plays the data, it looks a predetermined offset distance ahead in the data stream to determine whether any events are upcoming in event queue . When the audio object encounters an event in the data stream, it transfers the event and its offset from the event queue  to its completed queue . After this is accomplished it invokes the Notify method on the ISpNotifySink interface  on the SpVoice object . This is indicated by block  in FIG. . This notifies SpVoice object  that audio object  has reached an event in its event queue.","After being notified of an event, SpVoice object  invokes the GetEvents method on the ISpEventSource interface  of audio object . This is indicated by block  in FIG. . Audio object  then returns to the SpVoice object  the contents of its completed events queue . This is indicated by block .","Upon receiving the event information indicative of the particular event about which SpVoice object  was notified, SpVoice object  determines whether an application  is even interested in being notified of this event. Recall that application  indicated the events in which it was interested to SpVoice object  by invoking the SetIntersts method on the ISpEventSource interface . Therefore, SpVoice object  simply needs to compare the event type which was returned from audio object  with those specified as being of interest to application . This is indicated by blocks  and  in FIG. . Of course, if the application is not interested in this event, the SpVoice object updates it's internal state and then the event is simply discarded and SpVoice object  awaits notification of the next event encountered by audio output object . This is indicated by blocks  and .","If SpVoice object  determines that application  is interested in this event, then what happens next depends on whether the notification system is an embodiment in which the notification assistance object  is implemented. If not, the processing simply proceeds at block  and SpVoice object  simply notifies application  according to the notification mechanism which it selected at the outset.","However, if notification assistance object  is implemented (as shown in ) then SpVoice object  indicates to object  that an event has been encountered. In one illustrative embodiment, SpVoice object  does this by invoking the Notify method on the ISpNotifySink interface  supported by notification assistance object . This is indicated by block  in FIG. . At that point, notification assistance object  notifies application  according to the selected notification method, for which it was initialized. This is indicated by block . In one embodiment, notification assistance object retrieves the event from the SpVoice object and makes it available to the application, as indicated by block . In another embodiment, the notification assistance object simply notifies the application of the event and the application accesses the SpVoice object for the event.",{"@attributes":{"id":"p-0076","num":"0075"},"figref":["FIG. 9","FIGS. 5 and 6"],"b":["206","304","304"]},"SpRecoContext object  is the counterpart to the SpVoice object  in TTS middleware component  in that it generally manages data flow and performs services within SR middleware component . The SpRecoContext object  exposes one or more interfaces that can be used with application . SpRecoContext object  also calls interface methods exposed by SR engine object . A more detailed discussion of the SpRecoContext object  can be found in the above-identified and related patent application. However, no further explanation is needed for the sake of the present invention.","Processing events is highly similar to that with respect to the TTS systems shown in . In other words, SR engine  receives voice data to be recognized. The recognition results are provided to Site object . Also, events (such as recognitions, word boundaries, phoneme boundaries, etc.) are added to Site object  by invoking the AddEvents methods. These items are provided back to the SpRecoContext object . In response, in the embodiment in which notification assistance object  is not present, SpRecoContext object  simply determines whether application  is interested in the events (because the application has previously notified SpRecoContext object  of the events in which it is interested) and, if so, notifies application  by calling the applicatoion's Notify method.","In the embodiment shown in , in which the notification assistance object  is implemented, SpRecoContext object  calls Notify on the ISpNotifySink interface  supported by object . This indicates to object  that SpRecoContext object  has encountered an event. In response, the notification assistance object  notifies application  according to its selected notification mechanism, by which notification assistance object  was initialized. Of course, where an input object is used with SpRecoContext object , interaction is similar to that with respect to the audio output object  in SpVoice object .","Appendix A illustrates a number of the interfaces discussed herein in greater detail, simply for the sake of completeness. They can be implemented in other ways as well and still comport with the inventive features of the present invention.","It can thus be seen that the present invention is directed to a middleware layer that is arranged between the applications and engines. The middleware layer supports interfaces that allow the applications to select one of a variety of different types of notification mechanisms. The middleware layer then configures itself to provide event notification from the engine to the application in the selected manner. In addition, the present invention allows an output device to simply notify the middleware layer when it has encountered the location in the output data stream which coincides with an event. The middleware layer then takes care of notifying the application, and the output device need not even be aware of the type of the event for which notification was sent. This also significantly simplifies the coding required to make output devices consistent with the other components in the system.","Although the present invention has been described with reference to preferred embodiments, workers skilled in the art will recognize that changes may be made in form and detail without departing from the spirit and scope of the invention."],"heading":["INCORPORATION BY REFERENCE","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE ILLUSTRATIVE EMBODIMENTS"],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 3","b":"2"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 4","b":"3"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":["FIG. 8","FIGS. 5 and 6"]},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 9"}]},"DETDESC":[{},{}]}
