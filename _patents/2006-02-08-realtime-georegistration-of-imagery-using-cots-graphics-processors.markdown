---
title: Real-time geo-registration of imagery using COTS graphics processors
abstract: A method of performing real-time geo-registration of high-resolution digital imagery using existing graphics processing units (GPUs) already found in current personal computers, rather than the main central processing unit (CPU). Digital image data captured by a camera (along with inertial navigation system (INS) data associated with the image data) is transferred to and processed by the GPU to perform the calculations involved in transforming the captured image into a geo-rectified, nadir-looking image. By using the GPU, the order-of-magnitude increase in throughput over conventional software techniques makes real-time geo-registration possible without the significant cost of custom hardware solutions.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07555143&OS=07555143&RS=07555143
owner: Lawrence Livermore National Security, LLC
number: 07555143
owner_city: Livermore
owner_country: US
publication_date: 20060208
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["I. CLAIM OF PRIORITY IN PROVISIONAL APPLICATION","II. FIELD OF THE INVENTION","III. BACKGROUND OF THE INVENTION","IV. SUMMARY OF THE INVENTION","VI. DETAILED DESCRIPTION"],"p":["This application claims the benefit of U.S. provisional application No. 60\/651,839 filed Feb. 9, 2005, entitled, \u201cReal-Time Geo-Registration of Imagery Using Commercial Off-the-Shelf Graphics Processors\u201d by Laurence M. Flath et al.","The United States Government has rights in this invention pursuant to Contract No. W-7405-ENG-48 between the United States Department of Energy and the University of California for the operation of Lawrence Livermore National Laboratory.","The present invention relates to geo-registration methods, and more particularly relates to a method of performing real-time geo-registration of high-resolution digital imagery using existing graphics processing units (GPUs) already available with current personal computers.","A wide range of sensor technologies (visible, infrared, radar, etc.) and a wide variety of platforms (mountaintops, aircraft, satellite, etc) are currently used to obtain imagery of planetary surfaces. Geo-registration is the process of mapping imagery obtained from such various sources into predetermined planetary coordinates and conditions, i.e. calibrating\/correlating such image data to the real world so as to enable, for example, the determination of absolute positions (e.g. GPS (global positioning system) coordinates), distances, etc. of features found in the image. However, in order to overlay the images from the various types of cameras (sensor fusion), the image data from all of the disparate sources must first be modified into a common coordinate system. Geo-rectification is the process of converting or otherwise transforming an image (e.g. an off-axis image) recorded from an arbitrary position and camera orientation, into one that appears as a nadir view, i.e. a view from directly above the scene\/object\/features of interest looking straight down, as at a map. Geo-rectification thus enables various images to share the same orthogonal perspective so that they can be geo-registered and correlated to\/against each other or a reference image.","Image processing, however, is often computationally expensive; the number of image processing computations necessary to perform geo-rectification of off-axis high-resolution images is typically very large even for small source images, requiring significant computational resources and making real-time visualization of live data difficult. Real-time image data processing is therefore typically managed as a trade off between image size (number of pixels) and data rate (frames per second).","Current techniques for fast image rendering and geo-registration either employ software only for post processing of data, or require expensive custom hardware i.e. dedicated pixel processors, even for relatively low-resolution source data. Software-only techniques, for example, can perform the image transformation calculations necessary for geo-registration on the central processing unit (CPU) of a computer or workstation. Due to inadequate memory bandwidth, however, these methods typically take 2-3 seconds per mega-pixel of image data, even with currently available high-end workstations preventing such software only methods from performing in real-time.","And the custom hardware approach typically utilizes dedicated pixel processors, which are specially designed graphics cards (printed circuit boards) and software capable of high throughputs which enable real-time performance of image transformations and geo-registration. For example, one particular custom hardware\/dedicated pixel processor for performing real time geo-registration, known as Acadia\u2122, is commercially available from Sarnoff\/Pyramid Vision Technologies. This representative custom device, however, operates at a low resolution with RS-170 quality video, which is \u02dc640\u00d7480 pixels at 30 Hz Moreover, there is a high cost for such custom hardware and the programming time to custom-configure such hardware. For example, such custom dedicated pixel processors typically cost in the tens of thousands of dollars for the hardware alone, and an additional cost ranging up to $100K for the configuration of the software.","What is needed therefore is a digital image processing methodology for performing geo-registration that is faster (real time streaming), more cost effective, and with higher resolution than software-only techniques or the use of expensive custom hardware\/dedicated pixel processors.","One aspect of the present invention includes a geo-registration method comprising: obtaining digital image data of an image source using a camera located a distance D from a center field of view (CFOV) of the image source, where f is the focal length of a lens of the camera and D>>f; obtaining inertial navigation system (INS) data of the camera associated with the digital image data, said INS data including camera position data and camera attitude data including roll, pitch, and heading; loading the digital image data into a GPU of a computer system to be processed thereby; in the GPU of the computer system: calculating relative angles and distances between the camera and the image source from said INS data; performing geometry correction of the digital image data using the calculated relative angles and distances; performing geo-rectification of the geometrically corrected digital image data using an affine homogenous coordinate transformation, to produce a geo-rectified digital image data; performing a rotation transformation about a z-axis of the geo-rectified digital image data to remove \u03b1; and performing geo-registration with the heading-adjusted and geo-rectified digital image data.","Another aspect of the present invention includes an article of manufacture comprising: a computer usable medium having computer readable program code means embodied therein for geo-registering digital image data of an image source using a GPU of a computer, said digital image data obtained using a camera located a distance D from a center field of view (CFOV) of the image source, where f is the focal length of a lens of the camera and D>>f, and said digital image data associated with inertial navigation system (INS) data of the camera including camera position data and camera attitude data, the computer readable program code means in said article of manufacture comprising: computer readable program code means for causing the GPU to calculate relative angles and distances between the camera and the image source from said INS data; computer readable program code means for causing the GPU to perform geometry correction of the digital image data using the calculated relative angles and distances; computer readable program code means for causing the GPU to perform geo-rectification of the geometrically corrected digital image data using an affine homogenous coordinate transformation, to produce a geo-rectified digital image data; computer readable program code means for causing the GPU to perform a rotation transformation about a z-axis of the geo-rectified digital image data to remove \u03b1; and computer readable program code means for causing the GPU to perform geo-registration with the heading-adjusted and geo-rectified digital image data.","The present invention is directed to a geo-registration method for performing transformations of high-resolution digital imagery that operates in real time, is faster and typically produces higher resolution images than software only or custom hardware\/dedicated pixel processor techniques, and is less expensive than such existing techniques by using commercial off-the-shelf (COTS) image processing hardware already found in most current personal computers (PCs). In particular, the method and system of the invention performs image transformations using a COTS graphics processing unit (GPU) typically found in a conventional PC rather than the main central processing unit (CPU) of the computer as is often done with existing geo-registration technology. Personal computers with graphics cards typically cost less than $3K.","A. Theory of Geo-registration","In order to describe the geometric transformations involved in geo-rectification,  shows the simple case of first-order transverse imaging of a planar object surface that is perpendicular to the optical axis, onto the real image plane of a camera. In particular,  shows a camera system, represented by lens , located at a position C(x, y, z) in space, and staring in a nadir-looking direction along an optical axis  at a point G(x, y, ) on the ground plane . The distance of the optical axis  between the lens  and the point G of the planar object is shown as D. If the focal length of the lens is f, and the distance between the lens  and a real image plane is D(not shown), then the thin lens equation is:",{"@attributes":{"id":"p-0029","num":"0028"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mfrac":{"mn":"1","mi":"f"},"mo":"=","mrow":{"mfrac":[{"mn":"1","msub":{"mi":["D","c"]}},{"mn":"1","msub":{"mi":["D","g"]}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"1"}}]}}}},"br":{},"sub":["g","c "]},{"@attributes":{"id":"p-0030","num":"0029"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mfrac":{"mn":"1","mi":"f"},"mo":"=","mrow":{"mrow":[{"mrow":{"mfrac":[{"mn":"1","msub":{"mi":["D","c"]}},{"mn":"1","msub":{"mi":["D","g"]}}],"mo":"+"},"mo":"\u21d2","msub":{"mi":["D","c"]}},{"mfrac":{"msub":{"mi":["fD","g"]},"mrow":{"mi":"f","mo":"+","msub":{"mi":["D","g"]}}},"mo":"\u2248","mi":"f"}],"mo":"="}},{"mrow":{"mi":"for","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"msub":{"mi":["D","g"]}},"mo":"\u2aa2","mi":"f"}],"mo":","}},{"mrow":{"mo":["(",")"],"mn":"2"}}]}}}},"br":{},"b":["12","12"],"sub":"g","figref":"FIG. 1"},{"@attributes":{"id":"p-0031","num":"0030"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mfrac":{"msub":{"mi":["x","c"]},"mi":"f"},"mo":"=","mrow":{"mo":"-","mrow":{"mfrac":{"msub":[{"mi":["x","g"]},{"mi":["D","g"]}]},"mo":"."}}}},{"mrow":{"mo":["(",")"],"mn":"3"}}]}}}},"br":{},"sub":["g ","g","c ","c"],"b":"12"},"In contrast to the example of , a more general imaging scenario may be considered of a remote mobile camera system imaging a ground plane at an oblique (\u201coff-axis\u201d) angle (see ). Generally, several quantities define the projection of the off-axis ground plane to the camera focal plane. The first three quantities are the relative position (X, Y, Z) of the camera with respect to the center field of view (CFOV). In aviation parlance, X, Y, and Z are the relative longitude, latitude, and altitude, respectively. The next three quantities involve the camera orientation or attitude, which include the camera rotation angles corresponding to heading (\u03b1), pitch (\u03b1), and roll (\u03b1). It is appreciated that \u03b1is characterized as the angular deviation from a reference direction (e.g. North) due to rotation about a Z-axis orthogonal to the X-Y ground plane, \u03b1is the angle the optical axis makes with the X-Y ground plane, and \u03b1is the angular deviation of the camera from a reference direction due to rotation about the optical axis. And lastly, optical system characteristics, such as the lens focal length, f, the focal plane array (FPA) size, and FPA pixel pitch determine the magnification of the camera's optical system. It is appreciated that images captured by the camera will be uniquely associated with the camera positions and attitudes at the time of image acquisition.","It is notable that since it is impossible to map an arbitrary 3-D surface onto a 2-D image plane without distortion, many types of planetary-scale projections exist to prioritize a particular property. For example, Mercator gives straight meridians and parallels, Azimuthal-Equidistant gives equal areas, etc. All such projections require a \u2018warping\u2019 of the source data and are, at best, an approximation of reality. However, for the imaging applications considered here involving mapping a localized area, the earth's curvature can essentially be ignored. As such, the transforming of imagery from a recorded first perspective into another for geo-registration is possible with only the position and orientation\/attitude information of the sensor (i.e. camera) platform. To this end, a global positioning system (GPS) locator is typically used to provide the camera position data (accurate to better than a few meters) anywhere on the globe, and inertial measurement hardware (i.e. inertial measurement unit, \u201cIMUs\u201d) known in the art, is used to provide the camera attitude, e.g. roll, pitch, and heading. Together the GPS and IMU is characterized as an inertial navigation system, \u201cINS\u201d.","In the general imaging scenario, camera roll (\u03b1) about the optical axis will cause the off-axis projected image to appear as an asymmetric quadrilateral, making direct calculations of the corner positions rather involved. The problem can be simplified by performing two basic coordinate transformations. First, by viewing along a ground line from the camera's position to the CFOV, the relative positions X and Y of camera the can be reduced to a single quantity G (where G=\u221a{square root over (X+Y)}), and the three camera orientation angles (\u03b1, \u03b1,\u03b1) are also reduced to a single pitch angle, \u03b1. Second, by rotating the image to remove the roll component, \u03b1, rectangular ground regions map to a symmetrical trapezoidal areas on the camera focal plane, and vice-versa (where symmetrical trapezoidal ground regions map to a rectangular area on the camera focal plane, as shown in ). Deriving the relationship between the ground and the camera focal plane then becomes straightforward geometry, and geo-rectification may be subsequently performed.",{"@attributes":{"id":"p-0035","num":"0034"},"figref":["FIGS. 2-5","FIG. 2","FIG. 3","FIGS. 4 and 5","FIGS. 2 and 3","FIG. 1"],"sub":["roll ","pitch "],"b":["22","23","20","21","22","22","20","22","23"]},{"@attributes":{"id":"p-0036","num":"0035"},"figref":["FIG. 3","FIG. 3"],"b":"23","sub":["pitch","g","g","c","c","g ","c ","g ","c","c ","g ","g ","c ","g ","g ","c ","g "]},{"@attributes":{"id":"p-0037","num":"0036"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["y","c"]},"mo":"=","mrow":{"mrow":[{"mfrac":{"msub":[{"mi":["y","\u221e"]},{"mi":["x","F"]}]},"mo":"\u2062","msub":{"mi":["x","c"]}},{"msub":{"mi":["y","\u221e"]},"mo":"."}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}}}},"br":{},"sub":["F ","c","\u221e"]},"To relate the ground x-coordinate (x) of the ground patch  to the camera image plane coordinates (x), and the ground y-coordinate (y) of the ground patch to the camera image plane coordinates (y), the quantities y and xare computed. In particular, xis computed using equation (3) as follows.",{"@attributes":{"id":"p-0039","num":"0038"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msub":{"mi":["x","F"]},"mo":"=","mrow":{"mrow":{"mo":"-","mfrac":{"mi":["f","D"]}},"mo":"\u2062","msub":{"mi":["x","g"]}}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"5"}}]}}}},"br":{},"sub":"\u221e","figref":"FIG. 4"},{"@attributes":{"id":"p-0040","num":"0039"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["y","\u221e"]},"mo":"=","mrow":{"mrow":[{"mrow":{"mo":"-","mi":"f"},"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mi":"tan","msub":{"mi":["\u03b1","pitch"]}},{"mrow":[{"mo":"-","mi":"f"},{"mfrac":{"mi":["Z","G"]},"mo":"."}],"mo":"\u2062"}],"mo":"="}}},{"mrow":{"mo":["(",")"],"mn":"6"}}]}}}},"br":{}},{"@attributes":{"id":"p-0041","num":"0040"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["y","c"]},"mo":"=","mrow":{"mrow":[{"mrow":{"mfrac":{"msub":[{"mi":["y","\u221e"]},{"mi":["x","F"]}]},"mo":"\u2062","msub":{"mi":["x","c"]}},"mo":"+","msub":{"mi":["y","\u221e"]}},{"mrow":[{"mfrac":[{"mi":"D","msub":{"mi":["x","g"]}},{"mi":["Z","G"]}],"mo":["\u2062","\u2062"],"msub":{"mi":["x","c"]}},{"mi":"f","mo":"\u2062","mrow":{"mfrac":{"mi":["Z","G"]},"mo":"."}}],"mo":"-"}],"mo":"="}}},{"mrow":{"mo":["(",")"],"mn":"7"}}]}}}},"br":{},"sub":["g ","c"]},{"@attributes":{"id":"p-0042","num":"0041"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["x","g"]},"mo":"=","mrow":{"mfrac":{"mrow":[{"mrow":{"mo":["(",")"],"mfrac":{"mi":["D","f"]}},"mo":"\u2062","msub":{"mi":["x","c"]}},{"mrow":{"mrow":{"mo":["(",")"],"mfrac":{"mi":["G","Zf"]}},"mo":"\u2062","msub":{"mi":["y","c"]}},"mo":"-","mn":"1"}]},"mo":"."}}},{"mrow":{"mo":["(",")"],"mn":"8"}}]}}}}},"Similarly, the ground y-coordinate (y) is calculated as it relates to the y-coordinate (y) of the camera focal plane using the trigonometric arrangement illustrated in , as follows:",{"@attributes":{"id":"p-0044","num":"0043"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"mrow":[{"mi":"sin","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["\u03b1","pitch"]}},{"mrow":[{"mfrac":{"msub":[{"mi":"y","mo":"\u22a5"},{"mi":["y","g"]}]},"mo":"\u21d2","msub":{"mi":"y","mo":"\u22a5"}},{"msub":[{"mi":["y","g"]},{"mi":["\u03b1","pitch"]}],"mo":["\u2062","\u2062","\u2062"],"mi":"sin","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}],"mo":"="}],"mo":"="},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"9"}}]},{"mtd":[{"mrow":{"mrow":[{"mi":"cos","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["\u03b1","pitch"]}},{"mrow":[{"mfrac":{"mrow":{"mi":"D","mo":"-","msub":{"mi":"D","mo":"\u22a5"}},"msub":{"mi":["y","g"]}},"mo":"\u21d2","msub":{"mi":"D","mo":"\u22a5"}},{"mi":"D","mo":"-","mrow":{"msub":{"mi":["y","g"]},"mo":["\u2062","\u2062","\u2062"],"mi":"cos","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":["\u03b1","pitch"]},"mo":"."}}}],"mo":"="}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"10"}}]}]}}},"br":{}},{"@attributes":{"id":"p-0045","num":"0044"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mfrac":{"msub":{"mi":["y","c"]},"mi":"f"},"mo":"=","mrow":{"mrow":[{"mo":"-","mfrac":{"msub":{"mi":"y","mo":"\u22a5"},"mrow":{"mi":"D","mo":"\u22a5"}}},{"mfrac":{"mrow":[{"msub":{"mi":["y","g"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mi":["Z","D"]}}},{"mrow":{"msub":{"mi":["y","g"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mi":["G","D"]}}},"mo":"-","mi":"D"}]},"mo":"."}],"mo":"="}}},{"mrow":{"mo":["(",")"],"mn":"11"}}]}}}},"br":{}},{"@attributes":{"id":"p-0046","num":"0045"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["y","g"]},"mo":"=","mrow":{"mfrac":{"mrow":[{"mrow":{"mo":["(",")"],"mfrac":{"msup":{"mi":"D","mn":"2"},"mi":"Zf"}},"mo":"\u2062","msub":{"mi":["y","c"]}},{"mrow":{"mrow":{"mo":["(",")"],"mfrac":{"mi":["G","Zf"]}},"mo":"\u2062","msub":{"mi":["y","c"]}},"mo":"-","mn":"1"}]},"mo":"."}}},{"mrow":{"mo":["(",")"],"mn":"12"}}]}}}},"br":[{},{}],"sub":["g","c ","g","c "],"figref":"FIG. 5"},"The graphics subsystems in currently available personal computers are designed to rapidly perform 3-D calculations of extremely complex scenes in real-time, and are especially well-suited for performing perspective transformations like those in Equations 8 and 12. Mostly due to demand by the gaming industry, the GPUs found on the top-of-the-line video cards, such as for example ATI\u2122, nVidia\u2122, and Intel\u2122, contain more transistors than the main CPU of a PC, and utilize the same state-of-the-art semiconductor fabrication\/design rules. When perspective calculations are performed on a general-purpose CPU, the division and square-root operators are the most time-consuming and temporally non-deterministic computations, not to mention the possibility of a divide-by-zero exception, which would halt the process. This is not the case with GPUs, which are specifically designed for performing complex arithmetic operators such as inverse square root, clamping, and homogeneous coordinates, as well as employing affine transformations for shift, scale, rotation, and shear, and point operators for value scaling.  shows a schematic illustration of the Radeon\u2122 9700 video card commercially available from ATI Technologies, Inc. (Ontario, Canada), illustrating some of the many sophisticated data processing components and functions available in COTS GPUs.","It is appreciated that GPUs can be an adapter, i.e. a removable expansion card in the PC, or can be an integrated part of the system board. Basic components of a GPU include a video chip set for creating the signals to form an image on a screen; some form of random access memory (RAM), such as EDO, SGRAM, SDRAM, DDR, VRAM, etc. as a frame buffer where the entire screen image is stored; and a display interface, such as a RAMDAC (digital\/analog) through which signals are transmitted to be displayed on screen. The digital image transferred into RAM is often called a texture map, and is applied to some surface in the GPU's 3-D world. Preferably, the digital image is transferred into the GPU's dedicated high-speed video random access memory (VRAM). It is appreciated that VRAM is fast memory designed for storing the image to be displayed on a computer's monitor. VRAM may be built from special memory integrated circuits designed to be accessed sequentially. The VRAM may be dual ported in order to allow the display electronics and the CPU to access it at the same time.","Perhaps one downside of using GPUs, however, directly relates to the relative immaturity of available low-level development\/coding tools (assembly level). To achieve high-performance custom code, GPUs must be programmed in assembly language without the availability of debuggers. Fortunately, high-level application programming interfaces (APIs) to the GPU are provided by the manufacturers of COTS video cards. Using languages, such as for example OpenGL and DirectX, the vast majority of complex 3-D operations are performed transparently in the graphics subsystem hardware so that outputs are fully defined, known, and predictable given a set of inputs. At the high-level API, 3-D perspective transformations make use of homogeneous coordinates, as suggested in , Third Edition, Addison Wesley, 1999, Appendix F (pp 669-674). In this system, all coordinates are scale-invariant:",{"@attributes":{"id":"p-0050","num":"0049"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mo":["\u230a","\u230b"],"mtable":{"mtr":[{"mtd":{"mi":"x"}},{"mtd":{"mi":"y"}},{"mtd":{"mi":"z"}},{"mtd":{"mi":"w"}}]}},{"mo":["\u230a","\u230b"],"mtable":{"mtr":[{"mtd":{"mi":"ax"}},{"mtd":{"mi":"ay"}},{"mtd":{"mi":"az"}},{"mtd":{"mi":"aw"}}]}}],"mo":"\u2261"}},{"mrow":{"mo":["(",")"],"mn":"13"}}]}}}},"br":{}},{"@attributes":{"id":"p-0051","num":"0050"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"Homogeneous","mo":["\u2062","[","]"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mtable":{"mtr":[{"mtd":{"mi":"x"}},{"mtd":{"mi":"y"}},{"mtd":{"mi":"z"}},{"mtd":{"mi":"w"}}]}},{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mrow":{"mi":["x","w"],"mo":"\/"}}},{"mtd":{"mrow":{"mi":["y","w"],"mo":"\/"}}},{"mtd":{"mrow":{"mi":["z","w"],"mo":"\/"}}}]}},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mtext":"-"},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mn":"3","mi":["D","World"]}],"mo":"\u21d4"}},{"mrow":{"mo":["(",")"],"mn":"14"}}]}}}},"br":{}},{"@attributes":{"id":"p-0052","num":"0051"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mo":["\u230a","\u230b"],"mtable":{"mtr":[{"mtd":{"msup":{"mi":["x","\u2032"]}}},{"mtd":{"msup":{"mi":["y","\u2032"]}}},{"mtd":{"msup":{"mi":["z","\u2032"]}}},{"mtd":{"msup":{"mi":["w","\u2032"]}}}]}},{"mrow":[{"mo":["\u230a","\u230b"],"mtable":{"mtr":[{"mtd":[{"msub":{"mi":"m","mn":"0"}},{"msub":{"mi":"m","mn":"4"}},{"msub":{"mi":"m","mn":"8"}},{"msub":{"mi":"m","mn":"12"}}]},{"mtd":[{"msub":{"mi":"m","mn":"1"}},{"msub":{"mi":"m","mn":"5"}},{"msub":{"mi":"m","mn":"9"}},{"msub":{"mi":"m","mn":"13"}}]},{"mtd":[{"msub":{"mi":"m","mn":"2"}},{"msub":{"mi":"m","mn":"6"}},{"msub":{"mi":"m","mn":"10"}},{"msub":{"mi":"m","mn":"14"}}]},{"mtd":[{"msub":{"mi":"m","mn":"3"}},{"msub":{"mi":"m","mn":"7"}},{"msub":{"mi":"m","mn":"11"}},{"msub":{"mi":"m","mn":"15"}}]}]}},{"mo":["\u230a","\u230b"],"mtable":{"mtr":[{"mtd":{"mi":"x"}},{"mtd":{"mi":"y"}},{"mtd":{"mi":"z"}},{"mtd":{"mi":"w"}}]}}],"mo":"\u00b7"}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"15"}}]}}}},"br":{},"sub":["0","15 "]},{"@attributes":{"id":"p-0053","num":"0052"},"maths":{"@attributes":{"id":"MATH-US-00015","num":"00015"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":"m","mn":"0"},"mo":"=","mrow":{"mo":"-","mfrac":{"mi":["D","f"]}}},{"msub":{"mi":"m","mn":"5"},"mo":"=","mrow":{"mo":"-","mfrac":{"msup":{"mi":"D","mn":"2"},"mi":"Zf"}}},{"msub":{"mi":"m","mn":"7"},"mo":"=","mrow":{"mo":"-","mfrac":{"mi":["G","Zf"]}}}],"mo":[";","\u2062",";","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]}},{"mrow":{"mo":["(",")"],"mn":"16"}}]}}}},"br":[{},{},{}],"in-line-formulae":[{},{}],"sub":["1,2,3,4,6,7,8,9,11,112,13,14","10,15"]},{"@attributes":{"id":"p-0054","num":"0053"},"figref":"FIG. 6","b":["60","61","62","63","64","61","64"],"sub":["pitch ","pitch "]},{"@attributes":{"id":"p-0055","num":"0054"},"figref":"FIG. 7","b":["70","71","73","79","72"]},"Blocks  and  represent two calculations steps which are based on GPS position data obtained by the GPS locator. In particular, the relative altitude, Z, of the camera is calculated with respect to the CFOV at block , and the pitch angle, \u03b1, is calculated at block . Depending on the INS's coordinate transformation model, the calculation of \u03b1may be a simple substitution or require a complex set of trigonometric calculations involving heading, pitch, and roll.","Since it is unlikely that the GPU's 3D world, GPS coordinates, pixel spacing, etc. will have identical measurement units, block  shows the step of converting units, which may be considered a part of the geometry correction step  of , This potential problem can be addressed by using a scaling transformation matrix for unit conversion. It is notable, however, that care must be taken when operating on numbers of vastly different orders-of-magnitude. The high-level language may use double precision floating-point, but current GPUs typically only support 16 or 24-bit numbers. Thus in the alternative, unit conversion scaling may be propagated into the geo-rectification transformation matrix of equations","Another aspect of the geometry correction step includes removing the roll, \u03b1, of the camera at block , i.e. offsetting any angular deviation from a reference direction caused by rotation of the camera about the optical axis. This is accomplished by performing a rotation transformation on the source image.","Geo-rectification is then performed at block  by calculating m, m, and mfrom the previously calculated values for Z, D, G, and f, and calculating the transformation matrix of equations (16) and (17) above, to produce a geo-rectified image.","At block , the heading angle, \u03b1, is removed to offset and adjust for any angular deviation from a reference direction. This is accomplished by performing a rotation transformation about the z-axis of the geo-rectified image such that the result is oriented with a reference direction as \u2018up\u2019 (such as North). Similar to the removal of the pitch angle, the heading angle, \u03b1, may involve a transformation from the INS unit's coordinate system.","And in block , geo-registration is performed for spatial correlation, and a registration region is rendered. It is notable that this step is highly sensitive to errors in the determination of \u03b1. On the scale of most geo-registration applications, the uncertainty in position of GPS (<10 m) has very little effect on the calculation results. Unfortunately, this cannot be said for the angular data. Depending on the INS hardware, heading may drift up to several degrees per hour. Even so, as long as Z>>f; the result of small angular errors is a transverse offset in the resulting geo-rectified image. Correlation, or more sophisticated morphological techniques may be required to stabilize the imagery to a reference position. Note that GPUs are very efficient at translating an image and can automatically perform the anti-aliasing required for non-integer shifts.","Thus, when performing the geo-registration step, a preferred embodiment of the invention additionally provides jitter control of the generated image. In this enhancement, a small segment of the image is geo-registered, and after a process involving Fourier transforms and inverse transforms of two image segments, the position of the correlation \u201cpeak\u201d (i.e. the point of highest intensity light) in the resulting image is discerned. This identifies the appropriate relative positions of the two images, as a relative position shift. This relative position shift is then applied as part of a geo-registration of the complete image. The advantage of this approach is that the jittering of an image can be stabilized far more quickly than if the correlation technique had been applied to the entire image.","Additionally, while the method of the present invention may assume a flat-earth model (as previously discussed) it may also, in the alternative, incorporate digital elevation maps (DEMs) (which are available for much of the earth's surface) into the image projection process by \u2018draping\u2019 the texture map (source image data) over a 3-D surface built from DEM data.","D. Example Software Implementation","The procedure outlined in the previous section C. has been implemented by Applicants in an exemplary embodiment using a custom MacOS X application for the user interface, and OpenGL for the GPU code, collectively the \u201csoftware.\u201d  shows a screen shot showing a snippet of exemplary source code of the software representing the display routine used by OpenGL to perform the steps discussed above for geo-rectification. The signs of mand mare opposite to that described in Equation 16 due to the coordinate system of the texture map. Algebraic manipulation of mleads to an alternate equation, namely:",{"@attributes":{"id":"p-0065","num":"0064"},"maths":{"@attributes":{"id":"MATH-US-00016","num":"00016"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":"m","mn":"5"},"mo":"=","mrow":{"mrow":[{"mo":["(",")"],"mrow":{"mi":"Z","mo":"+","mfrac":{"msup":{"mi":"G","mn":"2"},"mi":"Z"}}},{"mfrac":{"mn":"1","mi":"f"},"mo":"."}],"mo":"\u2062"}}},{"mrow":{"mo":["(",")"],"mn":"18"}}]}}}}},{"@attributes":{"id":"p-0066","num":"0065"},"figref":["FIGS. 8-12","FIGS. 8 and 9","FIG. 10","FIG. 11","FIG. 12","FIG. 11","FIG. 11","FIG. 12"]},"When used to process sample mobile-platform imagery recorded with corresponding INS data, the software has been shown to process 4 Mpixel frames at over 10 Hz on an 800 MHz PowerBook G4. In contrast, geo-registration of the identical image data performed by custom software on a 2 GHz-class Pentium 4 system required approximately 15 seconds to process each 4 Mpixel frame. Thus the method of the present invention has been shown to work very well even on mid-range GPUs. However, where the technique of the present invention out-performs custom hardware solutions is in the area of high-resolution digital imagery. It is appreciated that the ultimate limitations of size\/speed will be determined by both the amount of VRAM of the GPU, and the bandwidth of the graphics card-to-PC motherboard interface. Rendering time is itself not a limitation, since texture mapping and transforming a single quadrilateral is naught for GPUs designed to process millions of triangles per second. Current GPUs typically have up to 64 MB of VRAM (some high-end cards now have up to","256 MB), which must be shared between the screen's frame buffer, texture maps, and other 3-D objects. Since this technique renders directly to the video buffer, output resolutions will be limited to the maximum frame buffer size allowed by the graphics card. Higher resolutions may be achieved by \u2018stitching\u2019 individual geo-rectifications together. For large, multi-MB images, the most critical times are those of moving the source data from the PC's main memory into the VRAM, processing to the frame buffer, and then reading the result back out. The advanced graphics port (AGP) interface is used for most graphics cards\/PC motherboards. The current 3.0 version (a.k.a. AGP 8x as suggested on the Intel AGP website: www.intel.com\/support\/graphics) supports transfers up to 2 GB\/sec, and unlike in previous versions, the bandwidth is symmetrical for reading and writing data. Nevertheless, if additional processing on the transformed imagery is required, it is extremely advantageous to leave it in the graphics card's VRAM and perform the calculations with the GPU. Depending on the algorithm, this may or may not be possible.","While particular operational sequences, materials, temperatures, parameters, and particular embodiments have been described and or illustrated, such are not intended to be limiting. Modifications and changes may become apparent to those skilled in the art, and it is intended that the invention be limited only by the scope of the appended claims."],"GOVINT":[{},{}],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"V. BRIEF DESCRIPTION OF THE DRAWINGS","p":["The accompanying drawings, which are incorporated into and form a part of the disclosure, are as follows:",{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":["FIG. 3","FIG. 2"],"sub":"pitch"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":["FIG. 4","FIG. 2"]},{"@attributes":{"id":"p-0018","num":"0017"},"figref":["FIG. 5","FIG. 4"],"sub":["g ","c"]},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":["FIG. 13","FIG. 11"]},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 14"}]},"DETDESC":[{},{}]}
