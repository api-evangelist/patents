---
title: Operation of mobile device interface using gestures
abstract: In general, this disclosure describes techniques for providing a user of a computing device (e.g., a mobile device) with the ability to utilize drawn gestures to operate objects displayed on a user interface of the computing device. Specifically, the techniques of this disclosure may, in some examples, include receiving user input comprising a gesture that defines an attribute associated with one or more target elements displayed in the user interface and graphically highlighting the one or more target elements in the user interface. The user may then utilize the drawn gesture to operate the highlighted target element by interacting with the defined selection area.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09021402&OS=09021402&RS=09021402
owner: Google Inc.
number: 09021402
owner_city: Mountain View
owner_country: US
publication_date: 20110923
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"p":["This application claims the benefit of U.S. Provisional Application No. 61\/386,441, filed Sep. 24, 2010, which is hereby incorporated by reference in its entirety.","Finger-based touch input, though intuitive, suffers from imprecision due to at least two known problems. One problem is that the area touched by a user's finger is, in some situations (e.g., with small mobile devices), much larger than a single pixel. (This is known as the \u201cfat finger\u201d problem.) This results in significant imprecision where the touchscreen device is rather small (e.g., a mobile device, such as a smartphone). This problem may be further amplified when the user is in motion, which further reduces the precision of his or her input movements. Another problem is that, given the small dimensions of many touchscreens, the pointing\/input finger often occludes the user interface (UI) target before touching the target on the touchscreen.","In one example, the disclosure is directed to a method comprising receiving, using a touch-sensitive device coupled to a computing device, user input corresponding to a drawn gesture, the drawn gesture defining an attribute associated with one or more target elements that are displayed in a graphical user interface output at the computing device, defining, in the graphical user interface, a graphical selection area that is associated with the drawn gesture, associating at least a first of the one or more target elements with the drawn gesture based at least in part on the attribute defined by the drawn gesture, and selecting the associated at least first of the one or more target elements in the graphical user interface in response to receiving, using the touch-sensitive device, user input associated with the defined graphical selection area.","In another example, the disclosure is directed to a computer-readable storage medium encoded with instructions that, when executed, cause one or more processors to perform operations comprising providing for display, in a graphical user interface of a computing device, a graphical selection area that is based at least in part on a drawn gesture received by user input using a touch-sensitive device, the drawn gesture defining an attribute associated with at least one target element displayed in the graphical user interface, emphasizing graphically the at least one target element, and selecting, by the computing device, the at least one target element in response to receiving user input associated with the graphical selection area.","In another example, the disclosure is directed to a computing device comprising one or more processors, a touch-sensitive device coupled to the computing device for receiving user input corresponding to a drawn gesture, the drawn gesture defining an attribute associated with one or more target elements that are displayed in a graphical user interface output at the computing device and a module operable by the one or more processors to define, in the graphical user interface, a graphical selection area that is associated with the drawn gesture, to associate at least a first of the one or more target elements with the drawn gesture based at least in part on the attribute defined by the drawn gesture, wherein the graphical selection area is larger in size than the associated at least first of the one or more target elements and is used to select the associated at least first of the one or more target elements in the graphical user interface.","The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other features, objects, and advantages of the disclosure will be apparent from the description and drawings, and from the claims.","In general, this disclosure describes techniques that facilitate selection of user interface (UI) targets displayed within a touchscreen (UI) of a computing device. The techniques leverage the visibility of graphical user interfaces and the casual interaction of gestures, and can be used to enhance a range of mobile interactions. In some examples, these techniques may be integrated with existing systems, such as mobile web browsers and other mobile applications. In the following discussion, one or more techniques of this disclosure may be implemented in conjunction with an operating mode of a computing device, which may be referred to as \u201cGesture Avatar mode.\u201d","Finger-based touch input, though intuitive, suffers from imprecision due to at least two known problems. One problem is that the area touched by a user's finger is, in some situations (e.g., with small mobile devices), much larger than a single pixel. (This is known as the \u201cfat finger\u201d problem.) This results in significant imprecision where the touchscreen device is rather small (e.g., a mobile device, such as a smartphone). This problem may be further amplified when the user is in motion, which further reduces the precision of his or her input movements. Another problem is that, given the small dimensions of many touchscreens, the pointing\/input finger often occludes the UI target before touching it on the touchscreen.","Although existing mobile user interface widgets are often designed to be easily operated by a finger, there may be a conflict between the imprecision associated with finger input and the high precision required for successful use of graphical user interfaces (GUIs). For example, UI widgets can consume precious screen space when designed to be large enough to be finger-operable. This is especially the case as the resolutions of mobile device displays rapidly increase. Further, many UI widgets included in a touchscreen web interface (e.g., hyperlinks or an embedded Flash player included in a mobile web browser) are often relatively small in size on the screen and thus difficult to operate using fingers. Although many solutions have been explored for addressing the above-described \u201cfat finger\u201d and occlusion problems, most require a high degree of precision in both user visual perception and motor control.","In contrast, some gesture-based techniques are better-tailored to allow casual user interaction without undue imprecision. Some examples are panning a map by swiping a finger across the touch screen or zooming in and out with pinching inward and outward motions. However, gesture-based interactions tend to lack the visual feedback provided by graphical user interfaces that include visually perceptible display\/interface components. Some existing solutions attempt to combine the advantages of both UI widgets and gesture-based techniques using, for example, rectilinear gestures to select graphical targets. However, some of these solutions can be limited in that they may require the target interface to be designed in a specific way. In addition, some of the existing systems focus mostly on one specific target acquisition step but leave other common interactions unexplored. Therefore, the applicability of many current solutions is often limited to certain functions and\/or applications on the device.","The techniques of this disclosure allow a user of a computing device (e.g., a mobile phone) to operate a user interface using gestures. These techniques may leverage the visibility of graphical user interfaces and the casual interaction of gestures. By virtue of the techniques described below, a user can dynamically associate a gesture shape with an arbitrary UI widget and then interact with the widget through the drawn gesture.","Techniques of this disclosure may be used to enhance a range of mobile interactions, resulting in potentially fewer errors during operation of a touchscreen computing device. The techniques of this disclosure may allow the user to operate or select smaller targets without requiring the level of input touch precision usually required for successful selection of such targets. The techniques of this disclosure may be integrated with a variety of functions and applications in existing system (e.g., a mobile web browser or other touchscreen-enabled application).",{"@attributes":{"id":"p-0026","num":"0025"},"figref":["FIG. 1","FIG. 1"],"b":["105","105","102","105","104","105","102","102","104"]},"In some examples, computing device  may comprise a mobile device. Computing device  may comprise or be part of a wireless communication device (e.g., wireless mobile handset or device), a video telephone, a digital multimedia player, a personal digital assistant (PDA), a video game console, a laptop computer, a tablet computer, or other devices. In some examples, computing device  may communicate with external, distinct devices via one or more networks (not shown), such as one or more wired or wireless networks, which may, in some cases, provide access to a network (e.g., a local area network, a wide area network, an intranet, or the Internet).","At least a portion of user interface  may be a presence-sensitive user interface device. The presence-sensitive user interface device may be, for example, a touch screen of computing device , responsive to, for example, tactile input via a user's finger or a stylus pen. Computing device  may execute applications such as application , with which a user of computing device  may interact. During execution, user interaction with user interface  may result in controlling operations of functions associated with application . The user may utilize tactile input to draw gestures that correspond to objects associated with functions or targets within application . The drawn gesture may occupy a large portion of user interface  and therefore, may be easily operable by the user. When the user manipulates the drawn gesture, the associated function or target is operated. In this manner, for application functions or targets that may otherwise require great precision and care in the location where the user touches the user interface, a drawn gesture that is more easily manipulated by the user may instead be used to operate the targets without requiring the same amount of precision in interacting with the user interface as that required when directly manipulating the target element itself.","In one example, the techniques of this disclosure may be implemented using an operating application running on computing device , where the operating application may utilize one or more modules or algorithms. When the operating application is running on computing device , user interface  may operate such that tactile input by the user may be interpreted for purposes of gesture drawing and not operating the touchscreen in the usual manner. In one example, the operating application may include a gesture module that acquires the drawn gesture and associates it with a target in an active application. The associated target may be graphically emphasized (e.g., highlighted) to indicate the association, and if it is the desired target, the user may manipulate the drawn gesture to operate the target, as will be described in more detail below.","In one example, the operating application may be operable on computing device  to perform, during execution, functions in accordance with the techniques of this disclosure. Computing device  may, in various examples, download or otherwise obtain the operating application from an external server via one or more networks (not shown). For example, a web browser hosted by computing device  may download one or more applications, such as the operating application, upon access of one or more web sites hosted by such as external server (e.g., web server).","During execution, the operating application may implement, invoke, execute, or otherwise utilize user interface  as a mechanism to obtain user input. For example, during execution, the operating application may invoke user interface  in gesture-based mode such that a user may provide input to draw gestures in user interface  to indicate a target that he\/she wishes to select or manipulate using the drawn gesture. The operating application may define a graphical selection area associated with the drawn gesture, which the user may utilize to interact with the drawn gesture. The graphical selection area may be an area defined around the drawn gesture, for example. The operating application may then associate the drawn gesture with a target (e.g., one of target elements , , or ) associated with an application executing on computing device , e.g., application . The user may then manipulate the drawn gesture, to operate the associated function or target.","User interface  may, during execution of the operating application, provide the user with at least one portion that is presence-sensitive and with which the user may interact via touch (e.g., by finger or a stylus pen) to draw gestures corresponding objects associated with application . A representation of the drawn gestures may be displayed on user interface  as the user draws the gestures. The operating application may include gesture-recognition capabilities, which may be capable of recognizing a drawn gesture and translating it into a matching character, e.g., a letter, a shape, or the like. As the user draws gestures on the portion of user interface  dedicated for gesture input, the operating application may associate the drawn gesture with a corresponding object in application . The association may be based on a first letter of a word, or shape of an operable object, or the like.","The drawn gesture may be associated with an object in application , where the target object may be displayed on user interface . An indication may be displayed to indicate the associated target object. In one example, the associated object may be emphasized, e.g., highlighted by displaying a box around the item, or displaying the target object in a different color or using a different pattern, or the like. The user may then utilize a gesture (e.g., touching user interface ) to interact with the drawn gesture to indicate desire to select the highlighted target object or manipulate it to perform a certain operation. In one example, interacting with the drawn gesture may result in executing an action associated with the target object (e.g., advance the playback of a video in a video playback application) or selecting the target object for further action (e.g., selecting a hyperlink in a browser application).",{"@attributes":{"id":"p-0034","num":"0033"},"figref":["FIG. 2","FIG. 1","FIG. 2","FIG. 2"],"b":["105","105","105","105","122","124","126","128","130","132","105","105","132","122","124","126","128","130","132","122","105","122","124","128"]},"User interface  may include, for example, a monitor or other display device for presentation of visual information to a user of computing device . User interface  may further include one or more input devices to enable a user to input data, such as a manual keyboard, mouse, touchpad, trackpad, etc. In some examples, user interface  may comprise a presence-sensitive user interface device such as, for example, a touch screen, which may be used both to receive and process user input and also to display output information. User interface  may further include printers or other devices to output information. In various examples in this disclosure, references made to user interface  may refer to portions of user interface  (e.g., touch screen) that provide user input functionality. In one example, user interface  may be a touch screen that is responsive to tactile input by the user (e.g., by user's finger or stylus pen).","Memory  may be configured to store information within computing device  during operation. Memory  may, in some examples, be described as a computer-readable storage medium. In some examples, memory  is a temporary memory, meaning that a primary purpose of memory  is not long-term storage. Memory  may also be described as a volatile memory, meaning that memory  does not maintain stored contents when the computer is turned off. Examples of volatile memories include random access memories (RAM), dynamic random access memories (DRAM), static random access memories (SRAM), and other forms of volatile memories known in the art. In some examples, memory  may be used to store program instructions for execution by processors . Memory  may be used by software or applications running on computing device  (e.g., operating application  shown in ) to temporarily store information during program execution.","Storage devices  may also include one or more computer-readable storage media. Storage devices  may be configured to store larger amounts of information than memory . Storage devices  may further be configured for long-term storage of information. In some examples, storage devices  may comprise non-volatile storage elements. Examples of such non-volatile storage elements may include magnetic hard discs, optical discs, floppy discs, flash memories, or forms of electrically programmable memories (EPROM) or electrically erasable and programmable (EEPROM) memories.","Computing device  also includes network interface . Computing device  may utilize network interface  to communicate with external devices (e.g., one or more servers, web servers, other computing devices) via one or more networks, such as one or more wireless\/wired networks. Computing device  may utilize network interface  in response to execution of one or more applications that require transferring data to and\/or from other devices (e.g., other computing devices, servers, or the like). Computing device  may include Wi-Fi or Bluetooth capabilities, for example, which may be configurable to establish communication with other devices through network interface .","Any applications implemented within or executed by computing device  (e.g., application  and\/or operating application ) may be implemented or contained within, operable by, executed by, and\/or be operatively coupled to processors , memory , network interface , storage devices , and\/or user interface . Application  may be one of many applications running on computing device  such as, for example, a multimedia application, a web browser, a word processing application, and so forth.","In one example, computing device  may include operating application , which allows a user to input gestures on computing device  to control operations within application  running on computing device . Operating application  may include a display module , a user interface controller , and a gesture module . Operating application  may provide or display user interface  on which a user may provide tactile input to draw a gesture that the user may utilize to operate application . Operating application  may be stored in memory  and\/or storage devices , and may be operable by processors  to perform various tasks during execution.","In one example, during implementation or execution of operating application , display module  may be operable by processors  to define at least a portion of user interface  to receive gestures via tactile user input. User interface controller  may be operable by processors  to receive, via user interface , user input specifying gestures drawn intended to define a shape or a character associated with an object or target associated with application . The user input may comprise contact with user interface  (e.g., contact with a touch screen) to draw gestures.","Gesture module  may be operable by processor  to determine, based on gestures the user draws on user interface , the appropriate matching objects or targets within application . The drawn gesture may be subsequently associated with the appropriate object or target in application . In one example, the object or target may be associated based on a matching shape, letter, or other types of visual characteristics or attributes associated with the object or target. In one example, display module  may define at least a portion of user interface  for gesture input. In one example, gesture module  may display the drawn gestures on user interface  and determine and associated the corresponding object in application . Subsequently, the associated corresponding object may be highlighted or indicated on the display to alert the user to the association. The user may then interact with the drawn gesture on user interface  to manipulate the object, where manipulating the object may result in performing an operation associated with the object, for example. In one example, the user's interaction with the drawn gesture may be in the form of moving the drawn gesture in different directions on user interface , which may result in moving the associated target object in application  in the same directions (e.g., advancing playback in a multimedia application). In another example, the user's interaction with the drawn gesture may be in the form of tapping the drawn gesture, which may result in activating the associated object in application  (e.g., selecting a hyperlink).","Operating application  may define at least a portion on user interface  where a user may use gestures to draw characters associated with an object or target in application  running on computing device . The characters may correspond to letters or shapes associated with objects displayed in a window associated with application . Furthermore, using additional gestures, the user may be able to apply operations to the associated objects in application , e.g., selecting, activating, moving, and the like.","Processor  may be operable to execute one or more algorithms including, for example, a gesture-interpretation algorithm. In one example, the gesture-interpretation algorithm may determine a character or shape corresponding to the drawn gestures. In some examples, the algorithm may associate the determined character and shape corresponding to drawn gestures with objects associated with application .",{"@attributes":{"id":"p-0045","num":"0044"},"figref":["FIGS. 3A-3C","FIG. 3A"],"b":["105","102","102","105","102"]},"As  illustrates, active application  may be running on computing device  and may be displayed in at least one portion of user interface . In one example, application  may include objects, which may correspond to specific operations or functions, or may be operable with tactile interactions. For example, application  may be a multimedia player application with widget , which may indicate the progress of playback within a multimedia file being played back in application . Widget  may be operable by tactile interaction to advance the playback to a desired spot (e.g., fast forwarding or replaying a certain portion). Widget  may be, for example, a media player scroll bar, which may be difficult to operate using a finger touch. Using the techniques of this disclosure, a user may activate Gesture Avatar mode, which provides the user with the ability to generate a drawn gesture to more easily operate and manipulate small targets on the user interface, e.g., widget . Implementation of Gesture Avatar may use various mode-switching techniques that we will discuss in more detail below.",{"@attributes":{"id":"p-0047","num":"0046"},"figref":"FIG. 3B","b":["102","308","104","306","105","102","108","306"]},"When the user completes drawing the desired gesture, by lifting his finger of user interface , for example, drawn gesture  may become highlighted, as  shows. Operating application  () may determine based on the shape of drawn gesture , the matching object in the active application (e.g., application ). Operating application  may then provide an indication of the best matching object in the active application, in this example widget . The indication may be for example, graphically emphasizing the matching object or displaying it differently from normal, for example. Once the corresponding object, e.g., widget , is highlighted and the user confirms that it is the object that he\/she wanted to control, the user may operate the object (e.g., widget ) using drawn gesture , as  shows. In this example, as the user moves drawn gesture , widget  moves accordingly. For example, the user may move drawn gesture  to the right by holding his finger down and swiping right, and as a result, the matching associated object, i.e., widget  moves in the same manner, therefore, advancing the playback of the multimedia, for example.",{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIGS. 3A-3C","b":["304","306","308","308","308","306","105"]},{"@attributes":{"id":"p-0050","num":"0049"},"figref":["FIGS. 4A-4B","FIG. 4A"],"b":["105","102","102","105","102"]},"As  illustrates, an active application (e.g., application  of ) may be running on computing device  and may be displayed in at least one portion of user interface . In one example, the active application may include objects, which may correspond to specific operations or functions, or may be operable with tactile interactions. For example, the active application may be a news website, which may include links to different sections and articles. The user may wish to navigate the to the \u201cSports\u201d section of the website. Using the techniques of this disclosure, the user may select the Gesture Avatar more, as discussed above, to be able to produce a drawn gesture using tactile input on user interface .","While in Gesture Avatar mode, the user may draw on user interface , a shape that looks like the letter \u201cS\u201d. In one example, the user may enter drawn gesture  anywhere on the screen or near the desired object, e.g., the \u201cSports\u201d link, as  shows. When the user completes drawing the gesture, the operating application (e.g., operating application  of ) may recognize drawn gesture . Additionally, the operating application may match drawn gesture  to an object in the active application (e.g., the active web browser window). As  shows, object  may match drawn gesture  and may be the nearest link to drawn gesture .","The operating application may indicate the association between object  and drawn gesture  by highlighting the associated object, as  illustrates. In this example, the bounding box around object  may indicate its association with drawn gesture . The user may then trigger the \u201cSports\u201d hyperlink or object , by tapping on drawn gesture , which is a larger target than the original target, e.g., the \u201cSports\u201d link on the screen. As we will be discussed below, to form the association between a drawn gesture and a target object, both the shape of the drawn gesture and its distance to potential targets on the screen may be leveraged.","As the example of  illustrates, a drawn gesture may provide a larger effective area for a user to comfortably interact with or operate a small target. The type of interaction that Gesture Avatar mode may provide is not limited to tapping-orientated interaction. A drawn gesture may provide the ability to translate most touch interactions with the touchscreen to the action corresponding to the target with which it is associated. For example, a user can long press a drawn gesture for long pressing the original target, e.g., to bring up a context menu.","In one example, a drawn gesture may be initially associated with an incorrect target due to the inaccuracy of gesture recognition or ambiguity of the targets on the screen. In one example, using the example of a web browser page, several targets may match the drawn gesture. For example, as  illustrates, when a user draws an \u201cS\u201d on a website containing several targets that match the drawn gesture (e.g., the letter \u201cS\u201d), the user may be able to interact further using the drawn gesture to select the desired target object.",{"@attributes":{"id":"p-0056","num":"0055"},"figref":["FIGS. 5A-5B","FIG. 5A","FIG. 5A","FIG. 5A"],"b":["102","508","103","508","103","508","104","506","508","508","506"]},"However, the nearest link may be the incorrect link in that the user may have meant to select a different target object. In one example, the user may dismiss the avatar by tapping outside of the avatar and redrawing the avatar. In another example, the user may re-assign a target to the avatar using directional (collinear) gestures, e.g., directional gesture . In this example, the user may want to select the link \u201cScience\u201d instead of the link \u201cSports,\u201d which is initially matched to drawn gesture . As  illustrates, the user may draw directional gesture  outside the region defined by drawing gesture , by swiping his\/her finger in the desired direction (e.g., up, down). As the user interacts using directional gesture , operating application  may determine the next best match in the direction defined by direction gesture . As  illustrates, using directional gesture  allows the user to change the matched object from \u201cSports\u201d to \u201cScience,\u201d for example. Operating application  may indicate the changed matching object by highlighting new target object , or in this example, the \u201cScience\u201d link. The user may repeat this process of using directional gesture , until drawn gesture  is associated with the desired target. In one example, the directional gestures used during Gesture Avatar mode may enable a user to easily navigate in the possible matches presented on the screen.","The examples of , A-B, and A-B illustrate using the Gesture Avatar mode in a computing device to generate a drawn gesture and utilizing the drawn gesture to operate or activate a target object in an application running on the computing device. The above examples are merely illustrative, and it should be understood that the Gesture Avatar mode in a computing device is not limited to widgets in multimedia applications and\/or hyperlinks in web browser applications. The techniques of this disclosure may include other characters and shapes, which may be associated with other types of target objects within applications. Other illustrative examples are shown in , which show other examples of interaction with a computing device application using drawn gestures in accordance with techniques of this disclosure.",{"@attributes":{"id":"p-0059","num":"0058"},"figref":"FIG. 6A","b":"608"},"Using the techniques of this disclosure, a user may be able to draw the character before (or after) the desired position of the cursor, and then tap on the right (or left) half of drawn gesture  to move the cursor. In this example, the user may wish to move cursor  in front of the letter \u201cZ\u201d in the already-entered text. The user may enter Gesture Avatar mode on the computing device and draw a gesture of the letter \u201cZ\u201d on user interface . Operating application  may then match drawn gesture  with a target object in the active application and associate drawn gesture  with the target object, as indicated by highlighting associated target object . The user may then tap on the right side or the left side of drawn gesture  to move cursor  either after or before associated target object .",{"@attributes":{"id":"p-0061","num":"0060"},"figref":["FIG. 6B","FIG. 6B"],"b":["102","618","618","616","618","616"]},{"@attributes":{"id":"p-0062","num":"0061"},"figref":"FIG. 7","b":["702","704","706","708"]},"The interaction with Gesture Avatar may involve four states, as illustrated in . The interaction process starts from the Initial state . Once the finger touches the screen, it enters Gesturing state . In Gesturing state , the touch trace is rendered on top of the graphical user interface that is being operated. When the user lifts his\/her finger and the trace is a valid gesture (e.g., the bounding box of the trace is large enough to be considered as a gesture instead of a tap), the process enters Avatar stage , in which the drawn gesture stroke forms the drawn gesture with an indication, e.g., a translucent background. The target object that is associated with the avatar may be also indicated, e.g., highlighted.","In Avatar stage , the user may choose to operate the associated target through the drawn gesture, or adjust the association if the drawn gesture is associated with an incorrect target object. At this stage, if the user touches down in the drawn gesture, the touch_down as well as the subsequent touch_move and touch_up events may be re-dispatched to the associated target in the user interface. If the user touches outside the drawn gesture, the user can either dismiss the drawn gesture by a tap or adjust the associated target using directional gestures.","Operating application  may determine the target object in an active application based on the drawn gesture. In determining the target object, the drawn gesture, g, is matched against a set of targets, T, available on the screen, so that the drawn gesture can be associated with the desired target, t, according to the following equation\u2014Equation 1:",{"@attributes":{"id":"p-0066","num":"0065"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":["t","match"]},"mo":"=","mrow":{"munder":{"mrow":[{"mi":["arg","m","a","x"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},{"msub":{"mi":["t","i"]},"mo":"\u2208","mi":"T"}]},"mo":"\u2062","mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["t","i"]},"mo":"|","mi":"g"}}}}}}}},"Both the semantics of the drawn gesture and its spatial relationship with the target objects on the screen may be leveraged. The combination of these two factors may effectively triangulate a desired target object on the screen. As a result, Equation 1 may be expanded as follows\u2014Equation 2:\n\n()=\u03b1()()\n\nwhere P(c|s) encodes how much the content of the target, c, matches the semantics of the drawn gesture, s, and P(p|b) captures the spatial relationship between a target and the drawn gesture, in particular, the distance between the gesture's bounding box, b, and the center of the target, p. The calculation of there two quantities is discussed below.\n","A drawn gesture may be a character or an arbitrary symbol or shape, and the content of a target may be a set of characters for a textual target (e.g., the initial letter of each word in a hyperlink, e.g., WN for \u201cWorld News\u201d) or the contour or shape of a graphical target (e.g., the triangular boundary of the play button of a media player). Character and shape recognition have several fundamental differences and distinct approaches have been developed in the field for handling each of these cases. Operating application  may initially determine a classification as to whether a gesture is a character or a shape, and then different recognizers are employed for each case. For character matching, operating application may employ, for example, a neural network handwriting recognizer that recognizes letters and numbers. The recognized character may then be used to search against the content of each target on the screen.","For shape matching, operating application  may use a template-based shape recognizer to which templates, e.g., the contour of each target on the screen, are added dynamically. The shape recognizer then finds the target that has the most similar contour to the drawn gesture. Therefore, rather than only keeping the best guess, this calculation gives a distribution of all possible characters and shapes fed into P(c|s) of Equation 2 above.","In addition to the above matching based on shape, operating application  may also perform matching based on spatial relationship. Since users tend to aim for the desired target, the position of the drawn gesture for interacting with the target may be a strong indicator of where the target is on the user interface. As a result, operating application  may also assume, at least initially, that the target is near the drawn gesture, and the farther an object is from the drawn gesture, the less likely that it is the desired target.","To capture this intuition, operating application  may utilize a 2D Gaussian distribution over the distance between the center of an object and the boundaries of the drawn gesture. In one example, operating application  may utilize the Manhattan distance of an object to the drawn gesture, to save computation when there are numerous objects on the screen, as illustrated in . In , the target is denoted as a circle, and the distance to the target is calculated based on the bounding box of the drawn gesture, which in this example is a rectangle, but may be any shape. The output of the Gaussian indicates how likely the object is the target, i.e., P(p|b).","The 2D Gaussian distribution may employ a changing variance that is dynamically adjusted based on the length of the longer edge of the gesture's bounding box, as  illustrates. This design of the algorithm implemented by operating application  may be based on the observation that users draw gestures in different sizes, and the smaller the drawn gesture is, the more specific area the user is targeting, or alternatively, the larger the bounding box, the less impact the distance has on the scoring.","Due to the possible inaccuracy of gesture recognition and the inherent ambiguity of objects available on the screen, operating application  may associate a drawn gesture with an incorrect object. For example, multiple objects that have similar content may be close to one another and to the drawn gesture. Furthermore, objects on screen edges may also be prone to incorrect matching, since it may be difficult for a user to draw gestures over these objects and the Gaussian distribution cannot efficiently capture this situation.","In one example, operating application  may correct a possible mismatch by asking the user to dismiss the drawn gesture and recreate it anew. However, in some situations, this approach may not be guaranteed successful and may be a nuisance if it is only a near miss, e.g., the drawn target is just next to the incorrectly-associated object. In one example of this disclosure, the user may be able to move to the next best match in a given direction according to a directional gesture that the user may indicate, as discussed above in the context of .","Based on the directional gesture that a user draws, operating application  may determine the next best match based on how well an object matches the drawn gesture semantically, and also based on the angular distance between the directional gesture and the direction from the currently matched object to the object being evaluated. A Gaussian function may be used to give each object a score based on it angular distance.  illustrates an example of a Gaussian function used in searching an area near the drawn gesture for matching objects, while restricting the search area to a certain angle, in accordance with techniques of this disclosure. To reduce computational cost, the search may be restricted to objects within the \u00b145-degree range of the direction.",{"@attributes":{"id":"p-0076","num":"0075"},"figref":["FIG. 10","FIGS. 1 and 2","FIG. 2","FIGS. 1 and 2"],"b":["105","128","122"]},"The method of  includes receiving, using a presence-sensitive user interface device (e.g., touch screen ) coupled to the computing device, user input instructing processor  to activate Gesture Avatar mode (). In Gesture Avatar mode, user interface  may be converted to an input forum for the user to draw gestures that the user may use to operate target objects displayed on user interface  in an active application (e.g., application ). In one example, to activate Gesture Avatar mode, the user may employ a hold-and-write mode switching technique, which includes pressing and holding a first finger (the holding finger), and starting to draw a gesture with a second finger (the gesturing finger). If the holding finger moves, Gesture Avatar mode may be deactivated and the user interface is operated in a normal manner in response to the user's touch. In another example, the Gesture Avatar mode may be selected from a menu item or a press of a dedicated button (soft or hard) on computing device .","Once in Gesture Avatar mode, the method further includes receiving, using a touch-sensitive device coupled to a computing device, user input corresponding to a drawn gesture, the drawn gesture defining an attribute associated with one or more target elements that are displayed in a graphical user interface output at the computing device (). The one or more target elements may be associated with an active application running on the computing device. The method also includes defining, in the graphical user interface, a graphical selection area that is associated with the drawn gesture (). The graphical selection area may be, for example, a transparent area around the drawn gesture with which the user may interact anywhere within the selection area. The method further includes associating at least a first of the one or more target elements with the drawn gesture based at least in part on the attribute defined by the drawn gesture (), and selecting the associated at least first of the one or more target elements in the graphical user interface in response to receiving, using the touch-sensitive device, user input associated with the defined graphical selection area ().",{"@attributes":{"id":"p-0079","num":"0078"},"figref":["FIG. 11","FIGS. 1 and 2","FIG. 2","FIGS. 1 and 2"],"b":["105","128","122"]},"The method of  includes receiving, using a presence-sensitive user interface device (e.g., touch screen ) coupled to the computing device, user input instructing processor  to activate Gesture Avatar mode (). Once in Gesture Avatar mode, the user interface may receive user input, a drawing of a gesture indicating a character or shape associated with a desired target object (). Processor  may employ algorithms to recognize the shape or the character associated with the drawn gesture and associate the shape or character with an object displayed on the user interface (). Associating the drawn gesture with the target object may be achieved by displaying an indication of the association, e.g., highlighting the associated target object. Processor  may then determine whether the user wishes to change the associated target object (). In one example, the user may indicate a desire to find another object to associate with the drawn gesture by utilizing a directional gesture, as described above.","If the user indicates wish to change the associated object, processor  may associate the drawn gesture with another object that matches the shape or character recognized for the drawn gesture (). This may be repeated, until processor  no longer receives a signal or indication that the user wishes to change the associated object. If the user does not indicate a desire to change the associated object, the method also includes receiving, using the presence-sensitive user interface, user input that activates the associated object by manipulating the drawn gesture (). The user input to activate the associated object may depend on the type of object. If the object is a link or an element that is operable with a click, for example, the user may tap on the drawn gesture to activate the object. In another example, the object may be operated in a different manner such as, for example, by pressing on the drawn gesture and moving it around, such as in the example of a widget on a multimedia application. In one example, once the object is operated or activated, processor  may exit the Gesture Avatar mode and return to normal operation. In another example, processor  may remain in Gesture avatar mode until the user indicates explicitly return to normal mode.","In one example of this disclosure, the operating application that controls the Gesture Avatar mode may be adaptable and may train the operating application as the user uses it. In one example, operating application may have access to all the information about the application running underneath user interface . One such example may be web pages where all information about the web pages' user interfaces may be accessed from the web pages' DOM. Therefore, to make Gesture Avatar a general technique, applicable in different user interfaces and\/or different computing devices, the knowledge that Gesture Avatar needs from the underneath user interface may be abstracted and formulated by defining a simple set of application programming interfaces.","The techniques described in this disclosure may be implemented, at least in part, in hardware, software, firmware, or any combination thereof. For example, various aspects of the described techniques may be implemented within one or more processors, including one or more microprocessors, digital signal processors (DSPs), application specific integrated circuits (ASICs), field programmable gate arrays (FPGAs), or any other equivalent integrated or discrete logic circuitry, as well as any combinations of such components. The term \u201cprocessor\u201d or \u201cprocessing circuitry\u201d may generally refer to any of the foregoing logic circuitry, alone or in combination with other logic circuitry, or any other equivalent circuitry. A control unit including hardware may also perform one or more of the techniques of this disclosure.","Such hardware, software, and firmware may be implemented within the same device or within separate devices to support the various techniques described in this disclosure. In addition, any of the described units, modules or components may be implemented together or separately as discrete but interoperable logic devices. Depiction of different features as modules or units is intended to highlight different functional aspects and does not necessarily imply that such modules or units must be realized by separate hardware, firmware, or software components. Rather, functionality associated with one or more modules or units may be performed by separate hardware, firmware, or software components, or integrated within common or separate hardware, firmware, or software components.","The techniques described in this disclosure may also be embodied or encoded in a computer-readable medium, such as a computer-readable storage medium, containing instructions. Instructions embedded or encoded in a computer-readable medium, including a computer-readable storage medium, may cause one or more programmable processors, or other processors, to implement one or more of the techniques described herein, such as when instructions included or encoded in the computer-readable medium are executed by the one or more processors. Computer readable storage media may include random access memory (RAM), read only memory (ROM), programmable read only memory (PROM), erasable programmable read only memory (EPROM), electronically erasable programmable read only memory (EEPROM), flash memory, a hard disk, a compact disc ROM (CD-ROM), a floppy disk, a cassette, magnetic media, optical media, or other computer readable media. In some examples, an article of manufacture may comprise one or more computer-readable storage media.","Various embodiments of the disclosure have been described. These and other embodiments are within the scope of the following claims."],"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF DRAWINGS","p":[{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":["FIG. 2","FIG. 1"]},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIGS. 3A-3C"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIGS. 4A-4B"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIGS. 5A-5B"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIGS. 6A-6B"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 8A"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 8B"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 11"}]},"DETDESC":[{},{}]}
