---
title: Method and apparatus for executing applications on a distributed computer system
abstract: The present invention provides a flexible and extensible execution control system for distributed computer systems including multiple nodes interconnected through a network. Distributed computer systems that include an execution control system consistent with the present invention can be used for more types of applications than systems using prior art. The applications running on a distributed computer system that includes an execution control system consistent with the present invention can achieve better protection from hardware and software failures than systems using prior art systems and methods.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07302609&OS=07302609&RS=07302609
owner: 
number: 07302609
owner_city: Huddinge
owner_country: SE
publication_date: 20040312
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION","Application, Application Modules, and Application Execution Model","Replication Of Application Modules","DESCRIPTION OF EMBODIMENTS OF THE INVENTION","Alternative Embodiments","Exemplary Applications"],"p":["This application claims priority to and incorporates by reference:","U.S. Provisional Patent Application No. 60\/454,510 titled METHOD AND APPARATUS FOR EXECUTING APPLICATIONS ON A DISTRIBUTED COMPUTER SYSTEM filed Mar. 12, 2003;","U.S. Provisional Patent Application No. 60\/508,150 titled METHOD AND APPARATUS FOR EFFICIENT ONLINE TRANSACTION PROCESSING filed Sep. 30, 2003; and","U.S. Provisional Patent Application No. 60\/519,904 titled METHOD AND APPARATUS FOR EXECUTING APPLICATIONS ON A DISTRIBUTED COMPUTER SYSTEM filed Nov. 14, 2003.","This invention relates to a method and system for executing application programs on distributed computer systems, including multiple nodes interconnected through a network, and to a method and system for protecting application programs from hardware and software failures.","Using a distributed computer system has three general advantages over using a single computer. First, it is possible to increase its performance by adding nodes. Second, a failure of one node does not impact the remaining nodes, which makes it possible for a software platform running on the distributed computer system to tolerate the failure. Tolerating node failures can greatly improve the overall availability of applications running on the distributed computer system. Third, it is possible to construct an inexpensive distributed computer system out of commodity parts, such as of small modular servers connected by high-speed Ethernet.","A distributed computer system typically includes a sophisticated software platform for applications to harness its power. The software platform includes an execution control system that can distribute the applications over the nodes of a distributed computer system and manage their execution. The main functions of the execution control system include starting of the application's modules on the nodes of the distributed computer system; handling of hardware and software failures; and balancing the applications' workload over the nodes of the system. In systems designed for continuous availability, the execution control system's functions also include coordinating the upgrades of applications and of the platform itself.","Software platforms and their execution control systems for distributed computer systems have been a subject of research and development since the advent of computer networks. We describe below the main forms of conventional systems related to software platforms for distributed computer systems and point out their limitations. The limitations are overcome by the system and method of the present invention.","The first form of system presently in use includes software platforms used in telecommunication systems, exemplified by Ericsson's Open telecom platform and An ATM transport and control platform. While these platforms achieve performance scalability and continuous availability, they suffer from several drawbacks that have prevented their use for applications outside of the telecommunication market segment. Their drawbacks include the following. Their application programming model is not standard, which inhibits adoption of the platform by application developers. Also, their application programming model is specific to the telecommunication application domain, and lacks the features required in other application domains. Further, their architecture is tightly coupled with the proprietary application programming model and cannot be easily extended to support other application programming models, such as the Java 2 Platform, Enterprise Edition (\u201cJ2EE\u201d) model. In addition, the applications of these systems use a proprietary, often complex, platform application programming interfaces (\u201cAPI\u201d) to achieve performance scalability and continuous availability. As each telecommunication platform uses a different API, application developers are unwilling to learn these APIs, which severely limits the adoption of the platforms. Also, some platforms require that applications be written in a proprietary programming language and rely on specialized proprietary hardware or operating system features. Additionally, the applications for some of the platforms include non-trivial execution control logic invoked by the platform's execution control system. The inclusion of such logic in the applications raises the barrier for developing applications. Furthermore, as the control logic is limited to a single application, the platform cannot easily optimize the execution of multiple applications, and the control logic in multiple applications can contradict each other. Some platforms cannot optimally distribute applications across the nodes of a distributed computer system if some nodes have higher CPU capacity than other nodes, and others require that the nodes all of the same type (for example, the nodes must be Intel processor-based computers running Linux). Lastly, some platforms, especially those for high-availability applications, dramatically restrict the network topology and sometimes require a \u201cstatic\u201d configuration (for example, an application can run only on two nodes and would not work if the system changed the number of nodes).","The second form of system presently in use includes high-availability clustering frameworks exemplified by Sun Cluster 3.0 Concepts and Windows Server 2003, Server Cluster Architecture. The main functionality of such a framework is to monitor a service and restart it on another node, should the node on which the service is running fail. The main drawbacks of these clustering frameworks include the following. The distribution model is limited to services with restartable processes. The platform starts and restarts the processes on the nodes according to some criteria associated with the service. This simplistic model does not work for applications that are composed of fine-grained distributable modules that are smaller than a process. For example, the service distribution model does not work for J2EE applications and cannot be easily extended to do so. Also, The framework does not provide a single-system view to management tools. Rather, each node is presented as an independent computer. The administrative complexity of administering each node individually prevents the frameworks from being used in distributed computer systems with large number of nodes. Additionally, most frameworks do not allow mixing nodes of different processor architectures, or nodes running different operating systems.","The third form of system presently in use includes platforms for distributed computer systems used for scientific applications such as Sun HPC 3.1 AnswerBook Collection from Sun Microsystems Inc. and Beowulf Linux Clusters. Their main drawbacks include the following. Their application programming model is limited to scientific applications. The programming model does not support other important applications types, such as transaction processing applications, Web services, database services, or telecommunication applications. Also, they do not provide the application availability typically required by enterprise and telecommunication applications, such as those listed in the previous item.","The fourth form of system presently in use includes clustered servers supporting Java 2 Platform, Enterprise Edition (\u201cJ2EE\u201d) applications such as WebLogic 7.0, Creating and Configuring Server Domains from BEA Systems. These clustered servers are designed to allow applications to execute on multiple identically-configured J2EE server processes and perform transactions on data in a shared database. The main limitations of this type of system include the following. The servers are limited to a single programming model (i.e. to the J2EE applications). If the execution environment includes other distributed components, such as Web or database servers, the execution of these other components is controlled by a different distributed software platform. Using multiple different software platforms increases the administrative complexity of the distributed computing environment. Also, with the exception of Web session state, which can be stored in the application servers' memories, J2EE applications are stateless (meaning that all their application state is stored in a database outside of the application). Therefore, J2EE clustering platforms cannot be used generally for applications that have in-memory state spanning transaction boundaries. For example, it would be difficult or impossible to use clustered J2EE servers to run distributed non-J2EE Java applications or telecommunication call-control applications with in-memory state. Additionally, they do not provide single-system images of the computing resources. Although the J2EE servers typically include some mechanism to automatically provision applications to multiple nodes, each node appears as an independent computer for system administration purposes. This makes the system harder to use with a large number of nodes.","The fifth form of system presently in use is described in two United States patent applications. The first is titled \u201cMethod and Apparatus for Providing Application Specific Strategies to a JAVA Platform Including Start and Stop Policies\u201d and has Ser. No. 09\/812,536. The second is titled \u201cMethod and Apparatus for Providing Application Specific Strategies to a JAVA Platform Including Load Balancing Policies\u201d and has Ser. No. 09\/812,537. These references describe how control modules included in Java applications customize how the execution control system starts, stops, recovers failures, and balances the application service modules. A similar system is described in Ericsson's A Java application platform, which uses the term \u201croot block\u201d instead of \u201ccontrol module\u201d. The main drawbacks of this type of system include the following. The mechanism seems to be limited to Java applications. Second, each application typically includes a \u201ccontrol module\u201d. The control module includes the implementation of callback operations invoked by the platform during starting, stopping, failure recovery, and load-balancing of the application. The development of these operations would likely require specialized expertise that is not common among application developers. Also, at a closer inspection of the description of such systems indicates that the control modules are likely to include execution control state. If the execution control state is lost as a result of a failure, it is not clear from the description of these systems how the lost state could be reconstructed. If the state could not be reconstructed, the applications associated with the failed control module must be stopped and restarted, at least in some systems, which is unacceptable for most applications that require continuous availability. Additionally, the applications are conventionally organized into a parent-child control hierarchy. This concept is not part of the standard Java application programming model and would be foreign to application developers. Finally, the control modules are written by application developers, who are not experts in the field of execution control. Therefore, the control modules are likely to contain more bugs than the platform that is developed and well tested by experts in the field. As a software error in one of the control modules could negatively impact the operation of the entire platform, including other applications, the use of control modules may reduce the overall reliability and availability of the entire distributed computer system.","Some prior art systems use replication as a technique for achieving tolerance to failure thereby increasing the availability of the application to its users. When replication is used, an application is partitioned into smaller execution modules and each execution module is replicated on two or more nodes of a distributed computer system. Many of the prior-art systems suffer from one or both of the following flaws. First, some systems using prior art distribute the replicas across the nodes such that after a failure of node, another node in the system will take over the entire workload of the failed node. The disadvantage of this approach is that the capacity of the system (and its price) is doubled in order to handle a node failure (these systems are sometimes called 2-N availability (or 2-N redundancy) systems because twice as many nodes are used in order to tolerate a single node failure). Second, some other systems of prior art distribute the replicas of execution modules across the nodes such that after a failure of any node, all the remaining nodes take over a portion of the workload of the failed node (these systems are often called N+1 availability (or N+1 redundancy) systems because only one extra node is required to tolerate a single node failure). The main problem with this approach is that if the number of nodes is high, the exposure to a double node failure could become significant. With a double node failure (when a second node fails shortly after the failure of a node), the state of an application could be lost because the state was replicated across the two failed nodes.","An execution control system for a distributed computer system consistent with the present invention can support more types of applications than systems using prior art by associating applications of the same type with an application controller that includes the control operations specific to the application type. The execution control system includes an execution controller, one or more node controllers, and one or more application controllers. Each node controller is associated with a node and has the capability to start and stop processes on the node. The execution controller maintains the information about the distributed computer system including the knowledge of the nodes that are present in the distributed computer system, the processes executing on all the nodes, the node groups defined in the distributed computer system, and the application controllers present in the system. An application controller implements the application execution model for a particular type of application. The application execution model specifies how an application includes distributable application modules, how the application modules could be associated with processes, and how the application modules should be managed by the application controller. The functionality provided by an application controller typically includes the starting of applications, stopping of applications, load-balancing of applications over the nodes, recovering applications from failures, and upgrading applications to a new version. The application controllers utilize the execution controller. To its users, the execution control system provides a single-system image of the distributed computer system and applications running on it. The execution control system enables the distributed computer systems to include nodes of different computer architectures and nodes that use different operating systems. The execution control system further uses node groups to simplify administration of distributed application on the distributed computer system.","The disclosure further describes replication constraints that an execution control system consistent with the present invention can use for replicating application modules across the nodes of a distributed computer system. A distributed computer system that includes replication constraints can protect applications better from hardware and software failure than systems using prior art and\/or it is easier to manage than systems using prior art.","The disclosure further describes how an application controller controls remote invocation and creation of server objects located in the execution modules. The advantages include location transparency of server objects; ability to relocate server objects; transparent failure recovery; partitioning of server objects; and on-demand creation of execution modules to store new server objects.","The various embodiments of the present invention include many features. The features include providing a flexible and customizable system for executing applications on distributed computer systems as well as allowing customers to execute on the same distributed computer system applications of different types, which could be written in different programming languages and according to different application programming models. The features also include, enabling the execution of many pre-existing applications without the need to modify them and allowing customers to extend the functionality of the execution control system by adding components developed by customers or third parties. Further features include providing for a distributed computer system comprising many individual nodes to be viewed as a single system by application developers and allowing a distributed computer system comprising a large number of individual nodes to be viewed as a single system from the perspective of system management and application deployment.","Various embodiments of the present invention also include additional features. The features including achieving continuous availability of applications even if the application programs or the operating system contain faults (software bugs) and even if some individual parts of the computer hardware fail. Other features include providing a flexible and customizable method for quickly (typically under one second) detecting and recovering applications from hardware and software failures and freeing the application developer from having to develop complex distribution and failure recovery logic in the application. Further features include providing a flexible method for load-balancing applications' work across the nodes of the distributed system, thereby achieving application scalability to a large number of nodes and allow a distributed computer system to include nodes with different computer architectures and operating systems. Also, the features include optimally using a distributed computer system that includes nodes with different mean-time between failures (MTBF) rating and allowing the applications and the software platform to be upgraded to a new version without stopping the applications or losing availability of the service implemented by the applications.","Various embodiments of the present invention also include other features. These features include optimally replicating the state of applications running on a distributed computers system across the nodes such that the probability of losing their state is minimized and allowing applications to specify in a flexible way how many replicas of their state should be created and how the replicas should be distributed across the nodes of the distributed computer systems. Additional features include achieving an optimal balance between reliability and system cost for applications using replication that are exposed to \u201cdouble failures\u201d and allowing applications to avoid failure exposures resulting from shared points of failure. Further features include allowing applications to exploit non-uniform network connectivity among the nodes and allowing applications requiring a resource to execute on a distributed computer system even if only a subset of the nodes includes the resource. One other feature is to allow concurrent execution of multiple applications on a distributed computer system even if the application uses different replication types.","An execution control system for a distributed computer system consistent with the present invention can support more types of applications than systems using conventional designs by associating applications of the same type with an application controller that includes the control operations specific to the application type. The execution control system includes an execution controller, one or more node controllers, and one or more application controllers. Each node controller is associated with a node and has the capability to start and stop processes on the node. The execution controller maintains the information about the distributed computer system including the knowledge of the nodes that are present in the distributed computer system, the processes executing on all the nodes, the node groups defined in the distributed computer system, and the application controllers present in the system. An application controller implements the application execution model for a particular type of applications. The application execution model specifies how an application includes distributable application modules, how the application modules could be associated with processes, and how the application modules should be managed by the application controller. The functionality provided by an application controller typically includes the starting of applications, stopping of applications, load-balancing of applications over the nodes, recovering applications from failures, and upgrading applications to a new version. The application controllers utilize the execution controller. To its users, the execution control system provides a single-system image of the distributed computer system and applications running on it. The execution control system enables the distributed computer systems to include nodes of different computer architectures and nodes that use different operating systems. The execution control system further uses node groups to simplify administration of distributed application on the distributed computer system.","The system of the present invention may be implemented with components or modules. The components and modules may include hardware (including electronic and\/or computer circuitry), firmware and\/or software (collectively referred to herein as \u201clogic\u201d). A component or module can be implemented to capture any of the logic described herein. For instance, the various controllers may be embodied as modules or components.","The disclosure further describes replication constraints that an execution control system consistent with the present invention can use for replicating application modules across the nodes of a distributed computer system. A distributed computer system that includes replication constraints can protect applications better from hardware and software failure (and is easier to manage) than systems presently in use.","The disclosure further describes how an application controller controls remote invocation and creation of server objects located in the execution modules. The advantages include location transparency of server objects; ability to relocate server objects; transparent failure recovery; partitioning of server objects; and on-demand creation of execution modules to store new server objects.","Execution Control System Concepts","Execution Control System Components",{"@attributes":{"id":"p-0085","num":"0084"},"figref":"FIG. 1","b":["101","101"]},"ECS  includes an execution controller , one or more node controllers , and one or more application controllers. Each application controller implements the execution model suitable for a particular type of applications supported by the distributed computer system.","For example, the exemplary ECS  in  includes three application controllers: a service application controller , which is suitable for controlling applications that are services  including operating-system level processes; a Java application controller , which is suitable for controlling Java applications  including containers and execution modules (container and execution modules are explained later in this disclosure); and custom application controller , which is suitable for controlling applications  that have a custom structure and use a custom application execution model.","The node controllers , execution controller  and application controllers are distributed over the nodes of a distributed computer system.  illustrates an exemplary distributed computer system including nodes interconnected by a network. The distributed computer system includes an execution control system. The exemplary distributed computer system includes six nodes but other embodiments of the invention may use fewer or more nodes. Each node in the distributed computer system includes a node controller.","Node A  includes the execution controller  and the service application controller . Node B  includes the Java application controller . Node D  includes the custom application controller .","In other embodiments of the invention, the components could be distributed over the nodes differently although each node may include a node controller. For example, while the exemplary system in  collocates the execution controller and the service application controller on the same node, in other embodiments, these two controllers might be on different nodes. Although the exemplary system in  includes three application controllers, other embodiments may include fewer or more application controllers, or include different application controllers than those illustrated in .","In some embodiments of the invention, the execution control system includes backup copies of the execution and application controllers located on different nodes. The backup copies are used to recover the state of the controllers in case of failure.","The nodes in the exemplary distributed computer system illustrated in  that include the parts of the execution control system also include application modules . Application modules are, for example, operating-system level processes and other types of executable objects such as execution modules of Java applications. In other embodiments of the invention, the application modules could be located on different nodes from the nodes that include the execution controller and the application controllers.","A distributed application (\u201capplication\u201d) is a computer program including one or more related application modules that could be distributed over the nodes of a distributed computer system. There are different types of distributed applications, each using different types of applications modules and each using a different application execution model. For example, while service applications use application modules that correspond directly to operating-system level processes, Java 2 Platform, Enterprise Edition (\u201cJ2EE\u201d) applications use two kinds of applications modules. The larger modules, which are called containers, correspond to operating-system level processes. The smaller modules, which are called \u201cexecution modules\u201d in this disclosure, include the application's Java classes loaded into the container.","An application execution model is a specification of how an application includes application modules and how the application modules shall be managed at runtime. The application execution model includes the rules for how many application modules should be created for an application; how the application modules should be distributed over the nodes of the distributed computer system; how the application modules could be associated with processes; how to redistribute the application modules to achieve more leveled load across the nodes; how to respond to failures; and how to upgrade the application to a new version.",{"@attributes":{"id":"p-0095","num":"0094"},"figref":["FIGS. 3 through 5","FIG. 3"],"b":["301","302"]},{"@attributes":{"id":"p-0096","num":"0095"},"figref":"FIG. 4","b":["401","402"]},{"@attributes":{"id":"p-0097","num":"0096"},"figref":"FIG. 5","b":["501","502","503","503","503","503"]},"It is apparent that the application execution model of the J2EE application depicted in  is different from the application execution model of the Web service and database service applications depicted in  respectively. While the application modules of the Web service and database service applications are processes, the structure of the J2EE application is more complex. The J2EE application uses a two-level application execution model that includes containers and execution modules as its application modules.","As described in detail later, the plurality of application controllers according to a method and system of the invention make it possible to support multiple, possibly very dissimilar, application execution models on the same distributed computer system.","Node",{"@attributes":{"id":"p-0101","num":"0100"},"figref":"FIG. 6","b":["601","602","603","604","605","605","601","606"]},"The node's main memory  is capable of including the program instructions and data of one or more processes . The node's main memory  also includes the instructions and data of the node controller .","Node Controller","Each node of a distributed computer system includes a node controller .  depicts the functionality provided by a node controller and its main components. A node controller  implements the following functions: node membership , master election , start process , stop process , and detect process failure .","The \u201cmembership\u201d function  includes sending messages to other nodes and receiving messages from other nodes in order to determine which nodes are currently participating in the distributed computer system. If one node controller detects missed membership messages from another node controller, it assumes that the other node has failed, or that the communication link between the two nodes failed. By periodically exchanging membership messages, the node controllers can reach agreement on the current set of nodes that are included in distributed computer system. This agreement is dynamically adjusted when nodes are added or removed from the distributed system, or when nodes fail and subsequently restart.","The \u201cmaster election\u201d function  is logic by which a node participates in a distributed master election protocol. The purpose of the master election protocol is to elect a node that will act as the \u201cmaster\u201d and another node that will act as the \u201cmaster backup\u201d. In the exemplary system in , node A has been elected the master and node B the master backup. If the master or master backup node fails, the master election protocol will quickly elect a replacement.","Although the membership and master election functions and their corresponding messages are depicted in  as separate functions, in some embodiments of the invention, these functions are combined into a single protocol to reduce the number of messages exchanged between the nodes. In other embodiments of the invention, these two functions could be implemented as two independent protocols.","When the master election protocol elects a node to be the master, the node controller on the master node starts the execution controller on that node. In the exemplary system depicted in , the execution controller is created on node A.","When the master election protocol elects a node to be the master backup, the node controller on the master backup node starts the execution controller backup on that node. In the exemplary system depicted in , the execution controller backup is created on node B. In some embodiments of the invention, leases are associated with the execution controllers to ensure consistency in the presence of node failures.","A node controller has the capability to start, stop, and supervise the execution of operating-system level processes (\u201cprocesses\u201d)  running on the node. The \u201cstart process\u201d  operation allows the node controller to start a new process with specified command line arguments. This operation is used by the execution control system to start processes that will be associated with application modules and to start parts of the execution control system itself. The \u201cstop process\u201d  operation allows the node controller to stop a process. This operation is used by the execution control system to stop processes and parts of the execution control system itself. The \u201cdetect process failure\u201d  function allows the node controller to detect that a process running on the node has failed. This operation is used by the execution control system to detect failures of processes and failures of the components of the execution control system itself. When a node controller detects a process failure, it sends an event notification to all the subscribers for the event. For example, the execution controller subscribes to the process failure events so that it can maintain an up-to-date state model of the processes in the distributed system.","The membership and master election protocol messages exchanged between the nodes of a distributed system are encoded in an architecture-neutral way. This allows the distributed computer system to include nodes of different hardware architecture.","A node controller provides the Node Controller Application Programming Interface (\u201cNC API\u201d)  that allows other components to invoke the node controller operations and to subscribe to its event notifications. The NC API  is primarily used by the execution controller  to manage the lifecycle of processes in the distributed computer system.","In some embodiments of the invention, the distributed computer system may include nodes with different operating systems. Each operating system may use a slightly different format of the operating system command line that is used to start a process (for example, the Windows operating system uses the backslash character as the file path separator while Linux uses the regular slash character). To shield the other components of the execution control system from the differences in the command line format, the NC API  uses a virtual, operating-system neutral format of the command line. The node controllers  have the logic to translate the virtual command line to the format understood by the local operating system.","In some embodiments of the invention, a node controller  is implemented using Java technology. In some embodiments of the invention, the NC API  is implemented as a set of simple text-based messages sent over TCP\/IP connections, which makes it possible for the node controllers  to be implemented in any language and for the nodes to run any operating system that supports the TCP\/IP protocol.","In some embodiments of the invention, the membership and master election protocol is based on the Paxos algorithm described in Lamport Leslie's Paxos Made Simple.","Node Groups","The distributed computer system might include a large number of nodes, which could make its administration tasks unwieldy. The method and system of the present invention utilizes the concept of \u201cnode groups\u201d to simplify the management of applications on the distributed computer system with a large number of nodes.","A node group is a named subset of the nodes in the distributed computer system. A node can be a member of multiple node groups. Using node groups in a distributed computer system has several advantages. First, node groups simplify many management tasks because instead of listing a large number of individual nodes when performing system a administration task, a node group name can be given instead. Second, it is easy to adjust the processing capacity or distribution of multiple applications by simply adding or removing nodes from a node group associated with the applications. For example, if a node is added to a node group, the application controllers can automatically distribute application modules of multiple applications to the node. Without the concept of node groups, the system administrator would have to distribute each application individually to the new node. Third, it is easier for administrators to refer to a set of nodes by mnemonic names that convey the function of the nodes, such as \u201cDatabase Nodes\u201d, than by the names of the nodes, which may not have any meaning to the administrators. Fourth, node groups could be used in the specification of replication groups and replication constraints according to a method of this invention. Replication constraints simplify the specification of high availability requirements for a distributed computer system and allow applications to be replicated across the nodes of the distributed computer system in a manner appropriate for the applications. In the present invention, node groups are used mainly for associating sets of nodes with applications with the purpose of distributing the applications' modules to those nodes.",{"@attributes":{"id":"p-0119","num":"0118"},"figref":["FIG. 8","FIG. 8","FIG. 8"]},"Execution Controller","The execution control system includes an execution controller. The main purpose of the execution controller is to provide an easy to use, fault-tolerant abstraction of the distributed computer system to the application controllers. The model of the distributed computer system provided by the execution controller includes nodes, node groups, and processes. Without the execution controller, each application controller would have to implement the execution controller's functionality, which would make the development of application controllers hard. It would also make achieving single-system image impossible because each application controller would include its own concept of processes and node groups, thus making the distributed computer system look like having a collection of several independent software platforms rather than a single software platform.","The functionality of the execution controller is depicted in . The execution controller  includes operations and a state model. The execution controller  interacts with application controllers , node controllers , and a system management tool .","The operations included in the execution controller  implement the management of node groups , nodes , processes , and application controllers . The operations describes below are typical to most embodiments of the invention, but some embodiments might omit some operations or include additional operations.","The \u201cnode group management\u201d  operations include the operations to create and remove a node group; operations to add and remove a node from a node group; and operations to obtain status information about the node groups.","The \u201cnode management operations\u201d  include operations to add and remove a node from the distributed computer system; an operation to respond to a node failure notification; an operation to respond to a notification indicating that a node has been repaired; and operations to obtain status information about the nodes.","The \u201cprocess management operations\u201d  include the operation to start an operating-system level process with specified command line arguments on a specified node; an operation to stop a previously started process; an operation to respond to a process failure notification; and operations to provide status information about processes.","The \u201capplication controller management\u201d  operations include operations to start an application controller and its optional backup copy on specified nodes; an operation to respond to an application controller or its backup copy failure notification; operations to add new application controllers to the system; operations to remove application controllers from the system; and operations to obtain information about application controllers.","The state model includes objects that present nodes , node groups , processes , and application controllers  in the distributed computer system. The state model also includes relationships among the objects. The execution controller maintains its state model objects up to date so that they reflect accurately the current state of the distributed computer system.","The illustration of the execution controller state model in  uses the notation for relationships used in class diagrams of the Unified Modeling Language (UML) described in Booch G., Rumbaugh J., and Jacobson I. The Unified Modeling Language User Guide. Addison-Wesley, 1999. A relationship between two objects is represented by a line between the objects. An optional number at the end of the line indicates whether an object can be associated with at most one, or with multiple instances of the other object. For example, the numbers at the end of the line between the Node  and Process  objects in  indicate that a Node object can be associated with multiple Process objects and that a Process object can be associated only with a single Node object. This UML-like notation is used also in the illustrations of the application controller's state models.","The execution controller  includes an event notification mechanism that sends event notifications to registered subscribers when an object in the state model has been created, removed, or its status changed. For example, an appropriate event notification is sent when a process has been started or stopped, or when the execution controller  has received a process failure notification from a node controller .","The execution controller  exposes its operations to the application controllers , system management tools , and other users via the Execution Controller Application Programming Interface (\u201cEC API\u201d) . The EC API  allows application controllers  to invoke the execution controller's  operations and subscribe to the event notifications generated in response to the state model changes. The EC API  is the primary means for the application controllers  to manage the execution of processes distributed over the nodes of the distributed computer system.","The EC API  could be used by other components in addition to the application controllers . For example, a system management tool uses the EC API  to define node groups and obtain status information about the nodes, node groups, and processes. The EC API  provides a single-system image of the distributed computer system including multiple nodes to its users.","In some embodiments of a method of the invention, the EC API  is bridged into a standard API used for system management, such as Java Management Extensions (\u201cJMX\u201d). This allows standard system management tools that conform to the JMX API to invoke the execution controller operations and to subscribe to its events.","The execution controller  communicates with node controllers  located on the nodes of the distributed computer system by using the NC API  provided by the node controllers . The NC API  allows the execution controller  to start and stop operating-system level processes on the nodes, and to receive a failure notification when a process fails.","The execution controller  is associated with its configuration file . The configuration file  includes information that the execution controller  uses at its startup. The information in the configuration file  includes the specification of the application controllers that the execution controller should create at startup; optional information specifying on which nodes each application controller should be started; optional information specifying on which nodes a backup copy of each application controller should be created; and optional specification of the node groups that should be created at execution controller startup.","In some embodiment of the invention, if the optional information for application controllers  is not specified in the configuration file , the execution controller  creates the application controllers  on the node that includes the execution controller , and their backup copies on the node that includes the backup copy of the execution controller . In some embodiments of the invention, the execution controller  is realized as a transaction processing application as described in U.S. Provisional Patent Application Ser. No. 60\/445,639 entitled Method and Apparatus for Online Transaction Processing and U.S. Provisional Patent Application Ser. No. 60\/508,150 entitled Method and Apparatus for Efficient Online Transaction Processing. Both of these provisional patent applications are incorporated by reference. It is also described in U.S. patent application Ser. No. 10\/774,207 entitled Method and System for Efficient Online Transaction Processing. This patent application is also incorporated by reference.","Application Controller","The execution control system separates application controllers from the execution controller. The execution controller includes the control functions that are common to all types of applications. Each application controller includes the control functions that are specific to a particular type of applications, thereby enabling the execution of applications of that type. Typically, an application controller can control the execution of multiple applications of the same type.","From the description of the invention, it will become apparent that the separation of application controllers from the execution controller yields advantages. These advantages include the ability to have multiple application controllers, each supporting different type of applications with a different application execution model. This feature makes the distributed computer system usable by a larger number of applications. Also, a customer can extend the execution control system by adding new application controllers developed by customers or by third parties. In this way, customers can use the execution control system for new types of applications that were not known or anticipated at the time the execution control system was developed. This extensibility by customers obviates the disadvantage of creating a new version of the entire execution control system each time the support for a new application type is desired.","An application controller is an abstract concept that can have many different implementations. Each implementation is specialized to support a specific type of applications. A description of two application controllers: the service application controller (\u201cSAC\u201d) and the Java application controller (\u201cJAC\u201d) are included. These two application controllers are illustrative rather than prescriptive of the method and system of the invention.","Replication Constraints and Replication Groups","Some applications executing on a distributed computer system replicate their application modules across two or more nodes to achieve tolerance to failures, thereby increasing the availability of the application to its users. One of the challenges in the development of applications that use replication is that the developer of the application might not know the exact configuration of the distributed computer on which an application will be deployed. Furthermore, it might be desirable that the same application could be deployed on several distributed computer systems, each having a different configuration, and each possibly requiring different assignment of application module replicas to nodes.","Therefore, in one embodiment the algorithm for assigning the replicas to nodes is not included in the application. Instead, the algorithm is included in the execution control system. However, it is possible, at the same time, for applications to \u201ccustomize\u201d the algorithm for assignment of application module replicas to nodes. Such customization might be useful, for example, to take advantage of application-specific information to increase the overall availability of the application, or to configure the application for a distributed computer system such that different application modules achieve different availability.","An execution control system consistent with the present invention may use the Replication Constraints and Replication Group objects described in the Replication Constraints and Replication Groups section to accomplish these and other goals that could arise in the development and deployment of applications with replicated application modules.","Service Application Controller","The service application controller (\u201cSAC\u201d) is one of the application controllers used in an embodiment of the invention. The service application controller implements the application execution model suitable for managing services.","Service Applications","A service is a type of a distributed application that includes one or more identical service processes that are distributed across one or more nodes. A service is associated with an operating-system level command line that SAC uses to start the service processes comprising the service. Examples of services that fit the SAC application execution model are: database servers, Web servers, file servers, name servers, directory servers, and others.","It is possible for a service process to start additional operating-system level processes. For example, a database service process could start several helper processes. From the perspective of SAC, these additional processes are logically included in the service process and are not necessarily individually managed by SAC.","Service Application Controller Components",{"@attributes":{"id":"p-0151","num":"0150"},"figref":"FIG. 10","b":["1001","1001","1002","1027","1003","1004","1005","1006","1007","1001","1008","1001","1009"]},"SAC  includes one or more Distribution Manager objects . The Distribution Manager objects  are explained later in this disclosure. SAC  includes a state model  including objects. The Service objects  represent services; the Node objects  represent nodes ; the Node Group objects  represent node groups ; and the Process objects  represent service processes . The notation used in the state model diagram is explained in the description of the execution controller state model in .","A Service object  represents a service. A Service object  includes the command line that is used to start associated service processes. Each Service object  is associated with a Distribution Policy object , a Node Group object , and one or more Service Process objects . The SAC  state model  can include multiple Service objects , each representing a service running on the distributed computer system.","The Distribution Policy objects  provide parameters to the algorithm of the Distribution Manager objects . They affect how many processes a Distribution Manager object  will create on behalf of a service  and how it will distribute the processes  over the nodes  of the associated node group . Exemplary Distribution Policy objects will be discussed later in this disclosure.","The Node Group objects  correspond to the Node Group objects  in the execution controller  and represent the node groups  defined in the distributed computer system. SAC  uses the EC API  to maintain its Node Group objects  synchronized with the Node Group objects  in the execution controller .","The Node objects  correspond to the Node objects  in the execution controller  and represent nodes  in the distributed computer system. SAC  uses the EC API  to maintain its Node objects  synchronized with the Node objects  in the execution controller .","The Process objects  represent service processes  running on some node  of the distributed computer system. A Process object  is associated with a Service object  and corresponds to a Process object  in the execution controller's  state model. SAC  uses the EC API  to maintain the state of its Process objects  synchronized with the state of the corresponding Process objects  in the execution controller .","SAC  includes an event notification mechanism that sends event notifications to interested subscribers when an object in the state model has been created, removed, or its state has been changed. For example, an appropriate event notification will be sent when a service has been started or stopped.","The service application controller  exposes its operations via the Service Application Controller Application Programming Interfaces (\u201cSAC API\u201d) . The SAC API  allows system management tools  and other system components to invoke the SAC operations  and to subscribe to its events. The SAC API  provides a single-system image of the distributed computer system including multiple nodes to its users.","In some embodiments of a method of the invention, the SAC API  is bridged into a standard API used for system management, such as Java Management Extensions (\u201cJMX\u201d). This allows standard system management tools that conform to the JMX API to invoke the SAC operations  and to subscribe to its events.","SAC  interacts with the execution controller operations by using the EC API . For example, SAC  uses the EC API  to start and stop processes on the nodes  of the distributed computer system. When SAC  is started, it reads its configuration file . In some embodiments of the invention, SAC is realized as a transaction processing application similar to those described in U.S. Provisional Patent Application Ser. No. 60\/445,639 entitled Method and Apparatus for Online Transaction Processing and U.S. Provisional Patent Application Ser. No. 60\/508,150 entitled Method and Apparatus for Efficient Online Transaction Processing. It is also described in U.S. patent application Ser. No. 10\/774,207 entitled Method and System for Efficient Online Transaction Processing. In some embodiments of the invention, a backup copy of SAC is used to protect the SAC  from failures.","Distribution Manager Customization","One of the main functions of SAC is to manage the distribution of service processes over the nodes of a distributed computer system. SAC gives users a lot of flexibility in customizing the distribution mechanism. SAC encapsulates its distribution algorithms in the Distribution Manager objects. A Distribution Manager object includes an algorithm for distributing service processes over the nodes of a node group. A Distribution Manager object uses the objects of the state model, such as Node and Node Group objects, to make its distribution decisions. SAC invokes internally the Distribution Manager objects in response to events that may require that the distribution of service processes be changed. SAC can include multiple Distribution Manager objects. Each Distribution Manager object may implement a different distribution algorithm suitable for a subset of services. In some embodiments of the invention, the Distribution Manager objects are implemented using Java objects.","The algorithm of a Distribution Manager object can be parameterized by the Distribution Policy objects. The Distribution Policy objects are objects that in some way limit, or give preferences to the Distribution Manager object's algorithm. A Distribution Manager object in some embodiments of the invention use two types of Distribution Policy objects: the NodeDistribution(N) and GroupDistribution(N) policy objects. Both types of policy objects take a numeric argument N. If a service is associated with the NodeDistribution(N) policy object, the associated Distribution Manager object in some embodiments will interpret the policy object such that the service requires that there be, at all times, N service processes on each node of the associated node group.",{"@attributes":{"id":"p-0165","num":"0164"},"figref":["FIG. 11","FIG. 11"],"b":["2","1102","1102","1103"]},"As the Distribution Policy object  for the service in  is NodeDistribution(), the Distribution Manager object  will handle the various events in the distributed computer system as follows. If a node is added to the node group associated with the service, the Distribution Manager object  will automatically create two processes on the added node. If a process fails, the Distribution Manager object will restart the process on the same node. If a node fails, the service processes running on the failed node are lost, but the service processes running on the remaining nodes will continue providing service.","The NodeDistribution(N) policy object is generally applicable for service including multiple service processes distributed over a number of nodes for service scalability and\/or availability purposes. An example of such a service is Web service including multiple Web server processes running on multiple nodes.","If a service is associated with the GroupDistribution(N) policy object, the associated Distribution Manager object in some embodiments will interpret the policy object such that the service requires that there be, at all times, a total of N service processes distributed in some way over the nodes of the associated node group.",{"@attributes":{"id":"p-0169","num":"0168"},"figref":"FIG. 12","b":["2","1201","1202"]},"As the Distribution Policy object  for the service in  is GroupDistribution(), the Distribution Manager object  will handle the various events in the distributed system as follows. If a node is added to the node group associated with the service, the Distribution Manager object will automatically try to take advantage of the new node by transferring some processes to the new node if this results in better distribution of the workload. If a process fails, the Distribution Manager object will restart it on the same node. If a node fails, the Distribution Manager object will restart the processes that were on the failed node on the remaining nodes of the node group. For example, if node B fails, the Distribution Manager object can restart service process  on node C.","Other embodiments of the invention might use other types of Distribution Policy objects. For example, some embodiments might use a LoadBasedDistribution(NodeLoad) policy object to achieve load-based distribution of processes. If the CPU utilization of the nodes including the processes associated with a service associated with the LoadBasedDistribution(NodeLoad) exceed the NodeLoad parameter, the Distribution Manager will automatically start a new process on a less loaded node in the node group. Such a policy object allows the system to dynamically grow and shrink the number of nodes used by a service depending on the service's current load. An example of using a LoadBasedDistribution(NodeLoad) is provided in the section regarding increasing an application's processing capacity section.","Method For Starting A Service",{"@attributes":{"id":"p-0173","num":"0172"},"figref":["FIGS. 13 and 14","FIG. 13","FIG. 14","FIG. 14"]},{"@attributes":{"id":"p-0174","num":"0173"},"figref":"FIG. 14","b":["1401","1402"]},"The SAC objects relevant to starting the exemplary service are: the Service object representing the service ; the Distribution Manager object  that includes the algorithm for distributing service processes over the nodes of a node group; the Distribution Policy object  specifying the distribution policy for the service; the Node Group object  representing the node group associated with the service; Node objects  A, B, and C representing their corresponding nodes of the distributed computer system; and Process objects   and  representing service processes  and . Service processes  and  are started on behalf of the service by the steps of the flow chart depicted in .","The execution controller state model objects relevant to starting the exemplary service are: the Node Group object  representing the node group associated with the service; Node objects  A, B, and C representing nodes A, B, and C, respectively; and Process objects   and  representing processes  and , respectively.","Node A  includes a node controller and service process  created by the start service method. Node B  includes a node controller and service process  created by the start service method. Node C  includes a node controller. There are no service processes created by the start method on node C.",{"@attributes":{"id":"p-0178","num":"0177"},"figref":"FIG. 14","b":["1420","1415","1401","1401","1416","1402","1402","1417","1401","1402","1418","1421","1421","1419"]},{"@attributes":{"id":"p-0179","num":"0178"},"figref":"FIG. 13"},"First the system management tool sends the \u201cStart Service\u201d message to SAC . In response to the message, SAC creates the Service and Distribution Policy objects in its internal state model . Then it associates the Service object with a Node Group object. The Distribution Policy object and the node group name either have been specified in the \u201cStart Service\u201d message, or are some default values provided by SAC. If there is no Distribution Manager object associated with the Node Group object, SAC also creates a Distribution Manager object and associates it with the Node Group object .","The Distribution Manager object uses the Distribution Policy object to determine how many service processes should be created on behalf of the service . The Distribution Manager object creates the representation of the processes in its internal state model . For the exemplary service, the Distribution Manager object creates the Process  and  objects.","For each Process object that has been created by the distribution manager in the SAC state model, SAC performs the following steps. The description of the steps illustrates the creation of service process . SAC sends the \u201cStart Process\u201d message to the execution controller . The message specifies the node on which the process should be created and the command line that should be used to start the process on the node. In some embodiments, particularly in those that include nodes with different operating systems, the command line may be sent in a virtual format that is translated to an operating-system specific format by a node controller. In response to the \u201cStart Process\u201d message, the execution controller creates Process  object in its state model . Then the execution controller sends the \u201cStart Process\u201d message to the node controller residing on the node on which the service process should be created . The message includes the command line that should be used to start the process. For process  of the exemplary service in , the message is sent to the node controller on node A.","The node controller receives the \u201cStart Process\u201d message. If necessary, the node controller translates the virtual command line to a format usable by the operating system running on the node . Then the node controller attempts to start the service process by issuing the command line to the operating system running on the node. The node controller checks if the process has successfully started . If the process has successfully started, the node controller sends the \u201cProcess Started\u201d message to the execution controller .","In response to the \u201cProcess Started\u201d message, the execution controller updates the state of Process  object in its state model to indicate that the process has successfully started . Then the execution controller sends the \u201cProcess Started\u201d message to SAC . SAC updates its internal Process  object in its state model to reflect that process  has successfully started . This completes the steps of starting a process. SAC checks if there are more service processes that should be started . If yes, the steps of starting a process are repeated. If no, the start service operation has completed.","If the check made by the node controller indicates that a process has not successfully started, the node controller sends the \u201cProcess Start Failed\u201d message to the execution controller . In response to the \u201cProcess Start Failed\u201d message, the execution controller marks Process  object as \u201cFailed\u201d . Then the execution controller sends the \u201cProcess Start Failed\u201d message to SAC . SAC marks Process  object in its state model as \u201cFailed\u201d . The Distribution Manager object applies the distribution policy associated with the service to determine the appropriate recovery action. In the case of the exemplary service, the Distribution Manager object could, for example, attempt to start the service process on a different node within the node group associated with the service, for example on node C.","In some embodiments of the invention the service processes are started sequentially as illustrated in the steps of the flow chart. In other embodiments, all service processes are started in parallel. In an alternative embodiment of the invention, the state model objects representing failed processes are removed instead of being marked as \u201cFailed\u201d.","Method For Handling A Process Failure",{"@attributes":{"id":"p-0188","num":"0187"},"figref":["FIGS. 15 and 16","FIG. 15","FIG. 16"]},"The steps of the flow chart in  are initiated when a service process, for example service process A  of our exemplary service illustrated in , fails . The node controller  on node A detects the failure  and sends a \u201cProcess Failed\u201d message to the execution controller . The execution controller  receives the message and marks Process A object in its state model as \u201cFailed\u201d . The execution controller then sends the \u201cProcess Failed\u201d notification to the subscribers of process failure events, which includes SAC .","When SAC  receives the \u201cProcess Failed message, it marks Process A object in its state model as \u201cFailed\u201d . SAC  reports the failed process to the system management tool  (assuming that the system management tool has subscribed to the event) and invokes the Distribution Manager object  to handle the process failure.","The Distribution Manager object  uses the Distribution Policy object associated with the service to determine how to handle the failure . The Distribution Policy object  of the exemplary service is GroupDistribution(), which means that, at least in some embodiments, there should be two service processes at all times. Therefore, the Distribution Manager object  makes the decision to create a replacement service process, service process B, on the same node. SAC  then uses the same steps to start the replacement service process that were used to start the original process . The steps were described previously in this disclosure as part of the description of the start service method illustrated in .","In an alternative embodiment of the invention, the SAC and EC state model objects representing failed processes are removed instead of being marked as \u201cFailed\u201d.","Method For Handling A Node Failure",{"@attributes":{"id":"p-0194","num":"0193"},"figref":["FIGS. 17 and 18","FIG. 17","FIG. 18"]},"The steps of the flow chart in  are initiated when a node, for example node A  in our exemplary distributed computer system illustrated in , fails . The membership protocol detects the node failure  and one of the node controllers  reports the failure to the execution controller by sending a \u201cNode Failed\u201d message to it .","When the execution controller  receives the \u201cNode Failed\u201d message it marks the Node object and its associated Process objects as \u201cFailed\u201d . In the case of the exemplary application in , the Node A  and Process A objects  are marked as \u201cFailed\u201d. The execution controller  reports the node failure to the management system and other subscribers registered to receive the node failure event notification . One of the subscribers is SAC . When SAC  receives the \u201cNode Failed\u201d event notification, it marks Node A object  as \u201cFailed\u201d . It also marks all its associated Process objects as \u201cFailed\u201d , including the Process A  object.","The SAC  invokes the Distribution Manager object  to recover the failure. The Distribution Manager object  uses the Distribution Policy object  in its decision on how to handle the node failure . As the Distribution Policy object for the exemplary service is GroupDistribution(), the Distribution Manager object in this embodiment should maintain, at all times, two service processes. Therefore, the Distribution Manager object will decide to start a replacement service processes, service process B , on node C . SAC  then uses the same steps to start the replacement service process that were used to start the original process. The steps were described previously in this disclosure as part of the description of the start service method illustrated in .","Method For Adding a New Node",{"@attributes":{"id":"p-0199","num":"0198"},"figref":["FIGS. 19 and 20","FIG. 19","FIG. 20"]},"The steps of the flow chart in  are initiated when an operator adds and starts a new node . In the exemplary distributed computer system in , the new node is node C .","When the node starts, it starts its node controller .  The node controller  starts exchanging membership protocol messages with other node controllers. Eventually, all the other node controllers detect the presence of the new node  and agree to allow the new node join the system.","One of the node controllers sends the \u201cNode Added\u201d message to the execution controller . When the execution controller  receives the \u201cNode Added\u201d message, it creates the Node C object  in its state model to represent the new node . Then it sends the \u201cNode Added\u201d notification to the system management tool  and other subscribers that registered to receive node-related event notifications . One of the subscribers is SAC . When SAC  receives the \u201cNode Added\u201d notification, it creates the Node C object  in its state model and associates it with the Node C object  in the execution controller .","If this is a new node, rather than a node that failed and restarted, the operator adds the new node to some node groups . For example, the operator can decide to improve the performance of the exemplary service illustrated in  by adding the new node to the node group used by the service. A node is added to a node group by operators sending the \u201cAdd Node to Node Group\u201d message to the execution controller. In response to the message, the execution controller creates the association between the Node Group object  and Node C object . Then the execution controller sends the \u201cNode Group Changed\u201d message to the subscribers registered to receive node group events . One of the subscribers is SAC . When SAC receives the \u201cNode Group Changed\u201d message, it updates its state model by creating an association between the Node Group object  and Node C object .","SAC  then invokes the Distribution Manager object  to determine how to take advantage of the new node in the node group. The Distribution Manager object  examines the Distribution Policy object  to decide on an appropriate action . The Distribution Policy object associated with the exemplary service is NodeDistribution() which specifies in some embodiments that there shall be, at all times, a single service process on each node of the node group. To comply with the policy object, the Distribution Manager object creates a new service process, service process , on the new node . SAC starts service process   using the steps that were described previously in this disclosure as part of the description of the start service method illustrated in .","Java Application Controller","The Java Application Controller (\u201cJAC\u201d) is another exemplary application controller used in the preferred embodiment of the invention. Although the description of JAC focuses on the execution control of Java applications, the system and method of the invention equally apply to any distributed applications written in any programming language if they have a similar application execution model to Java applications.","Java Applications","JAC is designed to control the execution of Java 2 Platform, Standard Edition (\u201cJ2SE\u201d) and Java 2 Platform, Enterprise Edition (\u201cJ2EE\u201d) applications. The J2SE and J2EE applications are collectively referred in this disclosure as \u201cJava applications\u201d. In the description of JAC, Java applications are often referred for short as \u201capplications\u201d.","Many Java applications are distributed applications. The term \u201cdistributed\u201d means that an application includes, at runtime, multiple application modules located possibly on different nodes of a distributed computer system. There are different ways to organize Java applications into distributable application modules.","The application execution model implemented by JAC uses a two-level approach to managing the distribution of applications over the nodes of a distributed computer system. The concepts of the two-level approach are depicted in . Each node  of the distributed computer system includes zero or more Java container processes  (\u201ccontainers\u201d). Each container  includes zero or more execution modules . An execution module  is a part of the distributed Java application and includes the program instructions and state (i.e. data). An execution module is typically realized by loading the classes from one or more Java Archive (\u201cJAR\u201d) files into a container. The state included in the execution module includes state elements. An example of a state element is a programming-language variable that represents the value of an account balance.","An example of an execution module is an executable Enterprise Java Bean (EJB) module (EJB modules are described in Sun Microsystems' Enterprise JavaBeans\u2122 2.1 Specification). The execution module's program instructions are the EJB business methods; the execution module's state is the content of the EJB CMP (container-managed persistence) and CMR (container-managed relationships) fields. For example, a \u201cdebit\u201d EJB business method is one exemplary embodiment of an execution module's program instructions and the \u201cbalance\u201d EJB CMP field is one exemplary embodiment of an execution module's state element.","JAC manages how the containers are distributed over the nodes of the distributed system and how the application execution modules are distributed over the containers.  also depicts the parts of the container used in an embodiment of the invention. The container  is a process that includes a Java Virtual Machine (\u201cJVM\u201d)  and application server classes . The application server classes  include Java classes that implement the operations that JAC uses to manage the lifecycle of the execution modules (such as the classes that implement the creation of execution modules) and Java classes that are provided as runtime library classes for the execution modules (such as classes that implement the Enterprise JavaBeans container functions).","JAC starts applications from applications definitions. An application definition could be a Java Archive File (\u201cJAR\u201d) or an Enterprise Application Archive File (\u201cEAR\u201d). The formats of the JAR and EAR file are standard and are specified in Sun Microsystems' Enterprise JavaBeans\u2122 2.1 Specification. Other embodiments of the invention might use a different format of the application definition.",{"@attributes":{"id":"p-0214","num":"0213"},"figref":"FIG. 22","b":["2201","2201","2202","2203","2202","2203","2204","2204","2204","2204"]},"The JAC used in an embodiment of the invention does not require that all the EM definition information be present in the deployment descriptor. If some information is missing, JAC will assume some appropriate default values that work for most applications. For example, JAC can treat the entire application as a single execution module that will be assigned to a default container group.","In its distribution algorithms, JAC uses the concept of a container group. A container group is a named set of containers distributed over a node group associated with the container group. The container group then logically includes all the nodes in the associated node group. Multiple container groups can be associated with a single node group. When a node is added to a node group, it is automatically considered added to all the container groups that are associated with the node group.",{"@attributes":{"id":"p-0217","num":"0216"},"figref":"FIG. 23","b":["1","2301","2","2302","1","2301","2303","2305","2306"]},"EM definition A  specifies that its associated execution modules shall be created in containers belonging to container group   and that each container shall include two execution modules. This specification is accomplished by including the EmContainerDistribution() policy object in the EM definition. The distribution policy objects are explained later in this disclosure.","Similarly, EM definition   specifies that its associated execution modules shall be created in containers belonging to container group   and that there shall be a single execution module in the container group, irrespective of the number of containers in the container group. This specification is accomplished by including the EmGroupDistribution() policy object.","Application definition   includes a deployment descriptor , which includes a single EM definition C . EM definition C  specifies that its associated execution modules shall be allocated to containers belonging to container group  , and that each container should include a single execution module. This is accomplished by including the EmGroupDistribution () policy object in the EM definition.  further illustrates that the exemplary distributed computer system includes two container groups, container group   and container group  .","Container group   includes three nodes, nodes A, B, and C. Container group   is associated with the NodeDistribution() policy object, which specifies that there shall be a single container on each node of the container group. Therefore, JAC creates three containers, one on each node of the container group.","Container group   includes all four nodes, nodes A, B, C, and D. Container group   is associated with the NodeDistribution() policy object, which specifies that there shall be a single container on each node in the container group. Therefore, JAC creates four containers, one on each node of the container group.",{"@attributes":{"id":"p-0223","num":"0222"},"figref":"FIG. 23","b":["2310","1","2"]},"In some embodiments of the invention, an application's execution modules include the application's state. The state is comprised of elements. For example, the state of a banking application may include the CMP and CMR fields of the Enterprise JavaBeans (EJB) objects representing customers' accounts. When an application is divided into execution modules, the application's state becomes partitioned across multiple execution modules. For example, referring to , the bank account EJB objects would be partitioned and distributed across the multiple execution modules. Each execution module would include a subset of the bank accounts.","In some embodiments of the invention, the algorithm for partitioning the application's state into partitions is application-specific. In some embodiments of the invention, the application's deployment descriptor may provide the partitioning information for the execution control system.","Advantages of partitioning the application's state across multiple execution modules include increased scalability and better availability. Increased scalability is achieved due to the execution modules could be assigned to different containers included in different nodes, thereby taking advantage of all the processors included in all the nodes. Better availability is achieved because when one execution module (or the container or node including the execution module) fails, other execution modules are unaffected and can provide services to the application's user. A method for achieving availability will be described in detail below.","Java Application Controller Components",{"@attributes":{"id":"p-0228","num":"0227"},"figref":"FIG. 24","b":["2401","2401","2402","2403","2402","2403","2402","2403"]},"The \u201cstart application\u201d operation  implements the algorithm for starting an application. JAC starts an application by starting containers and creating the execution modules associated with the application's EM definitions in the containers. The containers and execution modules are distributed in accordance to the distribution policies associated with the container groups and EM definition. A method for starting an application is described later in this disclosure.","The \u201cstop application\u201d operation  implements the algorithm for stopping an application. JAC stops an application by removing the execution modules associated with the application and optionally stopping the containers that are no longer used.","The \u201cupgrade application\u201d operation  implements the algorithm for upgrading an application to a new version of software. JAC orchestrates the upgrade by starting containers with the new version of the application server classes, creating execution modules using the new version of the application definition, removing the old execution modules, and stopping old containers.","The \u201crecover failures\u201d operations  implement the handling of various failures in the distributed computer system, including the failures of execution modules, containers, and nodes. In general, a failure is handled by restarting the failed execution modules and containers on some of the remaining nodes of the distributed computer system. In the preferred embodiment of the invention, JAC associates each execution module with a standby execution module. The standby execution module includes a copy of the execution module's state. If a failure results in the loss of the execution module's state, the backup execution module is used for failure recovery.","The \u201cbalance load operations\u201d  implement the algorithms for leveling the application workload across the nodes of the distributed computer system. If one node is becoming overloaded while other nodes have spare capacity, JAC may transfer some execution modules from the overloaded node to other, less loaded nodes.","The \u201crespond to hardware changes\u201d  operations implement the algorithms to redistribute the applications across the nodes of the distributed computer system in response to nodes being added or removed from the system. JAC uses its capabilities for transferring execution modules and starting and stopping containers to respond to hardware changes. The \u201cobtain application information\u201d  operations return status information about the running applications.","JAC  includes one or more Distribution Manager objects . A Distribution Manager object  implements the algorithm for how containers are distributed over nodes and how execution modules are distributed over containers. The algorithm of a Distribution Manager object is parameterized by distribution policy objects. The Distribution Manager and distribution policy objects are explained later in the disclosure.","JAC  maintains knowledge of the state of the distributed computer system and applications running on it in its state model .  illustrates representational JAC state model objects and relationships among the objects. The state model objects in  should be considered as illustrative rather than prescriptive.","The notation used in the relationships between state model objects is described in the description of . Each running application is represented in the state model by an Application object . Each Application object  is associated with one or more EM Definition objects . An Application object  corresponds to an application definition  from which the application has been started. An EM Definition object  includes the information from the EM definition in the application definition. Each EM Definition object  is associated with an EM Distribution Policy object  and one or more Execution Module objects . Every execution module  in the distributed computer system is represented in the JAC state model  by an Execution Module object . Each Execution Module object  is associated with an EM Definition object , and with a Container object . A Container object  represents a Java container process (\u201ccontainer\u201d) running on some node of the distributed computer system and is associated with a Process object in the execution controller state model.","Every container group  is represented in the JAC state model  by a Container Group object . Each Container Group object  is associated with a Distribution Manager object  which includes the algorithm for distributing containers and execution modules for this container group; a Container Distribution Policy object  which specifies how the Distribution Manager object  shall distribute containers over the nodes of the node group; one or more EM Definition objects  representing EM definitions assigned to the container group; and one ore more Container objects  representing the containers that belong to this container group.","As depicted in , a Container Group object  is also associated with a Node Group object  in the EC state model . A container group logically includes all the nodes the associated node group. As it was explained previously, the Node objects  and Process objects  within the EC state model  represent the nodes and processes in the distributed computer system.  also depicts the actual parts of the distributed computer system that are represented by the objects in the state model: container groups , node groups , nodes , containers , and execution modules .","JAC  is associated with a configuration file. At startup, JAC  reads the JAC configuration file  to discover, among other things, the default container group and default distribution policy object that the JAC shall use for the application whose EM definitions do not specify a container group or distribution policy objects. JAC includes an event mechanism. When a significant event has occurred, such as when an execution module or container has been created, JAC generates an event notification. The notification is sent to all subscribers who registered to receive the event.","JAC exposes its operations to the system management tool and other components via the Java Application Controller Application Programming Interface (\u201cJAC API\u201d) . The JAC API  allows the system management tool and other components to manage the lifecycle of application by invoking the JAC operations  and subscribing to the JAC event notifications. The JAC API  provides a single-system image of the applications running on a distributed computer system including multiple nodes to its users.","In some embodiments of the invention, the JAC API  is bridged into a standard API used for system management, such as Java Management Extensions (\u201cJMX\u201d). This allows standard system management tools that conform to the JMX API to invoke the JAC operations and to subscribe to its events.","In some embodiments of the invention, JAC is realized as a transaction processing application as described in U.S. Provisional Patent Application Ser. No. 60\/445,639 entitled Method and Apparatus for Online Transaction Processing and U.S. Provisional Patent Application Ser. No. 60\/508,150 entitled Method and Apparatus for Efficient Online Transaction Processing. It is also described in U.S. patent application Ser. No. 10\/774,207 entitled Method and System for Efficient Online Transaction Processing. In some embodiments of the invention, the execution control system maintains a backup copy of the JAC state model. The JAC backup copy is located on a different node than JAC. If JAC fails, the execution control system will recover JAC using the backup copy.","Distribution Manager Customization","One of the main functions of JAC is to manage the distribution of containers over the nodes of a distributed computer system and the distribution of execution module over the containers. JAC gives its users a lot of flexibility in customizing the distribution algorithm.","JAC encapsulates its distribution algorithms in the Distribution Manager objects. A Distribution Manager object includes an algorithm for how execution modules are distributed over the containers of a container group and how the containers of a container group are distributed over the nodes of a container group. JAC invokes the Distribution Manager object in response to events that may require that new execution modules or containers be created or removed, or that some execution modules be transferred from one container to another. A Distribution Manager object uses the objects of the state model to make its decisions. In some embodiments of the invention, a Distribution Manager object may store additional, distribution manager specific, information in the state model objects.","Many different implementations of the Distributed Manager objects may exist. In some embodiments of the invention, a \u201cdefault\u201d Distributed Manager object implementation is provided in the JAC implementation. In some embodiments, an application might include a custom Distribution Manager object in its application definition. When JAC starts the application, it loads the Distribution Manager object from the application definition. In other embodiments, a Distribution Manager object might be provided in a separate file from which JAC can load it when the Distribution Manager object is used.","JAC can include multiple Distribution Manager objects, each implementing a different distribution algorithm that might be suitable for a subset of the applications. The algorithm of a Distribution Manager object is parameterized by the distribution policy objects. The distribution policy objects are objects that in some way limit, or give preferences, to a Distribution Manager object's algorithm.","A Distribution Manager object in some embodiments of the invention uses the NodeDistribution(N) and GroupDistribution(N) policy objects that constrain the distribution of containers over nodes, and the EmContainerDistribution(N) and EmGroupDistribution(N) policy objects that constrain the distribution of execution modules over containers as follows. If a container group is associated with the NodeDistribution(N) policy object, the associated Distribution Manager object in some embodiments interprets the policy object as a requirement that there be, at all times, N containers on each node of the associated container group. If a container group is associated with the GroupDistribution(N) policy object, the associated Distribution Manager object in some embodiments interprets the policy object as a requirement that there be, at all times, a total of N containers in the associated container group, irrespective of the number of nodes in the container group.","If an EM definition is associated with the EmContainerDistribution(N) policy object, the associated Distribution Manager object in some embodiments interprets the policy object as a requirement that there be, at all times, N execution modules in each container that is part of the associated container group. If an EM definition is associated with the EmGroupDistribution(N) policy object, the associated Distribution Manager object in some embodiments interprets the policy object as a requirement that there be, at all times, a total N execution modules distributed in some way across the containers of the associated container group.","The Distribution Manager object in some embodiments of the invention allows an EM definition to specify a distribution policy object stating that a new execution module is to be created for each Enterprise JavaBean (\u201cEJB\u201d) object of a given EJB type. The Distribution Manager object in some embodiments of the invention controls the number of execution modules based on Service Level Agreements (\u201cSLA\u201d). If an application becomes overloaded and would not meet its SLA, the Distribution Manager creates additional execution modules in additional containers to increase the capacity of the application. The Distribution Manager object in some embodiments of the invention maintains one or more backup execution modules for each active execution module. The backup execution module includes the copy of the state of the active execution module and is used to recover the execution module from a failure. One embodiment of the invention that uses active and backup execution modules is described below.","Method For Starting Applications",{"@attributes":{"id":"p-0253","num":"0252"},"figref":["FIGS. 25 and 26","FIG. 25","FIG. 26"]},"The following description follows the steps in the flow chart in  and refers to the objects in . The shaded objects in  are objects that are created during the steps of starting the exemplary application.","A system management tool sends the \u201cStart Application\u201d message to JAC . In response to the message, JAC reads the application definition files and creates the Application object , EM Definition object , and EM Distribution Policy object  in its state model . The exemplary EM Distribution Policy object is the EmContainerDistribution() distribution policy object, which indicates that there shall be two execution modules in each container. The EM definition included in the application definition may specify the name of the container group to associate with the execution modules. If the container group is not specified in the EM definition, it might be specified alternatively in the \u201cStart Application\u201d message. If the container group name is specified neither in the application definition nor in the message, JAC would use a default container group specified in the JAC configuration file.","Then JAC checks if the container group exists . If the container group does not exist, JAC creates the Container Group object  in its state model and associates it with a Distribution Manager object .  The Distribution Manager object  could be specified in the application definition, in the \u201cStart Application\u201d message, or in the JAC configuration file. JAC associates the Container Group object  with a Container Distribution Policy object  that gives the Distribution Manager object  further information on how it should distribute containers over nodes. The exemplary Container Distribution Policy object  in  is GroupDistribution() which indicates to the Distribution Manager object that there shall be a total of two containers.","Now JAC invokes the Distribution Manager object  that decides how to distribute containers and execution modules. The Distribution Manager object  determines how many containers should be created and on which nodes. If the containers do not exist, the Distribution Manager  creates new Container objects . Then the Distribution Manager object  determines how many execution modules to create and how to distribute them over the containers . The Distribution Manager object  creates the Execution Module objects in the JAC state model that will represent the execution modules . The execution modules are initially in the \u201cCreating\u201d state.","If the Distribution Manager object created new Container objects, the containers represented by the Container objects are first created. JAC makes a check to see if any containers should be created . If some containers should be created, JAC creates them .","For the sake of brevity of the flow chart, it assumed that the containers have been created prior to initiating the start application method and therefore do not have to be created during the application start. If the containers did not exist when the application were started, JAC would create the containers using the same method that the SAC uses to start process including a service. The method is specified in detail in the description of .","Then JAC uses the following steps to create an execution module in a selected container. The steps are repeated for all execution modules that should be created. In some embodiments of the invention, the execution modules are created sequentially. In other embodiments, JAC creates the execution modules in parallel.","The Distribution Manager object  sends the \u201cCreate Message\u201d to the container  in which the execution module should be created . The container  receives the message and attempts to create the execution module . The container makes a check whether the execution module has been successfully created . If the execution module has been successfully created, the container  sends the \u201cEM Created\u201d message to JAC . JAC sets the state of the Execution Module object in its state model to \u201cReady\u201d to indicate that the execution module has been successfully created .","If the creation of an execution module fails, the container  sends the \u201cEM Create Failed\u201d message to JAC . JAC marks the Execution Module object in its state model as \u201cFailed\u201d and performs failure recovery by invoking the Distribution Manager object . The Distribution Manager object may, for example, attempt to retry the \u201cCreate EM\u201d message, or it may attempt to create the execution module in a different container.","While the flow chart illustrates that the distribution manager creates all the Container objects and Execution module objects in the state model in step  before the corresponding containers are created in step  and the corresponding execution modules are created in step , in some embodiments of the invention these steps might be done in different order or in parallel.","Method For Handling Node Failure","One of the functions of JAC is to recover applications from various failures in the distributed computer system.  illustrate a method used by JAC to recover applications from a node failure.  illustrates a flow chart including the steps of detecting and recovering an application from node failure and  depicts the objects related to an exemplary application that are relevant to understanding of the method.","The following description follows the steps in the flow chart in  and refers to the objects in . The shaded objects in  are objects that have been affected by the node failure or have been created during the recovery process. The method starts when a node  fails . A node could fail because of a hardware or software failure. All processes and execution modules on the failed node are lost. The node controller on the failed node stops sending membership protocol messages to the node controllers on the other nodes.","The node controllers on other nodes will miss the membership messages from the failed node and will reach agreement that the node has failed . One of the node controllers, for example, the node controller on node B in , will send the \u201cNode Failed\u201d event notification to the execution controller .","When the execution controller receives the node failure notification, it marks the Node object and all its associated Process objects as \u201cFailed\u201d . In the case of the exemplary application in , the Node A object  and Process  object  in the EC state model  are marked as \u201cFailed\u201d. Then the execution controller sends the \u201cNode Failed\u201d and \u201cProcess Failed\u201d event notifications to the subscribers that have registered to receive these event notifications .","One of the subscribers is JAC. JAC has registered to receive the \u201cNode Failed\u201d event notifications from the execution controller. When JAC receives the \u201cNode Failed\u201d event notification , it marks all the Container objects representing containers on the failed node as \u201cFailed\u201d . For the exemplary application in , Container  object  is marked as \u201cFailed\u201d. JAC marks as \u201cFailed\u201d also all the Execution Modules that represent the execution modules associated with the failed containers.  For the exemplary application, Execution Module A and B objects  are marked as \u201cFailed\u201d.","Now JAC invokes the Distribution Manager object  to perform recovery . The Distribution Manager object  examines the JAC state model to discover which objects have failed and will perform recovery according to its internal algorithm and the distribution policy objects associated with the Container Group object and EM Definition object. When performing failure recovery, the Distribution Manager object may create new Container and Execution Module objects in the JAC state model . In the case of the exemplary application in , the Distribution Manager object may create Container  object  and the Execution Module AA and BB objects .","JAC then creates the actual containers and execution modules that the Container and Execution Module objects represent . JAC uses the same steps to create the containers and execution module that were used to create them at the start of the application.","In some embodiments of the invention, JAC may utilize the backup execution modules to recover the state of the failed execution modules and thereby make the node failure transparent to the application users. In this case, JAC could initialize the replacement execution module with the state from the backup execution module, or it could enable the backup execution module to become the replacement for the failed execution module.","Advantages of the present invention include improved resilience to software and hardware failures, thereby improving the application's availability to its users. The term availability is usually defined as the percentage of the time that a system provides a minimal agreed-on level of service to its users. A system is called unavailable if it does not provide the minimal agreed-on level of service. As an example, a service level agreement might require that a system handle 1,000 transactions per second. Due to failures, the system could not meet the requirement for a total of 3 days of a year. Using the above definition of availability, the system achieved availability equal to (365\u22123)\/365=99.18%.","Telecommunications, financial services, and other applications that serve a large number of users typically require availability on the order of 99.999 to 99.9999%, which translates to 5 minutes to 30 seconds of unavailability each year (a telecommunication systems is in general considered available if it provides at least 80% of its capacity). The system and method of the present invention allow applications to achieve this very high level of availability without putting undue burden on the application developer, while maintaining low system cost.","The invention takes advantage of two techniques to achieve very high availability. First, embodiments partition applications into small, independently failing execution modules. If one of these modules fails, other modules remain unaffected. Partitioning of applications into execution modules is described elsewhere in the specification. Also, embodiments replicate the state of each application execution module on multiple nodes. Each copy of the application execution module is called a replica. If one replica of an execution module fails, other replicas of the execution module remain available, thereby allowing the application to provide service to its users.","Although this disclosure explains a method for achieving high availability in the context of Java applications and the Java Application Controller described elsewhere in the specification, other embodiments of the invention could use the method for achieving high availability in a different context. For example, in some embodiments, they could use the method for non-Java applications. In other embodiments, they could use it with a different or no application controller.","Replication Constraints And Replication Groups","To achieve protection from failures, the system maintains multiple copies of an execution module. These copies are called replicas. In some embodiments of the invention, each replica of an execution module is kept on a different node so that the execution module's state is protected not only from the failure of an execution module, but also from a failure of the node or the process that includes the execution module.","In some embodiments of the invention, all replicas of an execution module include both the execution module's program instructions and the state. Any of these replicas can perform the execution module's normal functions. In other embodiments, some replicas include only the execution module's state, but not the normal program instructions. These replicas that do not include program instructions are capable of keeping an up-to-date copy of the replica state, but they cannot perform the normal execution module's functions.",{"@attributes":{"id":"p-0280","num":"0279"},"figref":["FIG. 32","FIG. 32","FIG. 32"],"b":["100","199","200","299"]},{"@attributes":{"id":"p-0281","num":"0280"},"figref":["FIG. 33","FIG. 32"]},"In some embodiments of the invention, a single execution module's replica is assigned the active role while other replicas of the same execution module are assigned the backup role. Only the active replica performs the normal computation that the execution module is intended to carry out (for example, if the execution module includes the program instructions for banking transactions, only the active replica is configured to receive and execute users' debit and credit transactions). The active replica sends checkpoint messages to the backup replicas so that the state included in the backup replicas is synchronized with the state of the active replica. Different embodiments of the invention send checkpoint messages at different times. For example, in some embodiments used in transaction processing systems, the active replica sends a checkpoint message at transaction completion. The checkpoint message includes a description of the changes that a transaction has made to the state elements in the active replica. In other embodiments, checkpoint messages could be sent periodically (for example, every 5 seconds).","Different embodiments of the invention may take different approaches to handling a failure of a replica. There are several causes of replica failures, including the following ones: a replica may fail due to a programming error in its program instructions; a replica may fail because the process that includes the replica has failed; or a replica may fail because the node that includes the replica has failed. We will describe several exemplary approaches to dealing with replica failures.",{"@attributes":{"id":"p-0284","num":"0283"},"figref":["FIGS. 34-A","FIG. 34-A","FIG. 34-B"],"b":"34"},{"@attributes":{"id":"p-0285","num":"0284"},"figref":["FIGS. 35-A","FIG. 35-A","FIG. 35-B"],"b":"35"},"The advantages of hot-standby replication include very fast recovery from a failure. This is because when an active replica fails, the service provided by the execution module could be restored immediately by enabling one of the associated backup replicas (that's why the term \u201chot standby\u201d is used). On the other hand, with cold-standby replication, when the active replica fails, the state of a newly created active replica is created from the state included in a backup replica. This usually requires network communication and results in a short period of time in which the execution module does not provide service. The main disadvantage of hot-standby replication, when compared to cold-standby replication, is in decreased flexibility in load-balancing the system after a failure.","The advantages of cold-standby replication include: not requiring the program instructions to be included on the node that include only backup replicas; enabling the possibility to use a different (for example more compact) representation of the state in the backup replicas from the representation used in the active replicas; permitting a more flexible selection of the node on which a replacement active replica will be created after a failure of a node with the active replica; and potentially using a smaller number of nodes to include the backup replicas than the number of nodes that hold the active replicas (because in many embodiments, the backup replicas have smaller CPU capacity than the active replicas). The main disadvantage, when compared to hot-standby replication, is the increased period of unavailability of an execution module after a failure due to the time to create a new active replica from a backup replica. In some embodiments of the invention, the creation of replicas and the assignment of the active and backup roles are managed by an application controller.",{"@attributes":{"id":"p-0288","num":"0287"},"figref":["FIG. 36","FIG. 36","FIGS. 9 and 24"],"b":["3601","3601","3601","3602","3602","3601","3601","3603"]},"Although the Replication Constraint objects  and Replication Group objects  depicted in  are included in the Execution Controller (EC) , they do not have to be included in the EC  in other embodiments. One of the advantages of including them in the EC  is that they become available to all application controllers, thereby improving a single system image of administration of the distributed computer system. For example, multiple applications running under the supervision of multiple application controllers can be associated with the same replication constraint, thereby following the same replication rules.","Although the Replication Group objects  depicted in  illustrate a single \u201cbackup\u201d association between Replication Group and Node Group, in some embodiments of the invention there might be more than one \u201cbackup\u201d association between the Replication Group  and the Node Group .","The Java Application Controller (JAC) depicted in  illustrates how an application controller can take advantage of replication constraints. The JAC depicted in  is similar to the JAC depicted in , with the following differences. JAC associates EM Definition objects  with a Replication Constraint objects  included in EC. Also, JAC includes Execution Module Replica objects  representing the individual replicas of an execution module. In this particular embodiment, an execution module is associated with two replicas. One replica is assigned the active role and the other replica is assigned the backup role. In some embodiments of the invention, there could be more than one backup replica associated with an execution module.","In the embodiment of the invention depicted in , JAC uses the Distribution Manager objects, Replication Constraint policy objects, the Container Distribution Policy objects, and EM Distribution Policy objects when making decisions to which nodes and to which containers to assign execution module replicas. The objective of this flexible, policy-based distribution of replicas is to ensure high-availability of applications while maintaining distribution of the application workload across the nodes of the system. An exemplary method of using these policy object will be given in  through -O.","Replication constraints and replication groups could be used in systems consistent with the present inventions as follows.  illustrates an exemplary system that includes nodes, node groups, replication groups, and replication constraints. A node group is defined as a named set of nodes. Exemplary node groups in  are NG=(Node A, Node B, Node C) and NG=(Node D, node E, Node F).","A replication group is ordered set of node groups. The cardinality of the set (cardinality is the number of node groups in a replication group) is equal to the desired total number of replicas that should be created for an execution module. For example, the replication group RG=(NG, NG) in  has cardinality equal to 2 because it includes two node groups. In some embodiments of the invention, the same node group could appear multiple times in the definition of a replication group, as in the replication group RG=(NG, NG).","The specification of a replication constraint includes replication type, and one or more replication groups. A replication type specifies the type of replication that the execution control system should apply for the management of the replicas. The possible replication types include the Hot-Standby and Cold-Standby replication types. The system depicted in  includes one replication constraint RC=(HotStandby: RG).","In some embodiments of the invention, the application programmer or system administrator defines the node groups, replication groups, and replication constraints. The distributed computer system then observes the above rules when assigning execution module replicas to the nodes of the distributed computer system. For example, the replication constraint depicted in  constrains the assignment of replicas such that if the first replica of an execution module is assigned to one of the nodes A, B, or C, then the second replica of the same execution module is assigned to one of the nodes D, E, or F. The replication constraint further specifies that there should be two replicas of the execution module and that the replication type is Hot-Standby. For example, since one replica of the execution module Auction-1 is on node A, the second replica can be only on one of the nodes D, E, or F (in the example, it is on Node E).","One embodiment of an execution control system consistent with the invention uses the replication groups and constraints to control the assignment of replicas to the nodes of a distributed system as follows. If:\n\n","In some embodiments of the invention, including some of those that use the Cold-Standby replication type, the first replica (i.e. R) is always the active replica. In other embodiments of the invention, including some of those that use the Hot-Standby replication type, the active replica could be any of the R through Rn replicas.","In some embodiments of the invention, the last requirement (the one stating that nodes N though Nn are mutually different nodes) might be relaxed and allow replicas to be located on the same node. Using the same node for more than one replica might be useful, for example, in testing environments, or in systems in which the nodes are very reliable.","The above generic description covers replication of any cardinality. Many embodiments of the invention use two replicas: one active and one backup replica (i.e. cardinality is equal to two). For these embodiments, the above rules for controlling replica assignment to nodes could be re-phrased in a simpler way as follows. If:\n\n","We illustrate how the aforementioned rules are interpreted by using the exemplary system depicted in . The system includes six nodes. Two node groups, NG and NG, are defined with three nodes included in each node group. A replication group RG is defined that includes the two node groups. A replication constraint RC is specified such that it includes the Hot-Standby replication type and the replication group RG. A purpose of the replication constraint is to specify that if one replica of an execution module is assigned to a node that is included in the first node group, then the other replica of the same execution module should be assigned to a node included in the second node group. In other words, the replication constraint ensures that two replicas of the same execution module are not assigned to the nodes in the same node group.","The execution control system can assign the replica Auction-1 (active) to node A and replica Auction-1 (backup) to Node E because according to the aforementioned rules, if:\n\n","In contrast, in this embodiment replica Auction-1 (active) would not be assigned to node A and replica Auction-1 (backup) to node C, because there is no replication group that would satisfy the aforementioned rules.","A Method For Creating And Distributing Replicas",{"@attributes":{"id":"p-0305","num":"0323"},"figref":"FIGS. 38-A","b":"38"},"The objects used in the description of the flow charts correspond to the objects depicted in . In this particular embodiment of the invention, the \u201cdistribute\u201d method (by the term method we mean here, for example, a method of the Java programming language) of the Distribution Manager object associated with a container group is the entry point of the algorithm for creating containers and execution modules; and for assignment of execution module replicas to containers. The implementation of the method uses the Container Distribution policy objects associated with the container group, the EM Distribution policy objects associated with the EM Definition objects assigned to the container group, and the Replication Constraint objects associated with the EM Definitions objects.","The flow chart in  illustrates, at a high level, steps of the \u201cdistribute\u201d method of the Distribution Manager object. These steps include, if necessary, new containers are created on the nodes. Refer to the \u201cdistributeContainers\u201d method in . The steps also include, if necessary creating new execution modules. Refer to \u201ccreateExecutionModules\u201d method in . Last, if necessary, new replicas of the execution modules are created and assigned to the nodes and container. Refer to the \u201cdistributeEMReplicas\u201d method in .",{"@attributes":{"id":"p-0308","num":"0326"},"figref":["FIG. 38-B","FIG. 38-C","FIG. 38-D"]},{"@attributes":{"id":"p-0309","num":"0327"},"figref":"FIG. 38-C"},{"@attributes":{"id":"p-0310","num":"0328"},"figref":"FIG. 38-D"},{"@attributes":{"id":"p-0311","num":"0329"},"figref":["FIG. 38-E","FIG. 38-F","FIG. 38-G"]},{"@attributes":{"id":"p-0312","num":"0330"},"figref":["FIG. 38-F","FIG. 38-H"]},{"@attributes":{"id":"p-0313","num":"0331"},"figref":["FIG. 38-G","FIG. 38-H"]},{"@attributes":{"id":"p-0314","num":"0332"},"figref":["FIG. 38-H","FIG. 38-I"]},{"@attributes":{"id":"p-0315","num":"0333"},"figref":"FIG. 38-I","b":["38","38"]},{"@attributes":{"id":"p-0316","num":"0334"},"figref":["FIG. 38-J","FIGS. 38-K","FIG. 25"],"b":"38"},{"@attributes":{"id":"p-0317","num":"0335"},"figref":"FIG. 38-K"},{"@attributes":{"id":"p-0318","num":"0336"},"figref":"FIG. 38-L"},"In some embodiments of the invention, factors other than (or in addition to) the number of active execution modules are taken into consideration when picking the node and container for the active replicas. These factors might include any or all of the following: the CPU capacity of a node, the current load of the node, the amount of available memory on the node, service-level agreement for the application, security requirements, etc.",{"@attributes":{"id":"p-0320","num":"0338"},"figref":["FIG. 38-M","FIGS. 38-N","FIG. 25"],"b":"38"},"In some embodiments of the invention that associate more than one backup replica with each active replica, the above steps are repeated as many times as the desired number of backup replicas.",{"@attributes":{"id":"p-0322","num":"0340"},"figref":"FIG. 38-N"},{"@attributes":{"id":"p-0323","num":"0341"},"figref":"FIG. 38-O"},"In some embodiments of the invention, factors other than (or in addition to) the number of active and backup execution modules could be taken into consideration when picking the node and container for the backup replicas. These factors might include any or all of the following: the CPU capacity of a node, the current load of the node, the amount of available memory on the node, service-level agreement for the application, security requirements, etc.","The advantages of the above-described method for creating and distributing replicas include the fact that the distribution manager object could be designed and implemented as stateless. JAC can simply invoke the \u201cdistribute\u201d method at application start and then later at any time that the distribution of replicas are to be changed in response to some external events, such as a failure or system overload. The simplicity of coding the \u201cdistribute\u201d method is important because some applications may provide a custom distribution manager object that includes an implementation of the \u201cdistribute\u201d method. Furthermore, the \u201cstatelessness\u201d of the distribution manager's distribute method allow JAC to call it repeatedly in the case that one execution of the distribute method has been interrupted by another external event (for example, JAC can invoke the distribute method a second time in response to a second failure that occurred while handling the first failure).","A Method For Starting An Application","In some embodiments of the invention, an execution control system invokes the Distribution Manager's \u201cdistribute\u201d method to perform the initial distribution of containers and execution module replicas at the application's startup time. One of the advantages of the present invention is that the \u201cdistribute\u201d method can distribute the containers and replicas in a manner that best fits the application's availability requirements. This is possible because the distribute method could be application-specific (as it was described earlier in this disclosure, the distribution manager object that includes the distribute method could be included with the application or be a system-provided default distribution manager).","The flow chart in  is a modification of the flow chart in  to take into account multiple replicas of execution modules. The exemplary method for starting a Java application includes the following steps. The application is started by a system-management tool's sending the \u201cStart Application\u201d request to JAC. In response to the request, JAC creates the Application, EM Definitions, Container Group, Distribution Manager, and EM Distribution policy objects in the JAC state model. In some embodiments of the invention, the \u201cStart Application\u201d request may include also the definition of the Replication Constraints and Replication Group objects; in other embodiments, these objects are system-wide and are created when the system starts up. After JAC has created the objects in its state model, it invokes the \u201cdistribute\u201d method on the Distribution Manager objects associated with all the EM Definitions objects associated with the Application object. The \u201cdistribute\u201d method creates containers and execution module replicas according to the flow chart in  through -O.","A Method For Increasing An Application's Processing Capacity","Advantages of the present invention include allowing the execution control system to dynamically grow and shrink the number of containers and execution modules and their distribution across the nodes in response to changing application workload.","The flow chart in  includes steps of the algorithm used to increase an application's capacity by creating new containers and execution module replicas on an unused node. The algorithm includes the following steps. The execution control system determines that the application could use more resources to be able to service its clients. This could be determined, for example, by measuring the CPU utilization on the nodes that include the application's execution module replicas. JAC is informed of the condition. JAC marks the Container Group objects in its state model to indicate to the Distribution Manager objects that the container group could use more resources. JAC invokes the \u201cdistribute\u201d method of the Distribution Manager objects associated with the marked EM Definitions. The Distribution Managers perform the steps in the flow charts in  through -O. When the Distribution Manager checks the container policy distribution (See ), it discovers that the Container Group is associated with the LoadBasedDistribution(NodeLoad) policy object and that the Container Group object is marked as requiring more resources. It will check for an unused (or least loaded) node in the container group and it will create a new container on that node. The steps of the flow charts in  through -O will take advantage of the newly created container and will distribute some replicas of the execution modules to the container.","Methods For Recovering From Failures","In some embodiments of the invention, an execution control system invokes the Distribution Manager's \u201cdistribute\u201d method to create replacement containers and execution module replicas in response to failures. One of the advantages of the present invention is that the \u201cdistribute\u201d method can distribute the replacement containers and replicas in a manner that best fits the application's availability requirements. This is possible because the distribute method could be application-specific (as it was described earlier in this disclosure, the distribution manager object that includes the distribute method could be included with the application or be a system-provided default distribution manager).","The flow charts depicted in , -B, and -C include exemplary algorithms for recovering an application from a failure of execution module replica, container process, and node, respectively.","The flow chart depicted in  includes the steps of recovering an application from a failure of an execution module replica. The embodiment includes the following steps. An execution module replica fails. It could fail, for example, because of a software bug in the execution module's code. The container detects the failed replica. The container might detect the failed replica, for example by all or any of the following techniques: by monitoring the threads used by the replica; by monitoring software failures (for example uncaught programming language exceptions); by monitoring the amount of the memory used by the execution module; by monitor memory or object \u201cleaks\u201d; etc. The container sends \u201cEM failed\u201d message to JAC. JAC marks the replica as \u201cfailed\u201d in the state model. In some embodiments, JAC removes the state object representing the failed replica. JAC checks if the failed execution module replica was in the \u201cactive\u201d role. If it was, JAC attempts to promote one of the associated \u201cbackup\u201d replicas to the \u201cactive\u201d role by the following steps:\n\n","The flow chart depicted in  includes the steps of recovering an application from a failure of a container process (container). The embodiment includes the following steps. A container fails. The container could fail, for example, because of a software bug in the code implementing the container or in the code of a software library included in the container. The node controller (NC) included in the node that included the container detects the failure of the container. NC sends the \u201cProcess Failed\u201d message to the Execution Controller (EC). EC marks the corresponding Process object as \u201cfailed\u201d in its the state model. In some embodiments, EC removes the Process object from the state model. EC sends the \u201cProcess Failed\u201d message to JAC. This is because JAC registered earlier with EC to receive events related to the container process. JAC marks the Container object as \u201cfailed\u201d in its state model. JAC also marks as \u201cfailed\u201d all the Execution Module replica objects associated with the Container object. In some embodiments of the invention, JAC removes the failed Container and Execution Module Replica objects from the state model. In some embodiments of the invention, JAC sends a failure notification to the system-management tool. If some of the failed execution module replicas were in the \u201cactive\u201d role, JAC promotes their associated \u201cbackup\u201d replicas to the \u201cactive\u201d role. JAC uses the same EM promotion steps as it was described in . According to the description of , if no ready backup replica exists, the application is marked as \u201cfailed\u201d. In some embodiments of the invention, a failed application could be recovered from some external state. JAC invokes the \u201cdistribute\u201d method on the Distribution Manager objects associated with the Container Group object associated with the failed container. The \u201cdistribute\u201d method executes according to the steps of the flow chart in  through -O. It may create, if the Container Distribution policy object indicates so, a replacement container. It also creates replacement execution module replicas and distributes them according to the associated Replication Constraint object.","The flow chart depicted in  includes the steps of recovering an application from a failure of a node that includes containers and execution module replicas. The embodiment includes the following steps. A node fails. The node could fail, for example, because of a CPU or memory failure, because of power failure, or because of a software bug in the operating system included in the node. The node controllers (NC) on the remaining nodes detect the node failure by missing heart-beat messages from the failed node. One of the NCs sends the \u201cNode Failed\u201d message to the Execution Controller (EC). EC marks the corresponding Node object as \u201cfailed\u201d in its the state model. It also marks as \u201cfailed\u201d the Process objects that were associated with the failed node. In some embodiments, EC removes the Node and Process object from the state model. EC sends the \u201cNode Failed\u201d message to JAC. This is because JAC registered earlier with EC to receive events related to the nodes. JAC marks the Container and Execution Module Replica objects associated with the failed node as \u201cfailed\u201d in the JAC state model. In some embodiments of the invention, JAC removes the failed Container and Execution Module Replica objects from the state model. In some embodiments of the invention, JAC sends a failure notification to the system-management tool (this step is not illustrated in ). If some of the failed execution module replicas were in the \u201cactive\u201d role, JAC promotes some of their associated \u201cbackup\u201d replicas to the \u201cactive\u201d role. JAC uses the same EM promotion steps is described in . According to the description of , if no ready backup replica exists, the application is marked as \u201cfailed\u201d and could be, in some embodiments of the invention, recovered from external state. JAC invokes the \u201cdistribute\u201d method on the Distribution Manager objects associated with the Container Group objects affected by the failure. A Container Group object has been affected if any of its containers has been included on the failed node. The \u201cdistribute\u201d method executes according to the steps of the flow chart in  through -O. It may create, if the Container Distribution policy object indicates so, replacement containers for those that were included on the failed node. It also creates replacement execution module replicas (for those that were included on the failed node) and distributes them across nodes and containers according to the associated Replication Constraint object.","Other sections describe several example embodiments of the invention to illustrate some of the advantages of using Replication Constraints for managing how applications' execution module are replicated across nodes of a distributed computer system.","Configuring A System With Many Nodes To Improve Availability","One of the advantages (over the methods of prior art) of using replication constraints for achieving high availability is in their ability to provide better protection of an application from multiple simultaneous node failures at a low system cost.","In the following description, the term mean time between failures (MTBF) is defined as the mean time between failures of a single component in the system (for example, the mean time between two subsequent failures of the same node).","The probability that a component of a system will fail at any given time grows proportionally with the number of components. For example, if the MTBF of a node is 1 year (i.e. a node fails about once a year) and the system includes 100 nodes, it could be expected that the system will experience about 100 node failures each year.","A method of replication described in this disclosure protects an application not only from the failure of any single node, but also from many scenarios that include the failures of multiple nodes. If the execution module replicas that were on a failed node are replaced quickly enough so that replacement replicas exist before another node fails, the system remains available despite multiple node failures.","However, when a system includes a large number of nodes, there might be a non-trivial probability that after one node has failed, a second node will fail before replacement replicas have been created. If the first and second failed node both include the replicas of the same execution module, such a scenario would result in an unrecoverable failure. The mean time to unrecoverable failure in a system with N nodes that uses replication with cardinality equal to 2 is approximately (the result of the approximate is very close to actual number if SYNCT is much smaller than MTBF) equal to:\n\nTotal\/(*(1)*),\n\nwhere SYNCT is the mean time that it takes to create and synchronize replacement replicas for failed replicas and RFCF is a factor indicating the probability that the first and second failed nodes both include replicas of the same execution module, thereby leading to an unrecoverable failure. The approximate formula also assumes that a failed node will be repaired quickly (on the order of hours or days). As an example, if MTBF=1 year, SYNCT=25 minutes, RFCF=1, and N=100, the Total_MTBF would be about 2 years. In other words, the system would experience an unrecoverable failure approximately every 2 years despite the replication mechanism.\n","Some prior art systems improve the availability of the system by using twice as many nodes as would be required if failures weren't considered and configuring the replicas such that Node(N\/2+i) includes the backup replicas for active replicas located on Node(i). While this configuration provides good availability, it requires twice as many hardware resources (it is called 2N availability (or 2N redundancy)). In the above formula, RFCF is equal to 1\/N.","Other prior art systems reduce the system capacity requirements by distributing the backup replicas of the active execution modules located on a given node across all other nodes (the configuration is called N+1 availability (or N+1 redundancy) because it requires only the extra capacity of a single node to protect N nodes). The main problem with this configuration is that RFCF is equal to 1, and therefore the Total_MTBF might be too low for systems with many nodes.","One of the advantages of the replication constraints and replication groups of the present invention is that they allow configuring the system such that it is possible to choose the desired tradeoff between the system cost (measured by the number of nodes) and achieved availability (measured by Total_MTBF).","By using replication constraints it is possible to limit the number of nodes that include the backup replicas associated with the active replicas included on any given node. The reduction of nodes including the backup replicas for any given node reduces the risk of exposure to double node failure.","The system depicted in  includes 100 nodes. The node groups, replication groups, and replication constraint illustrated in the figure effectively divide the nodes into 10 regions. If an active replica is included in a node in a region, then the backup replica is included in another node in the same region. The computer system configuration achieves both low cost and high availability. Low cost is achieved by requiring only 11% more nodes (100 instead of 90) to protect the system from node failures. Total_MTBF for the computer system is 20 years, assuming MTBF=1 year and SYNCT=25 minutes. The configuration results in RFCF being equal to 10%.","Configuring a System for Applications with Different Availability Requirements","Advantages of using replication constraints include the capabilities to configure a system for applications with different. availability requirements. Some applications can achieve higher performance while running with lower protection from failures, whereas other applications running on the same system can achieve higher protection from failures at the cost of achieving lower performance. In some embodiments, different availability requirements might exist for different execution modules of the same application.",{"@attributes":{"id":"p-0352","num":"0373"},"figref":"FIG. 42"},"In the example, the Catalog EM Definition has lower availability requirements and is configured to use one active replica and one backup replica. This is accomplished by associating the Catalog EM Definition with the RC replication constraint that includes the RG replication group with cardinality equal to two.","In contrast, the Auction EM Definition has higher requirements on availability. Therefore it is configured to use one active replica and two backup replicas. This is accomplished by associating the Auction EM Definition with the RC replication constraint that includes the RG replication group with cardinality equal to three.","As depicted in , the replication constraints instructs the system to create 3 replicas of each Auction execution module (there are two Auction execution modules in the figure: Auction-1 and Auction-2, each with three replicas) and 2 replicas of each Catalog execution module (there is a single execution module Catalog-1 with two replicas).","The replication requirements for Catalog EM Def could be informally described as \u201ccreate two replicas of each Auction execution module and assign them to two different nodes\u201d. The replication constraint for Auction could be informally described as \u201ccreate three replicas of each Auction execution module and assign them to three mutually different nodes\u201d.","Configuring A System That Includes Nodes With Different Reliability","Advantages of using replication constraints include the capability to control precisely how applications are replicated across the nodes of varying reliability (nodes with different MTBF). This fine-grained control over replication is necessary in some embodiments of the invention to ensure application availability.","In the exemplary system depicted in , nodes A and B are more reliable than nodes C, D, E, and F. The objective of the replication constraint RC associated with the Auction EM Definition is to ensure that one replica of each Auction execution module is assigned to one of the reliable nodes while the other replica is assigned to one of the less reliable nodes. This is accomplished by defining the replication group RG that includes the group of reliable nodes and the group of unreliable nodes. RC includes RG.","The objective of the replication constraint RC associated with the Catalog EM Definition is to ensure that there are three replicas of each Catalog execution module and that these replicas are assigned to three different nodes in the unreliable node group. This is accomplished by defining the replication group RG that includes three times the UnreliableNG node group.","Informally speaking, replication constraint RC achieves protection of the Auction execution module replicas by ensuring that at least one replica of each execution module is included on one of the reliable nodes, and RC achieves protection of the Catalog execution modules by using three replicas of each execution module.","Configuring A System with A Shared Single Point of Failure","Advantages of using replication constraints include the capability to achieve high availability of applications on distributed computer systems that include a single point of failure whose failure could bring down more than one node in the system. Using an appropriate replication constraint ensures that the replicas of an execution module are not placed on nodes that shared a common single point of failure.","The exemplary distributed computer system depicted in  includes six nodes that are physically grouped into two enclosures. The nodes in the same enclosure share a common power supply. If the power supply fails, all the nodes in the enclosure will fail. Therefore, it is desirable that the replicas of an execution module are assigned to the nodes such that one replica is on a node included in Enclosure  while another replica is assigned to a node that is included in Enclosure .","The replication constraint and replication group depicted in  achieve this objective. According to the rules for replication constraints, the active replica of Auction-1 execution module is assigned to node A in Enclosure  while the backup replica is assigned to node E in Enclosure . Similarly, the active replica of execution module Auction-2 is assigned to node F in Enclosure  while the backup replica is assigned to node B in Enclosure . Both execution modules Auction-1 and Auction-2 are protected from a failure of either power supply.","Configuring A System with Non-uniform Network Connectivity","Advantages of using replication constraints include the capability to assign replicas to nodes of a distributed computer system in which the communication bandwidth or latency between a pair of nodes is different than the bandwidth between another pair of nodes.","In some embodiment of the invention, some execution modules may heavily communicate with each other. Therefore, it is desirable that the execution module replicas are assigned to the nodes in such a way that the execution modules communicating with each other can use the higher bandwidth and\/or shorter latency for communication.","The exemplary distributed computer system depicted in  includes six nodes that are physically connected using two fast networks. The fast networks are connected with each other using a slower network connection (e.g. via Wide Area Network). As a result, for example, node A can communicate with high bandwidth with nodes B and C, but only with slow bandwidth with nodes D, E, and F.","The distributed computer system in  also includes the execution modules associated with two EM Definitions: Cell and Dir. The Cell execution module implements the functionality of a cell controller (it is called usually a base station controller) in a mobile network, and the Dir execution module includes directory entries of telephone subscribers. The Cell execution modules communicate heavily with each other and therefore should use the fast network only. The Dir execution modules do not communicate with each other and therefore could be located on any node. However, because the number of checkpoint messages from the Dir active replica to its corresponding backup replica is high, in some embodiments the active and backup replicas of each Dir execution module are connected with a fast network.","The replication constraints and replication groups depicted in  achieve these objectives. They ensure that the Cell execution modules are included only in nodes A, B, and C which are connected with a fast network, while they permit placing the Dir execution modules on any node. The definition of the RC replication constraint ensures that the active and backup replica of each Dir execution module can communicate using the fast network.","Configuring a System in Which Some Nodes Lack Resources","Advantages of using replication constraints include the capability to configure applications on distributed computer systems in which only some of the nodes have access to a resource required by some execution modules. Example of such resources might be an external database or specialized hardware board.","The exemplary distributed computer system depicted in  includes six nodes where only nodes A and B have access to a database located on a dual-ported disk. The Catalog execution modules require access to the database on the disk whereas the Auction execution modules do not.","The replication constraints and replication groups depicted in  ensure that the Auction execution modules will be assigned only to nodes A and B, which have access to the disk, while the Auction execution modules will be assigned only to nodes C, D, E, and F, which do not have access to the disk.","Configuring a System to Use Separate Nodes for Active and Backup Execution Modules","One of the advantages of using replication constraints is that a distributed computer system can execute applications comprising execution modules such that the active replicas of the execution modules are located on one set of nodes while the backup replicas are located on another (possibly much smaller) subset of nodes. If a node that includes active replicas fails, the system can re-create new active replicas on the remaining nodes of the active subset in a way that optimizes the current workload.","In some embodiments of the invention, the cold standby replication type is used to accomplish this objective. The distributed computer system depicted in  includes six nodes. In this embodiment, the active replicas of the Auction execution modules be assigned to nodes A, B, C, and D, whereas their backup replicas be assigned to nodes E and F.","The replication constraint and replication group depicted in  achieves this objective. The replication constraint uses the ColdStandby replication type, which ensures that the active replicas are assigned only to node in node group NG (which is the first node group included in the definition of replication group RG).","If one of the nodes that include the active replicas failed, replacement active replicas would be created on another node in node group NG. For example, if node A failed, the replacement of replica Auction-1 (active) would be created on one of the nodes B, C, or D. The Auction-1 (backup) backup replica could not be promoted to the active role because it is not a node permitted to include active Auction replicas. Instead, the state of the Auction-1 (backup) replica would be used to reconstruct the state of the replacement Auction-1 (active) replica.","Configuring a System with Multiple Replication Types","One of the advantages of using replication constraints is that a distributed computer system can include applications associated with different replication types, each providing an optimal replication and distribution policy for different execution modules.","The exemplary system in  depicts the Auction EM Definition associated with the replication constraint RC and the Catalog EM Definition associated with the replication constraint RC. RC specifies the Cold-Standby replication type while RC specified the Hot-Standby replication type.","The RC replication constraint ensures that the active replicas of the Auction execution modules are assigned to nodes A, B, C, and D while their backup replicas are assigned to nodes E and F.","The RC replication constraint allows assigning the Catalog execution modules to any node in the system, while ensuring that the active and backup replicas of the same execution module are on different nodes.","Configuring a System to Take Advantage of Application-specific Knowledge to Improve Availability","One of the advantages of the present invention is its flexibility in taking advantage of application-specific criteria when distributing the execution module replicas. This capability is enabled by allowing the distribution manager object to be supplied by the application rather than by the execution control system.","An example application that could take advantage of this capability is a base station controller (BSC) used in cellular telephone networks. A BSC is an application that controls multiple cells in a network. The BSC application is partitioned into execution modules such that an execution module corresponds to a cell (we will call these execution modules \u201cCell execution modules\u201d).","Highly populated areas are typically divided into many cells such that each location is covered by more than one cell. A BSC can improve the availability of mobile telephone service by taking advantage of overlapping cells. If an execution module corresponding to one cell fails, the telephony service could still be provided by the execution modules corresponding to other cells that overlap the area of the failed cell. In order to maximize availability, a \u201cclever\u201d BSC implementation could distribute the Cell execution module replicas in a way that minimizes the impact of any node failure. For example, it is desirable that the active replicas of two overlapping cells be included in two different nodes of the distributed system. In this way, a clever BSC would achieve two levels of protection from failures: one level is by usual replication of each execution module and the second level is by taking advantage of overlapping cells.","In systems using prior art, the distribution of the execution module is performed by the system and cannot take advantage of application-specific knowledge (such as the knowledge of overlapping cells). In contrast, a system that uses the present invention could configure an application-specific Distribution Manager object that could be aware of overlapping cells and could distribute the Cell execution module replicas accordingly. In one embodiment of the invention, the algorithm depicted in  would take the distribution of overlapping cells into consideration and would assign active replicas to nodes to minimize the impact of a node failure on overlapping cells.",{"@attributes":{"id":"p-0391","num":"0412"},"figref":"FIG. 50","b":["1","2","4","7"]},"Method of Starting Execution Control System",{"@attributes":{"id":"p-0393","num":"0414"},"figref":"FIG. 29"},"Support for Server Object Invocation and Creation","In some embodiments of the invention, execution modules include server objects that could be invoked remotely by client programs. In some embodiments, client programs are written in the Java language, but many other embodiments of client programs exist.","The client programs performing server object invocations could be, but are not limited to, programs included in other execution modules within the same or other distributed computer system, and programs running outside of any execution module. Some examples of server objects are: Enterprise JavaBean objects, CORBA objects, Java Remote Method Invocation (RMI) remote objects, and server object accessible by the Simple Object Access Protocol (SOAP).","In some embodiments, a client program is included in a client module. A client module includes communication libraries that allow the client program to communicate with a server object. A client module may also include a stub for the remote server object. A stub (stub is also called remote proxy or remote object reference) is a programming language object that a client program uses to invoke operations on a server object as if the server object were a local object located in the client module. In some embodiments, a remote operation supported by the server object corresponds to a programming language method defined in the stub object. In many embodiments, the programming language class that implements a stub is generated automatically by software tools. In some embodiments, a client module is a process that includes the client program, Java Runtime Environment (JRE), a communication library for communicating with the server objects located in the execution modules, and stubs. Many other possible embodiments of a client module exist.","The method of the present invention for applications that include server objects includes four advantages. First, the location of a server object is transparent to the client programs. A client program does not necessarily know the location of the server object. In some embodiments, when a client program invokes a programming language method on a server object's stub located in the client module, the stub obtains the location of the execution module in which the server object is located from the application controller. The stub in the client module might cache the execution module location to avoid request to the application controller on future invocations of the server object. A server object could be relocated transparently to the client program. In some embodiments, an application controller can move the location of an execution module from one container to another container, possibly located on another node. The communication protocol between the client module and the application controller ensures that the stub finds the up-to-date location of the execution module that includes the server object. Second, server object failover could be provided transparently to the client program. If the execution module that includes the server object is replicated in an active and backup execution modules, the application controller sends the client module the location of the active execution module. If the active execution module fails and the backup execution module is promoted to become the active execution module, the communication protocol between the client module and the application controller ensures that the stub finds the location of the currently active execution module with the server object. Third, server objects could be partitioned transparently to the client program. In some embodiments, server objects might be partitioned across multiple execution modules according to some partitioning information. In some embodiments, a client program requests the creation of a server object by invoking a programming language method (\u201ccreate method\u201d) on a factory stub. The create method might have parameters. A factory stub is a programming-language object located in the client module and is used for sending server object creation requests to an execution module. If server objects are partitioned, the factory stub determines in which execution module to create a server object. The application controller provides to the factory stub partitioning information. The factory stub uses the partitioning information and the create method parameters to determine the execution module in which to create the server object. The factory stub might store the partitioning information received from the application controller in a partitioning information cache located in the client module and use it for subsequent invocations of create methods on the factory stub. Fourth, a client program can cause the creation of a new execution module by creating a new server object. Some applications require that each server object of a specified type be created in a new execution module (for example, a Base Station Controller application might require that each Cell object be located in its own execution module). When a client program invokes a create method on a factory stub, the stub sends a request to application controller to determine in which execution module to create the server object. The request includes the server object type and the create method parameters. In some embodiments, the application controller creates a new execution module (or an active and backup execution modules if replication is desired) and sends the location of the execution module to the factory stub.","The following paragraphs describe in detail the application controller's support for server object invocation and creation.",{"@attributes":{"id":"p-0399","num":"0420"},"figref":"FIG. 51"},"A client program  is included in a client module . The client module  includes a stub  that is used by the client program  to perform remote operations on a server object  included in an execution module . The execution module  is included in a container process  running on a node . The stub  includes execution module identifier (\u201cEM ID\u201d)  that uniquely identifies the execution module that includes the server object and a reference to the application controller (\u201cAC Ref\u201d)  that controls the execution of the application that includes the server object . The AC Ref  allows the stub  to communicate with the application controller . To improve performance, the client module  also includes a location cache  that stores entries that map an EM ID  to a network address of the container (\u201cContainer Address\u201d)  that currently includes the execution module identified by EM ID . The client module  can use the Container Address  to open a connection to the container  and send messages to it, including remote invocation request messages that cause the container to invoke operations on the server object.","The state model  included in the application controller  includes the Execution Module object  that contains the information about the corresponding execution module. This information includes the container in which the execution module is located. In some embodiments, this information is represented in the state model by an association between the Execution Module object  and the Container object . The Container object  includes the Container Address that allows client modules to communicate with the container.","In some embodiments of the invention, the application controller  keeps track of the client modules that might cache the location information of an execution module. In some embodiments, the application controller  stores this information in the state model as remote object references of the location cache (\u201cLocation Cache Reference\u201d). If the application controller  moves the execution module to another container, the application controller  can use the Location Cache Reference to update the location information in a client module's location cache by sending a message to the location cache.",{"@attributes":{"id":"p-0403","num":"0424"},"figref":"FIG. 52","b":["5116","5201","5202","5203","5204"]},"The client module sends a message to the application controller (the AC Ref included in the stub allows the client module to communicate with the application controller) . The message includes the execution module identifier. The application controller finds the Execution Module object corresponding to the execution module identifier in the state model . The information in the state model includes the Container object representing the container that currently includes the execution module. The Container object includes the network address of the container (\u201cContainer Address\u201d). The application controller sends a message to the client module . The message includes the Container Address. The stub enters the execution module identifier and the container address into the location cache . In either case, the following steps are included. The stub uses the Container Address to obtain a network connection to the container (the stub may use an existing cached connection or open a new connection). The stub uses the connection to send a remote invocation request to the container . The container locates the server object in the execution module . The container invokes the operation requested by the client program on the server object . If the client program expects to receive the result of the remote operation on the server object, the following steps are executed. The container sends the results to the stub, using the network connection . The stub returns the result to the client program that invoked the programming language method .","Some embodiments of the invention that use active\/backup replication of execution modules take advantage of the communication protocol between the stub located in the client module and the application controller to perform failover of a server object.  is a block diagram that depicts the relevant components of a system for remote invocation of a replicated server object before a failure.","EM Replica   is the active execution module replica that contains a server object. The execution module is included in Container , which is a process running on Node . EM Replica   is the corresponding backup execution module replica that contains a backup copy of the server object. The execution module is included in Container , which is a process running on Node .","The application controller's state model  includes the information about execution modules, their replicas, and the containers that contain the replicas. The Execution Module object is associated with the Replica  object  representing the active replica and the Replica  object  representing the backup replica. Replica  object  is associated in the state model with the Container  object to represent that EM Replica   is included in Container . Similarly, Replica  object  is associated with the Container  object to represent that EM Replica   is included in Container . The information included in the Container  object includes the network address of Container , depicted as Container  Address. Similarly, the information included in the Container  object includes the network address of Container , depicted as Container  Address.","In some embodiments, the Execution Module object is associated with one or more Location Cache Reference information that allows the application controller to communicate with the location caches that have cached information about the location of the currently active execution module replica.","A client module includes a stub that is used by the client program to perform remote operations on the server object. The stub includes execution module identifier (\u201cEM ID\u201d) that uniquely identifies the execution module. The AC Ref included in the stub allows the stub to communicate with the application controller. The location cache included in the client module stores entries that map an EM ID to a network address of the container (\u201cContainer Address\u201d) that includes the currently active replica of the execution module identified by EM ID. The stub sends the remote invocation requests to the container that includes the active replica using the steps of the flow chart depicted in .",{"@attributes":{"id":"p-0410","num":"0431"},"figref":"FIG. 53-B","b":["1","1","5306"]},"When the application controller is notified of the failure, it marks the state model objects representing the failed components as \u201cfailed\u201d. Replica  , represented in the state model by the Replica  state object , which was previously a backup replica, is promoted to become the active replica. When the application controller promotes the backup replica to active, it sends a message to the associated location caches represented by the Location Cache Reference information in the state model. The message includes the execution module identifier (\u201cEM ID\u201d) and the new location of the active replica, which in this example would be represented by Container Address . A location cache that receives this message updates the entry for EM ID.","When the client program subsequently performs a remote object invocation on the server object by invoking a programming language method on the stub, the stub will use the updated entry and will send the remote invocation request to the server object located in EM Replica  .","If the client program invokes a server object after the active execution module has failed but before the application controller updates the location cache, the remote invocation request sent to container  will fail with an error indicating that the execution module, container, or node has failed. If this happens, the client module will request the current execution module location from the application controller by sending a message to the application controller. When the current location is received, the stub located in the client module will retry the remote invocation request by sending it to container  which includes the replica promoted from backup to active status. The client module might also enter the new location into the location cache.","In some embodiments, when an execution module replica fails, the application controller creates a replacement backup replica, EM Replica  , to protect against future failures. The state model includes the information about the replacement replica.",{"@attributes":{"id":"p-0415","num":"0436"},"figref":"FIG. 54","b":["1","5401","1","1","2","5402","2","2","1","5403","1","2","5404","2"]},"The state model in the application controller includes factory information for each type of server objects that could be created by a remote create request from a client program. The factory information is associated with partitioning information. The partitioning information defines the criteria for how the server objects should be partitioned across execution modules, based on the parameters to a create operation.","A client module includes a factory stub  and partitioning information cache . The factory stub  is a programming language object that defines one or more create methods. Each create method is associated with a create operation on the factory server objects located in the execution modules. The create method takes zero or more parameters. The partitioning information cache stores partitioning information objects received from the application controller.","In some embodiments, a remote creation of a server object in a partitioned system includes the following steps. A client program invokes a create method on the factory stub, passing zero or more parameters to the method. The factory stub attempts to retrieve the corresponding partitioning information from the partitioning cache. If the partitioning information is not in the cache, the following steps are performed.","The factory stub sends a message to the application controller to request the partitioning information. The application controller locates the partitioning information from the state model and sends it to the factory stub in the client module. The factory stub enters the partitioning information in the cache.","Regardless of whether the partitioning information is in the cache, the following steps are performed. The factory stub uses the partitioning information and the create method parameters to determines in which execution module to create the server object. The factory stub sends a remote create request to the factory server object located in the execution module determined in the previous step. In some embodiments, the factory stub performs the invocation of the remote create operation by using the steps of the flow chart depicted in . The factory server object creates a new server object. In some embodiments, a remote object reference of the server object is passed back to the factory stub as the result of the create operation.","While the block diagram in  illustrates the factory stub and partitioning information cache as separate modules, in different embodiments, some functionality of the partitioning information cache might be included directly in the stub, or vice versa.","There are many possible embodiments of the factory server objects and factory stubs. In some embodiments, the Enterprise JavaBeans (EJB) home interfaces are used as factory stubs and the EJB container provides the corresponding factory server objects. In other embodiments, one server object may function as a factory server object for another server object. In some embodiments, the partition information is implemented as Java objects.",{"@attributes":{"id":"p-0423","num":"0444"},"figref":"FIG. 55"},"The protocol for creating a server object includes the following steps. A client program invokes a create method on the factory stub, passing zero or more parameters to the method. The factory stub sends a message to the application controller to obtain the identifier of the execution module in which to create the new server object. The message includes the create method parameters. When the application controller receives the message, it invokes the Get Execution Module operation on the distribution manager associated with the execution module definition, passing it the create method parameters. The distribution manager determines the execution module in which to create the new server object. The information that the distribution manager uses in its decision may include the create method parameters and the information in the entire state model. The distribution manager can choose an existing execution module, or cause a creation of a new execution module in which the server object will be created. The application controller sends a message to the factory stub. The message includes the identifier of the selected execution module. The factory stub sends a remote create request to the factory server object located in the selected execution module. In some embodiments, the factory stub performs the invocation of the remote create operation by using the steps of the flow chart depicted in . The factory server object creates a new server object. In some embodiments, a remote object reference of the server object is passed back to the factory stub as the result of the create operation.","The present invention has broad applications and therefore may have many different embodiments. The embodiments described here should be considered illustrative, not prescriptive.","In one embodiment of the invention, nodes are realized as general-purpose server computers or server blades. Each node includes an image of a general-purpose operating system (e.g. Linux, Solaris, or Windows). Node controllers are realized as a Java 2 Platform, Standard Edition (\u201cJ2SE\u201d) applications. The membership and master election protocol is based on the Paxos algorithm described in Lamport Leslie's Paxos Made Simple. The execution controller, service application controller, and Java application controller are realized as Enterprise JavaBeans (\u201cEJB\u201d) modules. Their state models include multiple Enterprise JavaBeans and their operations are realized as EJB business methods. The controllers are made highly available by checkpointing their state to backup modules. The checkpointing mechanism is realized according to a method described in U.S. Provisional Patent Application Ser. No. 60\/445,639 entitled Method and Apparatus for Online Transaction Processing and U.S. Provisional Patent Application Ser. No. 60\/508,150 entitled Method and Apparatus for Efficient Online Transaction Processing. It is also described in U.S. patent application Ser. No. 10\/774,207 entitled Method and System for Efficient Online Transaction Processing. The communication between nodes is realized by a specialized implementation of the Java Remote Method Invocation (RMI) API that includes the capabilities suitable for use with the invention. Processes are realized as operating-system level processes. Java containers are realized as operating-system level processes that include the Java Virtual Machine (JVM) and application server classes. Application server classes are Java classes that include the functionality of the J2EE platform. Execution modules include the Java classes from J2EE and J2SE applications and tools-generated Java classes representing the J2EE runtime artifacts. The Java platform's class-loader mechanism is used to isolate the execution modules from each other. The Distribution Manager object in the Java application controller implements the support for active and backup execution modules as described in above and in U.S. Provisional Patent Application Ser. No. 60\/445,639 entitled Method and Apparatus for Online Transaction Processing and U.S. Provisional Patent Application Ser. No. 60\/508,150 entitled Method and Apparatus for Efficient Online Transaction Processing. It is also described in U.S. patent application Ser. No. 10\/774,207 entitled Method and System for Efficient Online Transaction Processing. The Distribution Manager object is realized as a Java class. The Execution Controller, Java Application Controller, and Service Application Controllers are replicated using the \u201cactive\u201d and \u201cbackup\u201d replication model described in this disclosure.  depicts their exemplary distribution across the nodes of a distributed computer system.","The following is the description of some alternative embodiments of the invention. It would be impossible to list of possible embodiments and therefore the listed embodiment should be considered as illustrative. In some embodiments, the node controllers, execution controller, or application controllers could be implemented in C, C++, C#, or any other system programming language suitable for the operating system and hardware of the node. In some embodiments of the invention, the elected master nodes use the concept of a lease to prevent interference with each other after failures such as in Cary G. Gray's and David R. Cheriton's Leases: An Efficient Fault-Tolerant Mechanism for Distributed File Cache Consistency. In some embodiments, the nodes may communicate with each other via shared memory. In some embodiments, some components of the execution control system that are described in this disclosure as distinct components could be integrated with each other. For example, the execution controller and one or more application controllers could be combined into a single component in some embodiments.","In other embodiments, some of the components of the execution control system might be divided into parts and distributed across several nodes. For example, in a system with a large number of applications the Java application controller could be divided into multiple identical parts, each controlling a subset of the Java applications. Some embodiments may include an application controller that provides the support for the Microsoft .Net application execution model. In some embodiments, a custom application controller may emulate the functionality of an HA framework, such as of the Sun Cluster framework. This would be typically done to make it possible to execute pre-existing applications that conform to the HA framework's application execution model. In some embodiments, the execution modules managed by the Java application controller correspond to the modules of a J2EE application. J2EE applications and its modules are described in a Method and Apparatus for Execution Applications on a Distributed Computer System, U.S. Provisional Patent Application Ser. No. 60\/454,510, filed Mar. 12, 2003.","In some embodiments, the execution modules managed by the Java application controller correspond to transactional state partitions as described in U.S. Provisional Patent Application Ser. No. 60\/445,639 entitled Method and Apparatus for Online Transaction Processing and U.S. Provisional Patent Application Ser. No. 60\/508,150 entitled Method and Apparatus for Efficient Online Transaction Processing. It is also described in U.S. patent application Ser. No. 10\/774,207 entitled Method and System for Efficient Online Transaction Processing. In some embodiments, the operating-system level \u201cservices\u201d are used instead of \u201cprocesses\u201d. In some embodiments of the invention, the nodes of the distributed computer system could be geographically distributed to provide protection from catastrophic events, such as fire or flood. The method and system of the present invention allows placing the replicas of an execution such that they are assigned to nodes in different locations.","In some embodiments, different terminology might be used to refer to some concepts of the present invention. For example, the terms \u201cprimary\u201d and \u201csecondary\u201d could be used instead of \u201cactive\u201d and \u201cback\u201d when referring to the execution module replica roles, or the terms \u201clogical execution module\u201d and \u201cphysical execution module\u201d could be used instead of the terms \u201cexecution module\u201d and \u201cexecution module replica\u201d respectively. Other embodiments of the present invention may exist, in addition to those described above.","A typical embodiment of a distributed computer system includes multiple conventional hardware servers (these are the nodes) connected with each other by a Local Area Network (LAN) such as Ethernet or System Area Network (SAN) such as Fibrechannel. In some embodiments, the nodes might be geographically distributed and connected with each other via Wide Area Network (WAN). The connections between the nodes may also be by wireless networks such as radio, wireless WAN, or satellite communication. Further, some nodes of the distributed computers systems may be connected together through physical connections (as described above) and others nodes may be connected through wireless connections (as described above). Also, groups of nodes may be connected together through physical connections and others groups of nodes may be connected through wireless connections. The network technology that interconnects the nodes also might include switches, bridges, firewalls and network of various types.","Many alternative embodiments of the distributed computer system exist. For example, in some embodiments, some of the nodes might be special-purpose computer boards (such as real-time boards used in systems for telecommunications), base stations for wireless networks, RFID stations, mobile stations in wireless networks or any other type of computing devices. In some alternative embodiments, the network that connects the nodes might be a radio network.","A distributed computer system might include nodes of multiple types (e.g. some nodes are conventional hardware servers while other nodes are special-purpose telecommunication boards) and the network might include multiple network types, such as LAN and radio network.","Methods and systems of the present invention could be used with many different applications running on a distributed computer system. The purpose of the exemplary system and applications depicted in  is to illustrate the advantages of using multiple application controllers in the execution control system.",{"@attributes":{"id":"p-0435","num":"0456"},"figref":"FIG. 30"},"The Web service and database service use similar application execution model based on processes. The J2EE application uses a different application execution model based on two-level containment hierarchy: the application includes execution modules contained in containers and container processes (\u201ccontainers\u201d) running on nodes.","The system also includes an execution control system consistent with the present invention. The execution control system includes an execution controller and two application controllers: the service application controller and the Java application controller. The service application controller controls the execution of the Web service and database service. The Java application controller controls the execution of the J2EE application. The execution controller includes the state of the nodes in the distributed computer system, the definition of node groups, and the state of the processes running on the nodes.",{"@attributes":{"id":"p-0438","num":"0459"},"figref":"FIG. 31"},"The node group \u201cWeb Server Group\u201d, which includes nodes A, B, and C, has been associated with the Web service application. The distribution policy for the service instructs the service application controller to create a Web server process on each node of the node group.","The node group \u201cDatabase Group\u201d, which includes nodes D and H, has been associated with the database service. The distribution policy for the service instructs the service application controller to create a single database server on of the nodes of the node group (for example on node D). Should the node fail, the service application controller restarts the database server process on the other node of the node group (for example, on node H).","The node group \u201cJava Application Group\u201d, which includes nodes E, F, and G, is used by the Java application controller for the execution of the J2EE application. The node group is associated with a container group (not illustrated in the figure) and the container group is associated with the EM Definition of the J2EE application. The Java application controller distributes the execution modules over containers and of containers over the nodes of the container group according to distribution policies associated with the container group and the EM Definitions.","The execution control system starts the applications and responds to failures according to methods of the invention. The execution control system also allows expanding the distributed computer system by adding nodes, according to a method of the invention. If nodes are added to the system, the execution control system will automatically allow the applications to take advantage of the new nodes according to their distribution policies."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF DRAWINGS","p":[{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 18"},{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 19"},{"@attributes":{"id":"p-0041","num":"0040"},"figref":"FIG. 20"},{"@attributes":{"id":"p-0042","num":"0041"},"figref":"FIG. 21"},{"@attributes":{"id":"p-0043","num":"0042"},"figref":"FIG. 22"},{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 23"},{"@attributes":{"id":"p-0045","num":"0044"},"figref":"FIG. 24"},{"@attributes":{"id":"p-0046","num":"0045"},"figref":"FIG. 25"},{"@attributes":{"id":"p-0047","num":"0046"},"figref":"FIG. 26"},{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIG. 27"},{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIG. 28"},{"@attributes":{"id":"p-0050","num":"0049"},"figref":"FIG. 29"},{"@attributes":{"id":"p-0051","num":"0050"},"figref":"FIG. 30"},{"@attributes":{"id":"p-0052","num":"0051"},"figref":"FIG. 31"},{"@attributes":{"id":"p-0053","num":"0052"},"figref":"FIG. 32"},{"@attributes":{"id":"p-0054","num":"0053"},"figref":"FIG. 33"},{"@attributes":{"id":"p-0055","num":"0054"},"figref":"FIGS. 34-A","b":"34"},{"@attributes":{"id":"p-0056","num":"0055"},"figref":"FIGS. 35-A","b":"35"},{"@attributes":{"id":"p-0057","num":"0056"},"figref":"FIG. 36"},{"@attributes":{"id":"p-0058","num":"0057"},"figref":"FIG. 37"},{"@attributes":{"id":"p-0059","num":"0058"},"figref":"FIGS. 38-A","b":"38"},{"@attributes":{"id":"p-0060","num":"0059"},"figref":"FIG. 39-A"},{"@attributes":{"id":"p-0061","num":"0060"},"figref":"FIG. 39-B"},{"@attributes":{"id":"p-0062","num":"0061"},"figref":"FIGS. 40-A","b":"40"},{"@attributes":{"id":"p-0063","num":"0062"},"figref":"FIG. 41"},{"@attributes":{"id":"p-0064","num":"0063"},"figref":"FIG. 42"},{"@attributes":{"id":"p-0065","num":"0064"},"figref":"FIG. 43"},{"@attributes":{"id":"p-0066","num":"0065"},"figref":"FIG. 44"},{"@attributes":{"id":"p-0067","num":"0066"},"figref":"FIG. 45"},{"@attributes":{"id":"p-0068","num":"0067"},"figref":"FIG. 46"},{"@attributes":{"id":"p-0069","num":"0068"},"figref":"FIG. 47"},{"@attributes":{"id":"p-0070","num":"0069"},"figref":"FIG. 48"},{"@attributes":{"id":"p-0071","num":"0070"},"figref":"FIG. 49"},{"@attributes":{"id":"p-0072","num":"0071"},"figref":"FIG. 50"},{"@attributes":{"id":"p-0073","num":"0072"},"figref":"FIG. 51"},{"@attributes":{"id":"p-0074","num":"0073"},"figref":"FIG. 52"},{"@attributes":{"id":"p-0075","num":"0074"},"figref":"FIG. 53-A"},{"@attributes":{"id":"p-0076","num":"0075"},"figref":"FIG. 53-B"},{"@attributes":{"id":"p-0077","num":"0076"},"figref":"FIG. 54"},{"@attributes":{"id":"p-0078","num":"0077"},"figref":"FIG. 55"}]},"DETDESC":[{},{}]}
