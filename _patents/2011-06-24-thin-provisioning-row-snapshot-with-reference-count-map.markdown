---
title: Thin provisioning row snapshot with reference count map
abstract: The storage of single or multiple references of the same data block in a storage pool is disclosed. Indexing of the data includes storing reference information in the storage pool as a mapping table. The mapping table indexes each data block in the storage pool. On any read or write request mapping information is used to retrieve the corresponding data block in storage pool.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08806154&OS=08806154&RS=08806154
owner: Chelsio Communications, Inc.
number: 08806154
owner_city: Sunnyvale
owner_country: US
publication_date: 20110624
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION","LIST OF ACRONYMS"],"p":["This application claims the benefit of priority under 35 USC 119(e) to U.S. Provisional Application No. 61\/483,378, filed on May 6, 2011, which is incorporated herein by reference in its entirety for all purposes.","The present invention pertains generally to indexing of data chunks in a storage pool. In particular, the present invention is directed to generating, storing, and later utilizing indexing information and a usage count of storage chunks for point in time snapshots taken for virtual volumes of a storage pool.","It is common in many computing systems to generate backups, including periodic snapshots, which are point-in-time copies of data. A storage device typically must support backup of data onto a safe and stable storage device by making multiple copies of data so that loss of any copy is not catastrophic. These additional copies may be used to restore the original data after a data loss event.","To further strengthen the safety of data backed up it is often required that clones of stored data also be created. The clone thus created at various time intervals occupies the same amount of space on storage device as the original (origin) data source at that point of time. However, if many snapshots are taken then the total amount of storage associated with the clones and the snapshots can become prohibitive and lead to duplicate copies of information that doesn't frequently change, which is wasteful of storage space. To minimize the storage space and prevent un-necessary copying of data it is advisable to share data between a primary data source and its several clones. Rather than creating a complete copy of an entire disk containing the source data, disk area may be divided into a smaller addressable storage space called a \u2018chunk\u2019. Thus, on clone creation it is the chunks which are copied rather than the entire disk.","To further save on storage space the chunks may be referred to by the parent and the clones are not copied but are shared.","However, conventional techniques to manage clones and snapshots have many drawbacks. As an example, in the write anywhere file-system layout of U.S. Pat. No. 5,963,962, the data sharing information is stored in a bitmap, the length of which limits the number of snapshots created. There are also other disadvantages of prior art storage solutions to index and manage clones and snapshots.","The present invention provides an alternative indexing and storage management approach which has advantages over the prior art for managing clones and snapshots such as increased scalability and eliminating the bitmap requirement of the prior art.","A storage server is disclosed that supports thin provisioning of a storage volume and also supports Redirect-On-Write (ROW) snapshots. A reference count map is used to index a storage pool. Data is stored in chunks in the storage pool and a mapping table indexes each data block in the storage pool. The metadata data for the mapping table may be stored in reference chunks. On any read or write request mapping information is used to retrieve the corresponding data block in storage pool. The storage server may be implemented as a Unified Storage Server having interfaces to support both file-based access and block-based access.","An exemplary method provides scalable indexing of a storage pool from multiple storage units. A virtual volume is formed in which the combined storage capacity of the storage pool is represented by a set of chunks having a pre-selected data size. The chunks are assigned to be either storage chunks or reference chunks. Metadata information is stored in the set of reference chunks to index the storage chunks and track usage of the storage chunks. The storage chunks are allocatable on demand and are referable more than once to reduce redundant storage for identical content. The total number of chunks and a chunk allocation may be adjusted in response to a change in the size of the available physical storage to include a sufficient number of reference chunks to index all of the storage chunks. In one implementation the reference chunks are updatable to store indexing information for Redirect-On-Write snapshots of the virtual volume.","I. Apparatus and System Overview","This present invention pertains generally to a storage server having several improved storage application aspects, including indexing of data chunks in a storage pool, including storing a usage count of storage chunks, when point in time snapshots are taken for the virtual volumes.","Thin provisioning is one such storage application which utilizes the present invention. Thin provisioning is a storage virtualization method to efficiently utilize the storage space. In conventional storage, storage space is allocated beyond current needs in anticipation of growing storage usage thus paying for the overcommitted storage which is never used. But in thin provisioning storage an individual managing the storage server can purchase less storage capacity upfront and defer storage capacity upgrades in line with actual business usage and save the operating costs associated with keeping unused disk capacity spinning at lower administrator efforts. Thin provisioning enables over-allocation or over-subscription. Over-allocation is a mechanism that allows server applications to be allocated more potential storage capacity than has been physically reserved on the storage array itself. This allows flexibility in growth and shrinkage of the physical application storage volume, without having to predict how much a volume will grow or shrink. For example, each user of a system can be allocated some theoretical storage limit, such as 10 GB. Physical space on the storage array is dedicated only when data is actually written by the storage application and not when the storage volume is initially allocated.",{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIGS. 1 and 1A"},"Thin provisioning allows for creating virtual volumes that are greater than the total size of physical devices present. This allows for pre-allocating large amounts of potential capacity for different end-users, and then populating the real devices as the actual usage by the end-users increases. Physical devices can be dynamically added to the storage pool to provide required disk space, while volumes in the pool are being accessed. The pool will stop all volumes and not allow further I\/O, once it runs out of space. The manager of the system can then add physical devices and reactivate all volumes and resume all I\/O.","Referring to , a storage server computer  includes computer hardware such as at least one processor and a local memory to store computer program code for implementing thin provisioning. In one embodiment the storage server  also includes at least one ASIC (not shown) such as a processor designed for storage server applications and\/or having additional hardware specific to storage server applications. The storage server  receives and responds to read and write requests from one or more client computers . Elements  are illustrated as being an array of hard disk drives forming the physical storage units for a storage pool, although it will be understood that elements  may comprise other types of storage units suitable for storing backup information.","The storage pool  is a virtual volume exposed by storage server computer  to client computers . Interfaces are provided to communicatively couple the storage server  to client computers and to the storage elements . In one implementation, storage server computer  includes one or more network ports connected to a network switch  on one side with other suitable switch(es) to connect to storage devices on the other side. Client computers , also referred to as initiators, may utilizes various advanced interface features such as Internet small computer systems (iSCSI), Fibre channel over Ethernet (FCoE), Network File system (NFS) etc provided by  to access the storage disk .","Referring to , in one embodiment a storage server  may be implemented as a highly scalable Unified Storage Server (USS).  is a functional block diagram but it would be understood that the physical implementation for the storage server, includes at least one processor and a local memory storing computer program code to implement the functional blocks, with the function being implemented using, for example, a combination of special purpose hardware and software executing on one or more processors. A USS provides interfaces and other features to support storage using different interface protocols such as iSCSI and FCoE. In an exemplary USS implementation both file based access and block based access is supported through a suite of network client services and protocols, with an exemplary set of common industry-standard protocols and services illustrated along with their corresponding layer in a conventional network model (e.g., application, transport, network, data link, and physical layers). An exemplary implementation supports 10 GbE connectivity as well as other data rates. The USS may use standard user space kernel access, such as an XFS file system for file based block access and a block device node for block based access. The virtualization of storage is performed at a volume management layer, which includes thin provisioning.","Additionally, other features to support volume management may be included, including volume cloning (e.g., cloning of an original volume), replication, Read-on-Write snapshots, SSD caching, and encryption. The volume management layer may leverage off of an industry standard platform, such as a Linux\u00ae platform, i.e., be Linux\u00ae compatible but have features and functions not found in conventional Linux\u00ae. Other features to support fault tolerant attachments to different physical disk storage units may be included, with exemplary examples such as software\/hardware Redundant Array of Independent Disks (RAID), and standard interfaces such as Serial ATA (SATA), Serial Attached SCSI (SAS), Peripheral Component Interface Express (PCIe), and Solid State Drive Fibre Channel (SSD)\/FC.","The present invention includes an improved thin provisioning function and additional support features which may be implemented in a storage server or in a USS, although it will be understood that other implementations are also contemplated as being within the scope of the invention.","In a data storage system, a snapshot of a volume is generally taken to store the point in time copy of data. The point in time copy allows the users to roll back to any previous state of volume in the past. There are several types of snapshot mechanisms: Copy-on-Write, Redirect-on-write, Clone or Split-Mirror, Copy-on-write with background copy, Incremental and Continuous data protection. In any of the above approaches a snapshot creates a logical copy of disk partitions.","An exemplary embodiment of the present invention supports snapshot operations using a Re-direct-on Write (ROW) although more generally other snapshot mechanisms may be used. Redirect-on-Write (ROW) is a method of protecting data that needs to be overwritten by new writes after a snapshot has been taken. It preserves the old data in its old location, and instead, redirects the new write to a new location. All subsequent reads and writes of data for the volume are performed at the new location. Snapshot reads continue to be performed from the old location.","In one embodiment, the thin provisioning of the present invention may be implemented to provide a suite of features comparable to conventional volume management, such as Linux Logical Volume Management (LVM), but optimized for performance and supporting other features such as allowing over provisioning of volumes, i.e., thin provisioning, where the physical storage capacity is added only on demand, but advertised upfront. Additionally, conventional LVM supports Copy-on-Write (COW).","In one implementation thin provisioning supports taking Redirect-On-Write (ROW) snapshots, which have a lower number of I\/O operations. In particular, one benefit of ROW is that it requires two I\/O operations on record updates instead of three I\/Os for Copy on Write. That is, ROW is more efficient than COW.","In one embodiment thin provisioning also supports cloning a volume instantly, an instant restore of a volume from a snapshot. A device mapper encryption driver can also be used with a thin provisioned volume, to encrypt data being stored in that volume. ROW creates a point in time view (snapshot) of the original volume wherein a snapshot volume shares all the data blocks created by the origin until any update or write happens. Any update\/new write to original or snapshot volume may allocate a new block with unique pointer not referred by previously created snapshot and origin.","Thin provisioning, in a shared storage environment, supports the allocation of data blocks as data is written real-time. This methodology eliminates almost all whitespace which helps avoid the poor utilization rates that occur in the traditional storage allocation method where large pools of storage capacity are allocated to individual servers but remain unused.","II. Indexing and Management of Storage Data","Referring to , the storage pool comprises all of the available storage, which may include the combined capacity of different storage units. The volume within a storage pool is divided into chunks, where the chunks preferably have the same size, such as an efficient minimum size for addressing. The chunks may further be allocated as storage chunks or as reference chunks storing reference data, i.e., metadata.","Each volume within a storage pool requires indexing to map the storage chunks allocated to that volume. An exemplary chunk size is 32 KB. When a chunk is allocated to store metadata this permits a single chunk to be allocated a large number of slots for metadata (e.g., 4096 slots for a chunk size of 32 KB in one implementation). Volume metadata, such as a radix tree and other information, is allocated to a set of chunks used to store metadata for indexing. Both the original volume and its snapshots can share the same radix tree. The metadata may also be dynamically allocated.","In one embodiment the first few chunks of the storage pool are used to maintain a reference count mapping and are known as reference count chunks. As illustrated, chunks CH1, CH2 etc are the chunk numbers in sequence as they appear in the disk. For example, element  is a reference count chunk. In one implementation a reference count chunk stores a table indexed by storage pool chunk number to identify the storage chunks indexed by the reference count chunk. A fixed space in a reference count chunk may also be assigned to store a usage count of each addressed storage pool that is called a \u2018bitset\u2019 . The bitset is used to store the usage count of a corresponding chunk on the disk, i.e., whether the chunk is used at all or is referenced more than once, which may occur in the case of data referred to by a clone and by snapshots. The length of the bitset may be varied from application to application to permit the usage count to be optimized for a particular application. It will be understood, however, that other alternate approaches to store the bitset information may be employed other than storing it within a reference count chunk.","The reference count mapping includes the full set of reference count chunks and associated bitsets required to index the storage volume. In many applications the performance may be acceptable if this information is accessed solely from the storage pool. However, to improve the ability to read and utilize indexing information the reference count chunk  and the bitset  may alternatively be stored on, for example, a dedicated storage unit (e.g., a disk storage) for better performance and used to reference other storage chunks in the storage pool. Additionally, caching may also be employed to improve the indexing performance when performing a read\/write operation on the storage server.","As illustrated in , there may be multiple disks for storage. The thin provisioned storage pool is implemented by dividing the actual storage disk space\u2014which may be from more than one individual storage system\u2014into chunks and logically addressing each chunk using a chunk mapping mechanism, what is called here the reference count map and associated bitset. The chunks preferably have the same size to facilitate addressing, data access, and indexing.","The chunks are allocated on demand.  illustrates in more detail aspects of a set of multiple reference count chunks CH1, CH2, CH3, etc. for addressing of chunks on physical memory (e.g., one of a set of disk drive storage units). There is a whole positive number \u201cm\u201d of reference count chunks for addressing the entire storage pool with each reference count chunk storing the usage count of a number \u201cn\u201d of chunks. As previously described, a set of initial chunks may be used to maintain the reference count mapping and a logical addressing scheme may be used to address the chunks. Element \u2018\u2019 is a reference count chunk while element \u2018\u2019 is a bitset portion which is used to store the usage count of corresponding chunks on the disk, i.e., to store information whether the chunk is in use or free to be allocated. The length of the bitset may be varied from application to application.","A radix tree may be used to index the storage chunks, where each node in the tree contains a number of slots. Each of these slots in turn contains a disk address of either the data item if it is a leaf node, or a next level tree node. The number of slots per node is dependent on the chunk size. A chunk size of 32 KB can have 4096 such slots. A portion of the logical address of the data chunk is used as the key for these slots. In this implementation the logical address is split into multiple keys each with n bits, where n is calculated from the chunk size i.e., a chunk size that is 32K requires 12 bits to address all the 4096 slots. That means, the first 12 Most Significant Bits (MSBs) of a logical address may be used as key for a root level node with the next 12 bits being for the second level, and so on to support a radix tree mapping from a logical address to disk addresses.","The reference count mapping approach is scalable and thus effectively allows the user to create unlimited snapshots and ensures that originals and snapshots share as many common pages as possible. Scalability is enhanced by using a sequential ordering system where the chunks have a sequence and the addition of a new storage unit results in an addition of new chunks in the sequence. The reference count is incremented and decremented as the volumes or snapshots are created and removed in the pool.","In a typical application the reference count chunks of the reference count map would be located at either end of the storage pool due to the benefits of having the indexing information in a contiguous region of memory. However, more generally, the reference count pool may be implemented in any region of contiguous memory with each storage pool chunk referred by a set of bits (depending on number of snapshots required for a volume) of memory space. For example, the reference count pool may be located in a contiguous region of memory at either the beginning or the end of the storage pool. The reference count pool maintains the reference count of each chunk which is incremented and decremented as and when chunk is allocated and referred by origin and snapshot volume. The reference count of a free block is zero.","The reference count pool serves the purpose of providing both bitmap and block references but is more efficient, flexible, and scalable than prior art approaches. If higher performance is desired, the reference count pool may sit on a separate disk and as a result access is faster and does not require any locking on data chunks when reference count calculation is in progress. A cache for reference count chunks may also be provided for a performance improvement.",{"@attributes":{"id":"p-0047","num":"0046"},"figref":["FIG. 4","FIG. 4"]},"As illustrated in , in one implementation a circular buffer maintains a free list of storage chunks. In this implementation, the circular buffer is used for logging of reference count chunks. The bitset reference count chunk  is also stored in the circular buffer; the circular buffer maintains a free list of storage pool chunks. The circular buffer with unused chunk addresses is logged onto the persistent storage and is played again when the storage application is re-initialized.","As previously discussed, the storage pool is implemented by dividing the actual storage disk space into chunks and addressing each chunk using the mapping mechanism which could be implemented by a radix tree or B-tree as an example, Radix tree referencing is described U.S. Pat. No. 5,873,078, the contents of which are hereby incorporated by reference. An exemplary tree mapping is illustrated in . The lowest level of the tree contains the leaf nodes or data chunks referred by  in . Data chunks are searched or inserted using a tree traversal procedure supported by the mapping mechanism. The height of the tree is determined by the number of bits in the keys. The larger the number of bits the smaller is the height but the tradeoff involves comparing more bits on each search to reach the leaf node.",{"@attributes":{"id":"p-0050","num":"0049"},"figref":["FIG. 6","FIG. 6"],"b":["620","630","640","620","610"]},"In the example of  several of the illustrated chunks, including a metadata chunk  or a data chunk , has a usage count of one as they are owned by only one virtual volume. Element  in  is one such reference count chunk which stores the usage count of both virtual volume and snapshot chunks. In this example all the chunks referred to by  have the usage count of 1. The chunks referred by to both the original volume and the snapshot volume need not be duplicated but have their usage count incremented by one on any new reference; hence saving the space occupied in storage pool. The chunk is duplicated only when a shared chunk is written to by any of the sharing volumes. When the snapshot of a volume is taken the chunks owned by a volume are shared with the snapshot, thus increasing the usage count of each shared storage chunk to 2.",{"@attributes":{"id":"p-0052","num":"0051"},"figref":["FIG. 7","FIG. 7","FIG. 7","FIGS. 6 and 7"],"b":["730","730","740","710","610","1"]},"III. Snapshot Scalability","Note that the usage count stored in reference count chunk  or  allows the chunks to be referenced by any number of volumes. This eliminates redundant storage, improving storage efficiency. This is particularly important for applications in which there is a potentially large number of snapshots. Referring back to , the usage count for all the storage pool chunks is stored at one location as a group at the beginning of the storage pool and is referred to as \u2018reference count map\u2019. Element  in  is the reference count map and occupies few chunks of the storage pool. The reference count map addresses all the chunks of the storage pool and maintains the storage pool chunk usage count. Element  illustrates an example of a reference count chunk addressing the storage pool chunks CH1 to CH6 in the sequential order as they appear on storage pool i.e. information in the reference count chunk is a table indexed by storage pool chunk number. The usage count of each addressed storage pool chunk occupies a fixed space in reference count chunk and is the \u2018bitse\u2019 . Each reference count chunk maintains the \u2018bitset\u2019 of fixed number of storage pool chunks. Each reference count chunk has a fixed size hence can address a fixed number of storage pool chunks. For example, in , reference count chunks ,  both address same number of storage pool chunks. The reference count map is thus a pool of several reference count chunks and the number of reference count chunks , and  in the map grows as the size of the storage pool grows. In this example, reference count chunk  maintains the usage count of the first \u2018n\u2019 storage pool chunks, reference count  maintains the usage count of the next \u2018n\u2019 storage pool chunks and so on till reference count chunk  maintain the usage count of last few storage pool chunks.",{"@attributes":{"id":"p-0054","num":"0053"},"figref":["FIG. 8","FIG. 8"],"b":["810","820","830","810","820","830","810"]},"Referring back to , in these examples the chunk allocation is expedited by maintaining a circular buffer of free chunks with usage count set to one in the bitset. The circular buffer has threshold limit set which when hit, triggers the further chunk allocation keeping the circular buffer abreast with the write operations on the pool. The circular buffer is written onto stable storage at a periodic interval for journaling purposes and is replayed when the chunk allocation is requested again on application initialization. For the situation of recovering from loss of the in-memory copy after being modified and after being written to stable storage, this step helps to maintain metadata sanity and thus aids in maintaining file system integrity. As discussed above in one implementation the storage pool chunk allocation proceeds in a sequential order starting with checking of the first reference count chunk  for free bitset followed by the next reference count chunk  and so on until a scan is performed for reference count chunk  scanning for a free bitset. Chunk allocation logic may then wrap around starting at  again looking for any free bitset. The deletion of virtual volumes and clones frees all the chunks owned exclusively by them, the bitset of all such chunks is marked a zero in the reference count map.","Referring to , if multiple virtual volumes and clones are performing Input\/Output operations simultaneously on the storage pool then the chunks assigned to for data access may be determined by the order of chunk allocation. At any point of time a reference count chunk  can contain the addresses of chunks allocated to multiple virtual volumes and their clones. Thus, same reference count chunk  can thus be in use by multiple virtual volumes or clones to update the usage count of storage pool chunks addressed by the same reference count chunk.",{"@attributes":{"id":"p-0057","num":"0056"},"figref":"FIG. 9","b":["910","920","930","940"]},"III. Metadata and Logging String","Metadata consistency may be ensured by logging the reference count changes. In one implementation, any increment or decrement to the reference count value is encapsulated in a logging string. The logging string comprises an address of reference count chunk, a word offset in the chunk, a bitset offset in a word, a previous reference count value and a new reference count of an addressed storage pool chunk. For example, Cha in  has a original reference count of 1 which increases to 2 on snapshot creation. Now consider CHa in  that has both the old and the new value which is logged in a logging string along with chunk address CHa.","A logging string is grouped in logging chunks and is written to persistent storage by a periodic timer along with metadata information. In one implementation the logging string is marked dirty and is available for replay as long as metadata addressed by it is stored in volatile memory. Once the metadata is successfully written to persistent disk logging information it may be marked void and is rendered useless. Logging information helps maintain the metadata consistency. A dirty log when played on system recovery ensures that reference count value of a chunk matches the number of volumes using it. If a failure occurs while committing the metadata chunk CHe in  to disk for which the reference count is changed then old reference count value, i.e. 1, is stored for that chunk in reference count map , else a new value is retained.","While a particular reference count mapping has been described, it will also be understood that other types of metadata could also be stored in one or more chunks, which is illustrated in . Thus, it would be understood that the previous descriptions of metadata are considered to be non-limiting examples and the reference count mapping and may be extended to include other types of information.","In one embodiment, metadata information is replicated and stored in individual storage units to increase resiliency against the failure of an individual storage unit containing metadata. The amount of redundant storage of metadata to achieve acceptable tolerance of the failure of an individual storage unit containing metadata can be based on the requirements of an individual application.","IV. Other Benefits of Reference Count Mapping","The reference count map of the present invention provides various benefits. Each chunk on a disk\/pool has a reference count stored in a reference count map chunk. The chunks reference count appears in a sequential order as the chunks appear in the storage pool. The reference count is incremented and decremented as the volumes or snapshots are created and removed in the pool. The reference count map allows user to create unlimited snapshots. Additionally it simplifies the logic for reading and writing on the origin and the snapshot. It also combines new chunk allocation location, removing the new need to create a separate bitmap for a new chunk allocation. As an example, in one implementation, the origin radix root is cloned on snapshot creation. This permits, during an update write, a traversal of the nodes and an appropriate update of the usage count to support features such as ROW snapshots.","ASIC: Application Specific Integrated Circuit","API: Application Programming Interface","ATA: Advanced Technology Attachment","BIO: Block I\/O","CIFS: Common Internet File System","COW: Copy On Write","DMA: Direct Memory Access","FC: Fibre Channel","FCoE: Fibre Channel over Ethernet","FCP: Fibre Channel Protocol","FTP: File Transfer Protocol","HTML: Hyper Text Markup Language","HTTP: Hyper Text Transfer Protocol","I\/O: Input\/Output","IP: Internet Protocol","iSCSI: Internet Small Computer System Interface","NFS: Network File System","PCI: Peripheral Component Interconnect","POSIX: Portable Operating System Interface for uniX","RAID: Redundant Array of Independent Disks","RDMA: Remote Direct Memory Access","ROW: Redirect On Write","SATA: Serial ATA","SAS: Serial Attached SCSI","SCSI: Small Computer System Interface","SSD: Solid State Drive","SSL: Secure Sockets Layer","TCP: Transmission Control Protocol","XML: eXtensible Markup Language"],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 1A"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 10"}]},"DETDESC":[{},{}]}
