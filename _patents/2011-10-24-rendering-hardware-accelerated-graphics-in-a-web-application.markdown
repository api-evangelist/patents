---
title: Rendering hardware accelerated graphics in a web application
abstract: The subject disclosure is directed towards providing a web application with access to hardware accelerated graphics. A rendering format for a set of video frames is established. A graphics component, which is coupled to a graphics device and associated with an unsupported file type, is identified. The graphics component generates image data compromising the hardware accelerated graphics. When the web application requests a set of video frames, the image data is transformed into the set of video frames in accordance with the format. Then, the set of frames is communicated to a display device.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09047390&OS=09047390&RS=09047390
owner: Microsoft Technology Licensing, LLC
number: 09047390
owner_city: Redmond
owner_country: US
publication_date: 20111024
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["Conventional media platforms (e.g., MICROSOFT\u00ae Media Foundation) support common media types\/formats in an operating system. These media platforms implement a pipeline between a media file and an output device, which includes various components for reading, decoding and\/or presenting the media file to the output device. Software applications may employ these media platforms to simplify media integration (i.e., video playback) and streamline video\/audio rendering. The software applications may also use the media platforms to capture video\/audio data, which is encoded and stored in a storage system.","The media platform pipeline may also provide video content services, such as software video\/graphics processing and software video\/graphics decoding. Most media platforms, however, cannot support hardware accelerated graphics processing and\/or decoding because of various reasons. For instance, a lack of compatibility between these media platforms and recent web development platforms (e.g., HTML5) makes developing web applications with hardware accelerated graphics impractical. In addition, a graphics processing unit (GPU) may not support hardware accelerated capabilities. Without the ability to use the GPU to offload certain graphics operations, the web applications often render video content and\/or graphics very slowly and irregularly.","With respect to the media platforms that provide some hardware graphics rendering capabilities, a desirable level of user experience quality and\/or rendering performance may not be achieved. Furthermore, the hardware accelerated graphics rendering capabilities may not support certain file types (i.e., media formats). Extension models for the media platforms are designed for custom codecs and proprietary media streams and not for arbitrary hardware accelerated graphics rendering. Even if a file type is supported, the media platform does not permit user interaction with the rendered graphics. This is especially true for hardware accelerated three-dimensional (3D) video\/graphics rendering inside web applications, such as HTML5 documents. Because of all these reasons, developing web applications with rendered hardware accelerated graphics is cumbersome to accomplish.","This Summary is provided to introduce a selection of representative concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used in any way that would limit the scope of the claimed subject matter.","Briefly, various aspects of the subject matter described herein are directed towards rendering hardware accelerated graphics in a web application. In one aspect, a video stream extension is implemented for a file type that is not supported by a media platform. The video stream extension is registered with the media platform as a scheme handler of the file type. Instead of using a media platform pipeline component (i.e., a codec known as a media transform), the video stream extension may operate an application-specific rendering component for the file type known as a graphics component in order to retrieve hardware accelerated graphics in the form of resources and\/or render targets. Image data within the resources (e.g., texture arrays or vertex buffers) or render targets (e.g., drawing surfaces) are transformed into a set of video frames in accordance with a rendering format. In another aspect, the video stream extension allocates video samples using the set of video frames, which are communicated to another media platform pipeline component and presented on a display device.","In one aspect, the graphics component is implemented to use a graphics device to produce texture resources and\/or drawing surfaces comprising hardware accelerated graphics, such as three-dimensional hardware accelerated graphics. The graphics component serves as an intermediary between the video stream extension and the graphics device. In one implementation, the graphics component is not one of the media platform pipeline components, and thus such an implementation of the graphics component is not confined to a pre-defined interface specification. In another aspect, images stored within the texture resources are formatted in accordance with at least a portion of the video frame rendering format, which is determined by the web application.","In one aspect, the graphics component exposes one or more functions that are projected to the web application via a runtime mechanism. The resulting interface from such a projection is known as an interaction component. The one or more functions are configured to control the graphics component. When the web application invokes function calls, the graphics component manipulates the image data and creates a new set of hardware accelerated graphics to be presented on the display device within an output rendering of the web application.","Other advantages may become apparent from the following detailed description when taken in conjunction with the drawings.","Various aspects of the technology described herein are generally directed towards providing a web application with access to hardware accelerated graphics. In one exemplary implementation, a video stream extension is instantiated when the web application recognizes a video tag for a certain file type. However, a media platform for the web application is not designed for hardware accelerated graphics rendering for unsupported file types. In order to bypass inappropriate media transforms on the media platform, the video stream extension routes video sample requests from the web application to a custom rendering component for the certain file type known as a graphics component.","In one exemplary implementation, the video stream extension also provides the custom rendering component with an empty resource or render target, such as a two-dimensional texture or a drawing surface, respectively. The custom rendering component uses hardware accelerated services provided by a graphics device to render hardware accelerated graphics to image data, which is returned to the video stream extension in the form of a non-empty drawing surface or the 2D-texture resource comprising the non-empty drawing surface. After this drawing surface is converted into a media buffer, the video stream extension transforms the media buffer into a video sample. Similarly, the video stream extension extracts the drawing surface from the 2D-texture resource and allocates the video sample. The video stream extension communicates the video sample to a display device and returns an event to the web application indicating a completion of the video sample request.","It should be understood that any of the examples herein are non-limiting. As such, the present invention is not limited to any particular embodiments, aspects, concepts, structures, functionalities or examples described herein. Rather, any of the embodiments, aspects, concepts, structures, functionalities or examples described herein are non-limiting, and the present invention may be used various ways that provide benefits and advantages in computing and graphics rendering in general.",{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 1","b":["102","104"]},"The web application  may include a combination of browser-supported software code (e.g., JavaScript) and browser-rendered markup language (e.g., HTML5) that provides a user with some functionality. Deploying the web application  utilizes one or more development platforms (e.g., HTML5, MICROSOFT\u00ae Silverlight and\/or like). In one exemplary implementation, the web application  may include a rich Internet application that operates in a similar manner as desktop application software and interacts with the user via a browser. The web application  may be a component (e.g., a plug-in) of the browser.","According to one exemplary implementation, the creation mechanism  may include various components, such as a graphics component , a video stream extension , an interaction component  and image data , which, in cooperation with a media platform , provide the web application  with accelerated graphics hardware functionality. As described herein, the media platform  (e.g., MICROSOFT\u00ae Media Foundation) enables the development of web applications and other applications for using digital media on an operating system. Furthermore, the media platform  may include an end-to-end pipeline that handles data flow from source data to a destination, such as a display device.","In one exemplary implementation, the creation mechanism  registers the video stream extension  with the media platform  as a provider of data of a particular file type (e.g., a MIME type). Hence, when the web application  recognizes a video tag  associated with the particular file type, the browser retrieves such data from the video stream extension . In one exemplary implementation, the web application  issues requests for a video stream to the media platform , which instantiates the video stream extension  via a class factory module  and\/or relays the requests to the video stream extension .","The video stream may correspond with the media platform  media type and includes video\/image frames. Typically, this media type (e.g., MIME types or content types) is associated with a classification system used to identify video files\/streams. The media type may refer to a format for rendering or presenting the video frames on a display device. The video stream extension  renders the video frames according to such a format. When communicated to the media platform , the video frames may or may not be stored within proprietary containers, such as video samples.","The video tag  generally includes a script (e.g., software code) that requests the browser to load video content (i.e., a file or stream) from a location (e.g., a Uniform Resource Identifier (URI) or Uniform Resource Locator (URL)) according to one exemplary implementation. The video tag , for example, may be an HTML5<video> tag that specifies an embedded video on an HTML5 document to which visitors may watch and listen. The video tag  also includes attributes that control how the video is to be played and\/or specify events that, when occur, cause the browser to invoke another script from within the HTML5 document.","The class factory module  may refer to a design pattern under Component Object Model (COM) (e.g., a binary interface standard for enabling inter-process communication) and may use various .NET features such as reflection, interfaces, dynamic class\/object creation and\/or the like. The class factory module  may be an object that enables the media platform  or another application to instantiate objects dynamically from an arbitrary class pool, such as the creation mechanism  and\/or the exemplary components thereof. The media platform  may assign the class factory module  to a particular media type through a local registration such that the creation mechanism  is available to the web application  but is not available to another web application. Alternatively, the class factory module  may be registered with a system registry. During source resolution of the video tag , locally registered objects take precedence over objects that are registered in the system registry.","In one exemplary implementation, the class factory module  may be a helper object known as an activation object, which defers instantiation of the creation mechanism  until the video tag  requests a video stream. After determining a file type of the requested video stream, the media platform  may load an implementation of the creation mechanism  that comprises the appropriate graphics component  that corresponds with such a file type, which is not supported by the web application  and the media platform  components other than the video stream extension .","In one exemplary implementation, the graphics component  may be configured to use a graphics device  to perform hardware accelerated graphics operations exclusively on data of this file type. Accordingly, the graphics component  may be restricted to one or more applications that use the file type (i.e., application-specific). For example, the file type may refer to a three-dimensional avatar (i.e., an .avatar file) for a gaming console (e.g., MICROSOFT\u00ae Xbox) for which the graphics component  may be a custom rendering component that is designed to operate on a desktop computing environment. The graphics component  may use various graphics APIs (e.g., MICROSOFT\u00ae Direct3D 11, Direct2D and\/or the like) to provide hardware-accelerated two-dimensional and three-dimensional graphics for rendering the avatar and hence, may not be implemented to directly operate with the media platform  or any component thereof. Instead, the graphics component  may function as a proxy or intermediary component between the video stream extension  for the avatar and various accelerated hardware capabilities provided by the graphics device  as described herein.","The graphics device  generally includes various hardware and\/or software components that support various graphics functionality, such as primitive-based or vector graphics rendering, animation creation, multimedia integration, content composition and\/or the like. A typical embodiment of the graphics device  includes the graphics processing unit (GPU), which coordinates with other processors (e.g., a CPU) and dedicated memory (e.g., display memory) to enable the development of user interfaces\/controls, video\/image content, animations and\/or the like. In addition, the graphics device  may also enable three-dimensional (3D) graphics functionality including drawing, transforming and\/or animating three-dimensional graphics. Furthermore, the graphics device  provides hardware accelerated graphics (i.e., objects or entire scenes), which are transmitted to an output device, such as the display device (i.e., a monitor).","In one exemplary implementation, after the graphics device  generates the hardware accelerated graphics by creating a scene, blending surfaces, drawing geometry and\/or the like, the graphics component  may render these graphics to the image data  (e.g., a drawing surface). The hardware accelerated graphics may result from the execution of various graphics processing unit (GPU) commands including per-vertex operations, per-primitive operations, per-pixel operations and\/or the like. The creation mechanism  may communicate the image data  to the display device in a form of video frames or proprietary video samples.","According to one exemplary implementation, in addition to creating an object for the graphics component , the creation mechanism  instantiates the video stream extension , which may include various media platform  objects, such as a handler object, a media source and a media stream, as described herein. The video stream extension  may be implemented to establish a rendering format for a set of video frames that are converted from source data associated with the file type. The web application  may provide one or more attributes that define the rendering format. An example rendering format of each video frame may include a width (in pixels), a height (in pixels), a number of bits, an aspect ratio, a frame rate, various modes and\/or the like.","In one exemplary implementation, the video stream extension  may be implemented to communicate with the graphics component  and retrieve the image data  in the form of resources, such as a texture resource, and\/or rendering target, such as a drawing surface. For example, the texture resource may be used as a container for the drawing surface in which case the video stream extension  extracts the drawing surface, which is transformed into a video frame and\/or stored in a video sample.","In another exemplary implementation, the creation mechanism  instantiates the interaction component  comprising interface functions for controlling the graphics component . Some of these interface functions modify the image data  in response to user interaction with the video stream being presented on the web application . Via the interaction component , for example, the web application  may control animation functionality over the video stream. The web application  may start, seek or restart an animation by specifying where to start a playback and the graphics component  creates transformation images from this position.","A runtime mechanism  projects the interaction component  onto the web application  by binding the interface functions to complementary functions in another programming language. These bindings may be represented by common COM events. Therefore, the web application  may include a component running in JavaScript known as a video control  that invokes one or more of the interface functions even though the interaction component  is built on C++ or C. For example, the video control  may communicate one or more common COM-events to the interaction component .","In one exemplary implementation, the runtime mechanism  may be an application programming model for an operating system. The runtime mechanism  (e.g., MICROSOFT\u00ae Windows Runtime (WinRT)) enables interfacing between different programming languages and platforms. For example, the runtime mechanism  may include an execution environment and a collection of application programming interfaces (APIs) that provide access to operating system functionality. In one exemplary implementation, the runtime mechanism  may provide an object-oriented class-based type system that is built on metadata and may support constructs on the interaction component  with corresponding constructs that are found in the .NET framework (e.g., classes, methods, properties, delegates and events). Classes for the interface functions may be compiled to target the runtime mechanism  and may be written in any supported language and for any supported platform. Class metadata permits these compiled functions to interface with compiled versions of the complementary functions from any other programming language.","In one exemplary implementation, the runtime mechanism  projects the interaction component  to the execution environment being used by the web application , which is typically JavaScript. Generally, a projection is a process of exposing (i.e., binding) a user interface or a library to a plurality of execution environments: Native (C and C++), HTML\/JavaScript and .NET. Hence, a component authored in C++ or a .NET language, such as the interaction component , may be consumed within all of the execution environments. In addition, the runtime mechanism  creates bindings to C or C++ libraries that may be exposed to C# as object-oriented libraries.","In another exemplary implementation, the runtime mechanism  also includes a XAML-based User Interface (UI) system that is available to C++ developers, HTML\/Javascript developers and .NET developers. Accordingly, user interfaces for C++ may be initially written in XAML and then, ported to the C++ execution environment where both implementations are compiled to native x86. The user interfaces are exposed as objects that C# and Visual Basic may consume directly. Because the user interfaces are written in C++ and support reflection, dynamic languages, such as JavaScript, may use them efficiently.",{"@attributes":{"id":"p-0040","num":"0039"},"figref":["FIG. 2","FIG. 1"],"b":"114"},"An exemplary media platform architecture may be partitioned into a plurality of layers. A first layer encapsulates an end-to-end pipeline that handles data flow from source data  (e.g., a video file) to a display device  (e.g., a monitor), which comprises a plurality of components including a media source  and a media sink . The media source  may be an object that provides media data. The media sink  may be an object that consumes the media data, which is presented on an output device, such as the display device  or stored as a file on a persistent storage system. Typically, the media data communicated between the media source  and the media sink  may be transformed into a different form by various media transforms (e.g., a decoder, an encoder, a digital signal processor (DSP) and\/or the like). In one exemplary implementation, the graphics rendering component  transforms the media data into video frames instead of the various media transforms.","A media session  may be an object that manages the data flow within the media platform and controls video stream playback on behalf of the web application . The media session  may also create an object that is known as a topology  that represents the data flow within the media platform pipeline. The media session  may use an activation object  to create an instance the media source  from an URI, a file or a byte stream.","In one exemplary implementation, the instantiation of the media source  may be achieved by implementing a helper object that corresponds with a form of the source data  and is depicted as a handler . For example, the activation object  instantiates a scheme handler that accepts a URL or file location as input and creates the media source . If the source data  is a byte stream, the activation object  may create a byte stream handler instead. In an alternative implementation, the media session  may use a source resolver object to instantiate the media source . For example, the source resolver object may examine a system registry and identify the handler  associated with a URL scheme.","The handler  may define a format for rendering the video stream by configuring a media source presentation descriptor and a stream descriptor. Such a format is used by the media platform to present the video stream on the display device . For example, the format may define the video stream as a set of 32 bit RGB images\/video frames that are 640 pixels wide by 480 pixels tall (FRAME_WIDTH_PX and FRAME_HEIGHT_PX) at about 30 frames per second (FRAMES_PER_SEC). The media session  is now ready to render hardware accelerated graphics and display the video stream by calling exposed methods\/functions on the media source .","Typically, the media source  represents one or more streams in which each stream delivers media data of one type, such as audio or video. In one exemplary implementation, a media stream  provides a video stream for the media source . The media stream  is created when the media session  starts the media source . A video frame format for the video stream may be stored in a stream descriptor within the media stream . Via the media session , the web application  may call functions on the media stream  and requests video samples from the media stream . For each new video sample, the media stream  sends an event along with the new video sample to the media session .","In one exemplary implementation, the graphics component  may be an application-specific rendering component that operates as a proxy to hardware-accelerated functionality on the graphics device . Such functionality may be provided by a video processor  and\/or an accelerator . The video processor  supports GPU-accelerated video processing (e.g., using MICROSOFT\u00ae Direct3D version 11 or later). For example, the graphics component  may utilize the video processor  to perform various video processing operations (e.g., color space conversion, video resizing, de-interlacing, frame rate conversion, rotation, cropping, spatial left and right view unpacking and mirroring) on the source data .","The accelerator  represents a hardware accelerator for the graphics device  (e.g., a Direct3D device). As an example, the graphics component  may utilize the accelerator  to perform various hardware-accelerated graphics operations (e.g., decoding, media sample allocating and\/or the like) on the source data . As another example, the graphics component  may use the accelerator  to speed up operations performed by the video processor  and\/or offload CPU-intensive operations to the GPU.","In one exemplary implementation, the graphics component  may be implemented to use with an interface (e.g., various DirectX APIs) that provides high-fidelity, hardware-accelerated graphics capabilities. When conventional media transforms are unable to support these capabilities, the graphics component  provides the rich-Internet application  and\/or the media platform with access to such an Interface. For example, the graphics component  may be a Direct3D-aware component if the graphics component  is able to process and\/or allocate video samples that include (Direct3D) resources, such as drawing surfaces or texture resources. Supporting Direct3D provides the video stream extension  with access to the hardware-accelerated graphics operations that are implemented by various APIs, such as Direct3D 11 Device Driver Interface (DDI) and\/or the like.","With respect to hardware graphics rendering, a resource in general comprises an area in memory that may be accessed by the graphics device  (e.g., MICROSOFT\u00ae Direct3D device). In order for the graphics device  to access memory efficiently, data that is provided to or provided by the graphics device  (e.g., input geometry or geometry-oriented resources, shader resources, drawing surfaces, texture resources and\/or the like) is stored as the resource. Each resource derives from two or more fundamental resource types, such as a buffer and a texture. In one exemplary implementation, components of the graphics device  create resources of various types. Examples of the various resource types include the following: vertex buffers, index buffers, constant buffers, texture arrays, surface buffers and shader resources. Other forms of resources may include maps, such as mipmaps, cube maps, volume maps, environmental maps and\/or the like. An environment map is used to texture-map scene geometry to provide a more sophisticated scene without using complex geometry.","In order to manage a collection of texel data, the graphics device  provides the graphics component  with a texture resource. A texel is a fundamental unit of texture space and may be referred to as a texture element or texture pixel. A texture array may include one or more texture resources, having a same type and dimensions, which may be indexed from within the graphics component  or the media sink . Each texture resource may be used as a render target or a depth-stencil resource. An example texture resource may be a drawing surface (e.g., a DirectX surface). Hence, each drawing surface within a row of the texture array may serve a different purpose when rendering a video frame. As an alternative, the drawing surface may be stored in a type of media buffer known as a surface buffer.","In one exemplary implementation, after rendering hardware accelerated graphics to the drawing surface (e.g., a MICROSOFT\u00ae DirectX Graphics Infrastructure (DXGI) surface), the graphics component  may store the drawing surface within the texture array, which is communicated to the media stream . Drawing surfaces may be used in a variety of ways, such as rendering targets, and may include a two-dimensional area of pixel values. After the graphics component  communicates the texture array, the media stream  queries the texture array for an interface, such as an ID3D11Texture2D interface, through which the media stream  retrieves the drawing sample.","In order to provide the media sink  with a video sample, the media stream  creates a buffer (e.g., a DXGIBuffer) that represents the drawing surface for a surface type, such as IID_ID3D11Texture2D, according to one exemplary implementation. Depending on a rendering format stored within a stream descriptor, the media stream  copies the single image or video frame into the buffer in a top-down format, which is compatible with MICROSOFT\u00ae Direct3D, or a bottom-up format. Then, the media stream  allocates the video sample using the single image or video frame.","In one exemplary implementation, the media sink  includes various objects, such as a mixer and a video presenter. The mixer receives the video samples from the media stream  and extracts a set of video frames. The mixer may also utilize the video processor  to perform deinterlacing and color correction. The video presenter may receive the set of video frames (typically from the mixer or directly from the media stream ) and renders the set of video frames to the display device  according to the video frame format, which is defined by the media stream  (e.g., in the stream descriptor).","In one exemplary implementation, the graphics component  may use a command list to generate hardware accelerated graphics. The command list includes a sequence of various GPU commands that may be recorded and played back by the graphics device . These GPU commands may be generated using a device context associated with the graphics device . The device context generally refers to a circumstance or setting in which the graphics device  may be used. The device context describes resources owned by the graphics device  and may be used to set a graphics pipeline state.","In one exemplary implementation, the device context includes an interface that may manage various graphics pipeline stages (e.g., a vertex shader, a geometry shader, a pixel shader and\/or the like), execute commands in a compute shader, draw primitives and\/or the like. The graphics component  uses such an interface to create render targets (i.e., image data) comprising hardware accelerated graphics. The graphics component  extracts the device context associated with the graphics device  from memory operated by a device manager  and examines GPU commands that are supported by a graphics driver. The graphics component  selects one or more appropriate GPU commands and generates the command list to be executed on the graphics device .","As a response, the graphics device  creates a video processor render target, such as a drawing surface, according to one exemplary implementation. The video processor  may define, in pixels, a width and a height of the drawing surface as well as a pixel format. In addition, the video processor  may set a drawing surface format. After performing a video processing operation on the source data , the video processor  writes results to a drawing surface. The accelerator  may improve performance of the video processor  and\/or may apply various accelerated decoding operations during the drawing surface rendering.","The video stream extension  may transform the drawing surface into a video frame and create a video sample from the video frame. For example, the video stream extension  may adjust an aspect ratio to match the rendering format for the video frame. The video sample refers to a specific media sample implementation and a container object for the video frame. Each media sample type contains zero or more media buffers. The buffers are maintained in an ordered list and accessed by index value.","The graphics component  may instruct the device manager  (e.g., a MICROSOFT\u00ae DirectX Graphics Infrastructure (DXGI) Device Manager) to allocate video samples from texture resources and\/or drawing surfaces. The graphics component  specifies the number of samples to allocate and the rendering format for the video frames within the video samples. The device manager  may create an allocator object to produce the video samples, which are consumed by the media sink  and presented to the display device .","In one alternative exemplary implementation, because the graphics component  is an application-specific component for the source data  format, the graphics component  may supply a custom implementation of the video presenter to the media sink . Such an application-specific implementation operates as an intermediary between the media sink  and various graphics acceleration services that are provided by the accelerator  and\/or the video processor . The video presenter uses the graphics  device to extract video frames from the video samples and perform various operations (e.g., accelerated decoding and\/or accelerated video processing) in order to render the video frames onto the display device .","Furthermore, the video presenter instantiates the device manager  for the purpose of operating the accelerator  and\/or the video processor . The device manager  in general enables two threads, such as the graphics component  and\/or the media sink , to share a logical version of the graphics device  (e.g., MICROSOFT\u00ae Direct3D 11 device). For example, the device manager  may an instance of the MICROSOFT\u00ae DXGI Device Manager. This logical version may provide some of the functionality implemented by a driver for the graphics device .","As described herein, the interaction component  projects interface functions exposed by a custom rendering component, such as the graphics component . Via the web application , a user may call some of these interface functions by interacting with image data being displayed. For example, the web application  may desire to change the clothing on a three-dimensional avatar that is produced by the graphics component . When the web application  invokes a corresponding function call on the interface, the graphics component  uses the graphics device  to render hardware accelerated graphics to new image data (e.g., a resource), which illustrates a new set of clothes. As another example, a user may instruct the three-dimensional avatar to perform an animation. In response to this user interaction, the web application  invokes an appropriate function call to which the graphics component  responds with a set of images illustrating movement of the three-dimensional avatar.",{"@attributes":{"id":"p-0062","num":"0061"},"figref":["FIG. 3","FIG. 3","FIG. 1"],"b":["302","304","104","106"]},"Step  is directed to implementing a video stream extension for the media type. In one exemplary implementation, the creation mechanism  defines a rendering format of a set of video frames forming the custom video stream, which is stored in various descriptors. The creation mechanism  also couples the video stream extension, such as the video stream extension  of , with an object implementing the graphics component. The creation mechanism  implements one or more functions that process resources provided by the graphics component and create the video frames. Some of these functions may also wrap the set of video frames into video samples that are consumable by the media platform.","Step  refers to registering the video stream extension with the media platform. In one exemplary implementation, the creation mechanism  generates an object (e.g., an activation object) that defers instantiation of the video stream extension until the web application recognizes a video tag for the media type. The creation mechanism  may register the video stream extension as a local handler for a URI scheme associated with the media type such that the video stream extension may not be used by another application.","Step  represents an instantiation of the video stream extension in response to a request. When the web application initiates requests for video samples having the media type, the creation mechanism  produces one or more objects that constitute the video stream extension, such as a scheme handler, a media source and a media stream. The media source reads bytes from source data (e.g., content that is stored locally and\/or downloaded from the URI) and prepares the media stream to complete the video sample requests.","Step  is directed to projecting an interface for controlling the graphics component. In one exemplary implementation, the creation mechanism  exposes functions provided by the graphics component  to the web application through a runtime mechanism. The exposed functions form an interaction component, such as the interaction component  of , and manipulate the custom video stream in response to user interactions with the web application. Step  terminates the steps described in .",{"@attributes":{"id":"p-0067","num":"0066"},"figref":["FIG. 4","FIG. 4"],"b":["402","404","108"]},"Step  refers to establishing a format for rendering a set of video frames onto an output device, such as a display device. An example format may define a number of video frames within a time period, a number of bits per video frame, known video\/image formats that are potentially within each video frame and\/or other attributes. Step  is directed to a determination as to whether a sample request is received from the web application. If such a request has not arrived, the video stream extension waits at step . For example, the web application may issue the sample request every thirty seconds.","As soon as the sample request is received, step  or step  proceeds to step  where the video stream extension routes the request to the graphics component along with a portion of the source data and the rendering format. As described herein, the graphics component uses the graphics device to perform various hardware accelerated graphics operations on the source data.","Step  is directed to receiving an image from the graphics component in a form of a resource and transforming the resource into a media buffer. In one exemplary implementation, the image may be a drawing surface embedded within a two-dimensional texture array. The video stream extension  extracts the drawing surface from the two-dimensional texture array and stores the drawing surface within the media buffer.","Step  is directed to allocating a video sample using the media buffer and setting a duration for the video sample. In one exemplary implementation, the video stream extension  creates an allocator object that consumes the media buffer and produces the video sample set to a specific time period. Accordingly, the display device presents the video sample for the specific time period. In another exemplary implementation, the video stream extension  creates the video sample from the media buffer (e.g., a surface buffer) using a pre-defined function in the media platform.","Step  refers to a determination as to whether there are more video samples to produce based on the rendering format. For example, there may be more sample requests remaining in a queue to be fulfilled. These sample requests may have been sent while the video stream extension was allocating a previous video sample. If there are additional sample requests to complete, the steps described in  return to step . If there are no more sample requests in the queue, the steps described in  return to step . On the other hand, if an \u201cend of stream\u201d condition is observed, the steps described in  proceeds to step . For example, a media source may return an end of stream event to a media stream signifying no more source data to be rendered. Alternatively, the steps described in  may return to step  and await user interaction via interface functions provided by the graphics component. Step  is directed to terminating the steps described in .",{"@attributes":{"id":"p-0073","num":"0072"},"figref":["FIG. 5","FIG. 5"],"b":["502","504","110"]},"As described herein, the interaction component  includes various functions exposed by a custom rendering component for the video stream, such as the graphics component . These functions are used to control operations performed by the custom rendering component. In order for these accessible to the web application, the runtime mechanism binds the functions to complementary functions in another programming language. For example, the functions may be implemented in C++ and projected into JavaScript enabling JavaScript software code running within the web application to invoke function calls.","Step  is directed to processing a function call to the interface. Step  is directed to instructing the graphics component to execute the function call. In one exemplary implementation, for each function call, the interaction component  routes the function call to the custom rendering component, which performs necessary hardware accelerated graphics operations for modifying the image data. When a video stream extension, such as the video stream extension  of , receives subsequent sample requests, the modified image data is returned to the web application as a video stream.","Step  is directed to receiving confirmation that the function call executed. Step  determines whether there is a next function call. If there is a next function call, step  return to step . If there are no more function calls, the step  proceed to step . Step  refers to terminating the steps described in .",{"@attributes":{"id":"p-0077","num":"0076"},"figref":["FIG. 6","FIG. 6"],"b":["602","604","106","106"]},"Step  is directed to extracting a device context from the graphics device and recording hardware accelerated graphics operations into a command list. In one exemplary implementation, the command list may refer to GPU rendering commands that, when executed on the source data, produces hardware accelerated graphics. Step  determines whether to produce three-dimensional graphics for the web application. If the graphics component is to produce the three-dimensional graphics, the steps described in  proceed to step . If the graphics component is to produce two-dimensional graphics, the steps described in  proceed to step .","Step  represents the creation of a two-dimensional rendering target. In one exemplary implementation, the graphics component  creates the two-dimensional rendering target that maps to specific areas of the GPU. Step  refers to using the graphics device to execute the command list on the rendering target. In one exemplary implementation, the graphics component  receives two-dimensional hardware accelerated graphics from the graphics device. Step  refers to rendering the two-dimensional hardware accelerated graphics onto the two-dimensional rendering target. Step  refers to returning the two-dimensional rendering target to the video stream extension.","Step  refers to creating one or more drawing surfaces. A typical drawing surface represents a rectangular pixel area and usually resides in the display memory within the graphics device. In one exemplary implementation, each drawing surface may be a rendering target for a video processor. Step  refers to using the graphics device to execute the command list on the one or more drawing surfaces. Step  refers to rendering the three-dimensional hardware accelerated graphics onto the one or more drawing surfaces.","Step  refers to creating a texture resource and then, returning the texture resource to the video stream extension. In one exemplary implementation, the graphics component  creates a two-dimensional texture resource if there is only one drawing surface. In another exemplary implementation, the graphics component  creates a two-dimensional texture array comprising a plurality of drawing surfaces. Alternatively, the graphics component  creates a surface buffer to manage the one or more drawing surfaces. The two-dimensional texture array may enable the graphics component  to add video effects, draw geometry and\/or blend two or more of the drawing surfaces. Optionally, the graphics component  may return the one or more drawing surfaces. Step  refers to terminating the steps described in .","Exemplary Networked and Distributed Environments","One of ordinary skill in the art can appreciate that the various embodiments and methods described herein can be implemented in connection with any computer or other client or server device, which can be deployed as part of a computer network or in a distributed computing environment, and can be connected to any kind of data store or stores. In this regard, the various embodiments described herein can be implemented in any computer system or environment having any number of memory or storage units, and any number of applications and processes occurring across any number of storage units. This includes, but is not limited to, an environment with server computers and client computers deployed in a network environment or a distributed computing environment, having remote or local storage.","Distributed computing provides sharing of computer resources and services by communicative exchange among computing devices and systems. These resources and services include the exchange of information, cache storage and disk storage for objects, such as files. These resources and services also include the sharing of processing power across multiple processing units for load balancing, expansion of resources, specialization of processing, and the like. Distributed computing takes advantage of network connectivity, allowing clients to leverage their collective power to benefit the entire enterprise. In this regard, a variety of devices may have applications, objects or resources that may participate in the resource management mechanisms as described for various embodiments of the subject disclosure.",{"@attributes":{"id":"p-0084","num":"0083"},"figref":"FIG. 7","b":["710","712","720","722","724","726","728","730","732","734","736","738","710","712","720","722","724","726","728"]},"Each computing object , , etc. and computing objects or devices , , , , , etc. can communicate with one or more other computing objects , , etc. and computing objects or devices , , , , , etc. by way of the communications network , either directly or indirectly. Even though illustrated as a single element in , communications network  may comprise other computing objects and computing devices that provide services to the system of , and\/or may represent multiple interconnected networks, which are not shown. Each computing object , , etc. or computing object or device , , , , , etc. can also contain an application, such as applications , , , , , that might make use of an API, or other object, software, firmware and\/or hardware, suitable for communication with or implementation of the application provided in accordance with various embodiments of the subject disclosure.","There are a variety of systems, components, and network configurations that support distributed computing environments. For example, computing systems can be connected together by wired or wireless systems, by local networks or widely distributed networks. Currently, many networks are coupled to the Internet, which provides an infrastructure for widely distributed computing and encompasses many different networks, though any network infrastructure can be used for exemplary communications made incident to the systems as described in various embodiments.","Thus, a host of network topologies and network infrastructures, such as client\/server, peer-to-peer, or hybrid architectures, can be utilized. The \u201cclient\u201d is a member of a class or group that uses the services of another class or group to which it is not related. A client can be a process, e.g., roughly a set of instructions or tasks, that requests a service provided by another program or process. The client process utilizes the requested service without having to \u201cknow\u201d any working details about the other program or the service itself.","In a client\/server architecture, particularly a networked system, a client is usually a computer that accesses shared network resources provided by another computer, e.g., a server. In the illustration of , as a non-limiting example, computing objects or devices , , , , , etc. can be thought of as clients and computing objects , , etc. can be thought of as servers where computing objects , , etc., acting as servers provide data services, such as receiving data from client computing objects or devices , , , , , etc., storing of data, processing of data, transmitting data to client computing objects or devices , , , , , etc., although any computer can be considered a client, a server, or both, depending on the circumstances.","A server is typically a remote computer system accessible over a remote or local network, such as the Internet or wireless network infrastructures. The client process may be active in a first computer system, and the server process may be active in a second computer system, communicating with one another over a communications medium, thus providing distributed functionality and allowing multiple clients to take advantage of the information-gathering capabilities of the server.","In a network environment in which the communications network  or bus is the Internet, for example, the computing objects , , etc. can be Web servers with which other computing objects or devices , , , , , etc. communicate via any of a number of known protocols, such as the hypertext transfer protocol (HTTP). Computing objects , , etc. acting as servers may also serve as clients, e.g., computing objects or devices , , , , , etc., as may be characteristic of a distributed computing environment.","Exemplary Computing Device","As mentioned, advantageously, the techniques described herein can be applied to any device. It can be understood, therefore, that handheld, portable and other computing devices and computing objects of all kinds are contemplated for use in connection with the various embodiments. Accordingly, the below general purpose remote computer described below in  is but one example of a computing device.","Embodiments can partly be implemented via an operating system, for use by a developer of services for a device or object, and\/or included within application software that operates to perform one or more functional aspects of the various embodiments described herein. Software may be described in the general context of computer executable instructions, such as program modules, being executed by one or more computers, such as client workstations, servers or other devices. Those skilled in the art will appreciate that computer systems have a variety of configurations and protocols that can be used to communicate data, and thus, no particular configuration or protocol is considered limiting.",{"@attributes":{"id":"p-0094","num":"0093"},"figref":"FIG. 8","b":["800","800","800","800"]},"With reference to , an exemplary remote device for implementing one or more embodiments includes a general purpose computing device in the form of a computer . Components of computer  may include, but are not limited to, a processing unit , a system memory , and a system bus  that couples various system components including the system memory to the processing unit .","Computer  typically includes a variety of computer readable media and can be any available media that can be accessed by computer . The system memory  may include computer storage media in the form of volatile and\/or nonvolatile memory such as read only memory (ROM) and\/or random access memory (RAM). By way of example, and not limitation, system memory  may also include an operating system, application programs, other program modules, and program data.","A user can enter commands and information into the computer  through input devices . A monitor or other type of display device is also connected to the system bus  via an interface, such as output interface . In addition to a monitor, computers can also include other peripheral output devices such as speakers and a printer, which may be connected through output interface .","The computer  may operate in a networked or distributed environment using logical connections to one or more other remote computers, such as remote computer . The remote computer  may be a personal computer, a server, a router, a network PC, a peer device or other common network node, or any other remote media consumption or transmission device, and may include any or all of the elements described above relative to the computer . The logical connections depicted in  include a network , such local area network (LAN) or a wide area network (WAN), but may also include other networks\/buses. Such networking environments are commonplace in homes, offices, enterprise-wide computer networks, intranets and the Internet.","As mentioned above, while exemplary embodiments have been described in connection with various computing devices and network architectures, the underlying concepts may be applied to any network system and any computing device or system in which it is desirable to improve efficiency of resource usage.","Also, there are multiple ways to implement the same or similar functionality, e.g., an appropriate API, tool kit, driver code, operating system, control, standalone or downloadable software object, etc. which enables applications and services to take advantage of the techniques provided herein. Thus, embodiments herein are contemplated from the standpoint of an API (or other software object), as well as from a software or hardware object that implements one or more embodiments as described herein. Thus, various embodiments described herein can have aspects that are wholly in hardware, partly in hardware and partly in software, as well as in software.","The word \u201cexemplary\u201d is used herein to mean serving as an example, instance, or illustration. For the avoidance of doubt, the subject matter disclosed herein is not limited by such examples. In addition, any aspect or design described herein as \u201cexemplary\u201d is not necessarily to be construed as preferred or advantageous over other aspects or designs, nor is it meant to preclude equivalent exemplary structures and techniques known to those of ordinary skill in the art. Furthermore, to the extent that the terms \u201cincludes,\u201d \u201chas,\u201d \u201ccontains,\u201d and other similar words are used, for the avoidance of doubt, such terms are intended to be inclusive in a manner similar to the term \u201ccomprising\u201d as an open transition word without precluding any additional or other elements when employed in a claim.","As mentioned, the various techniques described herein may be implemented in connection with hardware or software or, where appropriate, with a combination of both. As used herein, the terms \u201ccomponent,\u201d \u201cmodule,\u201d \u201csystem\u201d and the like are likewise intended to refer to a computer-related entity, either hardware, a combination of hardware and software, software, or software in execution. For example, a component may be, but is not limited to being, a process running on a processor, a processor, an object, an executable, a thread of execution, a program, and\/or a computer. By way of illustration, both an application running on computer and the computer can be a component. One or more components may reside within a process and\/or thread of execution and a component may be localized on one computer and\/or distributed between two or more computers.","The aforementioned systems have been described with respect to interaction between several components. It can be appreciated that such systems and components can include those components or specified sub-components, some of the specified components or sub-components, and\/or additional components, and according to various permutations and combinations of the foregoing. Sub-components can also be implemented as components communicatively coupled to other components rather than included within parent components (hierarchical). Additionally, it can be noted that one or more components may be combined into a single component providing aggregate functionality or divided into several separate sub-components, and that any one or more middle layers, such as a management layer, may be provided to communicatively couple to such sub-components in order to provide integrated functionality. Any components described herein may also interact with one or more other components not specifically described herein but generally known by those of skill in the art.","In view of the exemplary systems described herein, methodologies that may be implemented in accordance with the described subject matter can also be appreciated with reference to the flowcharts of the various figures. While for purposes of simplicity of explanation, the methodologies are shown and described as a series of blocks, it is to be understood and appreciated that the various embodiments are not limited by the order of the blocks, as some blocks may occur in different orders and\/or concurrently with other blocks from what is depicted and described herein. Where non-sequential, or branched, flow is illustrated via flowchart, it can be appreciated that various other branches, flow paths, and orders of the blocks, may be implemented which achieve the same or a similar result. Moreover, some illustrated blocks are optional in implementing the methodologies described hereinafter.","Conclusion","While the invention is susceptible to various modifications and alternative constructions, certain illustrated embodiments thereof are shown in the drawings and have been described above in detail. It should be understood, however, that there is no intention to limit the invention to the specific forms disclosed, but on the contrary, the intention is to cover all modifications, alternative constructions, and equivalents falling within the spirit and scope of the invention.","In addition to the various embodiments described herein, it is to be understood that other similar embodiments can be used or modifications and additions can be made to the described embodiment(s) for performing the same or equivalent function of the corresponding embodiment(s) without deviating therefrom. Still further, multiple processing chips or multiple devices can share the performance of one or more functions described herein, and similarly, storage can be effected across a plurality of devices. Accordingly, the invention is not to be limited to any single embodiment, but rather is to be construed in breadth, spirit and scope in accordance with the appended claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The present invention is illustrated by way of example and not limited in the accompanying figures in which like reference numerals indicate similar elements and in which:",{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 8"}]},"DETDESC":[{},{}]}
