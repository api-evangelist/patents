---
title: Method and apparatus for automatically detecting breast lesions and tumors in images
abstract: A method and apparatus for automatically detecting breast tumors and lesions in images, including ultrasound, digital and analog mammograms, and MRI images, is provided. An image of a breast is acquired. The image is filtered and contrast of the image is enhanced. Intensity and texture classifiers are applied to each pixel in the image, the classifiers indicative of the probability of the pixel corresponding to a tumor. A seed point is identified within the image, and a region of interest is grown around the seed point. Directional gradients are calculated for each pixel of the image. Boundary points of the region of interest are identified. The boundary points are passed as inputs to a deformable model. The deformable model processes the boundary points to indicate the presence or absence of a tumor.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07466848&OS=07466848&RS=07466848
owner: Rutgers, The State University of New Jersey
number: 07466848
owner_city: New Brunswick
owner_country: US
publication_date: 20031215
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["RELATED APPLICATIONS","STATEMENT OF GOVERNMENT INTERESTS","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION","Experimental Results","Qualitative Results","Quantitative Results","Sensitivity Analysis"],"p":["This application claims the benefit of U.S. Provisional Application Ser. No. 60\/433,028 filed Dec. 13, 2002 and U.S. Provisional Application Ser. No. 60\/439,031 filed Jan. 10, 2003, the entire disclosures of which are both expressly incorporated herein by reference.","The present invention was made under National Science Foundation Contract No. NSF 9820794. Accordingly, the Government may have certain rights to the present invention.","1. Field of the Invention","The present invention relates to a method an apparatus for automatically detecting breast tumors and lesions in images. More specifically, the present invention relates to a method and apparatus for automatically segmenting, classifying, and detecting breast lesions in ultrasound, digital and analog mammogram, and magnetic resonance imaging (MRI) images.","2. Related Art","Breast cancer is the most frequently diagnosed malignancy and the second leading cause of mortality in women. In the last decade, ultrasound imaging, along with digital mammography, has become the standard for breast cancer diagnosis. Mammography is the most effective method for early detection of breast cancer, and periodic screening of asymptomatic women reduces the mortality rate. Typically, the first step in breast cancer detection is a screening mammogram, which comprises a low-dose x-ray examination on asymptomatic women. This can be followed by a diagnostic mammogram, which is an x-ray examination done to evaluate a breast complaint or to investigate an abnormality found during a physical examination or screening mammogram. Breast ultrasound is sometimes used to evaluate breast abnormalities that are found during screening mammography, diagnostic mammography, or a physical exam. If a suspicious object is found in the ultrasound image, a surgical biopsy or core needle biopsy is then recommended.","Most ultrasound and digital mammograms are manually interpreted by radiologists. However, manual interpretation is often inaccurate, and can fail to detect the presence of breast tumors and lesions. For example, between 10-30% of women who have breast cancer and undergo mammography have negative mammograms. In about two-thirds of these cases, the radiologist failed to detect retrospectively evident cancer. Such misses have been attributed to the subtle nature of the visual findings, poor image quality, or fatigue and\/or oversight by the radiologist.","Several algorithms have been developed which claim to automatically classify breast lesions in ultrasound images. However, such algorithms rely on manual delineation of the tumor boundaries, and do not automatically delineate such boundaries. Further, automatically detecting tumors and extracting lesion boundaries in ultrasound images and digital mammograms is difficult due to the specular nature and the variance in shapes and appearances of sonographic lesions, as well as shadowing artifacts and tumor-like structures in the image, such as glandular tissue, coopers ligaments, and sub-cutaneous fat. Such obstacles make it difficult to automatically determine the lesion area using conventional image processing and computer vision techniques alone.","Many of the aforementioned algorithms rely on a priori shape information of the organ or structure of interest in order to effectuate segmentation. For example, a priori shape information has been used to segment ventricular structures in echocardiograms. However, such algorithms are not suitable for detecting breast lesions, due to variances of lesion shapes and the fact that lesion margins are often poorly defined. Region-based methods have been developed (e.g., fuzzy connectedness) which use homogeneity statistics coupled with low-level image features such as intensity, texture, histograms, and gradient to assign pixels to objects. In such methods, if two pixels are similar in value and connected to each other in some sense, they are assigned to the same object. These approaches, however, do not consider any shape information. As a result, such methods cannot deal with shadowing artifacts, which are common in ultrasound images.","Some researchers have proposed hybrid segmentation techniques to detect breast lesions. These approaches seek to exploit the local neighborhood information of region-based techniques, and the shape and higher-level information of boundary-based techniques. However, without manual intervention, these hybrid techniques cannot automatically distinguish other structures in the sonogram, such as sub-cutaneous fat, coopers ligaments and glandular tissue, from the true lesion.","In recent years, automated ultrasonic lesion segmentation schemes have been proposed, including techniques that uses a combination of the maximum a posteriori (MAP) and Markov Random Field (MRF) methods to estimate ultrasound field distortions and label image regions based on the corrected intensity statistics. However, the imaging model breaks down in the presence of shadowing artifacts. Other approaches attempt to automatically extract and classify ultrasonic breast lesions using fuzzy reasoning. All the pixels in the image are initially classified as normal, tumor, or boundary using a LOG filter. Subsequently, three types of images are generated corresponding to the grade of the pixel. The extracted tumor region is then classified as malignant or benign. Such systems do not consider the problem of speckle noise, shadowing artifacts, or tumor-like structures such as glandular and fatty tissue in the image.","Accordingly, what would be desirable, but has not yet been provided, is a method and apparatus for automatically segmenting and detecting breast lesions and tumors in images, including ultrasound, MRI, and digital and analog mammogram images, without requiring human intervention.","The present invention provides a method for automatically detecting breast tumors and lesions in images, including ultrasound, digital mammogram, and MRI images. The method comprises the steps of acquiring an image of a breast; filtering the image to remove speckle and enhance contrast; applying texture and intensity classifiers to each pixel of the image, the classifiers corresponding to probabilities of the pixel belonging to a lesion or tumor; determining a seed point in the image; growing a region of interest around the seed point; calculating directional gradients for each pixel in the image; determining boundary points of the region of interest using the directional gradients; and processing the boundary points with a deformable model to determine the presence or absence of a tumor or lesion in the image.","The present invention provides an apparatus for automatically detecting breast tumors and lesions in images. A scanner, such as a digital mammogram scanner, an ultrasound scanner, or an MRI scanner, generates an image of a breast. A filter filters the image to remove speckle and enhance contrast of the image. A processor calculates and applies texture and intensity classifiers to each pixel of the image, the classifiers corresponding to probabilities of the pixel belonging to a lesion or tumor. The processor determines a seed point in the image, and grows a region of interest around the seed point. Directional gradients for each pixel of the image are calculated by the processor, and boundary points of the region of interest are determined. A deformable model processes the boundary points to determine the presence or absence of a tumor or lesion.","The present invention relates to a method and apparatus for automatically detecting breast tumors and lesions in images, including ultrasound, digital and analog mammograms, and MRI images. An image of a breast is acquired. The image is filtered and contrast of the image is enhanced. Intensity and texture classifiers are applied to each pixel in the image, the classifiers indicative of the probability of the pixel corresponding to a tumor. A seed point is identified within the image, and region of interest is grown around the seed point. Boundary points of the region of interest are identified. The boundary points are passed as inputs to a deformable model. The deformable model processes the boundary points to indicate the presence or absence of a tumor.","The present invention analyzes spatial distributions of various anatomic structures within a breast image, in addition to echogenicity of a lesion and its internal echo pattern, as three discriminating features for segmenting breast lesions and tumors in images. In sonographic images where the ultrasound transducer placed on the region of interest, the lesion appears roughly in the middle of the image. The skin appears as a bright linear echo near the top of the image. Sub-cutaneous fat typically appears just below the skin region. Coopers ligaments appear as septum-like or tent-like structures that arise from the surface of the breast parenchyma. The glandular region is separated from the sub-cutaneous fat by the superficial fascia. The ribs appear in the lower most part of the image and are associated with dense posterior acoustic shadowing.","Internal echo pattern refers to the texture or arrangement of echoes within a focal sonographic lesion. A non-homogeneous arrangement with few echoes, or even more, is suspicious for malignancy. A homogeneous internal echo pattern is more characteristic of sub-cutaneous fat. The echogenicity of a focal lesion is assessed in relation to the echo characteristics of adjacent tissues. The various grades of echogenicity are stated in reference to known structures, i.e., fat and glandular tissue. If a focal lesion appears less echogenic than fat, it is described as \u201calmost anechoic.\u201d Such a lesion would appear darker than the surrounding fatty tissue. A \u201chypoechoic\u201d focal lesion is less echogenic than glandular structures but more echogenic than fat (i.e., it appears darker than the glandular tissue but lighter than the fatty tissue). \u201cIsoechoic\u201d closely approximates the echogenicity of the glandular structures, while \u201chyperechoic\u201d is used when the focal lesions appear brighter than the surrounding glandular tissue. Malignant lesions have been classified as \u201cmarkedly hypoechoic\u201d where there appear nodules that are very black compared to the surrounding isoechoic fat. \u201cAnechoic\u201d focal lesions have been identified as the hallmark of a cyst. Hence, both cysts and malignant lesion appear darker than glandular tissue or fat, which are usually either isoechoic or hyperechoic. Sub-cutaneous fat, on the other hand, is usually hypoechoic. These criteria are referred to in the relevant literature as the Stavros criteria, and are analyzed by the present invention to automatically segment and detect breast tumors and lesions in images.",{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 1","b":["10","20","30","40","50"]},"In step , an intensity and texture classifier is applied to each pixel in the filtered and contrast-enhanced image. The classifier indicates the probability of the pixel corresponding to a tumor, and is generated using joint probability distribution functions for intensities and textures based upon empirical data. In step , a seed point within the filtered and contrast-enhanced image is generated. The seed point generated in step  and the joint intensity and texture probabilities calculated in step  are processed in step  to grow a region of interest around the seed point. In step , a directional gradient is calculated for each pixel in the image using the seed point generated in step  and the region of interest grown in step .","In step , boundary points of the region of interest grown in step  are determined based upon the directional gradients calculated in step . Local maxima are also removed from potential boundary points. In step , the boundary points are passed as inputs to a deformable model. In step , the outputs of the deformable model indicate the presence or absence of a tumor. Thus, as can be readily appreciated, the present invention allows for the automatic segmentation and detection of breast tumors and lesions in mammograms.",{"@attributes":{"id":"p-0042","num":"0041"},"figref":["FIG. 2","FIG. 1"],"b":["30","32"]},"An advantage of using a Butterworth filter is its low computational complexity. For kernel-based methods such as un-sharp masking, and adaptive enhancement using local statistics, the computational times depend on the size of the image kernel. Hence, for a 5-by-5 kernel, 25 additions are required for a simple averaging operation. Importantly, Butterworth smoothing of the original image does not lead to an extensive loss of image details. The degree of enhancement can be adjusted by changing the coefficients of the low-pass filter, making the enhancement procedure very flexible. Further, the Butterworth filter can be easily extended to multiple dimensions without significant increase in computational complexity. The output of the low pass filter for the 2-dimensional case is computed using the following equations:\n\n()=\u03b3(\u03c6)\u03a8()\u03c6+() \u2003\u2003(1)\n\n\u03a8()=\u03b3(\u03c6)=\u03b3(\u03c6) \u03c6\u03c1()+\u03a8(1,) \u2003\u2003(2)\n\n\u03c1()=\u03b3(\u03c6)()\u03c6+\u03c1(1) \u2003\u2003(3)\n\nIn the above equations E(a,b) is the input image where a,b are the row and column positions of the pixels, \u0112(a,b) is the output of the low-pass filter and \u03c1(a,b) and \u03a8(a,b) are the results of the intermediate steps. The coefficient \u03c6 is proportional to the cut-off frequency, and \u03b3 controls the magnitude of the filter. The value of \u03c6 is in the range of 0.1-1. The values of \u03c6 and \u03b3 were varied and tested on a set of 21 training images that had been randomly selected from a database of images, and can be adjusted as desired. Optimally, the values of \u03c6 and \u03b3 are set to values that result in the least amount of loss of image detail, while at the same time suppressing speckle.\n","In step , contrast of the image is enhanced in a two-step process comprising steps  and . First, in step , each pixel \u0112(a,b) of the image is scaled to an original image intensity range of 0-255. The purpose of image enhancement is to adjust the display contrast and increase the edge sharpness of objects in the region of interest. As mentioned earlier, malignant tumors and benign cysts are typically anechoic or markedly hypoechoic, i.e., they constitute the darker parts of the image. This is different from digital mammography, where tumors and microcalcifications are typically the brighter regions in the image. Then, in step , each pixel is then histogram equalized to accentuate suspicious areas and enhance the boundary between lesions and the surrounding regions.",{"@attributes":{"id":"p-0045","num":"0044"},"figref":["FIGS. 3","FIG. 3","FIG. 3","FIG. 3","FIG. 3","FIG. 3","FIG. 3","FIG. 3","FIG. 3"],"i":["a","c ","a ","b ","a ","b","c ","a ","c","c "],"b":"3"},{"@attributes":{"id":"p-0046","num":"0045"},"figref":["FIG. 4","FIG. 1","FIG. 4"],"b":"40"},"The probability distribution functions are stored in a database , and are based upon empirical data acquired from a set of 24 images chosen for purposes of training and testing. The lesions were manually cropped out from the training images and used for generating the probability distribution functions for intensity and texture. Sample probability distribution functions for intensity and texture computed from the training images are shown in and , respectively.","In step  of , the texture of the pixel is calculated as the difference of the intensity of each pixel with the mean of its N nearest neighbors. The local variance texture feature captures the lesion's internal echo pattern, and can be defined as follows:",{"@attributes":{"id":"p-0049","num":"0048"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["T","v"]},"mo":"=","mrow":{"mrow":[{"mi":"G","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"v"}},{"mfrac":{"mn":"1","mi":"N"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"\u03b4","mo":"=","mn":"0"},{"mi":"N","mo":"-","mn":"1"}]},"mo":"\u2062","mrow":{"msub":{"mi":["W","\u03b4"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"v"}}}}],"mo":"-"}}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}}}},"br":{},"sub":["v ","\u03b4"]},"In step , the local variance and pixel intensity values are used as an index to retrieve the corresponding probability values from the database  of pre-computed texture and intensity probability density functions. The joint probability is then calculated according to the following equation and assigned to the pixel in step :\n\n\u0393()=\u0393()\u0393() \u2003\u2003(5)\n\nwhere \u0393(i),\u0393(t) are the intensity and texture probabilities for each pixel and \u0393(i,t) is the joint probability that the pixel belongs to a tumor.\n",{"@attributes":{"id":"p-0051","num":"0050"},"figref":["FIGS. 6","FIG. 6","FIGS. 6","FIG. 6"],"i":["a","d ","a ","b ","c","d"],"b":["6","6"]},{"@attributes":{"id":"p-0052","num":"0051"},"figref":["FIG. 7","FIG. 1"],"b":["50","50"]},"Beginning in step , a set Z of points of interest is identified. Assume that a random set of points Z={C, C, C, . . . , C} are in the image, which set contains the seed point Clying within a region of interest. To extract C:",{"@attributes":{"id":"p-0054","num":"0053"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mo":["\u2200","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":["C","\u03f0"]},"mo":"\u2208","mi":"Z"}},{"msub":{"mi":"\u03c4","msub":{"mi":["C","\u03f0"]}},"mo":"=","mfrac":{"mrow":{"mrow":{"msub":{"mi":"\u0393","msub":{"mi":["C","\u03f0"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["i","t"],"mo":","}}},"mo":["\u2062","\u2062"],"msub":[{"mi":"J","msub":{"mi":["C","\u03f0"]}},{"mi":"Y","msub":{"mi":["C","\u03f0"]}}]},"msub":{"mi":"d","msub":{"mi":["C","\u03f0"]}}}}],"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}}}},{"mrow":{"mo":["(",")"],"mn":"6"}}]}}}},"br":{},"sub":["C",{"sub2":"\u03c7"},"\u03c7","C",{"sub2":"\u03c7"},"\u03c7","C",{"sub2":"\u03c7"},"\u03c7","C",{"sub2":"\u03c7"}]},"In step , a potential seed point C is retrieved from set Z. Then, in step , the joint probability \u03c4 is computed for C. In step , the joint probabilities of points within a circular region around C are calculated, and in step , the mean of these probabilities is calculated. A determination is made in step  as to whether additional potential seed points exist within set Z. If a positive determination is made, step  is re-invoked. Otherwise, in step , the true seed point is selected as that C for which \u03c4 is maximum.",{"@attributes":{"id":"p-0056","num":"0055"},"figref":["FIG. 8","FIG. 1","FIGS. 4-6"],"b":["60","61","62","63","64","65","66","67","64"],"i":["d","if\u03b2","J","H","J","N","G","N","H"],"sub":["k","0","k ","k ","k ","k","k ","1","C",{"sub2":"0"},"k","2","C",{"sub2":"0 "},"k","k","k","k","1","2 ","k ","C",{"sub2":"0 "},"0"],"br":[{},{}],"in-line-formulae":[{},{}]},"The values of \u03b2,\u03b2and N were calculated based upon empirically data. A range of values for \u03b2,\u03b2and N were used to test the output of the system for the 21 training images. Those values that gave a segmented region closest to the manually delineated lesion for a majority of the training images were selected. The same values were then used for all subsequent testing.",{"@attributes":{"id":"p-0058","num":"0057"},"figref":["FIGS. 9","FIG. 9","FIG. 9","FIG. 9"],"i":["a","c ","a ","b ","c "],"b":"9","sub":["R","R "]},{"@attributes":{"id":"p-0059","num":"0058"},"figref":["FIG. 10","FIG. 1"],"b":["70","72","72","74","78"],"sub":["0","0","0","g "]},{"@attributes":{"id":"p-0060","num":"0059"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["I","g"]},"mo":"=","mrow":{"mrow":[{"mfrac":{"mrow":[{"mo":"-","mover":{"mi":"F","mo":"^"}},{"mo":["\uf605","\uf606"],"mover":{"mi":"F","mo":"^"}}]},"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"where","mover":{"mi":"F","mo":"^"}},{"mrow":[{"mfrac":{"mrow":[{"mo":"\u2202","mi":"I"},{"mo":"\u2202","mi":"x"}]},"mo":"\u2062","msub":{"mover":{"mi":"n","mo":"~"},"mi":"x"}},{"mfrac":{"mrow":[{"mo":"\u2202","mi":"I"},{"mo":"\u2202","mi":"y"}]},"mo":"\u2062","msub":{"mover":{"mi":"n","mo":"~"},"mi":"y"}}],"mo":"+"}],"mo":"="}}},{"mrow":{"mo":["(",")"],"mn":"8"}}]}}}},"br":{}},{"@attributes":{"id":"p-0061","num":"0060"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mfrac":[{"mrow":[{"mo":"\u2202","mi":"I"},{"mo":"\u2202","mi":"x"}]},{"mrow":[{"mo":"\u2202","mi":"I"},{"mo":"\u2202","mi":"y"}]}],"mo":","}}},"br":{},"sub":["x","y "]},{"@attributes":{"id":"p-0062","num":"0061"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"msub":{"mover":{"mi":"n","mo":"~"},"mi":"x"},"mo":"=","mfrac":{"mover":{"mi":"x","mo":"^"},"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mover":[{"mi":"x","mo":"^"},{"mi":"y","mo":"^"}],"mo":"+"}}}}},{"mrow":{"mo":["(",")"],"mn":"9"}}]},{"mtd":[{"mrow":{"msub":{"mover":{"mi":"n","mo":"~"},"mi":"y"},"mo":"=","mfrac":{"mover":{"mi":"y","mo":"^"},"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mover":[{"mi":"x","mo":"^"},{"mi":"y","mo":"^"}],"mo":"+"}}}}},{"mrow":{"mo":["(",")"],"mn":"10"}}]}]}}},"br":{},"sub":["x\u2212x",{"sub2":"0 "},"y\u2212y",{"sub2":"0"},"x","y "],"b":["79","72"]},"Thus, for every pixel in the contrast-enhanced image, the gradient magnitude of the pixel is calculated, as well as its direction. By incorporating information about the gradient direction and starting with a seed point within the tumor, it is possible to capture the lesion boundary better than by using only the gradient magnitude of the image.",{"@attributes":{"id":"p-0064","num":"0063"},"figref":["FIGS. 11","FIG. 11","FIG. 11","FIG. 11","FIG. 11","FIG. 11"],"i":["a","c ","a ","b ","b","c ","b. "],"b":"11"},{"@attributes":{"id":"p-0065","num":"0064"},"figref":["FIG. 12","FIG. 1","FIG. 1","FIG. 12"],"b":["80","80","60","70","81","86"],"sub":["R","g"]},"In boundary detection procedure , Iis retrieved in step . Then, in steps  and , Iis scanned horizontally and vertically to determine edge points. On each scan, two edge points are detected. This is achieved by finding the positions of all non-zero pixels on each scan line and taking the first and last non-zero pixels as the edge points for that scan line. Boundary points detected from the horizontal and vertical scans are then combined in step , and stored in set S. Since it is not essential to obtain every single boundary point, this approach is used for its speed.","Boundary detection procedure  is then invoked. In step , the directional gradient image Iis retrieved. In step , radial lines Lare drawn outward from the seed point. Then, in step , boundary points Bare found and plotted as positions of maximum intensity on these lines. Each radial line contributes one boundary point. The boundary points are then stored in step  in set S. The procedure can be described as follows:",{"@attributes":{"id":"p-0068","num":"0067"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mo":"\u2200","mrow":{"msub":{"mi":["L","q"]},"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":["we","find"]}},{"msub":{"mi":["B","q"]},"mo":"=","mrow":{"munder":{"mi":["max","\u03d5"]},"mo":"\u2062","mrow":{"mo":["{","}"],"msub":{"mi":"D","mrow":{"mi":["q","\u03d5"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}}}}],"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}}}},{"mrow":{"mo":["(",")"],"mn":"11"}}]}}}},"br":{},"sub":["q\u03c6","q","R ","g"],"figref":["FIGS. 13","FIG. 13","FIG. 13"],"i":["a","b","a ","b "]},"Turning back to , in step , outliers and local maxima are removed from the boundary points detected by procedures  and . Due to inhomogeneities in the tumor region, some points within the region are included as boundary points in I. These points correspond to local maxima and must be eliminated. Similarly shadowing produces some outlier points. Denoting S,Sas the set of boundary points determined from I,I, we find the distance of every point in Sfrom every point in S:",{"@attributes":{"id":"p-0070","num":"0069"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mo":"\u2200","mrow":{"mi":"l","mo":["\u2208","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"msub":{"mi":["S","g"]},"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":["we","find"]}}},{"munder":{"mi":["min","w"]},"mo":"\u2062","mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"l","mo":"-","msub":{"mover":{"mi":"j","mo":"^"},"mi":"w"}}}}],"mo":["\u2062","\u2062","="],"mstyle":{"mtext":{}},"msub":{"mi":["d","l"]}}},{"mrow":{"mo":["(",")"],"mn":"12"}}]}}}},"br":[{},{},{}],"sub":["R","v","l ","v ","1 ","l","g ","R ","R","R "],"in-line-formulae":[{},{}],"i":"d"},{"@attributes":{"id":"p-0071","num":"0070"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"\u03be","mo":"=","mrow":{"mfrac":{"mn":"1","mi":"\u03b7"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"u","mo":"=","mn":"0"},"mi":"\u03b7"},"mo":"\u2062","mrow":{"mo":["\uf605","\uf606"],"mrow":{"msub":[{"mi":["l","u"]},{"mi":["C","o"]}],"mo":"-"}}}}}},{"mrow":{"mo":["(",")"],"mn":"14"}}]}}}},"br":[{},{},{}],"sub":["u","g ","0 ","near ","far","n+1","n","n ","near ","far ","near ","far "],"in-line-formulae":[{},{}],"i":"|<\u2202"},{"@attributes":{"id":"p-0072","num":"0071"},"figref":["FIGS. 14","FIG. 14","FIG. 14","FIG. 14","FIG. 14","FIG. 14","FIG. 14","FIG. 14","FIG. 15"],"i":["a","d ","a ","b ","c","c ","b","b ","d"],"b":"14"},"After the boundary points have been determined, they are passed as inputs to a deformable model in step  of , which indicates in step  the presence or absence of a tumor. Deformable models such as snakes, active contours, and deformable superquadrics have become popular tools in image segmentation. Of late, they have become important in the medical imaging field. In these boundary-based methods, a shape model is initialized close to the object boundary. Image features are then used to fit the model to the boundary. To avoid local minima, most boundary-based methods require that the model be initialized near the solution and be controlled via an interactive user interface. Active contour models have in the past been applied to medical ultrasound images. However, in all of these techniques, an initial guess of the boundary or surface must be provided manually and placed close to the true contour. In the present invention, the boundary points are automatically detected for the initial contour, thereby avoiding manual intervention.","Preferably, the deformable model used in block  of  comprises a modified version of the physics-based deformable model disclosed in Metaxas, et al., \u201cImage Segmentation Based on the Integration of Markov Random Fields and Deformable Models,\u201d The Third International Conference on Medical Image Computing and Computer-Assisted Intervention, 2000, pp. 256-265, the entire disclosure of which is expressly incorporated herein by reference. However, other models could be used. In the Metaxas, et al. deformable model, the reference shape is the set of nodes corresponding to the boundary points computed by the present invention. Given this reference shape r, and the displacement S (local deformation), the position of points p on the model is described by:\n\n\u2003\u2003(16)\n\nTo keep the continuity of the model surface, a continuous loaded membrane deformation strain energy is imposed on the model. The model nodes move under the influence of external forces. The model dynamics are described by the first order Lagrangian method.\n\n\u2003\u2003(17)\n\nwhere K refers to the stiffness matrix of the model based on the strain energy, Q is a vector in which the parameters used to describe r and s are concatenated, and \u0192 refers to the external forces. The deformable model is initialized to the boundary points determined by the present invention, and moves under the influence of \u0192. The external forces used are the balloon forces proposed by Cohen, et al. in \u201cA Finite Element Method Applied to New Active Contour Models and 3D Reconstruction From Cross Sections,\u201d International Conference on Computer Vision, 1990, pp.587-591, the disclosure of which is expressly incorporated herein by reference. The snake is attracted to contours with large image gradients, i.e., strong edges. However, when an image has a complex background, such as in sonographic images, the snake gets confused. Hence, finding the true object boundary from the gradient magnitude alone is not easy. Instead of using just the gradient magnitude, the balloon forces \u0192 operate on the directional gradient of the contrast-enhanced image. Once the model reaches the estimated boundary, the associated nodes stop deforming. Nodal deformations are computed using finite elements. When most of the nodal points stop moving, the model stops deforming.\n","The present invention was tested on a database of 48 images in QUICKTIME movie format from the Department of Radiology, Hospital at the University of Pennsylvania. These images were converted to individual JPEG images using MOVIECONVERTER on a SILICON GRAPHICS workstation. Out of a database of 48 images, 42 images were chosen for purposes of training and testing. Only those sonograms from the database in which the lesion could be manually identified by a trained radiologist were utilized. In the remaining six images, the expert radiologist was unable to distinguish the lesion area visually, and hence these were discarded. Of the 42 images left in the image database, half were randomly selected for training the intensity and texture classifier. The suspicious masses were identified and then manually delineated by a trained radiologist. The manually identified lesions were then cropped out from these 21 training images. This was done using PHOTOSHOP. The cropped lesions were used for generating the probability distribution functions for intensity and texture. The system was built using IDL 6.0 and tested on a database of 42 images. These 42 images included the 21 images that had been used for training. The images in the database contained cysts and lesions, both benign and malignant. Running time to extract the boundary of a single image was 18 seconds on a 1.8 GHz PENTIUM processor.","For detecting the boundary points in the gradient image, the radial lines were sampled at intervals of \u03c0\/180. For the Butterworth filter, values of, \u03c6=0.3 and \u03b3=1.5 and as described above gave the best results. N=9 was used for the number of neighboring pixels in the texture computation module. \u03b2,\u03b2were set to 1 and 0.5, respectively, for the region growing operation. However, since this operation is only performed to get an approximate ROI containing the tumor region, the exact values of \u03b2,\u03b2may vary. For scaling the result of region growing, \u03b5=1.4 was used. The values for the free parameters N,\u03b2,\u03b2were decided upon empirically after varying their values and testing them on the training database of 21 images.","To validate the results, a radiologist manually delineated lesions in the images in the database. In 6 cases, the radiologist was unable to establish the ground truth, due to poor image quality. Hence, only 42 images were retained, and the other 6 discarded. Of the 42 images, the present invention produced results which showed very good agreement with the delineations of the radiologist.",{"@attributes":{"id":"p-0078","num":"0077"},"figref":["FIGS. 16","FIG. 16","FIG. 16","FIG. 16"],"i":["a","d","a","d","a","d ","a","c","d. "],"b":["16","17","17","18","18"]},"In -, the present invention was able to detect most of the boundary points on a much larger tumor. Using the boundary points shown in as the initial nodes, the deformable model accurately captured the margin of the lesion in . The algorithm successfully avoided including the vertical dark lines on either side of the tumor, corresponding to posterior acoustic shadowing.",{"@attributes":{"id":"p-0080","num":"0079"},"figref":"FIGS. 18","i":["a","d "],"b":"18"},"To evaluate the accuracy of the present invention, the automated results were quantitatively compared with the manual delineations produced by one expert radiologist. Two boundary-based error metrics were used to compare the automated and manual delineations of the lesion contour, including area and boundary metrics.","The two boundary error metrics used were the Hausdorff distance and the mean absolute distance. We denote the manually delineated boundary as M={m,m, . . . ,m} and the computer-aided segmentation result as P={\u03c1,\u03c1, . . . ,\u03c1}, where each element of P or M is a point on the corresponding contour. We find the distance of every point in P from all points in M. We define the distance to the closest point for \u03c1to the contour M as:",{"@attributes":{"id":"p-0083","num":"0082"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mo":"\u2200","mrow":{"msub":{"mi":["\u2118","j"]},"mo":"\u2208","mrow":{"mi":["P","we","find"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]}}},{"mi":"d","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["\u2118","j"]},"mo":",","mi":"M"}}},{"munder":{"mi":["min","\u03c9"]},"mo":"\u2062","mrow":{"mo":["\uf605","\uf606"],"mrow":{"msub":[{"mi":["\u2118","j"]},{"mi":["m","\u03c9"]}],"mo":"-"}}}],"mo":["\u2062","\u2062","="],"mstyle":{"mtext":{}}}},{"mrow":{"mo":["(",")"],"mn":"18"}}]}}}},"br":{},"sub":["j","j"]},"In Table 1 are listed the average Haussdorf (HE) and mean distance errors (ME) between the positions of the boundary pixels detected by the present invention and the manual delineations of a trained radiologist for \u03b1=0.6 and \u03b1=1.45 The average Haussdorf error for the database of 42 images was found to be 19.727 pixels and the average mean error was only 6.6 pixels. The corresponding normalized errors were computed by dividing HE and ME by the number of boundary pixels as determined in the manual segmentation. These were found to be 6.6% and 2.26% respectively. The tabulated results clearly indicate good correlation between the automated and manual segmentation results.",{"@attributes":{"id":"p-0085","num":"0084"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 1"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Boundary errors for \u03b1= 0.6 & \u03b1= 1.4."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"63pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},"Avg.","Norm.","Norm."]},{"entry":["Cases","HE","ME","HE","ME"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}},{"entry":["42","19.727","6.687","6.603","2.265"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}}]}}},"To measure area metrics, three metrics were established to find the difference in estimates between the manual and automatically delineated areas. We defined the False Positive (FP), False Negative (FN) and True Positive (TP) areas as:",{"@attributes":{"id":"p-0087","num":"0086"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mi":"FP","mo":"=","mfrac":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"msub":{"mi":["A","a"]},"mo":"\u22c3","mrow":{"msub":[{"mi":["A","m"]},{"mi":["A","m"]}],"mo":"-"}}},"msub":{"mi":["A","m"]}}}},{"mrow":{"mo":["(",")"],"mn":"19"}}]},{"mtd":[{"mrow":{"mi":"FN","mo":"=","mfrac":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"msub":{"mi":["A","a"]},"mo":"\u22c3","mrow":{"msub":[{"mi":["A","m"]},{"mi":["A","m"]}],"mo":"-"}}},"msub":{"mi":["A","m"]}}}},{"mrow":{"mo":["(",")"],"mn":"20"}}]},{"mtd":[{"mrow":{"mi":"TP","mo":"=","mfrac":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"msub":[{"mi":["A","a"]},{"mi":["A","m"]}],"mo":"\u22c2"}},"msub":{"mi":["A","m"]}}}},{"mrow":{"mo":["(",")"],"mn":"21"}}]}]}}},"br":{},"sub":["m ","a "],"figref":"FIG. 19"},"Table 2 lists the mean of the error in area for all the images in the dataset. The average percentage of normal pixels which were classified as tumor was 20.86% while the percentage of tumor pixels that were not detected by the present invention was 24.9%. The average true positive percentage was 75.1%. All but two of the images in the database had a true positive percentage over 50%. The mean true positive percentage of the best 40 images in the database was 78%. The two images that were ignored while computing this statistic belonged to the training dataset.",{"@attributes":{"id":"p-0089","num":"0088"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 2"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Errors in segmented area for \u03b1= 0.6 & \u03b1= 1.45."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"70pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"70pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Cases","FP %","FN %","TP %"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]},{"entry":[{},"42","20.856","24.959","75.041"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]}]}}]}}},"A segmentation algorithm can only be called robust if its performance is almost insensitive to variation of its parameters. Sensitivity analysis is an important component in the evaluation of the overall performance of a segmentation system. Three different factors for evaluating segmentation methods include precision (reproducibility), accuracy, and efficiency.","As mentioned earlier, outliers are eliminated by the present invention from the set of boundary points by using distance thresholds \u03b1and \u03b1. Hence, by changing the values of these two parameters, points would be closer or farther from the seed point. Consequently, the set of points supplied to the deformable model would change and result in a different segmentation. To determine how different the final segmentation result would be, \u03b1and \u03b1were varied and computed the resulting Haussdorf (HE) and mean distance errors (ME).","The intensity and texture classifier module of the present invention was trained on a set of 21 images. These were selected from the original database of 42 breast sonograms. The training module assigned individual image pixels probabilities of belonging to a tumor based on their texture and intensity values. The 21 training images were chosen randomly from the database in order to not bias the system performance. To investigate whether the 21 images constituted a representative database for classification, the training set was expanded by incorporating 11 more images. The segmentation results obtained using this larger training set were compared with those obtained using the original training set of 21 images. This was done to evaluate system sensitivity to the number of training samples.","Correctly determining the seed point within the lesion area is a prerequisite to getting an accurate segmentation. However, since the present invention selects the seed from a random set of points in the image, the seed point position changes every time the program is run. To determine how the precise location of the seed point would affect the final segmentation, its position was varied within the lesion area and computed the resulting Hausdorff distance (HE) and the average mean distance errors (HE).","A database of 48 breast sonograms was provided to determine the effects of increased training. In 6 of these images, the radiologist was unable to identify the lesion site. Of the remaining 42 images left in the database, 21 were used for training. The system performance using these 21 training images are listed in Tables 3and 4. To investigate whether segmentation performance would be enhanced by increasing the number of training samples, the present invention was also trained on 32 training images, i.e., \u00beof the database.",{"@attributes":{"id":"p-0095","num":"0094"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 3"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Boundary errors using 32 training samples with"},{"entry":"\u03b1= 0.6 & \u03b1= 1.4."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"63pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},"Avg.","Norm.","Norm."]},{"entry":["Cases","HE","ME","HE","ME"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}},{"entry":["42","18.435","5.600","6.18","1.95"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}}]}}},{"@attributes":{"id":"p-0096","num":"0095"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 4"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Errors in segmented area using 32 training samples with"},{"entry":"\u03b1= 0.6 & \u03b1= 1.45."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"70pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"70pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Cases","FP %","FN %","TP %"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]},{"entry":[{},"42","21.376","23.937","76.067"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]}]}}]}}},"Comparing the results in Tables 3 and 4 with those in Tables 1 and 2, it can be noticed that the average area and boundary errors for the entire dataset of 42 images had decreased slightly by increasing the number of training samples to 32. For the same values of \u03b1and \u03b1the average Haussdorf and mean boundary error over the entire database was reduced by 1.4 and 1.08 pixels respectively. The average true positive area percentage increased by 1.02% by expanding the training set. To determine whether these changes were statistically significant over the entire database, the boundary and area errors were compared using the two different training sets via a paired t-test. The t-test was performed under the null hypothesis that there is a decrease in TP %, FP % and the Haussdorf and mean boundary errors using the training set containing 32 images. The hypothesis was rejected at a significance level of P<0.05 indicating that the decrease was not statistically significant. The p values from the t-test are listed in Table 5. The p values for both the area and error metrics were greater than 0.05 indicating that the slight increase in performance using the larger training set was not statistically significant.","The system was also tested using a smaller training database (i.e. 10 images). However, the system performance was significantly worse when using \u00bcof the images in the database for training.",{"@attributes":{"id":"p-0099","num":"0098"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 5"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"p values for paired student t-test on area and boundary"},{"entry":"errors for the two training sets."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"70pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"70pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"TP %","FP %","HE","ME"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]},{"entry":[{},"0.2015","0.2111","0.098","0.1037"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]}]}}]}}},"The images in the database that had not been included in the training dataset were also separately analyzed. This was done to see whether these images performed worse than the training images. For the first training set containing 21 training images, the area and boundary errors for the remaining 21 of the 42 sonograms in the database were computed. Hence, both the test and training datasets contained 21 images. For the second training set containing 32 images, the corresponding test dataset contained 10 images (42\u221232=10). The average boundary and area errors for the results generated using the training set containing 21 and 32 sonograms respectively are listed in Tables 6 and 7, respectively. For the first training set of 21 images, the average performance of the 21 test images was better than that of the training sonograms with respect to the boundary error metrics. In terms of the area error metric, the 21 test images had a lower false positive % area than the training images. But, the training images performed marginally better than the test images in terms of true positive % area. For the second dataset containing 32 training images, the results were similar to those obtained with the first training set. The average Haussdorf and mean boundary errors were lower for the 10 test images compared to the 32 training images. The average false positive % and true positive % areas for the training images however were better than that for the 10 test images.",{"@attributes":{"id":"p-0101","num":"0100"},"tables":{"@attributes":{"id":"TABLE-US-00006","num":"00006"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 6"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Boundary errors for test images using 21 & 32 training samples"},{"entry":"(\u03b1= 0.6 & \u03b1= 1.4)."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"63pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},"Avg.","Norm.","Norm."]},{"entry":["Cases","HE","ME","HE","ME"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"5","colwidth":"63pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["21","17.4568","5.579","6.58","2.118"]},{"entry":["10","13.436","4.294","5.280","1.734"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}}]}}},{"@attributes":{"id":"p-0102","num":"0101"},"tables":{"@attributes":{"id":"TABLE-US-00007","num":"00007"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 7"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Errors in segmented area for test images using 21 & 32 training"},{"entry":"samples (\u03b1= 0.6 & \u03b1= 1.45)."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"70pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"70pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Cases","FP %","FN %","TP %"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]},{"entry":[{},"21","13.380","25.293","74.707"]},{"entry":[{},"10","25.168","25.891","74.108"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]}]}}]}}},"Based on the above results, the following conclusions can be made: (i) the training database of 21 images contained enough representative images in order to be able to accurately segment the lesions for the corresponding test images; (ii) increasing the number of training samples to 32 did not significantly improve the performance of the system; and (iii) the test images (entire dataset\u2014training dataset) performed as well as the images in the training dataset. This is true for both training datasets.","The seed point position in the horizontal and vertical directions within the lesion were also varied, and the corresponding boundary error metrics were computed. The results are tabulated in Tables 8 and 9. In Table 8 lists the errors in boundary position by varying the seed point by a horizontal distance (\u0394x), and the corresponding results by varying the seed point by a vertical displacement (\u0394y) are listed in Table 9.",{"@attributes":{"id":"p-0105","num":"0104"},"tables":{"@attributes":{"id":"TABLE-US-00008","num":"00008"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 8"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Boundary errors for variation in seed point position (horizontal"},{"entry":"direction) for \u03b1= 0.6 & \u03b1= 1.4."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"56pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Avg.","Avg.","Norm.","Norm."]},{"entry":["\u0394x","HE","ME","HE","ME"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"56pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["+2","19.99","6.951","6.684","2.326"]},{"entry":["+1","19.727","6.687","6.603","2.265"]},{"entry":["\u22122","20.266","6.966","6.674","2.307"]},{"entry":["\u22121","19.907","7.071","6.653","2.369"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}}]}}},{"@attributes":{"id":"p-0106","num":"0105"},"tables":{"@attributes":{"id":"TABLE-US-00009","num":"00009"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 9"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Boundary errors for variation in seed point position (vertical"},{"entry":"direction) for \u03b1= 0.6 & \u03b1= 1.4."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"56pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Avg.","Avg.","Norm.","Norm."]},{"entry":["\u0394y","HE","ME","HE","ME"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}},{"entry":["+2","21.056","7.399","6.903","2.435"]},{"entry":["+1","20.615","7.295","6.806","2.418"]},{"entry":["\u22122","21.056","7.399","6.904","2.435"]},{"entry":["\u22121","20.615","7.295","6.806","2.418"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}}]}}},"The standard deviation in error for the Haussdorf and mean average distance by varying \u0394x was 0.2242 and 0.1636 pixels respectively, as shown in Table 10. Similarly, the standard deviation in error for the two distance metrics obtained by varying \u0394y was 0.5338 and 0.2342 pixels respectively. The small standard deviations showed that the system was not dependent on the precise location of the seed point within the lesion.",{"@attributes":{"id":"p-0108","num":"0107"},"tables":{"@attributes":{"id":"TABLE-US-00010","num":"00010"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 10"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Standard Deviation in errors with variation of \u0394x & \u0394y."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"91pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"98pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Parameter",{},{}]},{"entry":["Variation","ME","HE"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":["\u0394x","0.1636","0.2242"]},{"entry":["\u0394y","0.2342","0.5338"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}]}}},"Having thus described the invention in detail, it is to be understood that the foregoing description is not intended to limit the spirit and scope thereof. What is desired to be protected by Letters Patent is set forth in the appended claims."],"GOVINT":[{},{}],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["These and other important objects and features of the invention will be apparent from the following Detailed Description of the Invention, taken in connection with the accompanying drawings, in which:",{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":["FIG. 2","FIG. 1"],"b":"30"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIGS. 3","i":["a","c "],"b":"3"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":["FIG. 4","FIG. 1"],"b":"40"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIGS. 5","i":["a","b "],"b":"5"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIGS. 6","i":["a","d "],"b":"6"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":["FIG. 7","FIG. 1"],"b":"50"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":["FIG. 8","FIG. 1"],"b":"60"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIGS. 9","i":["a","c "],"b":"9"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":["FIG. 10","FIG. 1"],"b":"70"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIGS. 11","i":["a","c "],"b":"11"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 12","b":"80"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIGS. 13","i":["a","b "],"b":"13"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIGS. 14","i":["a","d "],"b":"14"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIGS. 16","i":["a","d "],"b":"16"},{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIGS. 17","i":["a","d "],"b":"17"},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIGS. 18","i":["a","d "],"b":"18"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 19"}]},"DETDESC":[{},{}]}
