---
title: Tile-based optical flow
abstract: A computing device may determine a per-tile motion estimate between a first m×n pixel tile from a first captured image of a scene and a second m×n pixel tile from a second captured image of the scene. A per-tile confidence of the per-tile motion estimate may be obtained. The per-tile motion estimate and the per-tile confidence may be upsampled to obtain respective per-pixel motion estimates and associated per-pixel confidences for pixels of the first m×n pixel tile. The respective per-pixel motion estimates and associated per-pixel confidences may be iteratively filtered. The iterative filtering may involve multiplying the respective per-pixel motion estimates and associated per-pixel confidences by an affinity matrix. The iterative filtering may also smooth the respective per-pixel motion estimates and associated per-pixel confidences.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09342873&OS=09342873&RS=09342873
owner: Google Inc.
number: 09342873
owner_city: Mountain View
owner_country: US
publication_date: 20150523
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION","1. EXAMPLE IMAGE CAPTURE DEVICES","2. EXAMPLE IMAGE ALIGNMENT","3. TILE-BASED ANALYSIS","4. EXAMPLE APPLICATIONS","5. EXAMPLE OPERATIONS","6. IMPROVED OPTICAL FLOW","A. Noisy Per-Pixel Flow Estimates","B. Bilateral-Space Inference","C. Temporal Smoothing","D. Example Operations","7. CONCLUSION"],"p":["Digital imaging may refer to capturing and representing the color and brightness characteristics of scenes in digital images (e.g., photographs or motion video). When two or more digital images of a particular scene are captured, some of these digital images may be further enhanced and\/or combined to create new digital images or image effects. However, before this processing takes place, it is often beneficial to align groups of digital images. In this way, the relative locations of similar features in each digital image can be taken into account.","Digital imaging devices, such as wireless computing devices, digital cameras, head-mounted displays, and so on, may capture arrays of digital images of a scene. These digital images may be captured consecutively in time, perhaps a few milliseconds apart from one another. Alternatively or additionally, the digital images may be captured at approximately the same time, but with more than one image sensor. In the latter cases, for instance, a wireless computing device may include multiple individual image sensors, or multiple digital cameras may be arranged to capture digital images in a coordinated fashion.","Thus, a series of two or more digital images of a particular scene may represent temporally or spatially distinct versions of the scene. The information in these images may be used to enhance one another, or to synthesize new digital images of the scene. For instance, information in two of the digital images may merged to create an enhanced version of the scene that is sharper, or exhibits less noise, than any of the digital images in their original form. In another example, a third digital image may be interpolated from two of the captured digital images. This interpolated image may be a synthetic digital image that represents the scene at a point in time between when the two digital images were captured, or a view of the scene from a virtual camera.","Regardless of the application, synthesizing new digital images based on two or more captured digital images may involve aligning parts of the two or more digital images to one another. It is desirable for this alignment procedure to be computationally efficient so that it can operate in real-time, or near-real-time, on various types of image capture devices.","Accordingly, a first example embodiment may involve determining a per-tile motion estimate between a first m\u00d7n pixel tile from a first captured image of a scene and a second m\u00d7n pixel tile from a second captured image of the scene. The per-tile motion estimate may represent movement of the first m\u00d7n pixel tile to approximate the second m\u00d7n pixel tile. The first example embodiment may also involve, possibly based on the approximation of the first m\u00d7n pixel tile to the second m\u00d7n pixel tile, estimating a per-tile confidence of the per-tile motion estimate. The first example embodiment may further involve upsampling the per-tile motion estimate and the per-tile confidence to obtain respective per-pixel motion estimates and associated per-pixel confidences for pixels of the first m\u00d7n pixel tile. The first example embodiment may further involve iteratively filtering the respective per-pixel motion estimates and associated per-pixel confidences. The iterative filtering may involve multiplying the respective per-pixel motion estimates and associated per-pixel confidences by an affinity matrix. The iterative filtering may smooth the respective per-pixel motion estimates and associated per-pixel confidences.","In a second example embodiment, an article of manufacture may include a non-transitory computer-readable medium, having stored thereon program instructions that, upon execution by a computing device, cause the computing device to perform operations in accordance with the first example embodiment.","In a third example embodiment, a computing device may include at least one processor, as well as data storage and program instructions. The program instructions may be stored in the data storage, and upon execution by the at least one processor may cause the computing device to perform operations in accordance with the first example embodiment.","In a fourth example embodiment, a system may include various means for carrying out each of the operations of any of the first, second, and\/or third example embodiments.","These as well as other embodiments, aspects, advantages, and alternatives will become apparent to those of ordinary skill in the art by reading the following detailed description, with reference where appropriate to the accompanying drawings. Further, it should be understood that this summary and other descriptions and figures provided herein are intended to illustrate embodiments by way of example only and, as such, that numerous variations are possible. For instance, structural elements and process steps can be rearranged, combined, distributed, eliminated, or otherwise changed, while remaining within the scope of the embodiments as claimed.","Example methods, devices, and systems are described herein. It should be understood that the words \u201cexample\u201d and \u201cexemplary\u201d are used herein to mean \u201cserving as an example, instance, or illustration.\u201d Any embodiment or feature described herein as being an \u201cexample\u201d or \u201cexemplary\u201d is not necessarily to be construed as preferred or advantageous over other embodiments or features. Other embodiments can be utilized, and other changes can be made, without departing from the scope of the subject matter presented herein.","Thus, the example embodiments described herein are not meant to be limiting. Aspects of the present disclosure, as generally described herein, and illustrated in the figures, can be arranged, substituted, combined, separated, and designed in a wide variety of different configurations, all of which are contemplated herein.","Further, unless context suggests otherwise, the features illustrated in each of the figures may be used in combination with one another. Thus, the figures should be generally viewed as component aspects of one or more overall embodiments, with the understanding that not all illustrated features are necessary for each embodiment.","As image capture devices, such as cameras, become more popular, they may be employed as standalone hardware devices or integrated into various other types of devices. For instance, still and video cameras are now regularly included in wireless computing devices (e.g., mobile phones), tablet computers, laptop computers, video game interfaces, home automation devices, and even automobiles and other types of vehicles.","The physical components of a camera may include one or more apertures through which light enters, one or more recording surfaces for capturing the images represented by the light, and lenses positioned in front of each aperture to focus at least part of the image on the recording surface(s). The apertures may be fixed size or adjustable. In an analog camera, the recording surface may be photographic film. In a digital camera, the recording surface may include an electronic image sensor (e.g., a charge coupled device (CCD) or a complementary metal-oxide-semiconductor (CMOS) sensor) to transfer and\/or store captured images in a data storage unit (e.g., memory).","One or more shutters may be coupled to or nearby the lenses or the recording surfaces. Each shutter may either be in a closed position, in which it blocks light from reaching the recording surface, or an open position, in which light is allowed to reach to recording surface. The position of each shutter may be controlled by a shutter button. For instance, a shutter may be in the closed position by default. When the shutter button is triggered (e.g., pressed), the shutter may change from the closed position to the open position for a period of time, known as the shutter cycle. During the shutter cycle, an image may be captured on the recording surface. At the end of the shutter cycle, the shutter may change back to the closed position.","Alternatively, the shuttering process may be electronic. For example, before an electronic shutter of a CCD image sensor is \u201copened,\u201d the sensor may be reset to remove any residual signal in its photodiodes. While the electronic shutter remains open, the photodiodes may accumulate charge. When or after the shutter closes, these charges may be transferred to longer-term data storage. Combinations of mechanical and electronic shuttering may also be possible.","Regardless of type, a shutter may be activated and\/or controlled by something other than a shutter button. For instance, the shutter may be activated by a softkey, a timer, or some other trigger. Herein, the term \u201cimage capture\u201d may refer to any mechanical and\/or electronic shuttering process that results in one or more images being recorded, regardless of how the shuttering process is triggered or controlled.","The exposure of a captured image may be determined by a combination of the size of the aperture, the brightness of the light entering the aperture, and the length of the shutter cycle (also referred to as the shutter length or the exposure length). Additionally, a digital and\/or analog gain may be applied to the image, thereby influencing the exposure. In some embodiments, the term \u201ctotal exposure length\u201d or \u201ctotal exposure time\u201d may refer to the shutter length multiplied by the gain for a particular aperture size. Herein, the term \u201ctotal exposure time,\u201d or \u201cTET,\u201d should be interpreted as possibly being a shutter length, an exposure time, or any other metric that controls the amount of signal response that results from light reaching the recording surface.","A still camera may capture one or more images each time image capture is triggered. A video camera may continuously capture images at a particular rate (e.g., 24 images\u2014or frames\u2014per second) as long as image capture remains triggered (e.g., while the shutter button is held down). Some digital still cameras may open the shutter when the camera device or application is activated, and the shutter may remain in this position until the camera device or application is deactivated. While the shutter is open, the camera device or application may capture and display a representation of a scene on a viewfinder. When image capture is triggered, one or more distinct digital images of the current scene may be captured.","Cameras\u2014even analog cameras\u2014may include software to control one or more camera functions and\/or settings, such as aperture size, TET, gain, and so on. Additionally, some cameras may include software that digitally processes images during or after these images are captured. While the description above refers to cameras in general, it may be particularly relevant to digital cameras.","As noted previously, digital cameras may be standalone devices or integrated with other devices. As an example,  illustrates the form factor of a digital camera device . Digital camera device  may be, for example, a mobile phone, a tablet computer, or a wearable computing device. However, other embodiments are possible. Digital camera device  may include various elements, such as a body , a front-facing camera , a multi-element display , a shutter button , and other buttons . Digital camera device  could further include a rear-facing camera . Front-facing camera  may be positioned on a side of body  typically facing a user while in operation, or on the same side as multi-element display . Rear-facing camera  may be positioned on a side of body  opposite front-facing camera . Referring to the cameras as front and rear facing is arbitrary, and digital camera device  may include multiple cameras positioned on various sides of body .","Multi-element display  could represent a cathode ray tube (CRT) display, a light emitting diode (LED) display, a liquid crystal (LCD) display, a plasma display, or any other type of display known in the art. In some embodiments, multi-element display  may display a digital representation of the current image being captured by front-facing camera  and\/or rear-facing camera , or an image that could be captured or was recently captured by either or both of these cameras. Thus, multi-element display  may serve as a viewfinder for either camera. Multi-element display  may also support touchscreen and\/or presence-sensitive functions that may be able to adjust the settings and\/or configuration of any aspect of digital camera device .","Front-facing camera  may include an image sensor and associated optical elements such as lenses. Front-facing camera  may offer zoom capabilities or could have a fixed focal length. In other embodiments, interchangeable lenses could be used with front-facing camera . Front-facing camera  may have a variable mechanical aperture and a mechanical and\/or electronic shutter. Front-facing camera  also could be configured to capture still images, video images, or both. Further, front-facing camera  could represent a monoscopic, stereoscopic, or multiscopic camera. Rear-facing camera  may be similarly or differently arranged. Additionally, front-facing camera , rear-facing camera , or both, may be an array of one or more cameras.","Either or both of front facing camera  and rear-facing camera  may include or be associated with an illumination component that provides a light field to illuminate a target object. For instance, an illumination component could provide flash or constant illumination of the target object. An illumination component could also be configured to provide a light field that includes one or more of structured light, polarized light, and light with specific spectral content. Other types of light fields known and used to recover three-dimensional (3D) models from an object are possible within the context of the embodiments herein.","Either or both of front facing camera  and rear-facing camera  may include or be associated with an ambient light sensor that may continuously or from time to time determine the ambient brightness of a scene that the camera can capture. In some devices, the ambient light sensor can be used to adjust the display brightness of a screen associated with the camera (e.g., a viewfinder). When the determined ambient brightness is high, the brightness level of the screen may be increased to make the screen easier to view. When the determined ambient brightness is low, the brightness level of the screen may be decreased, also to make the screen easier to view as well as to potentially save power. Additionally, the ambient light sensor's input may be used to determine a TET of an associated camera, or to help in this determination.","Digital camera device  could be configured to use multi-element display  and either front-facing camera  or rear-facing camera  to capture images of a target object. The captured images could be a plurality of still images or a video stream. The image capture could be triggered by activating shutter button , pressing a softkey on multi-element display , or by some other mechanism. Depending upon the implementation, the images could be captured automatically at a specific time interval, for example, upon pressing shutter button , upon appropriate lighting conditions of the target object, upon moving digital camera device  a predetermined distance, or according to a predetermined capture schedule.","As noted above, the functions of digital camera device \u2014or another type of digital camera\u2014may be integrated into a computing device, such as a wireless computing device, cell phone, tablet computer, laptop computer and so on. For purposes of example,  is a simplified block diagram showing some of the components of an example computing device  that may include camera components .","By way of example and without limitation, computing device  may be a cellular mobile telephone (e.g., a smartphone), a still camera, a video camera, a fax machine, a computer (such as a desktop, notebook, tablet, or handheld computer), a personal digital assistant (PDA), a home automation component, a digital video recorder (DVR), a digital television, a remote control, a wearable computing device, or some other type of device equipped with at least some image capture and\/or image processing capabilities. It should be understood that computing device  may represent a physical camera device such as a digital camera, a particular physical hardware platform on which a camera application operates in software, or other combinations of hardware and software that are configured to carry out camera functions.","As shown in , computing device  may include a communication interface , a user interface , a processor , data storage , and camera components , all of which may be communicatively linked together by a system bus, network, or other connection mechanism .","Communication interface  may allow computing device  to communicate, using analog or digital modulation, with other devices, access networks, and\/or transport networks. Thus, communication interface  may facilitate circuit-switched and\/or packet-switched communication, such as plain old telephone service (POTS) communication and\/or Internet protocol (IP) or other packetized communication. For instance, communication interface  may include a chipset and antenna arranged for wireless communication with a radio access network or an access point. Also, communication interface  may take the form of or include a wireline interface, such as an Ethernet, Universal Serial Bus (USB), or High-Definition Multimedia Interface (HDMI) port. Communication interface  may also take the form of or include a wireless interface, such as a Wifi, BLUETOOTH\u00ae, global positioning system (GPS), or wide-area wireless interface (e.g., WiMAX or 3GPP Long-Term Evolution (LTE)). However, other forms of physical layer interfaces and other types of standard or proprietary communication protocols may be used over communication interface . Furthermore, communication interface  may comprise multiple physical communication interfaces (e.g., a Wifi interface, a BLUETOOTH\u00ae interface, and a wide-area wireless interface).","User interface  may function to allow computing device  to interact with a human or non-human user, such as to receive input from a user and to provide output to the user. Thus, user interface  may include input components such as a keypad, keyboard, touch-sensitive or presence-sensitive panel, computer mouse, trackball, joystick, microphone, and so on. User interface  may also include one or more output components such as a display screen which, for example, may be combined with a presence-sensitive panel. The display screen may be based on CRT, LCD, and\/or LED technologies, or other technologies now known or later developed. User interface  may also be configured to generate audible output(s), via a speaker, speaker jack, audio output port, audio output device, earphones, and\/or other similar devices.","In some embodiments, user interface  may include a display that serves as a viewfinder for still camera and\/or video camera functions supported by computing device . Additionally, user interface  may include one or more buttons, switches, knobs, and\/or dials that facilitate the configuration and focusing of a camera function and the capturing of images (e.g., capturing a picture). It may be possible that some or all of these buttons, switches, knobs, and\/or dials are implemented by way of a presence-sensitive panel.","Processor  may comprise one or more general purpose processors\u2014e.g., microprocessors\u2014and\/or one or more special purpose processors\u2014e.g., digital signal processors (DSPs), graphics processing units (GPUs), floating point units (FPUs), network processors, or application-specific integrated circuits (ASICs). In some instances, special purpose processors may be capable of image processing, image alignment, and merging images, among other possibilities. Data storage  may include one or more volatile and\/or non-volatile storage components, such as magnetic, optical, flash, or organic storage, and may be integrated in whole or in part with processor . Data storage  may include removable and\/or non-removable components.","Processor  may be capable of executing program instructions  (e.g., compiled or non-compiled program logic and\/or machine code) stored in data storage  to carry out the various functions described herein. Therefore, data storage  may include a non-transitory computer-readable medium, having stored thereon program instructions that, upon execution by computing device , cause computing device  to carry out any of the methods, processes, or operations disclosed in this specification and\/or the accompanying drawings. The execution of program instructions  by processor  may result in processor  using data .","By way of example, program instructions  may include an operating system  (e.g., an operating system kernel, device driver(s), and\/or other modules) and one or more application programs  (e.g., camera functions, address book, email, web browsing, social networking, and\/or gaming applications) installed on computing device . Similarly, data  may include operating system data  and application data . Operating system data  may be accessible primarily to operating system , and application data  may be accessible primarily to one or more of application programs . Application data  may be arranged in a file system that is visible to or hidden from a user of computing device .","Application programs  may communicate with operating system  through one or more application programming interfaces (APIs). These APIs may facilitate, for instance, application programs  reading and\/or writing application data , transmitting or receiving information via communication interface , receiving and\/or displaying information on user interface , and so on.","In some vernaculars, application programs  may be referred to as \u201capps\u201d for short. Additionally, application programs  may be downloadable to computing device  through one or more online application stores or application markets. However, application programs can also be installed on computing device  in other ways, such as via a web browser or through a physical interface (e.g., a USB port) on computing device .","Camera components  may include, but are not limited to, an aperture, shutter, recording surface (e.g., photographic film and\/or an image sensor), lens, and\/or shutter button. Camera components  may be controlled at least in part by software executed by processor .","A variety of image processing operations depend on being able to determine which pixels correspond to one another in two images. To this end, the embodiments herein provide techniques for efficiently matching patches of pixels in one image to locations in another image. A \u201cpatch\u201d of pixels may refer to a group of one or more pixels from the same general location in an image. Na\u00efve approaches to solving this problem are prohibitively expensive in terms of computation. But, the embodiments herein are efficient and fast, capable of processing high-resolution images (e.g., 15 megapixels or more) in less than a second.","Given the determined patch-to-location matches in the two images, a new image filtering technique, in some cases based on Kalman filtering, can produce accurate per-pixel motion and matching estimates. This matching procedure can be used for a variety of purposes, such as motion estimation and for merging multiple images.",{"@attributes":{"id":"p-0055","num":"0054"},"figref":"FIG. 3","b":["300","302","302","300","300","302","300","302","300","302"]},"Regardless, some image processing techniques may involve taking information from both captured images  and , and merging this information into synthetic image . For instance, information from captured images  and  can be combined into synthetic image  to create a sharpened or de-noised image.","Alternatively, to the extent that captured images  and  depict movement, synthetic image  may be an interpolation of an intermediate point of this movement. For instance, if captured images  and  are video frames of a sequence of captured video frames, synthetic image  may approximate an intermediate video frame of this sequence. Thus, if captured images  and  were captured 30 milliseconds apart from one another, synthetic image  may approximate a video frame that, hypothetically, could have been captured 15 milliseconds after captured image  and 15 milliseconds before captured image . By synthesizing one or more of such intermediate video frames, a slow motion video sequence can be created.","In another alternative, captured images  and  may have been captured by two spatially separated image sensors. In this case, synthetic image  may approximate an image captured by a virtual image sensor positioned at an intermediate location between the two \u201creal\u201d image sensors. By doing so, the scene depicted in the images may be viewed from more camera angles than were used for the actual capture of the images.","Regardless of how this sort of image synthesis is used, it is beneficial to first align captured images  and . In its simplest form, this alignment may involve shifting each pixel of one image by a certain number of pixel or sub-pixel offsets in the x and y directions, respectively. However, this simple approach usually results in a poor alignment, because different pixels may actually move by different offsets. A more robust alignment technique is to determine the x and y offsets for each pixel individually. However, for large images, such as 4, 6, 8, or 15 megapixel images, doing so may be computationally prohibitive.","Herein, a new image alignment technique is disclosed. A first image (e.g., captured image ) is divided into non-overlapping square or rectangular tiles. The tiles are mapped to respective offsets that identify locations in a second image (e.g., captured image ). These mappings are approximated with bivariate quadratic surfaces (i.e., two-dimensional quadratic functions), incorporating confidence levels of the mappings. However, it may be possible to use different types of functions in some situations. An information filtering technique (e.g., based on Kalman filters) may be applied to the surfaces, resulting in a per-pixel offset for each pixel in the first image, where the offsets represents the movement of the pixel between the first image and the second image.","As a result, the first image can be warped into alignment with the second image, or the second image can be warped into alignment with the first image. By merging information from the aligned images, sharpened or de-noised versions of these images may be created. Alternatively or additionally, synthetic images representing temporal-intermediate or spatially-intermediate versions of the scene may be created.","As an example of synthesizing an intermediate image from two captured images, consider . Part (a) and part (b) of this figure show respective first and second images. Part (c) shows a synthetic image formed by averaging the pixel values of the first and second images, without per-pixel alignment. This na\u00efve technique produces significant ghosting (objects blending into each other due to image misalignment), as the bird's head and wings are blurred. Part (d) shows another synthetic image formed by using the warping alignment embodiments described herein, as well as edge-aware filtering (described in more detail below). The outcome is an intermediate image with much less ghosting.","Example embodiments of the alignment technique are described in detail in the following sections.","As an example, consider the problem of taking two small sub-images, each from a different captured image of a scene, and computing a \u201cdistance matrix\u201d that measures the mismatch between the two sub-images for various offsets (translations) of the sub-images. Entries in the distance matrix indicate the relatively goodness-of-fit of the offsets of the two sub-images. The offsets that minimizes distance measure used to create the matrix is likely to be a reasonably accurate estimate of the motion that transforms the first sub-image into the second sub-image.","A. Example Determination of a Distance Matrix","In order to formally define the distance matrix, a simplified example may be helpful. The L2 distance between two vectors a and b may be calculated as:",{"@attributes":{"id":"p-0067","num":"0066"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"d","mo":"=","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":["a","b"],"mo":"-"}},"mn":["2","2"]}}},{"mrow":{"mo":["(",")"],"mn":"1"}}]}}}},"br":{},"sub":["2","i","i"],"sup":"2"},"This distance can be rewritten as:",{"@attributes":{"id":"p-0069","num":"0068"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"mi":"d","mo":"=","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":["a","b"],"mo":"-"}},"mn":["2","2"]}}}},{"mtd":{"mrow":{"mo":"=","mrow":{"munder":{"mo":"\u2211","mi":"i"},"mo":"\u2062","msup":{"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["a","i"]},{"mi":["b","i"]}],"mo":"-"}},"mn":"2"}}}}},{"mtd":{"mrow":{"mo":"=","mrow":{"munder":{"mo":"\u2211","mi":"i"},"mo":"\u2062","mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msub":[{"mi":["a","i"]},{"mi":["b","i"]}],"mo":"-"}},{"mo":["(",")"],"mrow":{"msub":[{"mi":["a","i"]},{"mi":["b","i"]}],"mo":"-"}}],"mo":"\u2062"}}}}},{"mtd":{"mrow":{"mo":"=","mrow":{"munder":{"mo":"\u2211","mi":"i"},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["a","i"],"mn":"2"},{"mi":["b","i"],"mn":"2"}],"mo":["+","-"],"mrow":{"mn":"2","mo":["\u2062","\u2062"],"msub":[{"mi":["a","i"]},{"mi":["b","i"]}]}}}}}}},{"mtd":{"mrow":{"mo":"=","mrow":{"msubsup":[{"mrow":{"mo":["\uf605","\uf606"],"mi":"a"},"mn":["2","2"]},{"mrow":{"mo":["\uf605","\uf606"],"mi":"b"},"mn":["2","2"]}],"mo":["+","-"],"mrow":{"mn":"2","mo":["\u2062","\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mi":["a","T"]},"mi":"b"}}}}}]}},{"mrow":{"mo":["(",")"],"mn":"2"}}]}}}}},"Thus, the squared L2 distance between two vectors decouples into the squared L2 norm of each vector, minus twice the inner product of the two vectors.","Relating this to image alignment, a distance matrix may be generated for an n\u00d7n image tile T being mapped to a p\u00d7p image portion I, where p>n. In other words, the distance matrix may contain distances relating to respective fits between tile T and each n\u00d7n sub-image of image portion I. Note that image portion I may be a whole image, or any portion of the image that is bigger than tile T.","For purposes of simplicity, throughout the following discussion, image T and image portion I are assumed to be square. However, either or both of these could be rectangular instead. Thus, tile T could be m\u00d7n, and image portion I could be p\u00d7q. Further, the following discussion also assumes that image T and image portion I are grayscale for convenience, though the techniques described herein may be generalized to color images.","Formally, it would be desirable to generate a (p\u2212n+1)\u00d7(p\u2212n+1) distance matrix D, such that:\n\n()=\u03a3\u03a3(()\u2212())\u2003\u2003(3)\n\nWhere T(x,y) is the value of the pixel at the (x,y) position of tile T, and I(x+u, y+v) is the value of the pixel at the (x+u, y+v) position of image portion I. This calculation can be simplified as follows:\n\n()=\u03a3\u03a3(()+\u03a3\u03a3()\u22122\u03a3\u03a3(()()\u2003\u2003(4)\n","The first term depends only on T and not at all on u or v, and so it can be computed once and re-used when computing each value of D (u,v). The second term can be computed for all values of (u,v) by box filtering I(x,y), which can be done efficiently using sliding-window image filtering techniques or using integral images. The third term can also be computed for all values of (u,v) by cross-correlating I and T.","In general, box filtering of an image applies a linear filter to an input image such that each pixel in the filtered image has a value equal to the average value of its neighboring pixels in the input image. For instance, a 3\u00d73 box filter can be applied to each pixel of the input image to blur, sharpen, detect edges, and perform other effects to the input image. Here, the box filter is applied to I squared.","Cross-correlation can be expensive to compute naively, but can be sped up significantly by using fast Fourier transforms (FFTs). From the convolution theorem:\n\n\u2003\u2003(5)\n\nWhere  is the Fourier transform, is the inverse Fourier transform, \u2218 is the pointwise product of two vectors, and {a}* is the conjugate transpose of {a}.\n","Based on these observations, D can be expressed, for all offsets (u,v), as:",{"@attributes":{"id":"p-0078","num":"0077"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"D","mo":"=","mrow":{"msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mi":"T"},"mn":["2","2"]},"mo":["+","-"],"mrow":[{"mi":["box","filter"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mrow":{"mo":["(",")"],"mrow":{"msup":{"mi":"I","mn":"2"},"mo":",","mi":"n"}}},{"mn":"2","mo":["\u2062","\u2062"],"msup":{"mrow":{"mo":"-","mn":"1"}},"mrow":{"mo":["{","}"],"mrow":{"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"msup":{"mrow":{"mo":["{","}"],"mi":"I"},"mo":"*"},"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"\u2022","mrow":{"mo":["{","}"],"mi":"T"}}}}]}}},{"mrow":{"mo":["(",")"],"mn":"6"}}]}}}}},"Where the first term is the sum of the squared elements of T, the second term is the squared elements of image portion I filtered with a box filter of size n\u00d7n (where the box filter is not normalized), and the third term is based on the cross-correlation of I and T, computed efficiently using an FFT.","B. Example Quadratic Approximation of the Distance Matrix","A distance matrix D (u,v) contains a rich amount of information describing how well matched tile T and image portion I are for all possible translations. This is a powerful description, but it is also a large and somewhat unwieldy representation. For a 32\u00d732 pixel tile T (the tile being matched) and a 64\u00d764 image portion I (the image portion being searched for a match with tile T), there is a 33\u00d733 distance matrix D. Given that a goal is to find the single best match between tile T and image portion I, it is desirable to produce a simplified representation of distance matrix D by fitting a simple function to distance matrix D near the location of its minimum. The minimum of distance matrix D indicates the x and y direction offsets of the best determined fit between tile T and image portion I.","In order to provide a compact representation of distance matrix D, a two-dimensional polynomial, such as a bivariate quadratic surface, can be fit at or near the entry in distance matrix D that has the minimum value of all entries in distance matrix D. If multiple minima exist, any one may be chosen. This quadratic surface may be useful in a variety of ways. Such a quadratic surface could be used to estimate the sub-pixel location of the minimum of distance matrix D, which is more accurate than simply taking the per-pixel location as the minimum for most motion-estimation tasks. Additionally, a quadratic approximation could also be used as a compact approximation to distance matrix D in a more sophisticated motion estimation algorithm, such as an optical flow algorithm. In optical flow algorithms, for example, the relative confidences of respective motion estimates are used to weigh these estimates.","To clarify, distance matrix D may be viewed as an error surface that is to be approximated by a bivariate quadratic surface, where D (u,v) is the L2 distance between the tile T and image portion I when the tile T is offset (e.g., shifted) by (u,v) in the image portion I. This approximation should accurately model the shape of distance matrix D near a minimum, and it is acceptable for the approximation to be poor far from this minimum. In most cases, distance matrix D, as a whole, is poorly modeled with a single bivariate quadratic surface. But for the purposes herein, since the goal is to have a reasonably accurate fit near the minimum, less accurate fits away from the minimum are not problematic.","More formally, distance matrix D can be approximated as follows:",{"@attributes":{"id":"p-0085","num":"0084"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msup":{"mi":["D","\u2032"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["u","v"],"mo":","}}},{"mrow":[{"msup":{"mrow":{"mfrac":{"mn":["1","2"]},"mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mi":["u","v"],"mo":";"}}},"mn":"2"},"mo":"\u2062","mrow":{"mi":"A","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mi":["u","v"],"mo":";"}}}},{"msup":{"mi":["b","T"]},"mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mi":["u","v"],"mo":";"}}}],"mo":["+","+"],"mi":"c"}],"mo":"\u2248"}},{"mrow":{"mo":["(",")"],"mn":"7"}}]}}}},"br":{}},"A matrix M is PSD if the expression zMz is non-negative for every non-zero column vector z of n real numbers. A is assumed to be PSD because the shape of D\u2032 near its minimum is expected to be an upward-facing quadratic surface, rather than a saddle or a downward-facing surface.","Let (\u00fb,{circumflex over (v)}) be the coordinate of a minimum entry in distance matrix D. A 3\u00d73 area around (\u00fb,{circumflex over (v)}), D, can be used when fitting the bivariate quadratic surface. Thus,",{"@attributes":{"id":"p-0088","num":"0087"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["D","sub"]},"mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mrow":{"mi":"D","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mover":{"mi":"u","mo":"^"},"mo":"-","mn":"1"},{"mover":{"mi":"v","mo":"^"},"mo":"-","mn":"1"}],"mo":","}}}},{"mrow":{"mi":"D","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mover":{"mi":"u","mo":"^"},"mo":",","mrow":{"mover":{"mi":"v","mo":"^"},"mo":"-","mn":"1"}}}}},{"mrow":{"mi":"D","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mover":{"mi":"u","mo":"^"},"mo":"+","mn":"1"},{"mover":{"mi":"v","mo":"^"},"mo":"-","mn":"1"}],"mo":","}}}}]},{"mtd":[{"mrow":{"mi":"D","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mover":{"mi":"u","mo":"^"},"mo":"-","mn":"1"},"mo":",","mover":{"mi":"v","mo":"^"}}}}},{"mrow":{"mi":"D","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mover":[{"mi":"u","mo":"^"},{"mi":"v","mo":"^"}],"mo":","}}}},{"mrow":{"mi":"D","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mover":{"mi":"u","mo":"^"},"mo":"+","mn":"1"},"mo":",","mover":{"mi":"v","mo":"^"}}}}}]},{"mtd":[{"mrow":{"mi":"D","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mover":{"mi":"u","mo":"^"},"mo":"-","mn":"1"},{"mover":{"mi":"v","mo":"^"},"mo":"+","mn":"1"}],"mo":","}}}},{"mrow":{"mi":"D","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mover":{"mi":"u","mo":"^"},"mo":",","mrow":{"mover":{"mi":"v","mo":"^"},"mo":"+","mn":"1"}}}}},{"mrow":{"mi":"D","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mover":{"mi":"u","mo":"^"},"mo":"+","mn":"1"},{"mover":{"mi":"v","mo":"^"},"mo":"+","mn":"1"}],"mo":","}}}}]}]}}}},{"mrow":{"mo":["(",")"],"mn":"8"}}]}}}}},"Each pixel in Dcan be weighted according to a 3\u00d73 set of binomial weights:",{"@attributes":{"id":"p-0090","num":"0089"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"W","mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mn":"1"},{"mn":"2"},{"mn":"1"}]},{"mtd":[{"mn":"2"},{"mn":"4"},{"mn":"2"}]},{"mtd":[{"mn":"1"},{"mn":"2"},{"mn":"1"}]}]}}}},{"mrow":{"mo":["(",")"],"mn":"9"}}]}}}}},"With Dand W, a least-squares problem can be set up with respect to the free parameters in the quadratic approximation (A,b,c). Solving such a linear system is computationally expensive to do in practice, but a closed-form solution can be derived for any arbitrary 3\u00d73 error surface with the weighting W. This solution is expressible in terms of six 3\u00d73 filters:",{"@attributes":{"id":"p-0092","num":"0091"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"msub":{"mi":"f","msub":{"mi":"A","mrow":{"mn":["1","1"],"mo":","}}},"mo":"=","mrow":{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mn":"1"},{"mrow":{"mo":"-","mn":"2"}},{"mn":"1"}]},{"mtd":[{"mn":"4"},{"mrow":{"mo":"-","mn":"8"}},{"mn":"4"}]},{"mtd":[{"mn":"1"},{"mrow":{"mo":"-","mn":"2"}},{"mn":"1"}]}]}},"mo":"\/","mn":"6"}}},{"mrow":{"mo":["(",")"],"mn":"10"}}]},{"mtd":[{"mrow":{"msub":{"mi":"f","msub":{"mi":"A","mrow":{"mn":["1","2"],"mo":","}}},"mo":"=","mrow":{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mn":"1"},{"mn":"0"},{"mrow":{"mo":"-","mn":"1"}}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mrow":{"mo":"-","mn":"1"}},{"mn":"0"},{"mn":"1"}]}]}},"mo":"\/","mn":"4"}}},{"mrow":{"mo":["(",")"],"mn":"11"}}]},{"mtd":[{"mrow":{"msub":{"mi":"f","msub":{"mi":"A","mrow":{"mn":["2","2"],"mo":","}}},"mo":"=","mrow":{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mn":"1"},{"mn":"4"},{"mn":"1"}]},{"mtd":[{"mrow":{"mo":"-","mn":"2"}},{"mrow":{"mo":"-","mn":"8"}},{"mrow":{"mo":"-","mn":"2"}}]},{"mtd":[{"mn":"1"},{"mn":"4"},{"mn":"1"}]}]}},"mo":"\/","mn":"6"}}},{"mrow":{"mo":["(",")"],"mn":"12"}}]},{"mtd":[{"mrow":{"msub":{"mi":"f","msub":{"mi":"b","mn":"1"}},"mo":"=","mrow":{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mrow":{"mo":"-","mn":"1"}},{"mn":"0"},{"mn":"1"}]},{"mtd":[{"mrow":{"mo":"-","mn":"4"}},{"mn":"0"},{"mn":"4"}]},{"mtd":[{"mrow":{"mo":"-","mn":"1"}},{"mn":"0"},{"mn":"1"}]}]}},"mo":"\/","mn":"6"}}},{"mrow":{"mo":["(",")"],"mn":"13"}}]},{"mtd":[{"mrow":{"msub":{"mi":"f","msub":{"mi":"b","mn":"2"}},"mo":"=","mrow":{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mrow":{"mo":"-","mn":"1"}},{"mrow":{"mo":"-","mn":"4"}},{"mrow":{"mo":"-","mn":"1"}}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mn":"1"},{"mn":"4"},{"mn":"1"}]}]}},"mo":"\/","mn":"6"}}},{"mrow":{"mo":["(",")"],"mn":"14"}}]},{"mtd":[{"mrow":{"msub":{"mi":["f","c"]},"mo":"=","mrow":{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mrow":{"mo":"-","mn":"1"}},{"mn":"2"},{"mrow":{"mo":"-","mn":"1"}}]},{"mtd":[{"mn":"2"},{"mn":"32"},{"mn":"2"}]},{"mtd":[{"mrow":{"mo":"-","mn":"1"}},{"mn":"2"},{"mrow":{"mo":"-","mn":"1"}}]}]}},"mo":"\/","mn":"36"}}},{"mrow":{"mo":["(",")"],"mn":"15"}}]}]}}}},"The free parameters of the quadratic approximation can be found by taking the inner product of Dwith these filters (assuming the error surface and the filter have been vectorized), or equivalently by computing the cross-correlation of Dwith these filters:",{"@attributes":{"id":"p-0094","num":"0093"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mi":"A","mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mrow":{"msub":[{"mi":"f","msub":{"mi":"A","mrow":{"mn":["1","1"],"mo":","}}},{"mi":["D","sub"]}],"mo":"\u00b7"}},{"mrow":{"msub":[{"mi":"f","msub":{"mi":"A","mrow":{"mn":["1","2"],"mo":","}}},{"mi":["D","sub"]}],"mo":"\u00b7"}}]},{"mtd":[{"mrow":{"msub":[{"mi":"f","msub":{"mi":"A","mrow":{"mn":["1","2"],"mo":","}}},{"mi":["D","sub"]}],"mo":"\u00b7"}},{"mrow":{"msub":[{"mi":"f","msub":{"mi":"A","mrow":{"mn":["2","2"],"mo":","}}},{"mi":["D","sub"]}],"mo":"\u00b7"}}]}]}}}},{"mrow":{"mo":["(",")"],"mn":"16"}}]},{"mtd":[{"mrow":{"mi":"b","mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mrow":{"msub":[{"mi":"f","msub":{"mi":"b","mn":"1"}},{"mi":["D","sub"]}],"mo":"\u00b7"}}},{"mtd":{"mrow":{"msub":[{"mi":"f","msub":{"mi":"b","mn":"2"}},{"mi":["D","sub"]}],"mo":"\u00b7"}}}]}}}},{"mrow":{"mo":["(",")"],"mn":"17"}}]},{"mtd":[{"mrow":{"mi":"c","mo":"=","mrow":{"msub":[{"mi":["f","c"]},{"mi":["D","sub"]}],"mo":"\u00b7"}}},{"mrow":{"mo":["(",")"],"mn":"18"}}]}]}}}},"Due to image filtering being a linear operation, the bivariate quadratic surface can be fit to a larger area of distance matrix D than a 3\u00d73 section. For instance, it is sufficient to pre-filter distance matrix D with a blur, and then perform the 3\u00d73 operation above on the blurred error surface.","In some cases, depending on the shape of D, the estimated A might not be positive semi-definite, contrary to the assumption above. To address this issue, the diagonal elements of A can be set as non-negative:\n\n=max(0,)\u2003\u2003(19)\n\n=max(0,)\u2003\u2003(20)\n","The determinant of A can be calculated as:",{"@attributes":{"id":"p-0098","num":"0097"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"det","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"A"}},{"mrow":{"msub":[{"mi":"A","mrow":{"mn":["1","1"],"mo":","}},{"mi":"A","mrow":{"mn":["2","2"],"mo":","}}],"mo":"\u2062"},"mo":"-","msubsup":{"mi":"A","mrow":{"mn":["1","2"],"mo":","},"mn":"2"}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"21"}}]}}}}},"If det(A)<0, then the off-diagonal elements of A can be set to be zero. These operations result in an A that is guaranteed to be positive semi-definite.","With this in place, the minimum of the bivariate quadratic surface fit to distance matrix D can be found. To do so, the surface can be rewritten in a different form:",{"@attributes":{"id":"p-0101","num":"0100"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":[{"mrow":[{"mfrac":{"mn":["1","2"]},"mo":["\u2062","\u2062","\u2062","\u2062"],"msup":{"mi":["x","T"]},"mi":["A","x"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"msup":{"mi":["b","T"]},"mo":"\u2062","mi":"x"}],"mo":["+","+"],"mi":"c"},{"mrow":{"mfrac":{"mn":["1","2"]},"mo":["\u2062","\u2062"],"msup":{"mrow":{"mo":["(",")"],"mrow":{"mi":["x","\u03bc"],"mo":"-"}},"mi":"T"},"mrow":{"mi":"A","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","\u03bc"],"mo":"-"}}}},"mo":"+","mi":"s"}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"22"}}]},{"mtd":[{"mi":"Where"},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}]},{"mtd":[{"mrow":{"mi":"\u03bc","mo":"=","mrow":{"mrow":{"mo":"-","msup":{"mi":"A","mrow":{"mo":"-","mn":"1"}}},"mo":"\u2062","mi":"b"}}},{"mrow":{"mo":["(",")"],"mn":"23"}}]},{"mtd":[{"mrow":{"mi":"s","mo":"=","mrow":{"mi":"c","mo":"-","mfrac":{"mrow":{"msup":{"mi":["\u03bc","T"]},"mo":["\u2062","\u2062","\u2062"],"mi":["A","\u03bc"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mn":"2"}}}},{"mrow":{"mo":["(",")"],"mn":"24"}}]}]}}}},"For a bivariate quadratic surface, this is equivalent to:",{"@attributes":{"id":"p-0103","num":"0102"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mi":"\u03bc","mo":"=","mfrac":{"msup":{"mrow":{"mo":["[","]"],"mrow":{"mrow":[{"mrow":[{"msub":[{"mi":"A","mrow":{"mn":["2","2"],"mo":","}},{"mi":"b","mn":"1"}],"mo":"\u2062"},{"msub":[{"mi":"A","mrow":{"mn":["1","2"],"mo":","}},{"mi":"b","mn":"2"}],"mo":"\u2062"}],"mo":"-"},{"mrow":[{"msub":[{"mi":"A","mrow":{"mn":["1","1"],"mo":","}},{"mi":"b","mn":"2"}],"mo":"\u2062"},{"msub":[{"mi":"A","mrow":{"mn":["1","2"],"mo":","}},{"mi":"b","mn":"1"}],"mo":"\u2062"}],"mo":"-"}],"mo":","}},"mi":"T"},"mrow":{"mrow":{"msub":[{"mi":"A","mrow":{"mn":["1","1"],"mo":","}},{"mi":"A","mrow":{"mn":["2","2"],"mo":","}}],"mo":"\u2062"},"mo":"-","msubsup":{"mi":"A","mrow":{"mn":["1","2"],"mo":","},"mn":"2"}}}}},{"mrow":{"mo":["(",")"],"mn":"25"}}]},{"mtd":[{"mrow":{"mi":"s","mo":"=","mrow":{"mi":"c","mo":"-","mfrac":{"mrow":{"mrow":[{"msub":{"mi":"A","mrow":{"mn":["1","1"],"mo":","}},"mo":"\u2062","msubsup":{"mi":"\u03bc","mn":["1","2"]}},{"mn":"2","mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":[{"mi":"A","mrow":{"mn":["1","2"],"mo":","}},{"mi":"\u03bc","mn":"1"},{"mi":"\u03bc","mn":"2"}]},{"msub":{"mi":"A","mrow":{"mn":["2","2"],"mo":","}},"mo":"\u2062","msubsup":{"mi":"\u03bc","mn":["2","2"]}}],"mo":["+","+"]},"mn":"2"}}}},{"mrow":{"mo":["(",")"],"mn":"26"}}]}]}}}},"These expressions can also be solved for b and c:",{"@attributes":{"id":"p-0105","num":"0104"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mi":"b","mo":"=","mrow":{"mrow":[{"mrow":{"mo":"-","mi":"A"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"\u03bc"},{"mo":"-","msup":{"mrow":{"mo":["[","]"],"mrow":{"mrow":[{"mrow":[{"msub":[{"mi":"A","mrow":{"mn":["1","1"],"mo":","}},{"mi":"\u03bc","mn":"1"}],"mo":"\u2062"},{"msub":[{"mi":"A","mrow":{"mn":["1","2"],"mo":","}},{"mi":"\u03bc","mn":"2"}],"mo":"\u2062"}],"mo":"+"},{"mrow":[{"msub":[{"mi":"A","mrow":{"mn":["1","2"],"mo":","}},{"mi":"\u03bc","mn":"1"}],"mo":"\u2062"},{"msub":[{"mi":"A","mrow":{"mn":["2","2"],"mo":","}},{"mi":"\u03bc","mn":"2"}],"mo":"\u2062"}],"mo":"-"}],"mo":","}},"mi":"T"}}],"mo":"="}}},{"mrow":{"mo":["(",")"],"mn":"27"}}]},{"mtd":[{"mrow":{"mi":"c","mo":"=","mrow":{"mrow":[{"mi":"s","mo":"+","mfrac":{"mrow":{"msup":{"mi":["\u03bc","T"]},"mo":["\u2062","\u2062","\u2062"],"mi":["A","\u03bc"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mn":"2"}},{"mi":"s","mo":"+","mfrac":{"mrow":{"mrow":[{"msub":{"mi":"A","mrow":{"mn":["1","1"],"mo":","}},"mo":"\u2062","msubsup":{"mi":"\u03bc","mn":["1","2"]}},{"mn":"2","mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":[{"mi":"A","mrow":{"mn":["1","2"],"mo":","}},{"mi":"\u03bc","mn":"1"},{"mi":"\u03bc","mn":"2"}]},{"msub":{"mi":"A","mrow":{"mn":["2","2"],"mo":","}},"mo":"\u2062","msubsup":{"mi":"\u03bc","mn":["2","2"]}}],"mo":["+","+"]},"mn":"2"}}],"mo":"="}}},{"mrow":{"mo":["(",")"],"mn":"28"}}]}]}}}},"Once the location of the minimum of the bivariate quadratic surface is determined, that is used as the sub-pixel location of the minimum of distance matrix D. Note that the fitted bivariate quadratic surface treats the center pixel of Das (0,0). So, after fitting, the per-pixel minimum location (\u00fb,{circumflex over (v)}) is added into \u03bc, which provides the actual location of the minimum in minimum of distance matrix D. In the presence of severe noise or flat images with little texture, it is possible for the predicted sub-pixel minimum \u03bc to be different from the observed per-pixel minimum (\u00fb,{circumflex over (v)}). If these two values are sufficiently different (e.g., more than 1 pixel removed), \u03bc is set to [\u00fb;{circumflex over (v)}].","C. Example Distance Matrixes and Quadratic Approximations of Real Images",{"@attributes":{"id":"p-0108","num":"0107"},"figref":"FIG. 5"},"The first column of  shows the three tiles, and the second column shows respective image portions. Each image portion may be searched for one or more matches of its associated tile. The third column shows the distance matrix D for each tile, calculated using Equation (6). The fourth column shows the bivariate quadratic fit to that distance matrix, around the minimum point of the distance matrix, and clipped to the maximum value of the distance matrix. The fifth column shows 3D visualizations of the fitted bivariate quadratic.","In addition to representing a fit between a tile and its associated image portion, each bivariate quadratic surface fits also represent confidence measures of the fit. Where the surface has a small value on the z-axis (the vertical axis), the confidence of the fit is higher, and where the surface has a larger value on the z-axis, the confidence of the fit is lower.","From , it can be noted that for tiles that are highly textured and distinctive, such as the one in the first row, the distance matrix has a very clear minimum value and the bivariate quadratic fit is curved and well-localized as a result. In the second row, the tile depicts only an edge in the image, the distance matrix has many possible minimum values oriented alongside that edge, and the bivariate quadratic is therefore confident in one axis, but ambiguous in the perpendicular axis. In the third row, where the tile and the image portion is un-textured and mostly flat, the minimum is poorly-localized, as shown by the low curvature of the bivariate quadratic in all directions.","D. Example Information Filtering","In the previous sections, estimating a bivariate quadratic surface for each tile in an image was demonstrated. The bivariate quadratic surface describes the local shape of an error surface, and assumes that the minimum of each error surface was a good estimate of the displacement vector (the offset between pixels) across the two images being matched. These operations provide a per-tile estimate of motion, but do not provide a per-pixel estimate of motion, which is desirable for many applications. This section introduces a technique for applying a linear filter (such as an image upsampling operation or an edge-aware filtering operation) to a set of bivariate quadratic surfaces. In doing so, estimates of per-pixel motion can be obtained.","In order to simplify calculations, it is assumed that each bivariate quadratic surface actually describes the negative log-likelihood of a multivariate normal distribution. A multivariate normal distribution may be parameterized by a vector of means \u03bc and a covariance matrix \u03a3:",{"@attributes":{"id":"p-0115","num":"0114"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["x","\u03bc"],"mo":"|"},"mo":",","mi":"\u03a3"}}},"mo":"=","mfrac":{"mrow":{"mi":"exp","mo":["(",")"],"mrow":{"mrow":[{"mo":"-","mfrac":{"mn":["1","2"]}},{"msup":{"mi":"\u03a3","mrow":{"mo":"-","mn":"1"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","\u03bc"],"mo":"-"}}}],"mo":["\u2062","\u2062"],"msup":{"mrow":{"mo":["(",")"],"mrow":{"mi":["x","\u03bc"],"mo":"-"}},"mi":"T"}}},"msqrt":{"mrow":{"msup":{"mrow":{"mo":["(",")"],"mrow":{"mn":"2","mo":"\u2062","mi":"\u03c0"}},"mi":"k"},"mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mi":"\u03a3"}}}}}},{"mrow":{"mo":["(",")"],"mn":"29"}}]}}}}},"Thus, a set of multivariate normal distributions (i.e., the bivariate quadratic surfaces for each tile in the first image), can be parameterized by means {\u03bc} and covariance matrixes {\u03a3}. A weighted geometric mean (according to a vector of weights w) of these normal distributions can be taken to get a weighted \u201caverage\u201d distribution parametrized by means  and a covariance matrix :",{"@attributes":{"id":"p-0117","num":"0116"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"x","mo":"|","mover":{"mi":["\u03bc","_"]}},"mo":",","mover":{"mi":["\u03a3","_"]}}}},{"mi":"exp","mo":["(",")"],"mrow":{"msub":{"mi":["w","i"]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"log","mo":["(",")"],"mrow":{"mi":"P","mo":["(",")"],"mrow":{"mrow":{"mi":"x","mo":"|","msup":{"mi":"\u03bc","mrow":{"mo":["(",")"],"mi":"i"}}},"mo":",","msup":{"mi":"\u03a3","mrow":{"mo":["(",")"],"mi":"i"}}}}}}}],"mo":"\u221d"}},{"mrow":{"mo":["(",")"],"mn":"30"}}]}}}},"br":{}},{"@attributes":{"id":"p-0118","num":"0117"},"maths":{"@attributes":{"id":"MATH-US-00015","num":"00015"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mover":{"mi":["\u03a3","_"]},"mo":"=","msup":{"mrow":[{"mo":["(",")"],"mrow":{"msub":{"mi":["\u03a3","i"]},"mo":"\u2062","msup":{"mrow":[{"msub":{"mi":["w","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":"\u03a3","mrow":{"mo":["(",")"],"mi":"i"}}}},{"mo":"-","mn":"1"}]}}},{"mo":"-","mn":"1"}]}}},{"mrow":{"mo":["(",")"],"mn":"31"}}]},{"mtd":[{"mrow":{"mover":{"mi":["\u03bc","_"]},"mo":"=","mrow":{"mover":{"mi":["\u03a3","_"]},"mo":["\u2062","\u2062","\u2062"],"msub":{"mi":["\u03a3","i"]},"msup":[{"mrow":[{"msub":{"mi":["w","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":"\u03a3","mrow":{"mo":["(",")"],"mi":"i"}}}},{"mo":"-","mn":"1"}]},{"mi":"\u03bc","mrow":{"mo":["(",")"],"mi":"i"}}]}}},{"mrow":{"mo":["(",")"],"mn":"32"}}]}]}}}},"Equation (30) is an awkward expression and difficult to manipulate, but can be simplified by re-writing it as an exponentiated polynomial:",{"@attributes":{"id":"p-0120","num":"0119"},"maths":{"@attributes":{"id":"MATH-US-00016","num":"00016"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["x","A"],"mo":"|"},"mo":",","mi":"b"}}},{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mo":"-","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mfrac":{"mn":["1","2"]},"mo":["\u2062","\u2062","\u2062","\u2062"],"msup":{"mi":["x","T"]},"mi":["A","x"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"msup":{"mi":["b","T"]},"mo":"\u2062","mi":"x"}],"mo":["+","+"],"mi":"c"}}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"33"}}]},{"mtd":[{"mi":"Where"},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}]},{"mtd":[{"mrow":{"mi":"A","mo":"=","mover":{"mo":"\u2211","mrow":{"mo":"-","mn":"1"}}}},{"mrow":{"mo":["(",")"],"mn":"34"}}]},{"mtd":[{"mrow":{"mi":"b","mo":"=","mrow":{"mrow":{"mo":"-","mi":"A"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"\u03bc"}}},{"mrow":{"mo":["(",")"],"mn":"35"}}]},{"mtd":[{"mrow":{"mi":"c","mo":"=","mrow":{"mfrac":{"mn":["1","2"]},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":["k","log","\u03c0"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mn":"2"},{"mi":"log","mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mi":"A"}},{"msup":[{"mi":["b","T"]},{"mi":"A","mrow":{"mo":"-","mn":"1"}}],"mo":["\u2062","\u2062"],"mi":"b"}],"mo":["-","+"]}}}}},{"mrow":{"mo":["(",")"],"mn":"36"}}]}]}}}},"Rewritten as such, this format has the convenient consequence of dramatically simplifying the process of taking a weighted geometric mean of a set of n distributions {A,b}:\n\n()\u221dexp(log(()))\u2003\u2003(37)\n\nWhere:\n\n\u2003\u2003(38)\n\n\u2003\u2003(39)\n","The averaged multivariate normal distribution in standard form is the average of the standard-form coefficients of the input distributions. Or put another way, the output parameters are simply a weighted sum of the input parameters. This result is based on the geometric mean of a set of distributions being the average of those distributions in log-space, and that in log-space the distributions are polynomials.","With this insight, a compact vectorised representation of the multivariate normal distributions can be expressed as:",{"@attributes":{"id":"p-0124","num":"0123"},"maths":{"@attributes":{"id":"MATH-US-00017","num":"00017"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"x","mrow":{"mo":["(",")"],"mi":"i"}},"mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msup":{"mi":"b","mrow":{"mo":["(",")"],"mi":"i"}}}},{"mtd":{"mrow":{"mi":"triu","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":"A","mrow":{"mo":["(",")"],"mi":"i"}}}}}}]}}}},{"mrow":{"mo":["(",")"],"mn":"40"}}]}}}}},"Where triu(A) is an operation that returns a k(k+1)\/2 dimensional vector containing the upper-triangular part of symmetric matrix A. Similarly, A=triu(\u2022) is an operation that takes such a vector and returns a k\u00d7k symmetric matrix A. With this vectorized representation of the multivariate normal distributions, the weighted geometric mean of normal distributions can be expressed as:\n\n\u2003\u2003(41)\n\n([(1):end])\u2003\u2003(42)\n\n=\u22121:\u2003\u2003(43)\n","Assuming for the moment that, in addition to a set of n multivariate normal distributions as input {x}, it is desirable to determine a set of p multivariate normal distributions as output {y}, and for each output distribution there is a different set of weights {w}. Then:\n\n=\u03a3\u2003\u2003(44)\n","This expression can be rewritten as a matrix-matrix product:",{"@attributes":{"id":"p-0128","num":"0127"},"maths":{"@attributes":{"id":"MATH-US-00018","num":"00018"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"mi":"X","mo":"=","msup":{"mrow":{"mo":["[","]"],"mrow":{"msup":[{"mi":"x","mrow":{"mo":["(",")"],"mn":"1"}},{"mi":"x","mrow":{"mo":["(",")"],"mn":"2"}},{"mi":"x","mrow":{"mo":["(",")"],"mn":"3"}},{"mi":"x","mrow":{"mo":["(",")"],"mi":"n"}}],"mo":[",",",",",","\u2062",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}},"mi":"T"}}}},{"mtd":{"mrow":{"mi":"W","mo":"=","msup":{"mrow":{"mo":["[","]"],"mrow":{"msup":[{"mi":"w","mrow":{"mo":["(",")"],"mn":"1"}},{"mi":"w","mrow":{"mo":["(",")"],"mn":"2"}},{"mi":"w","mrow":{"mo":["(",")"],"mn":"3"}},{"mi":"w","mrow":{"mo":["(",")"],"mi":"p"}}],"mo":[",",",",",","\u2062",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}},"mi":"T"}}}},{"mtd":{"mrow":{"mi":"Y","mo":"=","msup":{"mrow":{"mo":["[","]"],"mrow":{"msup":[{"mi":"y","mrow":{"mo":["(",")"],"mn":"1"}},{"mi":"y","mrow":{"mo":["(",")"],"mn":"2"}},{"mi":"y","mrow":{"mo":["(",")"],"mn":"3"}},{"mi":"y","mrow":{"mo":["(",")"],"mi":"p"}}],"mo":[",",",",",","\u2062",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}},"mi":"T"}}}},{"mtd":{"mrow":{"mi":["Y","WX"],"mo":"="}}}]}},{"mrow":{"mo":["(",")"],"mn":"45"}}]}}}},"br":{}},"With this matrix formulation of the problem, this process can be re-interpreted in terms of image filtering. Assume that for an image of multivariate normal distributions, each pixel has a mean and a covariance matrix. Based on a linear filtering operation, the set of input normal distributions can be averaged to get a filtered set of output normal distributions. This can be done by constructing an image with (k+3k)\/2 channels using the vectorization operation in Equation (40), which is equivalent to the X matrix in Equation (45). Each channel can be filtered (equivalent to taking the matrix-matrix product WX) to get the output filters. The vectorization operation can be unpacked as described earlier to get the set of per-pixel output multivariate normal distributions.","There is no restriction on W, and W need not be an actual matrix, but can instead by any linear operation (that is, any linear filter or resampling operation). W may be row-stochastic, so that each output normal distribution is a convex combination of the input normal distributions. But, due to the normalization involved in the filter, the value of the output mean of each normal distribution  is invariant to the scale of the each row of W. So, though W may be row-normalized to produce meaningful precisions, it does not need to be row-normalized to produce accurate output values.","Estimating 2D motion on an image plane is an example of the two-dimensional (k=2) case. Thus, a five-dimensional image can be constructed, in which the first two dimensions are the elements of b, and the last three dimensions are the three unique values in the precision matrix triu(A), as shown in Equation (40). After filtering this five-dimensional image, each pixel's estimated motion \u03bccan be extracted using the transformation described in Equation (43) on each pixel's five values.","Each dimension loosely corresponds to one the five free parameters of a two-dimensional normal distribution: mean in x, mean in y, variance in x, covariance of x and y, and variance in y. Using Equations (34) and (35), these five quantities are reworked so that they roughly correspond to: precision in x (where precision is the inverse of variance), precision in y, precision in xy, the mean in x decorrelated by the precision matrix, and the mean in y decorrelated by the precision matrix. In some cases, the three precision quantities are the elements of the precision matrix of the normal distribution.","This section provides two example applications, image burst de-noising and edge-aware optical flow, for the techniques disclosed herein. However, other applications may exist that could benefit from these techniques.","A. Image Burst De-Noising","The techniques described herein for matching tiles to image portions can be used as a way of matching image patches across a burst of images for the purpose of de-noising one or more images in the burst. Images captured by some sensors, such as those found on cell phones, tend to produce high amounts of noise, creating unattractive artifacts when the images are viewed at high resolutions. To lower the amount of noise, one could attempt to take a burst of images from the camera and combine (e.g., average) those images together. However, this approach does not always work well on some scenes, as the motion of the camera and of the subjects in the scene means that naively combining frames will cause ghosting. Therefore, the images in the burst should be aligned against a single image from the burst, and then those aligned images can be combined.","Given a burst, a single image is selected from the burst to use as a \u201ctemplate\u201d image. For each m\u00d7n (e.g., 32\u00d732) tile in the template image, the previously-described matching procedure is used to align that tile against the corresponding p\u00d7q (e.g., 64\u00d764) image region in the other images in the burst. The per-tile bivariate quadratic fits are upsampled with the previously described information filtering technique, where bicubic interpolation is used with each tile's bivariate quadratic surface, and from which the mean offset for each pixel can be extracted (see Equation (45)). Given this estimated per-pixel offset, the other images can be warped into the \u201ctemplate\u201d image, and then the warped images can be combined to create a de-noised image.","For an example of image burst de-noising, see . The first row is a four-frame burst of a scene, in which the camera and the subjects are moving. The second row contains sub-regions of one of the images from the burst, which shows that there is noise in the images. The third row contains the same sub-regions, where na\u00efve combining of the pixel values in the burst has been applied. Though this does reduce noise, but it also causes substantial ghosting due to the motion of the objects. For the fourth row, each image in the burst has been warped into a \u201ctemplate\u201d image, and then the warped images have been combined. These images have low noise and no ghosting.","B. Edge-Aware Optical Flow","Another application is optical flow. Given two images, a flow vector may be assigned to each pixel, where the flow vector represents movement of the pixel from one image to the other. Doing so for some images can be challenging because of the aperture problem. Motion is difficult to locally estimate from two images, because observing the motion of an edge only constrains the motion vector to a one-dimensional subspace of possible flow vectors. To correctly estimate global motion from local motion estimates, the information provided by image edges can be combined, together with the uncertainty inherent in such information, and propagated across an image to resolve flow ambiguities.","Note that for some image patches the aperture problem does not hold. For flat image patches, the motion is entirely unconstrained, and should be modeled accordingly. For highly texture images patches the motion may be entirely constrained in both dimensions. Note that the three types of patches\u2014flat, edge, and texture\u2014are all the same phenomenon viewed at different scales. An image patch containing a small square may be thought of as texture, while a patch containing the inside of a large square may be flat, and a patch of a medium-sized square will likely contain just one edge.","The information filtering technique described above may be used as the backbone of an optical flow algorithm. For every tile in one image, a bivariate normal distribution modeling the well-matched locations in the other image is estimated. Then the flow-field is upsampled to produce a per-pixel flow field, as previously described. An edge-aware filter may be applied, using the same information filtering approach. One filter that can be used is the recursive formulation of a domain transform (though it could be any linear filter). A domain transform is well-suited because it is an edge-aware filter\u2014it propagates information along edges but not across edges. This produces pleasant looking flow fields in which the output flow closely tracks edges in the input image. Such edge-aware flow fields are useful for tracking and segmenting objects, for video retiming, and so on. The edge-aware nature of this filter naturally complements the difficulties of motion estimation. For example, in flat regions of the image where local motion cues are weakest, the domain transform will \u201cin-paint\u201d those regions with the information gained from observing the edges that surround that flat region. See  for an example of this technique in practice.",{"@attributes":{"id":"p-0142","num":"0141"},"figref":["FIGS. 7 and 8","FIGS. 7 and 8"],"b":"100"},"Block  of  may involve obtaining a first captured image of a scene and a second captured image of the scene. Block  may involve, for a plurality of m\u00d7n pixel tiles of the first captured image, determining respective distance matrixes. The distance matrixes may represent respective fit confidences between the m\u00d7n pixel tiles and pluralities of target p\u00d7q pixel tiles in the second captured image (e.g., image portions of the second captured image).","In some embodiments, the m\u00d7n pixel tiles do not overlap with one another. Further, as an example, the m\u00d7n pixel tiles may be 32\u00d732 pixel tiles and the p\u00d7q pixel tiles may be 64\u00d764 pixel tiles. Thus, in some cases, m=n and p=q.","Block  may involve approximating the distance matrixes with respective bivariate quadratic surfaces. Block  may involve upsampling the bivariate quadratic surfaces to obtain respective offsets for pixels in the plurality of m\u00d7n pixel tiles, such that the respective offsets, when applied to pixels in the plurality of m\u00d7n pixel tiles, cause parts of the first captured image to estimate locations in the second captured image. Upsampling the bivariate quadratic surfaces may involve applying a Kalman filter.","In some embodiments, upsampling the bivariate quadratic surfaces uses bicubic interpolation of respective 3\u00d73 tile regions in the first captured image that surround each respective m\u00d7n pixel tile. In these embodiments, one of the first captured image or the second captured image may be selected for warping. Based on the respective offsets, pixels of the selected image may be moved to create a warped image. Then, respective pixel values of the warped image and the non-selected image may be combined to form a de-noised image.","In some embodiments, upsampling the bivariate quadratic surfaces uses an edge-aware filter on respective tile regions in the first captured image that surround each respective m\u00d7n pixel tile, and are defined by one or more edges in the first captured image. In these embodiments, an intermediate image that represents intermediate locations of pixels may be interpolated. These pixels may be from the first captured image and the second captured image, and may represent an intermediate version of the scene that is temporally or physically between those of the first captured image and the second captured image. The interpolation may be based on the first captured image, the second captured image, and the respective offsets.","In some embodiments, the first captured image and the second captured image may have been captured less than 1 second apart. Alternatively or additionally, obtaining the first captured image of the scene and the second captured image of the scene may involve capturing, by the computing device, the first captured image and the second captured image.","In some embodiments, one or more entries in each distance matrix are respective minima, and approximating the distance matrixes with respective bivariate quadratic surfaces may involve fitting minima of the respective bivariate quadratic surfaces to the respective minima of the distance matrixes. Fitting minima of the respective bivariate quadratic surfaces to the respective minima of the distance matrixes may involve fitting the respective bivariate quadratic surfaces to respective binomially-weighted 3\u00d73 pixel regions surrounding the respective minima of the distance matrixes.","In some embodiments, a particular distance matrix for a particular m\u00d7n pixel tile may be based on a linear combination of (i) a sum of squared values in the particular m\u00d7n pixel tile, (ii) squared values in the second captured image filtered by a box filter, and (iii) a cross-correlation of the second captured image and the particular m\u00d7n pixel tile.",{"@attributes":{"id":"p-0151","num":"0150"},"figref":["FIG. 8","FIG. 7"]},"Block  of  may involve obtaining a first captured image of a scene and a second captured image of the scene. Block  may involve, for an m\u00d7n pixel tile of the first captured image, determining a distance matrix. The distance matrix may represent fit confidences between the m\u00d7n pixel tile and a plurality of target p\u00d7q pixel tiles in the second captured image.","Block  may involve approximating the distance matrix with a bivariate quadratic surface. Block  may involve upsampling the bivariate quadratic surface to obtain respective offsets for pixels in the m\u00d7n pixel tile. The upsampling may take place such that the respective offsets, when applied to pixels in the m\u00d7n pixel tile, cause parts of the first captured image to estimate locations in the second captured image.","Additionally, embodiments related to  may include one or more of the features disclosed in the context of , or any other figure.","The previous sections described, among other aspects, embodiments for taking a per-tile estimate of the motion between two images and quickly upsampling that per-tile flow to produce a per-pixel flow, using just a single image filtering operation in an information-filtering framework. This approach is computationally efficient, but because it is a feed-forward filtering operation, the output per-pixel flow produced might not always be of sufficiently high quality. This section provides an alternative embodiment for producing a per-pixel flow field from a per-tile flow field. This alternative embodiment is designed to produce higher quality results, at the cost of being more expensive to compute.","The embodiment has two parts. The first part takes a per-tile flow (motion) estimate and produces per-pixel flow estimates that may be noisy. But there is a fairly reliable \u201cconfidence\u201d measurement for each pixel as to the reliability of the respective flow estimate. The second part takes these noisy per-pixel flow estimates, and from them produces a smooth and accurate flow field as well as a refined per-pixel measure of confidence. These flow estimates for individual image pairs can then be combined into a larger estimate of the motion of all of the frames of a video by assuming that the motion of the objects in the video is temporally smooth.","In this section, a slightly different notation for a tile of an image is used. The term (T,T) refers to some tile at offset (x,y) in the image. Further, any pixel referred to using the coordinates x and y are assumed to be within tile (T,T).","From the embodiments of the previous sections, for every tile (T,T), there is an estimate of that tile's motion that can be modeled by a mean motion vector \u03bc and a precision matrix A of that motion, as well as, d, the squared L2 distance of the patches which produced this motion. From these a per-tile confidence can be estimated:",{"@attributes":{"id":"p-0159","num":"0158"},"maths":{"@attributes":{"id":"MATH-US-00019","num":"00019"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"C","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["T","x"]},{"mi":["T","y"]}],"mo":","}}},{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mo":"-","mfrac":{"msub":{"mi":["d","patch"]},"msubsup":{"mi":["\u03c3","patch"],"mn":"2"}}},"mo":"+","mfrac":{"mrow":{"mi":"log","mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mi":"A"}},"msub":{"mi":["\u03c3","logdet"]}}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"46"}}]}}}},"br":{},"sub":["patch","log det "],"sup":"2 "},"In general, the \u03c3 parameters herein control the relative importance of each heuristic assumption made about the flow fields (smoothness, fidelity, symmetry, etc.). These parameters may be tuned by hand to produce as accurate a flow field as possible, as determined by visual inspection.","The quantity C(T,T) is large if the tiles matched (tile (T,T) of one image and another tile in another image) to produce the flow estimate of tile (T,T) were well matched (e.g., the error between the two tiles is small), and if the match is well-localized (e.g., the error between the two tiles increases quickly if the estimated movement is altered).","After determining C(T,T), the confidence measurements can be refined based on each local neighborhood of tiles, using the observation that reliable flow measurements tend to have nearby tiles with similar flow measurements. For each tile, the difference between the estimated flow of the tile and that of one or more of its neighbors can be measured. If none of the neighboring tiles have a similar flow, the confidence in the motion of the tile can be down-weighted. Or, more precisely:",{"@attributes":{"id":"p-0163","num":"0162"},"maths":{"@attributes":{"id":"MATH-US-00020","num":"00020"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"C","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["T","x"]},{"mi":["T","y"]}],"mo":","}}},{"mrow":[{"mi":"C","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["T","x"]},{"mi":["T","y"]}],"mo":","}}},{"mi":"exp","mo":["(",")"],"mfrac":{"mrow":{"munder":{"mi":"min","mrow":{"msubsup":[{"mi":["T","x","\u2032"]},{"mi":["T","y","\u2032"]}],"mo":","}},"mo":"\u2062","mrow":{"mo":["\uf605","\uf606"],"mrow":{"mrow":[{"mi":"\u03bc","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["T","x"]},{"mi":["T","y"]}],"mo":","}}},{"mi":"\u03bc","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["T","x","\u2032"]},{"mi":["T","y","\u2032"]}],"mo":","}}}],"mo":"-"}}},"msubsup":{"mi":["\u03c3","neigh"],"mn":"2"}}}],"mo":"\u2062"}],"mo":"\u2190"}},{"mrow":{"mo":["(",")"],"mn":"47"}}]}}}}},"Herein, the \u2192 operator may be interpreted as \u201cis assigned\u201d\u2014in other words the expression a\u2192b means that a is assigned the value of b. The term (T\u2032,T\u2032) refers to the neighbors of tile (T,T). In Equation (47), a minimum is iteratively taken of the L2 distance between tile (T,T) and these neighbors, and the neighbor with the lowest value is used. For instance, if tile (T,T) is at an edge or corner where there are less than four immediate neighbors, then only the neighbors present are considered. By exponentiating the negative of the minimum squared L2 distance, the confidence measures of tiles that have no neighbors with a similar estimated motion are down-weighted.","After applying this neighborhood operation to each tile, per-tile flow and confidence estimates can be upsampled to per-pixel flow and confidence estimates. At this point, each pixel could be assigned the flow and confidence of the tile in which that pixel is contained. However, this could result in blocky-looking flow fields that would inherently be inaccurate near object boundaries, where multiple motions are likely present in the same tile. So, in some embodiments, two or more tiles nearby each pixel may be examined (e.g., the tile that the pixel is in as well as neighboring tiles), and each pixel may be assigned to the tile with a motion estimate that best matches the image. That is, for each pixel, the two or more flow estimates are considered, and the matching pixel in the second image that corresponds to each motion is found.","As an example, for a pixel in tile (T,T), tiles above, below, right, and left of tile (T,T) may be considered. Then, the pixel is assigned the flow and confidence that produced the best per-pixel match between the two images. For each pixel, the squared L2 distance with the best-matching pixel in the other image, d(x,y), may be recorded as well. This distance may be used to update pixel's confidence measure.","As a result, a horizontal per-pixel flow field \u0168(x,y) and a vertical per-pixel flow field {tilde over (V)}(x,y), as well as a per-pixel confidence {tilde over (C)}(x,y), may be found. The latter is the per-tile confidence multiplied by a function of that pixel's matching distance, and can be expressed as:",{"@attributes":{"id":"p-0168","num":"0167"},"maths":{"@attributes":{"id":"MATH-US-00021","num":"00021"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mover":{"mi":"C","mo":"~"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}},{"mrow":[{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mo":"-","mfrac":{"msub":{"mi":["d","pix"]},"msubsup":{"mi":["\u03c3","pix"],"mn":"2"}}}}},{"mi":"C","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["T","x"]},{"mi":["T","y"]}],"mo":","}}}],"mo":"\u2062"}],"mo":"\u2190"}},{"mrow":{"mo":["(",")"],"mn":"48"}}]}}}},"br":{},"figref":"FIG. 9"},"Another cue that can be used is symmetry. Assume that two distinct flow fields for a pair of images, one from image A to image B, and the other from image B to image A, are being computed. If the estimated flow from pixel i in image A maps to pixel j in image B, then it may be expected that the estimated flow from pixel j in image B maps back to pixel i in image A. In some embodiments, these \u201cforward\u201d flow estimates may be determined alongside the \u201cbackward\u201d flow estimates for all image pairs in a sequence, so the symmetry assumption can be verified. After computing the per-pixel estimated forward flow field (\u0168,{tilde over (V)}) and the corresponding per-pixel estimated confidence {tilde over (C)}, these values can be compared with the corresponding estimated backward flow field (\u0168\u2032,{tilde over (V)}\u2032). For pixels where these flow fields are not symmetric, the confidence may be lowered accordingly. Formally:",{"@attributes":{"id":"p-0170","num":"0169"},"maths":{"@attributes":{"id":"MATH-US-00022","num":"00022"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mstyle":{"mspace":{"@attributes":{"width":"4.4em","height":"4.4ex"}}},"mo":"\u2062","mrow":{"mrow":[{"mover":{"mi":"C","mo":"~"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}},{"mrow":[{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mo":"-","mfrac":{"msub":{"mi":["d","sym"]},"msubsup":{"mi":["\u03c3","sym"],"mn":"2"}}}}},{"mover":{"mi":"C","mo":"~"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}}],"mo":"\u2062"}],"mo":"\u2190"}}},{"mrow":{"mo":["(",")"],"mn":"49"}}]},{"mtd":[{"mrow":{"mstyle":{"mspace":{"@attributes":{"width":"4.4em","height":"4.4ex"}}},"mo":"\u2062","mi":"Where"}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}]},{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["d","sym"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}},{"msup":[{"mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mover":{"mi":"U","mo":"~"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}},{"msup":{"mover":{"mi":"U","mo":"~"},"mi":"\u2032"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"x","mo":"+","mrow":{"mover":{"mi":"U","mo":"~"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}}},{"mi":"y","mo":"+","mrow":{"mover":{"mi":"V","mo":"~"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}}}],"mo":","}}}],"mo":"+"}},"mn":"2"},{"mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mover":{"mi":"V","mo":"~"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}},{"msup":{"mover":{"mi":"V","mo":"~"},"mi":"\u2032"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"x","mo":"+","mrow":{"mover":{"mi":"U","mo":"~"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}}},{"mi":"y","mo":"+","mrow":{"mover":{"mi":"V","mo":"~"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}}}],"mo":","}}}],"mo":"+"}},"mn":"2"}],"mo":"+"}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"50"}}]}]}}},"br":{}},"The final per-pixel flow field is noisy, but the per-pixel confidence assigned to each pixel is a good indicator of the trustworthiness of that pixel's flow. In the following section an algorithm to smooth out these noisy per-pixel flow estimates into a more reliable flow field is presented.","Given an estimated flow field (\u0168,{tilde over (V)}) and per-pixel weights representing the confidence of that flow field {tilde over (C)}, these values can be represented as one-dimensional vectors \u0169, {tilde over (v)}, and {tilde over (c)}. An improved version of the flow field (\u0169,{tilde over (v)}) and confidence {tilde over (c)} can be found using fast bilateral filtering. In general, bilateral filtering of an image adjusts the values of pixels based on values of similar and nearby pixels in the image. Here, bilateral filtering is used to adjust the estimated flow field of a pixel.","The embodiments herein may use one or more of the following approaches: (1) Both horizontal and vertical flow are found, (2) the data term is represented as a set of univariate normal distributions (means and confidences, where variance in the inverse of confidence), (3) this new data term allows the problem to be rewritten as a system of linear equations that can be solved using a preconditioned conjugate gradient, (4) a Jacobi preconditioner is derived, which allows the solution to converge quickly, and (5) a technique for repeatedly filtering a signal in bilateral space is derived and then used to produce a smoothed confidence measure of the flow field.","First, a global stereo optimization problem may be constructed. In it, a flow field (u,v) for every pixel in the image is found subject to a smoothness term that encourages (u,v) to be smooth, and encourages them to resemble the observed flow (u,v) proportionally to the confidence, {tilde over (c)}, in the observation. Formally, the optimization problem can be stated as finding a solution for the free parameters u and v such that:",{"@attributes":{"id":"p-0175","num":"0174"},"maths":{"@attributes":{"id":"MATH-US-00023","num":"00023"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"munder":{"mi":"min","mrow":{"mi":["u","v"],"mo":","}},"mo":"\u2062","mrow":{"mfrac":{"mi":"\u03bb","mn":"2"},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"mi":["i","j"],"mo":","}},"mo":"\u2062","msup":{"mrow":{"msub":{"mover":{"mi":"W","mo":"^"},"mrow":{"mi":["i","j"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["u","i"]},{"mi":["u","j"]}],"mo":"-"}}},"mn":"2"}}}},{"mfrac":{"mi":"\u03bb","mn":"2"},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"mi":["i","j"],"mo":","}},"mo":"\u2062","msup":{"mrow":{"msub":{"mover":{"mi":"W","mo":"^"},"mrow":{"mi":["i","j"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["v","i"]},{"mi":["v","j"]}],"mo":"-"}}},"mn":"2"}}},{"munder":{"mo":"\u2211","mi":"i"},"mo":"\u2062","msup":{"mrow":{"msub":{"mi":["w","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["v","i"]},{"mover":{"mi":"v","mo":"~"},"mi":"i"}],"mo":"-"}}},"mn":"2"}},{"munder":{"mo":"\u2211","mi":"i"},"mo":"\u2062","msup":{"mrow":{"msub":{"mi":["w","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["u","i"]},{"mover":{"mi":"u","mo":"~"},"mi":"i"}],"mo":"-"}}},"mn":"2"}}],"mo":["+","+","+"]}},{"mrow":{"mo":["(",")"],"mn":"51"}}]}}}}},"This problem decomposes into two independent optimization problems with respect to u and v. For the rest of the description, a solution for p is described, which refers to the per-pixel flow fields u or v respectively. In other words, let {tilde over (p)} refer to \u0169 or {tilde over (v)} respectively, and the optimization problem can be simplified to seek a solution for the free parameter p such that:",{"@attributes":{"id":"p-0177","num":"0176"},"maths":{"@attributes":{"id":"MATH-US-00024","num":"00024"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"munder":{"mi":["min","p"]},"mo":"\u2062","mrow":{"mfrac":{"mi":"\u03bb","mn":"2"},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"mi":["i","j"],"mo":","}},"mo":"\u2062","msup":{"mrow":{"msub":{"mover":{"mi":"W","mo":"^"},"mrow":{"mi":["i","j"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["p","i"]},{"mi":["p","j"]}],"mo":"-"}}},"mn":"2"}}}},{"munder":{"mo":"\u2211","mi":"i"},"mo":"\u2062","msup":{"mrow":{"msub":{"mi":["w","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["p","i"]},{"mover":{"mi":"p","mo":"~"},"mi":"i"}],"mo":"-"}}},"mn":"2"}}],"mo":"+"}},{"mrow":{"mo":["(",")"],"mn":"52"}}]}}}}},"It is convenient to rewrite this optimization problem in matrix\/vector notation:",{"@attributes":{"id":"p-0179","num":"0178"},"maths":{"@attributes":{"id":"MATH-US-00025","num":"00025"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"munder":{"mi":["min","p"]},"mo":["\u2062","\u2062","\u2062","\u2062"],"mi":["\u03bb","p"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msup":{"mi":["p","T"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"I","mo":"-","mover":{"mi":"W","mo":"^"}}}}},{"msup":{"mi":["p","T"]},"mo":["\u2062","\u2062"],"mrow":{"mi":"diag","mo":"\u2061","mrow":{"mo":["(",")"],"mover":{"mi":"c","mo":"~"}}},"mi":"p"},{"mn":"2","mo":["\u2062","\u2062"],"msup":{"mrow":{"mo":["(",")"],"mrow":{"mover":[{"mi":"c","mo":"~"},{"mi":"p","mo":"~"}],"mo":["\u2062","\u2218"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},"mi":"T"},"mi":"p"},{"msup":{"mrow":{"mo":["(",")"],"mrow":{"mover":[{"mi":"c","mo":"~"},{"mi":"p","mo":"~"}],"mo":"\u2218"}},"mi":"T"},"mo":"\u2062","mover":{"mi":"p","mo":"~"}}],"mo":["+","-","+"]}},{"mrow":{"mo":["(",")"],"mn":"53"}}]}}}}},"W is an affinity matrix, where the value of Wreflects how smooth the flow field between pixels i and j is expected to be, and where {tilde over (W)} is a bi-stochastic version of the affinity matrix (i.e., the rows and columns of {tilde over (W)} each sum to 1). A bilateral affinity measure for W can be used, where W is defined as:",{"@attributes":{"id":"p-0181","num":"0180"},"maths":{"@attributes":{"id":"MATH-US-00026","num":"00026"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":"W","mrow":{"mi":["i","j"],"mo":","}},"mo":"=","mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mo":"-","mfrac":{"msup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mrow":[{"mo":["[","]"],"mrow":{"msub":[{"mi":["x","i"]},{"mi":["y","i"]}],"mo":","}},{"mo":["[","]"],"mrow":{"msub":[{"mi":["x","j"]},{"mi":["y","j"]}],"mo":","}}],"mo":"-"}},"mn":"2"},"mrow":{"mn":"2","mo":"\u2062","msubsup":{"mi":"\u03c3","mrow":{"mi":["x","y"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mn":"2"}}}},"mo":"-","mfrac":{"msup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mrow":[{"mo":["[","]"],"mrow":{"msub":[{"mi":["r","i"]},{"mi":["g","i"]},{"mi":["b","i"]}],"mo":[",",","]}},{"mo":["[","]"],"mrow":{"msub":[{"mi":["r","j"]},{"mi":["g","j"]},{"mi":["b","j"]}],"mo":[",",","]}}],"mo":"-"}},"mn":"2"},"mrow":{"mn":"2","mo":"\u2062","msubsup":{"mi":"\u03c3","mrow":{"mi":["r","g","b"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},"mn":"2"}}}}}}}},{"mrow":{"mo":["(",")"],"mn":"54"}}]}}}}},"Where the \u03c3and \u03c3parameters control the spatial and range bandwidths of the filter, respectively.","The matrix W is an edge-preserving filter that blurs along edges, but not across edges, by locally adapting the filter to the image content. Applying a bilateral filter to a signal x to produce a filtered signal y can be thought of as a normalized matrix-vector multiplication:",{"@attributes":{"id":"p-0184","num":"0183"},"maths":{"@attributes":{"id":"MATH-US-00027","num":"00027"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"y","mo":"=","mfrac":{"mrow":[{"mi":["W","x"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mi":"W","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"1"}]}}},{"mrow":{"mo":["(",")"],"mn":"55"}}]}}}}},"The computational efficiency of bilateral filtering can be improved by decomposing the problem in a \u201csplat\/blur\/slice\u201d procedure. First, pixel values are \u201csplatted\u201d onto a small set of vertices in a grid or lattice (this may involve a soft histogramming operation). Then, those vertex values are blurred, and then the filtered values for each pixel are produced via a \u201cslice\u201d (an interpolation) of the blurred vertex values. This splat\/blur\/slice procedure corresponds to a factorization of W:\n\n\u2003\u2003(56)\n\nWhere multiplication by S is the \u201csplat\u201d, multiplication by  is the \u201cblur\u201d, and multiplication by Sis the \u201cslice\u201d.\n","Even though W may be large and dense, by construction S is a short, wide, and sparse matrix, and  is small and sparse.  may be a product of several sparse matrices (though it can be treated as a single matrix for convenience). While the na\u00efve filtering of Equation (55) is often intractably slow, this factorization allows for fast filtering:",{"@attributes":{"id":"p-0187","num":"0186"},"maths":{"@attributes":{"id":"MATH-US-00028","num":"00028"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mfrac":[{"mrow":[{"mi":["W","x"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mi":"W","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"1"}]},{"mrow":[{"msup":{"mi":["S","T"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mover":{"mi":["B","_"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"S","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}}}},{"msup":{"mi":["S","T"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mover":{"mi":["B","_"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"S","mo":"\u2061","mrow":{"mo":["(",")"],"mn":"1"}}}}}}]}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"57"}}]}}}}},"Thus, W can be expressed as a matrix factorization by performing a variable substitution which reformulates the problem in terms of vertices instead of pixels:\n\n\u2003\u2003(58)\n\nWhere q is a small vector of values for each bilateral-space vertex, while p is a larger vector of values for each pixel.\n","The same substitution can be performed to solve a similar optimization problem in bilateral space to recover q*, and from that a per-pixel solution can be produced:\n\n\u2003\u2003(59)\n","The previously discussed matrix factorization was phrased in terms of the affinity matrix W, but it is more convenient to work with the bi-stochastic affinity matrix \u0174, which is why the optimization problem is framed in terms of \u0174. To produce \u0174 from W, the technique shown in Algorithm 1 (see ) may be used. This procedure results in a set of diagonal matrices which can be used throughout the algorithm to perform various operations in bilateral space instead of pixel space.","These bilateral-space diagonal bi-stochastization matrices allow description of \u0174, the bi-stochasticized version of W, in terms of splat\/blur\/slice factorization:\n\n\u2003\u2003(60)\n","Further, these bi-stochastization matrices also allow the pixel-space loss function in Equation (53) to be expressed in bilateral space:",{"@attributes":{"id":"p-0193","num":"0192"},"maths":{"@attributes":{"id":"MATH-US-00029","num":"00029"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":[{"munder":{"mi":["min","q"]},"mo":"\u2062","mrow":{"mfrac":{"mn":["1","2"]},"mo":["\u2062","\u2062","\u2062","\u2062"],"msup":{"mi":["q","T"]},"mi":["A","q"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},{"msup":{"mi":["b","T"]},"mo":"\u2062","mi":"q"}],"mo":["-","+"],"mi":"c"}},{"mrow":{"mo":["(",")"],"mn":"61"}}]},{"mtd":[{"mstyle":{"mtext":"Where:\u00a0\u00a0\u00a0\u00a0"}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}]},{"mtd":[{"mrow":{"mi":"A","mo":"=","mrow":{"mrow":[{"mi":"\u03bb","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["D","m"]},"mo":"-","mrow":{"msub":[{"mi":["D","n"]},{"mi":["D","n"]}],"mo":["\u2062","\u2062"],"mover":{"mi":["B","_"]}}}}},{"mi":"diag","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"S","mo":"\u2062","mover":{"mi":"c","mo":"~"}}}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"62"}}]},{"mtd":[{"mrow":{"mi":"b","mo":"=","mrow":{"mi":"S","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mover":[{"mi":"c","mo":"~"},{"mi":"p","mo":"~"}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"\u2022"}}}}},{"mrow":{"mo":["(",")"],"mn":"63"}}]},{"mtd":[{"mrow":{"mi":"c","mo":"=","mrow":{"mfrac":{"mn":["1","2"]},"mo":["\u2062","\u2062"],"msup":{"mrow":{"mo":["(",")"],"mrow":{"mover":[{"mi":"c","mo":"~"},{"mi":"p","mo":"~"}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"\u2022"}},"mi":"T"},"mover":{"mi":"p","mo":"~"}}}},{"mrow":{"mo":["(",")"],"mn":"64"}}]}]}}}},"Equations (61)-(64) can be solved using preconditioned conjugate gradient descent with a Jacobi preconditioner. To produce a Jacobi preconditioner, the diagonal of the A matrix can be computed, which can be done analytically:\n\ndiag()=\u03bb(diag())+\u2003\u2003(65)\n","This computation can be performed directly, as Dand Dare already diagonal matrices, and  has a constant value along the diagonal. With this, a Jacobi preconditioner can be found by taking the inverse of the values along the diagonal of A:\n\n=diag()\u2003\u2003(66)\n","With this preconditioner, Equations (61)-(64) can be solved efficiently using the preconditioned conjugate gradient method (PCG). PCG usually converges in 10-30 iterations, while non-preconditioned CG can take thousands of iterations to converge. Once Equations (61)-(64) have been optimized in bilateral-space to produce q*, that value can be sliced using Equation (59) to obtain p*=Sq*. As noted above, two distinct optimization problems are solved, one to get a smoothed horizontal flow-field u* and another to get a smoothed vertical flow-field v*.","In addition to producing a smoothed estimate of the flow fields, a smoothed estimate of confidence in the smoothed flow fields may also be derived. Smoothed confidence weights can be obtained by repeatedly applying a bilateral filter to the noisy input weights e. Na\u00efvely, this can be done by repeatedly multiplying the bi-stochastic affinity matrix \u0174 with c, but this is computationally expensive. As an efficient alternative, the confidence map can be repeatedly filtered entirely in bilateral-space.","To that end, the process of repeatedly filtering a signal x with the bi-stochastic affinity matrix W to produce a filtered signal y can be expressed as:\n\n({circumflex over ()}( . . . () . . . ))\u2003\u2003(67)\n","For the case where n=2, this multiplication can be written in terms of the factorization of \u0174:",{"@attributes":{"id":"p-0200","num":"0199"},"maths":{"@attributes":{"id":"MATH-US-00030","num":"00030"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"msup":{"mover":{"mi":"W","mo":"^"},"mn":"2"},"mo":"=","mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msup":{"mi":["S","T"]},"mo":["\u2062","\u2062","\u2062","\u2062"],"msub":[{"mi":["D","r"]},{"mi":["D","r"]}],"mover":{"mi":["B","_"]},"mi":"S"}},{"mo":["(",")"],"mrow":{"msup":{"mi":["S","T"]},"mo":["\u2062","\u2062","\u2062","\u2062"],"msub":[{"mi":["D","r"]},{"mi":["D","r"]}],"mover":{"mi":["B","_"]},"mi":"S"}}],"mo":"\u2062"}}}},{"mtd":{"mrow":{"mo":"=","mrow":{"msup":{"mi":["S","T"]},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"msub":[{"mi":["D","r"]},{"mi":["D","r"]},{"mi":["D","m"]},{"mi":["D","r"]},{"mi":["D","r"]}],"mover":[{"mi":["B","_"]},{"mi":["B","_"]}],"mi":"S"}}}},{"mtd":{"mrow":{"mo":"=","mrow":{"msup":{"mi":["S","T"]},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"msubsup":[{"mi":["D","m"],"mrow":{"mo":"-","mfrac":{"mn":["1","2"]}}},{"mi":["D","m"],"mfrac":{"mn":["1","2"]}},{"mi":["D","m"],"mfrac":{"mn":["1","2"]}},{"mi":["D","m"],"mfrac":{"mn":["1","2"]}},{"mi":["D","m"],"mfrac":{"mn":["1","2"]}},{"mi":["D","m"],"mrow":{"mo":"-","mfrac":{"mn":["1","2"]}}}],"msub":[{"mi":["D","r"]},{"mi":["D","r"]},{"mi":["D","r"]},{"mi":["D","r"]}],"mover":[{"mi":["B","_"]},{"mi":["B","_"]}],"mi":"S"}}}},{"mtd":{"mrow":{"mo":"=","mrow":{"msup":[{"mi":["S","T"]},{"mrow":{"msubsup":{"mi":["D","m"],"mrow":{"mo":"-","mfrac":{"mn":["1","2"]}}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["D","m"],"mfrac":{"mn":["1","2"]}},{"mi":["D","m"],"mfrac":{"mn":["1","2"]}}],"mo":["\u2062","\u2062","\u2062","\u2062"],"msub":[{"mi":["D","r"]},{"mi":["D","r"]}],"mover":{"mi":["B","_"]}}}},"mn":"2"}],"mo":["\u2062","\u2062","\u2062"],"msubsup":{"mi":["D","m"],"mrow":{"mo":"-","mfrac":{"mn":["1","2"]}}},"mi":"S"}}}}]}},{"mrow":{"mo":["(",")"],"mn":"68"}}]}}}}},"This refactorization can be generalized to an arbitrary value of n:\n\n()\u2003\u2003(69)\n","Note that Equations (68)-(69) use some of the diagonal matrices that were computed using Algorithm 1. With this factorization, a signal x can be efficiently filtered with \u0174 n times by multiplying x with this factorized representation of \u0174, as is shown in Algorithm 2 (see ). This repeated-filtering technique can be used to compute a smoothed confidence image c from the noisy input confidence {tilde over (c)}.","The splat\/blur\/slice factorization of the bilateral affinity matrix is the simplified bilateral grid. This data structure is efficient to compute and use, but it has the unfortunately side-effect of introducing \u201cblocky\u201d grid-shaped artifacts in the output flow and confidence images. To ameliorate this, the estimated flow and confidence can be filtered using a domain transform to smooth out these blocky artifacts.","A visualization of the flow fields and confidence estimates before and after this bilateral-space inference procedure are represented in . In the first row of  there are two input images. The second row depicts coarse estimated per-pixel flow and confidence images, computed from the per-tile flow. The third row depicts the smoothed flow and confidence produced by the bilateral-space inference. The last row demonstrates that the per-pixel flow does a good job of matching the two images, by contrasting a warped composite with a na\u00efve composite of these images.","Motion estimation techniques such as those described herein can be applied to entire video sequences of many frames. When processing a complete video, it would be desirable to produce motion estimates for each frame that are not only smooth and consistent within each frame (by using the bilateral-space inference technique) but are also smooth and consistent across frames. This is useful for many applications, as solving for a flow-field for all pairs of images independently in a video sequence can often produce shaking or flickering artifacts in the flow fields, which may in turn produce artifacts in any graphics application that relies on these flow fields.","There are two extensions to the embodiments herein that produce temporally-smooth motion estimates. The first is a computationally efficient modification to the per-frame solution, in which pairs of frames are processed in sequence, and in which the estimated flow from image t\u22121 to image t is used to inform the estimate of the flow from image t to image t+1.","Solving for the flow from image t\u22121 to image t results in a post-processed (smooth) flow field (U,V) and confidence C. Then, the flow from image t to image t+1 is processed. First, a rough flow field (\u0168,{tilde over (V)}) and an associated confidence {tilde over (C)}are estimated. Then, before applying the bilateral inference to this flow field, it may be combined with the post-processed flow from time t.",{"@attributes":{"id":"p-0208","num":"0207"},"maths":{"@attributes":{"id":"MATH-US-00031","num":"00031"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"msub":{"mover":{"mi":"U","mo":"~"},"mrow":{"mi":"t","mo":"+","mn":"1"}},"mo":"\u2190","mfrac":{"mrow":[{"mrow":[{"mrow":{"mo":["(",")"],"mrow":{"mi":"\u03b1","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["C","t"]}}},"mo":"\u2a2f","msub":{"mi":["U","t"]}},{"mrow":{"mo":["(",")"],"mrow":{"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","mi":"\u03b1"}},"mo":"\u2062","msub":{"mover":{"mi":"C","mo":"~"},"mrow":{"mi":"t","mo":"+","mn":"1"}}}},"mo":"\u2a2f","msub":{"mover":{"mi":"U","mo":"~"},"mi":"t"}}],"mo":"+"},{"mrow":[{"mo":["(",")"],"mrow":{"mi":"\u03b1","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["C","t"]}}},{"mo":["(",")"],"mrow":{"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","mi":"\u03b1"}},"mo":"\u2062","msub":{"mover":{"mi":"C","mo":"~"},"mrow":{"mi":"t","mo":"+","mn":"1"}}}}],"mo":"+"}]}}},{"mrow":{"mo":["(",")"],"mn":"70"}}]},{"mtd":[{"mrow":{"msub":{"mover":{"mi":"V","mo":"~"},"mrow":{"mi":"t","mo":"+","mn":"1"}},"mo":"\u2190","mfrac":{"mrow":[{"mrow":[{"mrow":{"mo":["(",")"],"mrow":{"mi":"\u03b1","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["C","t"]}}},"mo":"\u2a2f","msub":{"mi":["V","t"]}},{"mrow":{"mo":["(",")"],"mrow":{"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","mi":"\u03b1"}},"mo":"\u2062","msub":{"mover":{"mi":"C","mo":"~"},"mrow":{"mi":"t","mo":"+","mn":"1"}}}},"mo":"\u2a2f","msub":{"mover":{"mi":"V","mo":"~"},"mi":"t"}}],"mo":"+"},{"mrow":[{"mo":["(",")"],"mrow":{"mi":"\u03b1","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["C","t"]}}},{"mo":["(",")"],"mrow":{"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","mi":"\u03b1"}},"mo":"\u2062","msub":{"mover":{"mi":"C","mo":"~"},"mrow":{"mi":"t","mo":"+","mn":"1"}}}}],"mo":"+"}]}}},{"mrow":{"mo":["(",")"],"mn":"71"}}]},{"mtd":[{"mrow":{"msub":{"mover":{"mi":"C","mo":"~"},"mrow":{"mi":"t","mo":"+","mn":"1"}},"mo":"\u2190","mrow":{"mrow":[{"mo":["(",")"],"mrow":{"mi":"\u03b1","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["C","t"]}}},{"mo":["(",")"],"mrow":{"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","mi":"\u03b1"}},"mo":"\u2062","msub":{"mover":{"mi":"C","mo":"~"},"mrow":{"mi":"t","mo":"+","mn":"1"}}}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"72"}}]}]}}}},"In Equations (70)-(72), \u03b1 is a scalar that controls the relative performance of the last frame's flow versus the current\u2032 frame's flow, and can be tuned accordingly (in some embodiments \u03b1=0.5, but \u03b1 can take on other values, e.g., in the range of 0.5 to 0.9). This process is an inverse-variance weighting, where the confidences C are treated as inverse-variance.","This technique has many benefits. The model of temporal smoothness is causal, in that the estimate of the motion at time t+1 is only a function of the observed motions at all times <t+1. This model also has an infinite support, in that the motion estimated for time t+1 considers the motions for all times <t+1, which makes the estimated motion very smooth and stable in time. This technique is also simple to implement, computationally inexpensive, and memory efficient, as all frames are processed independently in order and the only communication across frames t+1 and t is just the last estimated motion and confidence at time t. In some cases, when processing a sequence of frames, the first few frames of the sequence may have unstable, \u201cshaky\u201d motion estimates because the causal nature of this filter means that early frames have no prior motion estimates to stabilize themselves. So, when using this technique, the first few frames of a sequence may be discarded.","A second technique for temporally-consistent motion estimation of a video sequence may also be used. Instead of solving for the motion of each frame independently in a serial order, this technique solves for all motions of all frames in one joint optimization problem. The approach is similar to the bilateral inference technique discussed above. To review, the single-frame technique optimizes over a bilateral-space representation of a single image, and performs inference in a sparse five-dimensional representation\u2014each pixel's color (red, green, blue) and spatial location (x-coordinate, y-coordinate). In this grid representation, each vertex is connected to its adjacent vertices, which is what induces smoothness in the estimated depth. To generalize from a single image to a video, each frame is augmented with an additional dimension t, such that each pixel exists in a 6-dimensional space in which pixels are not only connected to nearby pixels with similar colors, but are also connected to pixels in adjacent frames in time with similar colors and positions. The same or a similar inference procedure that was detailed previously can be performed to produce a single motion estimate for a sequence of frames or an entire video.","This joint approach has several benefits over the per-frame approach. The motion estimates are generally more stable, as the inference procedure is able to look at all frames simultaneously when estimating motion, while the per-frame approach only considers a single image at a time. The joint approach is also computationally efficient, as solving for all frames jointly avoids much of the redundant computation that is performed when processing all frames individually.",{"@attributes":{"id":"p-0213","num":"0212"},"figref":["FIG. 13","FIG. 13","FIG. 13","FIGS. 7 and 8"],"b":"100"},"Block  of  may involve determining a per-tile motion estimate between a first m\u00d7n pixel tile from a first captured image of a scene and a second m\u00d7n pixel tile from a second captured image of the scene. The per-tile motion estimate may represent movement of the first m\u00d7n pixel tile to approximate the second m\u00d7n pixel tile. The per-tile motion estimate may be based on a motion vector and a precision matrix of the motion vector.","Block  may involve, possibly based on the approximation of the first m\u00d7n pixel tile to the second m\u00d7n pixel tile, estimating a per-tile confidence of the per-tile motion estimate. Estimating the per-tile confidence of the per-tile motion estimate may involve (i) measuring similarities between the per-tile motion estimate and respective per-tile motion estimates of two or more tiles that are adjacent, in the first captured image, to the first m\u00d7n pixel tile, and (ii) adjusting the estimated the per-tile confidence based on the measured similarities. The measured similarities may be based on respective exponentiated L2 distances between the per-tile motion estimate and the respective per-tile motion estimates of the two or more tiles that are adjacent, in the first captured image, to the first m\u00d7n pixel tile.","Block  may involve upsampling the per-tile motion estimate and the per-tile confidence to obtain respective per-pixel motion estimates and associated per-pixel confidences for pixels of the first m\u00d7n pixel tile.","Block  may involve iteratively filtering the respective per-pixel motion estimates and associated per-pixel confidences. The iterative filtering may involve multiplying the respective per-pixel motion estimates and associated per-pixel confidences by an affinity matrix. The iterative filtering may also smooth the respective per-pixel motion estimates and associated per-pixel confidences.","In some embodiments, the affinity matrix may represent respective L2 distances between locations and color channel pixels in the first captured image and pixels in the second captured image. Alternatively or additionally, the affinity matrix may be a bi-stochastic matrix.","In some embodiments, a particular per-pixel motion estimate of a particular pixel in the first captured image may be based on per-tile motion estimates of at least two tiles in the first captured image. The particular per-pixel motion estimate may be assigned such that the particular pixel is matched with a target pixel in the second captured image, wherein the target pixel is not located in the second m\u00d7n pixel tile.","In some embodiments, one or more per-pixel confidences may be adjusted. For instance, it may be determined that a first per-pixel motion estimate of a first pixel in the first captured image indicates that the first pixel is matched to a second pixel in the second captured image. A second per-pixel motion estimate of the second pixel that matches the second pixel to a third pixel in the first captured image may be obtained. It may be determined that the first pixel is different from the third pixel. Possibly based on the first pixel being different from the third pixel, a per-pixel confidence of the first per-pixel motion estimate may be lowered.","In some embodiments, the first captured image, the second captured image, and a third captured image are sequential frames of a sequence of video frames. A second per-tile motion estimate between a third m\u00d7n pixel tile from the second captured image and a fourth m\u00d7n pixel tile from the third captured image may be determined. The second per-tile motion estimate may represent movement of the third m\u00d7n pixel tile to approximate the fourth m\u00d7n pixel tile. Possibly based on the approximation of the third m\u00d7n pixel tile to the fourth m\u00d7n pixel tile, a second per-tile confidence of the second per-tile motion estimate may be obtained. The second per-tile motion estimate and the second per-tile confidence may be upsampled to obtain respective second per-pixel motion estimates and associated second per-pixel confidences for pixels of the third m\u00d7n pixel tile. The respective second per-pixel motion estimates and associated second per-pixel confidences may also based on the respective smoothed per-pixel motion estimates and associated smoothed per-pixel confidences for pixels of the first m\u00d7n pixel tile.","In some embodiments, the first captured image and the second captured image are sequential frames of a sequence of video frames. Each pixel of the first captured image may be represented in 6-dimensional space. The 6 dimensions may include x and y locations of the respective pixels, 3 color channel values of the respective pixels, and a frame number of the respective pixels. The upsampling and the iterative filtering may consider per-pixel motion estimates and associated per-pixel confidences from each frame in the sequence of video frames.","The present disclosure is not to be limited in terms of the particular embodiments described in this application, which are intended as illustrations of various aspects. Many modifications and variations can be made without departing from its scope, as will be apparent to those skilled in the art. Functionally equivalent methods and apparatuses within the scope of the disclosure, in addition to those enumerated herein, will be apparent to those skilled in the art from the foregoing descriptions. Such modifications and variations are intended to fall within the scope of the appended claims.","The above detailed description describes various features and functions of the disclosed systems, devices, and methods with reference to the accompanying figures. The example embodiments described herein and in the figures are not meant to be limiting. Other embodiments can be utilized, and other changes can be made, without departing from the scope of the subject matter presented herein. It will be readily understood that the aspects of the present disclosure, as generally described herein, and illustrated in the figures, can be arranged, substituted, combined, separated, and designed in a wide variety of different configurations, all of which are explicitly contemplated herein.","With respect to any or all of the message flow diagrams, scenarios, and flow charts in the figures and as discussed herein, each step, block, and\/or communication can represent a processing of information and\/or a transmission of information in accordance with example embodiments. Alternative embodiments are included within the scope of these example embodiments. In these alternative embodiments, for example, functions described as steps, blocks, transmissions, communications, requests, responses, and\/or messages can be executed out of order from that shown or discussed, including substantially concurrent or in reverse order, depending on the functionality involved. Further, more or fewer blocks and\/or functions can be used with any of the ladder diagrams, scenarios, and flow charts discussed herein, and these ladder diagrams, scenarios, and flow charts can be combined with one another, in part or in whole.","A step or block that represents a processing of information can correspond to circuitry that can be configured to perform the specific logical functions of a herein-described method or technique. Alternatively or additionally, a step or block that represents a processing of information can correspond to a module, a segment, or a portion of program code (including related data). The program code can include one or more instructions executable by a processor for implementing specific logical functions or actions in the method or technique. The program code and\/or related data can be stored on any type of computer readable medium such as a storage device including a disk, hard drive, or other storage medium.","The computer readable medium can also include non-transitory computer readable media such as computer-readable media that store data for short periods of time like register memory, processor cache, and random access memory (RAM). The computer readable media can also include non-transitory computer readable media that store program code and\/or data for longer periods of time. Thus, the computer readable media may include secondary or persistent long term storage, like read only memory (ROM), optical or magnetic disks, compact-disc read only memory (CD-ROM), for example. The computer readable media can also be any other volatile or non-volatile storage systems. A computer readable medium can be considered a computer readable storage medium, for example, or a tangible storage device.","Moreover, a step or block that represents one or more information transmissions can correspond to information transmissions between software and\/or hardware modules in the same physical device. However, other information transmissions can be between software modules and\/or hardware modules in different physical devices.","The particular arrangements shown in the figures should not be viewed as limiting. It should be understood that other embodiments can include more or less of each element shown in a given figure. Further, some of the illustrated elements can be combined or omitted. Yet further, an example embodiment can include elements that are not illustrated in the figures.","Additionally, any enumeration of elements, blocks, or steps in this specification or the claims is for purposes of clarity. Thus, such enumeration should not be interpreted to require or imply that these elements, blocks, or steps adhere to a particular arrangement or are carried out in a particular order.","While various aspects and embodiments have been disclosed herein, other aspects and embodiments will be apparent to those skilled in the art. The various aspects and embodiments disclosed herein are for purposes of illustration and are not intended to be limiting, with the true scope being indicated by the following claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 13"}]},"DETDESC":[{},{}]}
