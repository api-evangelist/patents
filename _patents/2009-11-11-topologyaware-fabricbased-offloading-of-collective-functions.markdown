---
title: Topology-aware fabric-based offloading of collective functions
abstract: A computing method includes accepting a notification of a computing task for execution by a group of compute nodes interconnected by a communication network, which has a given interconnection topology and includes network switching elements. A set of preferred paths, which connect the compute nodes in the group via at least a subset of the network switching elements to one or more root switching elements, are identified in the communication network based on the given interconnection topology and on a criterion derived from the computing task. The network switching elements in the subset are configured to forward node-level results of the computing task produced by the compute nodes in the group to the root switching elements over the preferred paths, so as to cause the root switching elements to calculate and output an end result of the computing task based on the node-level results.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09110860&OS=09110860&RS=09110860
owner: MELLANOX TECHNOLOGIES TLV LTD.
number: 09110860
owner_city: Ra'Anana
owner_country: IL
publication_date: 20091111
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF EMBODIMENTS","Overview","System Description","Fabric-Based Offloading"],"p":["The present invention relates generally to distributed computing, and particularly to methods and systems for performing collective computing operations in distributed computer networks.","Large and complex computing tasks are sometimes carried out by distributed computer systems, which comprise multiple interconnected compute nodes. Parallelizing a computing task among multiple compute nodes helps to reduce task execution time, and enables execution of large computing tasks in reasonable run times.","Some known communication protocols and Application Programming Interfaces (APIs) have been designed particularly to facilitate task execution over multiple distributed compute nodes. An example protocol is the Message Passing Interface (MPI), which is specified by the MPI forum in \u201cMPI: A Message-Passing Interface Standard,\u201d version 2.2, Sep. 4, 2009, which is incorporated herein by reference. In particular, chapter 5 of the MPI standard specifies collective communication functions involving a group or groups of software processes. Several techniques for performing collective functions are known in the art. Example methods are described in U.S. Patent Application Publication 2009\/0063816, whose disclosure is incorporated herein by reference.","Coti et al. propose a programming method and a modification of the MPI runtime environment that take Wide Area Network (WAN) topology into consideration, in \u201cMPI Applications on Grids: a Topology Aware Approach,\u201d Proceedings of the 15International European Conference on Parallel and Distributed Computing (EuroPar'09), Delft, the Netherlands, August 2009, which is incorporated herein by reference.","Petrini et al. describe a network interconnection scheme called the Quadrics Network (QsNet), in \u201cThe Quadrics Network (QsNet): High-Performance Clustering Technology,\u201d Proceedings of the 9IEEE Symposium on Hot Interconnects (HotI'01), Palo Alto, Calif., August 2001, which is incorporated herein by reference. QsNet integrates a programmable processor in the network interface, which allows the implementation of intelligent communication protocols.","Some known techniques attempt to offload the processing of collective functions from the compute nodes. For example, Sancho et al., describe a method that dedicates some compute node processors in the system to perform only collective functions, in \u201cEfficient Offloading of Collective Communications in Large-Scale Systems,\u201d Proceedings of the 2007 IEEE International Conference on Cluster Computing, Austin, Tex., Sep. 17-20, 2007, which is incorporated herein by reference. As another example, U.S. Patent Application Publication 2005\/0097300, whose disclosure is incorporated herein by reference, describes a dedicated hardware-based collective offload engine that provides collective processing of data from processing nodes in a distributed computing environment. The dedicated collective offload hardware engine is coupled to a switch fabric. A result is produced by the collective offload hardware engine based on the collective processing of the data, and is forwarded to at least one processing node. Collective processing is facilitated by communication among a plurality of dedicated collective offload engines via the switch fabric or via a private channel disposed between the collective offload engines.","An embodiment of the present invention provides a computing method, including:","accepting a notification of a computing task for execution by a group of compute nodes interconnected by a communication network, which has a given interconnection topology and includes network switching elements;","identifying, based on the given interconnection topology and on a criterion derived from the computing task, a set of preferred paths in the communication network that connect the compute nodes in the group via at least a subset of the network switching elements to one or more root switching elements; and","configuring the network switching elements in the subset to forward node-level results of the computing task produced by the compute nodes in the group to the root switching elements over the preferred paths, so as to cause the root switching elements to calculate and output an end result of the computing task based on the node-level results.","In some embodiments, determining the preferred paths includes selecting one or more of the network switching elements to serve as the root switching elements. In an embodiment, determining the preferred paths includes selecting two or more root switching elements, and the method includes configuring a given compute node to receive two or more end results from the respective root switching elements and to reduce the two or more end results to a single end result. In another embodiment, configuring the network switching elements includes causing a given network switching element in the subset to reduce a respective portion of the node-level results forwarded thereto to a single interim result, and to forward the interim result to one of the root switching elements.","In a disclosed embodiment, configuring the network switching elements includes notifying a Central Processing Unit (CPU) in a given network switching element in the subset of an identity of another switching element to which the node-level results are to be forwarded, so as to cause the CPU to configure a switching fabric in the given network switching element to forward the node-level results to the other switching element. In another embodiment, the method includes configuring a given compute node in the group to reduce multiple results of the computing task produced in the given compute node, so as to produce a single node-level result, and to forward the single node-level result to one of the root switching elements.","Configuring the network switching elements may include configuring the root switching elements to forward the end result to a specified compute node. Alternatively, configuring the network switching elements may include configuring the root switching elements to forward the end result to all the compute nodes in the group. In some embodiments, the method includes detecting a change in the interconnection topology, and modifying the set of the preferred paths responsively to the detected change. In an embodiment, the computing task includes a Message Passing Interface (MPI) function.","There is additionally provided, in accordance with an embodiment of the present invention, a computing method, including:","for a computing task that is assigned for execution by a group of compute nodes, receiving multiple node-level results of the computing task from the compute nodes by two or more network switching elements that are different from the compute nodes assigned to execute the computing task;","reducing a respective portion of the node-level results received in each network switching element to produce a respective single interim result, and forwarding the interim results to one or more root switching elements; and","computing and outputting an end result of the computing task by the root switching elements.","In some embodiments, the computing task specifies an operation to be performed by each of the compute nodes in the group on respective local data, and reducing the node-level results includes performing the specified operation on the respective portion of the node-level results by each of the switching elements, so as to produce the interim results.","There is also provided, in accordance with an embodiment of the present invention, a computing apparatus, including:","an interface coupled to communicate with a communication network, which includes network switching elements that interconnect compute nodes according to a given interconnection topology; and","a processor, which is configured to accept a notification of a computing task for execution by a group of the compute nodes, to identify, based on the given interconnection topology and on a criterion derived from the computing task, a set of preferred paths in the communication network that connect the compute nodes in the group via at least a subset of the network switching elements to one or more root switching elements, and to configure the network switching elements in the subset to forward node-level results of the computing task produced by the compute nodes in the group to the root switching elements over the preferred paths, so as to cause the root switching elements to calculate and output an end result of the computing task based on the node-level results.","There is further provided, in accordance with an embodiment of the present invention, a computing system including two or more network switching elements, which are configured to receive multiple node-level results of a computing task that is assigned for execution by a group of compute nodes different from the switches, to reduce a respective portion of the node-level results received in each network switching element to produce a respective single interim result, and to forward the interim results to one or more root switching elements so as to cause the root switching elements to compute and output an end result of the computing task.","The present invention will be more fully understood from the following detailed description of the embodiments thereof, taken together with the drawings in which:","Collective functions comprise various computation and communication functions that are carried out collectively by multiple software processes, typically running on multiple compute nodes. Reduction functions are a specific type of collective functions, which collect data from multiple processes and process the data to produce an end result (\u201creduce the data to an end result\u201d). For example, the MPI specification defines an MPI_REDUCE function, which collects data from a group of processes, performs a certain operation (e.g., sum, maximum or minimum) on the data, and sends the end result to one of the processes. An MPI_ALLREDUCE function is similar to MPI_REDUCE, but sends the end result to all the processes in the group. In many distributed computing applications, and particularly in large scale applications having large numbers of processes and nodes, collective functions account for a significant portion of the overall application run time.","Embodiments of the present invention that are described hereinbelow provide improved methods and systems for performing collective functions (e.g., MPI_REDUCE and MPI_ALLREDUCE) in a distributed computing system. Such systems comprise multiple compute nodes that are interconnected by a communication network. The network comprises one or more switching elements (e.g., network switches) and has a certain interconnection topology. The methods and systems described herein use the switching elements to offload most of the processing and communication associated with collective functions from the compute nodes.","In some embodiments, offloading is controlled by a server, which is referred to as an offload manager. The offload manager is notified of a computing task that is assigned for execution by a designated group of compute nodes. The offload manager selects, based on the interconnection topology of the communication network, a preferred set of paths in the network for collecting interim results of the computing task from the compute nodes. The selected set of paths (referred to as a collection tree) connects the compute nodes in the group to a given switching element (referred to as the root of the collection tree). The offload manager may apply various criteria for optimal selection of the collection tree based on the interconnection topology of the network. In particular, the offload manager selects the switching element that will serve as the root.","The offload manager configures the switching elements and the compute nodes to forward the interim results from the nodes toward the root switching element. Typically, each switching element is also configured to reduce the interim results it receives from the compute nodes to a single interim result, and to forward the single result toward the root node. Using this configuration, the switching elements progressively reduce the interim results as they flow toward the root, thus reducing the traffic volume.","Once the switching elements are configured, the system executes the computing task in question. Each compute node in the group produces a node-level result based on its local data, and forwards the node-level result to one of the switching elements. Each switching element reduces the node-level results it receives to a single interim result, and forwards this result up the collection tree toward the root, as configured by the offload manager. The root switching element reduces the interim results it receives to produce an end result of the computing task. In some embodiments (e.g., in an MPI_REDUCE function) the end result is forwarded to a specific process in the group. In alternative embodiments (e.g., in an MPI_ALLREDUCE function) the end result is forwarded to all the processes in the group.","Since the methods and systems described herein are implemented mainly in the switching elements, they offload most of the processing and networking associated with collective functions from the compute nodes. This sort of offloading reduces the execution time of collective functions, and also improves the performance of the computing system's application layer. Since most of the collective function processing is offloaded to the switching elements, the disclosed techniques enable collective functions to be implemented in a non-blocking manner (i.e., in a manner that minimizes application blockage during execution of collective functions). As a result, application performance is further improved. Since each switching element reduces the interim results it receives before forwarding, the likelihood of congestion is reduced. Since offloading is based on the actual interconnection topology of the network, the run time of collective functions can be reduced considerably while using network resources efficiently.",{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 1","b":["20","20","24","28","24","28","28","28"]},"Network  comprises one or more network switching elements , which forward traffic to and from nodes . In the present example, network  comprises four switches denoted A . . . D. Each switch comprises switching fabric  and a Central Processing Unit (CPU) . The CPUs are closely integrated with the switching elements. Each switch comprises multiple ports for connecting to nodes  and\/or to other switches. Switching fabric  may be implemented, for example, in one or more Application-Specific Integrated Circuits (ASIC) or Field-Programmable Gate Arrays (FPGA).","System  comprises a job scheduler , which schedules computing jobs for execution by the compute nodes. The embodiments described herein refer mainly to computing tasks that are specified using MPI (\u201cMPI jobs\u201d). Alternatively, however, computing tasks can be specified using any other suitable protocol or language. In particular, some of the computing tasks scheduled by scheduler  comprise collective functions such as MPI_REDUCE and MPI_ALLREDUCE. Typically, scheduler  designates a certain group of compute nodes  to execute each given computing task.","System  comprises an offload manager , typically a server that is connected to network . For each computing task scheduled by job scheduler , offload manager  configures at least some of switches , as well as some of nodes , to perform the computing task efficiently. In particular, when the computing task comprises a collective function such as MPI_REDUCE and MPI_ALLREDUCE, the offload manager configures switches  to carry out some of the processing associated with the collective function execution, thereby offloading the compute nodes. These offloading techniques are explained in greater detail below. Offload manager  comprises an interface  for connecting to network , and a processor  that carries out the configuration methods described herein.","The system configuration shown in  is an example configuration, which is chosen purely for the sake of conceptual clarity. In alternative embodiments, any other suitable configuration can also be used. For example, in some embodiments the offload manager and the job scheduler can be integrated in a single computer platform, e.g., in a single server.","In one embodiment, each switch  comprises an Infiniband switch, which is fabricated either in a single ASIC, or on a line card that plugs into a backplane so as to create a switching fabric in a single chassis. In this example embodiment, the switch has eighteen ports for connecting to compute nodes, e.g., using Quad Small Form-factor Pluggable (QSFP) connectors. The switch may comprise eighteen ports for connecting to other switches in the network. The switch provides non-blocking, any-port-to-any-port switching capability. Any other suitable switch configuration can also be used. In an example embodiment, fabric  comprises an InfiniScale\u00ae IV device, produced by Mellanox\u00ae Technologies (Sunnyvale, Calif.). Alternatively, any other suitable devices and protocols (e.g., Gigabit Ethernet switching) can also be used. CPU  is tightly integrated to the switching element, and may communicate with switching fabric  using a Peripheral Component Interconnect (PCI), a Peripheral Component Interconnect Express (PCI-Exp) connection, or using any other suitable protocol.","Typically, CPUs  and processor  comprise general-purpose processors, which are programmed in software to carry out the functions described herein. The software may be downloaded to the processors in electronic form, over a network, for example, or it may, alternatively or additionally, be provided and\/or stored on tangible media, such as magnetic, optical, or electronic memory. CPUs  may comprise, for example, microprocessors supporting the x86 architecture, 64-bit x86-64 microprocessors, Itanium processors (also referred to as IA64), PowerPC (PPC) 32-bit or 64-bit microprocessors, or any other suitable processor type. CPUs  may run any suitable operating system, such a Linux. The software running on CPUs  may be written in any suitable language, such as C or C++.","The embodiments described herein refer mainly to collective functions that are specified as part of MPI jobs. A given MPI job typically specifies a group of processes, which run on one or more compute nodes. MPI defines \u201ccommunicators,\u201d which comprise logical groups of processes that interact with one another. A given MPI job typically specifies a default communicator, which comprises the entire group of processes assigned to that job. Within a given job, MPI may define one or more sub-communicators, which comprise partial subsets of the processes, for carrying out specific tasks. Once a certain communicator is defined, it can be used to perform collective functions. The description that follows refers to collective functions, which are performed using MPI communicators specified in MPI jobs. The disclosed methods and systems, however, are not limited to MPI applications, and can be used to perform any other type of collective functions specified in any other suitable manner.","Consider a given MPI application job, which is scheduled by scheduler  for execution by specific MPI processes running on a group of two or more compute nodes in system . The MPI job specifies one or more collective functions to be performed by the processes running on the nodes in the group. Some collective functions specify an operation (e.g., summation, maximum, minimum, product, or logical or bit-wise AND, OR or XOR) to be performed on data that is accessible to the nodes in the group. Other collective functions (e.g., MPI_ALLTOALL, MPI_GATHER and MPI_SCATTER) do not perform arithmetic operations, but rather synchronize the processes or exchange data among the processes.","In some embodiments, offload manager  configures one or more of the switches of network , and the compute nodes in the group, to perform the collective functions efficiently.",{"@attributes":{"id":"p-0044","num":"0043"},"figref":["FIG. 2","FIG. 3"]},"Note that the functionality of network switches  is different and distinct from that of compute nodes . Nodes  are assigned by job scheduler  to run MPI processes that perform collective functions, and hold (or have access to) local data on which they perform the operations specified in the collective functions. Switches , on the other hand, forward network traffic in network . Switches  are not designated by the job scheduler to perform collective functions. (In MPI applications, for example, the switches are not specified in the MPI functions.) The switches offload some of the processing associated with the collective function (e.g., reduction of node-level results) from the compute nodes, and also reduce communication volume. The switches do not operate on local data, but rather on node-level or interim results that are forwarded from the compute nodes or from other switches.",{"@attributes":{"id":"p-0046","num":"0045"},"figref":"FIG. 2","b":["20","48","44","60","24","44"]},"In response to the notification, offload manager  selects a preferred set of paths through network  for collecting interim results of the communicator from the compute nodes in the group, at a tree selection step . The paths connect the compute nodes in the group to one of the switches, which is assigned to produce the end result of the collective function. Each path in the set traverses one or more of the network switches. The set of paths is referred to as a collection tree, and the switch that is selected to produce the end result is referred to as the root of the tree. In some embodiments, the end result is produced by multiple switches\u2014these embodiments are described in detail further below.","Offload manager  selects the optimal collection tree based on the interconnection topology of network , and in particular the connectivity among the compute nodes in the group designated to perform the collective function in question. The offload manager may use various policies and criteria for selecting the optimal collection tree based on the network topology, and\/or the packet handling and\/or computing power capabilities of different system components. Typically, the selection criteria are derived from the collective function to be performed.","For example, the offload manager may identify the tree that spans the compute nodes in the group with the minimal number of network hops, given the physical connectivity of the nodes and switches. As another example, the offload manager may select the collection tree based on the communication capabilities (e.g., bandwidth or latency) of the network links. The offload manager may consider the current utilization of specific network links by different communicators, so as to balance the load over available links and reduce link buffering utilization. The offload manager may identify situations in which certain CPUs  lack messaging or reduction capabilities, and select the collection tree in a way that does not utilize these CPUs.","In some embodiments, the offload manager may select the optimal tree based on characteristics of the switches and compute nodes. Such characteristics may comprise, for example, the load on CPUs  by the communicator, and\/or the computation or communication performance of the switches and compute nodes. The offload manager may consider the capabilities of CPUs  to perform data reduction and receive messages. For example, in some scenarios the offload manager may conclude that it is preferable to perform a certain reduction operation in the compute nodes rather than in the CPUs of the switches. As yet another example, the offload manager may select the optimal tree in a way that aggregates messages, minimizes communication traffic and reduces fabric congestion. Additionally or alternatively, the offload manager may apply any other suitable criteria for selecting the optimal collection tree.","Offload manager  configures one or more of the network switches, and the compute nodes in the group, to forward interim results of the collective function according to the selected collection tree, at a configuration step . Typically, the offload manager configures each compute node (and in particular each process) in the group with the identity of a given switch to which the interim result of the collective function is to be forwarded.","For a given switch that lies on a path belonging to the collection tree, the offload manager configures the switch to forward interim results of the collective function (received from compute nodes in the group either directly or via one or more other switches) to a given switch that is \u201chigher\u201d along the collection tree. In other words, each switch is notified of its parent switch in the tree, and of its lower-level tree elements (compute nodes or switches).","The offload manager configures the selected root switch to reduce the interim result it receives to a single end result. If the collective function specifies that the end result is sent to a single target process (e.g., in an MPI_REDUCE function), the offload manager configures the root switch to forward the end result to the node running that process. If the collective function specifies that the end result is sent to all the nodes (e.g., in an MPI_ALLREDUCE function), the offload manager configures the root switch to forward the end result to all the nodes in the group (e.g., using multicast).","Typically, the configuration in each switch  is managed by CPU , which in turn controls fabric  accordingly. Offload manager  configures a given switch by communicating with CPU  of that switch. The CPU then stores the relevant configuration information (e.g., communicator identities of parents and lower-level elements in the tree) and configures fabric  using this information. A given switching fabric  is configured to pass traffic to the respective CPU  when this CPU is configured in the collective messaging as the address.",{"@attributes":{"id":"p-0055","num":"0054"},"figref":["FIG. 3","FIG. 3","FIG. 2"],"b":["20","40"]},"The method begins with the software processes, which run on the compute nodes in the group, producing interim results of the collective function, at a process execution step . Each compute node in the group reduces the interim results produced by its local processes to a single node-level result, at a node-level reduction step . The actual reduction operation depends on the type of collective function. For example, for a summation operation, the node-level result comprises a sum of the interim results produced by the processes of the node. For a collective function that finds the maximum value of the data, the node-level result comprises the maximal value of the interim results produced by the processes of the node. For a compute node that runs only a single process, the node-level result is equal to the interim result produced by the single process. For collective operations that do not involve reduction operations but rather data exchange between processes (e.g., MPI_ALLTOALL, MPI_GATHER and MPI_SCATTER), the switches along the collection tree reduce the number of exchanged messages, synchronize message flow and thus reduce fabric congestion.","The compute nodes forward their respective node-level results along the optimal collection tree, at a node-level forwarding step . Each compute node  in the group forwards its node-level result to the appropriate network switch, as configured by the offload manager at step  of the method of  above.","The network switches belonging to the collection tree reduce the interim results they receive, at a switch-level reduction step . In each switch , CPU  reduces the interim results received at the switch (directly from nodes  or from other switches) to a single interim result, also referred to as a switch-level result. The actual reduction operation depends on the type of collective function, as explained above with regards to the node-level reduction operation.","The switches belonging to the collection tree forward their respective switch-level results toward the root switch, at a switch-level forwarding step . Each switch forwards its switch-level result to a given switch (which may be the root switch itself or another switch en-route to the root switch along the collection tree), as configured by the offload manager at step  of the method of  above.","CPU  of the root switch receives the interim results and reduces them to an end result of the collective function, at an end result calculation step . For example, when the collective function comprises a summation operation, the CPU of the root switch sums the interim results. When the collective function comprises a minimum operation, the CPU of the root switch finds the minimal value among the interim results. The root switch may receive the interim results from the switches along the collection tree and\/or from compute nodes in the group that are connected locally to its ports.","The root switch outputs the end result, at an output step . For example, if the collective function comprises an MPI_REDUCE function, the root switch sends the end result to the designated target node. If the collective function comprises an MPI_ALLREDUCE function, the root switch sends the end result to all the nodes in the group. Sending the end result to multiple nodes can be performed, for example, by sending a multicast message.","As can be seen in , the interim results of the collective function are progressively reduced by the compute nodes and switches as they flow up the collection tree to the root. This technique optimizes the use of network resources and reduces the likelihood of network congestion.","In some embodiments, when selecting the collection tree for a given communicator, offload manager  defines two or more root switches instead of a single root switch. In these embodiments, each of the root switches calculates a respective end result of the collective function, and forwards the end result to the appropriate target process or processes (e.g., using multicast). Each target process receives multiple end results\u2014one end result from each root switch, and performs a final reduction to produce a single end result of the collective function.","Typically, offload manager  holds a topology map, which defines the interconnection topology of network . The offload manager uses this topology map in determining optimal collection trees for collective functions. In some embodiments, the offload manager accepts a definition of the interconnection topology of the network, e.g., from an operator. In alternative embodiments, the offload manager discovers and learns the interconnection topology of network  automatically, during system operation. In particular, the offload manager may identify topology changes that occur in the network, for example as a result of equipment failures, network re-configuration or upgrades. Upon detecting a topology change, the offload manager updates its topology map, so that collection tree selection will be based on the up-to-date network topology.","The description above addresses a single collective communicator assigned to a specific group of compute nodes. Typically, however, system  handles multiple collective communicators concurrently, each communicator assigned to a respective group of compute nodes. Groups of compute nodes that are assigned to execute different collective functions may overlap. In other words, a given compute node may belong to two or more groups that are assigned to execute multiple different communicators and collective functions. Offload manager  typically handles multiple collective communicators concurrently\u2014e.g., identifies optimal collection trees and configures switches and compute nodes for multiple collective functions. Typically, offload manager  is aware of all the communicators that are being configured to operate at a given time, and thus optimizes the utilization of the network and switch CPUs in an optimal and balanced way.","Although the embodiments described herein mainly address efficient offloading of collective reduction functions and to offloading of data exchange messaging, the methods and systems described herein can also be used in other applications, such as in messaging for financial algorithm trading that involve group messaging or in other reduction and group communication systems.","It will thus be appreciated that the embodiments described above are cited by way of example, and that the present invention is not limited to what has been particularly shown and described hereinabove. Rather, the scope of the present invention includes both combinations and sub-combinations of the various features described hereinabove, as well as variations and modifications thereof which would occur to persons skilled in the art upon reading the foregoing description and which are not disclosed in the prior art."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":["FIG. 3","FIG. 2"]}]},"DETDESC":[{},{}]}
