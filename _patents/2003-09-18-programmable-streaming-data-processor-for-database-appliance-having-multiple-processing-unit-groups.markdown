---
title: Programmable streaming data processor for database appliance having multiple processing unit groups
abstract: A data processing system having two or more groups of data processors that have attributes that are optimized for their assigned functions. A first group consists of one or more host computers responsible for interfacing with applications and/or end users to obtain queries and for planning query execution. A second processor group consists of many streaming record-oriented processors called Job Processing Units (JPUs), preferably arranged as an MPP structure. The JPUs typically carry out the bulk of the data processing required to implement the logic of a query. Each of the JPUs typically include a general purpose microcomputer, local memory, one or more mass storage devices, and one or more network connections. Each JPU also has a special purpose programmable processor, referred to herein as a Programmable Streaming Data Processor (PSDP). The PSDP serves as an interface between the CPU of a JPU and the mass storage device, to offload functions from the CPU of the JPU.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07577667&OS=07577667&RS=07577667
owner: Netezza Corporation
number: 07577667
owner_city: Marlborough
owner_country: US
publication_date: 20030918
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["RELATED APPLICATIONS","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","Brief Description of a Preferred Embodiment","Discussion of Advantages","DETAILED DESCRIPTION OF A PREFERRED EMBODIMENT","A. System Level Architecture","B. Host Software Functions","C. JPU Software Components","D. Detailed Description of PSDP Architecture","D. Query Processing Example"],"p":["This application claims the benefit of U.S. Provisional Application No. 60\/412,057 entitled \u201cAsymmetric Streaming Record Processing Computer System,\u201d filed on Sep. 19, 2002, and U.S. Provisional Application No. 60\/411,686 entitled \u201cIntelligent Storage Device Controller,\u201d filed on Sep. 18, 2002. The entire teachings of these provisional applications is hereby incorporated by reference.","This application is also related to U.S. Patent Application entitled \u201cIntelligent Storage Device Controller,\u201d (application Ser. No. 10\/667,203); U.S. Patent Application entitled \u201cField Oriented Pipeline Architecture for a Programmable Data Streaming Processor,\u201d (application Ser. No. 10\/665,726); U.S. Patent Application entitled \u201cAsymmetric Streaming Record Data Processor Method and Apparatus,\u201d (application Ser. No. 10\/666,729); and U.S. Patent Application entitled \u201cAsymmetric Data Streaming Architecture Having Autonomous and Asynchronous Job Processing Unit,\u201d (application Ser. No. 10\/667,128), all of which are being filed together on the same date as this application. The entire teachings of each of these patent applications is also hereby incorporated by reference. This application and the above applications are also all assigned to Netezza Corporation.","This invention relates to distributed data processing systems that use multiple processing unit groups, and in particular to programmable data streaming processor that performs initial processing before tuples are handled by a job processor.","With continued development of low cost computing systems and proliferation of computer networks, the world continues to see an exponential growth in the amount and availability of information. Indeed, the Massachusetts-based Enterprise Storage Group has observed a doubling of information every few months. Demand for easy and efficient access to this ever-growing amount of digital information is another certainty. For example, World Wide Web traffic increased 300% in 2001 according to Forrester Research. Included among the applications that continue to make the greatest demands are systems for processing:\n\n","Greg Papadopolous, the Chief Technical Officer of Sun Microsystems, Inc., has observed that the demand for access to decision support databases, referred to as the Input\/Output (I\/O) demand growth, doubles every nine months. To put this in context, Moore's Law predicts that Central Processing Unit (CPU) power doubles only about every 18 months. In other words, the demand for access to information is growing at least twice as fast the ability of a single CPU to process and deliver it.","In a typical general purpose data processing system, data is stored on one or more mass storage devices, such as hard disk drives. One or more computers are then programmed to read data from the disks and analyze it\u2014the programs may include special database software written for this purpose. The problem with a general purpose system architecture, however, is that all the data must be retrieved from the disk and placed in a computer's memory, prior to actually being able to perform any operations on it. If any portion of the data retrieved is not actually needed, the time spent fetching it is wasted. Valuable time is thus lost in the process of retrieval and storage of unnecessary data.","The speed at which the data analysis can be performed is typically limited to the speed at which the entire set of data can be transferred into a computer's memory and then examined by the CPU(s). Usually, the aggregate data transfer rate of the disks does not govern the speed at which the analysis can be performed. Disks are inexpensive, and as such, data can be spread across a large number of disks arranged to be accessed in parallel. The effective data transfer rate of a set of disks, collectively, can therefore be almost arbitrarily fast.","The bandwidth of an interface or communications network between the disks and the CPUs is also typically less than the aggregate data transfer rate of the disks. The bottleneck is thus in the communications network or in the CPUs, but not in the disks themselves.","It has been recognized for some time that achieving adequate performance and scalability in the face of vast and rapidly growing data thus requires some kind of system architecture that employs multiple CPUs. The three most prevalent classes of so-called multiprocessing systems today include:\n\n","Two weaknesses of the SMP approach impair its performance and scalability when processing very large amounts of data. The first problem results from a limited ability to actually provide information to the processors. With this architecture, the I\/O subsystem and the memory bus are shared among all processors, yet they have a limited bandwidth. Thus, when the volume of data is too high, the speed of the processors is wasted waiting for data to arrive. A second problem with the SMP approach is cache coherence. Within each processor is typically a cache memory for storing records so that they may be accessed faster. However, the more that processors are added to an SMP system, the more that time must be spent synchronizing all of the individual caches when changes are made to the database. In practice, it is rare for SMP machines to scale linearly beyond about 64 processors.","Asymmetric Multiprocessing (ASMP) systems assign specific tasks to specific processors, with a master processor controlling the system. This specialization has a number of benefits. Resources can be dedicated to specific tasks, avoiding the overhead of coordinating shared access. Scheduling is also easier in an ASMP system, where there are fewer choices about which processor to assign to a task. ASMP systems thus tend to be more scalable than SMP systems. One basic problem with asymmetry is that it can result in one processor being overloaded while others sit idle.","Massively Parallel Processing (MPP) systems consist of very large numbers of processors that are loosely coupled. Each processor has its own memory and devices and runs its own operating system. Communication between the processors of an MPP system is accomplished by sending messages over network connections. With no shared resources, MPP systems require much less synchronization than SMP and ASMP systems.","One weakness of the MPP model is that communication among processors occurs by passing messages over a network connection, which is a much slower technique than communication through shared memory. If frequent inter-processor communication is required, then the advantages of parallelism are negated by communication latency. Another problem with the MPP approach is that traditional programming models do not map cleanly onto message passing architectures. Using approaches such as Common Object Request Broker Architecture (CORBA), which are designed to handle message passing, are considered awkward by some designers.","There have also been attempts over the years to use distributed processing approaches of various types. These began with proposals for \u201cDatabase Machines\u201d in the 1970s, for \u201cParallel Query Processing\u201d in the 1980s, and for \u201cActive Disks\u201d and \u201cIntelligent Disks\u201d in the last five to ten years. These techniques typically place a programmable processor directly in a disk sub-assembly, or otherwise in a location that is tightly coupled to a specific disk drive. This approach pushes processing power towards the disks, and thus can be used to reduce the load on a host computer's CPU.","More recently, system architectures have been adopted for parallel execution of operations that originate as standard database language queries. For example, U.S. Pat. No. 6,507,834 issued to Kabra et al. uses a multi-processor architecture to process Structured Query Language (SQL) instructions in a publish\/subscribe model such that new entries in a database are automatically processed as added. As explained in the Abstract of that patent, a first processor is used as a dispatcher to execute optimized queries, setup communication links between operators, and ensure that results are sent back to the application that originated the query. The dispatcher merges results of parallel execution by other processors to produce a single set of output tuples that is then returned to a calling procedure.","In a preferred embodiment, the present invention is a data processing system having two or more groups of processors that have attributes that are optimized for their assigned functions. A first processor group consists of one or more host computers, which are responsible for interfacing with applications and\/or end users to obtain queries, for planning query execution, and for, optionally, processing certain parts of queries. The hosts in the first group may be SMP type machines. A second processor group consists of many streaming record-oriented processors called Job Processing Units (JPUs), preferably arranged as an MPP structure. The JPUs typically carry out the bulk of the data processing required to implement the logic of a query.","Functions of the host computers in the first group can be divided into a \u201cFront End\u201d and an \u201cExecution Engine\u201d. The Front End is responsible for parsing queries, generating query execution plans, optimizing parallelizing execution plans, controlling transactions, sending requests for processing to the Execution Engine and receiving results of such requests from the Execution Engine. The Execution Engine is responsible for scheduling the execution of jobs and other operations to run on the JPUs or locally within the Execution Engine itself, (such as sorting, grouping, and relational joining).","Each of the JPUs in the second group typically include a general purpose microcomputer, local memory, one or more mass storage devices, and one or more network connections. The JPUs preferably use a multi-tasking operating system that permits multiple tasks to run at a given instant in time, in a priority-based demand scheduling environment.","The JPUs are responsible for:\n\n","In a preferred embodiment, each JPU also has a special purpose programmable processor, referred to herein as a Programmable Streaming Data Processor (PSDP). The PSDP acts as a storage controller, to serve as an interface between the CPU of a JPU and the mass storage device. The PSDP is a processor that is distinct from the more general purpose CPU in each JPU. It is also distinct from the CPU of the \u201chost\u201d in the first group.","The PSDP can be implemented as a Field Programmable Gate Array (FPGA), as in the preferred embodiment, or as an Application-Specific Integrated Circuit (ASIC), a fully-custom Application Specific Standard Product (ASSP), or even as discrete logic on a printed-circuit board. It can also be included in an integrated processor (i.e., a CPU that includes peripheral interface logic) on a single chip or in a single package, or it can be included with the circuitry of the mass storage device.","In addition to assisting the JPU in accessing data, the PSDP is specially programmable to also interpret data in a specific format as it is read from or written to the associated disk(s). This enables PSDP to perform portions of jobs on data directly, as it is read off the disk, prior such data ever being forwarded to the JPU.","In an embodiment specifically adapted for processing of record-oriented data, data can be filtered by the PSDP as records and fields of a database, so that only certain records, or certain portions of records, are actually forwarded to be written into the associated JPU's main memory.","However, many other operations beyond simple filtering are possible to implement in the PSDP. For example, records with certain characteristics can be tagged as they are written in the JPU's main memory, to indicate that such records are to be ignored in further processing, or to indicate certain attributes of such records, such as if they are to be handled differently in a transactions from other records.","While of use in processing field-oriented database records, it should be understood that the particular invention can also be used to advantage in processing many different types of data, including other field delimited data such as tables, indices, and views. The system is also advantageously used to process less structured data such as character strings, Binary Large Objects (BLOBS), XML, graphics files, and the like.","A number of advantages result from this architecture.","First, unlike prior art database machines that integrate special processing hardware into the disk assembly itself (e.g. on the heads, on the arms, or electronically nearby), the JPUs in the second group use the special purpose PSDP hardware to interface to a disk and filter data after it reads from a disk, but still prior to a more general purpose execution unit. As a result, the system designer may now use industry standard disk controllers and standard hard disk drives. This allows the designer to effectively leverage the ever increasingly higher density of standard IDE and SCSI compatible storage media, as soon as they become available.","Second, like the custom controller approach, any need to first read records into memory locations prior to performing any operation on them is still avoided. But when only a fraction of the available data is relevant to a query, the PSDP avoids inefficiencies of other approaches that:\n\n","The PSDP avoids these problems since database filtering operations are performed \u201con the fly\u201d in a streaming fashion, as data is read as records stream out of the mass storage devices.","In a preferred embodiment, the PSRP can also be programmed perform operations such as Boolean comparisons of record field values against either literal values or other record field values, or values held in registers of the processing element, and reject records that fail these Boolean comparisons before they are stored in memory. Of the records that pass the filtering conditions, the PSDP element can thus additionally filter out the subset of fields that are irrelevant to a particular query.","In addition to field-level record filtering, the PSDP also can perform other operations on records as they are read from mass storage. For example, the PSDP can be programmed to decompress records entering memory and to compress records being sent out of memory. It can be instructed to decrypt records entering memory or to encrypt records being sent out of memory. It can convert lowercase fields to mixed or uppercase. It can, in fact, be programmed to perform myriad other such operations. Because these operations occur as each record streams into memory, the PSDP offloads such tasks from the JPUs main CPU, freeing it for other useful work.","Other advantages result if the PSDP is programmed to perform simple Boolean operations, such as to compare field values of the record stream against values held in its local registers. This allows a limited class of join operations to be performed of records before they are stored in memory. For example, if the values of the fields being joined are limited in range (such as when a set of consecutive integers is used to represent each of the 50 United States), the presence or absence of a particular field value can be encoded as a bit within a sequence of bits, whose position within the sequence corresponds to the integer field value.","One advantage of this is that it allows field-level filtering and more complex processing to proceed in parallel within the JPU, for additional performance benefit. A more important advantage is that this configuration of processors is most effective at reducing the amount of data that must flow through the system.","In essence, by using a PSDP that is dedicated to performing as much field-level filtering as possible before records are stored into the JPU's memory, the JPU's CPU is thus free to perform as much record processing as possible before it must return records over the network (for aggregation with the results of other JPUs) into a final reply to the SMP host. Because moving vast amounts of data requires much overhead, it is advantageous to add a dedicated processing element before each step in the data movement pathway, from input to final result.","The JPU\/PSDP architecture, in effect, separates streaming record processing from other query processing functions. Because the PSDP can be programmed to recognize record formats, it is capable of producing tuple sets as an output. As a result, after data leaves the PSDP, it can always be handled in tuple set form. This permits very fast handling data procedures to be implemented, because a consuming operation (be it in the JPU or the host) never has to process a block of undifferentiated binary data.","Additionally, since there can now be one common data handling paradigm throughout the system, i.e., the streaming tuple set, all functions such as storage, network, data operations, and transaction operations can efficiently and consistently use the tuple set model. Therefore, any operation may be arranged to take as input(s) the output(s) from any other operation. Also, a common set of algorithms may be used for all operations whether on the host(s) or JPUs.","This is in contrast to most database systems, which may materialize data as blocks of binary information that needs to be parsed by differing operations; which use different paradigms for network, storage, and internal operations; and which are unable to stream efficiently because of those different paradigms.","The two group architecture also allows an application to be insulated from the details of the programming model of the JPU. The application interacts only with the first group, and the first group translates the application's requests into requests against the JPU. This approach has several advantages:\n\n","1. First Group Components","The present invention is a data processing system having at least two \u201cgroups\u201d of processing units, in which the individual components of each group are individual network \u201cnodes\u201d within the system. As will be explained in detail below, the present invention has to do with how the a first group of one or more host processors accepts and responds to queries for data, and transforms such queries into one or more jobs, a second group of nodes comprising one or more Job Processing Units (JPUs), wherein. each JPU has a streaming data interface, for receiving data from a streaming data source, one or more general purpose CPUs, for responding to requests from the host computers in the first group, and one or more Programmable Streaming Data Processors (PSDPs), which perform primitive functions directly on data received from the streaming data interface.","As more particularly shown in , the first group  consists of one or more SMP \u201chost\u201d computers , each with its own memory, network interface, and local storage (not shown in ). Each host  runs its own operating system, and typically, but not necessarily, each host  uses the same type of operating system as the other hosts .","The hosts  typically accept queries that are requests for data stored on mass storage devices, such as hard disk drives . The requests may originate from any number of business intelligence applications that may be residing on local processors  or client computers  or separately running application software , that may originate through a computer network  or locally. Queries are typically provided in a format such as Structured Query Language (SQL), Open DataBase Connectivity (ODBC), Java DataBase Connectivity (JDBC), or the like.","The hosts  accept queries that can retrieve, modify, create and\/or delete data stored on disk  and the schema for such data. The hosts  also accept requests to start, commit, and rollback transactions against the data. The hosts  also perform typical administrative functions such as reporting on the status of the system , start and shutdown operation, backing up the current state of the data, restoring previous states of the data, replicating the data, and performing maintenance operations.","Optionally, there is a load balancing function  in front of the host  processors, which directs individual transactions to specific host or hosts  so as to evenly distribute workload.","A catalog management component  contains descriptions of the fields and layout of data. Catalog management  also contains information about which users and applications have which permissions to operate in which ways on which types of records, datasets, and relations. The various hosts  interact with catalog management  in order to process the requests they receive. In one embodiment, catalog management  is embedded within one of the hosts , with parts replicated to the other hosts  and second group  components. As will be understood shortly, the catalog manager provides information to permit the components of the second group  to perform filtering functions.","With the exception of their need to consult catalog management , the hosts  are generally able to respond to requests without having to communicate among themselves. In very rare instances, inter-host  communication may occur to resolve a transaction sequencing issue.","2. Second Group Components","The second group  consists of a plurality of Job Processing Units (JPUs) . As shown in , each JPU  consists of a network interface  for receiving requests and delivering replies, a general purpose Central Processing Unit (CPU)  such as a microprocessor , memory , and a Programmable Streaming Data Processor (PSDP) . Each JPU  runs a multi-tasking schedule-based operating system. Each JPU  also has an attached disk  and disk controller from which the JPU  may read streaming data. In other embodiments, the JPU  can receive streaming record data from alternate or additional sources such as other on-board processors or via other network interfaces in place of the disk drives . Such streaming data might include stock quotes, satellite data, patient vital signs, and other kinds of \u201clive-feed\u201d information available via a network connection.","The JPU  accepts and responds to requests from host computers  in the first group  to process the streaming record-oriented data under its control. These requests are typically \u201cjobs\u201d of a larger query, and are expressed as sequences of primitive operations on an input stream. The primitive operations could be interpreted, but in the preferred embodiment, they are packaged as compiled code that is ready for execution. An exemplary job-based query is described in more detail below.","In addition to processing jobs, a JPU  also accepts and responds to requests from hosts for other operations such as:\n\n","Each JPU  also accepts and responds to requests from the hosts  to:\n\n","JPU(s)  typically use multi-tasking Operating System (OS) to allow receiving, processing, and reporting the results from multiple jobs in a job queue. The OS should also support overlapping job execution. To coordinate this, the OS typically is responsible for scheduling and prioritizing requests according to a number of factors that are determined in real time. These may include a job priority as assigned by the user and\/or host , as well as a job's expected impact on the JPU's  local resources includes the amount of memory, disk, network, and\/or I\/O queues needed to complete the job. The JPU  can also contain software for performing concurrency control, transaction management, recovery and replication of data for which the JPU is responsible.","JPUs  in the second group  are not directly visible or accessible to the users of, or the applications that run on, for example, the clients  or business intelligence applications  that present queries to the system . The JPUs are, instead, an embedded component that maintain significant autonomy and control over their own data. A given record (or other data primitive) in the system  is thus normally directly accessible to, and processed by only one JPU . While JPUs may replicate their records to increase reliability or performance, they do not share responsibility for processing a given record with other JPUs  when carrying at a job as part of a query. More details of this autonomous, asynchronous nature of the JPU's can be found in the above referenced co-pending U.S. Patent Application entitled \u201cProgrammable Data Streaming Architecture Having Autonomous and Asynchronous Job Processing Unit.\u201d","The storage manager  within each JPU  provides support for other functions such as error checking, creation and deletion of tables, the use of indices, record insert and delete, mass loading of existing user data among various JPUs, and the like.","Throughout the system, the components and sub-components are designed to optimize performance thru extensive use of streaming operations coupled with tuple set operations. As will be understood shortly, most operations are designed to take tuple sets (records or groups of records) as their input and output streams; these operations try not to materialize data, but instead they stream the output to the next operation. As a consequence many operations can be handled as one continuous data flow, whereas in a conventional system, it would be necessary to handle them in various layers.","For instance, a storage layer can be designed as a tuple set manager where (from the view of other JPU processes) it stores and retrieves tuple sets. From the storage layer onward, data is normally handled in tuple sets, providing a consistent, well organized, and easily accessible format for internal operations. This is in contrast to other systems where the storage layer stores and retrieves undifferentiated blocks of data which are later converted to tuple sets by some other downstream process. Another example of the streaming\/tuple set architecture is the network layer, which sends and receives tuple sets instead of blocks of data.","Yet another example is a merge aggregation mode, where a sorted data stream is aggregated as requested, and whenever a new key index value is received, the aggregation from the previous key index value may be streamed to the next node.","A streaming\/tuple set operation can be illustrated by tracking a typical dataflow during a load operation. In this example load case, as data is read into a host  over TCP\/IP network connection , that data is parsed, error-checked, and transformed, and the distribution value calculated, all while the specific byte\/field is in processor cache, and saved to the internal network output frame buffers as one step. The result is that the input data is read\/transformed in a streaming fashion and converted to network-ready tuple set packets at streaming speed with minimal overhead. As each packet is received, it is sent over the internal network  to an appropriate JPU  (as determined by the a distribution value in a Query Plan). At the JPU , the received data is read, converted into an approved storage format, and placed in memory buffers on a record-by-record basis. As memory buffers are filled, a storage layer in the JPU double-checks that the data corresponds to the indicated table, and that the table \u201cowns\u201d the physical space on the disk , and then writes that data to the disk . Note that during this process, a given byte of data was \u201ctouched\u201d only a few times, and that the data was manipulated in tuple sets thereby optimizing performance and reliability.","A second illustration of a streaming tuple set operation is a join\/aggregate operation where three joins and one co-located aggregation are performed on JPUs , and the results are returned through the host  via ODBC to the ODBC client  (e.g., Business Objects).","In this example, on each of three JPUs, the disk  is scanned and data read off the disk through the associated PSDP, which filters records of interest and fields of interest within those records, and places the resulting tuples into a tuple set buffer in JPU memory. As each tuple set buffer is filled, that tuple set is passed through each of three JPU join nodes and the aggregate node in turn. Each time a new key value is received by the aggregate node, the previous aggregate value and associated key value tuple are transformed as necessary per the ODBC request, and placed in the JPU network packet output buffer associated with the requesting host . When a network packet output buffer in the JPU is filled, its contents are sent to the host , where it is immediately placed in the user-side network buffer and is immediately sent to the ODBC client .","Note that, as in the previous example, the data was \u201ctouched\u201d only a few times. Because the data was handled in tuple sets, it could be operated on as integral units with very minimal overhead. Because the operations are extremely integrated, mixed operations such as joins, aggregates, output transformation, and network packet creation are all performed while the data is in processor cache memory.","More information regarding the streaming nature of data transfer can be found in the above referenced co-pending U.S. Patent Application entitled \u201cAsymmetric Streaming Record Data Processor Method and Apparatus,\u201d.",{"@attributes":{"id":"p-0072","num":"0099"},"figref":"FIG. 2","b":["12","22"]},"Postmaster ",{"@attributes":{"id":"p-0073","num":"0000"},"ul":{"@attributes":{"id":"ul0015","list-style":"none"},"li":{"@attributes":{"id":"ul0015-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0016","list-style":"none"},"li":["Serves as Front-end for query processing","Postmaster  accepts requests from user applications via API 200","Creates an Execution Plan","May use authentication\n\nPlan Generator \n","Parse\/query rewrite\/planner\u2014plans how query will be processed.","Supports SQL-92 DDL\/DML","Supports SQL Functions","Provides compatibility with Oracle, SQL Server","Integrated with SQL triggers, stored procedures\n\nPlan Optimizer \n","Cost-based optimizer, with the addition of locale costs which optimizes for most efficient operation\/highest level performance","Indicates which operations will be done within host and which will be done within JPU","Communicates with Plan Link, providing tips on what filtering should be done within the Programmable Data Streaming Processing (\u201cPSDP\u201d) if there are multiple filters that can be done there (more than the PSDP can handle)","Maintains usage\/reference statistics for later index creation, refreshing cluster indices\n\nPlan Link \n","Takes an Execution Plan as input","Analyzes Execution Plan and splits plan further, identifying what will be done within the PSDP , what will be done within the JPU  after the PSDP  has returned its data to the JPU , and what will be done in the Host  after the JPU  has returned its data\n\nSQL Expression Evaluator\/SQL Converter \n","Expression Evaluator","Creates object code for evaluating given expression to be executed on the Host, JPU, and PSDP based on the expressions, their type, and the capabilities of the installed hardware\n\nHost Dispatch \n","Similar to standard UNIX scheduler\/dispatcher","Queues execution plan and prioritizes based on (a) the plan's priority, history, and expected resource requirements, and (b) available resources and other plans' requirements","Controls number of jobs being sent to any one JPU  to avoid JPU Scheduler or JPU memory overload","Sends Host jobs to host execution engine\n\nCommunications Layer \n","Provides communications among the nodes","Includes Job Listener to await data from nodes","Uses striping data from a Topology Manager to direct multicast and unicast messages","Detects non-responsiveness of nodes and communicates with Topology Manager to trigger failover processing\n\nCall Home \n","Initiates message to a Technical Assistance Center (not shown) to identify failed part and trigger service call or delivery of replacement component (as appropriate given user support level)","Optionally communicates via SNMP to a defined app to receive a failure indicator and callhome trigger","Logs error(s)\n\nLogger\/Replication Server \n","Logs transaction plans, messages, failures, etc. to Netezza log in conventional fashion","Implemented as a standard transaction logger\/replication server\n\nSystem Manager \n","Defines and maintains JPU Configuration information, striping information","Mirror Master\u2014maintains mirrors info\u2014what JPUs are being mirrored where, maintains SPA data, maintains info on system spares","Initiates failover processing when informed by Comm layer of a non-communicative JPU\u2014directs mirror of failed JPU to take over as primary and begin copying to designated spare, directs primary of JPU mirrored on failed JPU to copy its data to that same designated spare, to reduce load on mirror of original failed JPU also directs mirror of the primary on that failed JPU's mirror to do double duty and act as new primary until failover copying has been completed","Communicates to callhome component to initiate replacement process","Manages system expansion and allows for redistribution of data as appropriate or as requested by user during expansion","Initiates JPU diagnostics when appropriate","Provides an API to allow client management interface to get configuration data for user display\/control\n\nHost Diags \n","Runs diagnostics on Host as required\/requested\n\nLoader \n","Provides fast loader capability for loading user data onto disks","Communicates directly to Host Dispatch to load database\/insert records","Communicates with System Manager to get configuration and mirroring data","Controls index creation on primary (and sets up job to run later to create indices on mirror)","Supports input via a number of methods (e.g., tab-separated data, backup\/recovery)","Does ETL, converts data from Oracle, SQL Server, DB\/2, etc. to the internal data format\n\nMOX\/OLAP \n","Provides OLAP\/MDX, ROLAP Engine on Host","Creates and maintains MOLAP cubes","Supports multi-user MDX","Creates Execution Plans for OLAP requests and communicates these directly to Host Dispatch","Supports metadata writeback","Provides administrative support for user creation, security","Access System Catalog through API\n\nCube Builder User Interface (UI) \n","Provides interface for defining and managing cubes to be used in OLAP Processing\n\nJPU Downloader \n","Downloads Firmware to System JPUs  at system initiation\/boot","Downloads PSDP  and JPU  images","Communicates with System Manager to understand number of JPUs and JPU configurations","Initializes spares for failover","Initializes replacements\n\nHost Disk Manager \n","Manages Host Disk (used for Catalog, Temp Tables, Transaction Log, Netezza Log, Swap space)\n\nHost Event Handler \n","Receives partial record sets from JPUs  through the Comm Layer Job Listener","Executes remainder of Execution Plan that has to be done at Host ","Provides intermediate and final sort-merge of JPU  sorted data as required","Handles joins of data returned from JPUs  as required","Communicates to JPUs through Comm Layer  to request partial result sets from JPU buffers when idle (e.g., to get and sort\/process partial records that the JPU currently has instead of waiting for JPU  to fill a buffer and then send to Host \n\nHost Transaction Manager \n","Manages transactions on the host ","Controls requests sent to JPUs  that will be involved in the transaction","Provides lock management and deadlock detection","Initiates abort processing","Sends state data to Recovery Manager ","Sends ID requests to the Transaction I.D.(TID) Manager ","Provides transaction IDs and deleted transaction IDs to ensure that disk records are preceded","Manages catalog requests as transaction requests as required\n\nTID Manager \n","Provides unique transaction identifiers (TIDs)","Coordinates with other hosts to avoid generating duplicate TIDs\n\nHost Recovery Manager \n","Ensures transaction atomicity after component (e.g., JPU) failure","Maintains journal of transaction state","Initiates rollback as required\n\nBackup\/Recovery \n","Supports Host side of Backup\/Recovery process","Interfaces with Transaction Manager and JPU Storage Manager"]}}}},{"@attributes":{"id":"p-0074","num":"0178"},"figref":"FIG. 3","b":"22"},"Communications Layer ",{"@attributes":{"id":"p-0075","num":"0000"},"ul":{"@attributes":{"id":"ul0017","list-style":"none"},"li":{"@attributes":{"id":"ul0017-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0018","list-style":"none"},"li":["Provides internal communication among nodes","Includes Job Listener  to await requests","Includes Network Poster  to send data when buffer filled, job completed, or at Host request\n\nJPU Dispatch\/Scheduler \n","Receives plan through Communications Layer ","Queues Plan","Schedules\/dispatches jobs according to their priority, \u201cfairness\u201d to date, expected resource requirements, and available resources\n\nJPU Transaction Manager \n","Processes changes in transaction state to begin a transaction, pre-commit a transaction, commit a transaction, or abort a transaction","Handles processing of dependencies among transactions as flagged by the lock manager; broadcasts information about these dependencies to relevant host(s); initiates deadlock checks\n\nJPU Lock Manager \n","Controls concurrent access to data","Interfaces with EventTask  before a query is executed and for each result set returned from a scan","Provides support for arithmetic locking\n\nJPU Recovery Manager \n","Maintains a Journal to track transaction status on the JPU , using the Storage Manager API","Performs transaction recovery when requested by JPU Transaction Manager\n\nJPU Mirror Manager \n","Mirror Sender receives copies of record updates from Storage Manager  and transmits these to the mirror for this JPU when an updating transaction commits","Mirror Receiver receives record updates, buffers these in memory, and flushes out to disk through the Storage Manager when the Mirror Receiver buffer is full","Transmits all data to a spare system during failover processing\n\nStorage Manager \n","Stores and manages information on disk in optimal fashion","Has an API that supports storage and retrieval of tuple sets","Supports error checking to insure that the data conforms to the indicated table and the indicated table \u201cowns\u201d the physical space to which the data is being written","Supports creation and deletion of tables, views, and indices","Handles record inserts and deletes","Supports ETL and mass loading of existing user data","Provides storage support for commit\/rollback","Provides support for Precise Indexes","Provides mirroring support for failover","Optimizes sort operations and utilizes smart hash algorithm for data distribution\/striping","Provides support for compression and smart storage optimization","Controls disk I\/O\n\nJPU Resource Scheduler \n","Schedules jobs to run on the PSDP ; communicates with JPU\/PSDP Scheduler  to queue up PSDP requests to retrieve required data","Optimizes the queue to keep the PSDP\/disk as busy as possible, with requests from multiple queries intermixed in the queue based on disk characteristics and location of data on the disk","Takes into account the needs of any data loading for new tables being created and transformed to internal data format (i.e., to optimize the loading process)","Supports heuristic-based scheduling, ensuring that jobs are scheduled on a priority basis, but also ensuring that all jobs do get serviced (e.g., raising a job in priority if it has not been run in a certain interval of time)","Supports synchronous\/piggy-backed scans, combining similar requests to optimize PSDP processing","Manages memory buffers\/memory allocation on JPU; allocates memory to Execution Plans based on expected needs and hints received from Plan Optimizer","JPU Paging (if required)\n\nPSDP Prep \n","Defines the instructions that will be given to the PSDP  in order to process a request (instructions tell the PSDP  what to do with each field being read from the disk)","Identifies what filtering, transformation, projection, and aggregation operations are to by run by the PSDP \n\nEventTask \n","Executes the portion of the Execution Plan that could not be handled by the PSDP but that does not have to be handled at the Host level","Handles sorts, joins, transformations, and aggregations that could not be done as data stream through the PSDP ","Maintains a memory buffer of result set records and returns these to Host through the Comm Layer when buffer filled, job completed, or at Host request\n\nJPU Diags \n","Runs diagnostics on JPU as required\/requested\n\nJPU Boot\/Init \n","Executes image burned into flash memory at boot time to bootstrap the JPU, run diagnostics, register the JPU with the primary Host server, and download new image from Host to run","Loads and transfers control to the image downloaded from the primary Host server to load the JPU application code, the operating system, the network stack, and disk driver code\n\nBackup\/Recovery \n","Supports JPU side of Backup\/Recovery process","Interfaces with Transaction Manager and JPU Storage Manager\n\nDBA Lite \n","Provides automatic and dynamic disk and Storage Manager support","Supports dynamic index creation, defragging, index garbage collection, timers, agents\n\nJPU\/PSDP Scheduler \n","Schedules jobs to run on the PSDP; queues up PSDP requests to retrieve required data"]}}}},"As discussed above, the PSDP allows data to be processed during Direct Memory Access (DMA) disk read operations. There are many different possible operations that can be performed by the PSDP , including transforming and comparing data with other data or with constants.","PSDP  functions fall into two general categories: disk driver logic interface  and data \u201cfilter\u201d . Each of these functions is described in some detail below. It is sufficient here to note that the disk driver logic interface  accepts standard disk drive interface signaling, such as IDE (Integrated Device Electronics) or SCSI (Small Computer Systems Interface), adapting it to a particular CPU native \u201cbus\u201d such as a Advanced Technology Attachment (ATA) bus or the like. Alternatively, if there is a communications network, such as Ethernet or Fibrechannel, instead of array of disks  to provide access to input data stream(s), the interface  becomes a network interface that is suitable to receive and\/or transmit data over a communications network. The disk driver logic  is usually implemented in an Integrated Circuit (IC) in a computer or communications device, in or part of an IC that contains other logic, such as other interface logic or the CPU  itself. The disk driver  can even be inside the disk  itself, making the disk a special-purpose unit attachable only to JPUs or communications devices for which the interface is specific.","In the preferred embodiment, the PSDP  is however an Integrated Circuit (IC) that interfaces a standard disk  to a peripheral bus of the JPU . All such controllers have the basic function of allowing the CPU  in the JPU  to read and write the disk , typically by setting up long data transfers between contiguous regions on the disk and contiguous regions in the CPU's  memory, a process usually referred to as Direct Memory Access (DMA).","The PSDP  also provides programmable hardware directly in the disk read path, to and from the controller. This function of the PSDP hardware, called the \u201cfilter\u201d unit , is programmed to understand the structure of the data the analysis software running on the JPU  wishes to read and analyze. The PSDP  can be this programmed to operate on data as it is received from the disk , before it is stored into the JPU's memory, and in the process discard data that the JPU  would otherwise have to analyze. In an embodiment specifically adapted for processing of record-oriented data, data can be filtered by the PSDP  as records and fields of a database, so that only certain fields from certain records are actually forwarded to be written into the associated JPU's main memory.","Many other operations beyond simple filtering are possible however. For example, records with certain characteristics can be tagged as they are processed, to indicate that such records are to be ignored in further processing, or to indicate certain attributes of such records, such as if they are to be handled differently in a transactions from other records. Other, non-filter like processes can be implemented such as compression\/decompression; encryption\/decryption; simple join operations, and the like.","Thus, while the PSDP  of particular use in processing field-oriented database records, it should be understood it may process many different types of data, including other field delimited data such as tables, indices, and views; or less structured data such as character strings, Binary Large Objects (BLOBS), XML, graphics files, and the like. So although referred to herein as a \u201cfilter\u201d unit that processes \u201crecords\u201d, it should thus be understood that filter  can also perform many other functions on various types of data, not just records.","As one example of filtering record-oriented data, the PSDP  can be programmed to recognize that a certain set of records in a database have a specified format, for example, a preamble or \u201cheader\u201d of determined length and format, perhaps a field including the length of the record, followed by data including some number of fields of a certain type and length (e.g., 4-byte integers), followed by some number of fields of a different type and length (e.g., 12-byte character strings), followed by some number of fields of variable length, whose first few bytes specify the length of the field in some agreed-upon manner, and so forth.","The filter unit  can then execute this program as it reads data from the disk , locate record and field boundaries, and even employ further appropriate Boolean logic or arithmetic methods to compare fields with one another or with literal value. This allows the filter unit  to determine precisely which fields of which records are worth transferring to memory. The remaining records are discarded, or tagged in a manner that signals the JPU  that a record need not be analyzed. Again, there will be more discussion of how this is done in detail below.","In the preferred embodiment, there are two basic reasons for which the filter unit  can discard a record (or mark it as unworthy of attention). The first is an analysis of the contents of the fields as described above. Using a previous example, the filter unit  can, for example, be programmed for a store sales database to check a purchase date field against a range of numbers that correspond to dates in the month of July in the year 2000, another field for a number or string uniquely associated with a particular store in North Carolina, another field for a set of SKU (Stock-Keeping Unit) values belonging to various styles or manufacturers of blue raincoats, and in this fashion mark only certain records for further processing. The filter unit  can further be programmed to know which fields contain the name and address of the customer who made the purchase, and return only these fields from the interesting records. Although other database software could perform these operations, the filter unit  can perform them at the same rate as the data is supplied by the disk . Far less data ends up in the JPU's memory as a result leaving the JPU  free for more complex tasks such as sorting the resulting list of names and addresses by last name or by postal code.","A second example of how the filter unit  can be used is to discard or mark a record, as in record creation and deletion in a multi-user environment. Databases are not static, and it is common for some users to be analyzing a database while others are updating it. To allow such users concurrent access to the database, records can be tagged with transaction numbers that indicate when or by whom a record was created or marked obsolete. A user querying a database may not wish to see records created by another user whose activity began subsequently, or whose activity began previously but is not yet complete; if so, he probably will want to see records marked obsolete by such a user. Or the user may wish to see only the results of transactions entered by certain users, or only the results of transactions not entered by certain users. To facilitate this kind of record filtering, record headers can contain creation and deletion identifiers that the filter unit  can be programmed to compare with the current user's identifier to determine whether records should be \u201cvisible\u201d to the current user. Once again, the filter unit can avoid transferring useless data to memory or relieve the JPU  of a time-consuming analysis task.","In the preferred embodiment there are two basic methods the filter  unit can use to filter out data that is unnecessary for a given query, thereby reducing traffic on the communications network and reducing the workload on the JPU . As described above, the filter unit  can simply discard the data. This is not always practical, however. Imagine a very long record with many fields, or large fields, many of which are to be returned to the JPU . Further consider a situation where a record meets the criteria is arranged in such a way that the contents of the last field are relevant to the decision to transfer or discard the selected fields of the record. Practical implementations of the filter unit  may not be able to store (\u201cbuffer\u201d) the largest possible set of returnable fields in a very long record, since there will be a limit on local buffer size. In such a case, the filter unit must begin sending the selected fields to the JPU  before it can tell whether they actually should be sent. After the record has been completely processed by the filter unit, and all the selected fields transferred to the JPU , the filter can tag the transferred data with a states bit that says \u201cnever mind\u201d, thus saving the JPU  and the communications network a great deal of work. In practice, the filter unit can append a length indication to every record fragment it does transfer to the JPU , so that the JPU  can find the boundaries between the record fragments the filter unit deposits in memory. This is a natural place for this status bit (or bits, if the JPU  must distinguish among multiple reasons) indicating the transfer of a useless record.","In addition to selecting certain fields from certain records for transfer to the JPU , the filter unit  can create and return additional fields not present in the database, by performing calculations on the contents of the fields that are present. This can further relieve the JPU  of work. An example of this is the calculation of a \u201chash\u201d function on the values of specified fields from a record, some of whose fields are to be transferred to the JPU . A hash function is a numerical key assigned to a collection of numeric or non-numeric field values that speeds up the process of searching through a list of records. Other examples of useful information that can be computed by the filter unit  include running sums or averages of field values from one record to the next. All of these benefits accrue from the filter unit's  ability to parse the data into records and fields as it transfers the data from the disk  to the JPU .","Another example is a transformation, such as an ASCII substitution. One usage for an ASCII substitution is to change the collation sequence of a given field. For example, if the LAST_NAME starts with the French \u2018\u00e7\u2019 (ASCII ) then the SQL clause \u201cWHERE LAST_NAME IS>\u2018H\u2019\u201d will erroneously fail unless \u2018\u00e7\u2019 has been mapped to \u2018C\u2019 (ASCII ). Similar issues involve the use of the UPPER( ) and LOWER( ) functions. In the preferred embodiment, the PSDP has 2 groups of registers, each 256 bytes. If transformation of a given field is selected then the PSDP setup loads transformation fields into these registers before the data is streamed in. Each register in the transformation fields corresponds to an extended ASCII value and the register contains the value that each extended ASCII character is to be converted into. In the example above, register number  contains the value . During the streaming phase, as each tuple streams through the PSDP, for those fields where a transformation is indicated, each byte is individually transformed to its converted value. Two registers are provided so that two types of transforms may be applied to different fields in a given stream, such as UPPER( ) and LOWER( ). The transforms may be applied either (a) in the \u201cfilter\u201d path, before evaluation and comparisons or (b) in the \u201cproject\u201d path so that a given field is converted before being output from the PSDP. This is especially useful for correcting collation sequences in preparation for the CPU performing a sort. More details of such a substitution table are contained in the above referenced co-peanding U.S. patent application entitled \u201cField Oriented Pipeline Architecture for a Programmable Data Streaming Processor,\u201d.","One preferred embodiment of the PSDP  is now described in further detail in connection with . As shown in , a PSDP  consists of a finite state machine called the Data Engine , which implements filter logic and other control operations; a JPU interface ; a disk interface (here the ATA interface ); First-In-First-Out (FIFO) memories  and ; and a DMA driver .","The PSDP  is in one sense an On-Line Analytic Processing (OLAP)-oriented disk drive interface. It contains logic that is capable of identifying records, filtering out the unwanted records, and selecting fields for return as the tuple sets. The PSDP  supports both a Programmed I\/O (PIO) Mode-2 for register access by the JPU  and a UDMA (Ultra-Direct Memory Access) mode-4 for data transfers.","The terms \u201cflow through\u201d and \u201cfiltered\u201d are used to differentiate DMA read modes. In flow-through mode, also referred to as \u201craw read\u201d mode, data moves directly from the input to the output of the Data Engine  without being filtered. Data that is filtered has been processed, perhaps by culling records via a comparison and\/or transaction ID processing (as described below), but typically by reformatting the records into tuple format, during which uninteresting fields can be dropped and PSDP-generated fields can be added. This process of culling records is called a \u201crestrict\u201d operation. The process of formatting fields into tuples is called a \u201cproject\u201d.","In filtering mode, disk blocks are pulled from a Disk FIFO , feeding them through the Block Header, Record Header, NULL Vector, Transaction ID, Field Parse, and Filter circuits in the Data Engine . Fields to be returned are pushed into the Memory FIFO .","There is of course also a DMA write mode, in which data from the JPU  flows through the DMA driver  directly to the ATA interface .","For all three DMA modes (write, raw read, and filtered read), the PSDP  shadows the read\/write disk command in order to control its own DMA state machines. It does not shadow the disk address or sector count, nor does it have access to the memory addresses. For writes and raw reads, the PSDP  blindly moves data from one interface to the other until the JPU  disables the mode. The JPU  knows the quantity of data to be moved for these modes and uses the disk and DMA controller  interrupts to identify the end of transfer. For filtered reads, the quantity of data to be transferred to memory is generally unknown, and the JPU  identifies the end of transfer from the disk and filter interrupts. All of the record info\u2014header and data\u2014can be projected during a filtered read, but the block header info can only be returned by a raw read. DMA data integrity is protected across the disk interface by an IDE CRC check.","As shown in , the Data Engine  includes filter logic , a data parser , header storage , transaction ID processing , error checking , and output tuple generator . In general, the data parser  is responsible for taking information from the disk  and formatting it into headers and fields so that the filter logic , header storage  and error checking  blocks can perform their respective tasks. The tuple generator  takes the output of the filter and TID processing  blocks and formats the results in a \u201ctuple\u201d, suitable for processing by the JPU  or host .","Raw user table data as read from the disk  is understood and interpreted by the data parser . In one preferred embodiment at the present time, user table data is stored on disk in 128 KB segments called \u201cblocks\u201d. Each block begins with an 8-word header, followed by 0 or more records. The format of the block header may be as follows:",{"@attributes":{"id":"p-0097","num":"0248"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"119pt","align":"left"}}],"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":["Block Header Field","Size","Details"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Magic number","4 B","identifies beginning of block,"]},{"entry":[{},{},"always \u201cFEEDFACE\u201d"]},{"entry":["CRC-32","4 B","not used"]},{"entry":["Block number","4 B","within the table, 0 based, only 19"]},{"entry":[{},{},"significant bits"]},{"entry":["Block address","4 B","starting sector number of the block"]},{"entry":["Block length","4 B","in bytes, including header, but"]},{"entry":[{},{},"not trailing 0's"]},{"entry":["Layout ID","4 B","like a version number on the"]},{"entry":[{},{},"data format"]},{"entry":["Table ID","4 B","the Postgres object ID that uniquely"]},{"entry":[{},{},"identifies the table"]},{"entry":["Sector count","1 B","defines block size, 0 means 256,"]},{"entry":[{},{},"as of this time, it's always 0 number of"]},{"entry":["Record count","3 B","records in the block, 0 means 0"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}}}},"The CRC-32 field is meant to be computed by software and written to the disk along with the rest of the block header. Its calculation was to include all data from the block number through the end of the last sector of the block, including any trailing 0's. Its primary purpose was to detect data corruption resulting from hardware or software bugs, but it could have detected disk data-retention problems as well. It is unrelated to the UDMA-mode CRC-16 calculation required by the ATA-5 specification, which only guards the physical interface between the PSDP  and disk-drive IO buffers.","The sector count is the number of sectors in the block, which must be from 1 to 256. Thus a 0 in this 1-byte field means 256. The sector count occupies the most-significant byte of the last word of the block header.","The record count is the number of records in the block, which may be 0. Although the record count occupies the least-significant three bytes of the last word of the block header, only 13 bits are used.","A record as read from disk  into the Data Engine  is typically composed of a record header and one or more data fields, where the record header consists of three special fields, a length, and a null vector. The special fields are the row number, created transaction ID, and deleted transaction ID. All of the record header entries are optional on a per-table (not per-record) basis. However, if the record has a null vector, it must also have a record length, but not vice versa. The allowed data types are described elsewhere below.",{"@attributes":{"id":"p-0102","num":"0253"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"119pt","align":"left"}}],"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":["Record Header Field","Size","Detail"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Row number","0 or 8 B","existence per RowNumberSize register"]},{"entry":["Created XID","0 or 8 B","existence per CreatedXIDSize register"]},{"entry":["Deleted XID","0 or 8 B","existence per DeletedXIDSize register"]},{"entry":["Record length","0 or 2 B","size per RecordLengthSize register"]},{"entry":["Record NULL","0 to 512 B","size per FieldCount register"]},{"entry":"vector"},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}}}},"The row number (sometimes called row_num) is the unique number of the row or record in the user's table. It is distinct from the row address (sometimes called row13 addr), which is the complete physical address of a row in node-table-block-record format. The row number is also distinct from the record number, which is the 0-based ordinal number of a record within a block. The record number is the final component of the row address. The row address is computed by the PSDP.","The created XID contains the number, or ID, of the transaction that created the record.","The deleted XID. In the preferred embodiment, records are not actually deleted. Rather, they are marked as deleted so they can be restored if the transaction that performed the deleting is rolled back. (There are system management tools to reclaim the space.) A value of 0 indicates the record has not been deleted. A value of 1 indicates that the record was created by a transaction that was rolled back.","These XIDs support visibility in a multi-version database system, as is described in a related application (visibility application)","The record length is the length of the record in bytes, excluding the row number and the transaction IDs, but including the record length, the record null vector, the data fields, and any pad bytes at the end of the record needed for proper alignment of the first item of the following record. Thus, it is the distance in bytes from the beginning of the record length field to the beginning of the next record. Note that although all records in a table must have the same makeup, record lengths may vary because of variable-length character fields. The RecordLengthSize register defines record length sizes of 0, 1, 2, and 4 bytes, but only 0 and 2 are used.","The record null vector specifies which fields in the record are null, thereby indicating validity, not existence. For instance, a null varchar is not the same as an empty one. The record null vector consists of an even number of bytes. If it exists, the record null vector has the same number of bits as the record has data fields, and computes the number of half-words in the null vector as (FieldCount +15)>>4. This vector is an array of bytes. Bit  of the byte immediately following the record length corresponds to the 0data field; bit  of that byte corresponds to the 7data field; bit  of the last byte of the word that contains the record length corresponds to the 8data field; and so on.","There are strict rules governing field order and alignment. Both the record and its first data field must start on a word boundary (addr[1:0]=0). All record fields are self-aligned up to word boundaries. This means that 16, 12, 8, and 4 byte fields are word-aligned, 2-byte fields are \u00bd-word-aligned (addr[0]=0), and 1-byte fields can start anywhere. The row number, created XID, and deleted XID are all 8 byte fields and do not require pad bytes to align them. If there is a record length but no record null vector, two pad bytes are required following the record length. If the record null vector exists, it immediately follows the record length and naturally starts on a two-byte boundary, but two pad bytes may be required following the record null vector to properly align the first data field.","The physical order of data fields, which often is not the same as the logical order, takes care of aligning non-character data fields; the physical order is N16, T12, N8, I8, F8, N4, I4, F4, D4, I2, D2, I1, C1, C2, . . . C16, V2. The fixed-length character fields are packed in as tightly as possible and are not aligned. Variable-length character fields start with a 2-byte length; they are \u00bd-word-aligned and may require a preceding pad byte. Up to three pad bytes may follow the record's last data field in order to align the next record. If so, they are counted in the length of the earlier record.","More details of TID processing as performed by the TID processing block , includes rollback are contained in our co-pending U.S. Patent Application entitled \u201cControlling Visibility in Multi-Version Database Systems\u201d, by Foster D. Hinshaw et al. filed on Aug. 22, 2003.","A detailed circuit diagram of the filter\/comparision logic  is shown in . The filter logic  supports up to 32 comparison (and hash) instructions. Each operates on a single field, which can be a header field, the row address, or a data field. Multiple instructions can operate on fields. Each instruction can perform two comparisons, using either a Data-String Register (DSR)  or temp registers  for the second operand(s); the two temp registers -, -lare used to store an early record field for comparison to a later record field. There are two programmable, byte-wide substitution tables  that simplify character field comparisons by switching to all upper or lower case, for example. The instruction results are combined in the use\/lose circuit . In the preferred embodiment, sixteen (16) different comparison function types are implemented by the logic units : e.g, true and false NOPs, the equality operators (=, !=, <, >=, >, <=), bit-vector join and its inverse, field is\/is not null, field begins\/does not begin with the operand, field contains\/does not contain the operand. Although every comparison can be programmed for every supported data type (which may include integer, floating point, fixed- and variable-length character types etc.), not all combinations of data types and comparison operations are useful. The equality and null comparisons are appropriate for all types. For character comparisons, the string length and ASCII value of the characters determine inequality, such that \u201cABC\u201d is less than \u201cABCD\u201d, and \u201cA\u201d is less than both \u201ca\u201d and \u201cB\u201d. If a field is null, every comparison except null will fail. The bit-vector join and its inverse are for the integer data type. Begins, contains, and their inverses are for character types, both fixed- and variable-length.","As in traditional systems, for bit joins a bit vector is created with \u20181\u2019s in the positions corresponding to those positions where the join condition is true. In the preferred embodiment, during the setup phase the PSDP is loaded with a 4096-bit vector and the specific field position of the streaming data which is to be matched to that bit vector. During the streaming phase, as each tuple streams through the PSDP, the lower 12 bits of that field are mapped into the bit vector to determine if the corresponding bit is set to \u201c1\u201d. If it is, then the join condition is \u201cTRUE\u201d, otherwise it is \u201cFALSE\u201d.","The SQL predicate \u201cEXISTS\u201d and other related predicates such as \u201cANY\u201d, \u201cALL\u201d, \u201cNOT IN\u201d, \u201cNOT EXISTS\u201d, \u201cIN\u201d are operated as an \u201cEXISTS JOIN\u201d type. This particular join type is implemented by modifying the nested loop hash, and merge join and bit join types. Typically, the smaller table is placed into memory with either a hash index or sorted with access methods of hash or binary. If the smaller table has a range smaller than 4096, then it is converted to a bit index and put into the PSDP as with the bit join above. During the streaming phase, as each tuple streams either through the PSDP  (in the case of exist bit joins) or through the CPU ; the smaller table is scanned to see if it contains (or does not contain) the target field from the streaming tuple.","A \u201cuse\/lose\u201d logic circuit consists of up to eight sum or product terms. Each term can accept an input from each of the 32 instructions. The term outputs can be individually inverted before they're combined by either a sum-of-products (SOP) or product-of-sums (POS) calculation. Normally the filter indicates which records are to be kept, but the output of the SOP\/POS calculation can be inverted to indicate which to reject. Taken altogether, the use\/lose options provide deMorgan's Law term minimization.","While the record's data fields are parsed and optionally filtered, another circuit determines whether the record is valid by comparing the created and deleted transaction identifiers (IDs) to a data structure called the Invisibility List. The list contains up to 32 entries. The first is always the ID of the current transaction, that is the transaction that created the current scan. The remainder define the transactions that are concurrent to the \u201ccurrent\u201d transaction. There are five modes: off, normal, dirty, silent, and rollback. Normal and dirty set a status bit in the returned tuple, as described in the section below on tuple formats; silent and rollback affect the tuple's return, in conjunction with the filter results.","A project function encompasses the selection of record fields, the generation of new fields, and the tuple formation and return. Tuples typically consist of a row number, some data fields, and a 2-byte length\/status, but they can also include the created and\/or deleted transaction IDs, the row address, up to 255 pad words, the 32 instructions results formed into a boolean word, the hash result, and a null vector.","The hash is used to organize similar tuples into groups for processing joins or grouping selects, and with the exception of the record null vector and length\/status, all record-header and data fields can be used in its calculation. There are 7 defined hash modes, such as full CRC, which calculate a 32-bit CRC hash starting with a seed of zero and using all of the bytes of all of the fields selected. Blank spaces in character fields are skipped, as are leading 0's in unsigned and positive numerics and leading 1's in negative numbers. Hash operations are defined on a per-field basis by the comparison instructions.","Within the PSDP , a \u201ctuple\u201d is used to describe the projected data as output by the tuple . The tuple generator  uses principally the filter  output but can also use TID processing  and error checking  outputs (). The term \u201ctuple\u201d is used here for the purpose of differentiating \u201craw\u201d disk  and PSDP  output record formats. A tuple contains fields projected from the source record and up to six \u201cvirtual\u201dfields: row address, pad words (tuple scratch pad), the boolean results from each of the filter operations, a hash result, the tuple null vector, and the tuple length. All are optional on a per-table basis. The order of these fields is given in the following table.",{"@attributes":{"id":"p-0120","num":"0271"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"77pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"98pt","align":"left"}}],"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":["Tuple Field","Size","Details"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Row number","0\/8 B","from record header; upper two"]},{"entry":[{},{},"bytes are 0"]},{"entry":["Created XID","0\/8 B","from record header; upper two"]},{"entry":[{},{},"bytes are 0"]},{"entry":["Deleted XID","0\/8 B","from record header; upper two"]},{"entry":[{},{},"bytes are 0"]},{"entry":["Row Address","0\/8 B","node.table.block.record"]},{"entry":["Pad Words","0-256 W","Zeroed entries between specials"]},{"entry":[{},{},"and fields."]},{"entry":["Data Fields","0-nB","the data selected for return"]},{"entry":["Boolean Filter Result","0\/4 B","32 bit results of the (up to) 32"]},{"entry":[{},{},"instructions."]},{"entry":["Hash Result","0\/4 B","computed by PSDP"]},{"entry":["Tuple Null vector","0-512B","computed by PSDP; size known"]},{"entry":[{},{},"by software"]},{"entry":["Tuple length and Status","0\/1\/2\/4 B","computed by PSDP; the"]},{"entry":[{},{},"tuple length in bytes; size per"]},{"entry":[{},{},"TupleLengthSize register."]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}}}},"The row number, created XID, deleted XID, and data fields are the same as described above.","The row address is a compressed version of the node, table, block, and record information. RowAddress[63:32] is defined by the NodeIDTableID register, a 32-bit register that is programmed with a 32-bit, merged version of the node ID and the table ID as part of the filter setup. RowAddress[31:13] is the 19-bit block number defined by the block header. RowAddress[12:0] is the 13-bit record number calculated by the PSDP ; it is 0-based within the current block.","Software may define up to 255 pad words in the tuple immediately following the special fields.","The Boolean filter result contains the pass\/fail result for each of the 32 filter instructions.","The hash result is the output of the hash circuit.","The tuple null vector contains the record null vector bits for each data field software requested. Note that record and tuple null vectors do not generally match up. The tuple null vector must consist of an even number of bytes and begin on a two-byte boundary. Software ignores any undefined bits as they may have been set by a previous scan. Once again, the existence of the null vector requires the existence of the length. Like the record null vector, the least-significant bit of byte 0 of the null vector refers to the 0field; the most-significant bit of byte 0 refers to the 7field; the least-significant bit of byte 1 refers to the 8field, and so on, but the alignment and therefore the location of each of the bytes is different.","The tuple length is the total length of the tuple in bytes, including leading specials at the beginning of the tuple and any pad bytes at the end needed for proper alignment of the first item in the following tuple. Although all tuples returned by a scan must have the same makeup, tuples sizes may vary due to variable-length character fields. The TupleLengthSize register defines tuple length sizes of 0, 1, 2, and 4 bytes. Because tuple fields are 4-byte aligned, tuple lengths are always multiples of four, and the least-significant two bits of the tuple length are available to indicate tuple status. Bit  is the overrun bit. When set, it means the tuple was returned despite failing to meet the filter conditions. This can happen if the tuple is so large that the PSDP must begin transferring it to JPU memory before the use\/lose decision can be made, as described above. Bit  is the invalid bit. When set, it means the record from which this tuple was constructed has transaction IDs that make it invalid (i.e., invisibly created or visibly deleted).","With the exception of the length and nulls, tuple field alignments are the same as record field alignments. In the record the length and nulls precede the data fields, and the record null vector is left-aligned against the preceding record length. In the tuple the length and nulls follow the data fields, and the tuple null vector is right-aligned against the tuple length, which ends the tuple. The size of the tuple null vector and the requirement that it end in byte lane 1 together determine the location of its first byte: byte lane 0 or 2 (see the examples below). Aligning the tuple length in this manner makes it possible for software to locate the length while striding through the tuples backwards in memory. CPU software leaves enough space in memory at the head of the first tuple for a tuple length and null vector. This space allows relocating the length and null vectors ahead of the corresponding data fields as it walks the tuples backwards, then reverse direction to process the tuples forward. Alignment can require as many as five pad bytes between the last byte of data and the tuple null vector or length and, if neither null vector nor length exists, as many as three pad bytes following the last data byte.","Alignment examples for valid end of field data, tuple null vector, and tuple length:",{"@attributes":{"id":"p-0130","num":"0281"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"No Length or Nulls"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"6"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"5","colwidth":"35pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Address","0","1","2","3"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"5","align":"center","rowsep":"1"}}]},{"entry":[{},"I","data","data","data","data"]},{"entry":[{},"i + 4","data","data","data","data"]},{"entry":[{},"i + 8","data","data","data","data"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"5","align":"center","rowsep":"1"}}]}]}}]}}},{"@attributes":{"id":"p-0131","num":"0282"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"No Length or Nulls"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"6"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"5","colwidth":"35pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Address","0","1","2","3"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"5","align":"center","rowsep":"1"}}]},{"entry":[{},"i","data","data","data","data"]},{"entry":[{},"i + 4","data","data","data","data"]},{"entry":[{},"i + 8","data","pad","pad","pad"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"5","align":"center","rowsep":"1"}}]}]}}]}}},{"@attributes":{"id":"p-0132","num":"0283"},"tables":{"@attributes":{"id":"TABLE-US-00006","num":"00006"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Length but No Nulls"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"6"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"4","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"5","colwidth":"42pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Address","0","1","2","3"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"5","align":"center","rowsep":"1"}}]},{"entry":[{},"I","data","data","data","data"]},{"entry":[{},"i + 4","data","data","data","data"]},{"entry":[{},"i + 8","pad","pad","length 1","length 0"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"5","align":"center","rowsep":"1"}}]}]}}]}}},{"@attributes":{"id":"p-0133","num":"0284"},"tables":{"@attributes":{"id":"TABLE-US-00007","num":"00007"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Length but No Nulls"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"6"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"4","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"5","colwidth":"42pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Address","0","1","2","3"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"5","align":"center","rowsep":"1"}}]},{"entry":[{},"i","data","data","data","data"]},{"entry":[{},"i + 4","data","pad","pad","pad"]},{"entry":[{},"i + 8","pad","pad","length 1","length 0"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"5","align":"center","rowsep":"1"}}]}]}}]}}},{"@attributes":{"id":"p-0134","num":"0285"},"tables":{"@attributes":{"id":"TABLE-US-00008","num":"00008"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Length and 2-Byte Null"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"6"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"4","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"5","colwidth":"42pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Address","0","1","2","3"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"5","align":"center","rowsep":"1"}}]},{"entry":[{},"I","data","data","data","data"]},{"entry":[{},"i + 4","data","data","data","data"]},{"entry":[{},"i + 8","null 0","null 1","length 1","length 0"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"5","align":"center","rowsep":"1"}}]}]}}]}}},{"@attributes":{"id":"p-0135","num":"0286"},"tables":{"@attributes":{"id":"TABLE-US-00009","num":"00009"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Length and 4-Byte Null"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"6"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"4","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"5","colwidth":"42pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Address","0","1","2","3"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"5","align":"center","rowsep":"1"}}]},{"entry":[{},"i","data","pad","pad","pad"]},{"entry":[{},"i + 4","pad","pad","null 0","null 1"]},{"entry":[{},"i + 8","null 2","null 3","length 1","length 0"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"5","align":"center","rowsep":"1"}}]}]}}]}}},"As an aid in the illustrating how the system  processes data, an example database will be described that contains store sales data. The database defines a SalesDetail data table, a Customer data table, and a Store data table as follows:\n\n","A sample query might be to \u201cshow me the total units and dollar amount of rain gear sold to females in North Carolina in 2000, by customer ID.\u201d This can be translated into the SQL statement:\n\n","An output from this sample query with the total units and dollar amount of rain gear sold to females in North Carolina in 2000 by customer ID might be shown in tabular format:",{"@attributes":{"id":"p-0139","num":"0314"},"tables":{"@attributes":{"id":"TABLE-US-00010","num":"00010"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"84pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"91pt","align":"center"}}],"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":["CustID","Sales Units","Sales Amount"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"84pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"91pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["021442","1,300","$45,000"]},{"entry":["021443","1,200","$41,000"]},{"entry":["021449","1,800","$60,000"]},{"entry":["021503","3,500","$98,000"]},{"entry":["021540","4,200","$112,000"]},{"entry":["021599","5,000","$150,000"]},{"entry":["021602","4,700","$143,000"]},{"entry":["021611","4,100","$104,000"]},{"entry":["021688","3,600","$101,000"]},{"entry":["021710","2,000","$65,000"]},{"entry":["021744","1,200","$41,000"]},{"entry":["021773","1,500","$43,000"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}]}}},"Using the above example, a basic execution plan can be created by the SQL Expression Evaluator , plan generator  and plan optimizer  of the host(s) . The plan might specify for example, to perform joins and aggregations on the JPUs , with restriction functions being performed on the Programmable Streaming Data Processor (PSDP) .",{"@attributes":{"id":"p-0141","num":"0316"},"tables":{"@attributes":{"id":"TABLE-US-00011","num":"00011"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"168pt","align":"left"}}],"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":["Job","Locale","Operation"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["1","JPU","SCAN Customer"]},{"entry":[{},"PSDP","RESTRICT Gender = \u201cFemale\u201d"]},{"entry":[{},"JPU","PROJECT CustomerID"]},{"entry":[{},"JPU","SAVE AS TEMPCustomer"]},{"entry":["2","JPU","SCAN Store"]},{"entry":[{},"PSDP","RESTRICT StoreLocation = \u201cNC\u201d"]},{"entry":[{},"JPU","PROJECT StoreID"]},{"entry":[{},"JPU","BROADCAST AS TEMPStore"]},{"entry":["3","JPU","SCAN SalesDetail"]},{"entry":[{},"PSDP","RESTRICT ProductCategory = \u201cRaingear\u201d AND"]},{"entry":[{},{},"Year(SaleDate) = \u201c2000\u201d"]},{"entry":[{},"JPU","PROJECT CustomerID, StoreID, Units, Amount"]},{"entry":["4","JPU","JOIN WITH TEMPStore, StoreID = TEMPStore.StoreID"]},{"entry":[{},"JPU","PROJECT CustomerID, Units, Amount"]},{"entry":["5","JPU","JOIN WITH TEMPCustomer, CustomerID ="]},{"entry":[{},{},"TEMPCustomer.CustomerID"]},{"entry":[{},"JPU","PROJECT CustomerID, Units AS \u201cUnits\u201d, Amount AS"]},{"entry":[{},{},"\u201cAmt\u201d"]},{"entry":["6","JPU","GROUP By CustomerID"]},{"entry":[{},"JPU","AGGREGATE Sum(Units) AS \u201cUnits\u201d, Sum(Amt) AS"]},{"entry":[{},{},"\u201cAmtTotal\u201d"]},{"entry":[{},"JPU","PROJECT CustomerID, \u201cUnits\u201d, \u201cAmtTotal\u201d"]},{"entry":[{},"JPU","RETURN HOST"]},{"entry":["7","HOST","RETURN USER"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}}}},"Referring back to , the query is passed from the application (which may be running on, Business Intelligence Application  local application server  or client ), the Plan Generator  then creates tentative execution plans. Plans not only specify the above job descriptions, but also may specify whether specific jobs can run on currently or must run in sequence on the JPUs. The Plan Optimizer  selects one of the plans and optimizes that plan and passes it to the Plan Link . The Plan Link  expands the plan as necessary, based on where parts of the plan will be executed, and then passes the expanded plan to the Host Dispatch . The Host Dispatch  then sends individual Jobs within the plan to the respective locales (i.e., the JPUs ) for execution. In this example, jobs 1-6 are sent to the JPUs  for execution, with job 7 reserved for the host .","For example, Job 1 scans the Customer table with the required restriction and projection, and materializes it. Job 2 scans the Store table with the required restriction and projection, and since it is a small table, broadcasts the resulting tuple set to all JPUs , where the tuples from all JPUs  are then accumulated and saved in memory as TEMPStore. Jobs 1 and 2 are specified or determined to run concurrently if possible.","The Host Dispatch  may thus combine Jobs 3-6 into one streaming job because they can all be implemented in a streaming manner without materialization of intermediate sets. This combined job scans the SalesDetail table, with its restrictions and projections. As the tuples are received from scan run by the PSDP , each tuple is joined with TEMPStore and TEMPCustomer and aggregated. On the aggregation node, as each new customer ID is received, the previous one and its sums are sent to the host, where Job 7 is then invoked in a streaming fashion, to return the aggregated tuples through the ODBC connection  back to the user.",{"@attributes":{"id":"p-0145","num":"0320"},"figref":"FIG. 7","b":"22"},{"@attributes":{"id":"p-0146","num":"0321"},"tables":{"@attributes":{"id":"TABLE-US-00012","num":"00012"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"154pt","align":"left"}}],"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["3","JPU","SCAN SalesDetail"]},{"entry":[{},"PSDP","RESTRICT ProductCategory = \u201cRaingear\u201d AND"]},{"entry":[{},{},"Year(SaleDate) = \u201c2000\u201d"]},{"entry":[{},"JPU","PROJECT CustomerID, StoreID, Units, Amount"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}}}},"Individual jobs are forwarded from the host  to typically many JPUs  in parallel as a broadcast message. The broadcast message is typically sent as a User Datagram Protocol (UDP) type message, but can also be sent in other ways, such as a Transmission Control Protocol (TCP) message or a unicast message.","Upon receipt of a job message at the job listener , the JPU dispatch unit  informs the transaction manager  and storage manager  to then schedule Job 3 for execution. More details of job execution can be found in the related co-pending U.S. Patent application entitled \u201cProgrammable Data Streaming Architecture Having Autonomous and Asynchronous Job Processing Unit,\u201d (application Ser. No. 10\/667,128) mentioned above.","While this invention has been particularly shown and described with references to preferred embodiments thereof, it will be understood by those skilled in the art that various changes in form and details may be made therein without departing from the scope of the invention encompassed by the appended claims."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The foregoing and other objects, features and advantages of the invention will be apparent from the following more particular description of preferred embodiments of the invention, as illustrated in the accompanying drawings in which like reference characters refer to the same parts throughout the different views. The drawings are not necessarily to scale, emphasis instead being placed upon illustrating the principles of the invention.",{"@attributes":{"id":"p-0041","num":"0058"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0042","num":"0059"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0043","num":"0060"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0044","num":"0061"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0045","num":"0062"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0046","num":"0063"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0047","num":"0064"},"figref":"FIG. 7"}]},"DETDESC":[{},{}]}
