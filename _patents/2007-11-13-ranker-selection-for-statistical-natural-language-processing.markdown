---
title: Ranker selection for statistical natural language processing
abstract: Systems and methods for selecting a ranker for statistical natural language processing are provided. One disclosed system includes a computer program configured to be executed on a computing device, the computer program comprising a data store including reference performance data for a plurality of candidate rankers, the reference performance data being calculated based on a processing of test data by each of the plurality of candidate rankers. The system may further include a ranker selector configured to receive a statistical natural language processing task and a performance target, and determine a selected ranker from the plurality of candidate rankers based on the statistical natural language processing task, the performance target, and the reference performance data.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07844555&OS=07844555&RS=07844555
owner: Microsoft Corporation
number: 07844555
owner_city: Redmond
owner_country: US
publication_date: 20071113
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION","EXAMPLES"],"p":["Statistical natural language processing (SNLP), which employs statistical techniques to automatically generate and analyze natural human languages, often requires parameter estimation for various statistical models employed for performing SNLP tasks. To achieve this end, various parameter estimation algorithms have been used. However, each parameter estimation algorithm may perform differently when applied to different types of SNLP tasks. Further, performance targets, such as training time, runtime speed, memory footprint, and accuracy, may vary depending on the type of application under development. For example, a web-based application may utilize frequent updates of the statistical model and a large memory footprint, while a mobile device application may utilize less frequent updates and a small memory footprint. These variations make it difficult for a software developer to select an appropriate parameter estimation algorithm for each project.","Selection of a parameter estimation algorithm that does not perform well for a particular SNLP problem can be undesirably time consuming, resulting in wasted processing time. Current systems are unable to suitably predict performance of a parameter estimation algorithm for different types of SNLP task. Therefore it is difficult for developers to select a parameter estimation algorithm suitable to a particular SNLP task.","Systems and methods for selecting a ranker for statistical natural language processing are provided. One disclosed system includes a computer program configured to be executed on a computing device, the computer program comprising a data store including reference performance data for a plurality of candidate rankers, the reference performance data being calculated based on a processing of test data by each of the plurality of candidate rankers. The system further includes a ranker selector configured to receive a SNLP task and one or more performance targets, and determine a selected ranker from the plurality of candidate rankers based on the natural language processing task, the performance targets and the reference performance data.","This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter. Furthermore, the claimed subject matter is not limited to implementations that solve any or all disadvantages noted in any part of this disclosure.",{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 1","b":["10","10","12","14","14","15"]},"The computer program  may include a data store  and a ranker selector . The data store  is configured to store, either temporarily or in a nonvolatile manner, a plurality of candidate rankers , as well as reference performance data  for each of the plurality of candidate rankers , and a test data set  to measure the relative performance of each of the candidate rankers . The test data set , for example, may include a sample of untranslated texts and corresponding verified translations (also referred to as reference translations).","The candidate rankers  may be any suitable rankers used for parameter estimation for statistical natural language models for performing natural language processing tasks. For example, the plurality of candidate rankers  may include: (1) maximum entropy estimation with L2 regularization (MEEL2) ranker using maximum entropy estimation with L2 regulation (MEEL2) algorithm, (2) maximum entropy estimation with L1 regularization (MEEL1) ranker using maximum entropy estimation with L1 regulation (MEEL1) algorithm, (3) averaged perceptron (AP) ranker using an averaged perceptron (AP) algorithm, (4) Boosting ranker using a Boosting algorithm, and (5) BLasso ranker using a Boosting with Lasso with L1 regularization (BLasso) algorithm, as explained in detail below.","The computer program  may further include a test engine  configured to calculate the reference performance data  for each of the plurality of candidate rankers  based on a processing of the test data set . The test engine  may be configured to cause each candidate ranker  to utilize its SNLP algorithms to output ranked translation alternatives. Each candidate ranker  may be judged for accuracy, memory footprint, processing time, and development time, and other factors, against a verified translation, and the result saved as reference performance data .","It will be appreciated that the test engine  may calculate the reference performance data  at some point in time prior to when a user is selecting a candidate ranker  with system , and that the test engine  and test data set  may be located on a different computing device than computing device . For example, the reference performance data  may be preloaded data that is calculated using a test engine  located at a manufacturer of computer program , and distributed by download or CD ROMs with the computing program .","The ranker selector  is configured to receive a statistical natural language processing (SNLP) task  and one or more performance targets  from the user, and determine a selected ranker  from the plurality of candidate rankers  based on the SNLP task  and the performance targets  received from the user, and the reference performance data  that has been calculated by test engine .","The SNLP task  may be any suitable task in statistical natural language processing that can be formulated as a ranking problem under the framework of a linear model. Example SNLP tasks  include but are not limited to, parse selection, language model adaptation, word segmentation, part-of-speech tagging, text categorization, text clustering, lexical acquisition, word-sense disambiguation, word class induction, syntactic parsing, semantic interpretation, grammar induction, machine translation, and combinations of the above.","The performance targets  may include any suitable performance targets for statistical natural language processing, including but not limited to accuracy, memory footprint, processing time, development time, model sparsity, model training time, etc.","The determination of the selected ranker  by the ranker selector  may be carried out, for example, by comparing the performance targets  with the reference performance data  for the plurality of candidate rankers . A candidate ranker of the plurality of candidate rankers  having the reference performance data  meeting the performance targets  and\/or performing superiorly as compared to the other candidate rankers  based on the performance targets , may be selected as the selected ranker .","The computer program  may be configured to display a graphical user interface (GUI)  on a display  associated with the computing device . The GUI  may include a user input pane  configured to receive user input of various parameters via a user input device , an output pane , and a recommend ranker selector . Upon selection of the recommend ranker selector  by a user, the computer program  is configured to determine the recommended ranker  based on the user input and to display the recommended ranker  in the output pane .","The user input pane  of the GUI  may be configured to display an SNLP task input tool  configured to receive user input indicating the SNLP task , and a performance targets input tool  configured to receive user input indicating one or more performance targets  for the SNLP task . The SNLP task input tool  may be configured to display an SNLP task dropdown list including one or more SNLP task options  to be selected by the user. Alternatively, a task input tool  of another form may be used, such as a text input field, etc. SNLP task input tool  may further include a load SNLP task tool , by which a user may load a data file containing one or more stored SNLP tasks, for example.","The load performance target tool  may include a list of performance targets and associated user editable values  corresponding to one or more of the listed performance targets. In the depicted embodiment, the performance targets are illustrated as accuracy, memory footprint, processing time, and development time. It will be understood that other suitable performance targets may also be utilized, as listed above. A load performance targets tool  may also be provided, by which a user may load a data file containing one or more stored performance targets, for example.","The user input pane  of the GUI  may also include a test data input tool  configured to receive user input of the test data . The user input pane  may further include an load candidate ranker tool  configured to receive user input of the plurality of candidate rankers , and a load reference performance data tool  configured to receive user input of the reference performance data . Test data input tool , load candidate ranker tool , and load reference performance data tool  are depicted as buttons, which upon selection by a user, present a browsing menu through which a user may select an appropriate data file to load. Alternatively, other selection mechanisms may be utilized.","Upon input of the SNLP task  via SNLP task input tool  and performance targets  via a targets input tool , and input of any desired test data , reference performance data  or candidate rankers  via load test data tool , load reference performance data tool , and load candidate ranker tool , respectively, the recommend ranker selector  of the GUI  may be selected by a user. Upon selection, the GUI  is configured to send a request to the ranker selector  to cause the ranker selector  to determine a selected ranker  from the plurality of candidate rankers  based on the SNLP task  and the performance targets . The selected ranker  selected by the ranker selector  may be displayed in the output pane  of the GUI .","Using such a system, a software developer may efficiently select a ranker having a parameter estimation algorithm suitable to a particular SNLP task and particular performance targets.",{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 2","b":["100","100","14","12"]},"The method may include, at , providing a plurality of candidate rankers. Each of the candidate rankers may include a parameter estimation algorithm for SNLP models that perform SNLP tasks. The plurality of candidate rankers may include predefined candidate rankers provided with the computer program  and stored in the data store . In some examples, the plurality of candidate rankers  may include user defined candidate rankers received from user input via for example the GUI  of the system  in . The plurality of candidate rankers may include any number of suitable rankers for statistical natural language model parameter estimation, including: maximum entropy estimation with L2 regularization (MEEL2) ranker using a maximum entropy estimation with L2 regulation (MEEL2) algorithm, averaged perceptron (AP) ranker using an averaged perceptron (AP) algorithm, Boosting ranker using a Boosting algorithm, and\/or BLasso ranker using a Boosting with Lasso with L1 regularization (BLasso) algorithm.","At , the method may include providing reference performance data, the reference performance data being calculated based on a processing of test data by each of the plurality of candidate rankers. The test data may include predefined test data provided by the manufacturer of computer program , for example, and stored in the data store , and\/or may include user defined test data received from user input via a GUI, as described above. Likewise, the reference performance data may be predefined reference performance data provided by the manufacturer of computer program  and stored in a data store of the computer program, or the reference performance data may be user defined reference performance data received from user input via a GUI, for example.","At , the method may include receiving an SNLP task and one or more performance targets. The SNLP and performance targets may be received, for example, via a GUI as described above. Examples of suitable SNLP tasks and performance targets are provided above.","At , the method may include receiving a user request to select a selected ranker from among the plurality of candidate rankers, for example via a recommend ranker selection mechanism of a GUI. At , the method may include determining the selected ranker, via for example a ranker selector, from the plurality of candidate rankers based on the SNLP task, the performance target, and the reference performance data.","Determining the selected ranker from the plurality of candidate rankers may include comparing a measured parameter from the reference performance data for each of the plurality of candidate rankers with a desired parameter from the performance targets. In one example, the SNLP task may be received from user input received via an SNLP task input tool of a GUI, and the performance target may be received from user input received via a performance targets input tool of a GUI, as described above.","Determining the selected ranker at  may further include comparing the performance target with the reference performance data of a candidate ranker of the plurality of candidate rankers in performing a particular SNLP task. A candidate ranker of the plurality of candidate rankers may be selected as the selected ranker if the reference performance data of that candidate ranker in performing one or more SNLP tasks meets the performance targets received the user input.","As , the method may further include displaying the selected ranker on a display associated with the computing device.","The above described method may be utilized by a software developer to efficiently select a ranker having a parameter estimation algorithm suitable to a particular SNLP task and particular performance targets. The above described systems and methods may enable a user, such as a software developer, to select a ranker that is suited to a particular SNLP task based on the user's specific performance targets, such as model training time, runtime speed, memory footprint, and accuracy. Thus, for example, for applications that are web-based, and which have rapid model updates as a performance target, the ranker selector may be configured to select a selected ranker that may be trained and updated quickly. The averaged perceptron ranker may be selected under these circumstances. For applications for which accuracy is a performance target, the ME\/L2 ranker may be chosen, and for applications for which a small memory footprint is a performance target, the ME\/L1, may be chosen. Finally, for applications that have a very small memory footprint as a performance target and that may be able to sacrifice some accuracy, the ranker selector may be configured to choose the BLasso or Boosting ranker.","Examples of various example parameter estimation algorithms that may be used for performing parameter estimation in generating SNLP models are illustrated as follows and in reference to a hypothetical linear statistical natural language processing model F(x) for performing an SNLP task, such as parsing.","F(x) may be represented by the following equation:\n\n()=arg max\u03a6()\u00b7\n","where GEN(X) is a procedure for generating all candidate y for each input x, \u03a6(x, y) is the various extracted features, and w is a parameter vector that assigns a real-valued weight to each of the extracted features \u03a6(x, y).","The task of a parameter estimation algorithm is to use a set of training samples to choose a parameter w, such that the mapping F(x) is capable of correctly classifying an unseen example.","1. Maximum Entropy Estimation with L2 Regularization Algorithm","The maximum entropy estimation with L2 regularization (MEEL2) algorithm for parameter estimation operates by finding a parameter w where the sum of empirical loss on the training set, as represented by L(w), and a regularization term, as represented by R(w), is minimum, with R(w)=\u03b1\u03a3w. The MEEL2 algorithm may be represented by the following equation:",{"@attributes":{"id":"p-0037","num":"0036"},"maths":[{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mover":{"mi":"w","mo":"^"},"mo":"=","mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":["min","w"]},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"w"}},{"mi":"R","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"w"}}],"mo":"+"}}}}},"mo":",","mi":"where"}}},{"@attributes":{"id":"MATH-US-00001-2","num":"00001.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"w"}},{"mo":"-","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":"\u2062","mrow":{"mi":"log","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["y","i"]},{"mi":["x","i"]}],"mo":"|"}}}}}}],"mo":"="}}},{"@attributes":{"id":"MATH-US-00001-3","num":"00001.3"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":[{"mi":"R","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"w"}},{"mi":"\u03b1","mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mi":"j"},"mo":"\u2062","msubsup":{"mi":["w","j"],"mn":"2"}}}],"mo":"="},"mo":",","mi":"where"}}},{"@attributes":{"id":"MATH-US-00001-4","num":"00001.4"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["y","x"],"mo":"|"}}},"mo":"=","mfrac":{"mrow":[{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"\u03a6","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}},"mo":"\u00b7","mi":"w"}}},{"munder":{"mo":"\u2211","mrow":{"msup":{"mi":["y","\u2032"]},"mo":"\u2208","mrow":{"mi":"GEN","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":"\u2062","mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"\u03a6","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"x","mo":",","msup":{"mi":["y","\u2032"]}}}},"mo":"\u00b7","mi":"w"}}}}]}}}}]},"2. Maximum Entropy Estimation with L1 Regularization Algorithm","The maximum entropy estimation with L1 regularization (MEEL1) algorithm used for parameter estimation operates by finding a parameter w where the sum of empirical loss on the training set, as represented by L(w), and a regularization term, as represented by R(w), is minimum, but with R(w)=\u03b1\u03a3|w|. The MEEL1 algorithm may be represented by the following equation:",{"@attributes":{"id":"p-0040","num":"0039"},"maths":[{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mover":{"mi":"w","mo":"^"},"mo":"=","mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":["min","w"]},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"w"}},{"mi":"R","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"w"}}],"mo":"+"}}}}},"mo":",","mi":"where"}}},{"@attributes":{"id":"MATH-US-00002-2","num":"00002.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"w"}},{"mo":"-","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":"\u2062","mrow":{"mi":"log","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["y","i"]},{"mi":["x","i"]}],"mo":"|"}}}}}}],"mo":"="}}},{"@attributes":{"id":"MATH-US-00002-3","num":"00002.3"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":[{"mi":"R","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"w"}},{"mi":"\u03b1","mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mi":"j"},"mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"msub":{"mi":["w","j"]}}}}],"mo":"="},"mo":",","mi":"where"}}},{"@attributes":{"id":"MATH-US-00002-4","num":"00002.4"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["y","x"],"mo":"|"}}},"mo":"=","mfrac":{"mrow":[{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"\u03a6","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}},"mo":"\u00b7","mi":"w"}}},{"munder":{"mo":"\u2211","mrow":{"msup":{"mi":["y","\u2032"]},"mo":"\u2208","mrow":{"mi":"GEN","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":"\u2062","mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"\u03a6","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"x","mo":",","msup":{"mi":["y","\u2032"]}}}},"mo":"\u00b7","mi":"w"}}}}]}}}}]},"An orthant-wise limited-memory quasi-Newton (OWL-QN) algorithm, which is a modification of L-BFGS, may be used to iteratively minimize the objective function L(w)+R(w). In the OWL-QN algorithm, an L-BFGS algorithm is used to approximate the Hessian of the loss function, as indicated by L(w), which is then used to approximate the objective function L(w)+R(w) for a given orthant. When the L-BFGS algorithm is used to approximate the Hessian of the loss function, L(w), the L-BFGS algorithm maintains vectors of the change in gradient g\u2212gfrom the most iterations, and uses them to construct an estimate of the inverse Hessian H. At each step, a search direction is chosen by minimizing a quadratic approximation to the function:",{"@attributes":{"id":"p-0042","num":"0041"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"Q","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}},{"mrow":[{"mfrac":{"mn":["1","2"]},"mo":["\u2062","\u2062"],"msup":{"mrow":{"mo":["(",")"],"mrow":{"mi":"x","mo":"-","msub":{"mi":"x","mn":"0"}}},"mi":"\u2032"},"mrow":{"mi":"H","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"x","mo":"-","msub":{"mi":"x","mn":"0"}}}}},{"msubsup":{"mi":["g","\u2032"],"mn":"0"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"x","mo":"-","msub":{"mi":"x","mn":"0"}}}}],"mo":"+"}],"mo":"="}}}},"where xis the current iterate, and gis the function gradient at x. If H is positive definite, the minimizing value of x can be computed analytically according to x*=x\u2212Hg.","3. Boosting Algorithm","The Boosting algorithm optimizes or minimizes a pairwise exponential loss function, ExpLoss(w), which is defined as follows:",{"@attributes":{"id":"p-0046","num":"0045"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":["Exp","Loss"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mrow":{"mo":["(",")"],"mi":"w"}},{"munder":{"mo":"\u2211","mi":"i"},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"msub":{"mi":["y","i"]},"mo":"\u2208","mrow":{"mi":"GEN","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mo":["(",")"],"msub":{"mi":["x","i"]}}}}},"mo":"\u2062","mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mo":"-","mrow":{"mi":"M","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["y","i"]},{"mi":["y","j"]}],"mo":","}}}}}}}}],"mo":"="}}}},"Given a training sample (x, y), for each possible output yin GEN(x), the margin of the pair (x, y) with respect to a model w, M(y, y), is provided by the following equation:\n\n()=\u03a6()\u00b7\u2212\u03a6()\u00b7\n","The Boosting algorithm may use the following incremental feature selection procedure:\n\n",{"@attributes":{"id":"p-0049","num":"0050"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":["Exp","Loss"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mrow":{"mo":["(",")"],"mi":"w"}},{"munder":{"mo":"\u2211","mi":"i"},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"msub":{"mi":["y","i"]},"mo":"\u2208","mrow":{"mi":"GEN","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mo":["(",")"],"msub":{"mi":["x","i"]}}}}},"mo":"\u2062","mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mo":"-","mrow":{"mi":"M","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["y","i"]},{"mi":["y","j"]}],"mo":","}}}}}}}}],"mo":"="}}},"ul":{"@attributes":{"id":"ul0003","list-style":"none"},"li":{"@attributes":{"id":"ul0003-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0004","list-style":"none"},"li":"3. Update \u03bb\u2190\u03bb+\u03b4*, and return to Step 2."}}}},"After initialization, Steps 2 and 3 are repeated a predefined T number of times. At each iteration, a feature is chosen and its weight is updated.","First, define Upd(w,k,\u03b4) is defined as an updated model, with the same parameter values as w with the exception of w, which is incremented by \u03b4:\n\n(,\u03b4)=()\n","Then, Steps 2 and 3 may be rewritten as the following equations, respectively:\n\n(,\u03b4*)=arg minExpLoss((,\u03b4))\n\n(*,\u03b4*)\n","No regularization function is used in the Boosting algorithm, but a small fixed step size \u03b5 may be used to as an implicit regularization to minimize over fitting and number of test errors.","4. Boosted Lasso(BLasso) Algorithm","The BLasso algorithm optimizes an Lregularized exponential loss function and may be represented as follows:\n\nLassoLoss()=ExpLoss()+(), where\n\n()=\u03b1\u03a3|.\n","An incremental feature selection procedure similar to that used by the Boosting algorithm is used to learn parameter vector w. At each iteration, the BLasso algorithm takes either a forward step or a backward step. At each forward step, a feature is selected and its weight is updated according to the following equations:\n\n(,\u03b4*)=arg minExpLoss((,\u03b4))\n\n(*,\u03b5\u00d7sign(\u03b4*))\n","The exponential loss function LassoLoss(w) may be calculated with an update of either +\u03b5 or \u2212\u03b5, i.e., grid search may be used for feature weight estimation.","At each backward step, a feature is selected and the absolute value of its weight is reduced by \u03b5 if and only if it leads to a decrease of the exponential loss function LassoLoss(w), as shown in the following equations:\n\n*=arg minExpLoss((,\u2212\u03b5sign()\n\n(,\u03b1)\u2212LassoLoss(,\u03b1)>\u03b8\n","where \u03b8 is a tolerance parameter.","The BLasso algorithm may use the following incremental training procedure:","1. Initialize w: set w=arg minExpLoss(w), and W=0 for d=1 . . . D.","2 Take a forward step according to the following equations and the updated model is denoted by w:\n\n(*,\u03b4*)=arg minExpLoss((,\u03b4))\n\nw(*,\u03b5\u00d7sign (\u03b4*))\n","3. Initialize \u03b1=(ExpLoss(w)\u2212ExpLoss(w))\/\u03b5","4. Take a backward step if and only if it leads to a decrease of LassoLoss according to the following equations:\n\n*=arg minExpLoss((,\u2212\u03b5sign(w)\n\n(,\u03b1)\u2212LassoLoss(,\u03b1)>\u03b8\n\nwhere \u03b8=0; otherwise\n","5. Take a forward step according to Step 2 above; update \u03b1=min(\u03b1,(ExpLoss(w)\u2212ExpLoss(w))\/\u03b5; and return to Step 4 above.","5. Averaged Perceptron","The averaged perceptron algorithm optimizes a minimum square error (MSE) loss function. The averaged perceptron algorithm may use the following incremental training procedure:","1. Set w=1 for w=0 for d=1 . . . D","2. For t=1 . . . T (T=the total number of iterations)","3. For each training sample (x, y), i=1 . . . N","4. Choose an optimum candidate zfrom GEN(x) using the current model w,",{"@attributes":{"id":"p-0072","num":"0074"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":["z","i"]},"mo":"=","mrow":{"munder":{"mrow":[{"mi":["arg","max"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mrow":{"mi":"z","mo":"\u2208","mrow":{"mi":"GEN","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x_i"}}},"mo":"\u2062","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":{"mi":"\u03a6","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["x","i"]},"mo":",","mi":"z"}}},"mo":"\u00b7","mi":"w"}}}}}},"5. w=w+\u03b7(\u03a6(xy)\u2212\u03a6(xz)\u00b7w, where \u03b7 is the size of learning step, optimized on held-out data.","The averaged perceptron algorithm starts with an initial parameter setting and updates it for each training example. If wis the parameter vector after the itraining sample has been processed in pass t over the training data, the averaged parameters are defined as",{"@attributes":{"id":"p-0075","num":"0077"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mover":{"mi":["w","_"]},"mo":"=","mrow":{"mfrac":{"mn":"1","mi":"TN"},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mi":"t"},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mi":"i"},"mo":"\u2062","msup":{"mi":"w","mrow":{"mi":["t","i"],"mo":","}}}}}},"mo":","}}},"br":{}},"Example devices that may be used as computing device  include devices that electronically execute one or more programs, including but not limited to personal computers, servers, laptop computers, hand-held devices, portable data assistant (PDA), cellular phones and other micro-processor-based programmable consumer electronics and\/or appliances, routers, gateways, hubs and other computer networking devices, etc. The computing device  may typically include a processor connected via a bus to volatile memory (e.g., Random Access Memory), non-volatile memory (e.g., Read Only Memory), and a mass storage device (e.g., a hard drive). The computing device also may include user input devices such as a mouse and keyboard, a display device, and a media drive configured to read media, such as a Compact Disk-Read Only Memory (CD-ROM) or Digital Video Disk-Read Only Memory (DVD-ROM). Software programs including executable code for implementing the embodiments described herein may be stored and distributed on media, loaded onto the computing device  via the media drive, saved on the mass storage device, and executed using the processor and portions of volatile memory. The computer program  may be an application programming interface (API) configured to be a communication intermediary between an application program and the ranker selector .","The computer program  may generally include routines, objects, components, data structures, and the like that perform particular tasks or implement particular abstract data types. The computer program  may be a single program or multiple programs acting in concert, and may be used to denote both applications, services (i.e. programs running in the background), and an operating system.","It should be understood that the embodiments herein are illustrative and not restrictive, since the scope of the invention is defined by the appended claims rather than by the description preceding them, and all changes that fall within metes and bounds of the claims, or equivalence of such metes and bounds thereof are therefore intended to be embraced by the claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0006","num":"0005"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0007","num":"0006"},"figref":["FIG. 2","FIG. 1"]}]},"DETDESC":[{},{}]}
