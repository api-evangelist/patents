---
title: Remote semiconductor microscopy
abstract: A method and apparatus are described for remote semiconductor microscopy whereby video signals are broadcast from one or more microscopes to remote viewers. A live video signal is broadcast from the microscope over a network to remote personal computers located in the offices of process engineers. The office-based process engineers are provided real-time, or substantially real-time, views of a wafer, including peripheral views of the wafer outside cell array boundaries. The process engineer, in his office, can direct a technician, operating the microscope in the clean room complex, to display a desired cell region-of-interest with the microscope. As a result, the process engineers can more efficiently collaborate to solve process problems or even develop new process techniques.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=06859760&OS=06859760&RS=06859760
owner: Micron Technology, Inc.
number: 06859760
owner_city: Boise
owner_country: US
publication_date: 20030520
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"p":["This application is a Divisional of U.S. application Ser. No. 10\/118,844, filed Apr. 9, 2002 now U.S. Pat. No. 6,567,770, which is a Divisional of U.S. application Ser. No. 09\/298,502, filed Apr. 23, 1999, now issued as U.S. Pat. No. 6,370,487, which is based on U.S. Provisional Patent Application No. 60\/082,846 entitled \u201cHost Based Frame Monitor for Synchronized Video Acquisition and Compression\u201d filed Apr. 23, 1998, and U.S. Provisional Patent Application No. 60\/103,669 also entitled \u201cHost Based Frame Monitor for Synchronized Video Acquisition and Compression\u201d filed Oct. 9, 1998, all of which are incorporated herein by reference.","This invention relates generally to the field of semiconductor devices and, more particularly, to a method and system for inspecting semiconductor wafers via remote microscopy.","A portion of the disclosure of this patent document contains material that is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office patent file or records, but otherwise reserves all copyright rights whatsoever. The following notice applies to the software and data as described below and in the drawing hereto: Copyright 1999, Micron Technology, Inc., All Rights Reserved.","Microscopes are used to visually analyze the structural results of semiconductor processing. Fine features of semiconductor devices, such as transistor gates having sub-micron dimensions, are not readily visible to the human eye. Therefore, high performance microscopes, including scanning electron microscopes (SEMs) and scanning tunneling microscopes (STMs), are used to make these features visible. Semiconductor process engineers can, therefore, view these features to more efficiently diagnose problems that exist in semiconductor processes.","Conventionally, the images produced by microscopes are present only on monitors located with the microscopes. See Lampso, B. W. and Redell, D. D. (1980), , Communications of the AACM, Vol. 23, No. 2:105-117. Often, the microscopes are located in the clean room complex of a wafer fabrication facility in which semiconductor processing is performed. Thus, wafers can be inspected in the midst of semiconductor processing without their removal from the clean room complex. As a result, the wafers are less likely to be contaminated by undesired particles that exist in far greater quantity outside the clean room complex. However, because the microscopes are located within the clean room complex, process engineers must necessarily don clean room uniforms, or bunny suits, and enter the clean room complex to view the inspected wafers. This technique is particularly inefficient when the process engineers, who are not normally stationed in the clean room complex, are required to enter the clean room complex to view microscopy results.","To enhance the efficiency of wafer inspection by process engineers, the present invention provides for a method and apparatus for remote semiconductor microscopy whereby video signals are broadcast from one or more microscopes to remote viewers. In one embodiment, a live video signal is broadcast from the microscope over a network to personal computers located in the offices of process engineers. The office-based process engineers are provided real-time, or substantially real-time, views of a wafer, including peripheral views of the wafer outside cell array boundaries. The process engineer, in his office, can direct a technician, operating the microscope in the clean room complex, to display a desired cell region-of-interest with the microscope.","Further, multiple process engineers can simultaneously view the video signal from the microscope(s). As a result, the process engineers can analyze, in real-time, or substantially in real-time, the information provided by the video signals. In this way, the process engineers can more efficiently collaborate to solve process problems, or even develop new process techniques.","Therefore, it is a benefit of the present invention that it diminishes the time in which semiconductor microscopy is performed. It is a further benefit of the present invention that it permits multiple process engineers to simultaneously review microscope data in real-time, or near real-time.","The present invention provides a method and apparatus for remote microscopy useful to analyze semiconductor wafers. The term \u201cwafer\u201d used in the following description includes any structure having an exposed surface on which an integrated circuit (IC) is or may be formed. In another embodiment, the method and apparatus for remote microscopy may be used for other applications, including medical procedures. For example, during an operative procedure, and under the control of a pathologist, remote microscopy can be used to obtain diagnostic-quality images of microscopic tissue samples. The images are transmitted between geographically separated sites in real-time to permit remote consultation by other physicians. Further information about remote medical microscopy is provided in Dr. Gary J. Grimes, \u201cRemote Microscopy for Hi Res Real-Time Interactive Pathology,\u201d , p. 12, July 1997, hereby incorporated by reference.",{"@attributes":{"id":"P-00032","num":"00032"},"figref":"FIG. 1A","b":["100","100","102","102","102","104","106","102","110","108","102","108","104","108"],"i":["a","n"]},"The server  is coupled to a video capture system  by a network , such as a corporate intranet. In one embodiment, illustrated in , the network  comprises two subnetworks ,  coupled by a router . The first subnetwork  couples the video capture system  to the router . The second subnetwork  couples the router to the server . The second subnetwork  also couples the server  to the clients . In one embodiment, the video capture system , server , and clients -operate at 10 Megabits per second. In another embodiment, the router operates at 100 Megabits per second.",{"@attributes":{"id":"P-00034","num":"00034"},"figref":"FIG. 1C","b":["100","111","161","165","163","185","185","185","111","187","118","120","116","180","182","180","182","116","190","192","1"],"i":["a","n"]},"The video capture system  may be located within or outside the clean room complex . The video capture system , for example a computer, includes a video capture card  coupled to a computer . In one embodiment, when the video capture card has limited, for example, one, analog video inputs, then an analog multiplexor  may be used to couple analog video signals from multiple microscopes ,  to the video capture card . The analog multiplexor  can be manipulated directly, for example, by a microscope operator, or remotely through the system , for example, by a process engineer in an office , to select analog video signals , from one microscope to be broadcast to clients . Manipulation may be performed manually or electronically. In a further embodiment, the system  can control the analog multiplexor , for example, to automatically and sequentially select analog video signals  from the multiple microscopes , .","In another embodiment, when the video capture card  has a sufficient number of analog video inputs to uniquely couple each microscope to an analog video input, then an analog multiplexor  is not required in the system . In this embodiment, the multiplexor is part of the video capture card . Also, in this embodiment, the analog video inputs may be selected automatically by the system , or manually by the SEM operator or process engineer.","In yet another embodiment, the computer system operates in the following manner. The microscopes ,  provide analog video signals . The analog video signals  may be in the RS-170 (without color burst) or RS-170A (with color burst) formats. One embodiment of an analog video waveform in the RS-170 format is illustrated in FIG. D. Alternate embodiments of such an analog video waveform  would include finite rise and fall times not illustrated in FIG. D. Analog video waveforms are further described in K. Jack, , HighText, 1993, which is hereby incorporated by reference.","In one embodiment, the analog video  signal is coupled from the microscope to the video capture card  by a 75 ohm coaxial cable. If the video capture card  is located a substantial distance from the microscope, for example outside the clean room, a video distribution amplifier ,  should be inserted between the microscope and the video capture card , as illustrated in FIG. C. In another embodiment, each frame of analog video  corresponds to one progressive scan of a scanning electron microscope (SEM) or scanning tunneling microscope (STM). Frames of analog video  from a microscope are digitized by the video capture card . The digitized frames of analog video  are provided by the video capture system  over the network  to the server . In one embodiment, the connection between the video capture system  and the server  uses a point-to-point transport control protocol-Internet protocol (TCP-IP). The digitized frames of analog video  are then stored in the server .","In one embodiment, still frames of video are captured, compressed and inserted into a database. Each image has a unique identifier which can be associated with a wafer or a lot of wafers. Therefore, a process engineer can select a specific frame of interest from stream content, and save a specific frame into a database.","In yet another embodiment, the digitized frames of analog video  are streamed over the network  from the server  to the clients . In a further embodiment, the streaming video format can be the Advanced Streaming Format (ASF) (Microsoft\u00ae Corporation, Redmond, Wash.), further described in a document published by Microsoft\u00ae Corporation and Real Networks\u2122, Inc., entitled () , Feb. 11, 1998, hereby incorporated by reference, and which may be found on the World Wide Web at http:\/\/www.microsoft.com\/ asf\/whitepr\/asfwp.htm. Frames of digitized video data  are streamed in the ASF format by Netshow Server software operating on the server . The ASF video is played on the clients  by Netshow Player software. Netshow software is also a product of Microsoft\u00ae Corporation (Redmond, Wash.). However, the present invention can utilize other client-server streaming software, such as Real Video by Real Networks, Inc. (Seattle, Wash.).","In yet another embodiment, the digitized frames of analog video  can be stored on the server  as a file, such as in ASF, for viewing at a later time. Thus, microscopy video can be viewed remotely at a time substantially after the digitized frames of analog video data  have been captured by the video capture system .","The video capture system  will now be further discussed. A video capture card  having a relatively high frame rate is desirable. In one embodiment, the video capture card  is coupled to the memory and processor of the video capture system  by an Industry Standard Architecture (ISA) bus. An example of a video capture card, using an ISA bus, is a Winnov Videum VO (http:\/\/www.winnov.com). However, video capture cards that operate with ISA buses have limited bandwidth. For example, ISA buses operate with 16 bits at about 4 Megabytes-per-second. Thus, for example, the video capture card has a resolution of about 640\u00d7480\u00d78; its corresponding maximum frame buffer-to-host memory transfer rate on the ISA bus is (4 Megabytes\/Second)\/307,200 Bytes=13 Frames\/Second.","The relatively slow frame rate of the ISA compatible video capture card limits the frame rate of the video broadcast on the local area network  by the server . Therefore, a video capture card  having a higher frame rate is preferably used.","One embodiment of a video capture card  having a higher frame rate is illustrated in FIG. . The video capture card  is coupled to the memory  and processor  of the video capture system  by a Peripheral Component Interface (PCI), or IEEE-1394, bus . A PCI bus compatible video capture card  has greater bandwidth than an ISA bus compatible video capture card.","The video capture card  operating with a PCI bus  can be implemented with either Coreco Ultra II or F\/64 video capture cards. The F\/64 video capture card, which originally operated with an ISA bus, includes a high speed module on a daughter board to permit operation with the PCI bus . The PCI bus  has a maximum data rate of 132 Megabytes per second. However, generally, the PCI bus  operates at a data rate of about 80 Megabytes per second. For 640\u00d7480\u00d78 resolution, the PCI bus compatible video capture card  has a maximum frame buffer-to-host memory transfer rate of (80 Megabytes\/Second)\/307,200 Bytes=260 Frames\/Second, which is much greater than the 13 Frames\/Second rate of the ISA bus compatible video capture card. Because of its higher frame rate, the video capture card  operating with a PCI bus  can facilitate higher frame rates on a local area network .","The Coreco F\/64 will now be further described. The video capture card  includes an analog to digital (A\/D) converter . The A\/D converter  transforms one or more analog signals, such as analog video signals, into digital signals. Thus, in one embodiment, analog video signals from a microscope can be sampled and converted to digitized video signals  by the A\/D converter. The sampling rate and number of bits of the A\/D converter  will vary depending upon the type of A\/D converter  used. The A\/D converter  is coupled to a frame buffer  which captures and stores digitized frames of analog video . However, in an alternative embodiment, digitized frames of analog video  can be provided from a microscope directly to the frame buffer . The frame buffer  of the Coreco F\/64, for example, can store up to 32 Megabytes of data.","The Coreco F\/64 includes one or more digital signal processor(s) , such as graphics signal and histogram processors, coupled to the frame buffer . The digital signal processor(s)  may be used to manipulate, for example, capture, filter and\/or analyze, the digitized frames of analog video . A captured digitized frame of analog video  is stored in the frame buffer . The digitized frame can be provided efficiently from the video capture card  to a processor , such as a Pentium II processor (Intel Corporation, Santa Clara, Calif.), through the PCI bus  by direct memory accessing (DMA). As a result, the processor is not required to perform extra processing, such as generating addresses. Alternatively, the digitized frame can be provided to the memory  through the PCI bus .","The Coreco F\/64 can perform image processing, and the inventor has used it to explore digitized video data  of semiconductor microscopy. Specifically, the Coreco F\/64 has been used to detect motion by evaluating changes in subsequent frames.","Generally, a video signal contains inherent redundancies both spatially and in time. Spatial redundancies, or statistical dependencies among neighboring pixels, are present because naturally viewed images are generally smooth. In other words, video images comprise primarily low frequency content, in addition to structured texture regions and connected edge boundaries. Temporal redundancies, or time-related statistical dependencies, are a function of how fast or slow object scenes move, as is discussed in M. J. T. Smith and A. Docef, , Riverdale, Ga., Scientific Publisher, 1997, hereby incorporated by reference. Digitized frames representing a semiconductor wafer generally illustrate no motion, except when a stage of the microscope is panned or optics of the microscope are adjusted. Thus, successive digitized frames of a semiconductor wafer are generally very similar to one another.","The static nature of digitized frames of semiconductor wafers can be verified by using the real-time histogram processor (Texas Instruments, Dallas, Tex.) resident on the Coreco F\/64. See, -\/64 , Edition 1.0, Revision 2, Coreco, Inc., p. 3-7, June 1994; http:\/\/www.coreco.com. The real-time histogram processor can analyze multiple sets of two successive (i.e., first and second) digitized frames of a semiconductor wafer. As a result, a relatively slow video frame rate of 5 frames-per-second was found to be adequate for remote microscopy of semiconductor wafers. Also, generally, the difference between means of the video information in the sets of first and second frame, approached zero. For this reason, the video data of semiconductor microscopy was found to be a suitable candidate for compression, or encoding.","Therefore, in one embodiment, the video capture system  includes a video encoder, such as found in the Duck True Motion Real-Time encoder-decoder (CODEC) (Duck Corporation, New York, N.Y.), which encodes, or compresses, the captured frames of digitized video, and converts them into the ASF. The HBFM can be implemented using the Component Object Model (COM) (Microsoft\u00ae Corporation, Redmond, Wash.), further described in a document published by Microsoft\u00ae Corporation entitled , version 0.9, Oct. 24, 1995. The Duck True Motion Real-Time CODEC is implemented in software, and is an In-Process Active X component that is loaded into an existing apartment when the COM client, Host Based Frame Monitor, calls CoCreateInstance.","Encoding in the present invention can be implemented in the following ways. In one embodiment, the Duck True Motion Real-Time CODEC can reside in the memory , volatile or non-volatile, fixed or removable, within the video capture system . The CODEC would then be executed by the processor  in the video capture system. In another embodiment, the CODEC can reside in memory on the video capture board , and be executed by a processor  on the video capture board.","The Duck True Motion Real-Time CODEC uses a wavelet compression algorithm. Currently, the Duck True Motion Real-Time CODEC can compress frames with a resolution of up to 320\u00d7240\u00d724, and at a frame rate of 30 frames-per-second. Because the output resolution of a SEM or STM is typically only an 8 bit grey scale, the Duck True Motion Real-Time CODEC is capable of being modified to handle higher frame rates provided by a PCI bus compatible video capture board, such as the Coreco F\/64.","Using compression the efficiency of the video capture system  can be enhanced. In one embodiment, the statistical data output of the video capture card's histogram processor, described above, can be used to sense whether a scene change occurs from a first frame to a successive second frame, as described above. If the statistical data, such as the differential mean, is less than a threshold level, the video capture system  will retransmit the previously broadcast encoded first frame, which can be stored in memory , and not expend resources (e.g. processor time) to encode and transmit the second frame.","The compressed digitized video data is provided to the server  over the network . In one embodiment, the Netshow server streams ASF video files to the clients  over the network . The video compression, described above, minimizes the network  bandwidth required for broadcasting, either uni- or multicasting, the remote microscopy video to clients . In another embodiment, the Netshow player, resident on the clients , also includes the Duck True Motion Real-Time CODEC, to permit decompression of the video before it is displayed on the client .","However, the capture or grabbing of video data, for example by the video capture card , and the transmission of digitized video data from a high-speed bus, such as a PCI bus, to the memory  or the processor  must be coordinated with real-time video compression. Also, as illustrated in , multiple video sources (e.g. SEMs) may be coupled to the video capture system . Therefore, the system  also needs a technique to permit and coordinate the capture of video signals from multiple sources.","Therefore, in another embodiment, the present invention provides a Host-Based Frame Monitor (HBFM). In one embodiment, the HBFM is a software system stored on a computer-readable medium and performed by the processor  of the video capture system . The HBFM coordinates frame capture, video data transfer along the high speed bus, and real-time encoding of video signals from multiple sources. The HBFM can also be used to integrate otherwise incompatible imaging components, such as a video capture card  and CODEC software. The HBFM achieves this integration by segregating and synchronizing the processing of each digitized frame of the analog video . For example, the HBFM ensures that write operations (such as analog-to-digital acquisition) and read operations (such as compression) are performed mutually exclusively. Also, the HBFM permits read operations to be executed in parallel to the write operations.","In one embodiment, the HBFM is implemented in software, rather than hardware, so that any number of threads may be created dynamically at run-time to service many application-specific digital image processing needs. For example, for a single frame grabber resource, which may be a video capture card , one thread can grab a frame of video, another thread can compress another frame of video data, while yet another thread performs edge detection analysis on another frame of video data that is being compressed. Like the CODEC, the HBFM can reside and be executed in either the video capture card , or the video capture system . In another embodiment, the HBFM can reside in memory, volatile or non-volatile, fixed or removable.","In a further embodiment, the HBFM is implemented with object-oriented software, as described in Rumbaugh et al., -, Prentice Hall, 1991, hereby incorporated by reference. The Appendix illustrates an exemplary embodiment of an Host Based Frame Monitor  that ensures that frames of video data are grabbed and compressed, or otherwise processed, in an orderly and synchronized manner. The embodiment illustrates an object-oriented implementation including classes used within the HBFM software system and the corresponding methods that collectively provide an application programming interface for retrieving and processing digitized video. In one embodiment, a producer thread object can be instantiated to grab video frame data from a resource, such as a SEM, and store the video frame data in a frame buffer object. A consumer thread object can also be instantiated to perform real-time encoding of other video frame data in another frame buffer object.",{"@attributes":{"id":"P-00060","num":"00060"},"figref":"FIG. 5","b":["300","302","302","304","306","304","310","114","312","306","102","302","304","306","302"]},"If producer thread object  cannot immediately access corresponding HostFrameBuffer  then an identifier for producer thread object , such as a pointer, is placed in Queue object . Queue object  is instantiated at this time, if it does not already exist. Upon completing the grabbing of the frame, the ProduceFrame method invokes the StopGrabbing method of HBFM  to indicate that it has finished populating HostFrameBuffer  so that any ConsumerFrameThread  can begin operating upon the frame.","In one embodiment, the producer thread object  and consumer thread object  are executed inside a single process. Note, the HBFM  does not define how an analog image is digitized or how a digital image is compressed, but rather HBFM  ensures that frames of video data are grabbed and compressed, or otherwise manipulated, in an orderly and synchronized manner.",{"@attributes":{"id":"P-00063","num":"00063"},"figref":["FIG. 3","FIG. 5"],"b":["300","302","1","2","3","312","5","1","1","2","2","3","3","302","1","2","3","1","2","3","1","1","2","2","3","3","306","5"]},"In one embodiment, each HBFM input signal source, such as a SEM signal, coupled to a single frame grabber resource , may be logically and uniquely associated with a distinct pair of producer and consumer threads as well as a corresponding HostFrameBuffer object . For example, referring to , if a frame grabber resource  is coupled to the outputs from three SEMs, then the most recent frame of digitized video from SEM  may be grabbed by the ProducerThread object, stored in HostFrameBuffer object, and compressed by the ConsumerThread object. The most recent frame of digitized video from SEM  may be grabbed by the ProducerThread object, stored in HostFrameBuffer object, and compressed by the ConsumerThread object. The most recent frame of digitized video from SEM  may be grabbed by the ProducerThread object, stored in HostFrameBuffer object, and compressed by the ConsumerThread object. The frames of digitized video are grabbed, stored and compressed in the manner described below.","However, for each HostFrameBuffer object HBFM  utilizes a single-producer\/multiple-consumer locking protocol such that HBFM  is able to support multiple consumers for each producer. This protocol comprises two mutually exclusive states: the producer (write) state and consumer (read) state. In the write state, each HostFrameBuffer object receives a frame of digitized video from only one corresponding producer thread object at any time. In one embodiment, only one HostFrameBuffer object receives a frame of digitized video from a producer thread object at any given time. However, each HostFrameBuffer object may provide a stored frame of digitized video to one or more consumer thread objects at any given time when the HostFrameBuffer object is not receiving digitized video data from a producer thread object. This protocol has two purposes: first, multiple consumer process objects may simultaneously access a frame of digitized video in a single host frame buffer, and second, access to each frame grabber resource or video source is serialized.","In one embodiment, a single frame grabber resource may be connected to three video sources, such as cameras or SEMs. Each video source is associated with a distinct HostFrameBuffer object, and a corresponding section of the memory . In one embodiment, two separate processes are executed in host memory, for example, in the memory  of the video capture system . A first process may be an application or producer thread object that captures still images. A second process may be an application or a consumer thread object that performs real-time encoding.","In another embodiment, a single-process, including single producer and multiple consumer thread objects, is performed in memory  of the video capture system . The multiple consumer thread objects are permitted parallel, shared access to one HostFrameBuffer object. However, when a produceFrame method is performed by the producer thread object, only the producer thread object can update the HostFrameBuffer object with another video data frame; no consumer thread objects, or other producer thread objects, are permitted to access the HostFrameBuffer.","In one embodiment, synchronization is achieved in the following manner. A produceFrame method invokes a startGrabbing method and stopGrabbing method, respectively, before and after every frame of digitized video is grabbed. Before grabbing a new frame, a produceFrame method invokes a startGrabbing method, to make sure it can begin grabbing the new frame. If a producer thread object is not permitted to begin grabbing, and accessing its corresponding HostFrameBuffer, then the producer thread object is placed in the GrabWaitQueue object. The GrabWaitQueue object is instantiated at this time, if it does not already exist.","Upon completing the grabbing of the frame, the ProduceFrame method invokes the StopGrabbing method to indicate that it has finished populating the HostFrameBuffer object so that any consumer thread object(s) in the CompressWaitQueue can begin operating upon the frame.","A ConsumeFrame method invokes the StartCompressing method and StopCompressing method, respectively, before and after compressing a frame of digitized video, in a HostFrameBuffer object. Before compressing a frame, each consumer thread object invokes the StartCompressing method, to ensure that a producer thread object is not currently writing to the HostFrameBuffer object. If a producer thread object is currently writing to the HostFrameBuffer object, the consumer thread object is not permitted access to the HostFrameBuffer, and is placed in the CompressWaitQueue object. If not already existing, the CompressWaitQueue object is instantiated at this time.","After compressing the frame of digitized video in a HostFrameBuffer object, the ConsumeFrame method invokes the StopCompressing method to signal that it has finished compression so that a producer thread object seeking to use the HostFrameBuffer can be activated.",{"@attributes":{"id":"P-00072","num":"00072"},"figref":"FIG. 4","b":["300","302","1","1","1"]},"Also at 1 millisecond, PT is placed in the GrabWaitQueue object because PT is not finished grabbing the frame. Only one producer thread object can access the frame grabber resource at a time. At 2 milliseconds, CT is placed in the CompressWaitQueue object because PT has not yet populated the HostFrameBuffer object. At 3 milliseconds, PT is placed in the GrabWaitQueue object because PT is still not finished grabbing the frame. Finally, at 4 milliseconds, PT finishes its frame grab and CT is permitted to access the frame stored in HostFrameBuffer object so that it can invoke the CODEC's CompressFrame operation. Thus, at 4 milliseconds, PT is permitted to proceed to write a frame to HostFrameBuffer object. Also, at 4 milliseconds, CT is placed in the CompressWaitQueue object because PT has not begun grabbing a frame.","For all producer threads PT-PT, the task of grabbing a frame is delegated to the FrameGrabber object; specifically its GrabFrame operation. For all consumer threads CT-CT, the task of compression (also called encoding) is delegated to the CODEC; specifically its CompressFrame operation. At 8 milliseconds, while CT delegates compression of the frame stored in HostFrameBuffer object to the CODEC object, PT finishes writing a frame. Thus, after 8 milliseconds, PT is removed from the GrabWaitQueue object, and proceeds to write a frame to HostFrameBuffer object. Further, CT is removed from the CompressWaitQueue object, and begins compressing the frame in HostFrameBuffer object.","At 10 milliseconds, CT finishes compressing the frame stored in HostFrameBuffer object. At 12 milliseconds, PT finishes writing the frame to HostFrameBuffer object. Thus, at this time, CT is removed from the CompressWaitQueue object, and begins compressing the frame stored in HostFrameBuffer object. Also, at 12 milliseconds, PT wants to produce a new frame, but cannot because CT is accessing the frame stored in HostFrameBuffer object. Therefore, PT is placed in the GrabWaitQueue object.","At 14 milliseconds, CT is placed in the Compress Wait Queue object because PT has not begun grabbing. At 15 milliseconds, PT also wants to produce a new frame, but cannot because PT is in the Grab Wait Queue object.","Therefore, PT is also placed in the Grab Wait Queue object after PT. At 16 milliseconds, PT also wants to produce a new frame, but cannot because PT and PT are in the Grab Wait Queue. Therefore, PT is also placed in the Grab Wait Queue object after PT and PT.","Once CT finishes compressing the frame stored in HostFrameBuffer object at 18 milliseconds, PT begins to write another frame to HostFrameBuffer object. Also at 18 milliseconds CT again wants to compress another frame stored in HostFrameBuffer object. Because PT has not completed writing another frame, CT is placed in the Compress Wait Queue object.","At 21 milliseconds, CT wants to compress another frame in HostFrameBuffer object. However, because PT has neither begun nor completed its writing of another frame to HostFrameBuffer object, CT is placed in the Compress Wait Queue object.","PT completes writing a frame at 22 milliseconds. Then, at 22 milliseconds, CT begins compressing this frame stored in HostFrameBuffer object. Also at 22 milliseconds, PT is removed from the GrabWaitQueue object, and proceeds to write another frame to HostFrameBuffer object.","At 26 milliseconds, PT finishes writing the frame to HostFrameBuffer object, and CT is permitted to compress the frame stored in HostFrameBuffer object. Also at 26 milliseconds, PT is moved off the GrabWaitQueue object, and begins writing a frame to HostFrameBuffer object. At 28 milliseconds, CT completes compressing the frame stored in HostFrameBuffer object.","PT stops grabbing the corresponding frame at 30 milliseconds. Thus, at 30 milliseconds, CT is taken from the CompressWaitQueue object, and begins compressing the frame stored in HostFrameBuffer object. CT and CT complete their compressions respectively at 32 and 36 milliseconds.","Various embodiment are described for remote semiconductor microscopy whereby video signals are broadcast from one or more microscopes to remote viewers. Although specific embodiments have been illustrated and described herein, it will be appreciated by those of ordinary skill in the art that any arrangement which is calculated to achieve the same purpose may be substituted for the specific embodiment shown. This application is intended to cover any adaptations or variations of the present invention. For example, those of ordinary skill within the art will appreciate that in one embodiment, a live video signal is broadcast from the microscope over a network to client computers located in the offices of process engineers. In another embodiment the process engineers can selectively view still images retrieved from a database. The client computers may receive the video signals via a local network or even a wide area network such as the Internet. In addition, the method and apparatus for remote microscopy may be used for other applications, including medical procedures.",{"@attributes":{"id":"P-00084","num":"none"},"ul":{"@attributes":{"id":"ul200001","list-style":"none"},"li":"This Appendix illustrates an exemplary embodiment of an Host Based Frame Monitor (HBFM) that ensures that frames of video data are grabbed and compressed, or otherwise processed, in an orderly and synchronized manner. The embodiment illustrates an object-oriented implementation including classes used within the HBFM software system and the corresponding methods that collectively provide an application programming interface for retrieving and processing digitized video.\n\nClass Name: HostBasedFrameMonitor\n"}},"Synchronizes production and consumption operations required for grabbing and compressing digital image frames.","Associations:","The HostBasedFrameMonitor is a specialized kind of Monitor. [Refer to FIG. A.]","One HostBasedFrameMonitor is part of one or more ProducerFrameThread(s) and the same HostBasedFrameMonitor is part of one or more ConsumerFrameThread(s). [Refer to FIGS.  and A.]","The HostBaseFrameMonitor is made up of two Queue(s). One Queue instance is the GrabWaitQueue, pGrab WaitQueue, and the other Queue is the CompressWaitQueue, pCompressWaitQueue. [Refer to FIGS.  and A.]","Attributes:","consumerCount\u2014The number of ConsumerFrameThreads who are accessing the HostFrameBuffer.","consumerWaitCount\u2014The number of ConsumerFrameThread who are waiting to enter the HostBasedFrameMonitor in order to access the HostFrameBuffer.","producerCount\u2014The number of ProducerFrameThreads who are accessing the HostFrameBuffer.","producerWaitCount\u2014The number of ProducerFrameThread(s) who are waiting to enter the HostBasedFrameMonitor in order to access the HostFrameBuffer. [Refer to FIG. E]","Operations:","StartCompressing\u2014If there are no other ProducerFrameThread(s) or ConsumerFrameThread(s) accessing or waiting to access the HostFrameBuffer, then the ConsumerFrameThread is allowed to proceed by invoking the CODEC's compressFrame operation. Otherwise, the ConsumerFrameThread is placed on the CompressWaitQueue.","StopCompressing\u2014If there are no other ConsumerFrameThread(s) waiting to access the HostFrameBuffer, then the one and only ProducerFrameThread is signaled to proceed. Otherwise, the CompressThread who has waited the longest on the CompressWaitQueue is signaled to consume.","StartGrabbing\u2014If there are no other ConsumerFrameThread(s) accessing or waiting to access the HostFrameBuffer resource, then the ProducerFrameThread is allowed to proceed by invoking the FrameGrabber's grabFrame operation. Otherwise, the ProducerFrameThread is placed on the GrabWaitQueue.","StopGrabbing\u2014If there are any ConsumerFrameThread(s) waiting to access the HostFrameBuffer, then the oldest ConsumerFrameThread who has waited the longest in CompressWaitQueue is signaled to start consume. Otherwise allow the oldest ProducerFrameThread on the Grab WaitQueue [Refer to FIG. E]","Class Name: Thread","The Thread class provides control operations dedicated to deterministic state dynamics of an operating system resource known as a thread. Refer to FIG. A.","Attributes:","hLifetime\u2014An auxiliary handle to an operating system event object used to assist in thread creation and thread termination. This event is a manual-reset type of event provided by the operating system. Internal to this class the event can be OWNED at creation time. When terminate operation is invoked, the event becomes AVAILABLE.","hThread\u2014The handle to the actual operating system thread object instance provided by the operating system.","threadID\u2014The identifier provided by the operating system which uniquely identifies a thread instance resident in the process.","threadPriority\u2014Specifies the priority value for the thread.","State:","A synchronization event object with manual-reset behavior is used to control Thread creation and Thread termination. Upon successful Thread creation the state is CREATED. Upon successful Thread termination the state is TERMINATED. During the Thread's lifetime, two additional states are used: SUSPEND and RESUME. [Refer to FIG. B]","Operations:","Create\u2014Bring a new thread to life. Upon successful creation, a transition to the SUSPENDED state, the threadID, threadHandle, and lifetimeHandle are each initialized.","GetThreadPriority\u2014Allows the client to access the threadPriority attribute.","Resume\u2014The thread is moved, per its threadID, from the SUSPENDED state to the RESUMED state.","Suspend\u2014The thread is moved, per its threadID, from the CREATED state or RESUMED state to the SUSPENDED state.","Terminate\u2014The thread is moved, per its threadID, from the SUSPENDED state or RESUMED state to the TERMINATED state.","SetThreadPriority\u2014Allows the client to change the threadPriority attribute.","class Name: ProducerFrameThread","The producerFrameThread is dedicated to the production of raster frames.","Associations","The ProducerFrameThread subclass is a specialized kind of Thread class.","A ProducerFrameThread is related to only one HostFrameBuffer, pBuf.","A ProducerFrameThread is related to only one FrameGrabber, pFrameGrabber.","A ProducerFrameThread is made up of only one HostBasedFrameMonitor, pHBFM. [Refer to FIG. ]","Operations:","ProduceFrame\u2014When not blocked by the Monitor. the ProduceFrame operation continuously populates the HostFrameBuffer with raster data. This raster data originates from the FrameGrabber. [Refer to FIG. C]","Class Name: ConsumerFrameThread","The ConsumerFrameThread is dedicated to consuming the contents of the HostFrameBuffer.","Associations:","The ConsumerFrameThread is a specialized kind of Thread class.","A ConsumerFrameThread is related to only one HostFrameBuffer, pBuf.","A ConsumerFrameThread is related to only one CODEC, pCODEC.","A ConsumerFrameThread is made up of only one HostBasedFrameMonitor, pHBFM. [Refer to FIG. ]","Operations:","ConsumeFrame\u2014When not blocked by the Monitor, the ConsumeFrame operation continuously delegate the task of encoding (compressing) the contents of the HostFrameBuffer to the CODEC. [Refer to FIGS.  and D]","Class Name: BLOb","Binary data is the heart of the raster frame structure and it was therefor appropriate to abstract this key entity into a class called Binary Long Object (BLOb). The class BLOb enables the client to access large pixel data. For example, a BLOb object can be used to hold an uncompressed raster frame prior to being compressed by a CODEC or it can be used to transfer a raster from a FrameGrabber. The BLOb class contains a pointer to the class BLObData. To relieve the client from heap memory management, reference counting is employed. For clients who require direct memory access to the raster frame, the Lock and Unlock operations are provided. During a raster frame lock, reference counting is not incremented and it is the responsibility of the client to control any outstanding shallow references to the raster frame. When the client is finished with the raster frame, the client must call the Unlock operation to re-invoke reference counting. [Refer to FIG. ]","Assertion:","BLObSize<=bufferSize","Operations:","GetBLOBSize\u2014Return the size of the raster contained inside the buffer.","GetBufferSize\u2014Returns the size of the allocated buffer.","Initialize\u2014Deallocate memory associated with the raster if there is any.","IsNull\u2014Determine if the BLOb contains valid raster data.","LockBuffer\u2014Turn the reference counting off and obtain access to the start address of the raster frame.","ReallocBuffer\u2014Change the size of the buffer to accommodate raster frame growth.","ReallocBufferToBLObSize\u2014Adjust the size of the allocated buffer to match the size of the contained raster frame.","SetBLObDeep\u2014Set the contents of the BLOb with new raster frame by performing a deep memory copy from client's source raster frame.","SetBLObSize\u2014For all the allocated buffer memory, set the length of the contained raster frame.","SetBufferSize\u2014Set the size of the Buffer.","UnlockBLOb\u2014Turn reference counting on.","Class Name: HostFrameBuffer","The HostFrameBuffer is derived from the superclass Binary Long Object (BLOb). The HostFrameBuffer contains the frame raster data produced by the ProducerThread. Once the HostFrameBuffer is populated and time stamped the ConsumerThread accesses this frame faster data for encoding. [Refer to FIG. ]","Attributes:","CreationTime\u2014Represents an absolute time and date of when the HostFrameBuffer was populatated with new raster frame data. The creationTime incorporates the ANSI time_data type and its associated run-time functions. creationTime values are based on coordinated universal time (UTC), which is equivalent to Greenwich mean time (GMT). The local time zone is controlled by an internal environment variable provided by the operating system.","Operations:","SetCreationTime\u2014write-only access to set the time when the BLOb was created.","GetCreationTime\u2014read-only access to when the BLOb was created.","Class Name: Monitor","The monitor class provides protection and synchronization of a shared resource.","Association:","pEntryQueue\u2014The Monitor is associated with one Queue. This Queue is responsible for ordering Threads who have requested to enter the Monitor. These new Threads are placed on the entry queue if the Monitor is already Locked. [Refer to FIG. A]","Attributes:","hMutex\u2014The mutual exclusion synchronization object which is provided by the operating system and is used to permit only one thread at a time from entering the Monitor.","numberOfWaitingThreads\u2014The number of Thread(s) suspended on the associated pEntryQueue who desire Monitor ownership.","State:","The hMutex synchronization object yields two states: LOCKED and UNLOCKED.","When the Monitor is in a LOCKED state, a single Thread resides in the Monitor.","When the Monitor is in an UNLOCKED state, the next Thread in the entry Queue is permitted to transition to the LOCKED state.","The CONSTRUCTED AND DESTRUCTED states ensure correct initialization and de-initialization of the Monitor object. [Refer to FIG. C]","Operations:","ObtainOwnership\u2014A Thread calls this operation to obtain exclusive ownership of the Monitor.","ReleaseOwnership\u2014A Thread calls this operation to releases its ownership of the Monitor for the next stacked Thread on the entry queue.","Class Name: Condition","The Condition class is responsible for creating BinarySemaphore(s) which make Threads wait their turn in order to access the HostFrameBuffer object.","Associations","The Condition is associate with one HostBasedMonitor, pHBFM.","The Condition is associated with one Queue, pConditionQueue. [Refer to FIG. A]","State:","The Condition object has two states: SIGNALED and WAITING [Refer to FIG. B]","Attributes:","numberOfBlocks\u2014The number of blocked threads waiting on a BinarySemaphore instance.","Operations:","Signal\u2014Take thread off Queue and allow it to access to the Monitor.","Wait\u2014Make thread wait its turn on Queue.","Class Name: Queue","Each SemaphoreItem refers to one SemaphoreItem.","The Queue supports two common behaviors:\n\n","Logically, the Queue class contains zero or more SemaphoreItem objects. However, to implement the Queue, only the addresses of the first and last SemaphoreItem are required.","The pointer pHead is the address of the first SemaphoreItem in the Queue.","The pointer pTail is the address of the last SemaphoreItem in the Queue. [Refer to FIG. A and B]","Attributes:","semaphoreItemCount\u2014The number of SemaphoreItem objects referenced by the Queue.","Operations:","AddFIFO\u2014Adds a new SemaphoreItem to the end of the Queue. By adding the SemaphoreItem to the tail of the Queue, FIFO behavior is achieved.","AddLIFO\u2014Adds a new SemaphoreItem to the head of the Queue. By adding the SemaphoreItem to the head of the Queue, LIFO behavior is achieved.","GetHead\u2014Returns the first SemaphoreItem object in the Queue and deletes the returned SemaphoreItem object from the Queue. When all SemaphoreItem objects have been deleted, this method returns NIL.","Class Name: BinarySemaphore","A BinarySemaphore controls thread access to the HostFrameBuffer and it has an internal counter. The range of this internal count is from zero to one.","Attribute:","hSemaphore\u2014handle to Semaphore provided by operating system. [Refer to FIG. A]","Associations:","A BinarySemaphore class is derived from the superclass Semaphore. [Refer to FIG. A]","State:","The BinarySemaphore has two states: AVAILABLE or OWNED. A signaled semaphore means AVAILABLE. A not-signaled semaphore means OWNED. [Refer to FIG. B]","Operations:","Block\u2014Gains access to the shared resource. If the wait operation is successful the semaphore count decrements and it is then safe to access the OWNED HostFrameBuffer resource. If the BinarySemaphore internal count is greater than zero, a wait operation on the semaphore succeeds, and its count decrements.","Unblock\u2014When the Thread is finished executing it increments the Semaphore count to indicate the Semaphore is AVAILABLE. If the BinarySemaphore count is zero, then any threads waiting are blocked until the thread who owns the Monitor releases ownership."],"BRFSUM":[{},{}],"heading":["TECHNICAL FIELD","COPYRIGHT NOTICE\/PERMISSION","BACKGROUND INFORMATION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION","Conclusion","Appendix"],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"P-00010","num":"00010"},"figref":"FIG. 1A"},{"@attributes":{"id":"P-00011","num":"00011"},"figref":"FIG. 1B"},{"@attributes":{"id":"P-00012","num":"00012"},"figref":"FIG. 1C"},{"@attributes":{"id":"P-00013","num":"00013"},"figref":"FIG. 1D"},{"@attributes":{"id":"P-00014","num":"00014"},"figref":"FIG. 2"},{"@attributes":{"id":"P-00015","num":"00015"},"figref":["FIG. 3","FIG. 5"]},{"@attributes":{"id":"P-00016","num":"00016"},"figref":["FIG. 4","FIGS. 3 and 5"]},{"@attributes":{"id":"P-00017","num":"00017"},"figref":"FIG. 5"},{"@attributes":{"id":"P-00018","num":"00018"},"figref":"FIG. 6A"},{"@attributes":{"id":"P-00019","num":"00019"},"figref":"FIG. 6B"},{"@attributes":{"id":"P-00020","num":"00020"},"figref":"FIG. 6C"},{"@attributes":{"id":"P-00021","num":"00021"},"figref":"FIG. 6D"},{"@attributes":{"id":"P-00022","num":"00022"},"figref":"FIG. 6E"},{"@attributes":{"id":"P-00023","num":"00023"},"figref":"FIG. 7"},{"@attributes":{"id":"P-00024","num":"00024"},"figref":"FIG. 8A"},{"@attributes":{"id":"P-00025","num":"00025"},"figref":"FIG. 8B"},{"@attributes":{"id":"P-00026","num":"00026"},"figref":"FIG. 8C"},{"@attributes":{"id":"P-00027","num":"00027"},"figref":"FIG. 9A"},{"@attributes":{"id":"P-00028","num":"00028"},"figref":"FIG. 9B"},{"@attributes":{"id":"P-00029","num":"00029"},"figref":"FIG. 10A"},{"@attributes":{"id":"P-00030","num":"00030"},"figref":"FIG. 10B"}]},"DETDESC":[{},{}]}
