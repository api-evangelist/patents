---
title: User-controllable learning of policies
abstract: Various embodiments are directed to a computer implemented method for updating a policy that is enforced by a computer program. In one embodiment, a computer communicates, to a user, data regarding one or more decisions made by the program over a period of time according to a policy. Each decision is made on the particular policy in force at the time the decision is made. Policy data for the policy is stored in a machine readable format. The user feedback data indicative of feedback by the user regarding the one or more decisions is stored. The computer identifies and ranks one or more potential variations to the policy based on a score of an objective function for each potential variation. The computer communicates, to the user, one or more suggested modifications based on the ranking of the one or more potential variations to the policy. The computer modifies the policy data based on one or more selections by the user in response to the transmission of the one or more suggested modifications.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08423483&OS=08423483&RS=08423483
owner: Wombat Security Technology, Inc.
number: 08423483
owner_city: Pittsburgh
owner_country: US
publication_date: 20090516
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS REFERENCE TO RELATED APPLICATION","BACKGROUND","SUMMARY","DESCRIPTION"],"p":["This application claims the benefit under Title 35, United States Code \u00a7119(e), of U.S. Provisional Patent Application Ser. No. 61\/127,866, filed May 16, 2008 and entitled \u201cUser-Controllable Policy Learning,\u201d which is hereby incorporated by reference in its entirety.","Computational policies (hereafter referred to as policies) are machine understandable representations of rules that govern a computational agent's behavior. They include security policies, privacy policies, user preferences, workflow policies, and a variety of other application domains. Studies have shown that users generally have great difficulty specifying policies. While machine learning techniques have been used successfully to refine policies such as in recommender systems or fraud detection systems, they are generally configured as \u201cblack boxes\u201d that take control over the entire policy and severely restrict the ways in which the user can manipulate it.","A broad and growing number of applications allow users to customize their policies, whether as system administrators, end-users or in other relevant roles. From the network administrator maintaining complex and verbose firewall access control lists to the social networking (e.g., Facebook\u00ae) user struggling with the site's privacy settings, studies have consistently shown that novice and expert users alike find it difficult to effectively express and maintain such policies. In one study, for instance, test users asked to express file permission policies within the native Windows\u00ae XP interface achieved very low accuracy rates, thus reflecting a significant gap between the users' intended policies and the policies that they manage to express in policy specification languages and their associated interfaces.","Given this difficulty, it is highly desirable to support users in the tasks of policy specification and maintenance, with the aim of helping them narrow this gap. While a number of machine learning applications rely on simple forms of user feedback to improve their performance (e.g., spam filters or recommender systems employed by Amazon and Netflix), little work has been done to develop configurations of these techniques that support closer collaboration between machines and users. Most recommender systems base their recommendations on explicit and\/or implicit user ratings of products or services they have been presented. In these systems, however, the user does not have transparency into the underlying policies upon which the system bases its recommendations and, accordingly, the systems' underlying policies appear as a black-box to the user. This makes it significantly more difficult for a user to modify the policy, be it because it does not yet adequately reflect the user's intent or because the user's intended policy has suddenly changed. This same limitation applies to environments where a policy is intended to capture the preferences of multiple users (e.g., multiple system administrators and\/or end-users in a complex firewall deployment).","In one general aspect, the described embodiments are directed to a computer implemented method for updating a policy that is enforced by a computer program. In one embodiment, a computer system communicates, to at least one user, data regarding one or more decisions made by the program over a period of time according to a policy. Each decision is made based on the particular policy in force at the time the decision was made. The policy data for the policy is stored in a machine readable format in a memory coupled to the computer system. The computer system comprises at least one processor coupled to a memory. User feedback data indicative of feedback by the at least one user regarding the one or more decisions made by the program according to the policy is stored in a machine readable format. The computer system identifies and ranks one or more potential variations to the policy based on a score of an objective function. The objective function comprises a plurality of function elements. The plurality of function elements comprises a user satisfaction function element and at least one other function element that measures the extent to which the one or more potential variations are likely to be understandable by the at least one user. The computer system communicates, to the at least one user, one or more suggested modifications to the policy based on the ranking of the one or more potential variations to the policy. The one or more suggested modifications can assist the at least one user refine the policy. In one embodiment, the at least one user can accept, reject, or modify one or more of the suggested policy modifications. Acceptance, rejection, or modifications by the at least one user of the suggested policy modifications can themselves be used as an additional source of user feedback. The computer system modifies the policy data stored in the memory based on one or more selections by the at least one user in response to the communication of the one or more suggested modifications.","Various embodiments are directed to apparatuses, systems, and methods for user-controllable learning of policies enforced by a computer system. It will be appreciated by those skilled in the art, however, that a computer system may be implemented as an electronic computer, photonic computer, quantum computer, neural computer, mechanical computer, and the like. The computer system can be configured to enforce policies. Numerous specific details are set forth to provide a thorough understanding of the overall structure, function, manufacture, and use of the embodiments as described in the specification and illustrated in the accompanying drawings. It will be understood by those skilled in the art, however, that the described embodiments may be practiced without the specific details. In other instances, well-known operations, components, and elements have not been described in detail so as not to obscure the embodiments described in the specification. Those of ordinary skill in the art will understand that the embodiments described and illustrated herein are non-limiting examples, and thus it can be appreciated that the specific structural and functional details disclosed herein may be representative and do not necessarily limit the scope of the embodiments, the scope of which is defined solely by the appended claims.","In one general aspect, a user-controllable policy learning system according to the described embodiments assists users refine policies based on user feedback. Policies may be defined as a collection of rules and can be used to guide a broad range of decisions. They include security and privacy policies, workflow policies, corporate policies, user preferences and more. The user-controllable policy learning system is applicable in any domain where users specify such policies. This may include firewall policies, spam filtering policies, calendar sharing policies, location sharing policies in cell phone friend finder applications, file access permission policies, Facebook\u00ae privacy policies, dispute resolution policies and the like. This may also include preferences organized according to various attributes such as preferences on web sites such as Amazon\u00ae or Netflix\u00ae, restaurant recommendation preferences, dating site preferences, carpooling preferences, message filtering and forwarding preferences (e.g., in the context of multi-channel messaging systems such as that provided by Microsoft\u00ae). For conciseness and clarity, throughout the remainder of this specification, references to a policy or policies is intended to cover all applicable domains in which users specify preferences, policies, and\/or combinations thereof.","The user-controllable policy learning system applies to a wide range of application domains (e.g., specifying privacy or security policies, capturing user preferences for different types of movies, restaurants or carpooling partners, as previously discussed) where users have been shown to have difficulty expressing policies and are willing to occasionally provide feedback on decisions made based on possibly imperfect specifications of their policies. The term user includes any number of users including an end user, a policy administrator, a person associated with specifying and modifying the policy, and the like. Using this feedback, the user-controllable policy learning system generates suggestions for the user on how to modify existing policies. In contrast to traditional machine learning solutions, which generate brand new policies that are not specifically meant to be understood by users, user-controllable policy learning offers a solution capable of suggesting user-understandable modifications to an existing policy. Ultimately, the user continues to understand and retains control over the policies as they evolve\u2014in contrast to having a \u201cblack box\u201d learning algorithm take over and develop policies that the user can no longer relate to. As the user remains in control, the user can directly manipulate their policies if or when they identify inaccuracies in them (e.g., because they failed to properly specify them, because learning imperfectly refined them, or simply because their preferences\/policies have changed). The resulting approach is one where user and machine learning can work hand in hand and complement one another.","In various embodiments, a user-controllable policy learning system provides transparency to the user of the underlying policy upon which the system bases its decisions. As such, the system's underlying policy does not appear as a black-box to the user, but rather, appears as a user-understandable model of the user's policies. Improvements can thus be undertaken by the policy learning system or the user, with both manipulating the same common policy. The end result is an approach where the user continues to understand the policy as it evolves.","Furthermore, because the user can understand the suggested modifications being presented, the user can decide whether or not to accept, reject, or modify them. Accordingly, in one configuration the learning system can ensure that no modifications to the policy are undertaken without explicit user consent. This is particularly critical in the context of security and privacy policies, where all elements of the policy need to be vetted.","With full control of the policy, the user can account for sudden policy changes that machine learning would adapt to only slowly (e.g., a user's privacy preferences might be very different when on vacation). This significantly reduces the risk of the system introducing particularly poor policy decisions. This is of heightened importance in the context of security and privacy policies where poor policy modifications could have negative consequences.","Before describing various embodiments and implementation details of a user-controllable policy learning system, a suitable computing environment in which the various embodiments may be implemented will be described. Accordingly,  and the following discussion are intended to provide a brief general description of a suitable computing environment in which the described embodiments for user-controllable learning of policies may be implemented. It should be understood, however, that handheld, portable, and other computing devices and computing objects of all kinds are contemplated for use in connection with the described embodiments. While a general purpose computing environment is described, this is but one example, and the described embodiments may be implemented with other computing devices, such as a client having network\/bus interoperability and interaction. Thus, the described embodiments may be implemented in an environment of networked hosted services in which very little or minimal client resources are implicated, e.g., a networked environment in which the client device serves merely as an interface to the network\/bus, such as an object placed in an appliance, or other computing devices and objects as well. In essence, anywhere that data may be stored or from which data may be retrieved is a desirable, or suitable, environment for operation according to the described embodiments.","Moreover, those skilled in the art will appreciate that the described embodiments may be practiced with other computer configurations. Other well known computing systems, environments, and\/or configurations that may be suitable for use with the described embodiments may comprise personal computers (PCs), server computers, hand-held or laptop devices, multi-processor systems, microprocessor-based systems, programmable consumer electronics, network PCs, minicomputers, mainframe computers, mobile computing devices, which may comprise or be implemented as a combination handheld computer and mobile telephone or smart phone such as a Palm\u00ae Treo\u2122 smart phone as well as other types of wireless computing devices having voice and\/or data communications functionality such as a handheld device, personal digital assistant (PDA), mobile telephone, combination mobile telephone\/PDA, mobile unit, subscriber station, game device, messaging device, media player, pager, or any other suitable communications device in accordance with the described embodiments. The described embodiments also may be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network\/bus or other data transmission medium. In a distributed computing environment, program modules may be located in both local and remote computer storage media including memory storage devices and client nodes may in turn behave as server nodes.","Although not required, the described embodiments can be implemented via an operating system, for use by a developer of services for a device or object, and\/or included within application software that operates according to the described embodiments. Suitable operating systems include, but are not limited to, UNIX\u00ae from the SCO Group, Inc., GHOST for UNIX\u00ae, WINDOWS\u00ae from MICROSOFT\u00ae OS (e.g., 95, 98, NT, ME, 2000, XP, CE, Longhorn, Vista), MAC OS X from Apple Computer, Inc., Internetwork Operating System (IOS) from Cisco, Juniper JUNOS, IBM OS, LINUX, SOLARIS, 3COM, PALM OS, and the like. Software may be described in the general context of computer-executable instructions, such as program modules, being executed by one or more computers, such as client workstations, servers or other devices. Generally, program modules include routines, programs, objects, components, data structures, and the like that perform particular tasks or implement particular abstract data types. Generally, the functionality of the program modules may be combined or distributed as desired in various embodiments.",{"@attributes":{"id":"p-0036","num":"0035"},"figref":["FIG. 1","FIG. 1","FIG. 1"],"b":["100","100","100","100","100"]},"As shown in , the system  may be realized as a network  comprising nodes  and . In various embodiments, the nodes ,  may be arranged to operate as computers and (where n and m may be any positive integer) connected via the network . In one embodiment, the computers and may communicate with the network  via a network interface , for example. For conciseness and clarity, the description a suitable general purpose computing system in which the computers and may be implemented is deferred to the discussion with reference to  hereinbelow.","As used herein, a node may comprise any physical or logical entity having a unique address in the system . The unique address may comprise, for example, a network address such as an IP address, a device address such as a Medium Access Control (MAC) address, and so forth. The user  can access the system from any node, including multiple nodes at the same time.","In one embodiment, the system  may be arranged such that the nodes ,  may be arranged as any one of the computers and and may be configured to share the interface  to the network  (e.g., a LAN interface). In one embodiment, any two or more computers and may share a single IP address because of limited allocation of IP addresses in the network  (e.g., IPv4) or because any two or more computers and may likely be accessed using a single IP address or using the same name for the network  as though it was a single system, for example.","The nodes ,  of the system  may comprise or form part of the network , such as a LAN, a Metropolitan Area Network (MAN), a Wide Area Network (WAN), a Wireless LAN (WLAN), an Internet network, a World Wide Web network, a telephony network (e.g., analog, digital, wired, wireless, Public Switched Telephone Network (PSTN), Integrated Services Digital Network (ISDN) or Digital Subscriber Line (xDSL)), a radio network, a television network, a cable network, a satellite network, and\/or any other wired or wireless communications network configured to carry data. The network  may include one or more elements, such as, for example, intermediate nodes, proxy servers, firewalls, routers, switches, hubs, adapters, sockets, and wired or wireless data pathways, configured to direct and\/or deliver data to other networks.","In one embodiment, the nodes ,  may be arranged to operate in accordance with one or more protocols, such as MAC protocols, such as from the IEEE 802.3 series of Ethernet protocols, for example. The nodes ,  may be implemented as a high bandwidth switch, such as a Fast Ethernet switch operating at 100 megabits per second (Mbps), a Gigabit Ethernet switch operating at 1000 Mbps or 10 Gigabits per second (Gbps), a router configured as a DHCP server, and so forth.","The nodes of the system  may be arranged to communicate one or more types of information, such as media information and control information. Media information generally may refer to any data representing content meant for a user, such as image information, video information, graphical information, audio information, voice information, textual information, numerical information, alphanumeric symbols, character symbols, and so forth. Control information generally may refer to any data representing commands, instructions or control words meant for an automated system. For example, control information may be used to route media information through a system, or instruct a node to process the media information in a certain manner. Other types of information include, without limitation, context information, e.g., information about the user  such as location, calendar, policy information, logging information, and the like. The information may be communicated from and to a number of different devices or networks.","The nodes of the system  may communicate in accordance with one or more protocols. A protocol may comprise a set of predefined rules or instructions to control how the nodes communicate information between each other. The protocol may be defined by one or more protocol standards as promulgated by a standards organization, such as the Internet Engineering Task Force (IETF), International Telecommunications Union (ITU), the Institute of Electrical and Electronics Engineers (IEEE), and so forth. For example, the system  may comprise a packet network communicating information in accordance with one or more packet protocols, such as one or more Internet protocols, including the Transport Control Protocol (TCP) and IP, TCP\/IP, X.25, Hypertext Transfer Protocol (HTTP), User Datagram Protocol (UDP), and DHCP protocol. In another example, the system  may communicate packets using a medium access control protocol such as Carrier-Sense Multiple Access with Collision Detection (CSMA\/CD), as defined by one or more IEEE 802.x Ethernet standards. In yet another example, the system  may communicate packets in accordance with one or more Asynchronous Transfer Mode (ATM) protocols, Frame Relay, Systems Network Architecture (SNA), and so forth. It will be appreciated that the system  may communicate packets in accordance with more than one or all of these standards simultaneously.","In various embodiments, the system  may be illustrated and described as comprising several separate functional elements, such as modules and\/or blocks. Although certain modules and\/or blocks may be described by way of example, it can be appreciated that additional or fewer modules and\/or blocks may be used and still fall within the scope of the embodiments. Further, although various embodiments may be described in terms of modules and\/or blocks to facilitate description, such modules and\/or blocks may be implemented by one or more hardware components (e.g., processors, Digital Signal Processors (DSPs), Programmable Logic Devices (PLDs), Application Specific Integrated Circuits (ASICs), circuits, registers), software components (e.g., programs, subroutines, logic), and\/or combinations thereof.","In various embodiments, the system  may comprise multiple modules connected by one or more communications media. Communications media generally may comprise any medium capable of carrying information signals. For example, communications media may comprise wired communications media, wireless communications media, or a combination of both, as desired for a given implementation. Examples of wired communications media may include a wire, cable, printed circuit board (PCB), backplane, semiconductor material, twisted-pair wire, co-axial cable, fiber optics, and so forth. An example of a wireless communications media may include portions of a wireless spectrum, such as the radio-frequency (RF) spectrum.","The modules may comprise, or may be implemented as, one or more systems, sub-systems, devices, components, circuits, logic, programs comprising computer executable instructions, or any combination thereof, as desired for a given set of design or performance constraints. For example, the modules may comprise electronic elements fabricated on a substrate. In various implementations, the electronic elements may be fabricated using silicon-based Integrated Circuit (IC) processes such as complementary metal oxide semiconductor (CMOS), bipolar, and bipolar CMOS (BiCMOS) processes, for example.","Some computer applications may comprise two or more computer systems each comprising a network management module embedded within a host computer computational platform that share an interface to the network . In one embodiment, the two or more computer systems may share a single IP address. The two or more computer systems comprising a network management module may work together or separately, so that when one computer is shut down, hanged, or in standby, the other one is still functional and may be accessed over the network  and may operate on behalf of the inoperable computer, for example.","In one embodiment, a network management module may comprise a Network Interface Chip (NIC) that serves a host OS with its own OS driver that includes an embedded manageability module to operate and communicate over the network  (e.g., LAN, Internet) while the host computer system is operational as well as when the host computer system and its OS are inoperable, disconnected, shutdown, hanged or in standby mode.","In one embodiment, the user-controllable policy learning system functionality may be implemented by a policy engine (PE) module . The PE module  according to the described embodiments assists users refine policies based on user feedback. In various embodiments, the PE module  may be deployed at any node ,  such as any one or all of the two or more computers and as well as the network . In the illustrated embodiment, the computers and the PE module  are provided within the same computational platform (e.g., a single computer system), for example. In one embodiment, the PE module  may be provided as a separate module located on different computational platforms or separate remote computer systems, for example. In one embodiment, the modules, sub-modules, components or elements of the system  may comprise, for example, a network management module to manage computational platform deployment and maintenance in an Information Technology (IT) organization or environment. The computers and each may comprise a display device and , respectively, to communicate information to one or more users operating the computers and\/or to one or more users operating the computers .","In various implementations, the computers , , and\/or the PE module  may be arranged to perform various processing operations. Processing operations may generally refer to one or more operations, such as generating, managing, communicating, sending, receiving, storing, forwarding, accessing, reading, writing, manipulating, encoding, decoding, compressing, decompressing, encrypting, filtering, streaming or other computerized processing of information, which may be implemented by executing computer program instructions. Accordingly, the computers , , and\/or the PE module  may comprise embedded applications implemented as firmware, software, hardware or any combination thereof, for example. The PE module  may comprise various executable modules such as software, programs, data, drivers, application program interfaces (APIs), and so forth. The firmware may be stored in NVM, such as in bit-masked read-only memory (ROM) or flash memory. In various implementations, storing the firmware in ROM may preserve flash memory. The NVM may comprise other types of memory including, for example, programmable ROM (PROM), erasable programmable ROM (EPROM), electrically erasable programmable ROM (EEPROM), or battery backed random-access memory (RAM) such as dynamic RAM (DRAM), Double-Data-Rate DRAM (DDRAM), and\/or synchronous DRAM (SDRAM).","In one embodiment, the PE module  provides functionality to enable the user(s) to control learning of policies  in a variety of application domains, such as security privacy policies, models of user preferences in, for example, recommender systems, and policies in supervisory control and data acquisition (SCADA) systems. In one embodiment, the PE module  provides incremental manipulation of policies  in a context in which the system  and the user(s) can refine a common policy . In one implementation, this may be achieved by the user(s) providing feedback on decisions, each of which is made based on the policy  in force at the time the decision is taken, and using the feedback to identify (learn) incremental policy improvements which are presented as suggestions to the user(s) , by way of a user interface . The policy  may, for the sake of example, and not limitation, be stored locally, externally on policy-relevant devices such as firewalls and\/or routers, in a central policy repository, on multiple distributed nodes, or \u201cin the cloud.\u201d The policy  may be expressed as collections of condition\/action rules sufficient to capture a broad range of policies, including a wide variety of policies such as XACML policies, among others. The user(s) , in turn, can review these suggestions and decide which, if any, to accept, reject, or modify. The incremental nature of the suggestions enhances usability, and because the user(s) and the system  manipulate a common policy representation, the user(s) retains control and can still make policy modifications by hand.","Various functional aspects of the PE module  are subsequently described with respect to , , and . In various embodiments, the user-controllable learning process in accordance with the described embodiments be implemented or applied in a variety of contexts within a suitable computing environment such as those illustrated by system ,  (), and  (). Accordingly, by way example, and not limitation, the process may be implemented as a user-controllable learning process in the context of a network firewall, wherein the policy  is configured to govern the filtering behavior of the firewall in response to incoming and outgoing packets on the network . As applied in the context of an application level firewall, the policy  governs the behavior of the policy system computer  (e.g., the policy enforcement computer) in response to traffic on the network  that pertains to specific applications. In other implementations, the process may be applied in the context of an intrusion detection system, wherein the policy  is configured to determine patterns of traffic on the network  that will or will not be considered indicative of an intrusion. In other implementations, the process may be applied in the context of social networking applications, wherein policy  is configured to govern the selective disclosure of all types of personal information of the user . In other implementations, the process may be applied in the context of routing devices on the network , wherein the policy  is configured to govern the routing of packets on the network . In other implementations, the process may be applied in the context of home networking devices such as routers, firewalls, and access points wherein the policy  is configured to govern the behavior, connectivity, and security of the home networking device. In other implementations, the process may be applied in the context of recommender systems for products and services, wherein the policy  is defined as a model of the preferences, likes, and dislikes of the user  pertaining to the products and services in question. In other implementations, the process may be applied in the context of software development environments for programming, debugging, and static analysis wherein the policy  is defined as a set of patterns within computer programs that represent errors or inefficiencies and a set of proposed improvements to the errors or inefficiencies. In other implementations, the process may be applied in the context of email filtering applications wherein the policy is configured to determine which email messages will be deemed to be \u201cspam\u201d or junk emails, \u201cphishing,\u201d or illegitimate solicitation emails, or legitimate emails. The implementation and application contexts described above are not limited in this context.",{"@attributes":{"id":"p-0053","num":"0052"},"figref":["FIG. 2","FIG. 2","FIG. 1","FIG. 1","FIG. 1","FIG. 1"],"b":["200","110","120","100","140","190","202","110","120","140","110","140","102","104","100","110","110","140","120","120","190","110","120","110","120","190","190","110","120","150","120","110","190","185"],"sub":["1\u2212m","1\u2212n "]},"Accordingly, in one embodiment the PE module  enables a user-oriented approach to refining the policy  that collects feedback  from the user  to help identify (or learn)  incremental improvements to the policy . The most promising incremental improvements, or suggested policy modifications , e.g., top policy suggestions, are presented to the user  via a user interface, who in turn decides whether or not to accept, reject, or modify them. This user-controllable learning approach contrasts with black box configurations in which most machine learning techniques are traditionally deployed. Rather than restricting the level of control the user  has over the policy , for example, limiting the user  to providing occasional feedback on system decisions, the user-controllable learning process, as implemented by the PE module  in accordance with the described embodiments, enables the user  and the policy system computer  to work in tandem on a common policy . By focusing on incremental changes to the policies  already in force, the user-controllable learning process according to the described embodiments makes it easier for the user  to understand the suggested policy modifications  and decide whether or not to accept, reject, or modify them. A policy already in force may include, without limitation, a policy specified by a user, a default policy that shipped with a device, a null policy, and the like. An audit log  is provided to store one or more policy decision records . The audit log  is provided to maintain a record of (a) the actions  taken in response to incoming events  in accordance with the policy currently in force , and (b) the feedback  collected from the user  on these actions . Actions taken as a result of application of a policy  may include actions such as the disclosure of information either in full detail or in an obfuscated form (e.g., disclosing the city the user is in but not the street address he is at), or no action. An action  may be a logical action, and\/or a physical action taken in accordance with the policy . More than one action can be taken as a result of a single event. An action  may include transmitting a signal by the PE module  to open a door, take a picture, filter an electronic mail, transfer a packet, making a logical inference, launching a new process, and the like. It will be appreciated by those skilled in the art that the physical action  may be enforced by a policy enforcement point rather than a policy decision point. Upon acceptance by the user , those suggested policy modifications , subject to whatever additional modifications the user  may have made to these suggestions, are incorporated into the policy , and the resulting modified policy becomes the new policy in force. Although some of the directed arrows in  (and other figures throughout the present specification) intended to reflect the direction of information flow are shown as uni-directional, those skilled in the art will appreciate that information may flow bi-directionally based on system implementation and design constraints. For example, with reference to , the user  provides feedback  because he or she is able to view the policy decision records  stored in the audit log .","At any point in time, the user  can override the system and make changes without the assistance of the policy system computer . This is particularly useful in situations where new conditions or events  arise (e.g., changes in the user's  social network, new types of network attacks) and where user-defined changes are more effective than waiting for the policy system computer  to refine the policies  or relearn from scratch. In addition, allowing the user  to retain control over improvements learned by the policy system computer  reduces the risk of introducing particularly bad policy modifications. This is especially important when dealing with policies  where the ramifications of an incorrect policy decision can be quite costly, but useful in any environment where the users  benefit from this additional control.","In one embodiment, the user-controllable learning process provides a simplified interface for the user  to interact with the policy system computer , which is simpler than the interaction between the user  and the policy system computer  with the languages and associated interfaces used to specify the policies  themselves. Accordingly, the PE module  provides suggested policy modifications  to the user  upon which the user  can base its response. This provides the advantage of presenting the user  with practical, concrete, understandable, and useful examples of suggested policy modifications .","Accordingly, to promote the understandability and usability of the policy learning system in accordance with the described embodiments, the user-controllable learning process provides system transparency to enable the user  to maintain control over the policy  and any modifications thereto at all times. To ensure control over system-based changes, the user-controllable learning process provides a selection of improvements, leaving control as to whether to accept, reject, or modify any of the suggested policy modifications  or suggestions up to the user . Thus, the user  retains full control over policy modifications. This may be a consideration when it comes to the disclosure of personal information, which is has been shown to be an integral component of the user's  perception of privacy. Accordingly, the user-controllable learning process according to the described embodiments provides the functionality to enable the user  to make changes directly to the policy  at any time. This is in contrast to current recommender systems, where the user's sole interaction with the system is through provision of feedback.",{"@attributes":{"id":"p-0058","num":"0057"},"figref":"FIGS. 3-6","b":["300","208","190","204","300","110","202","300","190","110","208","190","300"]},"The natural language interface  also may incorporate one or more user feedback elements such as text input boxes, check box (GUI), clickable buttons, clickable text, and so on, as a means for the user  to provide feedback  to the PE module . Some of these may be highlighted by three-dimensional shading or color. By way of example and not limitation, the illustrated natural language user interface  enables the user  to interact with the PE module  through the one or more feedback elements.",{"@attributes":{"id":"p-0060","num":"0059"},"figref":["FIG. 3","FIGS. 1-3"],"b":["300","300","301","190","303","305","190","302","190","303","305","303","305","304","306","190","190","308","309"]},{"@attributes":{"id":"p-0061","num":"0060"},"figref":["FIG. 4","FIGS. 1-4"],"b":["300","304","306","300","310","190","190","312","190","303","190","80","140","208","190","190","208","190","314","208"]},{"@attributes":{"id":"p-0062","num":"0061"},"figref":["FIG. 5","FIGS. 1-5"],"b":["300","208","80","300","320","324","190","326","310","1","324","190","328","190","202","330"]},{"@attributes":{"id":"p-0063","num":"0062"},"figref":["FIG. 6","FIGS. 1-6"],"b":["300","190","208","80","300","332","334","190","190","202","336","208","338","190"]},{"@attributes":{"id":"p-0064","num":"0063"},"figref":["FIG. 7A","FIGS. 1-2"],"b":["360","7","360","362","364","190","190","190","366","362","368","362","140","208","190","370","372"]},{"@attributes":{"id":"p-0065","num":"0064"},"figref":["FIG. 7B","FIGS. 1-2"],"b":["360","7","7","360","374","364","190","190","190","204","190","190","204","140","208","190","370","372"]},{"@attributes":{"id":"p-0066","num":"0065"},"figref":["FIG. 8","FIGS. 1"],"b":["340","208","190","204","380","2","8","380","110","380","190","380","380","110","208","190"]},"As shown,  provides an illustration of incremental policy modification based on user feedback. With reference to , , and , the policies  are expressed as rules granting access to a user's  location as indicated by boxes  with rule attributes including people or groups of people, day of the week, and time of the day. The smiling faces  denote location requests for which the user  is willing to disclose his location, and the \u201cno entry\u201d signs  denote requests for which the user  wishes no disclosure to be permitted. The policy system computer  gathers information on the user's  preferences as he or she audits (or gives feedback  on) actions taken by the policy system computer  on incoming requests. Policy errors correspond to the smiley faces  falling outside the boxes  (i.e., a location request should have been granted but was instead denied based on the current rules) and the \u201cno-entry\u201d signs  falling inside the boxes  (of which there are none in this particular example). Based on this feedback  and a discretized model of the policy  space, neighborhood search operators (e.g., extending a green time interval, shrinking a green time interval, splitting a green time interval in two, splitting a group of people) can be used to quickly generate a large number of incremental variations of the current policy . Each variation, which may be the result of applying one or more neighborhood search operators, depending on how the search is configured, can be evaluated based on its overall error, departure from the current policy  (to avoid incremental modifications that are too difficult for the user  to understand) and level of complexity\/fragmentation (to avoid overfitting and to introduce a bias towards simpler policies). This can be done using an objective function defined as a weighted sum of different metrics as described in more detail below.","As previously discussed, any of the natural language or GUI interfaces , ,  described with reference to  may enable the user  to interact with the policy system computer  by presenting the suggested policy modifications  to the user  in a variety of contexts or applications. Accordingly, as previously discussed, these include, without limitation: data loss prevention applications, network firewalls, application level firewalls, intrusion detection systems, social networking applications, routing devices on the data networks, home networking devices such as routers, firewalls, and access points, recommender systems for products and services, software development environments for programming, debugging, and static analysis, email filtering applications, or SCADA systems, among others.",{"@attributes":{"id":"p-0069","num":"0068"},"figref":["FIG. 9A","FIG. 9B","FIGS. 13-16"],"b":["400","400","202","400","400","110","140","400","190"]},"With reference to , , and , policies  may be viewed as condition\/action rules that specify what actions should be taken under what circumstances. It is assumed that some set of actions \u201cAction\u201d can be restricted according to various criteria (e.g., the time of day or identities of users generating events), all of which are captured in the set \u201cRestriction.\u201d A rule describes the restrictions under which a given set of actions may be executed. These restrictions may be defined as a logical conjunction: all must be fulfilled before the given actions may be taken. A rule thus may be defined as: Rule=P(Restriction)\u00d7P(Action), where we denote with P(S) the power set construction of set S. The policies  themselves may be represented by a set of such rules, connected by logical disjunction; thus, a policy  is simply a set of condition\/action pairings, and we have Policy=P(Rule). With respect to the examples described with reference to , B, it is also assumed that policies  are consistent, in the sense that no rule or set of rules with non-disjoint premises entail actions that conflict with one another. It should be understood, however, that the described embodiments are not limited to such policies. Rather, any machine understandable computational policy that is a representation of rules that govern a computational agent's behavior is intended to fall within the scope of the described embodiments. A set of events is represented by the set \u201cEvent.\u201d Events are evaluated by a function \u201cEvaluate,\u201d which compares an event to the appropriate policy  and executes either the actions specified within the rule or does nothing, modeled via the null action. As previously discussed, it is assumed that the users  have some means of providing feedback  on the suggested policy modifications  proposed by the PE module . The possible feedback  options are modeled via the set \u201cFeedback,\u201d which could, for instance, be a binary \u201cyes\u201d or \u201cno\u201d or a numeric scale, or some sort of free response, that indicates user satisfaction. This model may be generalized to support feedback  in the form of an alternate set of actions that the user  would have preferred to the actions taken by the PE module  on behalf of the user . This audit data, for a given event R and the associated evaluation decision D, may be captured via an audit function Audit (R, D).",{"@attributes":{"id":"p-0071","num":"0070"},"figref":["FIG. 10","FIGS. 1"],"b":["202","2","10","110","190","208","202","202"]},"With reference now to , , A, B, and , restrictions and actions, and by extension rules and policies , can be transformed in various ways, according to the specifics of the given policy enforcement system. A restriction that limits disclosure to a set of specified users , for instance, can be transformed via the deletion of one of the users  or the inclusion of a new one. For purposes of illustration, and not limitation, a restriction transformation function (Transform in ) may be implemented by the PE module  to return a set of neighboring restrictions reachable from a given restriction by a single, incremental transition step however that might be defined in a refinement of the policy , for example. Similarly, an action transformation function (GenAction in ) may be implemented by the PE module  to yield all incremental transformations of an action. These transformations may be extended to rules by implementing a rule generation function (GenRules in ) by the PE module  to take the union of all possible transformations on the restrictions and\/or actions within the rule and to eliminate one of those restrictions or actions or add an arbitrary new one. This transformation is further lifted to policies, as reflected in a neighborhood generation function (Neighbor in ), which may be implemented by the PE module  to consider all possible rule transformations yielded by the rule generation function, and additionally allows for the deletion of an entire rule from the policy  or the addition of a new rule with no restrictions or actions, modeled as the addition of the empty rule (\u00d8, \u00d8) to the policy .","An objective function may be defined by which to evaluate each neighbor in order to select those that offer the greatest policy improvement. As discussed in more detail below, the PE module  ranks one or more potential variations to the policy  based on a score of an objective function for each potential variation. In one embodiment, the objective function comprises a plurality of function elements. The plurality of function elements comprises a user satisfaction function element and at least one other function element. In various other implementations, the objective function is formulated with respect to additional functional elements based on complexity, deviation, and diversity, either alone or in any combination with the user satisfaction function element.","In one embodiment, the potential variations to the policy  may comprise one or more neighboring policies. The neighboring policies alone or in combination may be evaluated by the PE module  in accordance with the objective function. One or more of the neighboring policies may be selected by the PE module  in accordance with the score provided by the objective function. Additionally, candidate sets of neighboring policies are evaluated according to a diversity metric in order to ensure that a meaningfully diverse set of suggested policy modifications  is presented to the user . It should be understood, that the diversity metric is evaluated not on individual policies but on the whole set or a subsets of the potential suggested policy modifications .","Accordingly, in one implementation, the user-controllable policy learning system employs neighborhood search techniques to explore incremental modifications of the user's  current policy . Neighboring policies can be generated using operators that incrementally modify a current policy (e.g., restricting or generalizing a rule, merging two or more rules). These neighboring policies can be rated based on how well they match the feedback  from the user  (i.e., favoring those that minimize error), how much they deviate from the current policy (e.g., penalizing deviations that are too great), and how fragmented they are (e.g., to avoid overfitting and favor those that are easier to understand). By rapidly searching the neighborhood associated with an existing model, it is possibly to quickly identify a small number of suggestions that can be presented to the user . These and other implementation details of the neighborhood search techniques in the context of the user-controllable policy learning system are described hereinbelow.","In various embodiments, the policy system computer  determines a plurality of metrics: (1) user satisfaction metric, (2) deviation metric, (3) complexity metric, and (4) diversity metric to evaluate a set of neighboring policies in order to ensure that a meaningfully diverse set of suggested policy modifications  is presented to the user . The objective function, defined as a weighted sum of these different metrics, generates a score, which is used by the policy system computer  to select neighboring policies that offer the greatest policy improvement. These metrics may be applied by the PE module  alone or in any combination to evaluate a set of suggested policy modifications .","In one embodiment, the objective function may be defined simply in terms of a user satisfaction metric. The user satisfaction metric can be determined by the PE module  based on the feedback  provided by the at least one user . In one implementation, the user satisfaction metric on which to score neighboring policies may be ascertained by determining the amount of negative feedback generated by the user . By way of example, and not limitation, the user satisfaction metric may be determined by minimizing the value of a function E on a policy P given a history of events R that also incorporates user feedback . Assuming a numeric scale of user feedback, where higher numeric values indicate lower satisfaction (e.g., 0 indicates highest user satisfaction), an objective function may be defined as the sum of Audit(r, Evaluate(P, r)) for all events r\u03b5R. It will be appreciated, however, that maximizing user satisfaction does not protect against overfitting nor is it a guarantee of understandability or user satisfaction. Accordingly, additional metrics may be determined and evaluated by the policy system computer  as follows.","In addition, the objective function may be defined in accordance with a deviation metric. In one embodiment, the deviation metric determined by the PE module  measures by how much each of the one or more potential variations to a policy under consideration deviates from the policy  currently in force. In various embodiments, the deviation metric may be determined by the PE module  in accordance with one or more sub-function elements selected in accordance with a plurality of criteria such as, for example: (i) a function based on the number of individual variation steps required to transform the policy  currently in force into the potential policy variation under consideration, wherein variation steps are limited to variation steps that have been identified as easy to understand in a domain under consideration; (ii) a function that captures the effective difference between the potential policy variation under consideration and the policy  currently in force, when the policy specifies a finite number of decisions (i.e., the decisions that the potential policy variation would have made differently than the policy  currently in force); (iii) a function of user tolerance for deviation as collected through experiments with a representative set of users ; (iv) a function of user tolerance for deviation as measured based on historical data comprising user feedback ; and (v) a function based on the volumetric difference between the policy  currently in force and the potential policy variation under consideration when represented as geometric shapes in a multidimensional space, and (vi) domain-specific sub-function elements as appropriate for implementations of the method in practical application domains.","In one implementation, the deviation metric penalizes potential variations to a policy under consideration that result in the greatest deviation from the current policy , given that the suggested policy modifications  should be user-comprehensible. A penalty function may be implemented to assign a distance metric to two policies P and Q, \u0394(P, Q), where \u0394(P, P)=0.","Another metric that may be employed to define the objective function is a complexity metric. In one embodiment, the complexity metric may be determined by the PE module  to measure how complex each of the one or more potential variations to a policy under consideration is. The complexity metric may be determined in accordance with one or more sub-function elements selected in accordance with a variety of criteria such as, for example: (i) a function of a number of rules comprising the policy ; (ii) a function of a complexity of the rules based on attributes comprising any one of conditions, options, and actions associated with each rule; (iii) a function of complexity based on data collected through experiments with a representative set of users ; (iv) a function of complexity based on historical data comprising user feedback ; (v) a function based on patterns among the rules and patterns within individual rules, including entropy metrics; and (vi) domain-specific sub-function elements as appropriate for implementations of the method in practical application domains.","In one implementation, the complexity metric penalizes fine-granularity for the twofold purpose of preventing overfitting and giving precedence to less complex policies that are likely to be more readable by the user . By way of example, and not limitation, a complexity function may be implemented to assign a measure of complexity to a policy P, where 0 is the least complex. At the level of abstraction illustrated, for instance, in the abstract model of , complexity criteria may be defined as the number of rules and the number of restrictions and actions per rule.","Yet another metric that may be employed to define the objective function is a diversity metric. The diversity metric may be determined by the PE module  to measure how diverse the set of potential variations to a policy under consideration is. The diversity metric may be determined in accordance with one or more sub-function elements selected in accordance with a plurality of criteria such as, for example: (i) a function based on the number and\/or type of individual variation steps required to transform each of the one or more policy variations in the combination into any other policy variation in the combination; (ii) a function based on the number and\/or type of individual variation steps required to transform the policy  currently in force into each of the policy modifications in a combination under consideration; (iii) a function of the entropy of the policy modifications in a combination under consideration; and (iv) domain-specific sub-function elements as appropriate for implementations of the method in practical application domains. In one implementation, the diversity metric can be calculated as the entropy of a set of potential policy variations encoded appropriately for the relevant practical application domain.","In one embodiment, the objective function may be defined as a weighted sum of two or more of the previously described metrics: (1) user satisfaction metric, (2) deviation metric, (3) complexity metric, and (4) diversity metric. In one implementation, the objective function may be defined as a weighted sum of the user satisfaction metric, deviation metric, complexity metric, and diversity metric to determine a general objective function (GOF) or score as follows:\n\nGOF=user satisfaction metric+deviation metric+complexity metric+diversity metric.\n","In one implementation, penalty coefficients of \u03b3, \u03c1, \u03c6, and \u03c3 may be assigned for the function elements of user satisfaction, complexity, deviation, and diversity, respectively. Letting S be a set of potential suggested policy modifications, and letting P\u2032 be the user's  current policy  and R be a history of events, a general evaluation function may be defined for policies in this abstract as follows: E(S, P\u2032, R)=\u03a3(\u03b3\u03a3Audit(r, Evaluate(P, r))+\u03c1Complex(P)+\u03c6\u0394(P, P\u2032))+Diversity(S).","The above described policy evaluation framework can be refined as appropriate for specific applications and settings. For example, as discussed hereinbelow with respect to the location sharing example, location sharing policies are evaluated based on the number of negative audits they generate, and complexity is defined by the number of rules contained in a policy, the number of groups, users, and weekly patterns contained within those rules. Policy deviation is not here penalized explicitly. Rather, policy deviation is precluded above a certain threshold by restricting the number of transition steps allowed when generating the space of neighbors. The described embodiments, however, are not limited in this context.","In various embodiments, the policies described herein in connection with the described embodiments are not limited to policies that may be evaluated based on the application of individual rules (referred to hereafter as \u201cone-step\u201d policies). Rather, the policies described in connection with the described embodiments are intended to cover any and all possible computational policies, including those where multiple rules need to be sequentially evaluated to determine which action(s) to take. An example of a \u201cone-step\u201d policy is a policy where there is only a single layer of inference needed before determining what action(s) to take. In contrast, policies with multiple \u201clayers of inference\u201d may require the sequential firing of multiple rules before determining the next action. A simple example illustrating this concept might be the following multi-layer inference: \u201cIf X is my brother, then X is a family member,\u201d \u201cIf X is a family member, then X can see my location.\u201d A more complex example would be one where some rules just constrain the range of available options (e.g., \u201cIf X is a family member, then X can see my location,\u201d \u201cIf X is my brother, then X should not be able to see my location at a finer granularity than the city I am in.\u201d). It will be appreciated by those skilled in the art that the types of policies to be refined are not limited and there are many more complex examples, especially when one allows for negation, conflicts, and conflict resolution rules. The embodiments, however, are not limited in this context.",{"@attributes":{"id":"p-0087","num":"0086"},"figref":["FIG. 11","FIG. 1"],"b":["500","202","140","110","110","502","502","500","500","100","500","110","140","120"]},"The client computer  comprises at least an output device such as the user interface  to enable the user  to interact with the policy system computer  and the PE module  using a variety of representations such as textual representation, audible representation, electronic Braille, and electrodes, and the like. In various embodiments, the user interface  may be implemented with any of the user interfaces previously described with reference to , such as for example, the natural language interface , the GUIs  and , and\/or any combinations thereof.","As previously discussed with reference to , the policy system computer  communicates with the client computer  through the network . The policy system computer  communicates to the user  one or more suggested modifications  made by the PE module  to the existing policy . The policy system computer  receives feedback  from the user  in response to the suggested modifications  proposed by the PE module .","Operations for the above systems , ,  may be further described with reference to a logic flow. Although a particular logic flow is shown, it can be appreciated that the logic flow merely provides an example of how the general functionality described herein can be implemented within the systems , ,  previously discussed. Further, the given logic flow does not necessarily have to be executed in the order presented unless otherwise indicated. In addition, the given logic flow may be implemented by a hardware element, a software or firmware element executed by a processor, or any combination thereof.",{"@attributes":{"id":"p-0091","num":"0090"},"figref":["FIG. 12","FIG. 11"],"b":["600","600","100","200","500","110","140","600","110","140","110","140","600","600","140","110","600","500"]},"In one embodiment, the logic flow  provides a computer implemented method for updating a policy that is enforced by a program such as the PE module  executed by the policy system computer . The policy system computer  comprises at least one processor coupled to the memory . At  the policy system computer  communicates to at least one user  data regarding one or more decisions made by the PE module  over a period of time according to a policy . Each decision is made by the PE module  on the particular policy in force at the time the decision is made. The policy data for the policy  is stored in a machine readable format in the memory  portion of or coupled to the policy system computer . The term user  includes any number of users including an end user, an administrator of the policy enforcing computer system, a person associated with specifying and modifying the policy, and the like.","At , user feedback data indicative of feedback  provided by the user  regarding the decisions made by the PE module  according to the policy  are stored in a machine readable format in the memory . In various embodiments, the feedback  from the user  may include explicit feedback or implicit feedback in regards to the policy decisions made by the PE module  according to the policy . In one implementation, the implicit feedback may be considered by the policy system computer  unless the user  provides explicit feedback. This may occur, for example, when the user  provides only partial feedback on the policy decisions. In the absence of explicit feedback from the user , the user satisfaction function element may apply different weights to the implicit feedback according to whether the implicit feedback is determined positively or negatively. The implicit feedback may be determined in accordance with one or more variables. By way of example, and not limitation, these variables include the relative recentness of the one or more decisions made by the PE module , what users  provided the feedback, and the roles played by the users . In yet another implementation, the implicit feedback may include feedback data calculated by interpolating between two or more feedback data for which the user  has provided feedback.","In one embodiment, a plurality of users  interact with the policy . Accordingly, at least one of the plurality of users  reviews the suggested modifications  to the policy . The user  then selects one or more of the suggested modifications to the policy  and provides feedback on the policy . The selecting, by the plurality of users, the one or more suggested modifications to the policy  may be based on a vote by the plurality of users .","At , the policy system computer  identifies and ranks one or more potential variations to the policy  based on a score of an objective function. The plurality of function elements comprises a user satisfaction function element and at least one other function element that measures the extent to which the one or more potential variations are likely to be understandable by the at least one user . In one embodiment, the plurality of function elements includes a function that measures a likelihood that the one or more policy variations will be understandable by the at least one user  based on measures of the complexity of the policy variations under consideration and measures of the deviation between the policy variation under consideration and the policy  currently in force. In one embodiment, the policy variations are made substantially understandable in accordance with the one or more function elements by limiting a complexity of the policy variations and restricting a deviation from the policy  currently in force. In one embodiment, the policy variations are made substantially understandable by collectively representing a meaningfully diversified set of options for the policy variations presented to the at least one user . In one embodiment, the plurality of function elements comprises a function that measures the diversity of the policy variations presented to the at least one user  with the objective of presenting the at least one user with a meaningfully diversified set of policy variations.","In one embodiment, the one or more potential variations to the policy  comprise one or more neighboring policies. The one or more neighboring policies alone or in combination may be evaluated by the PE module  in accordance with the objective function. One or more of the neighboring policies are selected by the policy system computer  in accordance with the score provided by the objective function. The objective function comprises a plurality of function elements, and the plurality of function elements comprises a user satisfaction function element and at least one other function element.","In one embodiment, identification and ranking of potential policy variations based on a score of an objective function at  includes applying a computational procedure to identify candidate variations and to score the identified candidate policy variations according to the objective function. The computational procedure may be a computational search procedure. Suitable computational search procedures may include, for example, a meta-heuristic search procedure (e.g., a simulated annealing search procedure, a genetic algorithm, a Tabu search procedure, a random optimization procedure, a swarm intelligence algorithm, an evolutionary algorithm), a local search procedure or neighborhood search procedure, a breadth-first search procedure, a depth-first search procedure, a best-first search computational procedure, a hill-climbing search procedure, a beam search procedure, an iterative improvement search procedure, an A * search procedure, a branch-and-bound search procedure. In another embodiment, the computational procedure may be any of an optimization algorithm, a non-deterministic computational procedure, a heuristic computational procedure and a computational learning procedure.","At , the policy system computer  communicates to the at least one user  one or more suggested modifications  made by the PE module  based on the ranking of the one or more potential variations to the policy  currently in force. The suggested modifications  provide transparency to the underlying mechanics (rules) of the policy  to enhance user understandability of the policy  and promote the at least one user  to provide feedback  in response to the communication. Although currently shown as a user interface  presented at the client computer , the communication of the suggested modifications  to the user  may be made in any suitable manner. By way of example and not limitation, the suggested modifications  may be communicated to the user  through any suitable visual representation, textual representation, audible representation, electronic Braille, electrodes, and the like.","At , the policy enforcement computer modifies the policy data stored in the memory  based on one or more selections by the user  in response to the transmission of the suggested modifications . In one embodiment, the suggested modifications  are applied by subject to acceptance by the user . In addition, the user  also may reject or modify the suggested modifications  in response to the communication thereof. In one embodiment, modifying comprises selecting any one of accepting, rejecting, and modifying the one or more suggestions by the at least one user. In one embodiment, the at least one user  can initiate a modification independently of any of the suggested modifications.","As previously discussed, the objective function comprises a plurality of function elements, and the plurality of function elements comprises a user satisfaction function element and at least one other function element. The at least one other function element comprises a deviation function element, a complexity function element, and a diversity function element. In one embodiment, the PE module  determines at least user satisfaction metric based on the feedback  provided by the at least one user  to the PE module .","In one embodiment, the PE module  determines a deviation metric that measures by how much each of the one or more potential variations to a policy under consideration deviates from the policy  currently in force. In one embodiment, the deviation metric may be determined by the PE module  in accordance with one or more sub-function elements selected in accordance with a plurality of criteria such as, for example: (i) a function based on the number of individual variation steps required to transform the policy  currently in force into the potential policy variation under consideration, wherein variation steps are limited to variation steps that have been identified as easy to understand in a domain under consideration; (ii) a function that captures the effective difference between the potential policy variation under consideration and the policy  currently in force, when the policy specifies a finite number of decisions (i.e., the decisions that the potential policy variation would have made differently than the policy  currently in force); (iii) a function of user tolerance for deviation as collected through experiments with a representative set of users ; (iv) a function of user tolerance for deviation as measured based on historical data comprising user feedback ; and (v) a function based on the volumetric difference between the policy  currently in force and the potential policy variation under consideration when represented as geometric shapes in a multidimensional space, and (vi) domain-specific sub-function elements as appropriate for implementations of the method in practical application domains.","In one embodiment, the PE module  determines a complexity metric that measures how complex each of the one or more potential variations to a policy under consideration is. The complexity metric may be determined in accordance with one or more sub-function elements selected in accordance with a variety of criteria such as, for example: (i) a function of a number of rules comprising the policy ; (ii) a function of a complexity of the rules based on attributes comprising any one of conditions, options, and actions associated with each rule; (iii) a function of complexity based on data collected through experiments with a representative set of users ; (iv) a function of complexity based on historical data comprising user feedback ; (v) a function based on patterns among the rules and patterns within individual rules; and (vi) domain- specific sub-function elements as appropriate for implementations of the method in concrete application domains.","In one embodiment, the PE module  evaluates the set of one or more suggested modifications  according to a diversity function element to determine a diversity metric. The diversity metric measures how diverse the set of one or more potential variations to the policy under consideration is. The diversity metric may be determined in accordance with one or more sub-function elements selected in accordance with a plurality of criteria such as, for example: (i) a function based on the number or type of individual variation steps required to transform each of the one or more sub-function element; (ii) a function based on the number or type of individual variation steps required to transform the policy  currently in force into each of the policy modifications in a combination under consideration; (iii) a function of the entropy of the policy modifications in a combination under consideration; and (iv) domain-specific sub-function elements as appropriate for implementations of the method in concrete application domains.","In one embodiment, the objective function comprises a general policy evaluation function comprising at least the user satisfaction metric function element. The general policy evaluation function also comprises a deviation function element to determine by how much each of the one or more potential variations to a policy under consideration deviates from the policy currently in force. The general policy evaluation function also comprises a complexity function element to determine how complex each of the one or more potential variations to a policy under consideration is. The general policy evaluation function also comprises a diversity function element to determine how diverse the set of one or more potential variations to the policy under consideration is. In one embodiment, the policy system computer  determines a general objective function (GOF) by any combination of the user satisfaction metric element according to the following combination:\n\nGOF=user satisfaction metric+deviation metric+complexity metric+diversity metric.\n","In yet another embodiment, the objective function comprises a policy evaluation function comprising at least the user satisfaction function element based on the feedback  provided by the at least one user . The objective function also comprises a combination of any one of a deviation function element to determine by how much each of the one or more potential variations to a policy under consideration deviates from the policy currently in force , a complexity function element to determine how complex each of the one or more potential variations to a policy under consideration is, and a diversity function element to determine how diverse the one or more potential variations to the policy under consideration is.","While the embodiments have been described generally above, a location sharing example is now described with reference to  for illustrative purposes. The location sharing example is based on the PeopleFinder location-based social network application (PeopleFinder Application) developed at the Mobile Commerce Lab at Carnegie Mellon University, screenshot  of which is shown in . The PeopleFinder Application allows users  () of location-enabled laptops and cell phones to share their location with their network of friends in a privacy-sensitive way. Privacy policies  () in the PeopleFinder Application permit disclosure based on three criteria: the identity (or group membership) of the user making the request, the weekday and time of the request, and the location of the requested user . Thus, privacy policies  can be comprised of rules such as \u201cAllow disclosure to the group Co-Workers on weekdays between 9 and 5, but only when I am actually at my office.\u201d In addition, users  of the PeopleFinder Application can provide feedback  () on system decisions. The background window  illustrates the feedback interface, whereby users  can review a history of requests for their location and indicate their level of satisfaction with the disclosure decisions made by the system. Users  can also ask for additional details about the requests and obtain explanations of the system's actions: for instance, what policy rule applied to allow disclosure of the user's  location, or why a request was denied.","The PeopleFinder Application has been deployed in numerous field studies involving over 100 total users . Detailed log data collected during these deployments form one basis for validation experiments discussed in the subsequent description. These data also illustrate why the PeopleFinder Application is a suitable example on which to validate the framework: first and foremost, users of the PeopleFinder Application have demonstrated the usual difficulty specifying their privacy policies  by hand, achieving an average initial accuracy of 60%, and are thus prime candidates for a support system in accordance with the described embodiments.","Furthermore, the observed user behavior indicates that, when attempting to improve their policies  by hand, users  generally make small, incremental changes. As will be shown the space of neighboring policies is vast. Therefore, those skilled in the art will appreciate that the learning system in accordance with the described embodiments, which can sample a larger subspace of neighbors than the user could realistically hope to, will assist users  in selecting the best incremental change to their policies .",{"@attributes":{"id":"p-0109","num":"0108"},"figref":["FIG. 9B","FIG. 9A","FIG. 9B"],"b":["202","202"]},"We now turn to an instantiation of the user-controllable policy learning system for incremental policy refinement in the context of the PeopleFinder Application previously introduced above. Accordingly, a simple neighborhood search implementation of the user-controllable policy learning system according to the described embodiments, will be described in the context of the PeopleFinder Application. The performance is based on simulated scenarios based on data derived from experimental deployments of the PeopleFinder Application system and compared with the performance achieved by users who manually modified their policies during the course of these deployments.",{"@attributes":{"id":"p-0111","num":"0110"},"figref":"FIG. 14","b":["750","752"]},"As previously discussed, one implementation of the user-controllable policy learning system in accordance with the described embodiments involves using neighborhood search to explore incremental modifications of the user's current policy. These modifications can be generated using transformation operators that are selected to cover a meaningful set of easy-to-understand modifications to the user's  current policy . Results presented below assume the following set of policy transformation operators: (1) the deletion of an existing rule, or the addition of a new rule permitting disclosure to a given user during a given time span on a given day; (2) the expansion or contraction of either the start or end of a time-span by up to an hour; (3) the deletion of a day from a duration within a rule, or the addition of a day to a rule duration; and the addition of a person to a group, or the deletion of a person from a group.","In the results provided below, suggestions were selected by randomly generating and evaluating a large number of neighbors of the user's  current policy . Although each neighbor was equally likely to be selected, those skilled in the art will appreciate that more sophisticated implementations that could assign different probabilities to different operators are clearly within the scope of the described embodiments. Experiments were also conducted with instantiations that varied based on the number of neighbors generated for a given policy as well as the number of successive moves (or policy transformations) allowed at each step. Intuitively, several successive moves allow the procedure to explore a wider, more diverse neighborhood, though at the risk of suggesting policy modifications that are more difficult for the user  to understand. Accordingly, the sensitivity of the experiment to variations in these parameter values was evaluated to determine to what extent a very limited number of moves (e.g., just one policy transformation) might be sufficient to generate suggestions that would yield meaningful improvements in accuracy.","Each time the user-controllable learning process in accordance with the described embodiments is invoked, it generates and evaluates a number of policy modifications (based on user satisfaction, complexity, and deviation from the current policy) and uses the top rated transformations to suggest possible policy modifications to the user . The following experimental results were limited to generating a single suggestion each time, namely the policy transformation with the highest score among all those generated by the neighborhood search procedure. The described embodiments, however, are not limited in this context.","To validate the user-controllable learning process in accordance with the described embodiments, data derived from experimental campus deployments of the PeopleFinder Application was used. These deployments, which spanned between 1 and 8 weeks, involved a total of over 100 participants. The pilots, which confirmed that the users  often have great difficulty articulating their policies , also provided a baseline against which we were able to compare the performance of the user-controllable policy learning algorithms in accordance with the described embodiments. Specifically, detailed logs collected during the deployments of the PeopleFinder Application were used to characterize the complexity of initial policies  defined by the users , the average number of daily requests users received, and the frequency with which the users  revise these policies . This information was, in turn, used (1) to simulate a large number of scenarios representative of user behavior observed during our deployments and (2) to extrapolate plausible usage scenarios for the user-controllable policy learning process. In the experiments reported herein, the results from a first set of these scenarios are presented in which the users  received an average of five location requests per day, audited and revised their policies every other day, with policy revision limited to a single policy modification. Assuming this level of usage, the results on policy improvement are reported based on the number of weeks using the system. It is further assumed that the modification selected by the user  was the top ranked suggestion generated by the neighborhood search implementation of the user-controllable policy learning algorithm, using all previously audited data.",{"@attributes":{"id":"p-0116","num":"0115"},"figref":"FIG. 15","b":"760"},{"@attributes":{"id":"p-0117","num":"0116"},"figref":"FIG. 16","b":["770","190"]},{"@attributes":{"id":"p-0118","num":"0117"},"figref":["FIG. 17","FIG. 17","FIG. 17"],"b":["110","120","1000","1000","1000","1000","1100","1100","1200","1300","1210","1200","1210"],"sub":["1\u2212n ","1\u2212m "]},"The computer system  generally comprises a variety of computer readable media. Computer readable media can be any available media that can be accessed by the computer system  and includes both volatile and nonvolatile media, removable, and non-removable media. Computer storage media includes volatile and nonvolatile, removable, and non-removable media implemented in any method or technology for storage of information such as computer readable instructions, data structures, program modules, or other data. Computer storage media includes, but is not limited to, Random Access Memory (RAM), Dynamic RAM (DRAM), Double-Data-Rate DRAM (DDRAM), Synchronous DRAM (SDRAM), Static RAM (SRAM), Programmable ROM (PROM), Read Only Memory (ROM), Electrically Erasable Programmable Read Only Memory (EEPROM), flash memory, polymer memory such as ferroelectric polymer memory, ovonic memory, phase change or ferroelectric memory, silicon-oxide- nitride-oxide-silicon (SONOS) memory, Compact Disk Read Only Memory (CDROM), Compact Disc-rewritable (CDRW) Digital Versatile Disks (DVD) or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can accessed by the computer system . It is worthy to note that some portion or the entire computer storage medium may be included in other elements of the apparatus computer system . For instance, some or all of computer storage medium may be included on a same integrated circuit or chip with elements of the computer system  (e.g., processing unit ). Alternatively, some portion or the entire computer storage medium may be disposed on an integrated circuit or other medium (e.g., a hard disk drive) that is external. Communication media typically embodies computer readable instructions, data structures, program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term \u201cmodulated data signal\u201d means a signal that has one or more of its characteristics set or modified in such a manner as to encode information in the signal. By way of example, and not limitation, communication media includes wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, RF, infrared, and other wireless media. The embodiments are not limited in this context.","The system memory  includes computer storage media in the form of volatile and\/or nonvolatile memory such as ROM  and RAM . A basic input\/output system  (BIOS), containing the basic routines that help to transfer information between elements within the computer system , such as during start-up, is typically stored in the ROM . The RAM  typically contains data and\/or program modules that are immediately accessible to and\/or presently being operated on by the processing unit . By way of example, and not limitation,  illustrates an operating system , one or more application programs , other program modules , program data . As previously discussed, the PE module  may comprise embedded applications implemented as firmware, software, hardware or any combination thereof. Accordingly, the other program modules  may comprise software instructions to implement the PE module . Alternatively, the PE module , which is shown in phantom to indicate that it may be implemented as hardware, software, or firmware, may reside within the computer system .","The computer system  also may comprise other removable\/non-removable, volatile\/nonvolatile computer storage media. By way of example only,  illustrates a hard disk drive  that reads from or writes to non-removable, nonvolatile magnetic media, a magnetic disk drive  that reads from or writes to a removable, nonvolatile magnetic disk , and an optical disk drive  that reads from or writes to a removable, nonvolatile optical disk , such as a CD ROM, CDRW or other optical media. Other removable\/non-removable, volatile\/nonvolatile computer storage media that can be used in the example operating environment include, but are not limited to, magnetic tape cassettes, flash memory cards, digital versatile disks, digital video tape, solid state RAM, solid state ROM, and the like. The hard disk drive  is typically connected to the system bus  through a non-removable memory interface such as interface , and magnetic disk drive  and optical disk drive  are typically connected to the system bus  by a removable memory interface, such as interface .","The drives and their associated computer storage media discussed above and illustrated in  provide storage of computer readable instructions, data structures, program modules, and other data for the computer system . In , for example, the hard disk drive  is illustrated as storing an operating system , one or more application programs , other program modules , and program data . Note that these components can either be the same as or different from the operating system , the one or more application programs , the other program modules , and the program data . The operating system , the one or more application programs , the other program modules , and the program data  are given different numbers here to illustrate that, at a minimum, they are different copies. A user may enter commands and information into the computer system  through input devices such as a keyboard  and pointing device , commonly referred to as a mouse, trackball, or touch pad, and a scanner . Other input devices (not shown) may include a microphone, joystick, game pad, satellite dish, or the like. These and other input devices are often connected to the processing unit  through a user input interface  that is coupled to the system bus , but may be connected by other interface and bus structures, such as a parallel port, game port or a universal serial bus (USB). A display device  or other type of display device is also connected to the system bus  via an interface, such as a video interface , which may in turn communicates with video memory (not shown). In addition to the display device , computer systems also may include other peripheral output devices such as speakers  and a printer , which may be connected through an output peripheral interface .","The computer system  may operate in a networked or distributed environment using logical connections to one or more remote computers, such as a remote computer . The remote computer  may be a personal computer, a server, a router, a network PC, a peer device or other common network node, and typically includes many or all of the elements described above relative to the computer system , although only a memory storage device  has been illustrated in . The logical connections depicted in  include a local area network (LAN)  and a wide area network (WAN) , but may also include other networks\/buses. Such networking environments are commonplace in homes, offices, enterprise-wide computer networks, intranets, and the Internet.","When used in a LAN networking environment, the computer system  is connected to the LAN  through a network interface or adapter . When used in a WAN networking environment, the computer system  generally includes a modem  or other means for establishing communications over the WAN , such as the Internet. The modem , which may be internal or external, may be connected to the system bus  via the user input interface , or other appropriate mechanism. In a networked environment, program modules depicted relative to the computer system , or portions thereof, may be stored in the remote memory storage device. By way of example, and not limitation,  illustrates one or more remote application programs  as residing on the memory device . It will be appreciated that the network connections shown are non limiting examples and other means of establishing a communications link between the computers may be used.","Various distributed computing frameworks have been and are being developed in light of the convergence of personal computing and the Internet. Individuals and business users alike are provided with a seamlessly interoperable and Web-enabled interface for applications and computing devices, making computing activities increasingly Web browser or network-oriented.","For example, the MICROSOFT\u00ae .NET platform includes servers, building-block services, such as Web-based data storage and downloadable device software. While the embodiments described herein in connection with software residing on a computing device, one or more portions of the described embodiments also may be implemented via an operating system, application programming interface (API) or a \u201cmiddle man\u201d object between any of a coprocessor, a display device, and a requesting object, such that operation according to the described embodiments may be performed by, supported in, or accessed via all of .NET's languages and services, and in other distributed computing frameworks as well.","Reference throughout the specification to \u201cvarious embodiments,\u201d \u201csome embodiments,\u201d \u201cone embodiment,\u201d or \u201can embodiment\u201d means that a particular feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment. Thus, appearances of the phrases \u201cin various embodiments,\u201d \u201cin some embodiments,\u201d \u201cin one embodiment,\u201d or \u201cin an embodiment\u201d in places throughout the specification are not necessarily all referring to the same embodiment. Furthermore, the particular features, structures, or characteristics may be combined in any suitable manner in one or more embodiments. Thus, the particular features, structures, or characteristics illustrated or described in connection with one embodiment may be combined, in whole or in part, with the features structures, or characteristics of one or more other embodiments without limitation."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"FIGURES","p":["The novel features of the various described embodiments are set forth with particularity in the appended claims. The various embodiments, however, both as to organization and methods of operation may be understood in accordance with the following description taken in conjunction with the accompanying drawings as follows.",{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 7A"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 7B"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 9A"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":["FIG. 9B","FIG. 9A"]},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 17"}]},"DETDESC":[{},{}]}
