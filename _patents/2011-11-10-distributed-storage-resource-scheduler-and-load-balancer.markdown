---
title: Distributed storage resource scheduler and load balancer
abstract: Distributed storage resources having multiple storage units are managed based on data collected from online monitoring of workloads on the storage units and performance characteristics of the storage units. The collected data is sampled at discrete time intervals over a time period of interest, such as a congested time period. Normalized load metrics are computed for each storage unit based on time-correlated sums of the workloads running on the storage unit over the time period of interest and the performance characteristic of the storage unit. Workloads that are migration candidates and storage units that are migration destinations are determined from a representative value of the computed normalized load metrics, which may be the 90th percentile value or a weighted sum of two or more different percentile values.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08935500&OS=08935500&RS=08935500
owner: VMware, Inc.
number: 08935500
owner_city: Palo Alto
owner_country: US
publication_date: 20111110
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["This application is a continuation-in-part of U.S. patent application Ser. No. 12\/566,435, filed Sep. 24, 2009, the entire contents of which are incorporated by reference herein.","A distributed storage system employs multiple storage arrays and serves multiple client computers over a network. In such a system, loads on the storage arrays will vary as demands from the client computers fluctuate. To optimize performance of the storage arrays, loads on the multiple storage arrays are observed and balanced when they become uneven.","Various approaches have been developed to improve the performance of storage arrays. The focus of one approach is storage array reconfiguration. In this approach, the storage array is reconfigured to better serve the applications running on it. Another approach is storage array performance modeling, where the storage arrays are modeled in terms of their performance. The goal is to create a model, which accurately describes the performance of the storage arrays. Given such a model, various \u201cwhat if\u201d scenarios can be modeled prior to implementation. A third approach implements block level migrations. The idea here is carry out block level migrations within a storage array or across multiple storage arrays to improve performance.","In virtualized computer systems, in which disk images of virtual machines are stored in the storage arrays, disk images of virtual machines are migrated between storage arrays as a way to balance the loads across the storage arrays. For example, the Storage VMotion\u2122 product that is available from VMware Inc. of Palo Alto, Calif. allows disk images of virtual machines to be migrated between storage arrays without interrupting the virtual machine whose disk image is being migrated or any applications running inside it. However, the usage so far has been mostly manual and based on observations of loads on the storage arrays and there is a need for automating the task of identifying what to move and where.","One or more embodiments of the present invention provide a distributed storage system having multiple storage units that are managed based on data collected from online monitoring of workloads on the storage units and performance characteristics of the storage units. The collected data is sampled at discrete time intervals over a time period of interest, such as a congested time period. Normalized load metrics are computed for each storage unit based on time-correlated sums of the workloads running on the storage unit over the time period of interest and the performance characteristic of the storage unit. Workloads that are migration candidates and storage units that are migration destinations are determined from a representative value of the computed normalized load metrics, which may be the 90th percentile value or a weighted sum of two or more different percentile values.","A method of managing distributed storage resources including at least a first storage unit and a second storage unit, according to an embodiment of the present invention, includes the steps of: while the first storage unit and the second storage unit are online, monitoring workloads associated with objects stored in the first storage unit and the second storage unit at multiple points in time over a time interval, and monitoring performance of the first storage unit and the second storage unit; computing normalized load metrics for the first storage unit based on time-correlated sums of the workloads monitored on the first storage unit over the time interval and the performance of the first storage unit; computing normalized load metrics for the second storage unit based on time-correlated sums of the workloads monitored on the second storage unit over the time interval and the performance of the second storage unit; and identifying one or more of the objects as candidates for migration between the first storage unit and the second storage unit based on the computed normalized load metrics of the first storage unit and the second storage unit.","A method of migrating workloads between a first storage unit and a second storage unit of a shared storage system that includes physically separate storage arrays, according to an embodiment of the present invention, includes the steps of: while the first storage unit and the second storage unit are online, monitoring workloads on the first storage unit and the second storage unit at multiple points in time over a time interval; computing normalized load metrics for the first storage unit based on time-correlated sums of the workloads monitored on the first storage unit over the time interval; computing normalized load metrics for the second storage unit based on time-correlated sums of the workloads monitored on the second storage unit over the time interval; and migrating one of the workloads between the first storage unit and the second storage unit based on the computed normalized load metrics of the first storage unit and the second storage unit.",{"@attributes":{"id":"p-0017","num":"0016"},"figref":["FIG. 1","FIG. 1"],"b":["110","110","121","122","123","110","112","114","116","118","116","118"]},"The virtual machines, VM , VM , and VM , run on top of a virtual machine monitor , which is a software interface layer that enables sharing of the hardware resources of host computer  by the virtual machines. Virtual machine monitor  may run on top of the host computer's operating system or directly on hardware components of the host computer. In some embodiments, virtual machine monitor  runs on top of a hypervisor that is installed on top of the hardware resources of host computer . Together, the virtual machines and virtual machine monitor  create virtualized computer systems that give the appearance of being distinct from host computer  and from each other. Each virtual machine includes a guest operating system and one or more guest applications. The guest operating system is a master control program of the virtual machine and, among other things, the guest operating system forms a software platform on top of which the guest applications run.","Data storage for host computer  is served by a storage area network (SAN), which includes a storage array  (e.g., a disk array), a storage array  (e.g., a disk array), and a switch (SAN fabric)  that connects host computer  to storage array  and storage array . Switch , illustrated in the embodiment of , is a SAN fabric switch, but other types of switches may be used. As illustrated, switch  is further connected to virtual machine host computers, other than host computer , whose files are also stored in storage array , and storage array . Typically, storage array  and storage array  are exposed to the host computers as logical unit numbers (LUNs), and there is a mapping between each LUN and physical devices, such as disk drives, in the storage arrays. In certain embodiments disclosed herein, it is assumed that there is a one-to-one correspondence between the LUNs and the storage arrays, such that storage array  is LUN  and storage array  is LUN . However, the invention is applicable to storage array configurations where the correspondence between the LUNs and the storage arrays is not one-to-one. Distributed storage systems other than SAN systems may also be used. For example, a network attached storage (NAS) system configured to expose a file system volume to clients may also be used.","In the embodiment illustrated in , LUN  includes disk images of four virtual machines and LUN  includes disk images of five virtual machines. VM  running in host computer  has a disk image  stored in LUN , and VM  running in host computer  has a disk image  stored in LUN . Disk image , which is the disk image corresponding to VM  running in host computer , is illustrated in dashed lines because it has been migrated to LUN . Disk images , , , , ,  are disk images of virtual machines running in other virtual machine host computers. Disk images , ,  are stored in LUN , and disk images , ,  are stored in LUN .","A software component  is implemented inside virtual machine monitor  to monitor input-output operations (IOs) of the virtual machines. Alternatively, software component  may be implemented in the file system layer of the hypervisor. One example of software component  is the vscsiStats utility that is available from VMware Inc. Software component  generates histograms for the following parameters: (1) seek distance or randomness, which is a measure of the spatial locality in the workload measured as the minimum distance in terms of sectors or logical block numbers from among the last k number of IOs, a small distance signifying high locality; (2) IO data length, represented in different bins of size 512 Bytes, 1 KB, 2 KB, etc.; (3) outstanding IOs, denoting the queue length that virtual machine monitor  sees from a virtual machine; (4) 10 rate; (5) 10 latency, which is measured for each 10 from the time it gets issued by the virtual machine until the virtual machine is interrupted for its completion; and (6) read\/write ratio, which is a measure of number of read requests in relation to write requests. The histograms may be collected on a per virtual machine basis, a per virtual-disk basis (e.g., in cases where a single VM has multiple virtual disks), or any other technically feasible basis.",{"@attributes":{"id":"p-0022","num":"0021"},"figref":["FIG. 2","FIGS. 3-4","FIG. 3","FIG. 4","FIGS. 6 and 7"],"b":["126","101","6","7"]},"The processes running in virtual machine management center  that carry out the methods illustrated in  and - rely on an analysis model that provides a workload metric, w, in relation to a performance metric, P, of the LUN that has been assigned to handle the workload. This measure is referred to herein as the \u201cload metric\u201d and is modeled as w\/P. This load metric varies over time, as  requests associated with the workload being modeled and the loads on the LUN assigned to the workload being modeled change over time. In alternative embodiments, the performance metric, P, may be monitored at a RAID (Redundant Array of Independent Disks) group level. This can be done either by aggregating the statistics for all LUNs created per RAID group or by using some special interface provided by the storage array vendor.","The workload is modeled using the following equation for JO latency, L, where OIO is the number of outstanding IOs, IOsize is the average size of IOs, read % is the percentage of read requests in the IOs, and random % represents the spatial locality of the workload.",{"@attributes":{"id":"p-0025","num":"0024"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"L","mo":"=","mfrac":{"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msub":{"mi":"K","mn":"1"},"mo":"+","mi":"OIO"}},{"mo":["(",")"],"mrow":{"msub":{"mi":"K","mn":"2"},"mo":"+","mi":"IOsize"}},{"mo":["(",")"],"mrow":{"msub":{"mi":"K","mn":"3"},"mo":"+","mfrac":{"mrow":{"mi":["read","%"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},"mn":"100"}}},{"mo":["(",")"],"mrow":{"msub":{"mi":"K","mn":"4"},"mo":"+","mfrac":{"mrow":{"mi":["random","%"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},"mn":"100"}}}],"mo":["\u2062","\u2062","\u2062"]},"msub":{"mi":"K","mn":"5"}}}},{"mrow":{"mo":["[","]"],"mrow":{"mi":"Eqn","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"1"}}}]}}}}},"The numerator of Eqn. 1 represents the workload metric, w. The denominator Krepresents a normalization factor, which characterizes overall performance that is specific to a related storage device. Higher performance storage devices are characterized by larger values of K. In one embodiment, K5 corresponds to performance metric P. Variations of this equation, which should be evident to those skilled in the art, may be used in other embodiments of the invention. In Eqn. 1, a high random % value correlates to the workload being highly random. A low random % value correlates to the workload being highly sequential. In one embodiment, the random % is derived as 100\u00d7(sum of all IOs that are greater than 2-3 MB away in the logical space)\/(all IOs). It is also possible to assign randomness weight values in such a manner that IOs that are farther away receive higher weight values. One way to compute the random % without histograms is to keep a runlength parameter, where runlength is incremented if the next  is very close to the previous one; otherwise it is reset. In such a case, the random %=100\/runlength.","As a first step, IOs to the workload are varied over a period in such a manner that OIO, IOsize, read %, and random % of varying sizes are collected by software component . For each set of <OIO, IOsize, read %, random %>, the IO latency is also collected by software component . The constants, K, K, K, and K, are computed on the basis of the collected data in the following manner.","To compute K, two IO latency values, Land L, with different OIO values and the same value for the other three parameters are used.",{"@attributes":{"id":"p-0029","num":"0028"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":"K","mn":"1"},"mo":"=","mfrac":{"mrow":[{"msub":{"mi":"OIO","mn":"1"},"mo":"-","mrow":{"msub":{"mi":"OIO","mn":"2"},"mo":"*","mrow":{"msub":[{"mi":"L","mn":"1"},{"mi":"L","mn":"2"}],"mo":"\/"}}},{"mrow":{"msub":[{"mi":"L","mn":"1"},{"mi":"L","mn":"2"}],"mo":"\/"},"mo":"-","mn":"1"}]}}},{"mrow":{"mo":["[","]"],"mrow":{"mi":"Eqn","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},"mn":"2"}}}]}}}}},"This is repeated for all pairs of IO latency values where the OIO values differ while the other three variables remain the same. A number of Kvalues are obtained in this manner and the median of the different Kvalues is selected. Selecting a median ensures that the Kvalue is not biased by few extreme values. This procedure is repeated to compute each of K, K, and K. In one embodiment, K1 to K4 are computed empirically and then used as fixed values in the algorithm. Once these values are computed and fixed in the algorithm, online monitoring is performed to obtain workload specific parameters, such as OIO, IO size, read % and random %.","The performance metric of LUNs is dependent on device level characteristics, such as number of physical disks backing the LUN, rotational speed of the disk drives, average seek delay, etc., which are device-level information generally unknown to the host computers. Storage arrays are exposed to the host computers as a LUN and generally do not expose an application programming interface that allows the host computer to query the LUN regarding device level information. This complicates any load balancing decisions because an administrator who is contemplating a virtual disk migration from one storage array to another needs to know if the move is to a LUN that is backed with 20 disks or to a LUN that is backed with 5 disks.","For modeling the performance of LUNs, the IO latency is used as the main performance metric. For each LUN, data pairs consisting of number of outstanding IOs (OIOs) and average IO latency observed are collected for a number of time intervals. This information can be gathered with little overhead because the host computer knows the average number of outstanding IOs that are sent to a LUN and it already measures the average IO latency experienced by the IOs. As previously described, this information is collected by software component .","It is well understood that IO latency increases more or less linearly with the increase in number of outstanding IOs. Given this knowledge, the set of data points <OIO, IO latency> is collected online over a period of time and a linear fit line which minimizes the least squares error for the data points is computed. The parameter P is taken as the inverse of the slope of the linear fit line, such that higher P's generally correlate to higher performance. The parameter P is computed in this manner for each LUN. Alternatively, the technique for modeling the performance of LUNs that is described in U.S. patent application Ser. No. 12\/869,878, filed on Aug. 27, 2010, which is incorporated by reference herein, may be used in computing the parameter P in the embodiments of the present invention.","In cases where device level information is available, the modeling can be performed on a per storage array basis, so that the parameter P is computed above for each storage array. Other ways of modeling the parameter P are possible. For example, in one embodiment, read OIOs and read latencies are used instead of overall OIOs and overall latency. In another embodiment, data points associated with large  sizes (e.g., greater than 64 KB) and\/or high sequentiality (i.e., low randomness, e.g., random % less than 10%) are ignored. The goal with each variation discussed above is to make the device model as independent of the workload as possible.","For load balancing, workloads of virtual machines are grouped based on the location of their disk images (i.e., LUN in which the disk images are stored) and a normalized load metric is computed for each LUN (i.e., the sum of workload metrics, W, for all workloads associated with the LUN divided by the parameter P for the LUN). For example, in the embodiment illustrated in , the normalized load metric for LUN  is the sum of workload metrics for workloads associated with disk images , , ,  divided by the parameter P for LUN , and the normalized load metric for LUN  is the sum of workload metrics for workloads associated with disk images , , , ,  divided by the parameter P for LUN . Heuristics are then employed to find migration candidates and migration destinations that will balance the workloads across the LUNs. Any technically feasible technique can be used to perform this balancing. In one embodiment, the LUNs with the highest and lowest normalized load metrics are identified. Then, a virtual machine disk image in the LUN with the highest normalized load metric is migrated to the LUN with the lowest normalized load metric. This process is repeated until there is good balance across the LUNs.","In alternative embodiments, searching for the migration candidates can be biased in several ways. In one embodiment, disk images of virtual machines that have the smallest size\/L are selected first, so the amount of data migrated can be minimized while maintaining the same effect on load balancing. In another embodiment, disk images of virtual machines that have the smallest current IO rate are selected first, so that the impact of migration now is minimal.","Recommendations for migrating disk images of virtual machines can be presented to the user as suggestions or can be carried out automatically during periods of low activity. In addition, recommendations on initial placement of disk images of virtual machines in LUNs can be made. For example, LUNs with relatively small normalized load metrics may be selected as candidates for initial placement of disk images of new virtual machines.","In a further refinement, workloads are divided into three groups: sequential, local (somewhat sequential) and random. Experiments have confirmed that random workloads interfere destructively with sequential workloads. Since the objective of the model described above is for virtual machine disk image placement, performance loss from this effect can be minimized by careful workload segregation. Thus, as part of the load balancing step, affinity and anti-affinity hints can be incorporated by running multiple rounds of the model, one each for the segregated set of storage units. For example, the model is run for all the storage units hosting sequential workloads and load balancing is performed amongst these storage units. The model is then run again for the second group of storage units hosting somewhat sequential workloads and load balancing is performed amongst these storage units. The model is then run again for the third group of storage units hosting random workloads and load balancing is performed amongst these storage units. In addition, sequential workloads are identified and placed on isolated devices as much as possible. In an alternative embodiment, workloads can be divided into two groups, sequential and random, so that the user can create separate LUNs for the sequential workloads or find better placement for the sequential workloads.",{"@attributes":{"id":"p-0039","num":"0038"},"figref":["FIG. 3","FIG. 5A"],"b":["311","312","312","313","330"],"sub":["i","i","i","i","1","2","3","4","1","2","3","4","1","2","3","4","i","i","i","i"]},"For the LUN for which a normalized load metric is being computed, the number of outstanding IO requests and the average IO latency are monitored over a time interval (Step ). Step  is repeated as needed for different time intervals. When a sufficient number of data points for a linear fit have been collected (Step ), the linear fit is carried out and the slope of the linear fit line is computed (Step ). In Step , the performance metric, P, for the LUN is computed as 1\/(slope of the linear fit line).","In step , the normalized load metric for the LUN is computed as the time-correlated sum of the workload metrics for all workloads associated with the LUN, as obtained from Step , divided by the performance metric for the LUN, as obtained from Step . This normalized load metric is computed at each point in time the time-correlated sum is computed in Step . In one embodiment, the 90th percentile value of this time series of normalized load metrics is used as the representative load metric for the LUN. In another embodiment, a weighted sum of two or more different percentile values of this time series of normalized load metrics is used as the representative load metric for the LUN. For example, the weighted sum may give a weight of 0.5 to the 90th percentile value, a weight of 0.3 to the 70th percentile value, and a weight of 0.2 to the 50th percentile value.","The method of  is repeated for all LUNs so that a workload metric for all workloads and a representative normalized load metric for all LUNs are obtained.",{"@attributes":{"id":"p-0043","num":"0042"},"figref":["FIG. 4","FIGS. 6 and 7"],"b":["412","414","416","416","414","414","416"]},"The selected workload is migrated from the source LUN to the destination LUN in Step . If the workload is a virtual machine workload, the migration may be carried out using the Storage VMotion\u2122 product that is available from VMware Inc. of Palo Alto, Calif. In Step , the normalized load metrics for the two LUNs between which the selected workload was migrated are recomputed. If the load balancing is satisfactory (Step ), the process ends. If not, Steps - are repeated.","When generating workload migration recommendations, priority may be given to recommendations that are consistent over a range of percentile values for normalized load metrics. For example a workload migration recommendation that is consistent when considering 90th percentile, 70th percentile, and 50th percentile values for normalized load metrics is given priority over a migration recommendation that only applies to a 90th percentile value for normalized load metrics.","The method associated with the flow diagrams of  may be carried out over an entire assessment period, which is in one example, 24 hours. Alternatively, the assessment period may be divided up into two or more periods of varying congestion levels of the storage system. In one embodiment, the assessment period is divided into a congested period and a non-congested period and load balancing is performed only during the congested period. Workload metrics and normalized load metrics may still be computed during the uncongested period because as mentioned above there may be instances where migration to a destination LUN may not be desired if the destination LUN exhibits large spikes in its normalized load metric not reflected in its representative normalized load metric. In one example, the state of the storage system is deemed to be congested at a particular point in time when one of the LUNs of the storage system at that point in time has an average latency that is greater than a threshold latency, which may be defined as latency corresponding to a certain percentage of the maximum throughput of the LUN.",{"@attributes":{"id":"p-0047","num":"0046"},"figref":["FIG. 5A","FIG. 5A"],"b":["520","522"]},"The set of N samples shown in  may be stored directly in an array comprising N elements, where each element stores one sample S(n). Alternatively, in certain applications the set of N samples may be stored in a more compact form as a histogram having less than N buckets.  illustrates a histogram  of samples and an associated graph of accumulated histogram area . One or more percentile values, such as the 90th percentile value , may be obtained using histogram  and the associated graph of accumulated histogram area  to characterize a given set of time samples, such as described above for normalized load metrics for a LUN. In the embodiments of the present invention where the assessment period is divided up into two or more periods of varying congestion levels of the storage system, a histogram of normalized workload metrics is maintained for each LUN and for each congestion level, and the associated graph of accumulated histogram area may be used to determine the N-th percentile value of the normalized workload metrics.",{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIG. 6","b":["600","610","600","600","612","620"]},"In step , the controlled workload is run on the two LUNs together and in step  the response times are monitored for requests associated with the workload. In step , the controlled workload is run on the two LUNs separately, and in step  the response times are monitored for requests associated with the workload. In step , the sharing status of the two LUNs is determined with respect to a physical storage array based on response times. If the response times of the two LUNs are highly correlated when subjected to the workload, then the two LUNs likely share a physical storage array. Importantly, if the two LUNs are sufficiently correlated, then they likely share a common set of physical storage devices, such as drive spindles. Correlation may be determined using any technically feasible technique. For example, if response times for the two LUNs exhibit concurrent and similar increases and decreases under a common workload, then correlation may be established. If the correlation is consistent in both time and changes in delay, then the correlation can be sufficient to conclude the two LUNs share a common physical storage media. The method terminates after step .",{"@attributes":{"id":"p-0051","num":"0050"},"figref":"FIG. 7","b":["700","710","700","700","712","716"]},"In step , the physical storage array is opened in raw mode and the controlled workload is run on a physical storage array. The controlled workload may be a random read across to the physical storage array or a file creation followed by writes to the created file. Then, in step , the IO latencies are observed for all of the workloads within host computer . In one embodiment, IO latencies are also observed for other host computers coupled to related storage arrays, such as storage arrays  and . The workloads whose IO latencies correlate with the controlled workload are determined to be running on the same physical storage array. Therefore, the LUNs associated with those workloads are determined to share the same physical storage array (step ). Step  is a check to see if the controlled workload has been run on all physical storage arrays. If there are more, method  returns to step  and steps - are carried out on a different storage array. If there are no more, method  ends.","In methods  and  described above, when two LUNs are determined to be mapped to the same underlying storage array, and a third LUN is determined to be mapped to the same underlying storage array as one of the two LUNs, then, it can be concluded based on the transitivity property that all three LUNs are determined to be sharing the same underlying storage array.","In the embodiments of the present invention described above, read and write IOs are considered together in the model. In alternative embodiments, read workloads and write workloads are modeled separately and migration decisions are made in accordance with one of the two models.","The various embodiments described herein may employ various computer-implemented operations involving data stored in computer systems. For example, these operations may require physical manipulation of physical quantities usually, though not necessarily, these quantities may take the form of electrical or magnetic signals where they, or representations of them, are capable of being stored, transferred, combined, compared, or otherwise manipulated. Further, such manipulations are often referred to in terms, such as producing, identifying, determining, or comparing. Any operations described herein that form part of one or more embodiments of the invention may be useful machine operations. In addition, one or more embodiments of the invention also relate to a device or an apparatus for performing these operations. The apparatus may be specially constructed for specific required purposes, or it may be a general purpose computer selectively activated or configured by a computer program stored in the computer. In particular, various general purpose machines may be used with computer programs written in accordance with the teachings herein, or it may be more convenient to construct a more specialized apparatus to perform the required operations.","The various embodiments described herein may be practiced with other computer system configurations including hand-held devices, microprocessor systems, microprocessor-based or programmable consumer electronics, minicomputers, mainframe computers, and the like.","One or more embodiments of the present invention may be implemented as one or more computer programs or as one or more computer program modules embodied in one or more computer readable media. The term computer readable medium refers to any data storage device that can store data which can thereafter be input to a computer system computer readable media may be based on any existing or subsequently developed technology for embodying computer programs in a manner that enables them to be read by a computer. Examples of a computer readable medium include a hard drive, network attached storage (NAS), read-only memory, random-access memory (e.g., a flash memory device), a CD (Compact Discs), CD-ROM, a CD-R, or a CD-RW, a DVD (Digital Versatile Disc), a magnetic tape, and other optical and non-optical data storage devices. The computer readable medium can also be distributed over a network coupled computer system so that the computer readable code is stored and executed in a distributed fashion.","Although one or more embodiments of the present invention have been described in some detail for clarity of understanding, it will be apparent that certain changes modifications may be made within the scope of the claims. Accordingly, the described embodiments are to be considered as illustrative and not restrictive, and the scope of the claims is not to be limited to details given herein, but may be modified within the scope and equivalents of the claims. In the claims, elements and\/or steps do not imply any particular order of operation, unless explicitly stated in the claims.","In addition, while described virtualization methods have generally assumed that virtual machines present interfaces consistent with a particular hardware system, persons of ordinary skill in the art will recognize that the methods described may be used in conjunction with virtualizations that do not correspond directly to any particular hardware system. Virtualization systems in accordance with the various embodiments, implemented as hosted embodiments, non-hosted embodiments, or as embodiments that tend to blur distinctions between the two, are all envisioned. Furthermore, various virtualization operations may be wholly or partially implemented in hardware. For example, a hardware implementation may employ a look-up table for modification of storage access requests to secure non-disk data.","Many variations, modifications, additions, and improvements are possible, regardless the degree of virtualization. The virtualization software can therefore include components of a host, console, or guest operating system that performs virtualization functions. Plural instances may be provided for components, operations or structures described herein as a single instance. Finally, boundaries between various components, operations and data stores are somewhat arbitrary, and particular operations are illustrated in the context of specific illustrative configurations. Other allocations of functionality are envisioned and may fall within the scope of the invention(s). In general, structures and functionality presented as separate components in exemplary configurations may be implemented as a combined structure or component. Similarly, structures and functionality presented as a single component may be implemented as separate components. These and other variations, modifications, additions, and improvements may fall within the scope of the appended claims(s)."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":["FIG. 2","FIG. 1"]},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 5A"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 5B"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 7"}]},"DETDESC":[{},{}]}
