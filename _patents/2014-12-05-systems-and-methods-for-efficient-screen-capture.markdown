---
title: Systems and methods for efficient screen capture
abstract: Systems and methods are provided for efficient screen capture and video recording on mobile and other devices. The video is recorded using a graphics rendering pipeline that includes a primary frame buffer, a secondary frame buffer, and a video writer module that encodes and writes data to a video file. The frame buffers include multiple textures to which graphics data can be rendered or copied, with at least the secondary frame buffer textures backed with memory that is quickly accessible by a central processing unit. In operation, a frame is rendered into a texture in the primary frame buffer, and the contents of the rendered frame are copied to a texture of the secondary frame buffer as well as to a default graphics rendering pipeline for output to a display. The contents of the rendered frame are then provided from the secondary frame buffer to the video writer for output to a video file.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09161006&OS=09161006&RS=09161006
owner: Kamcord, Inc.
number: 09161006
owner_city: San Francisco
owner_country: US
publication_date: 20141205
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","BRIEF SUMMARY","DETAILED DESCRIPTION"],"p":["The present disclosure relates generally to digital video recording and, more particularly, to systems and methods for efficient screen capture of a mobile application for the creation of digital video recordings via an integrated software library.","Mobile devices, such as smartphones, have specialized hardware and software that implicate unique considerations when providing for video processing functionality, as compared to desktop and laptop computers running, for example, the MICROSOFT WINDOWS operating system or the APPLE OS X operating system. There currently are, however, no third-party video recording solutions for mobile games and applications that support the intricacies of these mobile integrated graphical solutions while providing for minimal performance overhead.","Systems and methods are described for efficient screen capture using a third-party library integrated into a mobile application. As one example, a mobile game developer can obtain the library and integrate it into a game by, for example, utilizing an application programming interface provided by the library. In execution, the functionality provided by the library allows game sessions to be manually or automatically recorded on behalf of the user. Upon completion of a recording, a user interface can allow the player to review and edit the created video, and upload the video to the internet for sharing via social media.","In one aspect, a method for efficient screen capture includes the steps of: providing a video recording pipeline comprising: a primary frame buffer comprising a first plurality of textures; a secondary frame buffer comprising a second plurality of textures each comprising memory accessible by a central processing unit; and a video writer module for writing data to a video file; receiving into a texture of the primary frame buffer a rendering of a frame; copying contents of the rendered frame to a texture of the secondary frame buffer and to a default graphics rendering pipeline for output to a display; and providing the contents of the rendered frame from the secondary frame buffer to the video writer for output to a video file.","The primary frame buffer and\/or the secondary frame buffer can be configured as a circular buffer of textures, and the first and\/or second pluralities of textures can include OPENGL textures.","The rendered frame can be received from an application on a mobile device, and the video recording pipeline can be provided as a library that interfaces with the mobile device application.","In one implementation, providing the rendered frame to the default graphics rendering pipeline includes copying contents of the rendered frame to a default frame buffer in the default graphics rendering pipeline.","In another implementation, the method includes switching to a next one of the textures in the primary frame buffer such that a next received frame is rendered into the next texture of the primary frame buffer. The method can further include switching to a next one of the textures in the secondary frame buffer such that contents of the next received rendered frame are copied into the next texture of the secondary frame buffer.","In a further implementation, the video writer performs a color conversion on the contents of the rendered frame.","The details of one or more implementations of the subject matter described in the present specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.","Described herein are various implementations of methods and supporting systems for providing high-performance screen capture for recording videos in mobile applications. The techniques described herein can be implemented in any appropriate hardware or software. If implemented as software, the processes can execute on a system capable of running one or more commercial operating systems such as the MICROSOFT WINDOWS operating systems, the APPLE OS X operating systems, the APPLE IOS platform, the GOOGLE ANDROID platform, the LINUX operating system and other variants of UNIX operating systems, and the like. The software can be implemented on a general purpose computing device in the form of a computer including a processing unit, a system memory, and a system bus that couples various system components including the system memory to the processing unit.","Processors suitable for the execution of software include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memory devices for storing instructions and data. Information carriers suitable for embodying computer program instructions and data include all forms of non-volatile memory, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. One or more memories can store media assets (e.g., audio, video, graphics, interface elements, and\/or other media files), configuration files, and\/or instructions that, when executed by a processor, form the modules, engines, and other components described herein and perform the functionality associated with the components. The processor and the memory can be supplemented by, or incorporated in special purpose logic circuitry.","Referring to , a video of a game or other user activity or experience can be recorded on user device . User device  can be, for example, a smartphone, such as an APPLE IPHONE device or an ANDROID-based device, tablet, laptop, palmtop, television, gaming device, music player, information appliance, workstation, personal digital assistant, wireless device, or other computing device, that is operated as a general purpose computer or a special purpose hardware device that can execute the functionality described herein. User device  can include a display, such as an integrated or separate display screen, touchscreen, monitor, or other visual output feature. User device  can also include an audio output feature, such as speakers. A video recorded on the user device  can capture video and\/or audio output.","User device  includes application , video recording library , and graphics API , which can be implemented, for example, using a plurality of software modules stored in a memory and executed on one or more processors. The modules can be in the form of a suitable programming language or framework, which is converted to machine language or object code or interpreted to allow the processor or processors to read the instructions. Application  can be a game, web browser, video player, native application, web application or other application that can interface with video recording library  for the recording and creation of a video file. For example, video recording library  can be a software library integrated into application  and having an application programming interface (API) accessed by application  when rendering graphical data for display on an output screen of the user device . Video recording library  sits between application  and a native graphics API  used by the user device  to render graphics. In one instance, the graphics API  is an implementation of the OPENGL API, such as the OPENGL API for embedded systems (OPENGL ES), and video recording library  replaces, intercepts, or works in conjunction with OPENGL function calls to provide for the video recording functionality described herein. In other instances, the graphics API  includes APIs other than the OPENGL API, such as the IOS METAL interface.","User device  can communicate with a server  through communications network . Server  can be implemented in any appropriate hardware or software and can provide, for example, an interface through which user device  can upload videos created using the functionality described herein. In some instances, server  includes a web server and social medium that allows users to share videos with each other. In one implementation, server  includes an audio\/video processing server that performs video encoding, compression, color correction, audio processing, and\/or other functions on audio\/video data provided by user device . In one example, user device  encodes a video and uploads it to server , which can re-encode the video in various bitrates prior to making it available to other users.","Communication over network  can occur via any media such as standard telephone lines, LAN or WAN links (e.g., T1, T3, 56 kb, X.25), broadband connections (ISDN, Frame Relay, ATM), wireless links (802.11, Bluetooth, GSM, CDMA, etc.), and so on. The network  can carry TCP\/IP protocol communications and HTTP\/HTTPS requests made by a web browser, and the connection between user device  and server  can be communicated over such TCP\/IP networks. The type of network is not a limitation, however, and any suitable network and protocol(s) can be used.","Method steps of the techniques described herein can be performed by one or more programmable processors executing a computer program to perform functions by operating on input data and generating output. Method steps can also be performed by, and apparatus can be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). Modules can refer to portions of the computer program and\/or the processor\/special circuitry that implements that functionality.","It should also be noted that the present implementations can be provided as one or more computer-readable programs embodied on or in one or more articles of manufacture. The article of manufacture can be any suitable hardware apparatus, such as, for example, a floppy disk, a hard disk, a CD-ROM, a CD-RW, a CD-R, a DVD-ROM, a DVD-RW, a DVD-R, a flash memory card, a PROM, a RAM, a ROM, or a magnetic tape. In general, the computer-readable programs can be implemented in any programming language. The software programs can be further translated into machine language or virtual machine instructions and stored in a program file in that form. The program file can then be stored on or in one or more of the articles of manufacture.","In one implementation, the video recording library  enables recording of gameplay or other application activity on the user device  by redirecting the application engine to draw, or render, graphical data to a frame buffer containing a special texture instead of the default frame buffer otherwise used by the device . As used herein, a \u201ctexture\u201d can refer to an object which contains an image, such as an OPENGL texture object that can store one or more images of various types. A \u201cspecial texture\u201d can refer to a texture allocated using memory accessible by a central processing unit (CPU), such that the CPU can quickly read and access the bytes of a frame rendered to the special texture. The enhanced access speed allows for efficient reading of the texture data and writing of the texture data to a video file. In one instance, a texture is a special texture if its bytes are directly accessible by a CPU using a pointer to memory that was made available when the texture was instantiated.","In one instance, for an OPENGL-based application, instead of using native OPENGL interfaces to access the data of the frames being drawn by the application, the library  directs the application to render to a texture backed by non-OPENGL interfaces (e.g., Core Video can be used to allocate the texture on an IOS-based device, and EGL can be used to allocate the texture on an ANDROID-based device). Doing so allows CPU code to efficiently access the rendered graphics frames such that writing those frames to a video file is performed relatively quickly. The advantage of this approach is that gameplay or other application performance while recording is barely impacted, whereas prior approaches of using OPENGL interfaces to read frame data significantly and noticeably impact performance, making such solutions unviable for practical purposes.","The creation of a special texture that can be quickly accessed by the CPU and a video data writing component generally differs for each platform. In one implementation, on the IOS platform with the OPENGL API, a special texture can be created using Core Video framework as follows:\n\n","In another implementation, on the IOS platform with the METAL interface, a special texture can be created using CoreVideo.framework and Metal.framework:\n\n","In another implementation, on the ANDROID platform, a special texture can be created using EGL extensions:\n\n",{"@attributes":{"id":"p-0031","num":"0042"},"figref":"FIG. 2","b":["200","105","110","105","210","280","210"]},{"@attributes":{"id":"p-0032","num":"0043"},"figref":["FIG. 3","FIG. 2"],"b":["110","105","300","300","200","300","200","310","320","330","340","200","210","105","310","310","310"]},"The contents of the primary frame buffer  are then copied to a secondary frame buffer , which is backed by a special texture that allows the CPU to quickly and efficiently read the bytes that represent a rendered frame. The use of the special texture helps to avoid unacceptable performance degradation that would otherwise accompany video recording during gameplay or other application use. Further, the use of the secondary frame buffer  with the primary frame buffer  also acts as a double-buffering scheme to prevent tearing in recorded video. That is, if the contents of the primary frame buffer  were fed directly to video writer , it is possible for the application  to begin drawing the next frame while the video writer  is attempting to process and record the previous frame, thereby causing an undesirable tearing effect.","After being copied into secondary frame buffer , the special texture data (bytes) are sent to video writer  (using, for example, a background thread or process), which asynchronously encodes and writes the frame to video file . Further, while the rendered data is proceeding along the video recording pipeline, the frame contents can be copied from the primary frame buffer  to the default frame buffer  for output to screen  (i.e., along the standard pipeline ).","In another implementation, shown in , a graphics rendering pipeline  omits a secondary frame buffer. Instead, the primary frame buffer  is backed with a special texture that allows for fast access and reading of the allocated texture bytes by a CPU. Sending the contents of the primary frame buffer  directly to video writer , rather than copying the data to a secondary frame buffer (as shown in ), provides for better performance (i.e., less delay) when recording video compared to rendering pipeline . However, as noted above, the use of a single frame buffer can result in tearing.","Referring now to , a graphics rendering pipeline  provides the performance advantages of the pipeline  shown in , while preventing possible tearing at the cost of additional memory use. Pipeline  includes two frame buffers  and , each backed by special textures that allow for fast CPU access such that video writer  can efficiently write frames to video file . Here, application  alternates between frame buffer  and frame buffer  when drawing frames. For example, application  can draw all even-numbered frames to frame buffer , and can draw all odd-numbered frames to frame buffer . Once each frame is drawn, the pipeline  continues in a similar fashion to pipelines described above. That is, the contents of each frame can be copied to default frame buffer  for display on screen , and the contents of each frame can be sent to video writer  for output to video file .",{"@attributes":{"id":"p-0037","num":"0048"},"figref":"FIG. 6","b":["600","610","610","280","610","610","620","620","620","330","340"]},{"@attributes":{"id":"p-0038","num":"0049"},"figref":["FIG. 7","FIG. 3"],"b":["700","700","300","710","720","710","720","710","720","715","725","715","725","710","720","715","710","725","720"]},"Referring to both , in one example, pipeline  operates as follows, with the individual textures  in the primary frame buffer  being referred to as P0, P1, and P2, and the individual textures  in the secondary frame buffer  being referred to as S0 and S1. Assume that P0 and S0 are currently bound to their respective frame buffers  and . In STEP , application  renders a first frame into texture P0 of primary frame buffer . The contents of the first frame are copied from the default frame buffer  for display on an output screen  (STEP ). When the first frame is finished rendering and after it has been copied to the default frame buffer , P0 is detached from the primary frame buffer  and P1 is attached in its place (STEP ). P1 is then ready to receive the next rendering of a frame. The contents of the first frame are copied from P0 to texture S0 in secondary frame buffer  (STEP ). In STEP , S0 is detached from the secondary frame buffer  and S1 is attached in its place, allowing for S1 to receive the next rendered frame.","In some implementations, the contents of the frame in S0 are processed by performing a color conversion from RGB to YUV space. In STEP , the converted bytes are provided to a video encoder and writer. The process continues for each subsequent frame, and the textures ,  in both the primary frame buffer  and secondary frame buffer  are rotated forward one at a time in the manner described above. For example, for the next frame, P1 is attached to the primary frame buffer  and S1 is attached to the secondary frame buffer . The contents of the frame in P1 are then copied into S1, and both frame buffers ,  can be rotated to texture P2 and S0, respectively. Of note, the frame buffers ,  can operate independently of each other and rotate their respective textures ,  after the contents of a frame have been fully rendered to or copied to a particular buffer , . In some instances, if there are delays in the rendering pipeline the primary buffer  and\/or secondary buffer  can drop frames (i.e., not pass frames along the pipeline) in order to accommodate arriving frames.","Although the primary and secondary frame buffers ,  are shown as containing three and two textures, respectively, other numbers of textures are possible depending on the implementation. For example, it may be necessary to balance memory usage (more textures uses more memory) with the desire to avoid dropped frames (more textures allows for greater ability to queue frames). A sufficient number of textures allows the application  to continue rendering frames independently of the rest of the video processing pipeline (i.e., the secondary frame buffer  and onwards). For example, a sufficient texture queue allows the application  to render to P1 even if P0 has yet to be copied to the secondary frame buffer , or even to P2 if the rendering pipeline falls further behind. Using three textures in the primary frame buffer  and two textures in the secondary frame buffer  has shown to be sufficiently robust to record video without dropping too many frames and, further, has shown to prevent both flickering on the screen  and tearing in the final recorded video . In other implementations, a similar effect can be achieved by including additional frame buffers in the pipeline . Further, it should be noted that the above steps need not be performed in precisely the same order, depending on the implementation; for example, the rotation of buffer textures and copying of frame data can occur at differing times in the process and at different points with respect to each other.",{"@attributes":{"id":"p-0042","num":"0053"},"figref":"FIG. 9","b":["330","330","902","906","910","902","340","330","906","910","340"]},"In one example, referring again to the graphics rendering pipeline  of , the contents of rendered frames in the textures  of the secondary frame buffer  can be color converted from RGB to YUV by color converter . Each color converted frame can then be provided as input into video encoder . For instance, a frame can be input to the encoder  after it has finished processing the previous frame. After a frame has been encoded, it is sent to the video muxer , where it is combined with an audio track (if any) and written to video file .","As noted above, the functionality described herein can be provided as a video recording library  or an integrated third-party software development kit (SDK). This approach provides game and other application developers with the ability to simply download the SDK and easily integrate it into their applications, saving the developers from having to build the entire gameplay recording software themselves. Furthermore, game players and other application users do not have to purchase third party recording tools, nor do they have to actively think about when they want to record. Recording sessions can be automatically programmed by the application developer, and the recording, watching, and sharing process can be naturally built into the application's flow and user experience.","In one implementation, on the IOS platform, the library  is compiled and built into the application  (e.g., a game). When the game is launched from a mobile phone, the library  detects the game engine among a variety of known game engines and automatically integrates itself into the graphics rendering pipeline of the game engine. The integration can occur on application load using a method known as \u201cswizzling,\u201d where existing methods are replaced with methods provided by the library  that add certain functionality as described herein. This change allows for gameplay recording of the video and audio of the game.","In the case, however, that the application developer uses a game engine that is not automatically detected by the library, the developer can use API hooks to integrate the gameplay recording technology provided by the library  into the game engine. One example of this API for an OPENGL-based game engine in Objective-C is as follows:\n\n","By inserting the above function calls in the appropriate places in an IOS-based game engine's rendering pipeline, gameplay recording functionality can be provided in a way that resembles the graphics rendering pipeline  shown in .","An example API for the IOS METAL interface integration is as follows:\n\n","Referring to the METAL-based functions above, the configureLayer method allows the layer and device to be saved for later use, and also changes the layer's mode so that created textures can be accessed and sampled. The setCurrentDrawawble method updates the current METAL drawable such that the drawable and texture to read from is made known. In some implementations, this method is unnecessary and can be removed by a proper application of swizzling on the CAMetalLayer nextDrawable method. The addMetalCommands method determines if the current frame is being recorded and, if so, copies the screen's texture to a special texture that can be accessed by the CPU and provided to the video writer .","In another implementation, on the ANDROID platform with the OPENGL interface, the library  is compiled and built into the game. When the game is launched, the library  automatically integrates into the game engine's rendering lifecycle using, for example, a method known as \u201cfunction hooking,\u201d which allows the library  to intercept and replace function calls with its own function calls. Specific OPENGL and EGL methods can be replaced to change the game engine's rendering pipeline in a way that resembles the graphics rendering pipeline  shown in .","This technique can be used to modify the behavior of OPENGL functions so that graphics drawn by the application  are automatically directed to a frame buffer which can be captured, as described herein. In one example, in OPENGL ES 2.0 on the ANDROID platform, in native C code compiled for ARM, the function glBindFramebuffer( ) can be intercepted as follows:\n\n","In this example, the word 0xe51ff004 is an ARM machine-code instruction which redirects execution to the address in the following word. The above technique can be used whenever the instruction set of the patching code and the instruction set of the function being modified are both full 32-bit ARM instructions. With respect to glBindFramebuffer( ) specifically, for instance, the function it is replaced with (i.e., patched_glBindFramebuffer( )) is written to redirect the OPENGL function to bind the frame buffer in the added video recording pipeline, as opposed to the default frame buffer of the application  (most of the time). This way, the scene being drawn can be appropriately captured.","In other example, to patch a THUMB function (e.g., eglSwapBuffers( ), an additional instruction can be used as follows:\n\n","In this example, the word 0x4b434778 is two THUMB instructions which switch the processor to ARM mode just in time to use the ARM patching mechanism in the following word. Patching eglSwapBuffers( ) can be used to determine when the application  has finished drawing a frame. In this manner, per-frame work is made possible (such as performing a texture copy, binding certain frame buffers, etc.) without the application developer having to make an explicit per-frame call.","The performance gains provided by the techniques described herein (in particular, the use of special textures) are significant and make gameplay video recording a possibility for both the IOS platform and ANDROID platform. Table 1 and Table 2 show the performance impacts of these recording integrations with OPENGL-based games on various platforms.",{"@attributes":{"id":"p-0056","num":"0145"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 1"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"IOS OPENGL performance benchmarks per frame. Lower is better."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"35pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},{},"CPU","GPU","Video","Video"]},{"entry":[{},{},{},"ms\/frame","ms\/frame","recording","recording"]},{"entry":[{},"CPU","GPU","(with","(with ","CPU ms\/","GPU ms\/"]},{"entry":[{},"ms\/","ms\/","video","video","frame","frame"]},{"entry":["Device","frame","frame","recording)","recording)","overhead","overhead"]},{"entry":{"@attributes":{"namest":"1","nameend":"7","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"5","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"7","colwidth":"35pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["IPAD 2","35.3","28.8","39","29.1","3.7","0.3"]},{"entry":["device",{},{},{},{},{},{}]},{"entry":["IPAD ","27.7","10.8","30","10.8","2.3","0"]},{"entry":["MINI",{},{},{},{},{},{}]},{"entry":["device",{},{},{},{},{},{}]},{"entry":["IPAD 3","27.3","18.8","30.6","19.3","3.3","0.5"]},{"entry":["device",{},{},{},{},{},{}]},{"entry":["IPAD 4","14.5","13.9","15.7","14.5","1.2","0.6"]},{"entry":["device",{},{},{},{},{},{}]},{"entry":["IPAD ","19","17","21","17","2","0"]},{"entry":["MINI",{},{},{},{},{},{}]},{"entry":["RETINA",{},{},{},{},{},{}]},{"entry":["device",{},{},{},{},{},{}]},{"entry":["IPHONE ","16","5.9","16.7","5.9","0.7","0"]},{"entry":["5 device",{},{},{},{},{},{}]},{"entry":["IPOD","36.8","11.9","39.4","11.9","2.6","0"]},{"entry":["TOUCH ",{},{},{},{},{},{}]},{"entry":"5G device"},{"entry":{"@attributes":{"namest":"1","nameend":"7","align":"center","rowsep":"1"}}}]}}]}}},{"@attributes":{"id":"p-0057","num":"0146"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 2"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"ANDROID OPENGL performance benchmarks per frame."},{"entry":"Lower is better."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"1","colwidth":"105pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"49pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},"ms\/frame ","Video recording"]},{"entry":[{},"ms\/","(with video ","ms\/frame"]},{"entry":["Device (ANDROID OS Version)","frame","recording)","overhead"]},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"1","colwidth":"105pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"49pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["GALAXY S3 device-Exynos ","18.3","24.1","5.8"]},{"entry":["Board (4.3)",{},{},{}]},{"entry":["GALAXY S4 device-Qualcomm","24.3","32.1","7.8"]},{"entry":["Board (4.3)",{},{},{}]},{"entry":["GALAXY S5 device-Qualcomm","17.3","21.4","4.1"]},{"entry":["Board (4.4)",{},{},{}]},{"entry":["NEXUS 4 device (4.2)","21.7","25.1","3.4"]},{"entry":["NEXUS 7 device (4.1)","24.8","33.6","8.8"]},{"entry":["NEXUS 7 HD device (4.4)","24","30.1","6.1"]},{"entry":["XPERIA Z1 SO-01F device (4.2)","17.3","19.4","2.1"]},{"entry":["NOTE 2 device (4.3)","17.3","22.7","5.4"]},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}}]}}]}}},"The columns that are most relevant in the tables above are the \u201coverhead\u201d columns, which indicate the performance cost of using the present techniques. Notably, the overhead is very small. Specifically, for the IOS platform, most of the overhead values are near zero and, for the ANDROID platform, most of the numbers are low single digits. This is a massive improvement over the standard frame copying approach of using the glReadPixels function, which for the IOS platform can take 10-20 ms\/frame and for the ANDROID platform can take over 200 ms\/frame. This performance improvement is a quantitative and qualitative difference which enables video recording to be practically used in games as, otherwise, the glReadPixels approach would render the game unplayable.","The terms and expressions employed herein are used as terms and expressions of description and not of limitation, and there is no intention, in the use of such terms and expressions, of excluding any equivalents of the features shown and described or portions thereof. In addition, having described certain implementations in the present disclosure, it will be apparent to those of ordinary skill in the art that other implementations incorporating the concepts disclosed herein can be used without departing from the spirit and scope of the invention. The features and functions of the various implementations can be arranged in various combinations and permutations, and all are considered to be within the scope of the disclosed invention. Accordingly, the described implementations are to be considered in all respects as illustrative and not restrictive. The configurations, materials, and dimensions described herein are also intended as illustrative and in no way limiting. Similarly, although physical explanations have been provided for explanatory purposes, there is no intent to be bound by any particular theory or mechanism, or to limit the claims in accordance therewith."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["In the drawings, like reference characters generally refer to the same parts throughout the different views. Also, the drawings are not necessarily to scale, emphasis instead generally being placed upon illustrating the principles of the implementations. In the following description, various implementations are described with reference to the following drawings, in which:",{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIGS. 3-7"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 9"}]},"DETDESC":[{},{}]}
