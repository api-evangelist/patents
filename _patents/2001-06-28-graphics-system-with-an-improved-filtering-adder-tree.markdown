---
title: Graphics system with an improved filtering adder tree
abstract: A sample-to-pixel calculation unit in a graphics system may comprise an adder tree. The adder tree includes a plurality of adder cells coupled in a tree configuration. Input values are presented to a first layer of adder cells. Each input value may have two associated control signals: a data valid signal and a winner-take-all signal. The final output of the adder tree equals (a) a sum of those input values whose data valid signals are asserted provided that none of the winner-take-all signals are asserted, or (b) a selected one of the input values if one of the winner-take-all bits is asserted. The selected input value is the one whose winner-take-all bit is set. The adder tree may be used to perform sums of weighted sample attributes and/or sums of coefficients values as part of pixel value computations.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=06989843&OS=06989843&RS=06989843
owner: Sun Microsystems, Inc.
number: 06989843
owner_city: Santa Clara
owner_country: US
publication_date: 20010628
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["I. CONTINUATION DATA","II. BACKGROUND OF THE INVENTION","III. SUMMARY OF THE INVENTION","V. DETAILED DESCRIPTION OF SEVERAL EMBODIMENTS"],"p":["This application claims benefit of priority to U.S. Provisional Application Ser. No. 60\/215,030 filed on Jun. 29, 2000 titled \u201cGraphics System with an Improved Filtering Adder Tree\u201d.","1. Field of the Invention","This invention relates generally to the field of computer graphics and, more particularly, to high performance graphics systems.","2. Description of the Related Art","A host computer may rely on a graphics system for producing visual output on a display device. The graphics system may receive graphics data (e.g. triangle data) from the host computer, and may generate a stream of output pixels in response to the graphics data. The stream of output pixels may be stored in a frame buffer, and then dumped from the frame buffer to a display device such as a monitor or projection screen.","To obtain images that are more realistic, some prior art graphics systems have gone further by generating more than one sample per pixel. As used herein, the term \u201csample\u201d refers to calculated color information that indicates the color, depth (z), and potentially other information, of a particular point on an object or image. For example, a sample may comprise the following component values: a red value, a green value, a blue value, a z value, and an alpha value (e.g., representing the transparency of the sample). A sample may also comprise other information, e.g, a blur value, an intensity value, brighter-than-bright information, and an indicator that the sample consists partially or completely of control information rather than color information (i.e., \u201csample control information\u201d). By calculating more samples than pixels (i.e., super-sampling), a more detailed image is calculated than can be displayed on the display device. For example, a graphics system may calculate four samples for each pixel to be output to the display device. After the samples are calculated, they are then combined or filtered to form the pixels that are stored in the frame buffer and then conveyed to the display device. Using pixels formed in this manner may create a more realistic final image.","These prior art super-sampling systems typically generate a number of samples that are far greater than the number of pixel locations on the display. These prior art systems typically have rendering processors that calculate the samples and store them into a render buffer. Filtering hardware then reads the samples from the render buffer, filters the samples to create pixels, and then stores the pixels in a traditional frame buffer. The traditional frame buffer is typically double-buffered, with one side being used for refreshing the display device while the other side is updated by the filtering hardware. Once the samples have been filtered, the resulting pixels are stored in a traditional frame buffer that is used to refresh to display device. These systems, however, have generally suffered from limitations imposed by the conventional frame buffer and by the added latency caused by the render buffer and filtering. Therefore, an improved graphics system is desired which includes the benefits of pixel super-sampling while avoiding the drawbacks of the conventional frame buffer.","However, one potential obstacle to an improved graphics system is that the filtering operation may be computationally intensive. A high-resolution graphics card and display may need to support millions of pixels per frame, and each pixel may be generated by filtration of a number of samples. This typically translates into a large number of calculations. In particular, each pixel component such as red, green and blue may be generated by constructing a weighted sum of the corresponding sample components. However, it is important to guarantee that the filter weights used to generate the weighted sums do not introduce color gain or attenuation. In other words, if the filter weights are not appropriately chosen, a group of samples all having identical red intensity Xmay have a weighted sum equal to kXwhere k is not equal to one. This implies that the resulting red pixel value will be more or less intense than desired. Thus, there is a substantial need for a system and method which could provide for unity gain in the filtering process (i.e. in the process of generating pixel values from sample values) in a manner which is flexible and efficient.","Furthermore, because each pixel comprises a number of components such as red, green, and blue, the filtering process may require multiple summations to be performed per pixel. Thus, there exists a need for a system and method which may efficiently and flexibly perform summations.","The present invention comprises a computer graphics system configured to receive 3D graphics data, generate samples in response to the 3D graphics data, filter the samples to generate output pixels, and provide the output pixels to a display device such as monitor or projector. In some embodiments, the graphics system comprises a sample buffer, a graphics processor configured to render (or draw) the samples into the sample buffer, and a sample-to-pixel calculation unit. The sample-to-pixel calculation unit may be responsible for filtering samples to generate pixel values.","The graphics processor may perform various rendering operations on the received 3D graphics data (e.g. triangle data) to generate samples based on a selected set of sample positions in a 2D screen space. Each sample may comprise a set of values such as red, green and blue. The samples are stored into the sample buffer for subsequent access by the sample-to-pixel calculation unit. The graphics processor preferably generates and stores more than one sample per unit pixel area in the 2D screen space for at least a subset of the output pixels. Thus, the sample buffer may be referred to as a super-sampled (or \u201cover-sampled\u201d) sample buffer. In other embodiments, the graphics processor may generate one sample per unit pixel area, or, less than one sample per unit pixel area (e.g. one sample for every two pixels). In one embodiment, the samples may be more densely positioned in certain areas of the screen space and less densely positioned in other areas.","The sample-to-pixel calculation unit is configured to read the samples from the sample buffer and filter (or convolve) the samples into respective output pixels. The output pixels are then provided to refresh the display. As used herein, the terms \u201cfilter\u201d and \u201cconvolve\u201d are used interchangeably, and refer to the process of generating a pixel value by computing a weighted average of a corresponding set of sample values. The sample-to-pixel calculation unit filters samples based on a filter function which may be centered over a current pixel location in the screen space. The filter function has an associated domain of definition referred to herein as the filter support or filter extent. The sample-to-pixel calculation unit:\n\n","The sample-to-pixel calculation unit may access samples from the sample buffer, perform a real-time filtering operation, and then provide the resulting output pixels to the display in real-time. The graphics system may operate without a conventional frame buffer. In other words, there may be no frame-oriented buffering of pixel data between the sample-to-pixel calculation units and the display. Note some displays may have internal frame buffers, but these are considered an integral part of the display device, not the graphics system. As used herein, the term \u201creal-time\u201d refers to an operation that is performed at or near the display device's refresh rate. For example, filtering samples \u201con-the-fly\u201d means calculating output pixels at a rate high enough to support the refresh rate of a display device. The term \u201con-the-fly\u201d refers to a process or operation that generates images at a rate near or above the minimum rate required for displayed motion to appear smooth (i.e. motion fusion) and\/or for the light intensity to appear continuous (i.e. flicker fusion). These concepts are further described in the book \u201cSpatial Vision\u201d by Russel L. De Valois and Karen K. De Valois, Oxford University Press, 1988.","The filter weight assigned to each sample depends on the filter function being used and on the distance of the sample from the pixel center or the filter center. It is noted that the terms filter weight and filter coefficient are used interchangeably herein. For each pixel attribute (e.g. the red attribute), the pixel value (e.g. the red pixel value) is given by a weighted sum of the corresponding samples values (e.g. the red sample values) for samples falling within the filter support. If the filter weights are not pre-normalized to one, i.e. the sum of the coefficients used in each weighted sum does not equal one, then the weighted sums for the various pixel attributes may be divided by the sum of the filter weights. This sum of the filter weights is referred to herein as the normalization factor.","In cases where the filter function, the filter support, and the set of relative positions of samples with respect to the filter center remain constant from pixel to pixel (and thus, the filter coefficients remain constant), the normalization factor remains the same. In those cases, the normalization factor may be calculated once before the filtering process begins. The coefficients may be pre-normalized by dividing the original coefficients by the normalization factor to generate a set of normalized coefficients. Then, the normalized coefficients may be used in the filtering process for an array of pixels.","However, in many cases, the normalization factor may vary from pixel to pixel. For example, the filtering may take place over a region of non-uniform sample density or at the edges of the screen space. The size and\/or shape of the filter support may change from one pixel to the next. The samples may be distributed in the screen space in a random fashion. Thus, the number of samples interior to the filter support and\/or their relative positions with respect to the pixel center may vary from pixel to pixel. This implies that the normalization factor (i.e. the sum of the coefficients of the interior samples) varies from pixel to pixel.","In such cases, the normalization factor may be individually computed for each pixel, and instead of pre-normalizing the filter coefficients, the weighted sum (computed on the basis of the non-normalized coefficients) may be post-normalized. In other words, after generating a weighted sum for each attribute, each weighted sum may be divided by the normalization factor. In one embodiment, the computation of the normalization factor may be performed in parallel with the computation of one or more of the pixel attribute summations.","In one set of embodiments, one or more of the per-pixel summations (e.g. the coefficient summation and\/or any combination of the attribute summations) may be performed by an adder tree. The adder tree may comprise a plurality of addition levels, and each addition level may include a set of adder cells. An adder cell may receive two input operands and generate one output operand. In one alternative embodiment, an adder cell may receive three input operands and generate two output operands.","The first addition layer may receive a set of numeric values which are to be summed. Each adder cell in the first layer may generate a sum of two (or three) of the numeric values and pass its output operand(s) to the second layer. Each adder cell in layers after the first and before the last layer may receive two (or three) output operands from the previous layer and pass its output operand(s) to the next layer. Thus, the final output from the last layer may represent a sum of all the numeric operands presented to the first layer. Registers may be placed after each adder cell in order to buffer the intermediate summation results.","In some embodiments, the adder tree is configured to add any desired subset of the input numeric values. Thus, in addition to the numeric values, the adder tree is configured to receive a set of data valid signals, i.e. one data valid signal for each numeric value. The data valid signal indicates whether the corresponding numeric value is to be included in the summation output from the adder tree. An adder cell may be configured to support such an adder tree as follows. The adder cell may receive two input operands Xand X, and two corresponding data valid inputs DVand DV, and may generate a single output operand X. The adder cell output Xmay equal zero, X, Xor the sum X+Xdepending on the state of the data valid input signals. Namely, the output equals zero when both data valid inputs are low, equals Xwhen only data valid input DVis high, equals Xwhen only data valid input DVis high, and equals the sum when both data valid inputs are high. The adder cell may also generate a data valid output signal DVto indicate to an adder cell of the next layer whether the operand output Xis \u201cvalid\u201d, i.e. to be incorporated in a further summation or ignored. Another embodiment of the adder cell contemplates use of a carry-save adder with three operand inputs and two operand outputs. Various embodiments of circuits (such as the adder cell) are described in terms of active high logic. However, it is understood that these circuit embodiments may be realized in terms of active low logic or a combination of active high logic and active low logic.","In one set of embodiments, the sample-to-pixel calculation unit may be configured to turn off sample filtering, and to generate pixel values based on a \u201cwinner-take-all\u201d criterion. For example, the values of a current pixel may be determined by an identified sample or the first sample (in sequence order) in a memory bin corresponding to the current pixel. Alternatively, the values of the current pixel may be determined by the sample closest to the current filter center or pixel center as suggested by . The red, green, blue and alpha values of this closest sample are assigned as the values of the current pixel.","Previous generation video products have generated pixel values from 3D graphics primitive without intervening super-sampling and super-sample filtering. Thus, in order satisfy users\/customers who want their displayed video output to have the same appearance as a previous generation video product, the sample-to-pixel calculation unit may be programmed to disable sample-filtering and enable winner-take-all sample selection.","As described above, an adder tree may be configured to perform an addition of any desired subset of its input numeric values based on the data valid signal inputs to the adder tree. In some embodiments, the adder tree also performs winner-take-all selection of a selected one of the input numeric values. Thus, in addition to data valid signals, the adder tree may receive a set of winner-take-all signals, one winner-take-all signal per input numeric value. In the preferred embodiment, at most one of the winner-take-all signals may be high. When a winner-take-all signal is high, the adder tree passes the corresponding input numeric value to the adder tree output. When all the winner-take-all signals are low, the adder tree generates a summation of those input numeric values having high data valid signals as described above.","Such an adder tree may be facilitated by an adder cell configured as follows. The adder cell may receive two input operands Xand X, two corresponding data valid input signals DV, and DV, and two corresponding winner-take-all input signals WTAand WTA. The adder cell generates an output operand X. When both winner-take-all inputs signals are low, the output operand Xequals 0, X, Xor X+Xdepending on the state of data valid bits as before. When winner-take-all signal WTA, is high and winner-take-all signal WTAis low, the output operand equals X. When winner-take-all signal WTAis high and winner-take-all signal WTAis low, the output operand equals X. Furthermore, the adder cell may generate a data valid output signal DV and a winner-take-all output signal WTA. The data valid output signal DV indicates to an adder cell in the next layer whether or not the operand output Xis valid so far an inclusion in a further addition is concerned. The winner-take-all output signal WTAindicates to the next-layer adder cell whether the output operand Xrepresents the winner of the winner-take-all process. Each adder cell in a given layer (after the first layer) may receive the operand output X, the data valid output DV and the winner-take-all output WTAfrom two adder cells from the previous layers. Thus, when one of numeric values presented to the first layer has a winner-take-all bit set, that numeric value propagates through each layer to the adder cell output. In one alternative embodiment, an adder cell may be modified to operate with a carry-save adder, and thus, to receive three operands inputs and generate two operand outputs.","Typically, a different summation may be loaded into the adder tree every n clock cycles. This period of time, i.e. the n clock cycles may be referred to as an adder cycle. In certain cases, however, for one or more adder cycles, no valid data may be introduced into the adder tree. For example, in cases where the rate of outputting pixels is much less than the native rate of the graphics system.","In cases where the filter support covers regions with two or more different sample densities, the samples from the lower density regions may contribute less to the final pixel value than the samples from the higher density region. This is because there are typically fewer samples in the lower density region. In one embodiment, the filter coefficients corresponding to samples from the lower sample density regions may be multiplied by a factor approximately equal to the ratio of the sample densities. This may provide more weight to the less-represented samples from the lower density region. In cases where the filter support may include more than two regions of different sample densities, filter coefficients for samples in other regions may also be multiplied by a factor equal to the ratio of the sample densities.","In another embodiment, as the sample density decreases, the extent (e.g., diameter) of the filter may be increased in order to keep the number of samples included in the filtering approximately constant. For example, in an embodiment where the filter is circularly symmetric, the square of the support diameter of the filter may be set to a value that is inversely proportional to the sample density in that region.","While the invention is susceptible to various modifications and alternative forms, specific embodiments thereof are shown by way of example in the drawings and will herein be described in detail. It should be understood, however, that the drawings and detailed description thereto are not intended to limit the invention to the particular form disclosed, but on the contrary, the intention is to cover all modifications, equivalents and alternatives falling within the spirit and scope of the present invention as defined by the appended claims. Please note that the headings are for organizational purposes only and are not meant to limit the description or claims. The word \u201cmay\u201d is used in this application in a permissive sense (i.e., having the potential to, being able to), not a mandatory sense (i.e., must). Similarly, the word include, and derivations thereof, are used herein to mean \u201cincluding, but not limited to.\u201d","1. Computer System\u2014","Referring now to , one embodiment of a computer system  that includes a three-dimensional (3-D) graphics system is shown. The 3-D graphics system may be comprised in any of various systems, including a computer system, network PC, Internet appliance, a television, including HDTV systems and interactive television systems, personal digital assistants (PDAs), wearable computers, and other devices which display 2D and or 3D graphics, among others.","As shown, the computer system  comprises a system unit  and a video monitor or display device  coupled to the system unit . The display device  may be any of various types of display monitors or devices (e.g., a CRT, LCD, reflective liquid-crystal-on-silicon (LCOS), or gas-plasma display). Various input devices may be connected to the computer system, including a keyboard , a mouse  and\/or other input devices (e.g., a trackball, digitizer, tablet, six-degree of freedom input device, head tracker, eye tracker, data glove and\/or body sensors). Application software may be executed by the computer system  to display 3-D graphical objects on display device . As described further below, the 3-D graphics system in computer system  includes a super-sampled sample buffer with a programmable \u201con-the-fly\u201d and \u201cin-real-time\u201d sample-to-pixel calculation unit to improve the quality and realism of images displayed on display device .","2. Computer System Block Diagram\u2014","Referring now to , a simplified block diagram illustrating the computer system of  is shown. Elements of the computer system that are not necessary for an understanding of the present invention are not shown for convenience. As shown, the computer system  includes a central processing unit (CPU)  coupled to a high-speed memory bus or system bus  also referred to as the host bus . A system memory  may also be coupled to high-speed bus .","Host processor  may comprise one or more processors of varying types, e.g., microprocessors, multi-processors and CPUs. The system memory  may comprise any combination of different types of memory subsystems, including random access memories, (e.g., static random access memories or \u201cSRAMs\u201d, synchronous dynamic random access memories or \u201cSDRAMs\u201d, and Rambus dynamic access memories or \u201cRDRAM\u201d, among others) and mass storage devices. The system bus or host bus  may comprise one or more communication or host computer buses (for communication between host processors, CPUs, and memory subsystems) as well as specialized subsystem buses.","A 3-D graphics system  according to the present invention is coupled to the high-speed memory bus . The 3-D graphics system  may be coupled to the bus  by, for example, a crossbar switch or other bus connectivity logic. It is assumed that various other peripheral devices, or other buses, may be connected to the high-speed memory bus . It is noted that the 3-D graphics system may be coupled to one or more of the buses in computer system  and\/or may be coupled to various types of buses. In addition, the 3D graphics system may be coupled to a communication port and thereby directly receive graphics data from an external source, e.g., the Internet or a network. As shown in the figure, display device  is connected to the 3-D graphics system  comprised in the computer system .","Host CPU  may transfer information to and from the graphics system  according to a programmed input\/output (I\/O) protocol over host bus . Alternately, graphics system  may access the memory subsystem  according to a direct memory access (DMA) protocol or through intelligent bus mastering.","A graphics application program conforming to an application programming interface (API) such as OpenGL\u00ae or Java 3D\u2122 may execute on host CPU  and generate commands and data that define a geometric primitive (graphics data) such as a polygon for output on display device . As defined by the particular graphics interface used, these primitives may have separate color properties for the front and back surfaces. Host processor  may transfer these graphics data to memory subsystem . Thereafter, the host processor  may operate to transfer the graphics data to the graphics system  over the host bus . In another embodiment, the graphics system  may read in geometry data arrays over the host bus  using DMA access cycles. In yet another embodiment, the graphics system  may be coupled to the system memory  through a direct port, such as the Advanced Graphics Port (AGP) promulgated by Intel Corporation.","The graphics system may receive graphics data from any of various sources, including the host CPU  and\/or the system memory , other memory, or from an external source such as a network, e.g., the Internet, or from a broadcast medium, e.g., television, or from other sources.","As will be described below, graphics system  may be configured to allow more efficient microcode control, which results in increased performance for handling of incoming color values corresponding to the polygons generated by host processor . Note while graphics system  is depicted as part of computer system , graphics system  may also be configured as a stand-alone device (e.g., with its own built-in display). Graphics system  may also be configured as a single chip device or as part of a system-on-a-chip or a multi-chip module.","3. Graphics System\u2014","Referring now to , a block diagram illustrating details of one embodiment of graphics system  is shown. As shown in the figure, graphics system  may comprise one or more graphics processors , one or more super-sampled sample buffers , and one or more sample-to-pixel calculation units A\u2013D. Graphics system  may also comprise one or more digital-to-analog converters (DACs) A\u2013B. Graphics processor  may be any suitable type of high performance processor (e.g., specialized graphics processors or calculation units, multimedia processors, DSPs, or general purpose processors). In one embodiment, graphics processor  may comprise one or more rendering units A\u2013D. In the embodiment shown, however, graphics processor  also comprises one or more control units , and one or more schedule units . Sample buffer  may comprises one or more sample memories A\u2013N as shown in the figure.","A. Control Unit","Control unit  operates as the interface between graphics system  and computer system  by controlling the transfer of data between graphics system  and computer system . In embodiments of graphics system  that comprise two or more rendering units A\u2013D, control unit  may also divide the stream of data received from computer system  into a corresponding number of parallel streams that are routed to the individual rendering units A\u2013D. The graphics data may be received from computer system  in a compressed form. In one embodiment, control unit  may be configured to split and route the data stream to rendering units A\u2013D in compressed form.","The graphics data may comprise one or more graphics primitives. As used herein, the term graphics primitive includes polygons, parametric surfaces, splines, NURBS (non-uniform rational B-splines), sub-divisions surfaces, fractals, volume primitives, and particle systems. These graphics primitives are described in detail in the text book entitled \u201cComputer Graphics: Principles and Practice\u201d by James D. Foley, et al., published by Addison-Wesley Publishing Co., Inc., 1996. Note polygons are referred to throughout this detailed description for simplicity, but the embodiments and examples described may also be used with graphics data comprising other types of graphics primitives.","B. Rendering Units","Rendering units A\u2013D (also referred to herein as draw units) are configured to receive graphics instructions and data from control unit  and then perform a number of functions, depending upon the exact implementation. For example, rendering units A\u2013D may be configured to perform decompression (if the data is compressed), transformation, clipping, lighting, texturing, depth cueing, transparency processing, set-up, and screen space rendering of various graphics primitives occurring within the graphics data. Each of these features is described separately below. In one embodiment, rendering units  may comprise first rendering unit  and second rendering unit . First rendering unit  may be configured to perform decompression (for compressed graphics data), format conversion, transformation and lighting. Second rendering unit  may be configured to perform screen space setup, screen space rasterization, and sample rendering. In one embodiment, first rendering unit  may be coupled to first data memory , and second data memory  second data memory . First data memory  may comprise SDRAM, and second data memory  may comprise RDRAM. In one embodiment, first rendering unit  may be a processor such as a high-performance DSP (digital signal processing) type core, or other high performance arithmetic processor (e.g., a processor with one or more a hardware multiplier and adder trees). Second rendering unit  may be a dedicated high speed ASIC (Application Specific Integrated Circuits) chip.","Depending upon the type of compressed graphics data received, rendering units A\u2013D may be configured to perform arithmetic decoding, run-length decoding, Huffinan decoding, and dictionary decoding (e.g., LZ77, LZSS, LZ78, and LZW). In another embodiment, rendering units A\u2013D may be configured to decode graphics data that has been compressed using geometric compression. Geometric compression of 3D graphics data may achieve significant reductions in data size while retaining most of the image quality. Two methods for compressing and decompressing 3D geometry are described in\n\n","In embodiments of graphics system  that support decompression, the graphics data received by each rendering unit  is decompressed into one or more graphics \u201cprimitives\u201d which may then be rendered. The term primitive refers to components of objects that define its shape (e.g., points, lines, triangles, polygons in two or three dimensions, polyhedra, or free-form surfaces in three dimensions). Rendering units  may be any suitable type of high performance processor (e.g., specialized graphics processors or calculation units, multimedia processors, DSPs, or general purpose processors).","Transformation refers to manipulating an object and includes translating the object (i.e., moving the object to a different location), scaling the object (i.e., stretching or shrinking), and rotating the object (e.g., in three-dimensional space, or \u201c3-space\u201d).","Lighting refers to calculating the illumination of the objects within the displayed image to determine what color and or brightness each individual object will have. Depending upon the shading algorithm being used (e.g., constant, Gouraud, or Phong), lighting may be evaluated at a number of different locations. For example, if constant shading is used (i.e., each pixel of a polygon has the same lighting), then the lighting need only be calculated once per polygon. If Gouraud shading is used, then the lighting is calculated once per vertex. Phong shading calculates the lighting on a per-pixel basis.","Clipping refers to the elimination of graphics primitives or portions of graphics primitives that lie outside of a 3-D view volume in world space. The 3-D view volume may represent that portion of world space that is visible to a virtual observer situated in world space. For example, the view volume may be a solid truncated pyramid generated by a 2-D view window and a viewpoint located in world space. The solid truncated pyramid may be imagined as the union of all rays emanating from the viewpoint and passing through the view window. The viewpoint may represent the world space location of the virtual observer. Primitives or portions of primitives that lie outside the 3-D view volume are not currently visible and may be eliminated from further processing. Primitives or portions of primitives that lie inside the 3-D view volume are candidates for projection onto the 2-D view window.","In order to simplify the clipping and projection computations, primitives may be transformed into a second, more convenient, coordinate system referred to herein as the viewport coordinate system. In viewport coordinates, the view volume maps to a canonical 3-D viewport that may be more convenient for clipping against.","Generally, set-up refers to computations necessary to prepare for sample value computations (e.g., calculating triangle edge slopes and\/or coefficients for triangle plane equations and initial pixel positions).","Graphics primitives or portions of primitives that survive the clipping computation may be projected onto a 2-D viewport depending on the results of a visibility determination. Instead of clipping in 3-D, graphics primitives may be projected onto a 2-D view plane (which includes the 2-D viewport) and then clipped with respect to the 2-D viewport.","Screen-space rendering refers to the calculations performed to actually calculate the data used to generate each pixel that will be displayed. In prior art systems, each pixel is calculated and then stored in a frame buffer. The contents of the frame buffer are then output to the display device to create the final image. In the embodiment of graphics system  shown in the figure, however, rendering units A\u2013D calculate \u201csamples\u201d instead of actual pixel data. This allows rendering units A\u2013D to \u201csuper-sample\u201d or calculate more than one sample per pixel. Super-sampling is described in greater detail below. The rendering units A\u2013D may also generate a greater area of samples than the viewable area of the display  for various effects such as panning and zooming. Note that rendering units A\u2013B may comprise a number of smaller functional units, e.g., a separate set-up\/decompress unit and a lighting unit.\n\n","Each rendering unit A\u2013D may comprise two sets of instruction and data memories  and . In one embodiment, data memories  and  may be configured to store both data and instructions for rendering units A\u2013D. While implementations may vary, in one embodiment data memories  and  may comprise two 8 MByte SDRAMs providing 16 MBytes of storage for each rendering unit A\u2013D. Data memories  and  may also comprise RDRAMs (Rambus DRAMs). In one embodiment, RDRAMs may be used to support the decompression and setup operations of each rendering unit, while SDRAMs may be used to support the draw functions of rendering units A\u2013D.","D. Schedule Unit","Schedule unit  may be coupled between the rendering units A\u2013D and the sample memories A\u2013N. Schedule unit  is configured to sequence the completed samples and store them in sample memories A\u2013N. Note in larger configurations, multiple schedule units  may be used in parallel. In one embodiment, schedule unit  may be implemented as a crossbar switch.","E. Sample Memories","Super-sampled sample buffer  comprises sample memories A\u2013N, which are configured to store the plurality of samples generated by the rendering units. As used herein, the term \u201csample buffer\u201d refers to one or more memories which store samples. As previously noted, one or more samples are filtered to form output pixels (i.e., pixels to be displayed on a display device). The number of samples stored may be greater than, equal to, or less than the total number of pixels output to the display device to refresh a single frame. Each sample may correspond to one or more output pixels. As used herein, a sample \u201ccorresponds\u201d to an output pixel when the sample's information contributes to final output value of the pixel. Note, however, that some samples may contribute zero to their corresponding output pixel after filtering takes place.","Stated another way, the sample buffer stores a plurality of samples that have positions that correspond to locations in screen space on the display, i.e., the samples contribute to one or more output pixels on the display. The number of stored samples may be greater than the number of pixel locations, and more than one sample may be combined in the convolution (filtering) process to generate a particular output pixel displayed on the display device. Any given sample may contribute to one or more output pixels.","Sample memories A\u2013N may comprise any of a number of different types of memories (e.g., SDRAMs, SRAMs, RDRAMs, 3DRAMs, or next-generation 3DRAMs) in varying sizes. In one embodiment, each schedule unit  is coupled to four banks of sample memories, wherein each bank comprises four 3DRAM-64 memories. Together, the 3DRAM-64 memories may form a 116-bit deep super-sampled sample buffer that stores multiple samples per pixel. For example, in one embodiment, each sample memory A\u2013N may store up to sixteen samples per pixel.","3DRAM-64 memories are specialized memories configured to support full internal double buffering with single buffered Z in one chip. The double buffered portion comprises two RGBX buffers, wherein X is a fourth channel that can be used to store other information (e.g., alpha). 3DRAM-64 memories also have a lookup table that takes in window ID information and controls an internal 2-1 or 3-1 multiplexer that selects which buffer's contents will be output. 3DRAM-64 memories are next-generation 3DRAM memories that may soon be available from Mitsubishi Electric Corporation's Semiconductor Group. In one embodiment, four chips used in combination are sufficient to create a double-buffered 1280\u00d71024 super-sampled sample buffer. Since the memories are internally double-buffered, the input pins for each of the two frame buffers in the double-buffered system are time multiplexed (using multiplexers within the memories). The output pins may similarly be time multiplexed. This allows reduced pin count while still providing the benefits of double buffering. 3DRAM-64 memories further reduce pin count by not having z output pins. Since z comparison and memory buffer selection is dealt with internally, this may simplify sample buffer  (e.g., using less or no selection logic on the output side). Use of 3DRAM-64 also reduces memory bandwidth since information may be written into the memory without the traditional process of reading data out, performing a z comparison or blending operation, and then writing data back in. Instead, the data may be simply written into the 3DRAM-64, with the memory performing the steps described above internally.","However, in other embodiments of graphics system , other memories (e.g., SDRAMs, SRAMs, RDRAMs, or current generation 3DRAMs) may be used to form sample buffer .","Graphics processor  may be configured to generate a plurality of sample positions according to a particular sample positioning scheme (e.g., a regular grid or a perturbed regular grid). Alternatively, the sample positions (or offsets that are added to regular grid positions to form the sample positions) may be read from a sample position memory (e.g., a RAM\/ROM table). Upon receiving a polygon (e.g. a triangle) that is to be rendered, graphics processor  determines which samples fall within the polygon based upon the sample positions. Graphics processor  renders the samples that fall within the polygon and stores rendered samples in sample memories A\u2013N. Note as used herein the terms render and draw are used interchangeably and refer to calculating color values for samples. Depth values, alpha values, and other per-sample values may also be calculated in the rendering or drawing process.","F. Sample-to-pixel Calculation Units","Sample-to-pixel calculation units A\u2013D may be coupled between sample memories A\u2013N and DACs A\u2013B. Sample-to-pixel calculation units A\u2013D are configured to read selected samples from sample memories A\u2013N and then perform a convolution (e.g., a filtering and weighting function or a low pass filter) on the samples to generate the output pixel values which are output to DACs A\u2013B. The sample-to-pixel calculation units A\u2013D may be programmable to allow them to perform different filter functions at different times, depending upon the type of output desired. In one embodiment, sample-to-pixel calculation units A\u2013D may implement a filter to convert the super-sampled sample buffer data (stored in sample memories A\u2013N) to single pixel values. The filter may cover a 5\u00d75 support in the sample buffer (i.e. five pixel units horizontally and vertically). In other embodiments, calculation units A\u2013D may filter a selected number of samples to calculate an output pixel. The filtered samples may be multiplied by a variable weighting factor that gives a variable weight to samples based on the sample's position relative to the center of the pixel being calculated. Any of a variety of filters may be used either alone or in combination, e.g., the box filter, the tent filter, the cone filter, the cylinder filter, the Gaussian filter, the Catmull-Rom filter, the Mitchell-Netravali filter, the windowed sinc filter, or in general, any form of bandpass filter or any of various approximations to the sinc filter.","Sample-to-pixel calculation units A\u2013D may be implemented with ASICs (Application Specific Integrated Circuits), or with a high-performance DSP (digital signal processing) type core, or other high performance arithmetic processor (e.g., a processor with one or more hardware multipliers and adder trees). Sample-to-pixel calculation units A\u2013D may also be configured with one or more of the following features: color look-up using pseudo color tables, direct color, inverse gamma correction, programmable gamma corrections, color space conversion and conversion of pixels to non-linear light space. Other features of sample-to-pixel calculation units A\u2013D may include programmable video timing generators, programmable pixel clock synthesizers, cursor generators, and crossbar functions. Once the sample-to-pixel calculation units have manipulated the timing and color of each pixel, the pixels are output to DACs A\u2013B.","G. DACs","DACs A\u2013B operate as the final output stage of graphics system . The DACs A\u2013B serve to translate the digital pixel data received from the sample-to-pixel calculation units  into analog video signals that are then sent to the display device. Note in one embodiment DACs A\u2013B may be bypassed or omitted completely in order to output digital pixel data in lieu of analog video signals. This may be useful when display device  is based on a digital technology (e.g., an LCD-type display or a digital micro-mirror display).","4. Super-sampling\u2014","Turning now to , an example of traditional, non-super-sampled pixel value calculation is illustrated. Each pixel has exactly one data point calculated for it, and the single data point is located at the center of the pixel. For example, only one data point (i.e., sample ) contributes to value of pixel .","Turning now to , an example of one embodiment of super-sampling is illustrated. In this embodiment, a number of samples are calculated. The number of samples may be related to the number of pixels or completely independent of the number of pixels. In this example, there are 18 samples distributed in a regular grid across nine pixels. Even with all the samples present in the figure, a simple one to one correlation could be made (e.g., by throwing out all but the sample nearest to the center of each pixel). However, the more interesting case is performing a filtering function on multiple samples to determine the final pixel values. Also, as noted above, a single sample can be used to generate a plurality of output pixels, i.e., sub-sampling.","A circular filter  is illustrated in the figure. In this example, samples A\u2013B both contribute to the final value of pixel . This filtering process may advantageously improve the realism of the image displayed by smoothing abrupt edges in the displayed image (i.e., performing anti-aliasing). Filter  may simply average samples A\u2013B to form the final value of output pixel , or it may increase the contribution of sample B (at the center of pixel ) and diminish the contribution of sample A (i.e., the sample farther away from the center of pixel ). Circular filter  is repositioned for each output pixel being calculated so the center of filter  coincides with the center position of the pixel being calculated. Other filters and filter positioning schemes are also possible and contemplated.","Turning now to , another embodiment of super-sampling is illustrated. In this embodiment, however, the samples are positioned randomly. More specifically, different sample positions are selected and provided to graphics processor  (and render units A\u2013D), which calculate color information to form samples at these different locations. Thus the number of samples falling within filter  may vary from pixel to pixel.","5. Super-sampled Sample Buffer with Real-time Convolution\u2014","Turning now to , a diagram illustrating one possible configuration for the flow of data through one embodiment of graphics system  is shown. As the figure shows, geometry data  is received by graphics system  and used to perform draw process . The draw process  is implemented by one or more of control unit , rendering units , memories , and schedule unit . Geometry data  comprises data for one or more polygons. Each polygon comprises a plurality of vertices (e.g., three vertices in the case of a triangle), some of which may be shared. Data such as x, y, and z coordinates, color data, lighting data and texture map information may be included for each vertex.","In addition to the vertex data, draw process  (which may be performed by rendering units A\u2013D) also receives sample coordinates from a sample position memory . In one embodiment, position memory  is embodied within rendering units A\u2013D. In another embodiment, position memory  may be realized as part of the texture and render data memories, or as a separate memory. Sample position memory  is configured to store position information for samples that are calculated in draw process  and then stored into super-sampled sample buffer . In one embodiment, position memory  may be configured to store entire sample addresses. However, this may involve increasing the size of position memory . Alternatively, position memory  may be configured to store only x- and y-offsets for the samples. Storing only the offsets may use less storage space than storing each sample's entire position. The offsets may be relative to bin coordinates or relative to positions on a regular grid. The sample position information stored in sample position memory  may be read by a dedicated sample position calculation unit (not shown) and processed to calculate example sample positions for graphics processor . More detailed information on sample position offsets is included below (see description of ).","In another embodiment, sample position memory  may be configured to store a table of random numbers. Sample position memory  may also comprise dedicated hardware to generate one or more different types of regular grids. This hardware may be programmable. The stored random numbers may be added as offsets to the regular grid positions generated by the hardware. In one embodiment, the sample position memory may be programmable to access or \u201cunfold\u201d the random number table in a number of different ways. This may allow a smaller table to be used without visual artifacts caused by repeating sample position offsets. In one embodiment, the random numbers may be repeatable, thereby allowing draw process  and sample-to-pixel calculation process  to utilize the same offset for the same sample without necessarily storing each offset.","As shown in the figure, sample position memory  may be configured to store sample offsets generated according to a number of different schemes such as a regular square grid, a regular hexagonal grid, a perturbed regular grid, or a random (stochastic) distribution. Graphics system  may receive an indication from the operating system, device driver, or the geometry data  that indicates which type of sample positioning scheme is to be used. Thus the sample position memory  is configurable or programmable to generate position information according to one or more different schemes. More detailed information on several sample position schemes are described further below (see description of ).","In one embodiment, sample position memory  may comprise a RAM\/ROM that contains stochastic sample points (or locations) for different total sample counts per bin. As used herein, the term \u201cbin\u201d refers to a region or area in screen-space and contains however many samples are in that area (e.g., the bin may be 1\u00d71 pixels in area, 2\u00d72 pixels in area, or more generally, M\u00d7Npixels in area where Mand Nare positive integer values). The use of bins may simplify the storage and access of samples in sample buffer . A number of different bin sizes may be used (e.g., one sample per bin, four samples per bin, or more generally, Nsamples per bin where Nis a positive integer). In the preferred embodiment, each bin has an xy-position that corresponds to a particular location on the display. The bins are preferably regularly spaced. In this embodiment the bins' xy-positions may be determined from the bin's storage location within sample buffer . The bins' positions correspond to particular positions on the display. In some embodiments, the bin positions may correspond to pixel centers, while in other embodiments the bin positions correspond to points that are located between pixel centers. The specific position of each sample within a bin may be determined by looking up the sample's offset in the RAM\/ROM table (the offsets may be stored relative to the corresponding bin position). However, depending upon the implementation, not all bin sizes may have a unique RAM\/ROM entry. Some bin sizes may simply read a subset of the larger bin sizes' entries. In one embodiment, each supported size has at least four different sample position scheme variants, which may reduce final image artifacts due to repeating sample positions.","In one embodiment, position memory  may store pairs of 8-bit numbers, each pair comprising an x-offset and a y-offset (other possible offsets are also possible, e.g., a time offset and\/or a z-offset). When added to a bin position, each pair defines a particular position in screen space. The term \u201cscreen space\u201d refers generally to the coordinate system of the display device. To improve read times, memory  may be constructed in a wide\/parallel manner so as to allow the memory to output more than one sample location per clock cycle.","Once the sample positions have been read from sample position memory , draw process  selects the sample positions that fall within the polygon currently being rendered. Draw process  then calculates the z and color information (which may include alpha, other depth of field information values, or other values) for each of these samples and stores the data into sample buffer . In one embodiment, the sample buffer may only single-buffer z values (and perhaps alpha values) while double buffering other sample components such as color. Unlike prior art systems, graphics system  may double buffer all samples (although not all sample components may be double-buffered, i.e., the samples may have components that are not double-buffered, or not all samples may be double-buffered). In one embodiment, the samples are stored into sample buffer  in bins. In some embodiments, the size of bins, i.e., the quantity of samples within a bin, may vary from frame to frame and may also vary across different regions of display device  within a single frame. For example, bins along the edges of display device may comprise only one sample, while bins corresponding to pixels near the center of display device  may comprise sixteen samples. Note the area of bins may vary from region to region. The use of bins will be described in greater detail below in connection with .","In parallel and independently of draw process , filter process  is configured to read samples from sample buffer , filter (i.e., filter) them, and then output the resulting output pixel to display device . Sample-to-pixel calculation units  implement filter process . Thus, for at least a subset of the output pixels, the filter process is operable to filter a plurality of samples to produce a respective output pixel. In one embodiment, filter process  is configured to:\n\n","Turning now to , a diagram illustrating an alternate embodiment of graphics system  is shown. In this embodiment, two or more sample position memories A and B are utilized. Thus, the sample position memories A\u2013B are essentially double-buffered. If the sample positions are kept the same from frame to frame, then the sample positions may be single buffered. However, if the sample positions may vary from frame to frame, then graphics system  may be advantageously configured to double-buffer the sample positions. The sample positions may be double buffered on the rendering side (i.e., memory A may be double buffered) and or the filter\/convolve side (i.e., memory B may be double buffered). Other combinations are also possible. For example, memory A may be single-buffered, while memory B is doubled buffered. This configuration may allow one side of memory B to be used for refreshing (i.e., by filter\/convolve process ) while the other side of memory B is being updated. In this configuration, graphics system  may change sample position schemes on a per-frame basis by shifting the sample positions (or offsets) from memory A to double-buffered memory B as each frame is rendered. Thus, the positions used to calculate the samples (read from memory A) are copied to memory B for use during the filtering process (i.e., the sample-to-pixel conversion process). Once the position information has been copied to memory B, position memory A may then be loaded with new sample position offsets to be used for the second frame to be rendered. In this way the sample position information follows the samples from the draw\/render process to the filter process.","Yet another alternative embodiment may store tags to offsets with the samples themselves in super-sampled sample buffer . These tags may be used to look-up the offset\/perturbation associated with each particular sample.","6. Sample Positioning Schemes",{"@attributes":{"id":"p-0121","num":"0134"},"figref":"FIG. 8","b":"190"},"Perturbed regular grid positioning scheme  is based upon the previous definition of a regular grid. However, the samples in perturbed regular grid scheme  may be offset from their corresponding grid intersection. In one embodiment, the samples may be offset by a random angle (e.g., from 0\u00b0 to 360\u00b0) and a random distance, or by random x and y offsets, which may or may not be limited to a predetermined range. The offsets may be generated in a number of ways, e.g., by hardware based upon a small number of seeds, looked up from a table, or by using a pseudo-random function. Once again, perturbed regular gird scheme  may be based on any type of regular grid (e.g., square, or hexagonal). A rectangular or hexagonal perturbed grid may be particularly desirable due to the geometric properties of these grid types.","Stochastic sample positioning scheme  represents a third potential type of scheme for positioning samples. Stochastic sample positioning involves randomly distributing the samples across a region (e.g., the displayed region on a display device or a particular window). Random positioning of samples may be accomplished through a number of different methods, e.g., using a random number generator such as an internal clock to generate pseudo-random numbers. Random numbers or positions may also be pre-calculated and stored in memory.","Turning now to , details of one embodiment of perturbed regular grid scheme  are shown. In this embodiment, samples are randomly offset from a regular square grid by x- and y-offsets. As the enlarged area shows, sample  has an x-offset  that specifies its horizontal displacement from its corresponding grid intersection point . Similarly, sample  also has a y-offset  that specifies its vertical displacement from grid intersection point . The random offset may also be specified by an angle and distance. As with the previously disclosed embodiment that utilized angles and distances, x-offset  and y-offset  may be limited to a particular minimum and or maximum value or range of values.","Turning now to , details of another embodiment of perturbed regular grid scheme  are shown. In this embodiment, the samples are grouped into \u201cbins\u201d A\u2013D. In this embodiment, each bin comprises nine (i.e., 3\u00d73) samples. Different bin sizes may be used in other embodiments (e.g., bins storing 2\u00d72 samples or 4\u00d74 samples). In the embodiment shown, each sample's position is determined as an offset relative to the position of the bin. The position of the bins may be defined as any convenient position related to the grid, e.g., the lower left-hand corners A\u2013D as shown in the figure. For example, the position of sample  is determined by summing x-offset  and y-offset  to the x and y coordinates of the corner D of bin D. As previously noted, this may reduce the size of the sample position memory used in some embodiments.","Turning now to , one possible method for rapidly converting samples stored in sample buffer  into pixels is shown. In this embodiment, the contents of sample buffer  are organized into columns (e.g., Cols. \u2013). Each column in sample buffer  may comprise a two-dimensional array of bins. The columns may be configured to horizontally overlap (e.g., by one or more bins), and each column may be assigned to a particular sample-to-pixel calculation unit A\u2013D for the convolution process. The amount of the overlap may depend upon the extent of the filter being used. The example shown in the figure illustrates an overlap of two bins (each square such as square  represents a single bin comprising one or more samples). Advantageously, this configuration may allow sample-to-pixel calculation units A\u2013D to work independently and in parallel, with each sample-to-pixel calculation unit A\u2013D receiving and converting its own column. Overlapping the columns will eliminate visual bands or other artifacts appearing at the column boundaries for any operators larger than a pixel in extent.","Turning now to , more details of one embodiment of a method for reading the samples from a super-sampled sample buffer are shown. As the figure illustrates, the convolution filter kernel  travels across column  (see arrow ) to generate output pixels. The sample-to-pixel calculation unit assigned to column  may implement the convolution filter kernel . A bin cache may used to provide quick access to the bins that may potentially contribute samples to the output pixel. As the convolution process proceeds, bins are read from the super-sampled sample buffer and stored in the bin cache. In one embodiment, bins that are no longer needed  are overwritten in the cache by new bins . As each pixel is generated, convolution filter kernel  shifts. Kernel  may be visualized as proceeding in a sequential fashion within the column in the direction indicated by arrow . When kernel  reaches the end of the column, it may shift down one or more rows of bins and then proceed again. Thus, the convolution process proceeds in a scan line manner, generating one column of output pixels for display.","Turning now to , a diagram illustrating potential border conditions is shown. In one embodiment, the bins that fall outside of sample window  may be replaced with samples having predetermined background colors specified by the user. In another embodiment, bins that fall outside the window are not used by setting their weighting factors to zero (and then dynamically calculating normalization coefficients). In yet another embodiment, the bins at the inside edge of the window may be duplicated to replace those outside the window. This is indicated by outside bin  being replaced by mirror inside bin .",{"@attributes":{"id":"p-0129","num":"0142"},"figref":["FIG. 12A","FIG. 12A"],"b":["102","106","200","150","202","204","150","206","208","208","210","212"]},"Next, one of the sample position schemes (e.g., regular grid, perturbed regular grid, or stochastic) is selected from the sample position memory  (step ). The sample position scheme will generally have been pre-programmed into the sample position memory , but may also be selected \u201con-the-fly\u201d. Based upon this sample position scheme and the sample density of the region containing the triangle, rendering units A\u2013D determine which bins may contain samples located within the triangle's boundaries (step ). The offsets for the samples within these bins are then read from sample position memory  (step ). Each sample's position is then calculated using the offsets and is compared with the triangle's vertices to determine if the sample is within the triangle (step ). Step  is discussed in greater detail below.","For each sample that is determined to be within the triangle, the rendering unit draws the sample by calculating the sample's color, alpha and other attributes. This may involve lighting calculation and interpolation based upon the color and texture map information associated with the vertices of the triangle. Once the sample is rendered, it may be forwarded to schedule unit , which then stores the sample in sample buffer  (step ).","Note the embodiment of the method described above is used for explanatory purposes only and is not meant to be limiting. For example, in some embodiments the steps shown in the figure as occurring serially may be implemented in parallel. Furthermore, some steps may be reduced or eliminated in certain embodiments of the graphics system (e.g., steps \u2013 in embodiments that do not implement geometry compression or steps \u2013 in embodiments that do not implement a variable resolution super-sampled sample buffer).","7. Determination of which Samples Reside within the Polygon being Rendered","The comparison may be performed in a number of different ways. In one embodiment, the deltas between the three vertices defining the triangle are first determined. For example, these deltas may be taken in the order of first to second vertex (v2\u2212v1)=d12, second to third vertex (v3\u2212v2)=d23, and third vertex back to the first vertex (v1\u2212v3)=d31. These deltas form vectors, and each vector may be categorized as belonging to one of the four quadrants of the coordinate plane (e.g., by using the two sign bits of its delta X and Y coefficients). A third condition may be added determining whether the vector is an X-major vector or Y-major vector. This may be determined by calculating whether abs(deltax) is greater than abs(deltay).","Using these three bits of information, the vectors may each be categorized as belonging to one of eight different regions of the coordinate plane. If three bits are used to define these regions, then the X-sign bit (shifted left by two), the Y-sign bit (shifted left by one), and the X-major bit, may be used to create the eight regions as shown in .","Next, three edge equations may be used to define the inside portion of the triangle. These edge equations (or half-plane equations) may be defined using slope-intercept form. To reduce the numerical range needed, both X-major and Y-major equation forms may be used (such that the absolute value of the slope value may be in the range of 0 to 1). Thus, the two edge equations are:\n\n-major: <0, when the point is below the line\n\n-major: <0, when the point is to the left of the line\n","The X-major equations produces a negative versus positive value when the point in question is below the line, while the Y-major equation produces a negative versus positive value when the point in question is to the left of the line. Since which side of the line is the \u201caccept\u201d side is known, the sign bit (or the inverse of the sign bit) of the edge equation result may be used to determine whether the sample is on the \u201caccept\u201d side or not. This is referred to herein as the \u201caccept bit\u201d. Thus, a sample is on the accept side of a line if:\n\n-major: (<0)<x or>accept\n\n-major: (<0)<x or>accept\n","The accept bit may be calculated according to the following table, wherein cw designates whether the triangle is clockwise (cw=1) or counter-clockwise (cw=0):","1: accept=!cw","0: accept=cw","4: accept=cw","5: accept=cw","7: accept=cw","6: accept=!cw","2: accept=!cw","3: accept=!cw","Tie breaking rules for this representation may also be implemented (e.g., coordinate axes may be defined as belonging to the positive octant). Similarly, X-major may be defined as owning all points that tie on the slopes.","In an alternate embodiment, the accept side of an edge may be determined by applying the edge equation to the third vertex of the triangle (the vertex that is not one of the two vertices forming the edge). This method may incur the additional cost of a multiply-add, which may not be used by the technique described above.","To determine the \u201cfaced-ness\u201d of a triangle (i.e., whether the triangle is clockwise or counter-clockwise), the delta-directions of two edges of the triangle may be checked and the slopes of the two edges may be compared. For example, assuming that edge has a delta-direction of 1 and the second edge (edge) has a delta-direction of 0, 4, or 5, then the triangle is counter-clockwise. If, however, edge has a delta-direction of 3, 2, or 6, then the triangle is clockwise. If edge has a delta-direction of 1 (i.e., the same as edge), then comparing the slopes of the two edges breaks the tie (both are x-major). If edge has a greater slope, then the triangle is counter-clockwise. If edge has a delta-direction of 7 (the exact opposite of edge), then again the slopes are compared, but with opposite results in terms of whether the triangle is clockwise or counter-clockwise.","The same analysis can be exhaustively applied to all combinations of edge and edge delta-directions, in every case determining the proper faced-ness. If the slopes are the same in the tie case, then the triangle is degenerate (i.e., with no interior area). It can be explicitly tested for and culled, or, with proper numerical care, it could be let through as it will cause no pixels to render. One special case is when a triangle splits the view plane, but that may be detected earlier in the pipeline (e.g., when front plane and back plane clipping are performed).","Note in most cases only one side of a triangle is rendered. Thus, after the faced-ness of a triangle is determined, if the face is the one to be rejected, then the triangle can be culled (i.e., subject to no further processing with no pixels generated). Further note that this determination of faced-ness only uses one additional comparison (i.e., of the slope of edge to that of edge) beyond factors already computed. Many traditional approaches may utilize more complex computation (though at earlier stages of the set-up computation).",{"@attributes":{"id":"p-0151","num":"0164"},"figref":"FIG. 13","b":["250","170","252","254","256","258"]},"In one embodiment, the sample-to-pixel calculation units  may be configured to calculate this distance (i.e., the extent of the filter at sample's position) and then use it to index into a table storing filter weight values according to filter extent (step ). In another embodiment, however, the potentially expensive calculation for determining the distance from the center of the pixel to the sample (which typically involves a square root function) is avoided by using distance squared to index into the table of filter weights. Alternatively, a function of x and y may be used in lieu of one dependent upon a distance calculation. In one embodiment, this may be accomplished by utilizing a floating point format for the distance (e.g., four or five bits of mantissa and three bits of exponent), thereby allowing much of the accuracy to be maintained while compensating for the increased range in values. In one embodiment, the table may be implemented in ROM. However, RAM tables may also be used. Advantageously, RAM tables may, in some embodiments, allow the graphics system to vary the filter coefficients on a per-frame basis. For example, the filter coefficients may be varied to compensate for known shortcomings of the display or for the user's personal preferences. The graphics system can also vary the filter coefficients on a screen area basis within a frame, or on a per output pixel basis. Another alternative embodiment may actually calculate the desired filter weights for each sample using specialized hardware (e.g., multipliers and adders). The filter weight for samples outside the limits of the convolution filter may simply be multiplied by a filter weight of zero (step ), or they may be removed from the calculation entirely.","Once the filter weight for a sample has been determined, the sample may then be multiplied by its filter weight (step ). The weighted sample may then be summed with a running total to determine the final output pixel's un-normalized (and pre-gamma correction) color value (step ). The filter weight may also be added to a running total pixel filter weight (step ), which is used to normalize the filtered pixels. Normalization (i.e. step ) advantageously prevents the filtered pixels (e.g., pixels with more samples than other pixels) from appearing too bright or too dark by compensating for gain introduced by the convolution process. After all the contributing samples have been weighted and summed, the total pixel filter weight may be used to divide out the gain caused by the filtering. Finally, the normalized output pixel may be output for gamma correction, digital-to-analog conversion (if necessary), and eventual display (step ).",{"@attributes":{"id":"p-0154","num":"0167"},"figref":["FIG. 14","FIG. 14"],"b":["288","288","288","296","294","292","290","290"]},"The windowed sinc filter is particularly useful for anti-aliasing. Advantageously, the windowed sinc filter contains negative lobes that resharpen some of the blended or \u201cfuzzed\u201d image. Negative lobes are areas where the filter causes the samples to subtract from the pixel being calculated. In contrast, samples on either side of the negative lobe add to the pixel being calculated.","Example values for samples \u2013 are illustrated in boxes \u2013. In this example, each sample comprises red, green, blue, and alpha values, in addition to the sample's positional data. Block  illustrates the calculation of each pixel component value for the non-normalized output pixel. As block  indicates, potentially undesirable gain is introduced into the final pixel values (i.e., an output pixel having a red component value of 2000 is much higher than any of the sample's red component values). As previously noted, the filter values may be summed to obtain normalization value 308. Normalization value 308 is used to divide out the unwanted gain from the output pixel. Block  illustrates this process and the final normalized example pixel values.","Note the values used herein were chosen for descriptive purposes only and are not meant to be limiting. For example, the filter may have a large number of regions each with a different filter value. In one embodiment, some regions may have negative filter values. The filter utilized may be a continuous function that is evaluated for each sample based on the sample's distance from the pixel center. Also, note that floating point values may be used for increased precision. A variety of filters may be utilized, e.g., cylinder, cone, Gaussian, Mitchell-Netravalli, Catmull-Rom, windowed sinc, box, or tent.","A. Full-screen Anti-aliasing","The vast majority of current 3D graphics systems only provide \u201creal-time\u201d and \u201con-the-fly\u201d anti-aliasing for lines and dots. While some systems also allow the edge of a polygon to be \u201cfuzzed\u201d, this technique typically works best when all polygons have been pre-sorted in depth. This may defeat the purpose of having general-purpose 3D rendering hardware for most applications (which do not depth pre-sort their polygons). In one embodiment, graphics system  may be configured to implement full-screen anti-aliasing by stochastically sampling up to sixteen samples per output pixel, filtered by a 4\u00d74-convolution filter. Other filters may be used (e.g., a 5\u00d75 convolution filter, a 9\u00d79 convolution filter, an 11\u00d711 convolution filter, or more generally, an M\u00d7Nconvolution where Mand Nare positive integer values).","B. Variable Resolution Super-sampling","Currently, the straight-forward brute force method of utilizing a fixed number of samples per pixel location, e.g., an 8\u00d7 super-sampled sample buffer, would entail the use of eight times more memory, eight times the fill rate (i.e., memory bandwidth), and a convolution pipe capable of processing eight samples per pixel. Given the high resolution and refresh rates of current displays, a graphics system of this magnitude may be relatively expense to implement given today's level of integration.","In one embodiment, graphics system  may be configured to overcome these potential obstacles by implementing variable resolution super-sampling. In this embodiment, graphics system  mimics the human eye's characteristics by allocating a higher number of samples per pixel at one or more first locations on the screen (e.g., the point of foveation on the screen), with a drop-off in the number of samples per pixel for one or more second locations on the screen (e.g., areas farther away from the point of foveation). Depending upon the implementation, the point of foveation may be determined in a variety of ways. In one embodiment, the point of foveation may be a predetermined area around a certain object displayed upon the screen. For example, the area around a moving cursor or the main character in a computer game may be designated the point of foveation. In another embodiment, the point of foveation on the screen may be determined by eye-tracking, or eye-tracking and head-tracking. Even if eye\/head\/hand-tracking, cursor-based, or main character-based points of foveation are not implemented, the point of foveation may be fixed at the center of the screen, where the majority of viewer's attention is focused the majority of the time. Variable resolution super-sampling is described in greater detail below.","8. Variable-resolution Super-sampled Sample Buffer\u2014","A traditional frame buffer is one rectangular array of uniformly sampled pixels. For every pixel on the final display device (CRT or LCD), there is a single pixel or location of memory storage in the frame buffer (perhaps double buffered). There is a trivial one-to-one correspondence between the 2D memory address of a given pixel and its 2D sample address for the mathematics of rendering. Stated another way, in a traditional frame buffer there is no separate notion of samples apart from the pixels themselves. The output pixels are stored in a traditional frame buffer in a row\/column manner corresponding to how the pixels are provided to the display during display refresh.","In a variable-resolution super-sampled sample buffer, the number of computed samples per output pixel varies on a regional basis. Thus, output pixels in regions of greater interest are computed using a greater number of samples, thus producing greater resolution in this region, and output pixels in regions of lesser interest are computed using a lesser number of samples, thus producing lesser resolution in this region.","As previously noted, in some embodiments graphic system  may be configured with a variable resolution super-sampled sample buffer. To implement variable resolution super-sampling, sample buffer  may be divided into smaller pieces, called regions. The size, location, and other attributes of these regions may be configured to vary dynamically, as parameterized by run-time registers on a per-frame basis.","Turning now to , a diagram of one possible scheme for dividing sample buffer  is shown. In this embodiment, sample buffer  is divided into the following three nested regions: foveal region , medial region , and peripheral region . Each of these regions has a rectangular shaped outer border, but the medial and the peripheral regions have a rectangular shaped hole in their center. Each region may be configured with certain constant (per frame) properties, e.g., a constant density sample density and a constant size of pixel bin. In one embodiment, the total density range may be 256, e.g., a region could support between one sample for every 16 screen pixels (4\u00d74) and 16 samples for every 1 screen pixel. In other embodiments, the total density range may be limited to other values, e.g., 64. In one embodiment, the sample density varies, either linearly or non-linearly, across a respective region. Note in other embodiments the display may be divided into a plurality of constant sized regions (e.g., squares that are 4\u00d74 pixels in size, or more generally, M\u00d7Mpixels in size where parameter Mis a positive integer).","To simply perform calculations for polygons that encompass one or more region corners (e.g., a foveal region corner), the sample buffer may be further divided into a plurality of subregions. Turning now to , one embodiment of sample buffer  divided into sub-regions is shown. Each of these sub-regions are rectangular, allowing graphics system  to translate from a 2D address with a sub-region to a linear address in sample buffer . Thus, in some embodiments each sub-region has a memory base address, indicating where storage for the pixels within the sub-region starts. Each sub-region may also have a \u201cstride\u201d parameter associated with its width.","Another potential division of the super-sampled sample buffer is circular. Turning now to , one such embodiment is illustrated. For example, each region may have two radii associated with it (i.e., \u2013), dividing the region into three concentric circular-regions. The circular-regions may all be centered at the same screen point, the fovea center point. Note however, that the fovea center-point need not always be located at the center of the foveal region. In some instances it may even be located off-screen (i.e., to the side of the visual display surface of the display device). While the embodiment illustrated supports up to seven distinct circular-regions, it is possible for some of the circles to be shared across two different regions, thereby reducing the distinct circular-regions to five or less.","The circular regions may delineate areas of constant sample density actually used. For example, in the example illustrated in the figure, foveal region  may allocate a sample buffer density of 8 samples per screen pixel, but outside the innermost circle , it may only use 4 samples per pixel, and outside the next circle  it may only use two samples per pixel. Thus, in this embodiment the rings need not necessarily save actual memory (the regions do that), but they may potentially save memory bandwidth into and out of the sample buffer (as well as pixel convolution bandwidth). In addition to indicating a different effective sample density, the rings may also be used to indicate a different sample position scheme to be employed. As previously noted, these sample position schemes may stored in an on-chip RAM\/ROM, or in programmable memory.","As previously discussed, in some embodiments super-sampled sample buffer  may be further divided into bins. For example, a bin may store a single sample or an array of samples (e.g., 2\u00d72 or 4\u00d74 samples). In one embodiment, each bin may store between one and sixteen sample points, although other configurations are possible and contemplated. Each region may be configured with a particular bin size, and a constant memory sample density as well. Note that the lower density regions need not necessarily have larger bin sizes. In one embodiment, the regions (or at least the inner regions) are exact integer multiples of the bin size enclosing the region. This may allow for more efficient utilization of the sample buffer in some embodiments.","Variable-resolution super-sampling involves calculating a variable number of samples for each pixel displayed on the display device. Certain areas of an image may benefit from a greater number of samples (e.g., near object edges), while other areas may not need extra samples (e.g., smooth areas having a constant color and brightness). To save memory and bandwidth, extra samples may be used only in areas that may benefit from the increased resolution. For example, if part of the display is colored a constant color of blue (e.g., as in a background), then extra samples may not be particularly useful because they will all simply have the constant value (equal to the background color being displayed). In contrast, if a second area on the screen is displaying a 3D rendered object with complex textures and edges, the use of additional samples may be useful in avoiding certain artifacts such as aliasing. A number of different methods may be used to determine or predict which areas of an image would benefit from higher sample densities. For example, an edge analysis could be performed on the final image, and with that information being used to predict how the sample densities should be distributed. The software application may also be able to indicate which areas of a frame should be allocated higher sample densities.","A number of different methods may be used to implement variable-resolution super sampling. These methods tend to fall into the following two general categories: (1) those methods that concern the draw or rendering process, and (2) those methods that concern the convolution process. For example, samples may be rendered into the super-sampling sample buffer  using any of the following methods:\n\n","Varying sample density on a scan-line basis may be accomplished by using a look-up table of densities. For example, the table may specify that the first five pixels of a particular scan line have three samples each, while the next four pixels have two samples each, and so on.","On the convolution side, the following methods are possible:\n\n","A uniform convolve filter may, for example, have a constant extent (or number of samples selected) for each pixel calculated. In contrast, a continuously variable convolution filter may gradually change the number of samples used to calculate a pixel. The function may be vary continuously from a maximum at the center of attention to a minimum in peripheral areas.","Different combinations of these methods (both on the rendering side and convolution side) are also possible. For example, a constant sample density may be used on the rendering side, while a continuously variable convolution filter may be used on the samples.","Different methods for determining which areas of the image will be allocated more samples per pixel are also contemplated. In one embodiment, if the image on the screen has a main focal point (e.g., a character like Mario in a computer game), then more samples may be calculated for the area around Mario and fewer samples may be calculated for pixels in other areas (e.g., around the background or near the edges of the screen).","In another embodiment, the viewer's point of foveation may be determined by eye\/head\/hand-tracking. In head-tracking embodiments, the direction of the viewer's gaze is determined or estimated from the orientation of the viewer's head, which may be measured using a variety of mechanisms. For example, a helmet or visor worn by the viewer (with eye\/head tracking) may be used alone or in combination with a hand-tracking mechanism, wand, or eye-tracking sensor to provide orientation information to graphics system . Other alternatives include head-tracking using an infrared reflective dot placed on the user's forehead, or using a pair of glasses with head- and or eye-tracking sensors built in. One method for using head- and hand-tracking is disclosed in\n\n","As previously noted, eye-tracking may be particularly advantageous when used in conjunction with head-tracking. In eye-tracked embodiments, the direction of the viewer's gaze is measured directly by detecting the orientation of the viewer's eyes in relation to the viewer's head. This information, when combined with other information regarding the position and orientation of the viewer's head in relation to the display device, may allow an accurate measurement of viewer's point of foveation (or points of foveation if two eye-tracking sensors are used). One possible method for eye tracking is disclosed in U.S. Pat. No. 5,638,176 (entitled \u201cInexpensive Interferometric Eye Tracking System\u201d). Other methods for eye tracking are also possible and contemplated (e.g., the methods for head tracking listed above).","Regardless of which method is used, as the viewer's point of foveation changes position, so does the distribution of samples. For example, if the viewer's gaze is focused on the upper left-hand corner of the screen, the pixels corresponding to the upper left-hand corner of the screen may each be allocated eight or sixteen samples, while the pixels in the opposite corner (i.e., the lower right-hand corner of the screen) may be allocated only one or two samples per pixel. Once the viewer's gaze changes, so does the allotment of samples per pixel. When the viewer's gaze moves to the lower right-hand corner of the screen, the pixels in the upper left-hand corner of the screen may be allocated only one or two samples per pixel. Thus the number of samples per pixel may be actively changed for different regions of the screen in relation the viewer's point of foveation. Note in some embodiments, multiple users may be each have head\/eye\/hand tracking mechanisms that provide input to graphics system . In these embodiments, there may conceivably be two or more points of foveation on the screen, with corresponding areas of high and low sample densities. As previously noted, these sample densities may affect the render process only, the filter process only, or both processes.","Turning now to , one embodiment of a method for apportioning the number of samples per pixel is shown. The method apportions the number of samples based on the location of the pixel relative to one or more points of foveation. In , an eye- or head-tracking device  is used to determine the point of foveation  (i.e., the focal point of a viewer's gaze). This may be determined by using tracking device  to determine the direction that the viewer's eyes (represented as  in the figure) are facing. As the figure illustrates, in this embodiment, the pixels are divided into foveal region  (which may be centered around the point of foveation ), medial region , and peripheral region .","Three sample pixels are indicated in the figure. Sample pixel  is located within foveal region . Assuming foveal region  is configured with bins having eight samples, and assuming the convolution radius for each pixel touches four bins, then a maximum of 32 samples may contribute to each pixel. Sample pixel  is located within medial region . Assuming medial region  is configured with bins having four samples, and assuming the convolution radius for each pixel touches four bins, then a maximum of 16 samples may contribute to each pixel. Sample pixel  is located within peripheral region . Assuming peripheral region  is configured with bins having one sample each, and assuming the convolution radius for each pixel touches one bin, then there is a one sample to pixel correlation for pixels in peripheral region . Note these values are merely examples and a different number of regions, samples per bin, and convolution radius may be used.","Turning now to , the same example is shown, but with a different point of foveation . As the figure illustrates, when tracking device  detects a change in the position of point of foveation , it provides input to the graphics system, which then adjusts the position of foveal region  and medial region . In some embodiments, parts of some of the regions (e.g., medial region ) may extend beyond the edge of display device . In this example, pixel  is now within foveal region , while pixels  and  are now within the peripheral region. Assuming the sample configuration as the example in , a maximum of 32 samples may contribute to pixel , while only one sample will contribute to pixels  and . Advantageously, this configuration may allocate more samples for regions that are near the point of foveation (i.e., the focal point of the viewer's gaze). This may provide a more realistic image to the viewer without the need to calculate a large number of samples for every pixel on display device .","Turning now to , another embodiment of a computer system configured with a variable resolution super-sampled sample buffer is shown. In this embodiment, the center of the viewer's attention, i.e., the viewer's focal point (and very likely the viewer's point of foveation), is determined by position of main character . Medial and foveal regions are centered on or around main character  as the main character moves around the screen. In some embodiments, the main character may be a simple cursor (e.g., as moved by keyboard input or by a mouse).","In still another embodiment, regions with higher sample density may be centered around the middle of display device 's screen. Advantageously, this may require less control software and hardware while still providing a shaper image in the center of the screen (where the viewer's attention may be focused the majority of the time).","9. Computer Network\u2014","Referring now to , a computer network  is shown comprising at least one server computer  and one or more client computers A\u2013N. (In the embodiment shown in , client computers A\u2013B are depicted). One or more of the client systems may be configured similarly to computer system , with each having one or more graphics systems  as described above. Server  and client(s)  may be joined through a variety of connections , such as a local-area network (LAN), a wide-area network (WAN), or an Internet connection. In one embodiment, server  may store and transmit 3-D geometry data (which may be compressed) to one or more of clients . The clients  receive the compressed 3-D geometry data, decompress it (if necessary) and then render the geometry data. The rendered image is then displayed on the client's display device. The clients render the geometry data and display the image using super-sampled sample buffer and \u201con-the-fly\u201d filter techniques described above. In another embodiment, the compressed 3-D geometry data may be transferred between client computers .","10. Filtering","As mentioned above, a sample-to-pixel calculation unit generates output pixels by filtering rendered samples.  shows a graphical representation of the filtering process. The sample space is subdivided into an array of bins. In one embodiment, each bin may correspond to a unit pixel in screen space area. In other embodiments, a bin may correspond to a screen space area greater than or less than a unit pixel. The bin sample capacity, i.e. the number of samples stored per bin, may take any of a variety of values. For example, the bin sample capacity may equal 1, 2, 4, 8, 16, 32 or any power of two subject to the fundamental limitations of cost and geographical board area for additional memory chips. While powers of two are preferred, the bin sample capacity may take other positive integer values. In some embodiments, different regions of screen space may have different bin sample capacities. In one embodiment, the bin sample capacity (or capacities) may be determined at system initialization time and\/or may be dynamically reprogrammed.","In one embodiment, the convolution kernel  has a circular support with a diameter dequal to 5 bins. Every sample within the circular (or other shape) support of the filter may contribute to the pixel currently being computed. The filter center defines the pixel location. Pixel values (e.g. red, green, blue, and alpha) may be computed by forming a weighted sum of the corresponding values of the samples falling within the support of the filter.","The filter weight given to each sample depends on the filter function being used and on the distance of the sample from the filter center. Note that both the term \u201cfilter weight\u201d and the term \u201cfilter coefficient\u201d, as used herein, refer to the value of the filter function at a certain distance from the filter center. Also, note that each sample and each pixel may comprise a plurality of attributes, and therefore, each sample and each pixel may comprise a plurality of values. For example, when generating pixels for a color display, three values may be used for the representation of color for each pixel. In addition, for certain applications, an alpha value may also be included. In one embodiment, the samples and pixels may comprise values for red, green, blue, and alpha. That is, an independent filtering process may be performed for each of the different attributes (e.g., red, green, blue, and alpha). In general, the pixel values for red (r), green (g), blue (b), and alpha (\u03b1) are computed using the corresponding sample values (for samples within the filter support) as set forth in Equations 1\u20134 of . For example, the red pixel value ris determined by computing a weighted sum of the red sample values rfor samples falling in the filter support.","In the equations, the superscript p denotes a pixel value, and the superscript s denotes a sample value. The filter weights are represented by c, and the summation over j is a summation over the samples that are within the support (i.e. extent) of the filter. Equations 1\u20134 of  assume that the filter coefficients are pre-normalized, i.e., \n\n",{"@attributes":{"id":"p-0189","num":"0209"},"figref":["FIGS. 22A&B","FIG. 22B","FIG. 22A"],"sub":["1 ","2 ","1 ","2 ","1 ","2"]},"If the filter coefficients are not normalized to one, i.e. \n\n\nthen the filtering process will either attenuate or amplify color values in conversion from samples to pixels. Because unity gain is generally desirable, the filter coefficients cmay be pre-normalized, i.e. normalized before they are used in the weighted sum computations. The pre-normalization may be accomplished by dividing each filter weight cby the cumulative sum \n\n\ni.e. the sum of the filter weights corresponding to samples inside the filter support, as shown in Equation 5 of . The resulting filter weights cwill then obey the unity normalization condition, i.e. \n\n\nObserve that this pre-normalization may require as many divisions as there are samples in the filter support.\n","If the cumulative sum \n\n\nremains constant from one pixel to the next in a pixel array, the pre-normalizing divisions described above may be performed once, and the resulting normalized filter weights cmay be used repeatedly for each pixel in the pixel array. However, in many cases the cumulative sum \n\n\nof filter weights may vary from pixel to pixel. For example, the filter function, the filter support, the number of samples falling within the filter support, and\/or the relative positions of samples with respect to the filter center may vary from pixel to pixel. Furthermore, the filtering may take place over a region of non-uniform sample density, at the edges of the sample space (or screen space), and\/or over samples that are not at fixed positions from the filter center (i.e., the samples may not be located on a regular grid). In such cases, it may be computationally expensive to perform the pre-normalizing coefficient divisions for each pixel. Thus, in one set of embodiments, the original filter weights care used to generate intermediate weighted sums \n\n\nwhere xrepresents a generic sample value (e.g. red, green, blue, or alpha), and the intermediate weighted sums are normalized by the cumulative sum \n\n\nof filter weights. This computation may be referred to herein as post-normalization. See Equations 6\u20139 of  for an example of the post-normalization computation for the red, green, blue and alpha attributes. Post-normalization may require as many divisions as there are pixel attributes, and thus, may be significantly less expensive computationally than per-pixel pre-normalization. The cumulative sum \n\n\nwill be also referred to herein as the normalization factor.\n",{"@attributes":{"id":"p-0192","num":"0212"},"figref":["FIG. 23A","FIG. 15"],"b":["2310","2308","2300","2302","2302","354","2300","352"]},"The filter is positioned at pixel center . The filter has a circular support . Samples falling inside (outside) the filter support  are denoted by small black dots (small unfilled circles). The interior samples are used in computing the value of the current pixel. As shown, the filter has a support diameter of 5 bins.  shows a graph of the filter along a representative diameter of filter support  with the dotted lines representing the filter weights at various sample positions. If the filter center were shifted one bin distance to the right, the filter support  would contain a significantly larger number of high density samples and a smaller number of the low density samples, and the total collection of interior samples would have a distinct set of relative distances with respect to the new filter center. Thus, the normalization factor may vary significantly from pixel to pixel, especially for pixels near a boundary between two sample density regions, and especially in a direction normal to the region boundary.",{"@attributes":{"id":"p-0194","num":"0214"},"figref":["FIG. 24A","FIG. 24B"],"b":["2406","2410","2408","2404","2406","2412","2406","2406","2412"]},"It is noted that in some embodiments, rendering units  may generate dummy bins with dummy samples outside the screen space by reflection (and\/or translation) of corresponding bins inside the screen space. In these embodiments, the number of samples in the filter support may remain more uniform even for pixels near the screen space boundary.",{"@attributes":{"id":"p-0196","num":"0216"},"figref":"FIG. 25A","b":["2510","2508","2504","2504","2506"]},"Because the samples are not on a regular grid, the number of samples that are included in the computation of the pixel and the position of these samples relative to the filter center may vary from pixel to pixel. This is suggested in  where the dotted lines correspond to filter weights for the samples in the support of the filter. Therefore, in embodiments where the samples are not on a regular grid, the normalization factor for the filter coefficients may vary from pixel to pixel.","In some embodiments, for the cases mentioned above, the normalization factor \n\n\nmay be computed for every pixel. The normalization factor may be computed in parallel with one or more of the weighted sums \n\n\nassociated with each pixel. Recall that one weighted sum is associated with each pixel attribute such as red, green, blue and alpha. After performing the weighted sums for the pixel attribute values, the weighted sums may be divided by the normalization factor as suggested by Equations 6\u20139 of .\n",{"@attributes":{"id":"p-0199","num":"0219"},"figref":"FIG. 26","b":["2600","162"],"sup":["1 ","infinity ","infinity "],"sub":"1 "},"In step , the filter coefficients for each of the samples within the filter support are computed. The filter coefficient for each sample depends on the distance of the sample from the pixel center and on the filter function being used. The filter coefficients may be determined by function evaluation (e.g. Taylor series computation), table lookup, or any combination thereof. A table of filter coefficients may be indexed by sample radius (or radius squared). Any of a variety of interpolation schemes may be applied to more accurately estimate filter coefficients for sample radii intermediate to those represented in the table.","In step , the coefficients for the samples within the filter support are summed to obtain the normalization factor \n\n\nIn one embodiment, the coefficient summation may be implemented by an adder tree to be described more fully below.\n","In step , a weighted sum \n\n\nof the samples values xfor samples falling within the filter support is formed for each of the pixel attributes. In one embodiment, the pixels (and samples) may have, for example, red, green, blue, and alpha attributes. The samples may also have other attributes that may be weighted and summed.\n","As shown here, steps  and  (i.e. the sum of the coefficients and the weighted sums of the attributes) may be performed in parallel. Note that in other embodiments, the sum of the coefficients and the weighted sums of the attributes may be performed sequentially. In another embodiment, a first and second group of sums may be performed sequentially with the sums within each group being performed in parallel. For example, the sum of the coefficients and the weighted sums of red and green may be performed first, and then at a later time, the remaining weighted sums of blue and alpha may be performed. The sums may be performed in any order or combination.","In one embodiment, the computation of products c*xfor a given coefficient cmay be initiated as soon as (or soon after) the coefficient chas been computed. In other words, it is not necessary to wait until all the coefficients have been computed before initiating computation of the products c*x. Furthermore, the cumulative sum \n\n\nmay be initiated before all the coefficients have been computed. Thus, step  may operate in parallel (e.g. in a pipelined fashion) with step .\n","In step , the weighted sums for each pixel attribute may be divided by the normalization factor resulting in normalized pixel values as indicated in Equations 6\u20139 of .",{"@attributes":{"id":"p-0206","num":"0226"},"figref":["FIG. 27","FIG. 27"],"b":["2702","2710","2712","2708","170"]},{"@attributes":{"id":"p-0207","num":"0227"},"figref":["FIG. 28","FIG. 28"],"b":["2702","2702","2810","2820","2810","2702","2702"]},{"@attributes":{"id":"p-0208","num":"0228"},"figref":"FIG. 29","b":["2702","2702","2910","2920","2930","2930","2712"]},{"@attributes":{"id":"p-0209","num":"0229"},"figref":"FIGS. 28 and 29"},"As indicated above, each level of adder tree  comprises a set of adder cells.  illustrates one embodiment of an adder cell, i.e. adder cell , which conditionally adds input operands Xand Xdepending on the state of corresponding data valid inputs DV , and DV. Each of input operands Xand Xmay comprise an N-bit digital numeric value. In various embodiments, the parameter N may take any desired positive integer value. Adder cell  comprises multiplexors  and , adder , and OR gate . The output of multiplexor  equals the input operand XI when data valid DVis high (i.e. DV=1), and equals zero when data valid DVis low (i.e. DV=0). Similarly, the output of multiplexor  equals the input operand Xwhen data valid DVis high, and equals zero when data valid DVis low. Thus, the output of adder  equals one of zero, X, Xor X+Xdepending on the state of the data valid bits. In particular, each input operand is represented in the output value Xonly if its associated data valid signal is high. Adder cell  also generates a data valid output signal DV which is the logical OR of input data valid signal DV, and DV. See OR gate .","It is noted that a multiplexor may be realized by an appropriate combination of logic gates, i.e. AND gates and OR gates.","In one alternative embodiment, adder cell  may be modified to use a carry-save adder. In this configuration, adder cell  may receive three input operands, each with a corresponding data valid input signal, and to generate two output operands and a single data valid output signal (which indicates the validity of the two output operands). The data valid output signal may be the logical OR of the three data valid input signals.",{"@attributes":{"id":"p-0213","num":"0233"},"figref":"FIG. 30B","b":["2720","2714","2720","2722","2724","2726","2722","2724","2726","2720"],"sub":["1 ","2","1","2","1","2","1","2","out ","1","2 ","2 ","2 ","1","1","2 ","out ","1","2"]},"In one embodiment, adder tree  may be composed of adder cells as suggested by  and\/or . Each adder cell in layers after the top addition level  may receive the value Xand corresponding data valid signal DV from two adder cells of the previous level (through intervening buffer registers). The top addition level may receive a set of input data values and a corresponding set of data valid bits. Each data valid bit indicates the validity of a corresponding one of the input data values. Thus, adder tree  may generate a summation output from the bottom addition level  which selectively includes any combination of the input data values presented to the top addition level . In other words, only those input data values whose data valid bits are set are included in the summation output.","Any given adder cell and the adder cells from previous layers which feed the given adder cell comprise a subtree. The output Xfrom the given adder cell represents a summation of those input operands presented at the top layer of the subtree which are valid as indicated by their data valid signals. The data valid signal DV from the given adder cell is zero only if all the input operands presented to the top layer of the subtree have data valid signal equal to zero, i.e. all the subtree input operands are invalid.","Adder tree  may be used to generate the summation \n\n\nof filter coefficients and\/or any of the summations \n\n\nof the weighted sample values. In one embodiment, the top level  of the adder tree  may have a sufficient number of inputs to accommodate all the samples in a set of candidate bins, i.e. a set of bins which contain the filter support. For example,  illustrates a 5\u00d75 array of bins which minimally contain the filter support . Each input may carry a numeric operand and a corresponding data valid signal. The data valid signal indicates whether the corresponding numeric operand is to be included in the summation to be generated by adder tree .\n","Thus, for a summation of the filter coefficients (or any of the weighted sample values), the data valid bits corresponding to samples outside (inside) the filter support may be set to zero (one). In this fashion, only the terms corresponding to positions inside the filter support are incorporated in the summation output from the adder tree.","In some embodiments, graphics system  may include one or more sample request units. A sample request unit may request a group of samples from sample buffer  in anticipation of the next pixel computation (or next set of pixel computations) to be performed by a sample-to-pixel calculation unit. In one embodiment, the sample request unit may perform one or more positional tests on the received samples. For example, the sample request unit may determine the distance of the received samples with respect to a current filter center, and set the data valid bit for each sample to indicate whether the sample is interior or exterior to the current filter support. Each sample-to-pixel calculation unit may include, or couple to, one of the sample request units.","It is noted that sample-to-pixel calculation units  may be configured to turn off sample filtering, and to generate pixel values based on a winner-take-all criterion. In other words, the values of a current pixel may be determined based on a single sample referred to as the \u201cwinner-take-all\u201d (WTA) sample. For example, WTA sample may be an identified sample or the first sample in the bin corresponding to the current pixel. In an alternate embodiment, the sample closest to the current filter center may be the WTA sample as suggested by . Thus, the red, green, blue and alpha values of this closest sample are assigned as the attribute values of the current pixel.","A sample-to-pixel calculation unit may determine the closest sample by computing the distance (or square distance) of samples with respect to the filter center, and identifying the sample with the smallest sample distance (or square distance). It is noted that many of the bins which intersect the filter support may have no chance of contributing the closest sample. Thus, the sample-to-pixel calculation unit may increase the speed of the closest sample determination by searching a more efficient subset of bins. For example, the 3\u00d73 square of bins centered on the bin which contains the filter center may comprise a more efficient bin subset. It is noted that computation of the sample distance (or square distance) with respect to the filter center is included as part of the filter support inclusion-testing step  of . Thus, the closest sample determination described here may be integrated with step . In one embodiment, the sample request unit associated with each sample-to-pixel calculation unit may perform the closest sample determination.","In one set of embodiments, sample-to-pixel calculation units  may be configured to enable or disable sample filtering on a region-by-region basis. If the filter center for a current pixel resides in an unfiltered region of the sample space, a sample-to-pixel calculation unit may use a winner-take-all criterion as described above to determine pixel values. If the filter center resides in a filtered region of the sample space, the sample-to-pixel calculation unit may perform spatial filtering of sample values as described variously above to determine pixel values.",{"@attributes":{"id":"p-0222","num":"0242"},"figref":"FIG. 32","b":["2504","2506"]},"In one embodiment, the sample request unit associated with a sample-to-pixel calculation unit may test samples against region boundaries. For a filter center residing in a filtered region as suggested by , the sample request unit may examine each sample in a set of candidate bins (e.g. a rectangle of bins which contains the filter support) to determine if the sample falls inside the given filtered region. Any sample that resides inside the filtered region may have its data valid bit set to one. Any sample that falls outside the filtered region may have its data valid bit set to zero. This \u201cregion testing\u201d operation on samples may be performed in parallel with the testing to determine inclusion\/exclusion with respect to the filter support described above.","In another embodiment, each sample stored in the sample buffer may include a window ID. The sample request unit may read the window ID of each received sample and use the window ID to reference a window attribute table. The window attribute table stores a set of attributes\/parameters for a number of windows which are indexed by window ID value. One of the stored attributes for a window may be an indicator specifying whether the given window is to be treated as filtered or unfiltered for the sake of pixel computation. Thus, the sample request unit may label each sample as filtered or unfiltered by referencing the window attribute table using the sample's window ID.","Because a sample-to-pixel calculation unit may be called upon to perform summations (of coefficient values or weighted sample values) in filtered regions, and selection of values corresponding to the winner-take-all sample in unfiltered regions, it is desirable to have an adder tree which facilitates these dual modes.  illustrates one embodiment of an adder cell, i.e. adder cell , which is useful in implementing such a dual-mode adder tree. Adder cell  comprises OR gates , ,  and , multiplexors  and , and adder . Adder cell  receives two input operands Xand X, corresponding data valid signals DV, and DV, and corresponding winner-take-all signals WTAand WTA.","OR gate  generates the logical OR of data valid signal DV, and winner-take-all signal WTA. The output of OR gate  is the selection signal which controls multiplexor . Multiplexor  receives the input operand Xand a signal tied to zero. The output of multiplexor  equals zero when the selection signal is low, and equals input operand Xwhen the selection signal is high. Similarly, OR gate  generates the logical OR of data valid signal DVand winner-take-all signal WTA. The output of OR gate  is the selection signal which controls multiplexor . Multiplexor  receives the input operand Xand a signal tied to zero. The output of multiplexor  equals zero when the selection signal is low, and equals input operand Xwhen the selection signal is high. Adder  adds the outputs of multiplexors  and . Thus, the output Xof adder  obeys the input-output relation given by Table 1 below. It is assumed that at most one of the winner-take-all bits may be high. (In a field of input operands provided to the top layer of an adder tree, only the input operand corresponding to the winner-take-all sample will have its winner-take-all bit set.) In situations where the output is insensitive to the value of a particular input operand, the table entry for the input operand is symbolized by \u201cdc\u201d, i.e. \u201cdon't care\u201d.",{"@attributes":{"id":"p-0227","num":"0247"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 1"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Input-Output Relation for Adder Cell 2730"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"56pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["DV","DV","WTA","WTA","X"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}},{"entry":["dc","dc","1","0","X"]},{"entry":["dc","dc","0","1","X"]},{"entry":["0","0","0","0","0\u2002"]},{"entry":["0","1","0","0","X"]},{"entry":["1","0","0","0","X"]},{"entry":["1","1","0","0","X+X"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}}]}}},"Adder cell  also generates a data valid signal output DV and a winner-take all signal output WTA. The data valid output signal DV is the logical OR of the two data valid signal inputs. Similarly, the winner-take-all signal output WTAis the logical OR of the two winner-take-all input signals. In one alternative embodiment, adder cell  may be modified to use a carry-save adder instead of a carry-propagate adder. In this case, adder cell  may receive three input operands and generate two output operands.",{"@attributes":{"id":"p-0229","num":"0249"},"figref":"FIG. 33B","b":["2750","2730","2750","2752","2754","2760","2762","2756","2758","2752","2754","2752","2754","2758","2758","2758","2750","2750"],"sub":["1","1","2 ","2","1","2","1","2","out ","out ","out","out ","out "]},"In one embodiment, adder tree  may be composed of adder cells as suggested by  and\/or . Each adder cell in layers after the top addition level  may receive the value X, the corresponding data valid signal DV and the corresponding winner-take-all signal WTAfrom two adders cells of the previous level (through intervening buffer registers). The top addition level may receive a set of input data words. Each input data word comprises a numeric operand, a data valid bit and a winner-take-all bit. The data valid bit may indicate the validity of the corresponding numeric operand for inclusion in a summation. The winner-take-all bit may indicate whether the corresponding numeric operand corresponds to the winner-take-all sample in an unfiltered pixel computation. It is assumed that at most one of the input data words have a winner-take-all bit set (i.e. equal to one). Thus, this embodiment of adder tree  may generate an output from the bottom addition level  which equals either (a) a summation of selected ones of the input numeric operands when none of the winner-take-all bits presented at the top layer are set, or (b) the unique input numeric operand whose winner-take-all bit is set. In case (a), the summation includes all those numeric operands whose data valid bits are set (i.e. equal to one).","Any given adder cell and the adder cells from previous layers which feed the given adder cell comprise a subtree. The numeric operands presented to the subtree at the top layer are referred to as the subtree operands. The output Xfrom the given adder cell equals either (i) a summation of the valid subtree operands (i.e. those subtree operands which have data valid signals equal to one) when the winner-take-all bits of all the subtree operands are equal to zero, or (ii) the unique subtree operand whose winner-take all bit is set. The data valid output signal DVgenerated by the given adder cell equals the logical OR of the data valid bits of all the subtree operands. Similarly, the winner-take-all output signal WTAgenerated by the given adder equals the logical OR of the winner-take-all bits of all the subtree operands.",{"@attributes":{"id":"p-0232","num":"0252"},"figref":"FIG. 33C","b":["3002","3002","3004","3006","3008","2702"]},"Given a collection of numeric operands and their corresponding data valid bits and winner-take-all bits presented as input to the adder tree , it is to be noted that the setting of any one of the winner-take-bits makes the adder tree output insensitive to all the data valid bits. Thus, when a pixel is to be computed by the unfiltered winner-take-all criterion, it is not necessary to zero the data valid bits. Simply setting the winner-take-all bit of the numeric operand corresponding to the winner-take-all sample guarantees that the adder tree output will perform the winner-take-all selection. Thus, in some embodiments, the mechanism responsible for determining the data valid bits may run autonomously, i.e. without regard to whether the current pixel is filtered or unfiltered.","As described above, the graphics system  may be configured to generate super-samples in response to 3D graphics data, and to compute output pixel values based on sample filtering or based on an unfiltered \u201cwinner-take-all\u201d criterion. In another embodiment, graphics system  (e.g. rendering units ) may be configured to turn off super-sampling, and thus, to generate one sample per unit pixel area. This mode will be referred to as the critical sampling mode. For example, certain users\/customers may want the displayed video output to replicate the appearance of a previous generation video system which may have generated pixel values from 3D graphics primitives without intervening super-sampling and super-sample filtering. Thus, it may be desirable to determine pixel values by the unfiltered \u201cwinner-take-all\u201d criterion during the critical sampling mode. However, it is noted that sample-to-pixel calculation units  are equally capable of performing sample filtering to determine pixel values in the critical sampling mode as in the super-sampling mode.","The present invention contemplates a variety of winner-take-all criteria for selecting the single sample whose sample attributes become the attributes for the current pixel. In one embodiment, the selected sample is simply the first sample (or Ksample where K is a predetermined constant) of the bin which contains the current pixel center. In critical sampling mode, the rendering units  may store the single sample generated for each output pixel in the first sample position of the corresponding bin. In a second embodiment, the selected sample is the sample closest to the pixel center in terms of some (not necessarily Euclidean) distance measure. In a third embodiment, the selected sample is a random (i.e. arbitrary) one of the samples in the bin which contains the current pixel center.","In one embodiment, the critical sampling mode may be invoked when graphics data is received from legacy APIs such as X-Windows.","In order to increase performance, a graphic systems (e.g. a sample-to-pixel calculation unit) may introduce a new summation to the adder tree before a previously introduced summation has completely propagated through the adder tree thereby pipelining summations. For example, the summation data for a first attribute (e.g. red) may be introduced first to the adder tree. As described above, the first stage of the summation process occurs at the top (first) addition level  of the adder tree . After the partial summations associated with the first attribute have propagated beyond the top addition level, the summation data for a second attribute (e.g., green) may be introduced to the top addition level. After the partial summations associated with the second attribute have propagated through the first addition level, the summation data for a third attribute (e.g. blue) may be presented to the first addition level.","In general, the summation of another attribute may begin after the previous attribute has completed at least the first addition level. In one embodiment, multiple summations of different attributes may be occurring simultaneously in a pipelined fashion through the adder tree. In another embodiment, summations corresponding to different pixels may also be sequentially pipelined into the adder tree after summations corresponding to one pixel have been introduced thereby increasing the utilization of the adder tree. Typically, a different summation may be loaded into an adder tree on every n clock cycles, where n is a positive integer. This period of time, i.e. the n clock cycles, is referred to herein as an adder cycle. However, in certain cases, for one or more adder cycles, no valid data may be introduced into the adder trees, e.g., in cases where the intrinsic rate of the adder tree (i.e. one divided by the adder cycle period) is greater than the product of the pixel output rate times the number of summations per pixel.",{"@attributes":{"id":"p-0239","num":"0259"},"figref":"FIG. 34","b":["3100","3102","3104","3104","3104","3104","3104"],"sub":["1 ","2 ","1 ","2","1 ","2","1 ","2 ","1 ","2","1","2","1 ","1","2 ","2","1 ","2 "]},"In one embodiment, the summation operations for a pixel may be performed using a single adder tree, thereby reducing the hardware cost. However, in such an embodiment, the time required to perform the summations for the pixel attributes and the sum of the filter coefficients may be longer than desired. For example, if each addition level requires n clock cycles for completion, at least (L+K+1)n clock cycles may be required to output the results corresponding to one pixel, where L is the number of layers in the adder tree, K is the number attribute summations to be performed per pixel. Note that the total number of summations to be performed per pixel is (K+1) since each pixel also includes a coefficient summation in addition to the K attribute summations.","In another embodiment, (K+1) adder trees may be used, i.e. one adder tree for each of the (K+1) summations to be performed per pixel. Suppose that the latency of each adder tree (from input to output) is m clock cycles, and that the data for each summation is presented simultaneously to a corresponding one of the adder trees. In this case, the results corresponding to one pixel may be output in Ln clock cycles (i.e. each of the K+1 adder trees may simultaneously output a corresponding sum of one of the attributes or sum of the coefficients). Thus, a decrease in processing time per pixel may be purchased at the expense of additional hardware.","In another embodiment, an intermediate number of adder trees may be used, i.e. a number between 1 and (K+1). The (K+1) summations may be partitioned into two or more groups. A first group of summations may be introduced to the available adder trees. When the first group of summations have propagated beyond at least the first layer of each adder tree, a second group of summations may be introduced to the available adder trees, and so on. The number of adder trees and the number of groups may be chosen in such a way as to balance the cost of the hardware and the time it takes to perform the required summations.",{"@attributes":{"id":"p-0243","num":"0263"},"figref":"FIG. 35","b":["3202","3204","3206","3206"]},"The first cycle of the next pixel may follow the second cycle of the current pixel. In other words, the filter-coefficient, red and green summations of the next pixel may be initiated as soon as the blue and alpha summations of the current pixel propagate beyond at least the first layer of the adder trees.","In other embodiments, different groupings of summations for the different cycles may also be implemented. Assuming the filter coefficients are not pre-normalized, the computed attribute summations may be normalized after the completion of each summation. The normalization may be accomplished by dividing the computed attribute summations by the sum of the filter coefficients (i.e. the normalization factor). Thus, it may be advantageous for the sum of the filter coefficients and as many other attribute values as possible to be included in the first round of computations performed by the adder trees.","It is noted that the division of the attribute summations by the normalization factor may be implemented by (a) computing the reciprocal of the normalization factor and (b) multiplying the reciprocal value by the attribute summations. A single reciprocal and Nmultiplications is less expensive computationally than Ndivisions, where Nis the number of attribute summations.","Computing the value for each filter coefficient may be performed using a look-up table. Given a particular filter function, a look-up table may be generated using distance of the sample from the filter center as an index to look up corresponding values for the filter coefficients. Since not all distances may be represented in such a discrete representation, the distance may be sampled with a higher resolution for small distances and lower resolution for higher distances. Filter coefficients for distances that are not represented in the look-up table may be determined by interpolation. In one embodiment, a simple linear (or polynomial) interpolation may be used.","Furthermore, to save computational power, in one embodiment, the square of the distance of the sample from the filter center may be used to index into the look-up table. The sample space may be visualized as a 2D space since the sample space corresponds to the 2D screen space. Therefore, in order to compute the distance between two points in 2D where the points are represented by Cartesian x and y coordinates, the Pythagorean theorem may be used as set forth in Equation 10 of .","In order to avoid computing the square root every time the distance is computed, the lookup table may be indexed by square-distance instead of distance. Thus, the computation may then be limited as set forth in Equation 11 of .","As previously noted, when the filter support covers regions of two or more different sample densities, the samples from the lower density regions may contribute less to the filtering than samples from the higher density region. An example of a filter partially covering regions of different sample densities is shown in . In one embodiment, to compensate for this, the filter coefficients corresponding to samples from the region of lower sample density may be multiplied by a factor approximately equal to the ratio of the high density to the low density. For example, if the high density is 16 samples\/pixel and the low density is 4 samples\/pixel, the filter coefficients in the region of lower sample density may be multiplied by \n\n\nThis provides more weight to the less-represented samples from the region of lower density. In cases where the filter support may include more than two regions of different sample density, filter coefficients for samples in a lower density region may also be multiplied by a factor equal to the ratio of the high density to the sample density of that lower density region. For example, if a third region is included in the support of the filter with a density of 2 samples\/pixel, the filter coefficients for that region may be multiplied by \n\n","In another embodiment, as the sample density decreases, the diameter of the filter may increase in order to keep the number of samples included in the filtering approximately constant. In an embodiment where the filter is circularly symmetric, the square of the diameter of the support of the filter may be inversely proportional to the density of the samples in that region, i.e., \n\n\nwhere d is the diameter of the support of the filter and \u03c1 is the density of the samples. That is, when the density of the samples decreases by a factor of 4, the diameter approximately increases by a factor of 2. For example, if the diameter of the support of the filter is 5 pixels at a region where the density is 16 samples\/pixel, the diameter may be 10 pixels at region where the density is 4 samples\/pixel.\n","Although the embodiments above have been described in considerable detail, other versions are possible. Numerous variations and modifications will become apparent to those skilled in the art once the above disclosure is fully appreciated. It is intended that the following claims be interpreted to embrace all such variations and modifications. Note that the headings used herein are for organizational purposes only and are not meant to limit the description provided herein or the claims attached hereto."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"IV. BRIEF DESCRIPTION OF THE DRAWINGS","p":["The foregoing, as well as other objects, features, and advantages of this invention may be more completely understood by reference to the following detailed description when read together with the accompanying drawings in which:",{"@attributes":{"id":"p-0030","num":"0032"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0031","num":"0033"},"figref":["FIG. 2","FIG. 1"]},{"@attributes":{"id":"p-0032","num":"0034"},"figref":["FIG. 3","FIG. 1"]},{"@attributes":{"id":"p-0033","num":"0035"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0034","num":"0036"},"figref":"FIG. 5A"},{"@attributes":{"id":"p-0035","num":"0037"},"figref":"FIG. 5B"},{"@attributes":{"id":"p-0036","num":"0038"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0037","num":"0039"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0038","num":"0040"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0039","num":"0041"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0040","num":"0042"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0041","num":"0043"},"figref":"FIG. 11A"},{"@attributes":{"id":"p-0042","num":"0044"},"figref":["FIG. 11B","FIG. 11A"]},{"@attributes":{"id":"p-0043","num":"0045"},"figref":"FIG. 11C"},{"@attributes":{"id":"p-0044","num":"0046"},"figref":"FIG. 12A"},{"@attributes":{"id":"p-0045","num":"0047"},"figref":"FIG. 12B"},{"@attributes":{"id":"p-0046","num":"0048"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0047","num":"0049"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0048","num":"0050"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0049","num":"0051"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0050","num":"0052"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0051","num":"0053"},"figref":"FIGS. 18A\u2013B"},{"@attributes":{"id":"p-0052","num":"0054"},"figref":"FIGS. 19A\u2013B"},{"@attributes":{"id":"p-0053","num":"0055"},"figref":"FIG. 20"},{"@attributes":{"id":"p-0054","num":"0056"},"figref":"FIG. 21"},{"@attributes":{"id":"p-0055","num":"0057"},"figref":"FIGS. 22A\u2013B"},{"@attributes":{"id":"p-0056","num":"0058"},"figref":"FIGS. 23A\u2013B"},{"@attributes":{"id":"p-0057","num":"0059"},"figref":"FIGS. 24A\u2013B"},{"@attributes":{"id":"p-0058","num":"0060"},"figref":"FIGS. 25A\u2013B"},{"@attributes":{"id":"p-0059","num":"0061"},"figref":"FIG. 26"},{"@attributes":{"id":"p-0060","num":"0062"},"figref":"FIG. 27"},{"@attributes":{"id":"p-0061","num":"0063"},"figref":"FIG. 28"},{"@attributes":{"id":"p-0062","num":"0064"},"figref":"FIG. 29"},{"@attributes":{"id":"p-0063","num":"0065"},"figref":"FIG. 30A"},{"@attributes":{"id":"p-0064","num":"0066"},"figref":"FIG. 30B"},{"@attributes":{"id":"p-0065","num":"0067"},"figref":"FIG. 31"},{"@attributes":{"id":"p-0066","num":"0068"},"figref":"FIG. 32"},{"@attributes":{"id":"p-0067","num":"0069"},"figref":"FIG. 33A"},{"@attributes":{"id":"p-0068","num":"0070"},"figref":"FIG. 33B"},{"@attributes":{"id":"p-0069","num":"0071"},"figref":"FIG. 33C"},{"@attributes":{"id":"p-0070","num":"0072"},"figref":"FIG. 34"},{"@attributes":{"id":"p-0071","num":"0073"},"figref":"FIG. 35"},{"@attributes":{"id":"p-0072","num":"0074"},"figref":"FIG. 36"}]},"DETDESC":[{},{}]}
