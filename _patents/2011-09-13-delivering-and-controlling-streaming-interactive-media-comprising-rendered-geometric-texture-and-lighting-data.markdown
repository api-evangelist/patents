---
title: Delivering and controlling streaming interactive media comprising rendered geometric, texture and lighting data
abstract: In an exemplary embodiment, a method, conducted on a server, sends renderable graphics information to a client device, said graphics information including at least one set of graphics information visible from a second view region and not visible from a first view region. The method includes determining a likelihood that a viewpoint undergoes movement from said first view region to said second view region. The method further includes sending said at least one set of graphics information upon determination that the likelihood that the viewpoint undergoes movement from said first view region to said second view region is greater than a predetermined threshold, said movement determined according to a predetermined motion path.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09489762&OS=09489762&RS=09489762
owner: PRIMAL SPACE SYSTEMS, INC.
number: 09489762
owner_city: Raleigh
owner_country: US
publication_date: 20110913
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS REFERENCE TO RELATED APPLICATIONS","BACKGROUND OF THE INVENTION","SUMMARY","DETAILED DESCRIPTION OF THE EMBODIMENTS"],"p":["This application claims the benefit of the earlier filing date of PCT patent application number PCT\/US2011\/042309 entitled \u201cSystem and Method of From-Region Visibility Determination and Delta-PVS based Content Streaming Using Conservative Linearized Umbral Event Surfaces\u201d and filed on Jun. 29, 2011, which claims the benefit of the earlier filing date of U.S. Provisional Application 61\/360,283, filed on Jun. 30, 2010, the entirety of each of which is incorporated herein by reference. This application claims the benefit of the earlier filing date of U.S. Provisional Application 61\/382,056 entitled \u201cSystem and Method of Delivering and Controlling Streaming Interactive Media Comprising Predetermined Packets of Geometric, Texture, Lighting and Other Data Which are Rendered on a Receiving Device\u201d and filed on Sep. 13, 2010, the entirety of which is incorporated herein by reference. This application further claims the benefit of the earlier filing date of U.S. Provisional Application 61\/384,284 entitled \u201cSystem and Method of Recording and Using Clickable Advertisements Delivered as Streaming Interactive Media\u201d and filed on Sep. 19, 2010, the entirety of which is incorporated herein by reference. This application further claims the benefit of the earlier filing date of U.S. Provisional Application 61\/452,330 entitled \u201cSystem and Method of Controlling Visibility-Based Geometry and Texture Streaming for Interactive Content Delivery\u201d and filed on Mar. 14, 2011, the entirety of which is incorporated herein by reference. This application further claims the benefit of the earlier filing date of U.S. Provisional Application 61\/474,491 entitled \u201cSystem and Method of Protecting Game Engine Data Formats and Visibility Event Codec Formats Employing an Application Programming Interface Between the Game Engine and the Codec\u201d and filed on Apr. 12, 2011, the entirety of which is incorporated herein by reference. This application also claims the benefit of the earlier filing date of U.S. Provisional Application 61\/476,819 entitled \u201cSystem and Method of Delivering Targeted, Clickable, Opt-Out or Opt-in Advertising as a Unique, Visibility Event Stream for Games and Streaming Interactive Media\u201d and filed on Apr. 19, 2011, the entirety of which is incorporated herein by reference.","Field of the Invention","This invention relates to a method and system for sending and controlling streaming interactive content comprising prefetched packets of geometric, lighting, and other information which is rendered by a receiving device.","Description of Background","The method of transmitting interactive content as single-frame packets of geometry, texture, lighting and other data which is rendered in real-time on a client or receiver device is described in U.S. Pat. Nos. 6,057,847 and 6,111,582. In these methods the single-frame packets are generated in real-time using the method of primitive reprojection.","The method of transmitting interactive content as prefetched packets of geometry, texture, and other information, in which the prefetched packets correspond to delta-PVS (potentially visible set), also called visibility event information, is described in the PCT patent application number PCT\/US2011\/042309, the entire contents of which are incorporated herein by reference. In this method, the packets contain information representing newly visible graphic elements as well as information indicating newly occluded graphic elements. This information is organized as delta-PVS or visibility event packets, wherein each packet contains information reflecting graphics elements that become potentially newly visible or newly invisible as a viewpoint moves from one viewcell to another. In this method, the packets are not generated in real-time but instead pre-computed using a method of from-viewcell visibility precomputation, and related methods of compression to encode the packets. At runtime, the packets are prefetched to the client rendering device using navigation-based speculative prefetch driven by the viewer's current and predicted viewpoint location in a set of viewcells that represent navigable regions of a model. Together, the methods of PCT patent application number PCT\/US2011\/042309 comprise an efficient codec for the streaming delivery of interactive content such as video games.","The codec of PCT patent application number PCT\/US2011\/042309 can also deliver content that is not necessarily interactive. In this use of the codec, camera position and direction is not interactively controlled by the user, but is determined by one or more scripted camera motion paths. This approach allows the codec to deliver scripted, non-interactive content which, of course, may include animated characters and other scripted dynamic effects. Applied in this way, the codec streams content that effectively corresponds to a conventional, non-interactive animated sequence. As detailed in PCT patent application number PCT\/US2011\/042309, this codec can deliver the information used to generate the corresponding image sequence at very high resolutions using a lower bandwidth to directly transmitting the image sequence using image-based codecs such as MPEG-4 or the real-time image compression method used by the OnLive corporation.","Broadband digital service providers such as cable and satellite television companies constantly seek ways to increase the amount of content that they can deliver using the bandwidth that is available. The codec of PCT patent application number PCT\/US2011\/042309, when delivering a unidirectional data stream driven by a scripted camera motion path as previously described, can be used to deliver the same compelling story, visual, and audio experience as conventional high-def video, but at a lower bandwidth than video.","Most adults do not play modern shooter or action games such as Call of Duty Modern Warfare 2, Assassin's Creed, Crysis, Bioshock 2 etc. Yet these types of games can often provide a compelling story, visual and audio experience that focus groups of older adults enjoy watching. But in their pure game form, this content often demands too much interaction and can be tiring to play to completion or even to the next \u201clevel\u201d or episode of the game.","Thus, there is a need for an efficient content delivery method which provides conventional, non-interactive or passive high-definition video experience while providing viewers the option of switching between the passive video-like experience and an interactive game-like experience on-demand.","Prefetch is a general method of data delivery which hides latency or improves hit rate by pre-sending data to a client unit before it is actually needed by the client unit. As specified in the co-pending application PCT patent application number PCT\/US2011\/042309, in the context of visibility event data streaming, specific prefetch methods are used to decrease the chance of late packet arrival which would otherwise cause noticeable visibility errors during the streamed visualization.","In general, regardless of the application, prefetch increases the amount of data streamed, since inevitably more data is delivered to a client unit than will actually be used. Rules and algorithms for optimal prefetching and caching in general applications are given by Pei Cao, (Cao, P, Application-Controlled File Caching and Prefetching. PhD Dissertation, Princeton University, January, 1996, the entire contents of which are incorporated herein by reference.) Cao's aggressive and conservative prefetch algorithms provide methods of minimizing the elapsed time of a known sequence of references.","Wu et al. (Wu, B., Kshemkalyani, D., Object-Greedy Algorithms for Long-Term Web Prefetching, Proceedings of the IEEE Conference on Network Computing and Applications, 2004 pp. 61-68, the entire contents of which are incorporated herein by reference) has developed an algorithm which provides a local solution for prefetching, which balances the increased hit rate benefit of prefetching with the cost of increased bandwidth consumption incurred by prefetch of long-term general-purpose web page content.","In 2003, Chim et al. (Jimmy Chim, Rynson W. H. Lau, Hong Va Leong, and Antonio Si, CyberWalk: A Web-Based Distributed Virtual Walkthrough Environment, iEEE TRANSACTIONS ON MULTIMEDIA, VOL. 5, NO. 4, DECEMBER 2003, the entire contents of which are incorporated herein by reference) described basic methods of prefetching graphics elements during a walkthrough application. The Cyberwalk system of Chim et al. prefetches graphics objects based on a viewer's proximity to the objects. Chim et al. describe three methods of predicting a user's future location given past current locations. These methods are based on a moving average of past locations and include mean, windowed, and exponential weighted moving averages of past motion vectors. These motion prediction methods do not explicitly take into account limits of velocity or acceleration of viewer motion, nor do they account for navigational restrictions within the environment. Both of these types of limitations could be employed to increase the predictability of navigation and thereby increase the hit rate and\/or lower the bandwidth requirement of navigation-based prefetch.","The prefetch method of Chim et al. are based on proximity of the viewer to the graphics objects. As shown in the co-pending PCT patent application number PCT\/US2011\/042309, visibility is a more efficient basis of navigation-based prefetch than proximity. This is because objects which are in close proximity to a viewer may actually be occluded from the viewer and, conversely, objects that are far from a viewer may be unoccluded. Using visibility instead of proximity as a basis for prefetch requires a practical and precise method of from-region visibility precomputation, which is specified in the co-pending PCT patent application number PCT\/US2011\/042309.","In exemplary embodiments, a method, conducted on a server, sends renderable graphics information to a client device, the graphics information including at least one set of graphics information visible from a second view region and not visible from a first view region. The method determines a likelihood that a viewpoint undergoes movement from said first view region to said second view region. The method further sends said at least one set of graphics information upon determination that the likelihood that the viewpoint undergoes movement from said first view region to said second view region is greater than a predetermined threshold, said movement determined according to a predetermined motion path.","In exemplary embodiments, a system sends renderable graphics information to a client device, said graphics information including at least one set of graphics information visible from a second view region and not visible from a first view region. The system includes a server having a processor configured to determine a likelihood that a viewpoint undergoes movement from said first view region to said second view region. The processor is further configured to send said at least one set of graphics information upon determination that the likelihood that the viewpoint undergoes movement from said first view region to said second view region is greater than a predetermined threshold, said movement determined according to a predetermined motion path. The system further includes said client device to display said at least one set of graphics information.","In exemplary embodiments, a non-transitory computer readable storage medium has executable instructions stored thereon, which when executed by a processor in a server causes the processor to execute a method for sending renderable graphics information to a client device, said graphics information including at least one set of graphics information visible from a second view region and not visible from a first view region. The method includes determining a likelihood that a viewpoint undergoes movement from said first view region to said second view region. The method further includes sending said at least one set of graphics information upon determination that the likelihood that the viewpoint undergoes movement from said first view region to said second view region is greater than a predetermined threshold, said movement determined according to a predetermined motion path.","In exemplary embodiments, a method, conducted on a server, sends renderable graphics information to a client device, said graphics information including at least one set of graphics information visible from a second view region and not visible from a first view region. The method includes determining a likelihood that a viewpoint undergoes movement from said first view region to said second view region. The method further includes sending said at least one set of graphics information upon determination that the likelihood that the viewpoint undergoes movement from said first view region to said second view region is greater than a predetermined threshold, said movement determined according to user input to said client device.","In exemplary embodiments, a system sends renderable graphics information to a client device, said graphics information including at least one set of graphics information visible from a second view region and not visible from a first view region. The system includes a server having a processor configured to determine a likelihood that a viewpoint undergoes movement from said first view region to said second view region. The processor is further configured to send said at least one set of graphics information upon determination that the likelihood that the viewpoint undergoes movement from said first view region to said second view region is greater than a predetermined threshold, said movement determined according to user input to said client device. The system further includes said client device to display said at least one set of graphics information.","In exemplary embodiments, a non-transitory computer readable storage medium has executable instructions stored thereon, which when executed by a processor in a server causes said processor to execute a method for sending renderable graphics information to a client device, said graphics information including at least one set of graphics information visible from a second view region and not visible from a first view region. The method includes determining a likelihood that a viewpoint undergoes movement from said first view region to said second view region. The method further includes sending said at least one set of graphics information upon determination that the likelihood that the viewpoint undergoes movement from said first view region to said second view region is greater than a predetermined threshold, said movement determined according to user input to said client device.","In exemplary embodiments, a method, conducted on a server, sends renderable graphics information to a client device, said graphics information including a first set of graphics information visible from a second view region and not visible from a first view region, and said graphics information including a second set of graphics information visible from said second view region and not visible from said first view region, said second set of graphics information including graphics information representing advertising. The method includes sending said first set of graphics information upon determination that a likelihood that a viewpoint moves from said first view region to said second view region is greater than a predetermined movement threshold, said movement determined according to a predetermined motion path. The method further includes sending said second set of graphics information upon determination that the likelihood that the viewpoint moves from said first view region to said second view region is greater than the predetermined movement threshold, said movement determined according to the predetermined motion path, and upon determination that a maximum available transmission bandwidth from said server to said client device exceeds a predetermined bandwidth threshold.","In exemplary embodiments, a system sends renderable graphics information to a client device, said graphics information including a first set of graphics information visible from a second view region and not visible from a first view region, and said graphics information including a second set of graphics information visible from said second view region and not visible from said first view region, said second set of graphics information including graphics information representing advertising. The system includes a server having a processor configured to send said first set of graphics information upon determination that a likelihood that a viewpoint moves from said first view region to said second view region is greater than a predetermined movement threshold, said movement determined according to a predetermined motion path. The processor is further configured to send said second set of graphics information upon determination that the likelihood that the viewpoint moves from said first view region to said second view region is greater than the predetermined movement threshold, said movement determined according to the predetermined motion path, and upon determination that a maximum available transmission bandwidth from said server to said client device exceeds a predetermined bandwidth threshold. The system further includes said client device configured to display said first set of graphics information or said second set of graphics information.","In exemplary embodiments, a non-transitory computer readable storage medium has executable instructions stored thereon, which when executed by a processor in a server causes said processor to execute a method for sending renderable graphics information to a client device, said graphics information including a first set of graphics information visible from a second view region and not visible from a first view region, and said graphics information including a second set of graphics information visible from said second view region and not visible from said first view region, said second set of graphics information including graphics information representing advertising. The method includes sending said first set of graphics information upon determination that a likelihood that a viewpoint moves from said first view region to said second view region is greater than a predetermined movement threshold, said movement determined according to a predetermined motion path. The method further includes sending said second set of graphics information upon determination that the likelihood that the viewpoint moves from said first view region to said second view region is greater than the predetermined movement threshold, said movement determined according to the predetermined motion path, and upon determination that a maximum available transmission bandwidth from said server to said client device exceeds a predetermined bandwidth threshold.","In exemplary embodiments, a method, conducted on a server, sends renderable graphics information representing an actual environment as a modeled environment to a client device, said graphics information including at least one first set of graphics information visible from a second view region and not visible from a first view region. The method includes receiving data, said data including information conveying position or velocity of said client device. The method further includes sending said at least one first set of graphics information to the client device upon determination that a likelihood that said position changes from said first view region to said second view region is greater than a predetermined movement threshold.","In exemplary embodiments, a system to send renderable graphics information representing an actual environment as a modeled environment to a client device, said graphics information including at least one first set of graphics information visible from a second view region and not visible from a first view region. The system includes a server having a processor configured to receive data, said data including information conveying position or velocity of said client device. The processor is further configured to send said at least one first set of graphics information to the client device upon determination that a likelihood that said position changes from said first view region to said second view region is greater than a predetermined movement threshold. The system further includes said client device configured to display said at least one first set of graphics information.","In exemplary embodiments, a non-transitory computer readable storage medium having executable instructions stored thereon, which when executed by a processor in a server causes the processor to perform a method for sending renderable graphics information representing an actual environment as a modeled environment to a client device, said graphics information including at least one first set of graphics information visible from a second view region and not visible from a first view region. The method includes receiving data, said data including information conveying position or velocity of said client device. The method further includes sending said at least one first set of graphics information to the client device upon determination that a likelihood that said position changes from said first view region to said second view region is greater than a predetermined movement threshold.","In exemplary embodiments, a method, conducted on a server, of sending renderable graphics information to a client device, said graphics information including at least one first set of graphics information visible from a second view region and not visible from a first view region, and at least one second set of graphics information visible from a fourth view region and not visible from a third view region. The method includes determining a first probability that a viewpoint changes position from said first view region to said second view region using data received from said client device, said received data including user input to said client device, said user input controlling the viewpoint position of said client device. The method further includes determining a second probability that the viewpoint changes position from said third view region to said fourth view region, using data received from said client device, said received data including user input to said client device, said user input controlling said viewpoint position of said client device. The method further includes sending said at least one first set of graphics information at a first rate, said first rate determined as a function of said first probability. The method also includes sending said at least one second set of graphics information at a second rate, said second rate determined as a function of said second probability.","In exemplary embodiments, a system to send renderable graphics information to a client device, said graphics information including at least one first set of graphics information visible from a second view region and not visible from a first view region, and at least one second set of graphics information visible from a fourth view region and not visible from a third view region. The system includes a server having a processor configured to determine a first probability that a viewpoint changes position from said first view region to said second view region using data received from said client device, said received data including user input to said client device, said user input controlling the viewpoint position of said client device. The processor is further configured to determine a second probability that the viewpoint changes position from said third view region to said fourth view region, using data received from said client device, said received data including user input to said client device, said user input controlling said viewpoint position of said client device. The processor is further configured to send said at least one first set of graphics information at a first rate, said first rate determined as a function of said first probability. The processor also is further configured to send said at least one second set of graphics information at a second rate, said second rate determined as a function of said second probability. The system further includes said client device configured to display said at least one first set of graphics information or said at least one second set of graphics information.","A non-transitory computer readable storage medium having instructions stored thereon, which when executed by a processor in a server causes the processor to perform a method for sending renderable graphics information to a client device, said graphics information including at least one first set of graphics information visible from a second view region and not visible from a first view region, and at least one second set of graphics information visible from a fourth view region and not visible from a third view region. The method includes determining a first probability that a viewpoint changes position from said first view region to said second view region using data received from said client device, said received data including user input to said client device, said user input controlling the viewpoint position of said client device. The method further includes determining a second probability that the viewpoint changes position from said third view region to said fourth view region, using data received from said client device, said received data including user input to said client device, said user input controlling said viewpoint position of said client device. The method further includes sending said at least one first set of graphics information at a first rate, said first rate determined as a function of said first probability. The method also includes sending said at least one second set of graphics information at a second rate, said second rate determined as a function of said second probability.","In exemplary embodiments, a method, conducted on a server, sends graphics information representing, as a modeled environment, an actual environment to a client device, said graphics information including at least one first set of graphics information visible from a second view region and not visible from a first view region. The method includes receiving data including information conveying a position or velocity of said client device, said received data determined using three dimensional map-matching, said three dimensional map-matching using said graphics information representing, as said modeled environment, said actual environment. The method further includes sending said at least one first set of graphics information to the client device upon determination that a likelihood that said position changes from said first view region to said second view region is greater than a predetermined movement threshold.","In exemplary embodiments, a system to send graphics information representing, as a modeled environment, an actual environment to a client device, said graphics information including at least one first set of graphics information visible from a second view region and not visible from a first view region. The system includes a server having a processor configured to receive data including information conveying a position or velocity of said client device, said received data determined using three dimensional map-matching, said three dimensional map-matching using said graphics information representing, as said modeled environment, said actual environment. The processor is further configured to send said at least one first set of graphics information to the client device upon determination that a likelihood that said position changes from said first view region to said second view region is greater than a predetermined movement threshold. The system further includes said client device configured to display said at least one first set of graphics information.","In exemplary embodiments, a non-transitory computer readable storage medium has executable instructions stored thereon, which when executed by a processor in a server causes the processor to perform a method for sending graphics information representing, as a modeled environment, an actual environment to a client device, said graphics information including at least one first set of graphics information visible from a second view region and not visible from a first view region. The method includes receiving data including information conveying a position or velocity of said client device, said received data determined using three dimensional map-matching, said three dimensional map-matching using said graphics information representing, as said modeled environment, said actual environment. The method further includes sending said at least one first set of graphics information to the client device upon determination that a likelihood that said position changes from said first view region to said second view region is greater than a predetermined movement threshold.","In exemplary embodiments, a method, conducted on a server, for controlling navigation-based pre-fetch sending of graphics information to a client device, said graphics information including at least one first set of graphics information visible from a second view region and not visible from a first view region. The method includes determining a first period during which said first set of graphics information will arrive after said client device is scheduled to access said first set of graphics information on said client device. The method further includes sending instructions to said client device which prevent navigation into said second view region during said first period.","In exemplary embodiments, a system to control navigation-based pre-fetch sending of graphics information to a client device, said graphics information including at least one first set of graphics information visible from a second view region and not visible from a first view region. The system includes a server having a processor configured to determine a first period during which said first set of graphics information will arrive after said client device is scheduled to access said first set of graphics information on said client device. The processor is further configured to send instructions to said client device which prevent navigation into said second view region during said first period. The system further includes said client device to receive said instructions.","In exemplary embodiments, a non-transitory computer readable storage medium having executable instructions stored thereon, which when executed by a processor in a server causes the processor to perform a method for controlling navigation-based pre-fetch sending of graphics information to a client device, said graphics information including at least one first set of graphics information visible from a second view region and not visible from a first view region. The method includes determining a first period during which said first set of graphics information will arrive after said client device is scheduled to access said first set of graphics information on said client device. The method further includes sending instructions to said client device which prevent navigation into said second view region during said first period.","In exemplary embodiments, a method, conducted on a server, for controlling navigation-based pre-fetch sending of graphics information to a client device, said graphics information including a first set of graphics information visible from a second view region and not visible from a first view region, and said graphics information including a second set of graphics information visible from said second view region and not visible from said first view region, said second set of graphics information having a lower level-of-detail than said first set of graphics information. The method includes determining a first period during which said first set of graphics information will arrive after said client device is scheduled to access said first set of graphics information on said client device. The method further includes sending said second set of graphics information during said first period. The method also includes sending instructions to said client device, during said first period, said instructions including instructions for causing rendering of at least one obscuring object in viewport regions containing the projection of said second set of graphics information.","In exemplary embodiments, a system to control navigation-based pre-fetch sending of graphics information to a client device, said graphics information including a first set of graphics information visible from a second view region and not visible from a first view region, and said graphics information including a second set of graphics information visible from said second view region and not visible from said first view region, said second set of graphics information having a lower level-of-detail than said first set of graphics information. The system further includes a server having a processor configured to determine a first period during which said first set of graphics information will arrive after said client device is scheduled to access said first set of graphics information on said client device. The processor is further configured to send said second set of graphics information during said first period. The processor is further configured to send instructions to said client device, during said first period, said instructions including instructions for causing rendering of at least one obscuring object in viewport regions containing the projection of said second set of graphics information. The system further includes said client device configured to display said second set of graphics and receive said instructions.","In exemplary embodiments, a non-transitory computer readable storage medium having executable instructions stored thereon, which when executed by a processor in a server causes the processor to perform a method for controlling navigation-based pre-fetch sending of graphics information to a client device, said graphics information including a first set of graphics information visible from a second view region and not visible from a first view region, and said graphics information including a second set of graphics information visible from said second view region and not visible from said first view region, said second set of graphics information having a lower level-of-detail than said first set of graphics information. The method includes determining a first period during which said first set of graphics information will arrive after said client device is scheduled to access said first set of graphics information on said client device. The method further includes sending said second set of graphics information during said first period. The method also includes sending instructions to said client device, during said first period, said instructions including instructions for causing rendering of at least one obscuring object in viewport regions containing the projection of said second set of graphics information.","In exemplary embodiments, a method, conducted on a server, of sending renderable graphics information to a client device, said graphics information including at least one first set of graphics information visible from a second view region and not visible from a first view region, and at least one second set of graphics information visible from a third view region and not visible from said first view region. The method includes determining a first probability that a viewpoint changes position from said first view region to said second view region using data received from said client device, said received data including user input controlling the viewpoint position. The method further includes determining a second probability that the viewpoint changes position from said first view region to said third view region using data received from said client device, said received data including user input controlling the viewpoint position. The method further includes increasing said first probability as a function of a distance of said second view region to a defined navigational path. The method further includes increasing said second probability as a function of a distance of said third view region to a defined navigational path. The further includes sending said at least one first set of graphics information at a first rate, said first rate determined as a function of said first probability. The method also includes sending said at least one second set of graphics information at a second rate, said second rate determined as a function of said second probability.","In exemplary embodiments, a system to send renderable graphics information to a client device, said graphics information including at least one first set of graphics information visible from a second view region and not visible from a first view region, and at least one second set of graphics information visible from a third view region and not visible from said first view region. The system includes a server having a processor configured to determine a first probability that a viewpoint changes position from said first view region to said second view region using data received from said client device, said received data including user input controlling the viewpoint position. The processor is further configured to determine a second probability that the viewpoint changes position from said first view region to said third view region using data received from said client device, said received data including user input controlling the viewpoint position. The processor is further configured to increase said first probability as a function of a distance of said second view region to a defined navigational path. The processor is further configured to increase said second probability as a function of a distance of said third view region to a defined navigational path. The processor is further configured to send said at least one first set of graphics information at a first rate, said first rate determined as a function of said first probability. The processor is further configured to send said at least one second set of graphics information at a second rate, said second rate determined as a function of said second probability. The system further includes said client device configured to display said at least one first set of graphics and said at least one second set of graphics.","In exemplary embodiments, a non-transitory computer readable storage medium having executable instructions stored thereon which when executed by a processor in a server causes the processor to perform a method for sending renderable graphics information to a client device, said graphics information including at least one first set of graphics information visible from a second view region and not visible from a first view region, and at least one second set of graphics information visible from a third view region and not visible from said first view region. The method includes determining a first probability that a viewpoint changes position from said first view region to said second view region using data received from said client device, said received data including user input controlling the viewpoint position. The method further includes determining a second probability that the viewpoint changes position from said first view region to said third view region using data received from said client device, said received data including user input controlling the viewpoint position. The method further includes increasing said first probability as a function of a distance of said second view region to a defined navigational path. The method further includes increasing said second probability as a function of a distance of said third view region to a defined navigational path. The further includes sending said at least one first set of graphics information at a first rate, said first rate determined as a function of said first probability. The method also includes sending said at least one second set of graphics information at a second rate, said second rate determined as a function of said second probability.","The present specification discloses methods of controlling and utilizing a content stream including delta-PVS or visibility event data packets which include geometric, texture and other renderable graphical information. The visibility event data is delivered by a decoder-server process and received and processed into precision-controlled PVS (potentially visible sets) by a decoder-client process. The resulting PVS data is efficiently rendered by a rendering process associated with the client unit. In the context of the present specification renderable graphics information includes 3D graphics primitives, including polygons and other surfaces from which image information is generated by a rendering process.","In some embodiments, the visibility event packets employed are generated in an encoding process which uses methods of from-region visibility precomputation using conservative linearized umbral event surfaces as disclosed in the PCT patent application number PCT\/US2011\/042309, and titled \u201cSystem and Method of From-Region Visibility Determination and Delta-PVS Based Content Streaming Using Conservative Linearized Umbral Event Surfaces\u201d the entire contents of which is incorporated herein by reference.","The terminology viewcell or view region, in exemplary embodiments, refers to a polyhedron, which may be represented as a polygon mesh, which describes a region to which the viewpoint is restricted. Viewcells and view regions in this specification are assumed to be convex unless otherwise indicated. A viewcell may be constrained to be a parallelpiped or box, while a view region may not necessarily be so constrained.","The terminology PVS (potentially visible set), in exemplary embodiments, refers to a set of polygons or fragments of polygons that are visible from a viewcell. Generally a PVS is computed to be conservative, including all polygons or polygon fragments that are visible as well as some that are not.","The terminology wedge (see also CLUES), in exemplary embodiments, refers to a visibility event surface formed by a feature (vertex or edge) of a viewcell and vertices or edges of the mesh polygons. In general a wedge defines the visibility from the viewcell's feature, and across the mesh polygon's vertex or edges.","The wedges employed in the prior-art method of discontinuity meshing are exact. These edges may be planar or quadric surfaces. The planar wedges described in the discontinuity mesh literature are of two types renamed here as:","1) SV-ME wedge-Formed by a vertex of the viewcell (or \u201csource\u201d) and an edge of the mesh. Also called a pivoted wedge or a supporting vertex wedge.","2) SE-MV wedge-Formed by an edge of the viewcell and a vertex of the polygon mesh. Also called a swept wedge or supporting edge wedge.","3) SE-ME wedge-Formed in the special case where the mesh silhouette edge is parallel to a supporting viewcell silhouette edge.","These definitions assume frontprojection (i.e. using the viewcell as the light souce). In the backprojection method a silhouette edge or segment of a silhouette edge is used as the \u201csource\u201d and various silhouette edges in the shaft between the source edge and the viewcell support the backprojection event surfaces. The definitions are otherwise identical for the backprojection case.","Since the wedges employed in discontinuity meshing are typically used to identify components of the sources penumbra they are constructed on a relatively large number of edges of the polygon meshes, called from viewcell silhouette edges.","Since the planar wedges used in discontinuity meshing are exact event surfaces they are not defined on regions for which the wedge's viewcell feature (vertex or edge) is occluded from the wedge's polygon mesh feature. This definition of a wedge creates \u201cgaps\u201d in the planar event surfaces that cause the surfaces to be discontinuous. In the method of complete discontinuity meshing these gaps are filled with higher-order visibility event surfaces which may be quadric wedges. The gaps are filled by these higher-order event surfaces the and resulting visibility event surfaces, in general, continuous.","Embodiments also employ planar from-feature event surfaces, the conservative linearized umbral event surfaces (CLUES) which are similar to the planar wedges employed in discontinuity meshing but differ from these wedges in important respects.","One difference between the planar wedges used in discontinuity meshing and the CLUES (also called first-order wedges, or simply wedges in the present specification) is that the wedges employed in the present method are only those wedges that could form from-viewcell umbral event surface, penumbral events per se are not considered in from-viewcell visibility. The wedges of the present method are constructed on fewer polygon mesh edges (called the first-order silhouette edges) and they are constructed using a pivot-and-sweep technique which generates only potential umbral event wedges. This means that the number of wedges constructed in the present method is far less than the number of wedges generated in discontinuity meshing.","Another difference between discontinuity meshing wedges and the wedges of the present method is that the wedges of the present method are defined and constructed using only by the wedge's viewcell feature and the wedge's polygon mesh feature. Any intervening geometry between these two features is ignored.","This method of wedge construction is based on the first-order model of visibility propagation in polyhedral environments which insures that conservative, continuous umbral boundaries are constructed.","In actuality, intervening geometry may cause regions for which the viewcell feature is occluded from the polygon mesh feature. These are regions of the wedge in which the corresponding discontinuity mesh wedge would not be defined (thereby producing a gap or discontinuity in the event surface which is normally filled by a higher-order wedge or quadric). By ignoring this intervening geometry the present method constructs wedges which define a continuous event surface without gaps. Since the wedges of the present method are constructed by ignoring this type of higher order occlusion they conservatively represent the actual from-feature umbral event surface. For regions of the wedge in which there is no intervening geometry, the wedges constructed by the present method are exact.","In regions where the wedge is inexact the wedge may be optionally replaced by other wedges constructed using a modified method of wedge construction which accounts for higher-order occlusion caused by the intervening geometry.","The present method includes three types of (first-order) wedges:","1) SV-ME wedge-formed by extending the edges of a corresponding pivoted supporting polygon. The corresponding pivoted supporting polygon is formed by a supporting vertex of the viewcell (SVV) and a first-order silhouette edge of the polygon mesh by the process of pivoting from the edge to the viewcell. The pivoted supporting polygon is also called a SV-ME supporting polygon or a vertex supporting polygon. This type of visibility event surface reflects containment at a point on the viewcell and occlusion by an (silhouette) edge of the mesh. Also called a pivoted wedge. The pivoting process is described as a process that identifies the supporting plane between the first-order silhouette edge and a viewcell. While the process may appear to a human being to be an actual continuous rotation of a plane about the silhouette edge until it touches the viewcell, in fact embodiments can measure specific discrete angles formed by each candidate supporting plane (formed by corresponding viewcell vertex) and another polygon. Comparing these angle measurements in one embodiment allows determination of the actual supporting polygon from a number of candidate supporting polygons.","2) SE-MV wedge-formed by extending the edges of a corresponding swept supporting polygon (also simply called a swept polygon or an edge supporting polygon), which is a supporting polygon formed by a supporting edge of the viewcell and an inside corner mesh silhouette vertex by the process of sweeping along the supporting viewcell silhouette contour (SVSC) between the SVVs supporting the adjacent SV-ME wedges. This type of visibility event surface reflects containment on a (boundary) edge of the viewcell restricted at an (inside corner) mesh silhouette vertex. An SE-MV wedge is also called a swept wedge.","3) SE-ME wedge-formed only where the supporting viewcell edge and the supported mesh silhouette edge are parallel. Formed by extending the edges of the corresponding SE-ME supporting polygon formed between the parallel supporting viewcell edge and the supported mesh silhouette edge. Unlike the other types of planar wedges the determination of on-wedge visibility for an SE-ME wedge is a from-region, not a from-point visibility problem. This type of visibility event surface reflects containment on a (boundary) edge of the viewcell and occlusion by an (silhouette) edge of the mesh.","Another important difference between the wedges used in prior-art discontinuity meshing and those used in the present invention is that in the present method on-wedge visibility is determined using a conservative method in which on-wedge silhouette vertices are constrained to occur on first-order, from-viewcell silhouette edges. This insures that each on-wedge silhouette vertex is a compound silhouette vertex (CSV), a point of intersection of two wedges (one corresponding to the current wedge). In contrast, in prior-art discontinuity meshing methods, on-wedge visibility is determined exactly, typically using from-point object space visibility methods like the Weiler-Atherton algorithm.","In exemplary embodiments, the terminology pivoted wedge refers to an SV-ME wedge formed by extending the edges of a pivoted supporting polygon.","The terminology CLUES (Conservative Linearized Umbral Event Surface) (See Wedge), in exemplary embodiments, refers to another name for the first-order umbral wedges constructed using the pivot-and-sweep method of the present invention. These wedges may be refined to reflect higher-order visibility interactions using the backprojection method of the present invention.","The terminology Umbra Boundary Polygon (UBP), in exemplary embodiments, refers to a polygon that is part of the surface of the from-viewcell umbral volume. In the present method the from-viewcell umbral volumes (called the polyhedral aggregate umbrae, or PAU) may be constructed using conservative UBPs that are derived from the corresponding (first-order) wedges.","The wedges employed by the present method are from-viewcell-feature umbral event surfaces that are guaranteed to be from-viewcell umbral event surfaces (from the entire viewcell) only in the immediate vicinity of the mesh silhouette edge that supports the wedge. This is because the wedge may intersect another wedge beyond the supporting silhouette edge in a way that restricts the from-viewcell umbral boundary on the wedges. That is to say that the wedge itself, which is tangentially visible from the supported viewcell feature, may become visible from other parts of the viewcell.","Higher-order UBPs may be constructed from the corresponding higher-order wedges.","The terminology polygon mesh, in exemplary embodiments, refers to a finite collection of connected vertices, edges, and faces (also called polygons) formed from the vertices and edges. If two polygons of a mesh intersect, the edge or vertex of intersection must be a component of the mesh. No interpenetration of faces is allowed. Also called a polygon mesh object, triangle mesh or simply mesh. If each edge of the mesh is shared by at most two polygons it is a manifold polygon mesh. If each edge is shared by exactly two faces then the mesh is a closed manifold polygon mesh. Polygon meshes in this specification are assumed to be closed manifold meshes unless otherwise indicated.","The terminology From-Region Silhouette Edge, in exemplary embodiments, (also called general from-region silhouette edge) is defined with respect to a region such as a viewcell (or an polygon mesh edge in the case of backprojection) acting as a light source. If the location is a viewcell the from-region silhouette edge may be called a from-viewcell silhouette edge. If the region is an edge then the from-region silhouette edge may be called a from-edge silhouette edge. In the present specification any of type of silhouette edge (from-point, from-viewcell, from-edge) may simply be called a silhouette edge, with the type of silhouette edge being implied by the context.","A from-viewcell general silhouette edge is any edge of a polygon mesh that is a from-point silhouette edge for any point on a viewcell (or area light source). This is the definition of from-viewcell silhouette edge employed by Nierenstein et. al. 2005 and in the complete discontinuity meshing method of Drettakis et. al. 1994.","In general such edges support from-region penumbral event surfaces but a subset actually support from-region umbral event surfaces which are typically quadric surfaces.","From-region silhouette edges may be defined exactly, when higher-order visibility interactions of edge triples are considered. Alternatively from-region silhouette edges may be defined, as in the present method, conservatively by considering only visibility event surfaces that arise as a result of interactions between edge pairs; as in the first-order visibility model of visibility propagation.","The terminology First-Order Silhouette Edge, in exemplary embodiments, refers to a first-order from-viewcell silhouette edge (also called simply a first-order silhouette edge) is an edge of a polygon mesh that has one component polygon that is backfacing for the entire viewcell, and the other component polygon that is front facing for at least one vertex of the viewcell, wherein the component polygons are backfacing with respect to each other.","This definition is based on a simple, conservative model of visibility propagation in polyhedral environments called first-order visibility, which considers only the visibility event surfaces that arise as a result of interactions between edge pairs.","One embodiment of the present invention employs polygon meshes that are manifold triangle meshes. In a manifold triangle mesh, each edge is completely shared by exactly two triangles. The specification of first-order silhouette edges is simplified by using manifold triangle meshes.","A first-order silhouette edge of a polygon mesh with respect to a viewcell is a locally supporting edge of the polygon mesh with respect to the viewcell. A locally supporting edge supports a polygon between the viewcell and the edge if only the viewcell and the two component polygons (triangles) sharing the edge are considered in the test for support. (See definition of test for support)","Generally first-order from-region silhouette edges are a small subset of the exact from-region silhouette edges of any polygon mesh.","In the present specification, any type of first-order silhouette edge (from-viewcell, from-edge) may simply be called a first-order silhouette edge, or simply a silhouette edge with the type of silhouette edge being implied by the context.","The terminology First-Order Visibility (also called first-order model of visibility propagation), in exemplary embodiments, refers to a model of from-region visibility propagation in which from-region umbral event surfaces are incident on (first-order) visible, first-order silhouette edges and are constructed (using the pivot and sweep method) which assumes that the entire view region (e.g., viewcell) is visible from the first-order silhouette edge.","The first-order model of visibility propagation is based on the simplifying conservative assumption that if a silhouette edge is visible from a viewcell, then it is visible from all parts of the viewcell. This assumption leads to a simple definition of first-order silhouette edges as those edges for which one component triangle is backfacing for all points on the viewcell and the other component triangle is frontfacing for at least one point on the viewcell, and further that the component triangles are not facing each other. This definition is effectively identical to the definition of a from-point silhouette edge and reflects the fact that the first-order model effectively treats the viewcell as a viewpoint in some important respects.","The terminology supporting polygon, in exemplary embodiments, refers to a supporting polygon that is \u201csupported\u201d by two structures. In the present method, a supporting polygon between a first-order silhouette edge of a polygon mesh and a viewcell is, in one case, formed by the first-order silhouette edge and a vertex of the viewcell (SV-ME supporting polygon). The vertex of the viewcell supporting this polygon is called the supporting viewcell vertex (SVV). It can be identified by pivoting the plane of the backfacing component polygon of the silhouette edge, wherein the pivoting occurs about the silhouette edge and in a direction of the normal of the backfacing component polygon of the edge toward the viewcell until the plane of the supporting polygon intersects the viewcell. This intersection will, in the general case, occur at the supporting viewcell vertex, which together with the first-order silhouette edge, forms a supporting polygon that is a triangle. If the supporting viewcell vertex is a vertex of an edge of the viewcell that is parallel to the silhouette edge of the mesh then the pivoting plane will intersect the edge of the viewcell, not just a single vertex, and the supporting polygon will be a quadrangle formed by the mesh silhouette edge and the intersected viewcell edge. This second type of supporting polygon is called a SE-ME supporting polygon.","In another case of the present method a different type of supporting polygon is formed between an inside corner vertex of a first-order silhouette edge and an edge of the viewcell (SE-MV supporting polygon also called a supporting triangle).","In the context of the present invention, supporting polygons are conservatively defined as being supported by a first-order silhouette edge (also called a locally supporting edge), or vertex thereof, and the corresponding viewcell, neglecting any occlusion or interference between the first-order silhouette edge and the viewcell. If a supporting polygon, as defined by the present invention, intersects geometry between the first-order edge and the viewcell, then the supporting polygon is not a supporting polygon as defined in the prior art (which does not generally allow a supporting polygon to be defined if such interference exists).","As defined in prior-art a polygon would pass a \u201ctest for support\u201d (i.e. be a supporting polygon) between two structures if the polygon is supported by a vertex or edge of one structure and a vertex or edge of the other structure without intersecting anything else. The test for support also requires that the extension of the supporting polygon (e.g. this extension is the \u201cwedge\u201d) in the direction away from the first supported object (e.g. the viewcell) also does not intersect the other supported structures (e.g. the polygon meshes) in a way that causes it to be \u201cinside\u201d the other supported structure (e.g. on the topological \u201cinside\u201d of a manifold mesh). This test for support effectively requires a supporting edge to be an \u201coutside\u201d edge of the structure (e.g. a polygon mesh) which will support a supporting polygon tangentially to the structure, as opposed to an \u201cinside\u201d or reflex edge of a structure such as a polygon mesh which will not.","In the present method this test for support is used in a more limited way by including only the polygons sharing an edge of a mesh in the determination of whether the edge supports a conservative supporting polygon between the viewcell and the mesh (i.e., whether the edge is a \u201clocally supporting\u201d or first-order silhouette edge, see definition of first-order silhouette edge and locally supporting edge).","In cases where the difference between the present, conservative, definition of the supporting polygon is distinguished from the prior-art definition of the supporting polygon is to be emphasized, a supporting polygon as defined by the present invention may be called a conservative supporting polygon. Otherwise a conservative supporting polygon as defined in the present invention is simply called a supporting polygon.","As defined in the present invention, wedges derived from (conservative) supporting polygons always form continuous conservative linearized umbral event surfaces that can be intersected with mesh polygons to conservatively determine the set of mesh polygons (or fragments thereof) that are visible from a viewcell, without the need for quadric surfaces that usually dominate (and complicate) exact solutions.","In exemplary embodiments, for the terminology Conservative Supporting Polygon see the above terminology for supporting polygon.","The terminology Test for Support, in exemplary embodiments, refers to a polygon that would pass a \u201ctest for support\u201d (i.e. be a supporting polygon) between two polygonal structures if the polygon is supported by a vertex or edge of one structure and a vertex or edge of the other structure without intersecting anything else. The test for support also requires that the extension of the supporting polygon (e.g. this extension is the \u201cwedge\u201d) in the direction away from the first supported object (e.g. the viewcell) also does not intersect the other supported structures (e.g. the polygon meshes) in a way that causes it to be \u201cinside\u201d the other supported structure (e.g. on the topological \u201cinside\u201d of a manifold mesh). This test for support effectively requires a supporting edge to be an \u201coutside\u201d edge of the structure (e.g. a polygon mesh) which will support a supporting polygon tangentially to the structure, as opposed to an \u201cinside\u201d or reflex edge of a structure such as a polygon mesh which will not.","In the present method this test for support is used in a more limited way by including only the polygons sharing an edge of a mesh in the determination of whether the edge supports a conservative supporting polygon between the viewcell and the mesh (i.e., whether the edge is a \u201clocally supporting\u201d or first-order silhouette edge, see definition of first-order silhouette edge and locally supporting edge).","In cases where the difference between the present, conservative, definition of the supporting polygon is distinguished from the prior-art definition of the supporting polygon is to be emphasized, a supporting polygon as defined by the present invention may be called a conservative supporting polygon. Otherwise a conservative supporting polygon as defined in the present invention is simply called a supporting polygon.","The terminology Supporting Viewcell Silhouette Contour (SVSC), in exemplary embodiments, refers to that portion of the viewcell silhouette contour, as viewed from an inside corner vertex of a mesh silhouette edge that produces the most extreme umbra boundary. This is the portion of the viewcell silhouette contour which produces the least occlusion when looking through the inside corner mesh silhouette vertex from the viewcell silhouette. It is also the contour that, when subjected to sweep operation, produces SE_MV wedges that have a consistent orientation with the connected SV-ME wedges and form a continuous surface. The supporting viewcell silhouette contour extends between two SVVs that corresponding to the mesh silhouette edges which produce the inside corner vertex.","SE-MV wedges are oriented visibility event surfaces that reflect the restriction of visibility at a mesh silhouette vertex by virtue of containment on the viewcell surface.","In contrast, SV-ME wedges are oriented visibility event surfaces that reflect the restriction of visibility at a mesh silhouette edge by virtue of the (from-viewcell) occlusion caused by the mesh polygon at the silhouette edge.","The SVSC is the set of (from mesh silhouette edge) viewcell silhouette edges that produces corresponding SE-MV wedges having a orientation that is consistent with the orientation of adjacent SV-ME wedges; thus producing a continuous, conservative, consistently oriented umbral event surface at the mesh silhouette vertex.","For the terminology swept triangle, see swept polygon.","The terminology Swept Polygon, (also called a swept supporting polygon or a swept triangle), in exemplary embodiments, refers to the visibility event boundary at an non-convex (or \u201cinside\u201d) corner of a first-order silhouette edge of a polygon mesh is formed not only by extending those supporting polygons supported by the silhouette edges forming the inside corner, but possibly also by one or more swept polygons which are a different type of supporting polygon formed between the inside corner vertex of the mesh silhouette and certain edges of the viewcell that are from-point silhouette edges from the perspective of the inside corner silhouette vertex of the mesh object. These from-point silhouette edges of the viewcell form a contour chain (the extremal or supporting viewcell silhouette contour) between the SVVs corresponding to the inside corner edges of the mesh object. Polygons (triangles) are \u201cswept\u201d out for each edge of this chain, forming the swept polygons. The edges of these swept polygons are extended to form SE-MV or swept wedges that also contribute to the first-order visibility event surface at inside corners of the mesh silhouette contour.","The terminology Swept Wedge, in exemplary embodiments, refers to a SE-MV wedge formed by extension of the edges of a swept supporting polygon.","The terminology Simple Silhouette Contour, in exemplary embodiments, refers to a chain of silhouette edges connected by shared vertices belonging to a single mesh object. Also called a simple contour.","The terminology Compound Silhouette Contour, in exemplary embodiments, refers to a chain of silhouette edges comprising silhouette edges connected by shared vertices or connected by vertices formed by the intersection of a wedge\/UBP from one contour with a non-adjacent silhouette edge. In the study of smooth manifolds such an intersection is called a t-junction. (See Durand, Fredo PhD thesis University of Grenoble)","In exemplary embodiments, for the terminology T-Junction, also called a compound silhouette vertex (CSV), see Compound Silhouette Contour.","The terminology CSV (Compound Silhouette Vertex), in exemplary embodiments, refers to the point of intersection of a wedge and a silhouette edge. For a first-order implementation the wedge is a first-order wedge and the silhouette edge is a first-order silhouette edge. In topological terms the CSV corresponds to a conservatively defined t-vertex of the from-region compound silhouette contour. Typically an inside corner of the compound silhouette contour occurs at a CSV.","The terminology ESO (Effective Static Occlusion), in exemplary embodiments, refers to a metric that is in some direct proportion to the number of (original mesh) polygons and\/or surface area of these polygons inside an occluded region of a visibility map. The ESO is also in some inverse proportion to the number of new polygons introduced in the visible region surrounding the occluded region as a result of retriangulation caused by the edges of the occlusion boundary. The metric is used in conservative simplification of a VM or unified VM.","The terminology EDO (Effective Dynamic Occlusion), in exemplary embodiments, refers to a metric that is in some direct proportion to the number of polygons and\/or surface area of polygons occluded in a delta region (DR) of occlusion wherein the DR represents the region of occlusion produced during a specific viewcell transition. The ESO is also in some inverse proportion to the number new polygons introduced in the visible region surrounding the DR as a result of retriangulation caused by the edges of the occlusion boundary.","The terminology EDV (Effective Dynamic Visibility), in exemplary embodiments, refers to a measure of the effectiveness of a delta region (DR) of a unified visibility map. If the DR is a DR(delta region of occlusion) for the specific viewcell transition then the EDV corresponds to the EDO of the DR.","If the DR is a DR(delta region of exposure) then the EDV is determined by examining the ESO of the surrounding occlusion regions. Simplification of the DRproceeds by simplification of the surrounding OR and extending the polygons of the DRinto the OR or DR.","The terminology Unified Visibility Map, in exemplary embodiments, refers to a visibility map including from-viewcell occlusion boundaries generated from two viewcells (e.g. A and B) wherein the viewcells are related in one of two ways: 1) one viewcell is completely contained in the other, or 2) the viewcells completely share a common face. The unified visibility map is an arrangement of VM regions such that some regions contain newly occluded mesh triangles\/fragments and other regions contain newly exposed mesh triangles\/fragments for the transition from viewcell A to viewcell B. The unified visibility map is used to construct delta-PVS data for direct storage. Alternatively the unified visibility map can be used to identify significantly occluding or significantly silhouette contours which can be labeled and used to generate the deltaG\/delta-PVS data later.","The terminology Visibility Event Packet, in some embodiments, refers to data packets that include, in some embodiments, delta-PVS data, labeled significantly occluding silhouette contour information, stored occlusion boundary information, and\/or PVS data.","Visibility Event Packets, in some embodiments, include other visibility event information. In some embodiments, Visibility Event Packets contain visibility event information. In this specification, visibility event information is also referred to as visibility event data.","The terminology Movement Threshold, in some embodiments, refers to a specific predetermined threshold value for a probability that an actual or predicted camera\/viewpoint position will change from one view region to another view region. For example, when a probability (i.e., likelihood) that a viewpoint changes from a first viewpoint and a second viewpoint exceeds the movement threshold, additional information may be sent to a client device.","The terminology, Bandwidth Threshold, in some embodiments, refers to a predetermined threshold value for an available transmission bandwidth between a server and a client unit. For example, when an available bandwidth is less than the bandwidth threshold, information at a lower level of detail may be sent to a client device.","The basic aspects of the first-order model of visibility propagation are illustrated in . Subsequent details are given in the Description-Embodiments sections of the specification.",{"@attributes":{"id":"p-0237","num":"0236"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0238","num":"0237"},"figref":"FIG. 2","b":["1","1","10","1"]},"The construction of conservative linearized umbral event surfaces (CLUES) incident on these first-order silhouette edges is now described. In the following discussion, the umbral event surfaces constructed are similar to discontinuity mesh wedges in the sense that they define visibility from a single feature of a viewcell (generally a supporting viewcell vertex or edge). In a subsequent section of this specification, it is shown that these wedges can be used to construct a conservative linearized from-viewcell visibility map (VM) from which a PVS can be derived.",{"@attributes":{"id":"p-0240","num":"0239"},"figref":["FIG. 2","FIG. 2"],"sub":["1","8","1","_","0 ","1","_","1","8","8 "],"b":["1","1","1","1","1","1","1","1","1","1","1","1","1"]},"With respect to first-order silhouette edge A, segment BO is occluded from supporting viewcell vertex SVV, as BO is on the occluded side of the event surface WEDGE that is formed between edge A and the corresponding supporting viewcell vertex SVV.","The first-order visibility event surface, labeled WEDGE, lies in the supporting plane between edge A and the viewcell. The supporting polygon SP between edge A and the viewcell is the triangle (labeled SP) formed by the vertices Aand Aof edge A and the viewcell vertex labeled SVV.","According to some embodiments, WEDGE, the first-order visibility event surface incident on edge A, is formed by extending the two edges of the corresponding supporting polygon (SP) that are incident on the vertices Aand Aof edge A. This extension occurs semi-infinitely starting at the vertices Aand Aof A, in a direction away from the viewcell. The two extended rays are connected to the vertices Aand Aof edge A to form the semi-infinite umbral visibility event surface labeled WEDGE. Only a portion of WEDGE is shown in , as it actually extends semi-infinitely away from the viewcell. In some embodiments, the plane of an event surface is represented by a 3D planar equation such as ax+by+cz=0.","Thus, in some embodiments, to form a (from-viewcell) first-order visibility event surface incident on a first-order silhouette edge and a viewcell vertex, the supporting polygon between the silhouette edge and the viewcell is first constructed. This construction is analogous to a pivot operation on the silhouette edge in the direction away from the backfacing component polygon and toward the viewcell until a supporting viewcell feature (edge or vertex) is encountered. In some embodiments the, wedge is formed by extending the non-silhouette edges of this supporting polygon away from the viewcell.","As illustrated in , event surface WEDGE intersects edge B, dividing B into the two segments BV, which is first-order visible from the viewcell feature (the viewcell vertex SVV) with respect to first-order silhouette edge A, and BO, which is not first-order visible from SVV with respect to first-order silhouette edge A. Wedge  intersects first-order silhouette edge B (comprised of segments BO and BV) at the point labeled CSV. This point is a compound silhouette vertex.","For the purposes of illustration, assume now that the segment BV is on the unoccluded side of all first-order visibility event surfaces formed by the edges of mesh A and the features of the VIEWCELL. In this case, BV is outside (on the unoccluded side) of the first-order polyhedral aggregate umbrae (PAU) formed by the intersection of the first-order wedges with the mesh polygons and with each other. Under these conditions segment BV is first-order visible from the viewcell.","If the segment BIV is first-order visible from the viewcell, then under the conservative assumptions of the first-order visibility model, segment BV is assumed to be visible from any part of the viewcell. Consequently, the first-order visibility event surface incident on the segment BV is constructed, by the previously described pivoting operation, which generates the supporting polygon (SP), between the segment BV and the supporting viewcell vertex labeled SVV. As illustrated in , the supporting polygon SP is defined by viewcell vertex V(SVV) the vertices of segment BV. The previously described method of extending the supporting polygon is once again employed. The resulting first-order visibility event surface incident on BV is labeled WEDGE.","WEDGE is an exact visibility event surface incident on edge A because in this case, the corresponding supporting viewcell vertex SVV is actually visible from the supported first-order silhouette edge A.","WEDGE is not an exact visibility event surface through edge BV because the conservative assumption of the first-order visibility model is violated in a very specific way: the corresponding supporting viewcell vertex SVV is not actually visible from the supported first-order silhouette edge BV, it is occluded when viewed from this edge.","The exactness of any first-order visibility event surface (e.g., wedge) incident on a silhouette edge can be determined using a 2D visibility test which tests the visibility of the supporting viewcell vertex from the silhouette edge. In some embodiments, if the supporting viewcell feature is a vertex, then this is a from-point visibility test that is equivalent to testing the visibility of the first-order silhouette edge from the corresponding supporting viewcell vertex (SVV). According to some embodiments, segments of the first-order silhouette edge that are visible from the corresponding SVV support exact visibility event surfaces, and segments of the first-order silhouette edge that are occluded from the corresponding SVV support inexact\/conservative visibility event surfaces.","In the special case where the silhouette edge is parallel to a supporting viewcell edge, a special from-edge visibility test is required.","Embodiments also include a method to increase the precision of inexact visibility event surfaces. In this method, for each segment of a first-order silhouette edge supporting an inexact wedge, a point on the surface of the viewcell is identified that is the visible supporting viewcell vertex (VSVV) for the segment. The VSVV is actually visible from the corresponding silhouette edge segment and forms a supporting polygon with the segment. These methods employ a higher-order adaptive, progressive refinement of the first-order model, they are described in the PCT patent application number PCT\/US2011\/042309.","Referring now to the drawings, wherein like reference numerals designate identical or corresponding parts throughout the several views.",{"@attributes":{"id":"h-0006","num":"0000"},"figref":["FIG. 1","FIG. 6"]},"One embodiment includes a method of conservative, linearized visibility map construction that is based on a simplified first-order model of visibility propagation in polyhedral environments. As previously described in embodiments, the first-order visibility model is based on the conservative assumption that if a silhouette edge of a polygon mesh is visible from any part of a viewcell, then it is visible from all parts of the viewcell. According to embodiments of this model, silhouette edges (called first-order silhouette edges) are limited to those triangle mesh edges that have one component polygon that is backfacing for all vertices of the viewcell and another component polygon that is front facing for at least one vertex of the viewcell. Additionally, to be a first-order silhouette edge, the component polygons is backfacing with respect to each other.","This model also leads to a method in which first-order conservative linearized umbral event surfaces (called CLUES, or also called first-order wedges or simply wedges) are formed either by pivoting from the (first-order) silhouette edge to a vertex of the viewcell (SV-ME wedge derived from the pivoted supporting polygons) or by sweeping from an (first-order) inside corner silhouette vertex through viewcell silhouette edges (SE-MV wedges derived from swept supporting triangles). The method also employs SE-ME wedges generated in the special case where the supported silhouette edge is parallel to a supporting viewcell edge. The first-order embodiment always produces a conservative umbra boundary, and in some cases, it is the exact umbra boundary.","Other embodiments are based on a higher-order model of visibility propagation in polyhedral environments. The higher-order model does not assume that if a silhouette edge is visible from any part of a viewcell then it visible from all parts of the viewcell. Rather, this model accounts for portions of the viewcell that are occluded from a silhouette edge. The higher-order model forms the basis of alternate embodiments which can produce a more precise approximation to the exact umbra boundaries in cases where the first-order method is imprecise. The higher-order refinement method is disclosed in the PCT patent application number PCT\/US2011\/042309.",{"@attributes":{"id":"h-0007","num":"0000"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0257","num":"0256"},"figref":["FIG. 1","FIG. 3"],"b":"110"},"Process flow proceeds to step  to determine if the first-order silhouette edge encountered in step  is parallel to the supporting viewcell edge.","If, in decision step , it is determined that the first-order silhouette edge is not parallel to the supporting viewcell vertex, then process flow proceeds to step  to construct a supporting polygon between the silhouette edge and the viewcell.  illustrates the details of this construction.","Process flow proceeds to step  to construct the SV-ME wedge incident on the first-order silhouette edge by extending specific edges of the corresponding pivoted supporting polygon incident on vertices of the first-order silhouette edge. Additional details of step  are disclosed in .","If the first-order silhouette edge is parallel to a supporting silhouette then process flow proceeds from  to step .","In step , the supporting quadrangle, called a SE-ME (source edge-mesh edge) quadrangle is constructed by pivoting from the viewcell edge to the viewcell as previously described.","Process flow proceeds to step  to construct the SE-ME wedge corresponding to the SE-ME supporting quadrangle by extending the line segments formed by diagonal vertices of the SE-ME supporting quad. The edges of the SE-ME wedge are comprised of the supported silhouette edge and the two lines formed by extending the diagonal line segments through the silhouette edge vertices and away from the viewcell.","Process flow proceeds for steps  or  to decision step  to determine if adjacent silhouette edges form an outside corner of the first-order silhouette contour. In some embodiments, this determination is made using a simple test for the relative orientation of adjacent silhouette edges. Each edge, being on the boundary of a polygon mesh, has a natural orientation in which one normal to the edge is facing outside the polyhedron (outfacing normal) and the opposite normal is facing inside the polyhedron. If the two outfacing normals for adjacent silhouette edges are facing away from each other, then the shared silhouette vertex is an inside corner of the silhouette contour. Otherwise the shared silhouette vertex forms an outside corner.","If it is determined, in decision step , that the adjacent silhouette edges form an outside corner of the silhouette contour, then process flow proceeds to step  to intersect the wedges incident on the adjacent silhouette edges with each other. In some embodiments, if the adjacent SV-ME wedges were generated by pivoting to the same supporting viewcell vertex (SVV), then they exactly intersect at a common edge. Otherwise, the adjacent SV-ME wedges intersect each other in their polygon interiors and an explicit polygon-polygon intersection determination is made. In either case, the intersecting SV-ME wedges produce continuous umbral event surface spanning portion of the first-order silhouette contour formed by the two supported silhouette edges. In some embodiments, adjacent SV-ME wedges are not intersected. In these embodiments the step  is optional. A SV-ME wedge which is not intersected with an adjacent SV-ME wedge can still be intersected with mesh polygons and the resulting wedge-mesh polygon intersection tested to determine if it is a from-viewcell occlusion boundary. Additional discussion of intersecting adjacent SV-ME wedges is given in conjunction with  and .","If it is determined, in decision step , that the adjacent silhouette edges do not form an outside corner of the silhouette contour, then process flow proceeds from step  to step . This case corresponds to an inside corner of a first-order silhouette contour.","In some embodiments, such inside corners formed by two silhouette edges that are connected by a vertex are simple silhouette vertices. Using the first-order model of visibility propagation inside corners can also form on compound silhouette contours in which the component silhouette edges do not share a vertex in the original manifold mesh. These are called compound silhouette vertices (CSVs), they correspond to from-region t-vertices of the manifolds and are discussed in detail in a later part of this specification.","In step , one or more supporting swept triangles are formed between the inside corner mesh silhouette vertex and certain edges of the viewcell that are from-point silhouette edges with respect to the inside corner mesh silhouette vertex. Additional details of this process are disclosed in  and .","Process flow proceeds to step , the corresponding SE-MV wedges are generated from the swept triangles by extending the edges of the swept triangles through the inside corner mesh silhouette vertex. Additional details of this process are disclosed in .","Alternate embodiments are possible in which the set of first-order wedges are constructed using a different method. For example, in one alternate embodiment, the entire conservative supporting hull between the viewcell and the polygon mesh objects may be constructed, and the first-order wedges selected as a subset of the conservative supporting hull polygons.",{"@attributes":{"id":"p-0271","num":"0270"},"figref":["FIG. 3","FIG. 1","FIG. 3","FIG. 1"],"b":["110","110","310"]},"Process flow proceeds to decision step  to test one component polygon, called polygon B or PB, to determine if the component is backfacing for all vertices of the viewcell. In this case, all vertices of the viewcell would be on the backfacing side of the plane that contains the component polygon.","If, in decision step  it is determined that PB is not backfacing for all viewcell vertices, then process flow proceeds from step  to step  to test the other component polygon, called PA, as described in step .","If, in decision step , it is determined that PA is backfacing for all vertices of the viewcell, then process flow proceeds to step  to determine if component triangle PB is front facing for at least one viewcell vertex.","If, in decision step , it is determined that PB is front facing for at least one viewcell vertex then process flow proceeds to decision step  to test PA and PB to determine if they are backfacing with respect to each other.","If, in decision step , it is determined that PA and PB are backfacing relative to each other, then process flow proceeds to step , where the edge being tested is a first-order silhouette edge.","If, in decision step , it is determined that PA and PB are not backfacing relative to each other, then process flow proceeds to step  which returns a result that the edge being tested is not a first-order silhouette edge.","If in step , if PB is backfacing for all vertices of the viewcell, process flow proceeds to step  to determine if PA is frontfacing for at least one viewcell vertex. If PA is frontfacing for at least one viewcell vertex, process flow proceeds to step  to determine if PA and PB are backfacing to each other as functionally described in step .","If PA and PB are backfacing with respect to each other, process flow proceeds to step  which returns a result that the edge being tested is a first-order silhouette edge. If PA and PB are not backfacing to each other, process flow proceeds from  to . If PA is not frontfacing for at least one viewcell vertex, process flow proceeds from  to . If any of the tests in steps , , , , or  fail, then the mesh edge is not a first-order silhouette edge, as indicated in step .",{"@attributes":{"id":"h-0008","num":"0000"},"figref":"FIG. 4A"},{"@attributes":{"id":"p-0280","num":"0279"},"figref":["FIG. 4A","FIG. 4A","FIG. 4A","FIG. 1"],"b":["116","116"]},"In some embodiments, the process of constructing supporting polygons starts at step  upon encountering a silhouette edge of the polygon mesh. In the present embodiment this is a first-order silhouette edge, although other embodiments may potentially use higher order silhouette edges.","Process flow proceeds to step  to set a SUPPORTING_ANGLE between the first-order silhouette edge and the viewcell to a MAX value (e.g., 180 degrees). According to some embodiments, the supporting angle is defined as the angle formed when pivoting a plane through the first-order silhouette edge, starting in the plane of the backfacing component polygon and pivoting toward the viewcell (in the general direction of the normal of the backfacing component polygon) until the first vertex or edge of the viewcell is encountered. The position of the pivoting plane on contact with the viewcell is the plane of the supporting polygon between the silhouette edge and the viewcell. The angle traversed during the pivot is called the supporting angle or the pivot angle, and it is measured between the supporting plane and the plane of the backfacing component polygon of the silhouette edge. The viewcell vertex, or edge if the supporting polygon is SE-ME type, that results in the smallest pivot angle is the supporting vertex or supporting edge.","The remainder of  shows the process of identifying the supporting viewcell vertex and constructing the supporting polygon. Process flow proceeds to step  to set the VERTEX to the first vertex of the viewcell. In embodiments, the VERTEX is a candidate vertex that is tested to determine if the candidate vertex is a supporting vertex. Process flow proceeds to step  to construct a triangle between the mesh silhouette edge EDGE and VERTEX. Process flow proceeds to step  to measure the angle between the visible sides of the plane of the TRIANGLE and the plane of the backfacing component polygon of the silhouette edge using a standard method for measuring the angle between planes at their line of intersection. Process flow proceeds to step  to compare this ANGLE to the current value of the SUPPORTING_ANGLE. If the ANGLE is less than the current value of the SUPPORTING_ANGLE, then process flow proceeds to step  to set the SUPPORTING_ANGLE to ANGLE. Process flow proceeds to  to set the SUPPORTING_VERTEX to the current VERTEX.","Process flow proceeds to step , where the supporting polygon is set to the triangle formed by the silhouette edge and the supporting vertex.","Process flow proceeds to step  to determine if unprocessed viewcell vertices remain. If, in decision step , it is determined that no unprocessed viewcell vertices remain, then process flow proceeds to step , where the supporting polygon is output.","If, in decision step , it is determined that unprocessed viewcell vertices remain, then process flow proceeds to step , where the next viewcell vertex is selected for processing.","If, in decision step , it is determined that the ANGLE (pivot angle) measured is not less than the current SUPPORTING_ANGLE, then process flow proceeds to step  to determine if the pivot angle (ANGLE) equals the current value of SUPPORTING_ANGLE. If this condition is true, then two vertices of the viewcell form the same pivot angle with the silhouette edge, corresponding to a SE-ME supporting polygon, and process flow proceeds to step  to set the quadrangle between both viewcell vertices and the viewcell edge (an SE-ME supporting polygon).","A quadrangular supporting polygon is constructed in step  only in the special case when the supporting angle between the silhouette edge and two viewcell vertices is equal. For a convex viewcell, which is assumed in the present embodiment, this occurs only when the two supporting viewcell vertices lie on an edge of the viewcell that is parallel to the mesh silhouette edge. In this case, the visibility from the viewcell \u201cacross\u201d the silhouette edge is not determined by the usual from-point visibility triangle but instead by a from-segment visibility quadrangle.","Other embodiments are possible which deal with this special case differently, for example by constructing two supporting triangles and a swept triangle incident on the parallel supporting viewcell edge. Using this approach, the resulting corresponding adjacent UBPs will not intersect only at an edge, but instead, they will overlap on their planes, causing a local degeneracy of the bounded polyhedral umbra volume. The present method of identifying quadrangular supporting polygons avoids such degeneracies in later steps.","Regardless of whether the candidate supporting polygon is a triangle or a quadrangle, the process flow proceeds from step  to step  to determine if any unprocessed vertices remain as described above. If viewcell vertices remain, then process flow returns to step , where the next viewcell vertex is selected. Subsequently the process follows the previously described steps.","At the final step , the process outputs a supporting polygon that is either a triangle, formed by the mesh silhouette edge and a vertex of the viewcell, or a quadrangle that is formed between the mesh silhouette edge and a viewcell edge.","Alternate embodiments of the method of constructing SV-ME supporting polygons are possible. In one alternate embodiment, the SUPPORTING_VERTEX corresponding to one first-order silhouette edge is limited to those viewcell vertices directly connected to the SUPPORTING_VERTEX for an adjacent first-order silhouette edge, wherein the adjacent edges form an outside corner (convex feature) of the mesh. This method is similar to the method employed in the classic prior-art method of divide-and-conquer method of constructing a convex hull in 3D. In the present application the viewcell is a very simple polyhedron and the speedup afforded by this method is very limited.",{"@attributes":{"id":"p-0293","num":"0292"},"figref":["FIG. 4B","FIG. 7A","FIG. 7B","FIG. 4B","FIG. 7A","FIG. 7B","FIG. 4B","FIG. 4B","FIG. 7B"],"b":["1","1","1","3"]},"Two candidate supporting polygons are shown as CANDIDATE SP and CANDIDATE SP. A candidate supporting polygon is identified for first-order silhouette edge B by constructing a triangle formed by edge B and a vertex of the viewcell. The angle of that the plane of this supporting polygon forms with the plane of the backfacing component polygon sharing edge B is measured. This angle corresponds to the variable SUPPORTING_ANGLE determined in step  of  and used in steps  and  of the same figure. In the example shown in  the backfacing component polygon of first-order silhouette edge B is the triangle formed by edge B and vertex V.","In this example, the angle formed by CANDIDATE SP (corresponding to viewcell vertex V is indicated by a dashed arc labeled ANGLE-.","In this example, the angle formed by CANDIDATE SP (corresponding to viewcell vertex V is indicated by a dashed arc labeled ANGLE-.","From the two arcs, it is apparent that ANGLE- is less than ANGLE-. According the exemplary flowchart of , CANDIDATE SP would be retained as a candidate for the actual supporting polygon on first-order silhouette. If all the vertices of VIEWCELL are tested by the process shown in the exemplary flowchart of  it will be found that vertex V results in the supporting polygon (CANDIDATE SP) giving the smallest supporting angle. CANDIDATE SP is shown as the actual supporting polygon SPB in .","Standard angle measures can be employed to determine the angle including the cross product between the normal vectors of the plane of the backfacing polygon and the candidate supporting polygon.",{"@attributes":{"id":"p-0299","num":"0298"},"figref":"FIG. 4C"},"Alternate embodiments are possible in which SV-ME supporting polygons are identified by considering both the \u201csidedness orientation\u201d of the candidate supporting polygon (relative to the interior of the polygon mesh) and the orientation of the candidate supporting polygon relative to the viewcell vertices.","In one embodiment, mesh polygons are all assumed to be \u201coutside\u201d polygons which have their normal vector locally oriented away from the \u201cinside\u201d of the region contained by the polygon mesh. In such embodiments, all mesh polygons of a polygon mesh consistently have this same \u201csidedness\u201d orientation.","A polygon is a planar structure which can have two sides, corresponding to the two sides of the plane containing the polygon. Exemplary embodiments include polygon meshes which are manifold or closed. Manifold meshes divide the volume of space in which they are embedded into an inside and an outside. In computer graphics, it is useful to employ manifold meshes in which the normal vector of each polygon in the mesh is locally oriented to face away from the inside of this enclosed volume. This can be called the \u201coutside\u201d side of the polygon. The opposite side can be called the \u201cinside\u201d side of the polygon. If all polygons have this consistent sidedness orientation in a mesh, then no inside side of a polygon should ever be visible from the outside.","In exemplary embodiments, it can be established that polygons of a mesh have the same sidedness orientation by examining the vertex orderings of adjacent polygons i.e., polygons which share an edge. (See Schneider (2003) Schneider, Philip J., Eberely, David H., \u201cGeometric Tools for Computer Graphics\u201d Morgan Kaufmann 2003 pp. 342-345, the entire contents of which are incorporated herein by reference). Let Fand Fbe two adjacent polygons sharing an edge comprised of two vertices Vand V. If vertices Vand Voccur in the order Vfollowed by Vfor polygon F, then they must occur in polygon Fin the order Vfollowed by V. Adjacent polygons in which shared edges have this ordering are said to have a consistent vertex ordering. Polygons with a consistent vertex ordering have the same sidedness orientation. The vertex ordering reflects the order in which the vertices are stored for each triangle. Vertices accessed in this same order for a triangle defines vectors (triangle edges) whose cross products are the coefficients A, B, C of the plane equation or normal vector of the triangle. In some embodiments, all mesh triangles have consistent vertex orderings and all will have normal vectors that point away from the inside of the mesh, i.e. they are all outside facing triangles. Embodiments may employ known algorithms to identify and repair inconsistent vertex orderings in a polygon mesh prior to processing (See MakeConsistent procedure of Schneider (2003), pp 345).",{"@attributes":{"id":"p-0304","num":"0303"},"figref":"FIG. 4D","sub":["0 ","1 ","0 ","1","3 ","1 ","3","1"]},{"@attributes":{"id":"p-0305","num":"0304"},"figref":"FIG. 4E","sub":["0 ","1 "]},"In one embodiment, a candidate SV-ME supporting polygon for a first-order silhouette edge is formed between a viewcell vertex and the first-order silhouette edge. The candidate supporting polygon is given the same sidedness orientation as the backfacing mesh polygon sharing the first-order silhouette edge. (Using this consistent sidedness orientation, for example, a person walking across the first-order silhouette edge on the \u201coutside\u201d surface of the backfacing mesh polygon would encounter the \u201coutside\u201d surface of the candidate supporting polygon). The orientation of the plane of each candidate supporting polygon is then examined relative to the viewcell vertices. If the plane of the candidate supporting polygon is not front-facing with respect to each viewcell vertex, then the viewcell vertex forming the candidate supporting polygon is a supporting viewcell vertex, and the candidate supporting polygon is a supporting polygon.","According to some embodiments, the employed definition of front-facing with respect to a viewcell vertex excludes viewcell vertices which are in the plane of the candidate supporting polygon (i.e. admitting a supporting viewcell vertex as not front-facing). Alternate embodiments can employ variations of the definitions of backfacing and front facing to determine that a candidate supporting polygon is not front-facing with respect to each viewcell vertex. In at least one exemplary embodiment, the test includes establishing that the candidate supporting polygon is backfacing with respect to each viewcell vertex, where the definition of a plane that is backfacing to a vertex includes vertices which are in the plane (i.e. admitting a supporting viewcell vertex as backfacing to a supporting polygon).","According to some embodiments, the process illustrated in  is entered from step . In step  a candidate supporting polygon is formed between the first-order silhouette edge and a viewcell vertex (V).","Process flow proceeds to step  to set the sidedness orientation of the candidate supporting polygon formed in step  to be the same as the backfacing component polygon sharing the first-order silhouette edge.","Process flow proceeds to step  to determine if the candidate supporting polygon is not front-facing for each of the viewcell vertices. If, in decision step , it is determined that the candidate supporting polygon is not front-facing with respect to each viewcell vertex then process flow proceeds to step  to identify the viewcell vertex (V) as a supporting viewcell vertex and to identify the candidate supporting polygon as a supporting polygon.","If, in decision step , it is determined that the candidate supporting polygon is front-facing for any viewcell vertex then process flow proceeds to step  to identify the viewcell vertex (V) as not a supporting viewcell vertex and to identify the candidate supporting polygon as not a supporting polygon.","The test illustrated by exemplary flowchart of  can also be employed to identify SE-ME type supporting polygons.",{"@attributes":{"id":"h-0009","num":"0000"},"figref":["FIG. 5A","FIG. 5B"]},{"@attributes":{"id":"p-0313","num":"0312"},"figref":["FIG. 5A","FIG. 5B","FIG. 1","FIGS. 5A and 5B","FIG. 1"],"b":["130","130"]},"In some embodiments, the process of constructing SE-MV supporting swept triangles starts at step  upon encountering an inside corner of a first-order silhouette contour of a polygon mesh. This inside corner may be formed from a simple first-order silhouette contour in which two first-order silhouette edges share a vertex. If the normals of the silhouette edges forming the intersection (with normal direction assumed to be facing away from the interior of their component polygons) are facing each other, then the intersection is an inside corner vertex.","Alternatively, the inside corner may be a vertex of a compound silhouette contour formed by the intersection of a wedge with a first-order silhouette edge. In the latter case, the inside corner silhouette mesh silhouette vertex is called a compound silhouette vertex (CSV).","Process flow proceeds to step  to identify the supporting viewcell vertex (SVV) for one of the silhouette edges forming the vertex using, for example, the process disclosed in . The identity of this vertex is stored as the variable SVV_START. Process flow proceeds to step , were the process for step  is repeated for the other edge of the inside corner, and the result is stored as the variable SVV_END.","If either supporting polygon of the inside corner is a quadrangle (generated in , step ) then the supporting polygon has two SVVs. In this special case, care must be taken to select, in steps  and , the initial (SVV_START) or terminal (SVV_END) viewcell vertex in the chain as the vertex that is farthest removed from the other end of the chain.","Process flow proceeds to step , where the variable CURRENT_POLYGON is set to identify the supporting polygon between the viewcell vertex SVV_START and the corresponding supported edge of the polygon mesh.","Process flow proceeds to step , where an initial point for the sweep of the viewcell silhouette contour, which ultimately occurs between the viewcell vertices SVV_START and SVV_END, is set to be the viewcell vertex SVV_START and stored as the variable CVV, which holds the current vertex of the sweep.","Process flow proceeds to decision step  to compare CVV to SVV_END to determine if the sweep should be terminated.","If, in decision step , it is determined that the current viewcell vertex being processed (CVV) is the same as the last vertex in the sweep (SVV_END), then process flow proceeds to step  and terminates. If both edges of the inside corner have the same supporting point on the viewcell then the corresponding SV-ME wedges intersect along a common edge and there is no swept triangle corresponding to the inside corner vertex. This situation would be identified on the initial execution of step  and the sweep would be terminated without producing a swept triangle.","If, in decision step , it is determined that CVV is not SVV_END, then process flow proceeds to step  to set a variable CURRENT_ANGLE to a maximum value.","Process flow proceeds to step , where a first viewcell edge sharing the viewcell vertex CVV is selected and referenced by the variable EDGE.","Process flow proceeds to decision step  to determine if the edge EDGE is a (from-point) silhouette edge with respect to the inside corner mesh silhouette vertex MV.","If, in decision step , it is determined that EDGE is a from-MV silhouette edge, then process flow proceeds to step  to form the triangle between the point MV and the edge EDGE. This triangle is a candidate swept triangle between MV and the viewcell, but it must be compared to other swept triangle candidates that share the same viewcell edge.","Process flow proceeds to , where the comparison of these other swept triangle candidates begins. In this regard, the angle between the current swept triangle candidate TRIANGLE and the CURRENT_POLYGON (supporting polygon) incident on MV is measured. The value is stored in the variable ANGLE. Since TRIANGLE and CURRENT_POLYGON share a common edge, the angle can be measured at the edge, adopting the convention that the angle is the angle between the occluded sides of each polygon. The occluded side of a supporting polygon is the side that connects to the interior of the mesh polygon at the silhouette edge. The occluded side of the candidate swept triangle is the side that connects to the interior of the mesh polygons at the vertex MV. This angle is stored in the variable ANGLE.","Alternate embodiments are possible in which the orientation of the swept triangle and corresponding SE-MV wedge relative to neighboring wedges is examined. All wedges are oriented surfaces having a \u201cvisible\u201d side and an \u201cinvisible\u201d side. For SE-MV wedges the visible side is the unoccluded side (visible on this side as a result of being not occluded by mesh polygon beyond the corresponding first-order silhouette edge). For SV-ME wedges the visible side is the \u201ccontained\u201d side (visible as a result of being contained in the viewcell when looking through and beyond the corresponding inside-corner first-order silhouette vertex.","In one embodiment the SWEPT_TRIANGLE is constructed from MV viewcell edges which produce a SWEPT_TRIANGLE that has a containment orientation that is consistent with the occlusion orientation of an adjacent SE-MV wedge and consistent with the containment orientation of neighboring SV-ME wedges. SV-ME wedge which do not have this consistent orientation do not contribute to the continuous, conservative linearized umbral event surface.","The orientation of an SV-ME wedge is opposite to the orientation of the corresponding SV-ME supporting polygon. This inversion occurs as a result of the edges of the SV-ME supporting polygons being effectively \u201cprojected\u201d through the inside-corner first-order silhouette vertex to form the corresponding SV-ME wedge. (e.g. a particular SE-MV supporting polygon which has the containment shaft between the viewcell and the inside-corner first-order silhouette vertex \u201cbelow\u201d the supporting polygon in the negative Y direction will produce a corresponding SE-MV wedge which has its \u201ccontained\u201d or visible side in the positive Y direction.","Process flow proceeds to decision step , to determine if this angle (ANGLE) is less than the current value of CURRENT_ANGLE.","If, in decision step , it is determined that the current value of ANGLE is less than the value of CURRENT_ANGLE, then TRIANGLE is a candidate swept triangle and process flow proceeds to process -, which starts at step  in .","In step , the variable CURRENT-ANGLE is set to the value of ANGLE.","Process flow proceeds to step  to set the variable SWEPT_EDGE to refer to the edge EDGE.","Process flow proceeds to step  to set the variable SWEPT_TRIANGLE to reference the triangle TRIANGLE.","Process flow proceeds to decision step  to determine if any other edges sharing the current viewcell vertex CVV have been unprocessed.","If, in decision step , it is determined that unprocessed edges sharing the viewcell vertex remain, then process flow proceeds to process -, which returns the process flow to step  (), where the variable EDGE is set to reference the next viewcell edge sharing the vertex CVV. Process flow then returns to step  to generate the next candidate swept triangle and test it.","If, in decision step , is determined that no other unprocessed viewcell edges share the vertex, then process flow proceeds to step , where the CURRENT_POLYGON variable is set to reference the triangle SWEPT_TRIANGLE.","Process flow proceeds to step  to output the swept triangle SWEPT_TRIANGLE.","Process flow proceeds to step  to construct a SE-MV wedge from the swept triangle. Further details of this step is disclosed in .","Process flow then proceeds to process -, which starts at step  () to advance to the next connected viewcell vertex. Process flow then returns to step .","If, in decision step , it is determined that the viewcell edge is not a from-point silhouette edge from the point MV, then process flow proceeds to process -, which starts at step  () to select a remaining viewcell edge for processing.",{"@attributes":{"id":"p-0342","num":"0341"},"figref":"FIG. 5C"},"Alternate embodiments are possible in which SE-MV supporting polygons are identified by considering both the \u201csidedness orientation\u201d of the candidate supporting polygon (relative to the interior of the polygon mesh) and the orientation of the candidate supporting polygon relative to the viewcell vertices.","In one embodiment, mesh polygons are all assumed to be \u201coutside\u201d polygons which have their normal vector locally oriented away from the \u201cinside\u201d of the region contained by the polygon mesh. In such embodiments, all mesh polygons of a polygon mesh consistently have this same \u201csidedness\u201d orientation.","As previously described, a polygon is a planar structure which can have two sides, corresponding to the two sides of the plane containing the polygon. Exemplary embodiments include polygon meshes which are manifold or closed. Manifold meshes divide the volume of space in which they are embedded into an inside and an outside. In computer graphics, it is useful to employ manifold meshes in which the normal vector of each polygon in the mesh is locally oriented to face away from the inside of this enclosed volume. This can be called the \u201coutside\u201d side of the polygon. The opposite side can be called the \u201cinside\u201d side of the polygon. If all polygons have this consistent sidedness orientation in a mesh, then no inside side of a polygon should ever be visible from the outside.","In exemplary embodiments, it can be established that polygons of a mesh have the same sidedness orientation by examining the vertex orderings of adjacent polygons i.e., polygons which share an edge. (See Schneider, Philip J., Eberely, David H., \u201cGeometric Tools for Computer Graphics\u201d Morgan Kaufmann 2003 pp. 342-345, the entire contents of which are incorporated herein by reference). Let Fand Fbe two adjacent polygons sharing an edge comprised of two vertices Vand V. If vertices Vand Voccur in the order Vfollowed by Vfor polygon F, then they must occur in polygon Fin the order Vfollowed by V. Adjacent polygons in which shared edges have this ordering are said to have a consistent vertex ordering. Polygons with a consistent vertex ordering have the same sidedness orientation.","In one embodiment, a candidate SE-MV supporting polygon for an inside-corner first-order silhouette vertex is formed between a viewcell edge and the inside-corner first-order silhouette vertex. The candidate supporting polygon is given the same sidedness orientation as a backfacing mesh polygon sharing a first-order silhouette edge of the inside-corner first-order silhouette vertex. (Using this consistent sidedness orientation, for example, a person walking across the first-order silhouette edge on the \u201coutside\u201d surface of the backfacing mesh polygon would encounter the \u201coutside\u201d surface of the candidate supporting polygon). The orientation of the plane of each candidate supporting polygon is then examined relative to the viewcell vertices. If the plane of the candidate supporting polygon is not front-facing with respect to each viewcell vertex then the viewcell edge forming the candidate supporting polygon is a supporting viewcell edge, and the candidate supporting polygon is a supporting polygon.","According to some embodiments the process illustrated in  is entered from step . In step  a candidate supporting polygon is formed between the inside-corner first-order silhouette vertex and a viewcell edge (E). Process flow proceeds to step  to set the sidedness orientation of the candidate supporting polygon formed in step  to be the same as the backfacing component polygon sharing a first-order silhouette edge of the inside-corner first-order silhouette vertex. In exemplary embodiments, the sidedness orientation of the SE-MV supporting polygon can be set to be consistent with a previously determined adjacent SV-ME or SE-MV supporting polygon. Because the SE-MV supporting polygon shares an edge with these adjacent polygons the sidedness orientation can be set by insuring that the adjacent polygons have consistent vertex ordering.","Process flow proceeds to step  to determine if the candidate supporting polygon is not front-facing for each of the viewcell vertices. If, in decision step , it is determined that the candidate supporting polygon is not front-facing with respect to each viewcell vertex then process flow proceeds to step  to identify the viewcell edge (E) as a supporting viewcell edge and to identify the candidate supporting polygon as a supporting polygon.","If, in decision step , it is determined that the candidate supporting polygon is front-facing for any viewcell vertex then process flow proceeds to step  to identify the viewcell edge (E) as not a supporting viewcell edge and to identify the candidate supporting polygon as not a supporting polygon.",{"@attributes":{"id":"h-0010","num":"0000"},"figref":"FIG. 6A"},{"@attributes":{"id":"p-0351","num":"0350"},"figref":["FIG. 6A","FIG. 1","FIG. 6A","FIG. 1"],"b":["116","116"]},"In some embodiments, the process to construct SV-ME and SE-ME wedges from corresponding SV-ME and SE-ME supporting polygons starts at step , where the connecting edges of the supporting polygon are identified as those edges which have one vertex that is a vertex of the viewcell and another vertex that is a vertex of the polygon mesh.","Process flow proceeds to step , to construct rays from the connecting edges by extending the connecting edges in a semi-infinite fashion away from the viewcell starting at the corresponding vertices of the supported silhouette edge. If the supporting polygon is a triangle, then the two edges that connect the viewcell and the silhouette edge are extended. If the supporting polygon is a quadrangle (from , step ), then the diagonals connecting the viewcell edge and silhouette edge can be extended. Extending the diagonals produces a larger wedge that actually reflects the visibility from the viewcell edge through the silhouette edge.","Process flow proceeds to step  to connect the extended edges to the corresponding (supported) polygon mesh silhouette edge to form the semi-infinite SV-ME (or SE-ME) wedges.",{"@attributes":{"id":"h-0011","num":"0000"},"figref":"FIG. 6B"},{"@attributes":{"id":"p-0355","num":"0354"},"figref":["FIG. 6B","FIG. 1","FIG. 6B","FIG. 1"],"b":["135","135"]},"In some embodiments, the process of constructing a SE-MV wedge from the corresponding swept triangle starts at step , where the connecting edges of the swept triangle are identified as those edges which have one vertex that is a vertex of the viewcell and another vertex that is a vertex of the polygon mesh.","Process flow proceeds to step  to construct rays from the connecting edges by extending the these edges in a semi-infinite fashion away from the viewcell starting at the corresponding mesh silhouette vertex.","Process flow proceeds to step  to connect the extended edges to the corresponding polygon mesh inside corner silhouette vertex to form the semi-infinite wedge.","The process of  describe the construction of first-order wedges that are only restricted by their intersection with adjacent wedges on the silhouette contour. These may be called the initial wedges.","According to some embodiments, in subsequent processing, for example in the construction of first-order visibility maps, these initial wedges may later be intersected with mesh polygons and with other wedges. Initial wedges may also be explicitly intersected with other wedges to form umbral boundary polygons (UBPs), which bound the conservative from-viewcell polyhedral aggregate umbral volumes that contain (conservatively) occluded regions.",{"@attributes":{"id":"h-0012","num":"0000"},"figref":["FIG. 7","FIG. 11"]},{"@attributes":{"id":"p-0361","num":"0360"},"figref":"FIG. 7A","sub":["1","8 "],"b":"1"},"First-order silhouette edge A has one component polygon that is front facing for at least one viewcell vertex. This component polygon is the triangle formed by edge A and the mesh vertex labeled MV. The other component polygon for edge A is the triangle formed by edge A and the mesh vertex MV which is shown in . This component polygon is backfacing for all vertices V-Vof the viewcell. Note that these two component polygons sharing edge A are backfacing with respect to each other, making the edge A a locally supporting edge of the polygon mesh M and a first-order silhouette edge. It can be determined that the two component polygons sharing edge A are backfacing by selecting a first component polygon, e.g. the triangle formed by edge A and vertex MV, and determining if a vertex of the other component polygon which is not part of the shared edge, e.g. vertex MV in this case, is on the front side or the back side of the plane containing the first polygon. If the unshared vertex is on the back side of the other component polygon's plane then the two component polygons are backfacing, as in this case. This determination can be made using the plane equation as described in the definition of \u201cbackfacing\u201d provided in the glossary of terms. In some embodiments, the process illustrated in  is repeated for each edge included in polygon mesh M to identify each first order silhouette edge of polygon mesh M.",{"@attributes":{"id":"p-0363","num":"0362"},"figref":["FIG. 7B","FIG. 7A","FIG. 7A"],"b":["1","3","1","1"],"sub":["1","8 "]},{"@attributes":{"id":"p-0364","num":"0363"},"figref":["FIG. 7C","FIG. 7B"],"b":["3","3","1","2"]},"Component polygon T is backfacing for all vertices of the viewcell labeled VIEWCELL since all of the viewcell vertices are on the back side of the plane containing triangle T.","Component triangle T has at least one viewcell vertex that is on the front side of the plane containing triangle T, that is T is front facing with respect to at least one viewcell vertex.","Consequently, component triangles T and T meet two of the criteria required to make their shared edge a first-order silhouette edge with respect to the viewcell.","However the shared edge I, is not a first-order silhouette edge because the two component triangles are not backfacing with respect to each other. This can be determined by selecting triangle T and identifying a vertex of the other component triangle (T) that is not a vertex of the shared edge. In this case the vertex is P. The vertex P is on the front side of the plane containing the other component triangle T. This fact can be established using the plane equation of triangle T as described in the glossary of terms description for \u201cbackfacing\u201d.","Since T and T are not backfacing with respect to each other they would, in one embodiment, fail the decision test shown in the exemplary flowchart of  at steps  OR .",{"@attributes":{"id":"p-0370","num":"0369"},"figref":"FIG. 7D","sub":["4 ","8"]},{"@attributes":{"id":"p-0371","num":"0370"},"figref":["FIG. 7E","FIGS. 1, 4, and 6A"]},{"@attributes":{"id":"p-0372","num":"0371"},"figref":"FIG. 7F"},"Although  show wedges incident on first order silhouette edges A and B, further embodiments construct wedges for each first order silhouette edge included in the first order silhouette contour included in mesh M according to the processes illustrated in .",{"@attributes":{"id":"p-0374","num":"0373"},"figref":["FIG. 7G","FIG. 7D"],"b":"1"},{"@attributes":{"id":"p-0375","num":"0374"},"figref":["FIG. 7H","FIG. 7E"],"b":"1"},{"@attributes":{"id":"p-0376","num":"0375"},"figref":["FIG. 7I","FIG. 7E"],"b":"1"},{"@attributes":{"id":"p-0377","num":"0376"},"figref":["FIG. 7J","FIG. 7I","FIG. 7J"]},"One of the pivoted wedges is labeled SV-ME WA, which is also seen in . In  an additional pivoted wedge SV-ME WC is shown. This wedge is supported by the first-order silhouette edge labeled C, and the supporting viewcell vertex labeled SVVC.","The two pivoted wedges SV-ME WA and SV-ME WC share an outside corner vertex of a first-order silhouette edge. This vertex is labeled OCV. As prescribed in steps  and  of the exemplary flowchart of , in one embodiment pivoted polygons which share an outside corner vertex are intersected with each other.","Pivoted polygons which share an outside corner silhouette vertex and which pivot to the same supporting viewcell vertex will intersect each other exactly at a shared edge. In this case the shared edge is a ray extending from the shared vertex and on the line formed by the supporting viewcell vertex and the shared outside corner vertex. In this special case the two pivoted wedges restrict each other on the shared edge.","(Pivoted polygons which share an inside corner silhouette vertex and which pivot to the same supporting viewcell vertex also intersect each other exactly at the shared edge. In this case no swept supporting polygon exists and the corresponding swept wedge is not generated.)","In the general case, pivoted wedges sharing an outside corner vertex can pivot to different supporting viewcell vertices. In  wedge SV-ME WA is supported by viewcell vertex V, while SV-ME WC is supported by SVVC. In this case, the intersection of wedge SV-ME WA and SV-ME WC is the line segment labeled I. Line segment I divides wedge SV-ME WC into two parts. The proximal part of the subdivided wedge SV-ME WC is bounded by line segment I and the vertex labeled VE. A portion of this proximal part is occluded in this view.","This proximal part of wedge SV-ME WC is completely seen in , which shows the same objects as , from a different perspective. This proximal part is labeled SV-ME WCR in .","In general, the intersection of two pivoted wedges sharing an outside-corner vertex and pivoting to different supporting viewcell vertices will result in one of the wedges being restricted into a proximal portion [e.g., SV-ME WCR (indicating wedge C restricted)] and a distal portion. Only the proximal portion of such a locally restricted wedge is actually a from-viewcell umbral event surface. [Only this proximal portion is a polygon of the corresponding polyhedral aggregate umbra (PAU).] The distal portion, beyond the restriction and in a direction away from the viewcell does not represent a from-viewcell umbral event surface, since it is entirely on the unoccluded side of the adjacent wedge. In the example shown in  and , mesh polygons on both the unoccluded and the occluded side of the distal portion of SV-ME WC are actually unoccluded from viewcell vertex SVVA, and are therefore not occluded from the viewcell.","This local restriction of a pivoted wedge by an adjacent pivoted wedge sharing an outside corner silhouette vertex in some instances produces a substantially smaller wedge. This smaller, locally restricted wedge can require substantially less processing when it is submitted for the determination of on-wedge visibility since it has an additional containment boundary that limits processing (e.g. at step  in one embodiment using 2D mesh traversal process shown in exemplary flowchart ).","The local restriction process can therefore accelerate the determination of on-wedge visibility. Alternate embodiments which do not use this local restriction process can also be employed. Any wedges that have not been restricted by other wedges still intersect mesh polygons to produce discontinuity mesh segments. The determination of whether such a discontinuity segment is actually a from-viewcell umbral boundary is then made using the modified point-in polyhedron test described in the exemplary flowcharts of . This test accommodates both locally restricted and unrestricted wedges.","The preceding discussion assumes that the wedges employed are first-order wedges. Higher-order wedges are subjected to wedge-wedge intersection (restriction by other wedges) as described in one embodiment for example in step  of the exemplary flowchart showing a method for determining if a DM_SEG is an actual from-viewcell occlusion boundary segment.",{"@attributes":{"id":"p-0388","num":"0387"},"figref":["FIG. 8A","FIG. 5A","FIG. 5B","FIG. 7A","FIG. 7D"],"b":["1","1"],"sub":["4","8"]},{"@attributes":{"id":"p-0389","num":"0388"},"figref":["FIG. 8B","FIG. 5A","FIG. 5B","FIG. 6B","FIG. 7A","FIG. 7E"],"sub":["4","8"]},{"@attributes":{"id":"p-0390","num":"0389"},"figref":["FIG. 8C","FIG. 7A","FIG. 7F"]},{"@attributes":{"id":"p-0391","num":"0390"},"figref":["FIG. 8D","FIG. 8D","FIG. 8D"],"b":["1","1"]},{"@attributes":{"id":"p-0392","num":"0391"},"figref":["FIG. 8E","FIG. 8A"],"b":"1"},{"@attributes":{"id":"p-0393","num":"0392"},"figref":["FIG. 8F","FIG. 8B"]},{"@attributes":{"id":"p-0394","num":"0393"},"figref":["FIG. 8G","FIG. 8C"],"b":"1"},{"@attributes":{"id":"p-0395","num":"0394"},"figref":["FIG. 8H","FIG. 8D"],"b":"1"},{"@attributes":{"id":"p-0396","num":"0395"},"figref":["FIG. 8I","FIG. 8D"]},{"@attributes":{"id":"p-0397","num":"0396"},"figref":"FIG. 9A","b":"1"},"In Teller's prior-art method of linearized antipenumbra, Teller (1992), visibility event surfaces are approximated by intersecting only the planes of supporting polygons incident on portal edges and supported by source vertices wherein the source is an earlier portal in a sequence of portals. Theses supporting polygons correspond to the SV-ME supporting polygons (using the nomenclature of the present embodiments). Teller's method does not employ the corresponding SE-MV supporting polygons in the construction of umbral event surfaces, but the planes of these polygons.","In contrast, SV-ME wedges, as constructed by the present embodiments, are semi-infinite polygons, restricted laterally by the semi-infinite extension of the supporting polygon edges, which are rays. The SV-ME wedges are also restricted at the corresponding first-order silhouette edge. Teller \u201cwedges\u201d are actually planes that have no lateral restriction. The present embodiments of constructing \u201cTeller Wedges\u201d is to extend the planes of adjacent SV-ME wedges at an inside corner until the planes intersect.","In the following analysis, we show that by using visibility event surfaces constructed from both SV-ME and SE-MV supporting polygons, the present method can provide a significantly more precise from-region visibility solution than by using Teller's approach in which the planes of only one type of supporting polygon are intersected.","It must be emphasized that the method of Teller (1992 PhD Thesis U.C. Berkley) is designed only to provide a solution to the restricted visibility problem of visibility through a sequence of polygonal portals. Teller's method does not identify silhouette edges on which to construct visibility event surfaces, because in Teller's method, the edges supporting visibility event surfaces are limited to the edges of the portals. Since Teller's method does not apply the intersecting-planes method to construct visibility event surfaces on silhouette edges of general polygon meshes; the following analysis amounts to a theoretical comparison of Teller's intersecting-planes method if it were applied to the general problem of from-region visibility in polyhedral environments versus the present method of pivot-and-sweep visibility event surface construction, which is actually used in the more general visibility problem.",{"@attributes":{"id":"p-0402","num":"0401"},"figref":["FIG. 9B","FIG. 9A"]},{"@attributes":{"id":"p-0403","num":"0402"},"figref":["FIG. 9C","FIG. 9D","FIG. 9C","FIG. 9D","FIG. 9D"]},{"@attributes":{"id":"p-0404","num":"0403"},"figref":"FIG. 9D"},{"@attributes":{"id":"p-0405","num":"0404"},"figref":["FIG. 9D","FIG. 9C"]},"Flipbook Views of Identifying Conservative Supporting Polygons and Constructing Corresponding Wedges.","Subsets of , when viewed in specific sequences, provide flipbook views of the method of identifying conservative supporting polygons and constructing the corresponding wedges. These sequences are listed below:","Pivoted supporting polygon & wedge: View generally from behind viewcell: A, C, D, E.","Pivoted supporting polygon & wedge: View generally from in front of viewcell: B, D, G, H.","Swept supporting polygon & wedge: View generally from behind viewcell: A, A, A, B, (C showing combination of pivoted wedges and swept wedges).","Swept supporting polygon & wedge: View generally from in front of viewcell: B, B, E, F, (G showing combination of pivoted wedges and swept wedges).",{"@attributes":{"id":"p-0411","num":"0410"},"figref":["FIG. 10A","FIGS. 9A and 9B","FIG. 10A","FIG. 9C"]},"Two additional first-order SV-ME wedges, W and W, are also shown. The supporting viewcell vertex for wedges W and W is V. The intersection of these wedges is shown. Wedges intersect each other and other mesh polygons to form umbra boundary polygons (UBPs). These UBPs form the surface of first-order polyhedral aggregate umbrae (PAU). The volume of space enclosed by the PAU is first-order occluded from the corresponding viewcell. The UBPs corresponding to the intersections of the wedges are not explicitly shown in  but can be inferred from the intersection lines that are shown. Some of the wedges that would form the complete PAU are omitted so the interior structure of part of the first-order PAU can be seen (e.g. intersection of wedges W, W, SV-ME WA, SE-MV WAB, and SV-ME WB).",{"@attributes":{"id":"p-0413","num":"0412"},"figref":["FIG. 10B","FIG. 10A","FIG. 10B","FIG. 8I","FIG. 10A"],"b":["1","1","1","1","1","1"]},"In  the occluded side of the wedges is shown.","In  the unoccluded side of the corresponding UPBs is shown.",{"@attributes":{"id":"p-0416","num":"0415"},"figref":"FIG. 10B"},"UBP-A is formed by the intersection of the corresponding wedge (SV-ME WA) with wedge W (shown in ). UBP-A is also restricted by the intersection of SV-ME WA with wedge W shown in . W is completely hidden in , but the intersection of W and wedge SV-ME WA is shown as the edge labeled F in . Edge F is an edge of UBP-A. Additionally, UBP-A shares a common edge with UBP-AB (which is derived from SE-MV WAB, shown in ).","UBP-AB is formed by the intersection of SE-MV WAB with wedge W and with the wedge of UBP-D. UBP-AB shares a common edge with both UBP-A and UBP-B as a consequence of the sweep construction of the corresponding wedge SE-ME WAB. UBP-AB is also restricted by its intersection with the pivoted wedge corresponding to UBP-D (which is supported by mesh edge D).","UBP- is formed by the intersection of the corresponding pivoted wedge (W shown in , which has corresponding supporting viewcell vertex V) with W, and with SV-ME WA.","UPB-D is formed by the intersection of the wedge incident on first-order silhouette edge D (wedge is not shown, but having supporting viewcell vertex V) with wedges SV-ME B, SE-MV AB, and W as well as the wedge supported by edge E (wedge not shown).","The UBPs form the boundary of the PAU for M. Not all of UBPs forming the PAU of M are seen in the view given in .",{"@attributes":{"id":"p-0422","num":"0421"},"figref":["FIG. 10B","FIG. 26","FIG. 20A","FIG. 1"],"b":"140"},{"@attributes":{"id":"p-0423","num":"0422"},"figref":["FIG. 11A","FIG. 2","FIG. 2"],"b":["1","1","1","1","1","1","1","1"]},"The intersection of the first-order wedge WEDGE with the first-order silhouette edge is a compound silhouette vertex labeled CSV. The compound silhouette vertex corresponds to an inside corner of a compound silhouette contour. Using the terminology of catastrophe theory, the CSV corresponds to a t-vertex of the resulting manifold. Catastrophe theory includes the study of point singularities (e.g., CSVs or T-Vertex) and contour singularities (e.g., a first order silhouette edge) on manifold surfaces (e.g., manifold mesh).","Wedge is a first-order visibility event surface (a SV-ME wedge) that is supported by (incident on) the segment BV, which is the visible portion of the first-order silhouette edge B.","Thus WEDGE and WEDGE are both SV-ME wedges that intersect at the point CSV. Since WEDGE and WEDGE are constructed by the pivot process ( and ) of the pivot-and-sweep method using different supporting viewcell vertices (SVV and SVV, respectively) the two wedges do not join on-edge to form a continuous umbral visibility event surface.","The sweep process ( and , and ) of the present pivot-and-sweep method is used to construct SE-MV wedges (SE-MV WA and SE-MV WB) which join WEDGE and WEDGE into a continuous umbral visibility event surface. The wedge SE-MV WA is formed from the supporting SE-MV triangle generated between CSV, SVV, and the intervening vertex IVV on the supporting viewcell silhouette contour (SVSC). The extension of the two edges of this supporting triangle through the point CSV forms the semi-infinite wedge SE-MV WA. Similarly, the wedge SE-MV WB is formed from the supporting SE-MV (swept) triangle generated between CSV, SVV, and the intervening vertex IVV on the supporting viewcell silhouette contour (SVSC). The extension of the two edges of this supporting triangle through the point CSV forms the semi-infinite wedge SE-MV WB.","SE-MV WA and SE-MV WB connect at a common edge. SE-MV WA shares a common edge with WEDGE. SE-MV WB shares a common edge with WEDGE. The four connected wedges form part of the continuous first-order umbral visibility event surface incident on the silhouette edges A and BV. The view of  shows the occluded side of WEDGE (arrow O) and the unoccluded (from-viewcell, first-order visible) side of WEDGE (arrows U and U). The view of  shows the \u201ccontained\u201d (from-viewcell, first-order visible) side of SE-MV WA and SE-MV WB. As illustrated in  the intersection of wedges WEDGE, SE-MV WA, SE-MV WB, and WEDGE forms a continuous event surface with the arrows U and U indicating the unoccluded side of the even surface.  is a different view of the same structures shown in . In , the view is looking up to the occluded side of WEDGE  and the unoccluded side of WEDGE.  also shows the \u201ccontained\u201d (from-viewcell, first-order visible) side of SE-MV WA and SE-MV WB.","This concludes a description of a first embodiment. In this description, a process for generating first-order visibility event surfaces is presented. Additional embodiments specify the order of processing the polygons and edges of a mesh to generate the first-order visibility event surfaces. Further embodiments detail precisely how the visibility event surfaces are used to determine occluded polygons and polygon fragments. In the following detailed description of an alternate embodiment, a mesh traversal algorithm is disclosed in which first-order wedge construction and from-viewcell visibility determination are efficiently interleaved in a front-to-back visibility map construction algorithm which tends to have output-sensitive performance.",{"@attributes":{"id":"p-0430","num":"0429"},"figref":["FIG. 11A","FIG. 11A"],"b":"2"},{"@attributes":{"id":"p-0431","num":"0430"},"figref":["FIG. 12A","FIG. 12C"]},"In some embodiments, first-order, conservative linearized umbral event surfaces are constructed using the methods of the exemplary  through  are employed to generate conservative from-region visibility maps using the prior-art approach of discontinuity meshing or using the output-sensitive construction method described in  and related figures of the PCT patent application number PCT\/US2011\/042309.","In general, a PVS derived from a visibility map computed at a high precision results in less overdraw during runtime rendering, since more occluded polygon area is identified and removed.","However, each OB segment of the occlusion map also results in additional polygons being added to the visibility map\/PVS as a consequence of retriangulation of the original triangles intersected by the OB segment. (For one method of retriangluation see M. de Berg, M. van Dreveld et. al in \u201cComputational Geometry Algorithms and Applications, Springer c. 1997, page 45, the entire contents of which are incorporated by reference herein). This additional geometry tends to slow runtime rendering simply by increasing the number of primitives submitted to the initial geometry stages of the graphics pipeline.","In order to reduce the overall cost of runtime rendering, embodiments include a method of balancing the reduced cost of rasterization produced by an occluded region against the increased cost of geometry processing incurred because of the additional geometry introduced by the occluded region.","This method employs a heuristic called effective static occlusion (ESO) eliminate occluded regions which correspond to occluded regions of small area, especially if they introduce large numbers of additional triangles.","In a related method, the ESO is also used to guide the conservative simplification of occlusion boundaries, while attempting to maximize the surface area of the occluded region.","In some embodiments, the process of - is conducted as an off-line visibility precomputation. The result is an optimized from-region visibility map and related PVS. A related method of efficiently storing the results of the visibility map by labeling specific edge contours of the mesh polygons as silhouette edges associated with occluded regions having high effective occlusion is presented in conjunction with ,  and related figures. In this method, the visibility map\/PVS for a viewcell can be efficiently computed from the visibility map\/PVS of an adjacent (or containing) viewcell at runtime using the pre-labeled silhouette edges. The method of - includes steps to not only simplify occlusion boundaries of the visibility map, but also store this information as the corresponding simplified silhouette contours of the original mesh objects. This process is performed as precomputation to prepare for the actual incremental construction of visibility map\/PVS data using the runtime methods of  and related figures.","In some embodiments, process flow starts at step , upon encountering an occluded region (OR) of a visibility map.","Process flow proceeds to step  to determine the number of original mesh triangles completely occluded in this OR. In an alternative embodiment, the number of original mesh triangles that are partially occluded is determined. Also in step , the image-space area of the completely and partially occluded original mesh triangles in this OR is estimated using a viewpoint inside the viewcell for which the visibility map was constructed.","Process flow proceeds to step  to determine the number of additional triangles (in the unoccluded region bordering the current OR) that are produced by retriangulation of the original mesh triangles at the OR boundary.","Process flow proceeds to step , where the measured values determined in step  and  are employed to determine the value of a heuristic variable called the effective static occlusion (ESO) for the entire OR.","In some embodiments the ESO is a variable that is determined to be in some proportion to the image space area (ISA) of the occluded triangles\/triangle fragments in the OR. In further embodiments, the ESO heuristic also includes a coefficient of this term which, reflects the runtime cost of rasterization.","According to some embodiments, the ESO is determined to be in some inverse proportion to the number of additional unoccluded triangles produced in the mesh as a result of retriangulation at the OR border. In additional embodiments the ESO heuristic also includes a coefficient of this term, which reflects the runtime cost of geometry processing as well as another coefficient reflecting the storage and transmission cost of the additional triangles. Although using the incremental runtime PVS construction method of  and related figures, the storage\/transmission costs of these additional triangles can be eliminated by conducting the retriangulation at runtime.","According to some embodiments, the ESO is represented by the following formula: ESO=F(number of occluded polygons, image space area of occluded polygons, 1\/number of new visible polygons added at occlusion boundary). In further embodiments, the ESO can be represented by any desired function.","Other embodiments of the present method are possible which include additional variables in the determination of the \u201ceffectiveness\u201d of a particular OR in producing occlusion that actually results in improved runtime rendering performance.","Process flow proceeds from  to decision step  to determine if the ESO is greater than or equal to a specified value, VALUE. If, in decision step , it is determined that the ESO is not greater than or equal to the value VALUE, then process flow proceeds to step , where the entire OR is removed from the VM and the original, unretriangulated, triangles of the polygon mesh are restored. In this regard, in embodiments, when the ESO associated with OR is not greater than or equal to VALUE , the OR is not an effective occlusion boundary. For example, as the number of new triangles created by the retriangulation process increases, the ESO value decreases indicating that it may not be efficient to keep the OR, which causes the retriangulation process. Process flow terminates at .","Alternate embodiments are possible in which only those original mesh triangles that are partially occluded are restored but the OR is kept. The partially occluded triangles are those which produce additional triangles by retriangluation at the boundary of the OR (the OB). By restoring only these triangles, the number of triangles in the VM is reduced but completely occluded triangles are still removed from the VM\/PVS.","If, on the other hand, in decision step  it is determined that the ESO is greater than or equal to the value VALUE, then process flow proceeds to step  to determine if the additional number of triangles constructed in the unoccluded region bordering the current OR as a result of the OR boundary, exceeds a number VALUE.","If, in decision step , it is determined that the additional number of triangles constructed is not greater than or equal to VALUE, then process flow proceeds to step , where the current OR (and the triangles produced by retriangulation on the boundary of the current OR) are kept unmodified. In this regard, if the number of additional triangles is small, it may be more efficient to keep the OR.","If, on the other hand, it is determined in step  that the additional number of triangles constructed is greater than or equal to VALUE, then process flow proceeds to step .","Steps - implement a method attempting to reduce the number of additional triangles induced on the adjacent unoccluded region by conservatively removing the triangles in the OR and adjacent exposed regions of the VM that intersect the occlusion boundary of the OR. Using this approach, the triangles in the adjacent unoccluded regions that previously were restricted at the OR occlusion boundary are now considered to be completely unoccluded, and the restricted triangles are replaced with the original mesh triangles. This conservatively increases the area of the unoccluded from the corresponding viewcell and reduces the number of triangles by eliminating retriangulation at the occlusion boundary.","Process flow proceeds to step , where the BOUNDARY_TRIANGLES are identified as those triangles intersecting\/bordering the occlusion boundary of the VM occlusion region.","Process flow proceeds to step , where the BOUNDARY_TRIANGLES are removed from the OR and the corresponding (retriangulated) boundary triangles are removed from the adjacent exposed region of the VM and replaced with the original, larger, untriangulated mesh triangles. These larger triangles include some surface area that was originally included only in the occluded region, but after the untriangulation step of  the larger original triangles are considered to be completely unoccluded, even though parts of the triangles may be inside the occluded region.","Process flow proceeds to step , where the new boundary between the OR and the adjacent exposed regions is set to the polyline boundary formed by the triangles of the OR that are adjacent to the original mesh triangles identified in step . This step conservatively redefines the boundary of the OR to be inside the originally computed OR. It also potentially reduces the complexity of the boundary and the number of triangles in the OR by conservatively redefining the OR boundary.","Subsequent decision step  is similar to decision step  and in this case determines if the simple, conservative redefinition of the occlusion boundary along the edges of original mesh triangles, as determined in steps -, has resulted in an occlusion region that occludes a sufficient number of triangles and\/or occludes triangles having a sufficient image-space area.","If, in decision step , it is determined that the number and\/or image-space area of the occluded triangles exceeds a predetermined value (e.g. VALUE), then process flow proceeds to step , where the occlusion region and adjacent exposed regions are retained in their current state.","If, on the other hand, it is determined that the number of, or ISA (image-space surface area) of triangles in the OR do not exceed the predetermined value then process flow proceeds to process -, which starts at decision step  () to determine if the level of triangle subdivision in the OR exceeds a certain value (e.g. VALUE).","If, in decision step , it is determined that the level of subdivision of the triangles in the OR do not exceed the predetermined value, then process flow proceeds to step , where the triangles of the OR are further subdivided (e.g., using midpoint-edge subdivision to create 4 triangles from 1). This procedure, along with the test of , allows very large triangles in the OR, which also extend into adjacent exposed regions, to be progressively subdivided into smaller triangles until the number of triangles and\/or ISA of triangles in the occluded region exceed the value VALUE (step ) or until the level of subdivision exceeds the VALUE. Process flow proceeds from  to process -, which returns process flow to step  ().","If, in decision step , it is determined that the level of subdivision of triangles in the OR exceeds a predetermined value, then process flow proceeds to step  to conservatively simplify the ORs occlusion boundary. These simplifications are executed to increase the ESO of the OR.","In step  the in-order list of silhouette edges forming the silhouette contour (from which the occlusion boundary (OB) segments comprising the boundary of the current OR were constructed) is stored as an array SIL_LIST. For any inside-corner silhouette vertex (whether simple or compound) the associated viewcell silhouette edges of the VSVSC are also stored in the SIL_LIST, since they also have corresponding wedges which contribute to the OR boundary.","Process flow proceeds to step  to store the initial segment of the array in the variable SIL.","Process flow proceeds to step  to store the next segment in the SIL_LIST in the variable NEXT_SIL.","Process flow proceeds to decision step,  to determine if SIL and NEXT_SIL form an outside (convex) corner of the silhouette contour.","If, in decision step , it is determined that SIL and NEXT_SIL form an outside (convex) corner of the silhouette contour (and corresponding OR boundary), then process flow proceeds to process -, which starts at step , to construct a SV-ME wedge on a line segment connecting the non-shared vertices of SIL and NEXT_SIL. Using the previously specified method of VM map construction, the wedge is intersected with the mesh polygons to form DM_SEGS which are tested for from-viewcell visibility to determine if they are valid OB_SEGS of the VM. These new conservative OB_SEGS may intersect mesh triangles and other OB_SEGS not intersected by the original OR boundary.","Process flow proceeds to step  the ESO of the region between the new OB and the original OB is determined and stored in the variable D_ESO (indicating the ESO of the difference region between the old and new OBs.). This ESO estimates the \u201ceffectiveness\u201d of the occlusion region that has just been removed by the conservative repositioning of the OB.","Process flow proceeds to step  to determine if the D_ESO is less than a predetermined value VALUE.","If in decision step  it is determined that the D_ESO is less than the predetermined VALUE, then process flow proceeds to step . If the D_ESO of the difference region is low, then the number of triangles occluded in the difference region is small and\/or they have a relatively small surface area. Also a low D_ESO value may indicate that there are many partially occluded mesh triangles in the difference region that will cause additional geometry by retriangulation.","In step , reached because the difference region has a relatively low D_ESO value, the conservative boundary of the OR recomputed in step  is saved as the new OR boundary for the region of the map.","Process flow proceeds to step , where the retriangulated triangles introduced by the new OR boundary are also optionally saved. Again mesh triangles that are partially occluded can be conservatively considered to be unoccluded, thereby reducing the amount of new geometry that would have been produced by retriangluation.","Process flow proceeds to step , where the data for the two silhouette edges SIL and NEXT_SIL, which have been effectively collapsed into a single conservative silhouette edge is removed from the linked list representing the labeled from-viewcell silhouette contour of the mesh object. As discussed in detail, in conjunction with  and related figures of the embodiments, the labeled silhouette edge data is stored as a link list of data structures corresponding to simple silhouette vertices and CSVs. In step , a single node of the linked list (CONTOUR_NODE) is modified to reflect the collapse of the edge.","If in decision step  it is determined that the D_ESO of the difference region is not less than a predetermined value, then process flow proceeds to step , where the original OB is retained since the difference region tends to effectively occlude relatively large numbers of mesh triangles or portions of mesh triangles having a relatively large surface area without introducing too many additional triangles because of retriangulation at the OR boundary.","Process flow proceeds to step , where the SIL is set to NEXT_SIL and process flow proceeds to process -, which returns process flow to step , where the SIL_LIST is effectively incremented by setting NEXT_SIL to the next unprocessed edge in SIL_LIST.","If, in decision step , it is determined that SIL and NEXT_SIL form an inside corner of the silhouette contour corresponding to the boundary of the OR, then process flow proceeds to process -, which starts at step  () to determine if the current contour being processed corresponding to the SIL_LIST is an outer contour of a occluded region (OR) or an inside contour of the region.","If, in decision step  it determined that the current contour is an outer contour, then process flow proceeds to step  to set a variable BESTSIDE to a value INSIDE.","If, in decision step  it determined that the current contour is not an outer contour, then is the current contour is an inner contour and process flow proceeds to step  to set a variable BESTSIDE to a value OUTSIDE.","Process flow proceeds step , where two silhouette edges are identified lying on both sides of SIL or SIL_NEXT such that the edges are as close as possible on the contour (in the contour array SIL_LIST) and such that the corresponding wedge planes of the edges intersect to form a line that intersects on the BESTSIDE of the contour; where BESTSIDE is the INSIDE of the contour for outer contours and the OUTSIDE of the contour for inner contours. This insures a conservative simplification of the contour in the region of the inside corner silhouette vertex (either simple or compound). This process may \u201ccollapse\u201d more than one edge at a time with the span designated by the variable N.","Process flow proceeds to step , where the ESO of the region between the new OB and the original OB is determined and stored in the variable D_ESO (indicating the ESO of the difference region between the old and new OBs.). This ESO estimates the \u201ceffectiveness\u201d of the occlusion region that has just been removed by the conservative repositioning of the OB.","Process flow proceeds to decision step  to determine if the D_ESO is less than a predetermined value VALUE.","If in decision step  it is determined that the D_ESO is less than the predetermined VALUE, then process flow proceeds to step . If the D_ESO of the difference region is low then the number of triangles occluded in the difference region is small and\/or they have a relatively small surface area. Also a low D_ESO value may indicate that there are many partially occluded mesh triangles in the difference region that will cause additional geometry by retriangulation.","In step , reached because the difference region has a relatively low D_ESO value, the conservative boundary of the OR recomputed in step  is saved as the new OR boundary for the region of the map and the retriangulated triangles introduced by the new OR boundary are also optionally saved. Again, mesh triangles that are partially occluded can be conservatively considered to be unoccluded, thereby reducing the amount of new geometry that would have been produced by retriangluation.","Process flow proceeds to step , the data for the N silhouette edges, which have been effectively collapsed into a conservative span comprising extended silhouette edges, is removed from the linked list representing the labeled from-viewcell silhouette contour of the mesh object and removed from SIL_LIST. As discussed in detail in conjunction with  and related figures, the labeled silhouette edge data is stored as a link list of data structures corresponding to simple silhouette vertices and CSVs. Later in the process flow, in step , a node of the linked list (CONTOUR_NODE) may be modified to indicate the collapsed edge.","If in decision step  it is determined that the D_ESO of the difference region is not less than a predetermined value, then process flow proceeds to step , where the original OB is retained since the difference region tends to effectively occlude relatively large numbers of mesh triangles or portions of mesh triangles having a relatively large surface area without introducing too many additional triangles because of retriangulation at the OR boundary.","Process flow proceeds to step , where the SIL is set to NEXT_SIL+N, N representing the span of edges replaced or collapsed by the conservative simplification process of step . Process flow proceeds to process -, which returns process flow to step , where the SIL_LIST is effectively incremented by setting NEXT_SIL to the next unprocessed edge in SIL_LIST.","The method of  applies to both simple and compound silhouette contours. For compound silhouette contours the inside corner silhouette vertices may be CSVs. The method is applied as a post-process to a completed visibility map. Consequently, the silhouette edges corresponding to OB_SEGS are labeled. As previously described, during the construction of the from-viewcell visibility map a single first-order silhouette edge on a triangle mesh may ultimately be subdivided into multiple visible subsegments. In addition, each of these segments may a support an SV-ME wedge that intersects another silhouette edge to produce a CSV with its associated SE-MV swept wedges. In the present method, the SIL_LIST is comprised of the individual subsegments reflecting the visible complexity of the simple or compound silhouette contour.",{"@attributes":{"id":"p-0486","num":"0485"},"figref":"FIG. 12D"},"MESH F is a triangle mesh representing a box-like object having six sides. Each rectangular face of MESH F is modeled as two triangles. There are 12 triangles in MESH F, only 6 are shown, the other 6 are occluded and not shown. One triangle of MESH F is formed by the vertices labeled V, V, and V.","MESH G is a polygon mesh having 21 polygons, some of these polygons are represented as triangles while others are represented as rectangles. Some of these polygons are occluded in the view.",{"@attributes":{"id":"p-0489","num":"0488"},"figref":["FIG. 12E","FIG. 12D","FIG. 12E"],"b":["1","2","3"]},"In one embodiment, the construction of the occlusion boundary segments bounding OR-F is constructed according to the 3D mesh traversal process of  and related figures. Specifically these occlusion boundary segments are added to the intersected polygon in step  of the exemplary flowchart shown in .","In another step of the 3D mesh traversal process, depicted as step  in the exemplary flowchart of , the triangles intersecting or containing occlusion boundary segments are re-triangulated into occluded regions and unoccluded regions.  shows the result of this exemplary re-triangulation of original mesh triangle V-V-V using the occlusion boundary segments of OR-G. During this re-triangulation the original triangle V-V-V is re-partitioned into 10 new triangles, one for each of the 7 occlusion boundary edges of OR-G, and 3 additional triangles that result because OR-G further partitions triangle V-V-V into 3 regions defined by vertices of OR-G.","Since occlusion region OR-G is completely inside a single original triangle (V-V-V) of MESH F, it contains no complete triangles. That is, the polygon mesh (MESH G) that induces the occlusion boundary of OR-G on MESH F does not completely occlude even a single mesh polygon of MESH F, using VIEWCELL as source. Thus, the number of mesh triangles completely occluded by OR-G is 0.","Triangle V-V-V is partially occluded by MESH G. This region of partial occlusion is the area of OR-G. In this example the surface area of OR-G is small relative to the surface area of the containing triangle V-V-V.","In this example OR-G occludes no mesh polygons completely, and occludes a relatively small surface area of only a single polygon. Since the effective static occlusion (ESO) of an occlusion region is in some direct proportion to the number of polygons completely occluded by the occlusion region and the surface are of completely or partially occluded polygons, the ESO of OR-G is not substantially increased by these factors.","The ESO of an occlusion region varies in some indirect proportion to the number of new polygons created by re-triangulation at the corresponding occlusion boundary. In the example of OR-G, re-triangulation at the occlusion boundary results in a single original triangle being partitioned into 10 new triangles.","Thus, in this example, both the direct proportional factors of the ESO (number of polygons completely occluded and surface area of occluded polygons) as well as the inverse proportional factors (e.g. the number of new polygons generated by re-triangulation at the occlusion boundary) will tend to produce a relatively low value for the ESO of OR-G.","As described in the exemplary flowchart of , in one embodiment, OR-G which has a low ESO, can be removed completely from the visibility map (step ). Alternatively, according to the exemplary flowchart of , or the occlusion boundary of OR-G can be simplified and the ESO for the simplified occlusion boundary redetermined. In one method of simplification the occlusion boundary is made smaller by equating it with the boundary of a connected set of completely occluded original mesh triangles within the occlusion region (steps -). This results in redefining any partially occluded polygons as exposed. In this case only completely occluded polygons are eliminated from the from-viewcell visibility map and corresponding PVS. In another embodiment of simplification, the actual occlusion boundary of the occlusion region is conservatively simplified (steps -).","A method of determining an ESO, in one embodiment as described employs factors that measure the occlusion. The determination of the ESO also includes in one embodiment factors which reflect the number of new polygons generated by re-partitioning at the corresponding occlusion boundary. The method accommodates embodiments in which the relative importance of these individual factors can be tuned based on coefficients which assign weights to each factor.","Principles of Operation: Efficient Storage of DeltaPVS Information Using Labeled Silhouette Edges and Incremental Runtime Construction of Visibility Map\/PVS.","PVS data, especially if derived from high-precision visibility maps, can have high storage costs. As previously described, deltaPVS is a prior-art method of reducing storage costs for PVS data by storing only the difference between the PVS of adjacent viewcells. In the deltaPVS method of Chhugani et. al (2005), the deltaPVS is a list of polygon primitive IDs (called deltaI) stored for each viewcell boundary. Run-length encoding is used to store the deltaI, which is comprised of two components: deltaG+ (newly visible primitives), and deltaG\u2212 (newly occluded primitives). Even with this compression, however, the deltaPVS data is large. For a powerplant model of 13 million triangles and 500,000 viewcells, 7 GB is required to store the delta-PVS object IDs.","Embodiments include a method of efficiently storing DeltaPVS using an implicit scheme in which silhouette edges of the model that result in significant exposure or significant occlusion of geometry for a particular viewcell-viewcell transition are identified by comparing the corresponding visibility maps. These dynamically occluding or dynamically exposing silhouette contours are identified and labeled in an offline visibility preprocess. The labeling employs an efficient run length encoding which exploits the definition of first-order silhouette edges to achieve significant algorithmic compression. This run-length labeling method is presented in detail in conjunction with  and related figures.","During runtime, the visibility map\/PVS for a viewcell is constructed from the visibility map\/PVS of an adjacent viewcell by:","1) Conducting a modified 3D mesh traversal of the adjacent viewcell's visibility map. This fast traversal uses the labeled significantly occluding or significantly exposing silhouette contours to rapidly construct new occlusion boundaries on the newly constructed visibility map. Details are given in conjunction with , ,  and related figures.","Using these new occlusion boundaries, the newly occluded triangles are not traversed during the runtime traversal process. Since only traversed triangles are represented in the new PVS, this approach effectively allows the generation of a PVS that has the newly occluded triangles removed Details given in conjunction with ,  and related figures. This scheme significantly reduces the cost of deltaG\u2212 data storage and transmission, by eliminating the need to explicitly store deltaG\u2212 information (e.g. as large lists of newly occluded triangles) in most cases.","2) Adding newly exposed geometry as deltaG+ packets, which may include ID and\/or actual geometry, associated with the particular viewcell transition wherein the newly exposed geometry is \u201cattached\u201d to boundaries of the VM associated with new exposure. Because the from-region visibility maps can be computed at viewcell-to-triangle-fragment precision many of the newly visible triangles may be fragments of the original mesh triangles. The present method of incremental construction of a VM from an adjacent VM at runtime accommodates the technique of retriangulating triangles during the runtime traversal\/VM construction which avoids having to store triangle fragments in the deltaG+ packets.","In some embodiments, the method uses viewcells that are arranged hierarchically. Relatively large parent viewcells contain smaller child viewcells. The VM\/PVS for a large viewcell is constructed from the VM\/PVS for an adjacent large viewcell and additional deltaG+ data using the previously described incremental construction method. Since the deltaG+ data is for a viewcell transition between large viewcells tends to be large, the data is naturally clustered, thus reducing the number of disk accesses required to load the deltaG+.","The VM\/PVS for child (contained) viewcells is derived from the VM\/PVS of the parent (containing) viewcell by conducting the modified 3D mesh traversal at runtime. Because the VM\/PVS of the child viewcell is a subset of the VM\/PVS of the parent, the runtime 3D mesh traversal method used to construct the child VM from the parent's typically only uses labeled dynamically occluding silhouette contours to construct occlusion boundaries, which bypass the geometry occluded in the transition from parent to child. This allows construction of more precise VM\/PVS for runtime display without the need for additional deltaG+ data.","The hierarchical organization of the viewcells also facilitates efficient streaming of deltaPVS data. In some embodiments, only deltaG+ data packets corresponding to the transition between relatively large parent viewcells needs to be transmitted. The VM\/PVS for the corresponding child viewcells is constructed from the parent's at runtime using only deltaG\u2212 information (generated at runtime from the labeled silhouette information). Streaming only parent deltaG+ information is more efficient since typically the overall time required to seek, access, and transmit a unit of data decreases with increasing size of the packet.","Using the embedded labeled silhouette information and associated deltaPVS data packets, a precision-controlled VM\/PVS is efficiently constructed from the VM\/PVS of an adjacent (sibling) viewcell (using deltaG+ packets and runtime 3D mesh traversal to bypass the newly occluded triangles). For a parent viewcell to child viewcell transition, deltaG+ packets are not required since the entire VMS\/PVS for the child viewcell is derivable by a retraversal of the parent's VM, using the labeled silhouette edge hint information to bypass newly occluded triangles.","According to some embodiments, the runtime process is conducted as a prefetch operation. During interactive walkthrough, the location of the current viewpoint is used to predict likely future viewpoint locations based on the connectivity of the viewcells (which are also navigation cells of the model) as well as current viewpoint velocity and other factors. Using this informed speculative prefetch, the VM\/PVS of parent viewcells in the reachable vicinity of the current viewcell (i.e. the viewcell containing a current actual or predicted viewpoint) are incrementally constructed and maintained. The set of viewcells that are reachable in a specified period of time from the current viewcell may be constrained factors including intrinsic navigational constraints of a viewer's motion, including such factors as the maximum actual or predicted viewpoint velocity and acceleration and turning rates and accelerations. The local structure of the modeled environment including obstacles and other collision constraints can also influence the rate at which neighboring viewcells in the reachable vicinity of a viewpoint can be visited. In some embodiments, the construction of the child viewcell VM\/PVS is deferred until the viewcell is closer to the child viewcell, since the construction generally does not require streaming of deltaG+ data.","The method realizes an efficient visibility-based codec for streaming delivery of interactive content via local or remote server. The codec exploits the intrinsic dynamic or temporal visibility coherence of interactive walkthroughs to minimize the required bandwidth for on-demand streaming.","Unlike image-based streaming methods, the bandwidth required to stream the visibility event geometry and texture information is relatively independent of display resolution. In fact, the present method tends to increase runtime rendering performance at high resolutions since, at a relatively small CPU, cost of incrementally constructing VMs at runtime, it maintains a very precise PVS that improves both geometry and rasterization GPU performance. Moreover, since the codec can be implemented as a speculative prefetch; its performance is, unlike image-based streaming methods, relatively independent of client-server connection latency.",{"@attributes":{"id":"h-0015","num":"0000"},"figref":["FIG. 13A","FIG. 13B","FIG. 13C"]},"As described in the preceding section, embodiments include a method to identify significant dynamically occluding or dynamically exposing mesh silhouette edges and labeling them in an offline preprocess; and later using the labeled silhouette edges to effect incremental VM\/PVS construction during runtime.","As described in conjunction with , from-region silhouette contours may also be simplified based on the effective static occlusion of the corresponding occlusion boundary segments in the VM. As shown in , the simplified VM boundary can be stored as a simplified labeled silhouette contour (from which the simplified VM boundary will later be constructed at runtime).",{"@attributes":{"id":"p-0514","num":"0513"},"figref":"FIG. 13A"},"In some embodiments, a simple silhouette contour of a triangle manifold mesh is a connected sequence of edges comprising a polyline. The polyline may or may not form a cycle.","Assuming that a simple silhouette contour is unoccluded, then using only the definition of first-order, from-viewcell silhouette edge; an entire simple silhouette contour can be efficiently labeled by labeling a single edge of the contour. Given a single labeled starting edge (or a data structure pointing to this edge) the entire connected first-order silhouette contour can be identified by simply finding the connected edges and determining which connected edge is a first-order silhouette edge. This fact is employed in the present method to significantly reduce the cost of storing labeled silhouette contours by identifying most silhouette contour edges at runtime.","A from-viewcell silhouette contour may be a compound silhouette contour. A compound silhouette contour results when a from-region visibility event surface (e.g., a UBP) intersects a (different) silhouette edge. This intersection is a compound silhouette vertex or CSV.","Each inside corner vertex of a contour, whether simple or compound, can give rise to more than one from-region SE-MV umbral event surface (wedge\/UBP) as a result of the sweep process. Consequently, there may be more event surfaces incident on a contour than the number of edges or vertices in the contour.","The data structures used to label silhouette contours are organized, in some embodiments, as arrays of data structures corresponding to actual event surfaces incident on actual silhouette edges and vertices. Because adjacent silhouette edges can be rapidly identified at runtime and because UBPs (and the corresponding OB_SEGS of visibility maps) can be generated at runtime; many of the elements of the array do not actually need to be stored.","The reduced storage cost produced by the intrinsic algorithmic compression realized by identifying\/generating contour elements at runtime can be balanced against the runtime cost of generating this information using the contour node information of , discussed directly. This information is used to speed the generation the unstored data at runtime.  and  show embodiments of data structures used to label silhouette contours.","In some embodiments, a data structure \u201cContour\u201d is stored for each contour. The data structure contains three fields referring to a specific mesh object, an edge of the mesh, and a vertex of the edge. In storage form, all references are integer indices to specific arrays of elements, though at runtime these may be changed to pointers. The structure \u201cContour\u201d also contains the field int node_array, which is an index to a specific array of data structures of the type Contour_Node. The \u201cstruct Contour\u201d also contains an integer field num_nodes which gives the length of the node_array for the contour.","The data structure \u201cContour\u201d also contains an integer field, VMinfo, which is an index to a specific element in an array of data structures of type VM_Info.","VM_info (which is described in detail in a later part of this specification) contains information providing the specific mesh and mesh triangle that is intersected by the UBP associate the Contour_Node. By precomputing this information and storing it with the initial silhouette element of a span all of the visibility map OB_SEGS associated with the entire span of silhouette elements encoded by the Contour_Node can be rapidly constructed at runtime if the associated UBPs intersect the same triangle mesh. (This process, which exploits the intrinsic coherence of intersecting polyhedral manifolds, is described in detail in conjunction with  and related figures.)","The data structure \u201cContour\u201d also contains an integer field \u201clast_contour\u201d which is an index into an array of \u201cContour\u201d structures indicating a specific \u201cContour\u201d to which the current \u201cContour\u201d is connected at its tail end. The data structure \u201cContour\u201d also contains an integer field \u201cnext_contour\u201d which is an index into an array of \u201cContour\u201d structures indicating a specific \u201cContour\u201d to which the current \u201cContour\u201d is connected at the head end.","The data structure \u201cContour_Node\u201d stores information for individual elements of the contour. As previously indicated, since many of the event surfaces incident on a contour can be generated algorithmically they do not need to be stored explicitly. Thus the array of Contour_Nodes referenced by Contour typically has many fewer elements than the actual silhouette contour has edges and umbral event surfaces.","The data structure \u201cContour_Node\u201d contains a character type field \u201cnode_type\u201d which indicates what type of silhouette contour information is contained in the node. If the node corresponds to an outside corner of a silhouette contour then the value of the field is set to 1. If the node corresponds to a simple inside corner of the silhouette contour then the value of the field is set to 2. If the node corresponds to a compound silhouette vertex (CSV) then the value of the field is set to 3.","The data structure \u201cContour_Node\u201d also contains a character type field, span_type indicating the type of span corresponding to the node. If the node represents a span of outside corner silhouette edges for the contour then the value is set to 1. If the node represents a span that may contain both outside and inside corners then the value is set to 2. If the node represents a span of silhouette edges that are to be \u201cskipped\u201d in order to simplify the contour (as described in conjunction with  steps  and ).","The data structure \u201cContour_Node\u201d also contains an integer field indicating the length of the span represented. In general this corresponds to the number of umbral event surfaces generated on the silhouette edges and vertices encountered in the span and therefore may be a larger than the number of silhouette vertices in the span.","The data structure \u201cContour_Node\u201d also contains an integer field, ninfo which is an index to a specific element in an array of data structures which store additional information for the node, depending on the value of the field node_type.","The data structure \u201cOC_Info\u201d may store additional data referenced by a Contour_Node having node_type equal to 1. The character field svsc stores a reference to an index of a specific vertex of the viewcell which supports the umbral visibility event surface (UBP) for corresponding to the first silhouette edge in the span. This optional information could speed the runtime generation of UBPs but increases the storage size. Since a single contour can be used to generate multiple UBPs at runtime corresponding to multiple viewcells, this additional data may be constructed once at runtime rather than being stored as labeled silhouette contour information with the database.","The data structure \u201cIC_Info\u201d may store additional data referenced by a Contour_Node having the node_type equal to 2. The optional character field ic_type is a hint which indicates which type of construction strategy (pivot-and_sweep or intersection of adjacent SV-ME planes) should be employed to generate SE-MV event surfaces incident on the inside corner at runtime. While this decision can be made at runtime using the previously described heuristics, the runtime test can be avoided using prestored data. Note that this hint data can also be stored for an entire span of silhouette vertices by storing a different value for Contour_Node, span_type (value of 4 indicates pivot-and-sweep for all inside corner silhouette vertices in span vs value of 5 indicates intersection of adjacent SV-ME planes for all inside corner silhouette vertices in span).","The data structure \u201cIC_Info\u201d may contain an optional character array field SVSC[] indicating hints for the specific viewcell vertices which form the supporting viewcell silhouette contour on which the SE-MV event surfaces of the inside corner silhouette vertex are constructed.","All of the information contained in the IC_Info data structure can be generated at runtime and therefore does not actually have to be stored with the mesh database.","The data structure CSV_Info may store additional data referenced by a Contour_Node having the node_type equal to 3. The integer field \u201cmesh\u201d stores an index to a specific triangle mesh that is intersected by the current UBP in the contour. The integer field \u201cedge\u201d stores an index to a specific edge of the intersected triangle mesh. These two fields are used to define the CSV which is formed at the intersection of the UBP supported by the current element of the silhouette contour and another silhouette edge. Once again, the fields \u201cmesh\u201d and \u201cedge\u201d are optional since in about half the cases (cases in which the current contour is being processed in a direction which causes the associated UBP\/VM contour to \u201cwalk off\u201d the more distant mesh) the silhouette edge intersected by the UBP to form the CSV is easily determined. In other cases in which the contour is being processed at runtime in a direction that causes the UBPs and corresponding VM OB_SEGS are being constructed to encounter a new mesh","The \u201cmesh\u201d and \u201cedge\u201d fields will substantially reduce the runtime costs of incremental construction of a new VM contour.","The data structure CSV_Info may also optionally store the x,y,z values of the CSV in the double array field point[]. The CSV_Info structure may also contain the optional fields char ic_type and char SVSC[] as previously described in conjunction with the data structure IC_Info.","Since all of the fields of the CSV_Info are optional not every Contour_Node may link to a CSV_Info data structure, once again reducing storage costs.","The data structure VM_Info stores information about visibility map occlusion boundary segments that are associated with the initial silhouette elements of a Contour data structure. The storage of both the mesh and edge where the UBPs associated with these silhouette elements intersect the VM can be used to rapidly compute the corresponding OB_SEG of the VM at runtime. Once the initial OB_SEG is determined, subsequent OB_SEGs of the VM occlusion boundary polyline corresponding to the silhouette Contour can be rapidly generated at runtime. This is similar to the storage of mesh and edge references in CSV_Info data structures, which is used to accelerate runtime construction of VM data.","The data structures for labeled silhouette contours shown in  and , and  are also reproduced below.",{"@attributes":{"id":"p-0539","num":"0538"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"315pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"struct Contour"},{"entry":"{"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"280pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["int mesh;","\/\/ index to mesh"]},{"entry":["int edge;","\/\/ index to edge of mesh"]},{"entry":["int vertex;","\/\/ index to starting vertex of contour"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"259pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["int node_array;","\/\/ index to array of Contour_Nodes"]},{"entry":["int num_nodes;","\/\/ length of coutour_node array"]},{"entry":["int VMinfo;","\/\/ index to VM_Info array"]},{"entry":["int last_contour;","\/\/ index to next connected contour, head connected"]},{"entry":["int next_contour;","\/\/ index to last connected contour, tail connected"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"315pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"char next_edge_type \/\/ if ==1 then next edge is next connected silhouette edge"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"252pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\u2009\/\/ if ==0 then next edge is next previously labeled edge"]},{"entry":["char contour_type","\/\/ if == 1 dynamically occluding, if == 0 dynamically exposing"]},{"entry":[{},"\u2009\/\/ if ==2 hybrid contour"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"315pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"};"},{"entry":"struct Contour_Node"},{"entry":"{"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"252pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["char node_type;","\/\/ 1 = outside corner, 2=simple inside corner, 3=CSV"]},{"entry":["char span_type;","\/\/ 1= run of outside corners only"]},{"entry":[{},"\/\/ 2= run may contain outside and inside corners"]},{"entry":[{},"\/\/ 3=skip span_length segments & connect to next vertex-"]},{"entry":[{},"\/\/ forming simplified silhouette contour edge"]},{"entry":[{},"\/\/ 4= construct SE-MV on all inside corners of run using pivot and sweep"]},{"entry":[{},"\/\/ 5= construct all inside corner event surfaces by intersection adjacent SV-ME planes"]},{"entry":["int span_length;","\/\/ number of umbral event surfaces (UBPs) generated on contour until next node"]},{"entry":["int ninfo;","\/\/ if node_type==1 then ninfo is index to OC_Info array"]},{"entry":[{},"\/\/ if node_type==2 then ninfo is index to IC_Info array"]},{"entry":[{},"\/\/ if node_type==3 then ninfo is index to CSV_Info array"]},{"entry":"};"},{"entry":"Struct VM_Info"},{"entry":"{"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"273pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["int mesh;","\/\/ index to mesh containing OB_SEG corresponding to first silhouette element in run"]},{"entry":["int triangle;","\/\/ index to triangle containing OB_SEG corresponding to first silhouette element in run"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"315pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"int retriangulate_hint \/\/ bitmask indicating if mesh triangles intersected by OB_SEGS for 31 -"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"252pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\u2002\/\/ Contour_Nodes of Contour should be retriangulated at runtime 0=no, 1=yes"]},{"entry":[{},"\u2009\/\/ Last Bit indicates VM dynamic boundary type: 0==occluding 1==exposing"]},{"entry":["float point[3];","\u2009\/\/ x,y,z value of initial vertex if it is formed by intersection with other OB_SEG"]},{"entry":"};"},{"entry":["struct OC_Info","\/\/ information for an outside corner node of"]},{"entry":"{"},{"entry":["char svsc;","\/\/ (optional) hint for specific viewcell vertex forming UBP"]},{"entry":"};"},{"entry":"struct IC_Info"},{"entry":"{"},{"entry":["char ic_type;","\/\/(optional)1= form SE-MVs by pivot and sweep"]},{"entry":[{},"\/\/(optional)2= intersect planes of adjacent SV-MEs"]},{"entry":["char SVSC[4];","\/\/ (optional) hints for specific viewcell edges forming SVSC"]},{"entry":"};"},{"entry":"struct CSV_Info"},{"entry":"{"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"280pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["int mesh;","\/\/ (optional) index to mesh containing intersected edge"]},{"entry":["int edge;","\/\/ (optional) index to intersected edge"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"252pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["char ic_type;","\/\/(optional)1= hint-form SE-MVs by pivot and sweep"]},{"entry":[{},"\/\/(optional)2= hint- intersect planes of adjacent SV-MEs"]},{"entry":["char SVSC[4];","\/\/ (optional) hints for specific viewcell edges forming SVSC"]},{"entry":["double point[3];","\/\/ (optional) precomputed x,y,z values of vertex of CSV"]},{"entry":"};"},{"entry":["struct tri_seed","\/\/ index\/pointer to specific triangles of model used to initiate"]},{"entry":[{},"\/\/ simplified mesh traversal (FIG. 21) to construct viewcell B visibility"]},{"entry":[{},"\/\/map from viewcell A visibility map"]},{"entry":"{"},{"entry":["int tri_count;","\/\/ number of triangles in seed"]},{"entry":["int* mesh_array;","\/\/ sequence of mesh ids"]},{"entry":["int* tri_array;","\/\/ sequence of triangle ids"]},{"entry":"};"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"315pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"struct DeltaGplussubmesh_attach_polyline"},{"entry":"\/\/ precomputed list of mesh edges for attaching submesh and original mesh"},{"entry":"{"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"252pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["int contour;","\/\/ reference to a specific Contour"]},{"entry":["char attach_type;","\/\/ if ==0 free edges of submesh attached to free edges of mainmesh"]},{"entry":[{},"\/\/ if ==1, free edges of submesh attached to listed edges of mainmesh"]},{"entry":[{},"\/\/ if ==2, free edges of submesh linked to free edges of mainmesh"]},{"entry":[{},"\/\/ if == 3, free edges of submesh linked to listed edges of mainmesh"]},{"entry":["int submesh;","\/\/ reference to attaching submesh"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"315pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"Int edgenumber;\/\/ number of edges in the attaching polyline"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"84pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"231pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Int* submesh_edgelist;","\/\/ ordered list of edges in submesh which attach"]},{"entry":["int* mainmesh_edgelist;","\/\/ ordered list of edges in mainmesh to which attaches"]},{"entry":"};"},{"entry":"struct Triangle"},{"entry":"{"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"252pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["int global_id;","\/\/ global_id"]},{"entry":["int vertex[3];","\/\/ index of 3 vertices in DeltaGplussubmesh.vertexp array"]},{"entry":["int edge[3];","\/\/ index of 3 edges in DeltaGplussubmesh.edgep array"]},{"entry":"};"},{"entry":"Struct Edge"},{"entry":"{"},{"entry":["int global_id;","\/\/ global id"]},{"entry":["int vertex[2];","\/\/ index of two vertices in DeltaGplussubmesh.vertexp array"]},{"entry":"};"},{"entry":"struct vertex"},{"entry":"{"},{"entry":["int global_id;","\/\/ global id"]},{"entry":["float point[3];","\/\/ x,y,z value of vertex"]},{"entry":"};"},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}]}},"br":{},"figref":"FIG. 13D"},"In one embodiment, a deltaG+ packet of information may be associated with each viewcell-viewcell transition. The exemplary data structure DeltaGplus_Header includes fields indicating the starting (viewcell_start) and ending viewcell (viewcell_end) as well as a specific face (transition_face) for the associated transition. Another data element, deltaGplus_array, is a reference to an array of DeltaGplus data structures which actually contain or reference the mesh geometry. In some embodiments in which the deltaGplus_array is stored in main memory, it may be accessed through a pointer. In other instances the deltaGplus_array variable may be an index to an array of arrays, e.g. for the disc storage form of the deltaG+ data. Another field deltaGplus_count stores the number of DeltaGplus data structures in the deltaGplus_array. An additional field packet_size indicates the storage and transmission byte size of the associated information.","A DeltaGplus_Header references one or more DeltaGplus data structures, which in turn references the geometry, material, texture information for the corresponding polygon mesh. Exemplary data structures for this information are shown as data structures Triangle, Edge, vertex.",{"@attributes":{"id":"p-0542","num":"0541"},"figref":["FIG. 14A","FIG. 14B","FIG. 17A","FIG. 17B"]},"In some embodiments, connected regions of the manifold triangles meshes that are exposed from one viewcell but occluded from a contiguous (or contained) viewcell are called delta regions (DR).","A delta region corresponding to a connected region of a manifold triangle mesh that is occluded when viewed from viewcell B, but not occluded when viewed from viewcell A (i.e., is in the visibility map of viewcell A) is designated DRoAB (delta region of occlusion from A to B).","This is the same as the delta region corresponding to a connected region of a manifold triangle mesh that is exposed when viewed from viewcell A (i.e., is in the visibility map of viewcell A) but occluded when viewed from viewcell B. Such a delta region is designated as DReBA (delta region of exposure from B to A).","Of course DRoAB=DReBA.","Thus, while the method of  and  shows the determination of DRoAB, (the determination of a delta region of occlusion from viewcell A to viewcell B), the method is applied to determine a delta region of exposure by reversing the order of the viewcells being processed.",{"@attributes":{"id":"p-0548","num":"0547"},"figref":["FIG. 14A","FIG. 14B"]},{"@attributes":{"id":"p-0549","num":"0548"},"figref":"FIG. 14B","b":["1410","1410"]},"In some embodiments, the shaded hexagonal figure labeled \u201cOCCLUSION REGION VIEWCELL A\u201d is the from-viewcell occlusion region of the simple occluder shown in  generated using viewcell A as the source and using the method of first-order visibility map construction. Note that the occlusion region has one more edge than the occluder O as a result of the SE-MV wedge generated on the only inside corner of the original 5-sided occluder O.","Similarly, the unshaded hexagonal figure labeled \u201cOCCLUSION REGION VIEWCELL B\u201d is the from-viewcell occlusion region of the simple occluder shown in  generated using viewcell B as the source and using the method of first-order visibility map construction.","The portions of the OCCLUSION REGION VIEWCELL B that are outside the OCCLUSION REGION VIEWCELL A are labeled as DRAB (delta region of occlusion from A to B) and DRBA (delta region of exposure from B to A).","The region labeled  is outside both occlusion regions and since it is inside the region , which is visible from viewcell A and viewcell B, the region  is visible from both viewcell A and viewcell B and is not a delta region.",{"@attributes":{"id":"p-0554","num":"0553"},"figref":["FIG. 15","FIG. 14B","FIG. 14"],"sub":["O","E"],"b":["1510","1410","1520","1510","1520"]},"Accordingly, in some embodiments, when moving from view region B to view region A, a delta packet would include the difference in visible portions between view region B and view region A (i.e., delta region), which is DRAB. Therefore, by transmitting delta packets that only include the delta regions, the bandwidth requirements for transmitting graphics information is reduced since the entire set of visible graphic elements for each viewcell need not be retransmitted.",{"@attributes":{"id":"h-0017","num":"0000"},"figref":["FIG. 16A","FIG. 16B"]},"The flowchart of  shows a method of rapidly identifying all of the edges of a simple or compound, from-viewcell silhouette contour given a few edges of the contour that have been labeled using the data structures of .","The method of  exploits the facts that silhouette contours generally form polylines on manifold meshes. Since the meshes are represented as directed graphs with associated connectivity information (using winged-edge or similar data structures) the identification of edges connected to other edges is simplified.","Turning now to , and using the data structures of  in which data elements are stored in arrays and accessed by indices to these arrays (alternate embodiments may cast these references as runtime pointers). In some embodiments, process flow starts at step , where the current vertex c_vertex is identified using the index contour.vertex from the data structure for the current Contour. This is the edge number of the mesh contour.mesh.","Process flow proceeds to step , where the current edge is similarly accessed using the indeed contour.edge. Also in step  an integer used to update an index into an array of Contour_Node types, ni, is set to 0.","Process flow proceeds to step  to access the current contour node, c_node using the index contour.node_array[ni].","Process flow proceeds to decision step  to determine if the c_node.type is not 3. If the type is not 3, then the current node represents data for a simple contour node and process flow proceeds to step  to set a counter segi to 0.","Process flow proceeds to decision step  to determine if the c_node.span_type is equal to 2. If the c_node.span_type is equal to 2 then the segments of the current contour span may contain both outside corner and inside corner from-viewcell silhouette vertices and process flow proceeds to step .","In decision step  it is determined if the vertex shared by c_edge and next_edge is an inside-corner silhouette vertex using the method of identifying inside corner simple silhouette vertices previously specified.","If, in decision step , it is determined that the two silhouette edges form an inside corner then process flow proceeds to step .","In step , the integer value p is set to equal the number of SE-MV wedges incident on the inside corner vertex as determined by applying the sweep construction of SE-MV wedges (step ).","Process flow proceeds to step , where the counter segi, which represents the number of visibility event surfaces constructed for the contour span, is incremented by the number of SE-MV event surfaces incident on the CSV.","Process flow proceeds to decision step  to determine if the value of segi is equal to the span length of the current contour node.","If, in decision step , it is determined that the value of segi is equal to the span length, then the span has been processed and process flow proceeds to decision step .","In decision step , it is determined if the value of the integer variable ni, which is the index of the current contour node for the contour is equal to the number of nodes in the contour.","If in decision step  it is determined that that the current node is the last node of the contour then process flow proceeds to step  in which the next contour is processed. Process flow terminates at step .","If, on the other hand, it is determined in decision step , that the current node is not the last node of the contour then process flow proceeds to step .","In step , the node counter is advanced which is used in step  to access the next node.","If in, decision step  it is determined that the span_type of the current node indicates that no inside-corner nodes exist on the span, then process flow proceeds to step .","Likewise if, in decision step , it is determined that the current silhouette edge and the next silhouette edge do not form an inside corner, then process flow proceeds to step .","In step , the VM segments corresponding to the single SV-ME UBP incident on the current edge are formed (using the method of ,  and related figures. Process flow proceeds to step , to increment the variable segi by 1, consistent with the single umbral visibility event surface constructed on the silhouette edge.","Process flow proceeds from step  to decision step , which was already described.","If, in decision step , it is determined that the type of node is type , consistent with a compound silhouette vertex (CSV), then process flow continues to process -, which starts at step  ().","Process flow proceeds to step  to reference additional data in a CSV_Info node using an array index stored in cnode.ninfo. This information gives the mesh number of the more distant mesh containing the CSV.","Process flow proceeds to step  to access the edge number of the edge intersected by the current UBP (wherein the intersection is the current CSV) is accessed through the CSV_Info.","Process flow proceeds to  to calculate the CSV as the intersection of the current UBP and the C_EDGE. Alternately, this value may be precalculated and stored in the floating point CSV_Info.point[] field of the corresponding CSV_Info structure.","Process flow proceeds step  the C_Vertex is set to the index of the next vertex after the (on the unoccluded side) of the CSV, and process flow proceeds to process -, which returns process flow to step .","Overall, the method of  allows multiple edges of a silhouette contour to be identified using only a few labeled edges. Consequently, this labeling scheme uses very little storage. The method exploits the natural coherence of silhouette contours to facilitate the rapid runtime generation of VM segments from a few labeled silhouette edges and associated hint information. This runtime incremental construction of VM\/PVS using the labeled silhouette edges is discussed in detail in conjunction with  and .",{"@attributes":{"id":"p-0583","num":"0582"},"figref":["FIG. 17A","FIG. 17B","FIG. 17C","FIG. 17D"]},"The flowchart of , , and  show an embodiment of a method of identifying connected regions of manifold triangle meshes, called regions of interest or ROI, that reflect a change in visibility during a viewcell transition (e.g., from VC A to VC B).","This flowchart also shows a method of identifying the silhouette contour(s) corresponding to the occlusion boundary (OB) or boundaries which define a ROI. Further, this flowchart shows a method of labeling these silhouette contours (using the Contour data structure and associated data structures of ) and storing additional associated information with the contours (e.g. VM_Info data of ).","According to some embodiments, the method of - is conducted as an offline or precomputation process. The method can identify the deltaG+ and deltaG\u2212 components of the delta-PVS data, which can be used during a runtime process to incrementally construct a new PVS from an existing one. Alternatively, the method of - can identify and label silhouette contours corresponding to the boundaries of ROI. This labeled silhouette contour information can be later used to construct deltaG+ and deltaG\u2212 data at runtime. This approach can require considerably less storage\/transmission resources than explicitly storing\/transmitting the deltaG+ and deltaG\u2212 packets.","If one of the viewcells is completely contained in the other, then the ROI corresponds to the regions visible only from the contained viewcell. The labeled silhouette contours corresponding to these type of ROI are used, in some embodiments, to construct the VM of the child viewcell from the VM of the parent at runtime (using a hinted, simplified 3D mesh traversal), thereby avoiding in select cases the explicit storage of deltaG\u2212 information).","If the two viewcells share a face and one is not contained within the other (a relationship termed \u201ccontiguous\u201d), then the ROI correspond to delta regions (DR). For a viewcell transition from VC A to VC B (called an AB transition), the DR are of two types. One type of delta region, DRAB (delta region of occlusion from A to B) contains mesh triangle fragments visible from viewcell A but not B. The DRAB are also DRBA (delta region of exposure from B to A). Likewise, DRAB=DRBA. The DRAB corresponds to deltaG\u2212 data for the AB transition, while the DRAB corresponds to deltaG+ data for the AB transition. The labeled silhouette contours corresponding to the boundaries of these ROI can be used to construct the deltaG+ and or deltaG\u2212 data at runtime (also using a hinted, simplified 3D mesh traversal), thereby avoiding, in select cases, the explicit storage of deltaG+ and deltaG\u2212 polygon fragment information.","Turning now to -, in the first phase of the process, the type of ROI to be identified for a particular viewcell transition are specified. The ROI are specific regions of particular unified visibility map representing the viewcell transition. A unified visibility map for an AB transition is a visibility map containing the mesh triangle fragments visible from viewcell A and viewcell B, and the OB_SEGS of the from-viewcell VM for each of the viewcells.","In some embodiments, process flow starts at step, , to determine if the viewcells for which the delta-visibility information for particular viewcell transition is to be determined have a parent-child (containing-contained) relationship.","If in decision step , it is determined that the viewcells for which the delta visibility information is to be determined have a parent-child relationship, then process flow proceeds to step .","In step , the ROI to be identified are regions visible from the contained (child) viewcell for the specific viewcell transition. Since the VM for a child viewcell is always a subset of the parent viewcell, the child VM is constructed, in some embodiments, using explicit deltaG\u2212 information computed for the transition. However, if the child viewcell is significantly smaller than the parent viewcell then the corresponding deltaG\u2212 information will be relatively large. Alternatively, in the present method the regions of interest (ROI) for such a transition can be set to those regions visible only from the child viewcell. By identifying the seed triangles and boundaries of these regions, the VM\/PVS of the child viewcell can often be determined from the VM of the parent using much less information by conducting the simplified hinted 3D mesh traversal on the unified visibility map starting with the seed triangles.","If, in decision step , it is determined that the viewcells for which the delta visibility information is to be determined do not have a parent-child relationship, then process flow proceeds to step .","Step  indicates that the two viewcells are contiguous (the viewcells share a face and one is not contained within the other). Parent-child, and contiguous relationships are two special arrangements of two viewcells in which the transition from one viewcell to another occurs in a volume of space that is completely contained in the union of the two viewcells. Using these two arrangements of viewcells, the ROI, as constructed later in the process, are guaranteed to completely and conservatively reflect the newly visible and newly exposed regions (e.g., since there are no \u201cgaps\u201d between the related viewcells, no transiently visible geometry is missed).","Process flow proceeds to step  to indicate that the ROI are delta-regions of visibility (DR).","Process flow proceeds to decision step  to enumerate the various types of DR. If in decision step , it is determined that the DR to be identified are visible from VC A and not visible from viewcell B then, process flow proceeds to step .","In step , the ROI to be identified are DRAB and DRBA.","If in decision step , it is determined that the DR to be identified are visible from VC B and not visible from viewcell A, then process flow proceeds to step .","In step , the ROI to be identified are DRAB and DRBA.","In some embodiments, the steps  through  only enumerate the types of ROI that are to be identified for a particular viewcell transition, depending on the relationship between the transitioning viewcells and the desired use of the delta-visibility information. For a particular transition between contiguous viewcells A and B in the AB direction, both DRAB and DRAB ROI types are typically identified. Together, these two types of ROI completely describe the visibility change (delta-VM\/delta-PVS) for the viewcell transition.","Beginning in step , the actual identification of these ROI regions in the unified visibility map commences.","In step , the VM for a viewcell comprising the union of viewcell A and viewcell B is determined (e.g. using any of the previously described methods of from-viewcell VM construction). Any superset of this VM can also be used as the starting VM on which the OB_SEGS corresponding to the from-viewcell visibility map of both viewcells for the transition is later constructed (step ). This fact allows the determination of delta-visibility information (either explicit deltaG packets or corresponding labeled contour data) to be solved using an efficient hierarchical decomposition of the from-region visibility problem. In this hierarchical approach, the triangle fragments visible from any viewcell containing both viewcell A and viewcell B can be used as the VM on which the unified VM for viewcell A and viewcell B is constructed. (This fact can also be used to reduce delta-PVS storage requirements since the delta-PVS data for many viewcell transitions can ultimately be generated from the data for a single unified visibility map corresponding to a viewcell containing the other viewcells).","Process flow proceeds to step , where the OB_SEGs corresponding the from-viewcell visibility map determined from viewcell A and the OB_SEGs corresponding the from-viewcell visibility map determined from viewcell B are constructed on the triangle fragments visible from the viewcell (A+B). The set of triangle fragments visible from viewcell (A+B) together with the OB_SEGs from viewcell A and the OB_SEGs from viewcell B is called the unified visibility map for viewcell A and B. The construction of these OB_SEGS, in some embodiments, employs the previously described 3D\/2D mesh traversal method ( and related figures) for from-viewcell VM construction.","Of course, if viewcell A is a parent of viewcell B then the visibility map from viewcell (A+B) constructed in step  already contains of all of the mesh triangle fragments visible from viewcell A as well as the OB_SEGS corresponding to the from-viewcell A silhouette contours. The OB_SEGs corresponding to viewcell B are added in step . In the case where the two viewcells are contiguous, and\/or the starting VM being used is the superset of VM(A+B) then both sets of OB_SEGS must generally be constructed in step .","Process flow proceeds from step  to process -, which starts at step  (). In step , the unified visibility map is traversed to define the VM regions formed by the arrangement of OB_SEGs from both viewcell A and viewcell B. In this traversal, a triangle is selected and traversal proceeds to the boundary formed by the OB_SEGs. In this step, the traversal of the 3D mesh occurs as a breadth-first traversal of an already constructed unified visibility map. Traversal proceeds to silhouette contours and the corresponding occlusion boundaries where traversal is restricted. This simple method of traversal insures that all triangles\/fragments of a particular ROI are traversed to the boundaries of the ROI, even if the ROI has interior holes or spans multiple separate triangle meshes. (The previous construction of the VM may ultimately \u201cfuse\u201d parts of separate triangle meshes into a single ROI with interior holes corresponding to unoccluded regions of more distant mesh triangles visible through holes in a closer triangle mesh bounded by interior silhouette contours.)","Process flow proceeds to step  to determine if any untraversed triangles remain in the current traversal\/VM region (i.e., untraversed triangles connected to the current triangle wherein the connection does not require crossing an occlusion boundary). If in step , it is determined that untraversed triangles exist in the current traversal, then process flow proceeds to step , next triangle in the traversal.","If, on the other hand, it is determined in decision step  that no triangles remain in the current traversal then process flow proceeds to step  as all triangles of the current VM region have been traversed.","In step , it is determined if the unified VM region identified in the traversal steps  and  is visible from viewcell A, viewcell B, or both. In some embodiments, this is determined using the simplified point-in-polyhedron test of . In the case of a parent-child related viewcell transition, this test can be simplified somewhat since all of the traversed regions are visible from the parent viewcell.","Alternate embodiments are possible in which the visibility of a single point in the VM region is first determined (step  and )) before a traversal is initiated in the region (step ). This approach allows VM regions that are not ROI to be identified without a full traversal of the region.","Process flow proceeds to decision step  to determine if the traversed region of the VM corresponds to a region of interest (ROI) previously established in the earlier steps - for the specific viewcell transition. This is determined by comparing the result of step  (e.g. visible from A, from B, from both, or from neither; the latter only being possible if the VM being used is a superset of VM(A+B)); with the definition of the ROI determined in the earlier steps -.","If, in decision step , it is determined that the traversed region of the unified VM is not an ROI, then process flow proceeds to decision step  to determine if there are any untraversed triangles in the VM.","If, in decision step  it is determined that any untraversed triangles remain in the unified visibility map, then process flow proceeds to step , where the next triangle in the unified VM (belonging a new VM region) is selected for processing.","If, in decision step , it is determined that no untraversed triangles remain in the unified VM (no more unified VM regions to process) then process flow proceeds to step . Process flow terminates at .","If, in decision step , it is determined that the traversed region of the VM is an ROI, then process flow proceeds to step .","In decision step , it is determined if the current ROI is a delta region of occlusion (DR_O) for the viewcell transition. If the ROI is a DR, then process flow proceeds to step .","In step , the effective static occlusion of the ROI is determined using the metrics previously described for an occluded region. The value of a variable called the effective dynamic visibility (EDV) is set to the ESO of the ROI.","If, in decision step , it is determined that the current ROI is not a DR, then process flow proceeds to step .","In decision step , it is determined if the current ROI is a delta region of occlusion (DR_E) for the viewcell transition. If the ROI is a DR, then process flow proceeds to step .","In step , the effective static occlusion (ESO) of the occluded regions surrounding the current ROI (called the surrounding occluded regions or SOR) is determined using the metrics and previously described for an occluded region. The value of the variable called the effective dynamic visibility is set to the aggregate ESO of the SOR.","If, in decision step , it is determined that the ROI is not a delta region of exposure, then process flow proceeds to step .","In decision step , it is determined if the current ROI corresponds to region visible from a child viewcell for the specific parent-to-child viewcell transition. If the ROI is a child region, then process flow proceeds to step .","In step , the effective static occlusion (ESO) of the occluded regions surrounding the current ROI (called the surrounding occluded regions or SOR) is determined using the metrics and previously described for an occluded region. The value of the variable called the effective dynamic visibility is set to the aggregate ESO of the SOR. Note that the identical processing occurs for the case of a DRand a child ROI but they are differentiated here for the sake of exposition.","Following steps , , or , process flow proceeds to step .","In decision step , it is determined if the EDV (a measure of the \u201ceffectiveness\u201d or efficiency of the current ROI in representing delta visibility for the specific viewcell transition) for a ROI is greater than a predetermined value (e.g. VALUE).","If, in decision step , it is determined that the EDV for a ROI is not greater than a predetermined value (VALUE), then process flow proceeds to step .","In decision step  it is determined if the boundary of the current region of interest (and the corresponding silhouette contour) can be significantly simplified (e.g. using the method of , in which the ESO is used as a metric to direct the conservative simplification of the boundary). If the ROI is a DRthen the method of  can be applied directly to the region. If the ROI is a DRor Child region then the method of  is applied to occluded regions surrounding the current ROI (the SOR). The SOR may be defined as the occlusion regions immediately adjacent to the ROI. Optionally the SOR may include other occlusion regions connected to this set of SOR. This approach allows the conservative simplification process to spread into adjacent areas in order to ultimately achieve a sufficiently simplified ROI.","If, in decision step , it is determined that the boundary can be simplified, then the EDV of the new conservative representation of the region bounded by the simplified occlusion boundary is determined in decision step .","If, on the other hand, it is determined that the boundary of the current ROI cannot be simplified to achieve a target EDV value, then process flow proceeds to step .","In step , the current ROI is determined to have a low EDV and therefore, is ignored as a significant component of delta-visibility for the current viewcell transition. In this step if the ROI corresponds to a DRAB then the corresponding mesh triangles inside the region are conservatively included in the VM for viewcell A. The original triangles are included without the new triangles that would have been induced by the boundary segments of the DR. If the current ROI corresponds to a DRAB then the corresponding mesh triangles of the region are conservatively included in the VM for viewcell B. The original triangles are included without the new triangles that would have been induced by the boundary segments of the DR.","If the unified visibility map ROI corresponds to a parent-child viewcell transition and the EDV of the region is low, then the geometry of the surrounding occluded regions is conservatively included in the ROI, and the EDV of the expanded region can be recomputed. As with the case of a DR, the SOR region may be optionally extended into adjacent areas beyond the immediately bordering SOR. This approach can identify extreme cases in which the parent and child VM do not differ significantly. In such cases the child ROI is removed completely.","Steps , , and  together allow the ESV of the region to be determined and if the value of the ESV is too low, attempts can be made to conservatively simplify the boundary and thereby increase the ESV. If the ESV remains below a predetermined value then the ROI is not considered to correspond to a significant region of delta-visibility for the viewcell transition and the viewcell transition can be ignored.","Using the ESV (obtained from the ESO) as metric of the effectiveness of an ROI significantly reduces the storage and compute times required for the method. This is true because in many cases small regions of occlusion or exposure would otherwise induce large numbers of new triangles surrounding the ROI because of retriangulation at the ROI boundary. These regions tend to have a low ESO and therefore would not be considered effective occluding (or exposing) regions using the present method. Instead, for example, the newly visible set of primitives for a specific AB transition are simply conservatively to the VM\/PVS for viewcell A.","If, in decision step , it is determined that the EDO of the current ROI exceeds a predetermined value (e.g. VALUE), then process flow proceeds to process -, which starts at step .","In step , the storage size of the deltaG+ and\/or deltaG\u2212 (which may be deltaI-information comprising pointer or index information referencing actual newly occluded polygons), or child viewcell data (if the viewcell transition is parent-to-child) is estimated and the value of the variable SS is set in some direct proportion to this storage size. ROI containing many triangles\/triangle fragments tend to have a high storage cost for the corresponding deltaG+ or deltaG\u2212 packets. The alternate storage format used by the present method replaces explicit storage of the deltaG packets with labeling of the silhouette contour\/VM boundaries that define the corresponding ROI. The actual deltaG information is generated only when needed using a simplified 3D mesh traversal which employs the unified VM region boundaries generated from the labeled silhouette contour information for the specific viewcell transition.","Process flow proceeds to step , where the value of SS is compared to a predetermined value (e.g. VALUE). If, in decision step  it is determined that the value of SS is not greater than VALUE, then process flow proceeds to step .","In step , the deltaG data for the ROI is stored directly and process flow proceeds to decision step .","Decision step  is identical to the previously described step .","If, in decision step  it is determined that the value of SS is greater than the predetermined value VALUE, then process flow proceeds to step .","Steps  through  are steps to identify the silhouette contours corresponding to the OB_SEGS that form the boundaries (both outer boundaries and inner boundaries, since the ROI may contain holes) of the ROI. In these steps the corresponding silhouette contours (which are edges and vertices of the original triangle mesh plus some additional edges corresponding to SE-MV wedges at inside corner simple and compound silhouette vertices) are labeled and seed triangles, one for each connected component of a ROI is identified and stored.","Beginning at step , the OB_SEGS from viewcell A and the OB_SEGS from viewcell B forming the outer boundary of the ROI and the silhouette contours corresponding to these OB_SEGS are identified.","Process flow proceeds to step , where the OB_SEGS bounding the ROI are intersected with each other and the intersection points are designated as IP(S) and stored with the corresponding VM_INFO data structure for the corresponding silhouette contour (data structure given in ).","Process flow proceeds to step , where the silhouette contours corresponding to the outer boundary of the ROI are labeled and stored with the mesh, (including optionally DeltaGplus_attach_polyline info) using the data structures previously described in conjunction with  and  and .","Process flow proceeds to step , where the OB_SEGS from viewcell A and the OB_SEGS from viewcell B forming the inner boundaries of the ROI and the silhouette contours corresponding to these OB_SEGS are identified.","Process flow proceeds to step , where the OB_SEGS forming the inner boundaries of the ROI are intersected with each other and the intersection points are designated as IP(S) and stored with the corresponding VM_INFO data structure for the corresponding silhouette contour (data structure given in ).","Process flow proceeds to step , where the silhouette contours corresponding to the inner boundaries of the ROI are labeled and stored with the mesh using the data structures previously described in conjunction with  and  and .","Process flow proceeds from step  to process -, which starts at step  (FIG. D). In step , all of the (possibly simplified) outer and inner silhouette contours corresponding to the for the ROI corresponding the a specific viewcell transition are labeled and the labeled associated with the specific viewcell transition.","Process flow proceeds to step , where one triangle for each connected component of the ROI is stored in TRI_SEED_LIST for the specific viewcell transition.","Subsequently, process flow proceeds to step  and  (if no untraversed triangles exist in the VM). In some embodiments, these steps are identical to the previously described steps  and  respectively. If there are untraversed triangle sin the VM, process flow proceeds to process -, which starts at step","The ROI corresponding to the parent-to-child viewcell transition is not a delta region in the sense that the seed triangles for this type of ROI are visible from both viewcells for the parent-to-child viewcell transition. Using this type of ROI, the VM\/PVS for a child viewcell can be efficiently constructed from the parent VM using the outer and inner boundaries of the ROI constructed from the corresponding labeled silhouette contours. This construction uses the hinted, simplified 3D mesh traversal method of , and .","In contrast the ROI corresponding to the transition between contiguous viewcells are delta regions (DR) of visibility. Using this type of ROI, the deltaG+ and deltaG\u2212 can be efficiently constructed from the mesh triangle\/fragments visible from the viewcell A+B, together with the outer and inner boundaries of the ROI constructed from the corresponding labeled silhouette contours. This construction also uses the hinted, simplified 3D mesh traversal method of  and ",{"@attributes":{"id":"h-0018","num":"0000"},"figref":["FIG. 18A","FIG. 18B"]},"As previously described, the effective delta regions (DR)s for a viewcell transition (e.g. A to B) are identified and the corresponding silhouette contours generating the DR occlusion boundaries are established (using the method of ) and labeled using the data structures of  and . This labeling is conducted as an offline process.","Once the labeled silhouette contour information is stored (as data associated with the triangle mesh) this data can be used at runtime to incrementally construct a visibility map corresponding to a specific viewcell from the known visibility map of a connected or containing viewcell.",{"@attributes":{"id":"p-0653","num":"0652"},"figref":["FIG. 18A","FIG. 18B"]},"In some embodiments, process flow starts at step , a labeled silhouette contour (previously generated and stored for the specific viewcell transition being considered) is identified using the data structures associated with mesh for storing the labeled silhouette contour in formation ( and ) using labeling methods of  and  and .","Process flow proceeds to step  to set the current manifold triangle mesh (MESH) to the particular mesh referenced by the Contour.mesh field of the Contour data structure of .","Process flow proceeds to step  to set the CURRENT_EDGE (a manifold triangle mesh edge) to the particular edge referenced by the Contour.edge field of the Contour data structure of .","Process flow proceeds to step  to set the VERTEX (a manifold triangle mesh vertex) to the particular edge referenced by the Contour.vertex field of the Contour data structure of .","Process flow proceeds to step  to set a variable NODECOUNT to 0.","Process flow proceeds to step , where all of the visibility map occlusion boundary segments (VM OB_SEGS) corresponding to the CURRENT_EDGE are constructed. These VM OB_SEGS are constructed using the process shown in  and discussed in detail in conjunction with that figure.","Process flow proceeds to step , to set the variable NODE to reference the particular Contour_Node data structure referenced by the first node of the current contour.","Process flow proceeds to step  to determine if the NODE.node_type of the current Contour_Node data structure () is type . If the node_type is type , then the node corresponds to compound silhouette vertex and processing proceeds to process -, which starts at step  in .","In step , the MESH variable (initialized in step ) is now set to the particular mesh referenced by the CSV_INFO[NODE.NINFO].mesh referenced by the current node, which being a node_type  is a compound silhouette vertex (CSV) node","Process flow proceeds to step , where the variable NEXT_EDGE (indicating the next edge in the silhouette contour polyline) is set to the edge referenced by the CSV_INFO[NODE.NINFO].edge field of the CSV_Info data structure referenced by the current contour node.","Process flow proceeds to step , to set the variable VERTEX to the vertex referenced by CSV_INFO[NODE.NINFO].point field of the CSV_Info data structure referenced by the current contour node.","The effect of steps , , and  is to connect together two polylines on the surface of one or more manifold triangle meshes at a single point, the compound silhouette vertex. Having set the current MESH, NEXT_EDGE, and VERTEX variables to reflect this fusion into a compound silhouette contour, process flow proceeds to process -, which returns process flow to step  ().","If, in decision step , it is determined that the NODE_TYPE is not 3 (i.e., the node does not correspond to a compound silhouette vertex), then process flow proceeds to step .","In step , the value of a variable EDGECOUNT is initialized to zero.","Process flow proceeds to step  to set the variable NEXT_EDGE to reference the edge of the manifold triangle mesh that is connected to the current edge and that is also a (first-order, from-viewcell) silhouette edge. This edge can be easily identified based on the connectivity of the manifold mesh and on the definition of a first-order silhouette edge (see ). Alternatively, the NEXT_EDGE is identified as the edge connected to the current edge wherein the connected edge is already labeled as a labeled silhouette contour edge. This definition of the NEXT_EDGE is used in cases in which the silhouette contour does not shift or migrate (as a result of \u201cretraction of the silhouette edge previously described) substantially. This situation is established during the preprocessing and stored in the struct Contour data structure next_edge_type field. If the next_edge_type has value of 0, then the NEXT_EDGE is identified as the next connected edge that has been previously labeled. If, on the other hand, the next_edge_type value is 1 then the NEXT_EDGE is identified as the next connected edge that is a (first-order) silhouette edge.","Process flow proceeds step , to set the CURRENT_EDGE to the NEXT_EDGE.","Process flow proceeds to step , where the visibility map occlusion boundary segments (VM OB_SEGS) that result from the intersection of the umbral visibility event surface(s) that are supported by the current edge with the manifold triangle meshes. These elements of the visibility map derived from the current edge of the labeled silhouette contour are constructed using a method shown in a flowchart of  and discussed in detail in conjunction with that figure.","Once the VM OB_SEGS generated by the current silhouette edge are constructed (using the method shown in the flowchart of ), then process flow proceeds to step .","In step , the variable EDGECOUNT is incremented.","Process flow proceeds to step  to determine if the EDGECOUNT is less than the span_length for the current node (NODE.span_length) as specified in the Contour_Node data structure of .","If, in decision step , it is determined that the EDGECOUNT is less than the span_length, then process flow returns to step , where then next edge is identified.","If, on the other hand, it is determined in decision step  that the EDGECOUNT is not less than the span_length, then process flow proceeds to decision step .","In decision step  it is determined if the NODECOUNT is less than the number of nodes in the contour, given by the data field CONTOUR.num_nodes, where CONTOUR is a reference to the current labeled silhouette contour being processed and the data structure Contour shown in  is employed.","If, in decision step  it is determined that the NODECOUNT is less than the CONTOUR.num_nodes, then process flow proceeds to step .","In step , the NODECOUNT is incremented and processing returns to step , where the next node is selected and processing continues.","If, on the other hand, in decision step  it is determined that the NODECOUNT is not less than the CONTOUR.num_nodes, then process flow proceeds to step .","Step  indicates that the labeled silhouette contour has been processed and that processing should proceed to the next labeled silhouette contour. Process flow terminates at step .","The overall control of processing all of the labeled silhouette contours for a specific viewcell transition is controlled by the process shown in the flowchart of , and discussed in detail in conjunction with that figure.",{"@attributes":{"id":"h-0019","num":"0000"},"figref":["FIG. 19A","FIG. 19B"]},{"@attributes":{"id":"p-0682","num":"0681"},"figref":["FIG. 18","FIG. 13A","FIG. 19"]},"By calling the process of  during the processing of a silhouette contour (, step ) the process of  effectively results in the construction of all VM_OBSEGs for an entire labeled silhouette contour.","In some embodiments, process flow starts at step  upon encountering an edge of a labeled silhouette. This corresponds to step  of the calling process of .","Process flow proceeds to decision step  to determine if the span_type of the current labeled silhouette contour node (NODE, passed from the calling process) and specified in the Contour_Node.node_type data field specified in is equal to the value of 1.","If, in decision step , it is determined that the span_type of the current labeled silhouette contour node is equal to a value of 1 (indicating that the silhouette edge forms an outside corner on the labeled silhouette contour using the Contour_Node.node_type data field of ), then process flow proceeds to step .","In step , a SV-ME wedge is constructed using the pivot process previously described in conjunction with .","Process flow proceeds to step  to determine if the current silhouette edge for which the wedge has been constructed is the first edge in the contour.","If, in decision step , it is determined that the silhouette edge is the first edge in the contour, then process flow proceeds to step .","In step , the pre-stored wedge-mesh triangle intersection information is obtained from the CONTOUR.VMinfo.point, CONTOUR.VMinfo.mesh, and CONTOUR.VMinfo.tri data fields as specified in the data structures of . This point corresponds to the precomputed and pre-stored intersection of the wedge with a specific mesh triangle wherein the intersection is the initial OB_SEG of the chain of VM OB_SEGS comprising the VM boundary associated with the labeled silhouette contour. This information was previously precomputed and stored during the offline process of identifying silhouette contours for a specific viewcell transition that produce visibility delta regions having a minimum effective dynamic occlusion value (step  of ). Thus, if the edge is the first edge in the labeled silhouette contour the step  generates the first OB_SEG of the corresponding VM boundary.","If, on the other hand, it is determined in decision step  that the silhouette edge being processed is not the first edge in the contour, then process flow proceeds to step .","In decision step , it is determined if the current node has CSV_Info associated with it, that is does the current silhouette edge support a wedge that intersects another visible silhouette edge. If so, then the Contour_Node.node_type value will be equal to 3 () and the Contour_Node.ninfo value will be the index into an array of CSV_Info data structures (). In one embodiment CSV_Info data is not stored with a labeled silhouette contour but instead the initial point of each contour is defined at a CSV and therefore the corresponding data is stored in the Contour.mesh, Contour.triangle, and Contour.vertex fields.","If, in decision step , it is determined that the current node has associated CSV_Info data, then process flow proceeds to step .","In step , the initial VM OB_SEG data is read from the CSV_Info[Contour_Node.inifo].mesh, CSV_Info[Contour_Node.inifo].edge, and CSV_Info[Contour_Node.inifo].point data structures.","If, on the other hand, it is determined in decision step , that the current node does not have associated CSV_Info, then process flow proceeds to step .","In step , the VM OB_SEG corresponding to the current silhouette edge is constructed using VM OB_SEG mesh, triangle, and point intersection data from the last silhouette edge in the contour. Since the VM OB_SEGs form a polyline on the surface of a manifold triangle mesh the construction of a OB_SEG from an adjacent one is a straightforward piecewise construction of a polyline on a polyhedron.","Following either step  or step , process flow proceeds to process - and process -, which starts at decision step  in . In decision step , it is determined if the VM_INFO.RETRIANGULATE_HINT field is not equal to zero. This is a field of the VM_INFO data structure of  and is set to a value of 1 during the runtime construction of the visibility map if it is determined that the effective dynamic occlusion of the bordering visibility map region would be increased by retriangulating at the occlusion boundary.","If, in decision step , it is determined that the value of VM_INFO.RETRIANGULATION_HINT is not equal to zero, then process flow proceeds to step .","In step , the triangles bordering the corresponding occlusion boundary are retriangulated at the boundary.","If, on the other hand, it is determined in decision step  that the value of VM_INFO.RETRIANGULATION_HINT is equal to zero, then process flow proceeds to step .","In step , the triangles bordering the corresponding occlusion boundary are not retriangulated at the boundary. In this case, the triangles comprising the silhouette contour are \u201clinked\u201d to the partially occluded triangles without retriangulation. Process flow terminates at step .","If, in decision step , it is determined that the NODE.span_type is not equal to a value of 1 then process flow proceeds to step .","In decision step , it is determined if the NODE.span_type is equal to a value of 2, then process flow proceeds to step . If the node_type is equal to a value of 2, then the contour may contain both outside and inside corner silhouette vertices. Consequently, process flow subsequently continues to  to determine if the current silhouette edge is involved in an outside corner or inside corner with the next silhouette edge of the contour.","If, in decision step , it is determined that the current edge and next edge of the silhouette contour form an outside corner, then process flow proceeds to step , as previously described.","If, on the other hand, it is determined in decision step , that the current edge and the next edge of the silhouette contour form an inside corner, then process flow proceeds to step .","In step , the SE-MV wedges incident on the inside corner silhouette vertex are formed using the sweep process previously described in conjunction with . Subsequently, process flow proceeds to step , as previously described.","If, in decision step , it is determined that the NODE.span_type is not equal to a value of 2, then process flow proceeds to step .","In decision step , it is determined if the value of NODE.span_type is equal to 3. If in decision step  it is determined that the value of NODE.span_type is equal to 3, then process flow proceeds to step . In this case, the span_type indicates that the contour should be simplified by skipping a subset of the edges of the contour during umbral wedge\/VM OB_SEG construction. This information is precomputed and prestored in the corresponding Contour_Node.span_type and Contour_Node.span_length data structures during the identification of the differential effective static occlusion (also called the effective dynamic occlusion) of the DRs and simplification of the silhouette contour as shown in steps  and  of .","In step , the NODE.span_length is compared to a variable SPAN_COUNTER (which is initialized to zero before the contour is encountered) to determine between which vertices of the silhouette contour the simplified umbral visibility event surface and corresponding VM OB_SEG should be constructed. If, in decision step  it is determined that the SPAN_COUNTER is less than the NODE.span_length, then process flow proceeds to step , which indicates that a wedge is not formed on the current edge.","Process flow then proceeds to step  in which the current silhouette edge is linked directly to the single OB_SEG for the entire silhouette contour, which is ultimately constructed in step  or  when the decision step  directs processing toward steps  or .","If, in decision step  it is determined that the NODE.span_type is not equal to a value of 3, then process flow proceeds to step .","In decision step , it is determined if the NODE.span_type is equal to a value of 3.","If, in decision step , it is determined that the NODE.span_type is equal to a value of 3, then process flow proceeds to step . In this case, the span_type indicates that umbral event surfaces incident on inside corner silhouette vertices of the contour should not be constructed using the sweep process, but should be constructed using the simpler method of intersecting the planes of the adjacent SV-ME wedges.","Consequently, in step , the SE-MV wedges (and the corresponding VM OB_SEGs) are constructed using the intersection of the planes of the adjacent SV-ME wedges and process flow proceeds to step  as previously described.",{"@attributes":{"id":"p-0715","num":"0714"},"figref":"FIG. 20"},"As previously described in conjunction with -, delta visibility data for a specific viewcell transition can be described as regions of interest (ROI) in a unified visibility map containing mesh triangle fragments visible from both viewcells and also containing the from-viewcell occlusion boundaries corresponding to both of the viewcells.","The type of delta visibility data depends on the construction of the corresponding ROI, which depends on the relationship of the two viewcells for which the viewcell transition is described.","If one of the viewcells is completely contained in the other, then the ROI can correspond to the regions visible only from the contained viewcell. The labeled silhouette contours corresponding to these type of ROI can be used to construct the VM of the child viewcell from the VM of the parent at runtime (using a hinted, simplified 3D mesh traversal), thereby avoiding in select cases the explicit storage of deltaG\u2212 information.","If the two viewcells share a face and one is not contained within the other (a relationship termed \u201ccontiguous\u201d) then the ROI correspond to delta regions (DR). For a viewcell transition from VC A to VC B (called an AB transition) the DR are of two types. One type of delta region, DRAB (delta region of occlusion from A to B) contains mesh triangle fragments visible from viewcell A but not B. The DRAB is also a DRBA (delta region of exposure from B to A). Likewise DRAB=DRBA. The DRAB corresponds to deltaG\u2212 data for the AB transition while the DRAB corresponds to deltaG+ data for the AB transition. The labeled silhouette contours corresponding to the boundaries of these ROI can be used to construct the deltaG+ and or deltaG\u2212 data at runtime (also using a hinted, simplified 3D mesh traversal), thereby avoiding in select cases the explicit storage of deltaG+ and deltaG\u2212 polygon fragment information.","In some embodiments, process flow starts at step , where the list of all labeled silhouette contours for the specific viewcell transition is accessed as an array LAB_CON_LIST.","Process flow proceeds to step , where each of the labeled contours in the LAB_CON_LIST is subjected to further processing.","In a first step in the processing of a labeled silhouette contour in the LAB_CON_LIST, process flow proceeds to step , where the edges of the labeled silhouette contour are identified using the process shown in the flowchart of , and the edges are stored in the LAB_SIL_EDGE_LIST.","Process flow proceeds to step , where the edges of the LAB_SIL_EDGE_LIST are subjected to further processing.","In a first step, in the processing of edges in the LAB_SIL_EDGE_LIST, process flow proceeds to step , where the VM OB_SEG corresponding to an edge of the LAB_SIL_EDGE_LIST is constructed using the process shown in the flowchart of .","Process flow proceeds to decision step  to determine if the LAB_SIL_EDGE_LIST is empty. If there are more edges in the LAB_SIL_EDGE_LIST to process, then next unprocessed edge is selected and processing returns to step .","If, in decision step , there are no more edges in the LAB_SIL_EDGE_LIST to process, then process flow proceeds to step .","In decision step , it is determined if there are any more labeled contours to process in the LAB_CON_LIST. If, in decision step , it is determined that there are more labeled contours to process in the LAB_CON_LIST then the next unprocessed contour in selected and process flow returns to step .","If, on the other hand, in decision step , it is determined that there are no more labeled contours to process in the LAB_CON_LIST, then process flow proceeds to step .","In step , the triangle seed list, which is a precomputed list of references to one triangle for each of the delta regions corresponding to a specific viewcell transition (precomputed and stored in step  of ) is set to an array called TRI_SEED_LIST. In this case the triangle seed list contains one triangle from each VM region that is unoccluded from viewcell A (the containing viewcell) and unoccluded from viewcell B. One seed triangle is chosen from each VM region of VM A that is visible from viewcell A and viewcell B such that initiating the traversal on the set of seed triangles insures that the relevant (labeled) silhouette contours for the AB transition is encountered during the runtime simplified 3D mesh traversal of . This selection of seed triangles insures that the VM of viewcell B is constructed from the VM of viewcell A by the traversal process that \u201cshunts\u201d around geometry that becomes in the AB transition using the labeled silhouette contour information.","Process flow proceeds to step , where the triangles of the TRI_SEED_LIST are subjected to processing.","Process flow proceeds to step , where a triangle of the TRI_SEED_LIST is used to initiate a simplified manifold mesh traversal as shown in the flowchart of  and discussed in detail in conjunction with that figure. This traversal identifies all of the triangles visible from viewcell B by initiating traversal on a small subset of triangles (those in the TRI_SEED_LIST) visible from viewcell A.","Process flow proceeds to decision step  to determine if there are any more unprocessed triangles in the TRI_SEED_LIST.","If, in decision step , there are unprocessed triangles in the TRI_SEED_LIST then the next unprocessed triangle in the TRI_SEED_LIST is selected and process flow returns to step .","If, on the other hand, in decision step , it is determined that there no more unprocessed triangles in the TRI_SEED_LIST, then process flow proceeds to step .","Step  indicates that the specific ROI corresponding to the delta visibility information required has been constructed by the simplified runtime traversal. As previously discussed this delta visibility information may actually be the set of triangles\/fragments visible from a child viewcell when the corresponding viewcell transition corresponds is a parent-to-child transition. Alternatively this ROI information may correspond to deltaG+ and deltaG\u2212 data for a viewcell transition between contiguous viewcells, thereby allowing the option of generating deltaG packets when needed instead of storing all deltaG packets for every viewcell transition. Process flow terminates at step .",{"@attributes":{"id":"h-0020","num":"0000"},"figref":"FIG. 21"},{"@attributes":{"id":"p-0736","num":"0735"},"figref":["FIG. 21","FIG. 20"],"b":"2050"},"The process shown in the flowchart of  is similar to the general 3D mesh traversal process of  of the PCT patent application number PCT\/US2011\/042309. The process of  of the PCT patent application number PCT\/US2011\/042309 is generally conducted as an offline preprocess in order to construct visibility maps for the purpose of precomputing and storing PVS and labeled silhouette contour data.","In contrast, the 3D mesh traversal process of  is conducted at runtime and is employed generate ROI of unified visibility maps for specific viewcell transitions. These ROI describe changes in visibility that occur as a result of these specific viewcell transitions.","In one type of viewcell transition, from a parent viewcell to a contained child viewcell, the ROI contain only those triangle\/fragments visible from the child viewcell. This type of ROI can be generated from the simplified, hinted, 3D mesh traversal process of  when the seed triangles supplied are a set of triangles comprising one triangle from each connected component of those ROI containing triangles visible from the child viewcell.","Using these seed triangles and the related ROI boundaries generated at from the labeled silhouette contours, causes the simplified, hinted 3D mesh traversal process to bypass or \u201cshunt\u201d polygons or polygon fragments that become newly occluded during a specific viewcell transition (e.g. viewcell A to viewcell B wherein viewcell B is contained within viewcell A). This allows the removal of newly occluded polygons and\/or polygon fragments without explicitly storing the list of polygons to be removed. This can be more efficient than using explicit lists of polygons to be removed, if the list of polygons to be removed is large relative to the total number of visible polygons.","The hinted traversal method can also be used to directly generate both deltaG\u2212 and deltaG+ packets for a viewcell transition between two contiguous viewcells wherein one viewcell is not contained within the other but the two viewcells have a common face. In this case the starting VM must contain all the polygon or polygon fragments visible from the combined viewcell A+B. In addition the starting VM must contain the relevant occlusion boundaries for viewcell A and for viewcell B. (These can be generated from labeled silhouette edges.) Such a visibility map is called a unified visibility map. The unified visibility map for two connected viewcells contains all of the polygon fragments visible from viewcell A and visible from viewcell B (or visible from the Boolean sum viewcell A+B). In addition the unified visibility map contains the from-viewcell occlusion boundaries corresponding to both viewcell A and viewcell B.","To generate a deltaG+ packet for A-to-B transition (also called an AB transition) the corresponding unified VM is traversed using a seed triangle for each connected component of a VM region that is occluded from A but visible from B. This type of region is called a DRAB. Traversal is initiated using these seed triangles and proceeds to the occlusion boundary corresponding to viewcell A or viewcell B. The viewcell A boundary is encountered on the occluded side while the viewcell A boundary is encountered on the exposed side. This corresponds the DReAB regions shown in light gray in .","Table I summarizes the set of seed triangles needed to initiate the hinted runtime traversal for generating deltaG+ and deltaG\u2212 packets for contiguous viewcells and shows the side of the occlusion boundary encountered.",{"@attributes":{"id":"p-0744","num":"0743"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE I"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Generation of DeltaG+ and DeltaG\u2212 Submesh Data from Unified"},{"entry":"Visibility Map For Viewcell A and Viewcell B Using Method of"},{"entry":"FIG. 20 and FIG. 21"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"84pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"63pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},"Occlusion Boundary"]},{"entry":["Seed","Mesh Traversed","Encountered"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":["Triangles Occluded from A","DRoBA = deltaG\u2212 BA","Occluded Side of A"]},{"entry":["But Not Occluded from B","DReAB = deltaG+ AB","Boundary &"]},{"entry":[{},{},"Exposed Side"]},{"entry":[{},{},"of B Boundary"]},{"entry":["Triangles Occluded from B","DRoAB = deltaG\u2212 AB","Occluded Side of B"]},{"entry":["But Not Occluded from A","DReBA = deltaG+ BA","Boundary &"]},{"entry":[{},{},"Exposed Side"]},{"entry":[{},{},"of A Boundary"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}]}}},"This method allows both deltaG+ and deltaG\u2212 packets to be generated from a unified VM and the corresponding VM occlusion boundary contours for the viewcell transition (which can be generated from labeled silhouette contours). Using this method the deltaG+ and deltaG\u2212 packets for each viewcell transition do not need to be stored explicitly for every viewcell transition. Rather they can be generated by the hinted traversal method at any time before the packets are needed.","Also the runtime 3D traversal method of generating delta-visibility information can be more efficient in a distributed client-server implementation. In regions of high spatio-temporal visibility coherence the same labeled contour information can frequently be used for several specific viewcell transitions in the same region. The use of runtime 3D mesh traversal based on the labeled silhouette information can thereby allow incremental visibility map\/PVS computation with less transmitted data than would be required using direct deltaG\u2212 lists of polygons to remove for each viewcell transition.","Turning now to , in a first step of the simplified, runtime traversal, process flow starts at step , where the traversal is initiated at a specific triangle in the TRI_SEED_LIST for the specific viewcell transition.","Process flow proceeds to decision step  to determine if the traversed triangle contains a from-viewcell occlusion boundary. These boundaries would have been constructed in step  of .","If, in decision step  it is determined that the traversed triangle contains a from-viewcell occlusion boundary, then process flow proceeds to step .","Process flow proceeds to step , where traversal is interrupted at the occlusion boundary. In further embodiments, process flow proceeds from  to process -, which returns the process flow to step .","If, on the other hand, it is determined in decision step  that the current traversed triangle does not contain an occlusion boundary, then process flow proceeds to step .","In decision step  it is determined if the currently traversed triangle has a silhouette edge corresponding to a labeled silhouette edge for the specific viewcell transition being considered. These labeled silhouette edges correspond to inner boundaries of the corresponding ROI of the unified VM.","If, in decision step , it is determined that the currently traversed triangle does not have a silhouette edge, then process flow proceeds to step .","In step , the next (connected) un-traversed triangle in the mesh is selected and submitted to step  and subsequent steps for processing.","If, on the other hand, it is determined in decision step  that the current triangle dos contain a labeled silhouette edge for the current viewcell transition, then process flow proceeds to step .","In decision step  it is determined if any un-processed (un-traversed) triangles exist in the current \u201ctraversal\u201d, where a traversal is defined here as the set of mesh polygons connected by non-labeled-silhouette edges and on the unoccluded side of occlusion boundaries for the specific viewcell transition as constructed in step  of .","If, in decision step , it is determined that there are un-traversed triangles in the current traversal, then process flow proceeds to step , where the next triangle in the traversal is selected for processing as previously described.","If, on the other hand it is determined in decision step  that no un-traversed triangles exist in the current traversal, then process flow proceeds to step .","In step  the traversal is continued on the unoccluded sides of the occlusion boundary segment(s) constructed in step  of ; wherein said occlusion boundary corresponds to the labeled silhouette edge encountered in step . This continuation may involve continuing the traversal on triangles that were retriangulated (i.e. trimmed exactly at the occlusion boundary) or it may involve continuing the traversal on the unoccluded side of triangles that were not retriangulated at the boundary. Process flow terminates at step .",{"@attributes":{"id":"h-0021","num":"0000"},"figref":"FIG. 22"},"For any viewcell transition the polyline corresponding to the attachment curve of a newly exposed submesh can be determined and stored during the offline process of constructing the from-viewcell visibility maps and the corresponding delta-regions of visibility (). These polylines can be stored using the DeltaGplussumbesh_attach_polyline data structure of . This data can be stored and used later during the runtime process of incremental visibility map\/PVS construction described in  and . Specifically, in order to use the runtime method of determining newly occluded mesh elements by \u201cshunting\u201d during runtime 3D mesh traversal, any newly exposed mesh elements are connected to previously exposed elements at the attachment polyline in order to insure that a connected manifold is present for the runtime traversal.","Note that in some embodiments, the method of  is used when the method of  and  are used to compute newly occluded geometry at runtime by shunting.","In some embodiments, process flow proceeds to step, , where the DeltaGplussumbesh_attach_polyline data structure associated with the labeled silhouette edge (starting) for the specific viewcell transition is accessed and referenced by the variable AP.","Process flow proceeds to decision step , where it is determined if the attachment type corresponds to 0. If in decision step , it is determined that the attachment type corresponds to 0, then process flow proceeds to step .","In step , the edges of the deltaG+submesh (here simply called submesh) are directly connected to the corresponding edges of the main mesh. This connection is made between AP.edgenumber of edges starting at the single edges listed in AP.submesh_edgelist and AP.mainmesh_edgelist and proceeding along the free edges (edges having only one component polygon) of the corresponding meshes. In this mode only a single edge for each edgelist needs to be prestored.","Process flow proceeds to decision step , to determine if the attachment type corresponds to 1. If, in decision step , it is determined that the attachment type corresponds to 1, then process flow proceeds to step .","In step , the edges of the deltaG+submesh (here simply called submesh) are directly connected to the corresponding edges of the main mesh. This connection is made between AP.edgenumber of edges starting at the first edges listed in AP.submesh_edgelist and AP.mainmesh_edgelist and proceeding through the entire list of edges in sequence.","Process flow proceeds to decision step  to determine if the attachment type corresponds to 2. If, in decision step , it is determined that the attachment type corresponds to 2, then process flow proceeds to step .","In step , the edges of the deltaG+submesh (here simply called submesh) are \u201clinked\u201d to the corresponding edges of the main mesh wherein a linkage may be a one-to-many mapping from one polyline segment to another polyline. These links are identified during preprocessing such that they present a conservative representation of the corresponding mesh during runtime 3D traversal. This linkage is made between AP.edgenumber of edges starting at the single edges listed in AP.submesh_edgelist and AP.mainmesh_edgelist and proceeding along the free edges (edges having only one component polygon) of the corresponding meshes. In this mode only a single edge for each edgelist needs to be prestored.","Process flow proceeds to decision step  to determine if the attachment type corresponds to 3. If, in decision step , it is determined that the attachment type corresponds to 3, then process flow proceeds to step .","In step , the edges of the deltaG+submesh (here simply called submesh) are \u201clinked\u201d to the corresponding edges of the main mesh, wherein a linkage may be a one-to-many mapping from one polyline segment to another polyline. These links are identified during preprocessing such that they present a conservative representation of the corresponding mesh during runtime 3D traversal. This linkage is made between AP.edgenumber of edges starting at the first edges listed in AP.submesh_edgelist and AP.mainmesh_edgelist and proceeding through the entire list of edges in sequence.","In any case, process flow proceeds to step  for the next viewcell transition. In step , the starting boundary for the next viewcell transition may be derived from the ending boundary of the current viewcell transition, and processing proceeds to decision step  for the next viewcell transition. Process flow terminates at step .","In addition to using deltaG+ geometry packets for newly exposed polygons, some newly exposed surfaces are reconstructed, in some embodiments, procedurally at runtime without the need for explicit deltaG+ polygons. This method, in some embodiments, is employed to generate newly visible portions of a single large polygon (or tessellated surface), a floor or ceiling for example in newly exposed delta regions. In this case, the silhouette contour is specially labeled with a label that instructs the runtime process to procedurally generate the newly exposed portion of the surface in the entire delta region.",{"@attributes":{"id":"h-0022","num":"0000"},"figref":"FIG. 23"},{"@attributes":{"id":"p-0773","num":"0772"},"figref":"FIG. 23"},"This study shows the pronounced effect of exposure time on contrast sensitivity, particularly for high spatial frequencies up to 1000 ms.",{"@attributes":{"id":"p-0775","num":"0774"},"figref":"FIG. 23"},{"@attributes":{"id":"p-0776","num":"0775"},"figref":["FIG. 24A","FIG. 47","FIG. 46"]},"Turning now to , the data store labeled , according to some embodiments, comprises a single data store for delta-PVS data, also called visibility event data, representing changes in the potentially visible set of graphic elements at viewcell-viewcell boundaries or transitions in the modeled environment of the game. This data store may include data that has been processed using the from-region visibility precomputation method specified in the co-pending PCT patent application number PCT\/US2011\/042309. This data store may include data that has been processed using the algorithmic compression methods also detailed in the co-pending PCT patent application number PCT\/US2011\/042309.","In decision step , it determined if a user has requested an interactive data stream. This request can be input to the system by user interaction with the client unit. In one embodiment, any interaction with a controlling device (e.g., a wired or wireless game controller) is interpreted, in step , as a request for an interactive data stream of packets. Other implementations are possible including the use of specific button combinations to request the initiation of an interactive data stream. In some embodiments, the data stream is bi-directional, where data is transmitted from the server to the client and from the client to the server.","If, in decision step , it is determined that the user requests an interactive experience then process flow proceeds to step .","In step , the visibility event data stream is provided to the client unit from the data store  using navigation-based prefetch which is controlled by a camera position that is under interactive control of the user. This prefetch process, as detailed in the co-pending PCT patent application number PCT\/US2011\/042309, current position of the camera viewpoint is used to predict possible future locations of the viewpoint and delta-PVS data corresponding to viewcell boundaries that are likely to be penetrated by the viewpoint in a predetermined period of time in the future are fetched from the data store . For example, based on a current viewcell and history of viewcell transitions, a future viewcell transition is predicted. The prediction of a future viewpoint or camera location given a present location can utilize prior-art methods of dead reckoning or other prior-art methods of navigation prediction, including the method of Chim et. al. (Jimmy Chim, Rynson W. H. Lau, Hong Va Leong, and Antonio Si, CyberWalk: A Web-Based Distributed Virtual Walkthrough Environment, iEEE TRANSACTIONS ON MULTIMEDIA, VOL. 5, NO. 4, DECEMBER 2003, the entire contents of which are incorporated herein by reference). Alternatively, navigation prediction can employ the method of the exemplary flowchart shown in , and discussed in conjunction with that figure.","In step , the viewpoint position that is used to control the navigation-based prefetch process is determined by user input. This viewpoint may be the user's viewpoint in the modeled environment, a camera viewpoint following the user's movement in the modeled environment, or a camera viewpoint following some other user or non-player character in the modeled environment. In this case, the data stream delivers a fully interactive, game-like experience to the user of the client unit. As an example, when a user is playing an interactive game and moves a game controller left or right, the display of the user's current viewpoint on a monitor\/screen pans left or right, respectively.","The actual reading of the fetched data from the data store may be performed by a decoder process located on the same physical device as the client display unit (e.g., if the data stream is fetched from a local disc). Alternatively, the actual reading of the fetched data from the data store of delta-PVS data may be performed by a decoder process that is on a different physical device than the client display unit (e.g., the prefetching process may be located on a remote server unit which also contains the data store ).","In step , the prefetched data represents, in some embodiments, a bidirectional data stream containing delta-PVS renderable data prefetched from the server process to the client process, and user input data supplied to the server process to control the prefetch of data.","If, in decision step , it is determined that there is no request by the user, for an interactive, bidirectional data stream, then process flow proceeds to step .","In step , the viewpoint position that is used to control the navigation-based prefetch process is controlled using a predetermined camera motion path. In this case, the data stream supplied to the client decoder unit delivers a video-like experience which does not require active interactive input by the user of the client unit.","In the context of the present specification, a camera motion path includes data which specifies a sequence of locations of a virtual camera. In computer graphics, a camera is actually a virtual camera which is generally specified by a viewpoint location (corresponding to the camera location and center of projection) as well as a viewport, and typically a view direction vector. A camera may be positioned at the user's virtual eyepoint location in the modeled environment, in which case the camera is said to provide a first-person perspective. In this case the cameral viewpoint is the same as the user viewpoint.","Alternatively, the camera may be located at a point other than the user's viewpoint. The camera may define a view volume which includes an avatar representing the character, or a vehicle being controlled by the user. In this case the camera is providing a third-person perspective.","While this predetermined stream can use data from the same data store as the fully interactive, bidirectional data steam, the predetermined stream can optionally employ packet optimizations that exploit the deterministic structure of the stream. Some of these optimizations have been described in U.S. Pat. No. 6,057,847, the entire contents of which are incorporated herein by reference, for the transmission of renderable data packets representing a fixed camera path. Other optimizations for navigation-based prefetch of visibility event data using both a predetermined camera path and direction, and an interactively controlled camera are described in later sections of this specification.","A principle advantage of using a deterministic data stream controlled by a fixed, predetermined camera path is that the same exact stream can be broadcast to multiple users simultaneously. This uses considerably less bandwidth than if each user in a large digital network (e.g., broadband cable, satellite or other network) is provided a unique bidirectional, fully interactive data stream. In some embodiments, a deterministic data stream includes a predetermined sequence of data packets corresponding to a viewcell transition.","Even though the unidirectional, deterministic data streamed controlled by a prescribed camera motion path can be efficiently broadcast simultaneously to multiple users of the supporting network, a deterministic unidirectional data stream can still support significant, if not full, interactive input from each user according to some embodiments.","This limited interactivity provided by the deterministic data stream is a consequence of multiple features of the visibility event streaming method. First, each deterministic data stream is comprised of delta-PVS or visibility event packets representing a camera motion path through a region of space within the modeled database formed by a specific connected sequence of viewcells embedded in the model. In contrast, a conventional camera motion path (e.g., for a computer animated video) is formed by a space curve embedded in the model. Consequently, as long as the user navigation is confined to the region of space defined by the connected viewcell sequence defining the motion path, then the user is free to interactively change viewpoint location (i.e., navigate within) the portion of the modeled environment that has already been streamed to the client unit. This navigation can be limited by any memory limitations of the client unit that limit the amount of cached data that can be stored on the client. This navigation can also be intentionally limited by caching restrictions placed on the client unit by the server-provider or content owner. This type of limited interactive camera motion is available from the unidirectional, deterministic data stream even though it is being simultaneously supplied (broadcast) to multiple users.","In addition, the visibility event packets precomputed using the methods of PCT patent application number PCT\/US2011\/042309 can allow the incremental and progressive construction of PVS data representing all graphic elements visible from specific viewcells wherein the viewing direction within the viewcell is unrestricted (i.e., omnidirectional). That is, the resulting potentially visible sets allow omnidirectional viewing within the corresponding viewcell. When such visibility event packets are employed using a predetermined camera path, a user still can retain full control of viewing direction even as receiving a broadcast deterministic visibility event data stream determined by a predetermined camera path.","Another type of user interactivity that can be delivered using the unidirectional, deterministic data stream involves control of avatars, characters, vehicles and other objects that are within the regions of the model visible from the connected viewcells for which the corresponding delta-PVS data has been transmitted to the client unit and cached.","Full interactivity, in which the user is free to move throughout the modeled environment, uses the full bidirectional data stream as shown in steps  and . In this case the user has requested a unique data stream different from the deterministic broadcast stream. When this selection is made, the digital service provider can elect to add an additional fee reflecting the use of server resources required to prefetch the unique stream and the dedicated bandwidth required to deliver it to the user. In addition, the content owner may charge an additional fee for the fully interactive stream that reflects the increased functionality of the content when used as a game experience instead of a more passive video-like product. In some embodiments, this fee is charged to a user's subscription account associated with a server that delivers content to the user's client unit.","Step  shows the method of increasing fees when the bidirectional stream is selected by the user in step . This increased fee can be in the form of a fixed pay-per-play fee or an hourly rate. Alternatively, digital service providers and or content owners can allow the user to gain access to the interactive content for a monthly or other period fee.","Process flow proceeds to decision step , where it is determined if the user continues to require the interactivity supplied by the bidirectional data stream. If, in step , full interactivity is still required, then processing returns to step , in which the non-deterministic, navigation-based prefetch of delta-PVS packets is controlled by the user's location continues. In some embodiments, the subsequent step  is executed each time the user requests full interactivity. In alternative embodiments, the additional fee is charged only for the first request or for specific requests for interactivity, as determined by the service provider and\/or content owner.","When the user fails to supply input for a predetermined period, or when the user sends an active signal to end the fully interactive bidirectional stream, then the stream reverts to one or more deterministic data streams in some embodiments.","If, in decision step , it is determined that full interactivity is not requested or required by the user, then process flow proceeds to step . In step , the data stream is transitioned to a deterministic, unidirectional data stream.","This transition can occur by having the server or client process take control of the camera viewpoint and moving it so that it to a viewcell belonging to a viewcell of a viewcell path corresponding to a deterministic data stream. In some embodiments, precomputed and prestored PVS data, corresponding to the complete PVS of certain viewcells in one or more deterministic viewcell paths can be sent by the server to exactly re-synchronize the previously bidirectional data stream to one or more deterministic, unidirectional data streams corresponding to different \u201cstorylines\u201d or outcome paths for the streamed content.","This resynchronization can optionally be conducted to create a seamless transition from a fully interactive experience to one or more deterministic data streams (corresponding to \u201cstorylines\u201d) that are actually being broadcast to multiple other client units by the service provider. This method allows efficient use of available bandwidth by granting a unique stream only to those users that actually desire the unique stream. Further, this efficient use of available bandwidth decreases network congestion. Either the deterministic or the non-deterministic data streams can be recorded, for example on a hard disc, for later use. As previously described, the method includes, in some embodiment, a technique of intentionally limiting the amount of delta-PVS data that is cached on the client unit to prevent piracy and resale of used content.",{"@attributes":{"id":"p-0801","num":"0800"},"figref":"FIG. 24B","b":["2490","2492"]},"The cyclic process, in one exemplary embodiment, starts at decision step . In decision step  it is determined if the user has supplied input to the client unit producing control or the viewpoint position and\/or velocity (e.g., joystick input) or an intention to provide such control (e.g., button push signaling intention to begin user control of navigation).","If, in decision step  it is determined that no input to the controller has been made by a user indicating user control of viewpoint\/camera navigation, then process flow proceeds to step .","In step , no data controlling viewpoint position\/velocity is sent to the server. Alternative embodiments can use a positive indication by the user that the user wishes to relinquish control of viewpoint\/camera navigation.","Process flow proceeds to decision step , conducted on the server unit. In decision step  it is determined if data has been received indicating user control of viewpoint\/camera navigation. In some embodiments, the criteria for determining no input data from the client includes such parameters as no input over a specified period of time. Also, as previously indicated, the user's desire to relinquish control of viewpoint\/camera navigation is indicated by a positive signal (e.g., specific controller button push) in some embodiments.","If, in decision step , it is determined that no input data from the client unit is received (or input data indicating the desire to terminate control of viewpoint\/camera navigation), then process flow proceeds to step .","In step , the navigation of the viewpoint\/camera is driven by a predetermined camera motion path and the visibility event data packets (data store ) are read to the client unit using a navigation-based prefetch that is driven by the predetermined viewpoint\/camera motion path. The resulting visibility event data stream, , in this case will provide the client unit with graphics information that when rendered produces a scripted image sequence that does not support full interactivity (e.g., will not allow unrestricted viewpoint\/camera motion, but may allow a more prescribed interactivity, e.g. full or limited control of view direction vector) during viewing. In an alternative embodiment, instead of a determination stream being provided, the current viewpoint remains stationary until a user input is detected.","The visibility event packets are sent to the client unit and decoded in step  to produce PVS data, stored in data store . In exemplary embodiments, this decoding is performed using the methods specified in the co-pending PCT patent application number PCT\/US2011\/042309.","If, in decision step , it is determined that the user has entered an input indicating user control of viewpoint\/camera navigation then process flow proceeds to step .","In step , this input data is sent to the server unit. As an example, the input data includes a current position and velocity of a user.","If, in decision step , it is determined that input data from the client unit is received then, process flow proceeds to step .","In step , the navigation of the viewpoint\/camera is determined by user input and the visibility event data packets (data store ) are read to the client unit using a navigation-based prefetch that is driven by the user-controlled viewpoint\/camera. The resulting visibility event data stream, , in this case provides the client unit with graphics information that, when rendered, produces an unscripted image sequence which supports full viewer motion within the limits of the navigable space of the modeled environment and within any restrictions imposed on the cache size of the visibility event and PVS data stored on the client unit by the server unit.","Using the method of  and  to control the streaming of delta-PVS packets, previously specified in the co-pending PCT patent application number PCT\/US2011\/042309, a deterministic, broadcast delta-PVS data stream can be used to efficiently broadcast content that appears to be a high-definition computer animated program, but with an important advantage: at any time the viewer can choose to use a game controller linked to the decoder device and become a character in the story. The instant that the user chooses to participate, the packet stream switches from a deterministic broadcast stream to a fully interactive, bidirectional connection with the service provider, which can be provided as premium, on-demand type content. The streaming interactive media (SIM) delivered by the delta-PVS codec specified in the co-pending PCT patent application number PCT\/US2011\/042309 has seamlessly switched from a television program to a game. At any time, in some embodiments, the user can choose to resynchronize with one of the passive, scripted story lines of the program, which enhances the user's experience with the content.","The method of encoding visibility-event-based, delta-PVS data packets described in the co-pending PCT patent application number PCT\/US2011\/042309, allows existing game content to be converted to data capable of supporting the type of flexible, streaming interactive media (SIM). Of course, native SIM content can also be developed using existing game development tools for the target game engine together with the method of encoding the information produced by these development tools described in the co-pending PCT patent application number PCT\/US2011\/042309. As discussed in conjunction with , the client unit can incorporate a client-decoder process that is integrated into a game engine.","This SIM method of content delivery enables publishers to leverage the \u201cfreemium\u201d model of content delivery in which scripted, deterministic SIM content is delivered free to large audiences using a unidirectional, broadcast data stream. This exposes the content to broad audiences and allows viewers to instantly become players by purchasing the premium, bidirectional, fully interactive content.","Content streamed using the methods detailed in the co-pending PCT patent application number PCT\/US2011\/042309 can optionally be rendered and composited over conventional video on the client unit.","As developed more fully in conjunction with , in the exemplary flow diagram of , step , or , STEP , the user's location may be the user's virtual location in a modeled environment, or the actual location of the user\/client unit in a real environment on which the modeled environment is based. Modeled environments based on actual environments can be constructed from data acquired using LIDAR (Light Detection and Ranging) or other ranging modalities, from conventional 3D modeling, or a combination of both acquisition and hand modeling. The method of LIDAR is disclosed in the publication \u201cAirborne and Terrestrial Laser Scanning, Vosselman, G., Hans-Gerd, M., Eds. CRC Press 2010, ISBN 9781439827987\u201d, incorporated herein by reference. If the modeled environment is based on an actual environment then the streamed visibility event information includes, in some embodiments, a virtual, interactive representation of the real environment that is streamed to the client unit, based on the client unit's location in the real environment.",{"@attributes":{"id":"h-0023","num":"0000"},"figref":"FIG. 25A"},{"@attributes":{"id":"p-0818","num":"0817"},"figref":["FIG. 25A","FIG. 25A","FIG. 25B","FIG. 25A"],"b":["2501","2502","2503","2504"]},{"@attributes":{"id":"h-0024","num":"0000"},"figref":["FIG. 26A","FIG. 25A","FIG. 25B"]},{"@attributes":{"id":"h-0025","num":"0000"},"figref":["FIG. 26B","FIG. 26A"]},{"@attributes":{"id":"p-0819","num":"0818"},"figref":["FIG. 26C","FIG. 26A"]},{"@attributes":{"id":"p-0820","num":"0819"},"figref":["FIG. 26C","FIG. 26A"]},"A visibility event decoder-server process using navigation-based prefetched driven by the exemplary prescripted camera\/viewpoint motion path PATH A, results in the streaming of seven visibility event packets corresponding to the viewcell-viewcell transition boundaries between:\n\n","In exemplary embodiments, for a prescripted camera\/viewpoint motion path such as PATH A, only visibility event packets corresponding to viewcell-viewcell boundaries that are actually penetrated by the motion path are delivered. In , only the viewcells having boundaries corresponding to visibility event packets delivered during the visibility event content stream are shown. Thus, viewcell [,,] and viewcell [,,] (as shown in ) are not shown in , since the prescripted camera\/viewpoint motion path PATH A does not penetrate these viewcells.","In the exemplary embodiment of , the camera\/viewpoint motion path labeled PATH I, corresponds to a camera\/viewpoint motion path that comes under interactive control of a client-user at the point labeled POINT D. In the example of , the visibility event content streaming being received by the decoder-client process being used by the client-user corresponds to the visibility event packets prefetched by crossing the viewcell boundaries penetrated by PATH A until POINT D is encountered. In this example, at POINT D, the client-user chooses to receive an interactive visibility event packet stream that is prefetched based on the client-user's inputs into a controlling device such as a mouse or game controller. As described in conjunction with the method of  and  (as well as the later  and ), in some embodiments, the decoder-server process delivering the visibility event packets to a specific decoder-client process rapidly transitions from prefetching visibility event packets corresponding to a prescripted camera\/viewpoint motion path to prefetching visibility event packets corresponding to a camera\/viewpoint motion path that is being controlled by the client-user.","In the example of , ,  and , the viewcells correspond to navigation cells the viewcells shown in  and  are the only viewcells that exist in the region of the two camera\/viewpoint motion paths shown in  and . In this example the array of viewcells in the vicinity of the modeled objects corresponds to predefined path (e.g., narrow street) or region in which navigation is relatively restricted.","Consequently, in the example shown in , for the interactive camera\/viewpoint motion path PATH I, taken by the client-user, only the additional visibility event packets corresponding to the boundaries between the set of viewcells: viewcell [,,]; viewcell [,,]; viewcell [,,]; viewcell [,,]; and between this set of viewcells and their neighbors: [,,]; viewcell [,,]; viewcell [,,]; viewcell [,,] would, in some embodiments be prefetched during the interactive visibility event data stream corresponding to PATH I.",{"@attributes":{"id":"p-0826","num":"0832"},"figref":["FIG. 26D","FIG. 26C","FIG. 26D","FIG. 26C","FIG. 26E","FIG. 26D"],"br":[{},{}]},{"@attributes":{"id":"p-0827","num":"0833"},"figref":["FIG. 26E","FIG. 26D","FIG. 15A","FIG. 15B"]},"The camera\/viewpoint motion PATH R, in some embodiments, is determined at runtime as a trajectory that intersect the prescripted camera\/viewpoint motion path at an instant in time when the prescripted camera\/viewpoint is located at the intersected point of the prescripted camera\/viewpoint motion path. In the example of , the point of resynchronization with PATH A occurs at the labeled point POINT S. In such embodiments, the determined trajectory effects a seamless resynchronization with one or more prescripted camera\/viewpoint motion paths that comprise a broadcast visibility event based streaming interactive media (SIM) programming. As illustrated in , in some embodiments, at the instant a user decides to switch from the interactive stream to the deterministic stream (i.e., point Q), the Path R follows a natural progression back to the deterministic stream. For example, at point Q, the user is currently located in viewcell [,,], and the closest portion of the deterministic stream is located in viewcell [,,]. Instead of immediately jumping from viewcell [,,] to viewcell [,,], the user rejoins the deterministic stream at viewcell [,,] via viewcell [,,].","The motion path labeled PATH R, connecting the fully interactive motion path with the prescripted motion path is, itself, a scripted motion path (e.g., in some embodiments, determined at runtime). In the example of , the scripted motion path PATH R will not penetrate Viewcell [,,]. Consequently this viewcell is not shown in . This is consistent with the general principle of operation that a transition from an interactive camera\/viewpoint motion path to a prescripted path decreases the size of the visibility event data that must be prefetched and cached, and thus allowing faster navigation, less bandwidth consumption or both.","Method of Delivering Interactive Content as Un-Encoded Data and Performing Encoding on Client Unit Prior to Using Content and Method of Using Distributed Computing to Compute Visibility Event Data on Client Work Units",{"@attributes":{"id":"p-0830","num":"0836"},"figref":"FIG. 27"},"Data store  is un-encoded game content which may include geometry, texture, material, lighting and other data representing the modeled environment. In step , un-encoded game data is subject to the method of encoding described in the co-pending PCT patent application number PCT\/US2011\/042309. This encoding produces delta-PVS packets, also called visibility event packets. In some embodiments, the encoded data is stored in data store .","In some embodiments, these packets include geometry and\/or texture information that conservatively becomes newly visible during a corresponding viewcell transition between viewcells having a contiguous or a contained relationship. According to some embodiments, the phrase \u201cconservatively becomes visible\u201d includes all graphics elements that become visible for a viewcell transition, and may also include graphics elements that do not actually become visible for the viewcell transition (depending on the precision of the PVS\/delta-PVS determination). The methods described in the co-pending PCT patent application number PCT\/US2011\/042309, a subset of which are also described in  through  of the present specification, enable a precision-controlled determination of PVS\/delta-PVS. Alternatively, in some embodiments, as described in the co-pending PCT patent application number PCT\/US2011\/042309, the visibility event data is stored in an algorithmically compressed format as an intermediate representation using labeled silhouette edges. Newly occluded geometry and texture information may also be stored in .","In some embodiments, the step  is executed on the client unit. This implementation allows generation of all or part of the delta-PVS data directly on the client unit before it is required. This implementation may be useful when the client unit has a limited or no connection to a server unit. It can also be employed when a high-precision visibility precomputation is employed during the encoding process of the co-pending PCT patent application number PCT\/US2011\/042309. In this case, the size of the delta-PVS data, stored in data store , may be too large to practically store on distributable media and load directly into the client unit.","In one embodiment, the delta-PVS visibility event packets computed on a client unit is sent to a server unit for later use by other users. In exemplary flow diagram of  this optional transmission of the visibility event packet(s) determined on a client unit to a server unit is shown as step .","In step , conducted on a server unit, the visibility event packet\/work unit is received by the server unit and the data is stored in data store . In some embodiments, step  also sends a proof of work data token to the client unit that determined a corresponding delta-PVS data packet and transmitted it to the client. This data is labeled . This proof of work information acts as compensation to the operator-owner of the client unit for computing a specific visibility event (delta-PVS) PACKET. This proof of work information packet can later be used as currency in a manner similar to bitcoins (e.g., in this case allowing the user of the client unit to use the proof of work currency to purchase visibility event based content or other goods or services). In exemplary embodiments, each visibility event packet for each game or SIM application is given a serial number which simplifies the distribution of work units between servers and clients.","As an example, prior to release of a game on the market (or during a beta release), the precomputed delta-PVS data for each possible viewcell to viewcell transition in a modeled environment of the newly released game may not exist. Accordingly, in some embodiments, users are sent work packets during the pre-release or pre-purchase period and the delta-PVS data may be computed on the user's client device and transmitted to the server. As compensation for computing the delta-PVS data, the user receives a proof of work token redeemable for a discount on the game or for other products or services. Therefore, the user has incentive to pre-purchase the game and consequently receive the work packets.","In a subsequent step , the server unit determines additional delta-PVS packets that must be computed, and sends instructions to determine these packets to the client unit. In some embodiments, the data that is required by the client unit in order to compute the visibility event packet data is included with work unit instructions sent to the client unit as the data labeled  rather than supplied by the entire game database . In some embodiments, work packets are made available until the visibility event packets corresponding to each navigable viewcell boundaries\/transitions of a game are completed.",{"@attributes":{"id":"h-0027","num":"0000"},"figref":"FIG. 27B"},{"@attributes":{"id":"p-0838","num":"0844"},"figref":["FIG. 27B","FIG. 27B","FIG. 27A"],"b":"2742"},"The data structure of  includes a field indicating the client-user's identifier or subscriber ID. As each client-user may control more than one client device a unique client device field is also included. This device identifier may, in some embodiments include IP data or other unique device data identifying a client unit or IP address. In some embodiments, a third field identifies the game or SIM content for which the visibility precomputation is to be performed. In some embodiments, another field contains an unique identifier for a specific viewcell in the navigable space of the game or SIM. In some embodiments the viewcell identifier is determined from the indices of a three dimensional array of viewcells in the modeled environment. In some embodiments, another field includes a unique identifier for a specific boundary of the viewcell. This viewcell boundary information includes, in some embodiments, a reference to a specific face of the viewcell corresponding to a specific visibility packet. Another field includes parameters of the viewcell. In some embodiments these parameters include the dimensions of the viewcell and location of the viewcell. In some embodiments another field includes viewcell boundary parameters. Viewcell boundary parameters include, in some embodiments, information indicating neighboring viewcells on each face of the viewcell.","In some embodiments the visibility event precomputation work instructions include the graphical object information (e.g. polygon and polygon mesh connectivity information) required to perform from-viewcell visibility precomputation. In some embodiments this information includes texture vertex information.",{"@attributes":{"id":"h-0028","num":"0000"},"figref":"FIG. 27C"},{"@attributes":{"id":"p-0841","num":"0847"},"figref":["FIG. 27C","FIG. 27C","FIG. 27A","FIG. 27A"],"b":["2725","2730","2735"]},"The data structure of  includes a field indicating the client-user's identifier or subscriber ID. As each client-user may control more than one client device a unique client device field is also included. This device identifier may, in some embodiments include IP data or other unique device data identifying a client unit or IP address. In some embodiments, a third field identifies the game or SIM content for which the visibility precomputation is to be performed. In some embodiments, another field contains an unique identifier for a specific viewcell in the navigable space of the game or SIM. In some embodiments the viewcell identifier is determined from the indices of a three dimensional array of viewcells in the modeled environment. In some embodiments, another field includes a unique identifier for a specific boundary of the viewcell. This viewcell boundary information includes, in some embodiments, a reference to a specific face of the viewcell corresponding to a specific visibility packet.","In some embodiments the completed work packet includes PVS or visibility event information which may include the DeltaGplussubmesh information described in .",{"@attributes":{"id":"h-0029","num":"0000"},"figref":"FIG. 27D"},{"@attributes":{"id":"p-0844","num":"0850"},"figref":["FIG. 27D","FIG. 27C","FIG. 27A"],"b":["2730","2743"]},"The data structure of  includes a field indicating the client-user's identifier or subscriber ID. As each client-user may control more than one client device a unique client device field is also included. This device identifier may, in some embodiments include IP data or other unique device data identifying a client unit or IP address. In some embodiments, a third field identifies the game or SIM content for which the visibility precomputation is to be performed. In some embodiments, another field contains an unique identifier for a specific viewcell in the navigable space of the game or SIM. In some embodiments the viewcell identifier is determined from the indices of a three dimensional array of viewcells in the modeled environment. In some embodiments, another field includes a unique identifier for a specific boundary of the viewcell. This viewcell boundary information includes, in some embodiments, a reference to a specific face of the viewcell corresponding to a specific visibility packet.","In some embodiments the time and date of completion of the work is stored in another field. In some embodiments the monetary value of the token is stored.","The fields define a unique proof-of-work token corresponding to a specific visibility event precomputation. As described in conjunction with  these tokens can be used by a client-user for goods and services in return for the visibility precomputation work provided.","In some embodiments the proof-of-work token is a reusable proof-of-work currency, such as bitcoin, that can be exchanged for goods and services provided by a number of vendors.","Method of Encoding Interactive Content at Different Levels of Detail Based on the Available Transmission Bandwidth and Client Graphics Performance",{"@attributes":{"id":"p-0849","num":"0855"},"figref":["FIG. 28A","FIG. 28B","FIG. 28C"]},"The method of the co-pending PCT patent application number PCT\/US2011\/042309 specifies techniques in which relatively lower level-of detail visibility event packets are dynamically selected for transmission during periods when late packet arrival may occur. The methods of , , and  are static methods of encoding visibility event packets at a relative level-of-detail that reflects the maximum available transmission bandwidth or the maximum client rendering performance. The previously described method of encoding visibility event packets having a lower level-of-detail and selecting these packets for transmission during periods of potential late packet arrival are employed, in some embodiments, together with the methods of .","In decision step  of the exemplary flow diagram of , it is determined if the maximum available transmission bandwidth is greater than a predetermined value, in this case VAR. If, in decision step , it is determined that the available transmission bandwidth exceeds a predetermined value, then process flow proceeds to step . In step  encoding is conducted using a high level of detail model. The level-of-detail of the model is determined by factors which include, in some embodiments, the geometric level-of-detail, texture resolution, and lighting detail.","If, on the other hand, it is determined in decision step  that the available transmission bandwidth is not greater than a predetermined value, then process flow proceeds to step . In step , the encoding process described in the co-pending PCT patent application number PCT\/US2011\/042309 is conducted using a lower level of detail model.","In the exemplary flow diagram of , in decision step , it is determined if the maximum available client rendering performance is greater than a predetermined value VAR. If in decision step  it is determined that the maximum available client rendering performance is greater that the predetermined value VAR, then process flow proceeds to step  in which the visibility event packets are encoded at a high level-of-detail. If, on the other hand, it is determined in decision step  that the available client rendering performance is not greater than the predetermined value VAR, then process flow proceeds to step . In step , the encoding process described in the co-pending PCT patent application number PCT\/US2011\/042309 is conducted using a lower level-of-detail model. Once again, the level-of-detail of the model is determined by factors which may include, in some embodiments, the geometric level-of-detail, texture resolution, and lighting detail.","According to some embodiments, multiple encodings are conducted and stored for the same modeled environment. The appropriately encoded visibility event data can be statically selected for real-time prefetch-decoding based on the maximum available transmission bandwidth and\/or the client rendering performance. In addition, the level-of-detail, in some embodiments, is further adaptively selected to prevent late packet arrival based on the methods described in the co-pending PCT patent application number PCT\/US2011\/042309.","Various parametric representations of geometric models are possible in some embodiments. These parametric representations include curved surfaces represented by Bezier and other higher-order surfaces. Parametric surfaces are a type of procedural surface. Procedural surfaces also include, in some embodiments, procedurally defined models such as extrusions (with or without beveling), surfaces of revolution and other, more complex procedural models.","These procedurally defined models can generally be described using a small amount of information that comprises the parameters of the procedure. At runtime, relatively large amounts of information comprising surface geometry, e.g. polygons, can be generated from the small amount of parametric information and the corresponding procedural generation processes. As an example, parametric information can include control vertices for a rational b-spline surface, from which many polygons can be generated at runtime. (Examples of graphics information being efficiently stored as parametric models include Autodesk Corporation's 3D Studio Max product, and Parametric Technology Corporation's Pro-Engineer product and as described, in the case of spline-based surfaces in the publication \u201cGeometric Modeling with Splines, Cohen, E., Riesenfeld, R., Elber, G. 2001, A. K. Peters, the entire contents of which are incorporated by reference.)","In decision step , of the exemplary flow diagram , it is determined if the maximum available transmission bandwidth is greater than a predetermined value, in this case VAR. If, in decision step , it is determined that the available transmission bandwidth exceeds a predetermined value, then process flow proceeds to step . In step  encoding is conducted using the primary, non-procedural representation.","If, on the other hand, it is determined in decision step  that the available transmission bandwidth is not greater than a predetermined value then process flow proceeds to step . In step , the encoding process described in the co-pending PCT patent application number PCT\/US2011\/042309 is conducted using a parametric representation.","In addition, in some embodiments the selection of visibility event packets encoded with primary geometry vs. visibility event packets encoded with parametric representation of geometry is also made adaptively during runtime to prevent late packet arrival based on the methods described in the co-pending PCT patent application number PCT\/US2011\/042309, as well as methods discussed in conjunction with  and  of the current specification.",{"@attributes":{"id":"h-0031","num":"0000"},"figref":"FIG. 29"},"Exemplary flow diagram of  shows a method of controlling the amount of cached visibility event and PVS data that may be stored on a client unit. In exemplary flow diagram of , in decision step , it is determined if the size of the cached data on the client unit exceeds a predetermined value, in this case PERMITTED_SIZE. The cached data may be un-decoded visibility event data, decoded visibility event data (i.e., delta-PVS data), PVS data or a combination of all three.","If, in decision step , it is determined that the size of the cached data on the client unit exceeds the PERMITTED_SIZE, then process flow proceeds to step .","In step , cached data representing PVS data that is not visible from the vicinity of the of the user's viewpoint, or delta-PVS data that is not readily reachable from the user's current viewpoint (e.g., corresponding to viewcell transition boundaries that are not in the vicinity of the viewcell containing the user's current viewpoint location) are removed from the cache.","This removal process results in a cache that is, at any time, generally much smaller than the entire model. This prevents unauthorized duplication (e.g., copying or piracy) of the content, since only a small fraction of the data is stored on the client at any one time. If in step , if it is determined that the cache size is not greater than the PERMITTED_SIZE, then process flow proceeds to step , in which no cached elements are removed.","Method of Streaming Advertising and Targeted Advertising During Low Bandwidth Requirement Periods of a Streaming Interactive Media Data Stream.",{"@attributes":{"id":"p-0864","num":"0870"},"figref":"FIG. 30"},"A server unit is labeled . A specific client unit is labeled . In some embodiments, the server unit is connected to multiple client units.","In some embodiments, the non-variable visibility event data includes the majority of the content of the modeled environment including buildings, vehicles, avatars, and other content that do not represent advertising, cash store, or messaging objects that are unique to a particular client-user.","In contrast, the variable visibility event data (stored in data store  of this exemplary embodiment) represents advertising, cash store, messaging or other objects that are unique to a particular client-user.","Both non-variable and variable visibility event data is prefetched to client units based on camera\/viewpoint movements which may either be determined by a prescripted camera motion path or may be driven by interactive user control of camera\/viewpoint motion.","Cash store objects include objects or representations of objects in the modeled environment that can be selected by a client-user, and wherein the selection of the cash store object makes the object available for use by the client-user. In some embodiments, the selection of the cash store object results in a monetary charge to the client-user. Examples of cash store items include modeled apparel, weapons, accessories, vehicles, real estate, and any other types of objects that can be represented in a modeled environment.","Messaging objects include objects or representation of objects in the modeled environment that convey a message to the client-user. Examples of messaging objects include billboards, engravings or markings on avatars, vehicles, or apparel, flying logos or any other types of objects that can be represented in the modeled environment. In some embodiments, the messaging object can be interacted with by the client-user to send a return message.","Advertising objects also include objects or representation of objects in the modeled environment that convey an advertisement to the client-user. Examples of advertising objects include billboards, engravings or markings on avatars, vehicles, or apparel, flying logos or any other types of objects that can be represented in the modeled environment. In some embodiments the messaging object can be interacted with by the client-user and the interaction is recorded as a user click of a clickable advertisement.","In the exemplary embodiment of , the visibility event packets are streamed based on a scripted, predetermined camera path, in which case the same non-variable visibility event data stream is broadcast to a plurality of connected client units. Data store  stores non-variable delta-PVS or visibility event packets. In this exemplary embodiment the non-variable visibility event packets include geometry and texture information that is prefetched based on a prescripted camera path. The resulting visibility event data stream is called a streaming interactive media (SIM) data stream. In exemplary embodiments, a SIM data stream is delivered, in nominal operating mode, as a visibility event data stream that is driven by a prescripted camera motion path. (As illustrated in the exemplary embodiments of  and , as well as  and , the non-interactive SIM stream can transition to a fully interactive SIM stream under user control.) In further embodiments, this non-variable visibility event data stream is driven by a predetermined scripted camera motion path in which case the resulting SIM allows limited interactivity, as previously described in conjunction with  and . In this case, the scripted sequence of non-variable visibility event packets is broadcasted to multiple client units simultaneously according to some embodiments.","In step , the non-variable packets including the streaming interactive programming are delivered to client units as non-variable visibility event packets. In the exemplary embodiment of  embodiments, the non-variable visibility event packets are streamed based on a scripted, predetermined camera path, in which case the same non-variable visibility event data stream is broadcast to a plurality of connected client units. In alternative embodiments, non-variable visibility event packets are dynamically prefetched to a specific client unit using navigation-based prefetch in which the camera\/viewer motion is interactively controlled by the client user using a mouse, game controller or other device.","A second data store  stores variable data representing directed advertising messages that are streamed to individual, specific client units during the broadcast or non-broadcast delivery of the non-variable visibility event data in some embodiments. According to some embodiments, the directed advertising data is sent as variable packets that are added to the data stream during periods when the bandwidth requirement for sending the non-variable data is low.","In step , the geometric or texture information for an object representing a specific advertising message that will be visible to the user is placed in one or more variable packets that are combined with the broadcast, non-variable packets in step .","Alternate embodiments of the method insert variable packet data, in the step , wherein the variable packet data represents renderable graphics objects or instructions for generating renderable graphics objects that comprise instant messages sent from one user to another user.","In some embodiments, the visibility event data of  is actual geometric and\/or texture information. In alternate embodiments, the data is instructions for generating the actual geometric and\/or texture information on the client unit. In one embodiment, the data of  includes parameters for the parametric construction of the objects representing the geometric and\/or texture information that is the object representing the advertising message. In one embodiment, this parametric information includes references to specific glyphs as well as parameters that describe the extrusion and\/or beveling of the glyph outlines to generate 3D logos or engravings representing the advertising message. In further embodiments, other parametric construction techniques are employed to generate billboards, neon signage, scoreboard and other representations with a custom advertising message.","In some embodiments, the data of store  is, in step , associated with a geometric transformation matrix that places the associated object in a location that will likely become visible to the user in a predetermined amount of time, using on the navigation-based prefetch methods described in the co-pending PCT patent application number PCT\/US2011\/042309 and further improved in the present specification. In one embodiment, the transformation matrix transforms the variable object to a location that is not visible from the current viewcell, but is visible from a viewcell that will soon be penetrated by the camera\/viewpoint. The transformation matrix is derived using the prior-art method of matrix concatenation as described in the publication \u201cIntroduction to Computer Graphics, Foley, J., Van Dam, Addison Wesley, 1990\u201d incorporated herein by reference, wherein the transformation matrix locates the variable object in a viewcell or navigation cell that is not yet visible to the user but is part of the visibility event cache.","In step , the data representing the advertising object is streamed as a variable packet during periods when the number and size of variable packets being sent is low. In this exemplary embodiment, the variable packets are sent during periods of time when the bandwidth requirement for streaming non-variable packets is low. Even in exemplary implementations such as this, in which the non-variable packets represent prescripted visibility event packets that are broadcast simultaneously to multiple clients, each variable packet is sent only to a specific receiver\/client unit during these periods of low bandwidth requirement in some embodiments. The variable packets are sent to one or more specifically targeted receiver units using a packet addressing protocol such as TCP\/IP or any other desired routing scheme.","The method of adding variable non-broadcast packets to the data stream during periods of low bandwidth requirement for the non-variable, broadcast packets makes optimal use of the available bandwidth and tends to minimize cases in which the client's cache of visibility event data is outrun by the camera\/viewpoint location, which would otherwise correspond to late packet arrival and visibility errors during the client rendering.","In step , the objects representing the advertising messages are specifically located in a part of the modeled environment such that the objects are likely to be visible to the user in a predetermined amount of time. Also, in step  the scheduling of the variable packet transmission is made so that the objects arrive substantially before they become visible, thereby preventing visibility errors caused by late packet arrival.","Exemplary embodiments can also include the method of adaptively using lower level-of-detail visibility event packets to prevent anticipated late packet arrival which is further discussed in conjunction with the method of  and . As an example, if an object is expected to become visible at time t, a packet at the current level of detail is scheduled to arrive at the client device at a time prior to time t (e.g., time t). However, if it is determined that the packet at the current level of detail cannot arrive at time t, than a packet at a lower level of detail is sent to the client device at time t.","In subsequent step , the non-variable visibility event packets are combined with the variable visibility event packets into a combined data stream indicated as data .","The data stream  is the combined data stream of the broadcast (non-variable) and variable packets that is sent to a specific client unit, indicated by the rectangle .","In step , conducted on a specific client unit, the user may choose to interact with a specific object representing an advertising message. The data representing the time and location (and other information indicating, the object interacted with) is sent from the specific receiver to the server unit. This data transmitted back to the server unit is shown as data labeled .","In step , conducted on the server unit, the user click data received from the client unit data stream indicated by  is used to trigger the read of specific geometric or texture (or parametric) data representing an object associated with the clicked advertisement (AD). In this case, in some embodiments, the represented object includes additional information about the clicked advertisement including a representation of the advertised product or additional product literature or ordering information. In alternative embodiments, the represented object is a click-to-order icon. In some embodiments, a click-to-order icon is displayed on an object in the modeled environment. Upon selection of a click-to-order icon, an order form pops up permitting the user to order a product corresponding to the selected object If the advertised object is a virtual object that may be worn, driven, flown, navigated, or carried by the user's avatar during the remainder of the streaming interactive program then the click may result in a charge to the user as shown in step .","Method of Streaming Advertising and Targeted Advertising During Low Bandwidth Requirement Periods of a Fully Interactive Bidirectional Data Stream.","As previously described,  shows an exemplary embodiment in of the method of inserting variable visibility event data packets into a data stream of non-variable visibility event packets wherein the non-variable visibility event packets represent the majority of modeled objects and the variable visibility event data packets represent advertising, cash store, or messaging objects. In the exemplary embodiment of , both the non-variable and variable visibility event data packets are nominally prefetched based on camera\/viewpoint movement determined by a prescribed camera path, corresponding to an animated programming with limited interactivity.","In some embodiments, the method of  is applied during the streaming of visibility event data is prefetched based on camera\/viewpoint movement that is under interactive control of a client-user. Such an exemplary embodiment is illustrated in the exemplary flow diagram of . In the data store , the delta-PVS or visibility event data for the entire environment is represented in some embodiments. In step , the non-variable visibility event packets are prefetched based on the user's viewpoint location which is under interactive control of the user using the navigation-based prefetch method described in the co-pending PCT patent application number PCT\/US2011\/042309.","In step , the non-variable packets that include the streaming interactive programming are delivered to client units as non-variable visibility event packets. In this case, the non-variable visibility event packets are dynamically prefetched to a specific client unit using navigation-based prefetch in which the camera\/viewer motion is interactively controlled by the client user using a mouse, game controller or other device. In this case, the non-variable visibility event data packets are streamed as unique data stream to a specific client\/user (e.g., game-like content with significant interactivity).","A second data store  stores variable data representing directed advertising messages (or cash store items, or messaging objects) that are streamed to individual, specific client units during the non-broadcast delivery of the non-variable visibility event data according to some embodiments. The directed advertising data is sent as variable packets that are added to the data stream during periods when the bandwidth requirement for sending the non-variable data is low. The determination of current and predicted bandwidth requirements relative to the available bandwidth are made using information including the size of visibility event packets to be prefetched and the viewpoint velocity. Therefore, for example, when the number of visibility event packets that are scheduled for prefetched delivery in a given time are below a predetermined threshold, it is predicted that a user's bandwidth requirement is low permitting the transmission of advertising data to the user. In one exemplary embodiment, in step  the geometric or texture information for an object representing a specific advertising message that will be visible to the user is placed in one or more variable packets that are combined with the, non-variable packets in step .","In some embodiments, the visibility event data of  is actual geometric and\/or texture information. In alternative embodiments, the data is instructions for generating the actual geometric and\/or texture information on the client unit. In one embodiment, the data in the second data store  includes parameters for the parametric construction of the objects representing the geometric and\/or texture information that is the object representing the advertising message. In one embodiment, this parametric information includes references to specific glyphs as well as parameters that describe the extrusion and\/or beveling of the glyph outlines to generate 3D logos or engravings representing the advertising message. In further embodiments, other parametric construction techniques are employed to generate billboards, neon signage, scoreboard and other representations with a custom advertising message.","In some embodiments, the data of store  is associated with a geometric transformation matrix that places the associated object in a location that will likely become visible to the user in a predetermined amount of time, using on the navigation-based prefetch methods specified in the co-pending PCT patent application number PCT\/US2011\/042309 and further described, in additional embodiments, in the present specification.","In step , the data representing the advertising object is streamed as a variable packet during periods when the number and size of variable packets being sent is low. In this exemplary embodiment, the variable packets are sent during periods of time when the bandwidth requirement for streaming non-variable packets is low. Even in implementations in which the non-variable packets represent prescripted visibility event packets that are broadcast simultaneously to multiple clients (e.g., exemplary embodiment of ), each variable packet can potentially be sent to a specific receiver\/client unit during these low-bandwidth requirement periods. The variable packets are sent to one or more specifically targeted receiver units using a packet addressing protocol such as TCP\/IP or any other desired routing scheme in some embodiments.","The method of adding variable non-broadcast packets to the data stream during periods of low bandwidth requirement for the non-variable, packets makes optimal use of the available bandwidth and tends to minimize cases in which the client's cache of visibility event data is outrun by the camera location, which would otherwise correspond to late packet arrival and visibility errors during the client rendering.","In step , in some embodiments the objects representing the advertising messages are specifically located in a predetermined area of the modeled environment such that the objects are likely to be visible to the user in a predetermined amount of time. Also, in step , the scheduling of the variable packet transmission is made so that the objects arrive substantially before they will become visible, thereby preventing visibility errors caused by late packet arrival.","In subsequent step , the non-variable visibility event packets are combined with the variable visibility event packets into a combined data stream indicated as data .","The data stream  is the combined data stream of the non-variable and variable packets that is sent to a specific client unit, indicated by the rectangle .","In step , conducted on a specific client unit, the user may choose to interact with a specific object representing an advertising message. The data representing the time and location (i.e. the. object interacted with) is sent from the specific receiver to the server unit. This data transmitted back to the server unit is shown as data labeled .","In step , conducted on the server unit, the user click data received from the client unit data stream indicated by  is used, in some embodiments, to trigger the read of specific geometric or texture (or parametric) data representing an object associated with the clicked advertisement (AD). In this case, the represented object includes additional information about the clicked advertisement including a representation of the advertised product or additional product literature or ordering information in some embodiments. In alternative embodiments, the represented object is a click-to-order icon.","If the advertised object is a virtual object that may be worn, driven, flown, navigated, or carried by the user's avatar during the remainder of the streaming interactive program then the click may result in a charge to the user as shown in step .","Method of Recording User's Clicks of Advertising Objects on a Visibility Event Server and Sending Record of Clicks to a Website Server which Causes Advertising Urls to be Displayed when User Subsequently Enters Website.",{"@attributes":{"id":"p-0901","num":"0907"},"figref":["FIG. 32A","FIG. 32B"]},"A server unit is labeled . A specific client unit is labeled . In some embodiments, the server unit is connected to multiple client units. According to some embodiments, data representing a streaming interactive media stream is broadcast to all connected client units as non-variable packets.","Data store  stores non-variable delta-PVS or visibility event packets including the streaming interactive media (SIM) in which navigation-based prefetch of the visibility event packets is driven by a prescripted camera path. A second data store  stores variable data representing directed advertising messages that can be streamed to individual, specific client units during the broadcast of the non-variable data. The directed advertising data is sent as variable packets that added to the data stream during periods when the bandwidth requirement for sending the non-variable data is low.","This data store  may also store variable data representing objects that are purchased by the user or gifted to the user in return for clicking on an advertising url associated with an advertising object. In some embodiments, the advertising object, the purchased object, or the gifted object is transmitted from a website server, labeled  in . Alternatively, the data representing these objects may be prestored on the server unit , with data from website server () to the server unit () used to trigger the insertion of specific variable packets stored in  to the visibility event stream using step .","Alternatively, the data from website server  to server unit  may represent parameters for the construction of a gifted object, purchased object, or advertising object; with the parametric data used to actually construct the object using a parametric modeling method executed on the server unit .","In some embodiments, as described later, the data received from the website server  was ultimately triggered by an earlier interaction with an advertising object that was recorded by the server unit  in step  and which sends a record of the interaction to the website server  which stores a record of the interaction in data store .","In step  the non-variable packets representing the streaming interactive programming is broadcast to all receiver\/client units.","In step  the geometric or texture information for an object representing a specific advertising message, gifted object, or purchased object that will be visible to the user is placed in one or more variable packets that are combined with the broadcast, non-variable packets in step .","In some embodiments, the data of  is an actual geometric texture information. In alternative embodiments, the data includes instructions for generating the actual geometric and\/or texture information on the client unit. In one implementation the data of  includes parameters for the parametric construction of the objects representing the geometric and\/or texture information that is the object representing the advertising message. In one implementation this parametric information includes references to specific glyphs as well as parameters that describe the extrusion of the glyph outlines to generate 3D logos or engravings representing the advertising message. In additional embodiments, other parametric construction techniques are employed to generate billboards, neon signage, scoreboard and other representations with a custom advertising message, purchased object, or gifted object.","In some embodiments, the data of store  is associated with a geometric transformation matrix that places the associated object in a location that will likely become visible to the user in a predetermined amount of time, based on the navigational prefetch feedback scheme described previously and in the co-pending PCT patent application number PCT\/US2011\/042309.","In step , the data representing the advertising, gifted, or purchased object (hereafter called the variable object) is placed in a variable packet, which is combined with non-variable packets by sending the variable packets during periods when the number and size of variable packets being sent is low. In this regard, the variable packets are sent during low bandwidth periods for the variable periods. In some embodiments, each variable packet is sent only to a specific receiver\/client unit during these ebb periods. The variable packets are sent to one or more specifically targeted receiver units using a packet addressing protocol such as TCP\/IP or other routing scheme.","The method of adding variable non-broadcast packets to the data stream during periods of low bandwidth requirement for the non-variable, broadcast packets makes optimal use of the available bandwidth and minimizes cases in which the client's cache of visibility event data is outrun by the camera location.","In step , the variable objects are specifically located in a part of the modeled environment such that the objects are likely to be visible to the user in a predetermined amount of time. Also in step  the scheduling of the variable packet transmission is made so the objects arrived substantially before they will become visible, thereby preventing visibility errors.","The data stream  is the combined data stream of the broadcast (non-variable) and variable packets that is sent to a specific client unit, indicated by the rectangle .","In step , conducted on a specific receiver or client unit (labeled ), the user may choose to interact with a specific variable object representing an advertising message. The data representing the time and location (e.g., object interacted with) is sent from the specific receiver to the server unit.","In one implementation, the variable object is accented (e.g., by illumination or other means) for a period of time. A user interaction with the stream during this period of time (e.g., by a user click of a single button) is registered as a interaction with the specific variable object (e.g., advertising object). This allows multiple users to interact with such advertising objects during a single broadcast visibility event stream since it does not require that individual users control the camera path. Once the user has control of the SIM camera path outside the viewcells corresponding to the client-side visibility event cache, then that user would only need a unique bidirectional visibility event data stream that no longer corresponds to the broadcast stream.","In step , in some embodiments, there is a monetary charge to the advertiser sponsoring associated with the clickable advertising object clicked in step ","In step , the user click data received from the client unit data stream indicated by  is used to trigger the read of specific geometric or texture (or parametric) data representing an object associated with the clicked advertisement (AD). In this case, the represented object can include additional information about the clicked advertisement including a representation of the advertised product or additional product literature or ordering information. In alternative embodiments, the represented object may be a click-to-order icon.","In subsequent step , process flow proceeds to process - in , where the data representing a specific click event corresponding to the interaction of the user with an advertising object is sent to a website server unit labeled  in .","The website server unit  stores a record of the specific user's interaction with an advertising object in data store .","In a subsequent step , conducted by the website server , a url link corresponding to the advertised object that was earlier interacted with during the visibility event data stream (the record of this previous interaction having been stored in ) is displayed when the corresponding user visits the website of the website server unit as shown in decision step.","The overall effect is to record and save, for each user, the user's interactions with the advertising or other variable objects during viewing of a SIM\/game visibility event data stream and make them available to the user upon visiting the website.","In decision step , the user may choose to click one or more of the url links displayed in step . This may cause a monetary charge to the advertiser can then be made in the customary manner.","In one implementation, clicking on an advertiser's url in step  can cause the user to gain access to a specific variable object during later viewing of a particular SIM\/game visibility event data stream. If in decision step , it is determined that the variable object for which the user gains access is a \u201cfree gift,\u201d then process flow proceeds to step .","In step  data corresponding to the specific variable object for which the user has gained access by the clicking step  is sent to the visibility event server unit . In some embodiments, this data may includes parameters for constructing the specific variable object or data identifying the variable object that has been pre-stored on the server unit , or actual data representing the variable object, or a combination of all three types of information. Process flow proceeds from  to process -, which returns process flow to step  in .","If, in decision step , it is determined that the object corresponding to the url link displayed in step  and clicked in step  is not a free gift but a chargeable item, then process flow proceeds to step , in which the user is charged for the purchase of this \u201ccash store\u201d variable object.","Method of Streaming Advertising and Targeted Advertising During Low Bandwidth Requirement Periods of a Fully Interactive Bidirectional Data Stream.",{"@attributes":{"id":"p-0927","num":"0933"},"figref":["FIG. 32A","FIG. 32B"]},"According to some embodiments, the method of  and  is also applied to a fully interactive visibility event data stream as shown in . In the data store , the delta-PVS or visibility event data for the entire environment is represented. In step , the visibility event packets are prefetched based on the user's viewpoint location, which is under interactive control of the user using the navigation-based prefetch method described in the co-pending PCT patent application number PCT\/US2011\/042309.","In step , the data stream based on navigational prefetch is combined with the data packets representing targeted, clickable advertising objects and other variable objects in the manner described in conjunction with  and .",{"@attributes":{"id":"p-0930","num":"0936"},"figref":["FIG. 33A","FIG. 33B"]},"A server unit is labeled . A specific client unit is labeled . In some embodiments, the server unit is connected to multiple client units. Data representing a streaming interactive media stream is broadcast to all connected client units as non-variable packets.","Data store  stores non-variable delta-PVS or visibility event packets including the streaming interactive media (SIM) in which navigation-based prefetch of the visibility event packets is driven by a prescripted camera path. A second data store  stores variable data representing directed advertising messages that can be streamed to individual, specific client units during the broadcast of the non-variable data. The directed advertising data is sent as variable packets that added to the data stream during periods when the bandwidth requirement for sending the non-variable data is low.","In some embodiments, this data store  also stores variable data representing objects that are purchased by the user or gifted to the user in return for clicking on an advertising url associated with an advertising object. In some embodiments, the advertising object, the purchased object, or the gifted object is transmitted from a website server, labeled  in . In alternative embodiments, the data representing these objects is prestored on the server unit , with data from website server () to the server unit () used to trigger the insertion of specific variable packets stored in  to the visibility event stream using step .","In alternative embodiments, the data from website server  to server unit  represents parameters for the construction of a gifted object, purchased object, or advertising object; with the parametric data used to actually construct the object using a parametric modeling method executed on the server unit .","As disclosed below, the data received from the website server  was ultimately triggered by an earlier interaction with an advertising object that was recorded by the server unit  in step  and which sends a record of the interaction to the website server  which stores a record of the interaction in data store .","In step , the non-variable packets representing the streaming interactive programming is broadcast to all receiver\/client units.","In step , the geometric or texture information for an object representing a specific advertising message, gifted object, or purchased object that will be visible to the user is placed in one or more variable packets that are combined with the broadcast, non-variable packets in step .","In some embodiments, the data of  is actual geometric texture information. In alternative embodiments, the data may be instructions for generating the actual geometric and\/or texture information on the client unit. In one implementation, the data of  includes parameters for the parametric construction of the objects representing the geometric and\/or texture information that is the object representing the advertising message. In one implementation, this parametric information includes references to specific glyphs as well as parameters that describe the extrusion of the glyph outlines to generate 3D logos or engravings representing the advertising message. In additional embodiments, other parametric construction techniques are employed to generate billboards, neon signage, scoreboard and other representations with a custom advertising message, purchased object, or gifted object.","In some embodiments, the data of store  is associated with a geometric transformation matrix that places the associated object in a location that will likely become visible to the user in a predetermined amount of time, based on the navigational prefetch feedback scheme described previously and in the co-pending PCT patent application number PCT\/US2011\/042309.","In step , the data representing the advertising, gifted, or purchased object (hereafter called the variable object) is placed in a variable packet which is combined with non-variable packets by sending the variable packets during periods when the number and size of variable packets being sent is low. In this regard, the variable packets are sent during low bandwidth periods for the variable periods. In some embodiments, each variable packet may be sent only to a specific receiver\/client unit during these ebb periods. The variable packets are sent to one or more specifically targeted receiver units using a packet addressing protocol such as TCP\/IP or other routing scheme.","The method of adding variable non-broadcast packets to the data stream during periods of low bandwidth requirement for the non-variable, broadcast packets makes optimal use of the available bandwidth and tends to minimize cases in which the client's cache of visibility event data is outrun by the camera location.","In step , the variable objects are specifically located in a part of the modeled environment such that the objects are likely to be visible to the user in a predetermined amount of time. Also, in step  the scheduling of the variable packet transmission is made so the objects arrived substantially before they will become visible, thereby preventing visibility errors.","The data stream  is the combined data stream of the broadcast (non-variable) and variable packets that is sent to a specific client unit, indicated by the rectangle .","In step , conducted on a specific receiver or client unit (labeled ), the user may choose to interact with a specific variable object representing an advertising message. The data representing the time and location (e.g. object interacted with) is sent from the specific receiver to the server unit.","In one implementation, the variable object is accented (e.g., by illumination or other means) for a period of time. A user interaction with the stream during this period of time (e.g., by a user click of a single button) is registered as a interaction with the specific variable object (e.g., advertising object). This allows multiple users to interact with such advertising objects during a single broadcast visibility event stream since it does not require that individual users control the camera path. Once the user has control of the SIM camera path outside the viewcells corresponding to the client-side visibility event cache, then that user would only need a unique bidirectional visibility event data stream that no longer corresponds to the broadcast stream.","In step , there may be a monetary charge to the advertiser sponsoring the clickable advertising object clicked in step .","In step , the user click data received from the client unit data stream indicated by  is used to trigger the read of specific geometric or texture (or parametric) data representing an object associated with the clicked advertisement (AD). In this case, the represented object can include additional information about the clicked advertisement including a representation of the advertized product or additional product literature or ordering information. In alternative embodiments, the represented object is a click-to-order icon.","In subsequent step , the data representing a specific click event corresponding to the interaction of the user with an advertising object is sent to a website server unit labeled  via process - in .","In some embodiments, the website server unit  stores a record of the specific user's interaction with an advertising object in data store .","In a subsequent step , conducted by the website server , a url link corresponding to the advertised object that was earlier interacted with during the visibility event data stream (the record of this previous interaction having been stored in ) is displayed when the corresponding user visits the website of the website server unit as shown in decision step.","The overall effect is to record an save, for each user, the user's interactions with the advertising or other variable objects during viewing of a SIM\/game visibility event data stream and make them available to the user upon visiting the website.","In decision step , the user may choose to click one or more of the url links displayed in step . This may cause a monetary charge to the advertiser (e.g. pay per click model).","In one implementation, clicking on an advertiser's url in step  causes the user to gain access to a specific variable object during later viewing of a particular SIM\/game visibility event data stream. If, in decision step , it is determined that the variable object for which the user will gain access is a \u201cfree gift\u201d then process flow proceeds to step .","In step , data corresponding to the specific variable object for which the user has gained access by the clicking step  is sent to the visibility event server unit  via process - in . In some embodiments, this data includes parameters for constructing the specific variable object or data identifying the variable object which has been pre-stored on the server unit , or actual data representing the variable object, or a combination of all three types of information.","If, in decision step , it is determined that the object corresponding to the url link displayed in step  and clicked in step  is not a free gift but a chargeable item, then process flow proceeds to step , in which the user is charged for the purchase of this \u201ccash store\u201d variable object.","Embodiment with Separate Server and Client Components for Delivering Visibility Event Data Stream with Decoder-Server and Decoder-Client Modules Integrated into Game Engine Modules",{"@attributes":{"id":"p-0956","num":"0962"},"figref":"FIG. 34","b":["3410","3410"]},"In one embodiment, the encoding process runs as an off-line precomputation and generates the visibility event data. In one embodiment, the encoding process is performed on a separate computing device, labeled ENCODER UNIT in . The visibility event packets, which may be represented using the intermediate representation of labeled silhouette contours as described in the co-pending PCT patent application number PCT\/US2011\/042309, is stored in data store . In one embodiment, the data store  is stored on the same physical device as a visibility event decoder\/server unit labeled SERVER UNIT. This device serves the visibility event data packets to a client unit labeled CLIENT UNIT. Additional embodiments include data store  located on multiple physically distributed server units.","During runtime processing a server process labeled  reads the visibility event packets stored in data store  in some embodiments. This reading employs navigation-based prefetch of visibility event packets based either on prescripted (e.g., predetermined) camera motion or on the camera motion controlled by client\/user input using such devices as a mouse, joystick or game controller.","The navigation-based prefetch employs methods specified in the co-pending PCT patent application number PCT\/US2011\/042309 as well as methods specified in the present application.","In exemplary embodiments, depending on the run-time state of the visibility event client process (labeled ), the visibility event client process may read visibility event data from store  at different levels of geometric detail according the method of varying level-of-detail as a function of exposure time described in U.S. Pat. No. 6,028,608, the entire contents of which are incorporated herein by reference, and contours as described in the co-pending PCT patent application number PCT\/US2011\/042309.","In addition, depending on the connection latency between the visibility event decoder of the client unit the visibility event decoder of the server unit, the visibility event decoder of the server unit may actually perform all or part of an algorithmic decompression of packets (i.e., by generating the delta-PVS data from labeled silhouette contours as described in the co-pending PCT patent application number PCT\/US2011\/042309. This results in smaller packets which have a faster arrival time to the decoder-client unit.","The visibility event decoder-client process module is labeled . In exemplary embodiments, the decoder-client process module is implemented on a client unit labeled CLIENT UNIT in . This process module receives the visibility event data stream from the visibility event decoder-server process via the bidirectional data stream labeled . This bidirectional data stream includes the user's current viewpoint location in the modeled environment that is sent from the client to the server unit to support the navigation-based prefetch of visibility event data (). In alternate embodiments, the visibility event data stream  is a unidirectional data stream from the server unit which conveys a streaming interactive media driven by scripted camera motion to the client unit.","In some embodiments, the visibility event decoder-client is implemented as software which outputs cached visibility event data stored in store . This data may be compressed visibility event packets and or decompressed delta-PVS or direct-PVS\/visibility map data. In further embodiments, the visibility event decoder-client is implemented as hardware or a hardware\/software combination.",{"@attributes":{"id":"p-0964","num":"0970"},"figref":"FIG. 34"},"As with existing game engines, the interface to the graphics hardware is through a standard API such as directx or OpenGL, interfacing with a graphics driver. In some embodiments, the graphics hardware is a graphics processor such as NVIDIA's GeForce model.",{"@attributes":{"id":"p-0966","num":"0972"},"figref":"FIG. 35A"},{"@attributes":{"id":"p-0967","num":"0973"},"figref":"FIG. 35A","b":"3505"},"Data store  stores visibility event data representing most of the objects in the modeled environment. This corresponds to non-variable visibility event packets which are delivered to client units using navigation-based prefetch.","In the exemplary embodiment of , the visibility event content data stored in data store  is prefetched to three client units by the visibility event decoder-server process labeled . In this embodiment, the prefetch is driven by a prescripted camera motion path and the visibility event data stream is broadcast to the three client units.","Thus, in the exemplary embodiment , the visibility event server is delivering SIM content (e.g., SIM program A prefetched by process ) having limited interactivity and being, in some embodiments, simultaneously broadcast to the three client units.","A separate visibility event ad server\/cash store server is labeled . In some embodiments, the ad server may also deliver visibility event based messaging objects such as text messages.","The data store  of the visibility event ad server stores variable visibility event data. In the exemplary embodiment of , the visibility event ad server labeled  streams unique variable visibility event packets representing advertising objects to each of the three client units. In this exemplary embodiment, each client unit is receiving the same visibility event based SIM program, but each client also receives unique advertising objects that are seamlessly integrated into the SIM content as variable visibility event packets. Criteria used to determine specific advertising objects prefetched to an individual client-user can include client-user age, gender, prior-advertising clicks as well as other demographic criteria used to drive targeted advertising. The advertising visibility event packets delivered to Visibility Event Client [], labeled , is streamed by ad server decoder-server process  to decoder-client process .","Decoder server process  is shown to be connected to decoder-client process  with a bidirectional communication connection. In some embodiments only the visibility event-based unique advertising objects are delivered over this connection. In some embodiments, a network connection between the client unit  and the server process  is used to deliver information indicating that a client-user has interacted with a clickable advertising object or cash store object. The information transmitted from the client to the server in this example is represented by the double arrowed communication between the decoder-client and the decoder-server.","The advertising objects uniquely delivered to visibility event client [] are selected, in some embodiments, based on known demographic information describing the specific client user. In other embodiments the advertising objects selected for delivery to a specific client-user are determined based on selection clicks of other advertising objects previously sent to the client-user.","In order to deliver unique, variable, advertising visibility event packets into a modeled environment before they will become visible to a client-user, the advertising decoder-server process has knowledge of the current camera\/viewpoint position. In the exemplary embodiment of  this information is delivered from the visibility event content decoder-server (process ) to the visibility event advertising decoder-server processes , , and  through the communication connections indicated by the respective stippled connection lines from the content decoder-server to the advertising decoder-server. In some embodiments information describing the location of the camera\/viewpoint is delivered directly from the decoder-client the corresponding advertising decoder-server through a direct connection.","In the exemplary embodiment of , while all three visibility event clients receives the exact same visibility event-based SIM content from server , each receives a unique set of advertising objects during the SIM program from the visibility event advertising server . The advertising objects received by decoder-client  are sent by decoder-server process . The advertising objects received by decoder-client  are sent by decoder-server process . The advertising objects received by decoder-client  are sent by decoder-server process . In some embodiments a single visibility event-based advertising server can deliver unique advertising, cash store, or messaging content to a several client units receiving the same broadcast visibility event content.",{"@attributes":{"id":"p-0977","num":"0983"},"figref":"FIG. 35B"},{"@attributes":{"id":"p-0978","num":"0984"},"figref":["FIG. 35B","FIG. 35A","FIG. 35A","FIG. 35B","FIG. 35A"],"b":["3545","3505"]},"Data store  stores visibility event data representing most of the objects in the modeled environment. This corresponds to non-variable visibility event packets which are delivered to client units using navigation-based prefetch.","In the exemplary embodiment of , visibility event content data stored in data store  is prefetched to visibility event client [] (labeled  and being the same as the visibility event client [], labeled  in ) and is simultaneously prefetched to visibility event client [] (labeled  and being the same as the visibility event client [], labeled  in ). In the exemplary embodiment of , the visibility event data stream being simultaneously broadcast to visibility event client [] and visibility event client [] is a visibility event data stream driven by a prescripted camera motion path and delivered by visibility event decoder-server process labeled .","In the exemplary embodiment of , the visibility event client [] (labeled  and being the same client unit labeled  in , but in a different state of operation) which was previously (in ) receiving the same prescripted visibility event stream as visibility event client [] and visibility event client [] is not receiving a unique visibility event content stream from the new server process labeled .","The visibility event stream now being received by visibility event client [] as shown in  is a fully interactive visibility event data stream in which the navigation-based prefetch is driven by controller input provided by the client-user of visibility event client [] which is sent to the visibility event decoder-process U[] labeled .","The visibility event cash store\/advertising server labeled  (and being the same visibility event cash store\/advertising server labeled  in , but in a different state of operation) continues to deliver a unique variable visibility event stream including advertising (or cash store, or messaging information) to each of the client units.","In  the unique visibility event advertising data stream delivered to visibility event client [] is sent by visibility event decoder-server process U[] labeled  in . For any given client unit, the visibility data stream from the visibility event content server and the visibility event data stream from the visibility event advertising server are both being prefetched using the same camera\/viewpoint motions. In  the camera\/viewpoint for visibility event client [] has changed from being driven by a prescripted motion path to being driven by interactive client-user input controlling camera\/viewpoint motion. Consequently, in  the visibility event advertising server process delivering unique content to visibility event client [] (the process decoder-server process U[]), receives the controller input information generated by the client-user for visibility event client []. In some embodiments this information is received by the advertising server-decoder process (e.g.  of ) from the corresponding content sever-decoder process (e.g.  of ) as shown by the unidirectional dashed connector between these two processes. In other embodiments the navigational input information is sent directly from the client unit to the visibility event advertising server, e.g. using the bidirectional communication link between decoder-client process  and decoder-server process .","Thus the state of visibility event client [] changes from receiving a broadcast SIM stream (in ) to receiving a unique interactive visibility event stream (in ). Simultaneously, the corresponding visibility event based advertising stream also seamlessly transitions to the new interactive visibility event stream by receiving the interactive camera\/viewpoint motions driving the new interactive visibility event stream instead of the prescripted camera\/viewpoint motion data of the SIM stream that was previously being delivered.","In some embodiments, if a client-user sends the content server an indication that the client-user chooses to transition from receiving a broadcast, non-unique SIM visibility event stream (driven by prescripted camera motion) to a fully interactive visibility event stream (driven by client-user navigational input), then the visibility event content server records this indication and causes a monetary charge to result to the client-user making the transition.","Because the content server must commit additional connection bandwidth and compute resources (i.e. a unique content visibility event decoder-server process) to each decoder-client process that is receiving a unique visibility event stream, the monetary charge made for a unique interactive visibility event content stream can be used to pay for these additional resources. In such embodiments the client-user pays for the additional feature of enhanced interactivity.",{"@attributes":{"id":"p-0988","num":"0994"},"figref":["FIG. 35C","FIG. 35B","FIG. 35B","FIG. 35B"]},{"@attributes":{"id":"p-0989","num":"0995"},"figref":["FIG. 35C","FIG. 35B","FIG. 35C","FIG. 35B"],"b":"2"},{"@attributes":{"id":"p-0990","num":"0996"},"figref":["FIG. 35C","FIG. 24A"],"b":["1","3571","1","3545","2482","2430"]},{"@attributes":{"id":"p-0991","num":"0997"},"figref":["FIG. 35C","FIG. 35C","FIG. 35C"],"b":["3577","1","3561","3575","3582"]},"Methods of Navigation-Based Prefetch for Visibility Event Data","The methods of streaming visibility event data from a server unit to a client unit, as specified in the co-pending PCT patent application number PCT\/US2011\/042309 and as further specified in the present application, employ navigation-based prefetch and integrated caching to prevent late arrival of visibility event packets.","Methods of adaptively selecting lower level-of-detail packets (smaller size packets) for delivery during periods of predicted late packet arrival and\/or high viewpoint velocity are specified in the co-pending PCT patent application number PCT\/US2011\/042309. These methods work in conjunction with navigation-based prefetch that is driven either by prescripted camera motion or by a client\/user's interactive control of camera\/viewpoint movements in the modeled environment.","Embodiments include specific techniques for prefetching visibility event packets based on viewpoint navigation in a modeled environment.","As described in the co-pending PCT patent application number PCT\/US2011\/042309, the bandwidth requirement for streaming of visibility event data is a function of the spatiotemporal visibility coherence of the visualization. Spatiotemporal visibility coherence is determined by the spatial visibility coherence of a modeled environment (e.g., the inverse of the rate of change of visibility per unit distance), together with the velocity of the viewpoint. Bandwidth requirement is also related to the level-of-detail at which the geometry and texture information is represented in the visibility event packets sent. As specified in the methods of PCT patent application number PCT\/US2011\/042309, the level-of-detail of visibility event packets selected for transmission can be dynamically selected to prevent late packet arrival as specified in the co-pending PCT patent application number PCT\/US2011\/042309.","In some embodiments, prefetch is a general method of data delivery which hides latency or improves hit rate by pre-sending data to a client unit before it is actually needed by the client unit. In the context of visibility event data streaming, prefetch intrinsically decreases the probability of late packet arrival, which would otherwise cause noticeable visibility errors during the streamed visualization.","In general, regardless of the application, prefetch actually increases the amount of data streamed, since inevitably more data is delivered to a client unit than will actually be used.","The amount of additional data that is streamed by a prefetch implementation (i.e., the bandwidth requirement of the prefetch implementation) is determined, in part, by the precision with which the prefetch algorithm predicts future data accesses. The precision of future data accesses (i.e., hit rate) is a function of the intrinsic predictability of future data access given knowledge of the current data being accessed.","The following exemplary equations illustrate principles of operation and provide tools for estimating the bandwidth requirements for one embodiment of the methods of visibility event streaming driven by navigation-based prefetch. In additional embodiments, the principles of operation and bandwidth requirement estimates are governed by other theoretical models and equations.","In some embodiments, the bandwidth requirement for navigation-based prefetch of visibility event data is determined by the general parameters as specified in Equation 1.\n\nBandwidth Requirement=\u2003\u2003Equation 1\n","where\n\n","The inverse of the product of viewpoint velocity and visibility gradient reflects the coherence of visibility over time and is called the temporal visibility coherence, as shown in equation 2.\n\nTemporal Visibility Coherence==1\/()\u2003\u2003Equation 2\n","Temporal visibility coherence reflects the average visible lifetime of graphical elements during a visualization and can be expressed in units of time.","The bandwidth requirement is also proportional to the level-of-detail of the visibility event data being delivered, as shown in Equation 3.\n\n\u2003\u2003Equation 3\n","where\n\n","As described in the methods of , as well as  and  of PCT patent application number PCT\/US2011\/042309 the level of detail can be adaptively lowered during periods of low temporal visibility coherence in order to decrease bandwidth requirements and prevent late packet arrival while maintaining a perceptually lossless visibility event data stream in which the low level-of-detail substitutions cannot be easily perceived by the user's visual system. The reduced performance of the human visual system for surfaces having low exposure time is discussed in conjunction with  of the present application. Additionally, the reduced performance of the human visual system for rapidly moving targets is discussed in conjunction with  of the PCT patent application number PCT\/US2011\/042309. Both low exposure time and rapid image-space velocity of visual targets are encountered during periods of low temporal visibility coherence.","For any integrated prefetch-cache scheme, the amount of data sent is generally greater than the amount that will be accessed. The average cache size is influenced by the ratio the amount of data sent to the amount of data accessed at a given cache hit rate. This ratio is strongly influenced by the predictability of the data access, which for navigation-based prefetch is determined by the predictability of the navigation. Consequently, a highly predictable navigation will reduce the bandwidth requirements as shown in equation 4.\n\n\/()\u2003\u2003Equation 4\n","where\n\n","The predictability of navigation can be increased by imposing constraints on navigation, e.g. restricting navigation to a pathway in the modeled environment that is defined a-priori or that is imposed during navigation.","Note that the predictability of navigation is also intrinsically influenced by the maximum velocity and acceleration that is part of the underlying navigational model of the game or SIM. Consequently the Pand the Vare not strictly independent, although factors like navigational constraints can influence Pwithout directly influencing V.","In the next section, specific methods of delivering visibility event packets by navigation-based prefetch are disclosed. These methods increase cache hit rate and minimize bandwidth requirement by prefetching visibility event data packets using a packet priority or packet streaming rate that reflects the probability that the viewcell corresponding to packet will be the next viewcell penetrated by the viewer.",{"@attributes":{"id":"h-0038","num":"0000"},"figref":"FIG. 36A"},{"@attributes":{"id":"p-1012","num":"1025"},"figref":"FIG. 36A"},{"@attributes":{"id":"h-0039","num":"0000"},"figref":"FIG. 36A"},"The motion of a viewpoint in a 3 dimensional space is determined, in some embodiments, by the velocity and acceleration of the viewpoint along each axis of the space.","The following equations are employed, in some embodiments, in motion models for games and other walkthough type applications:\n\nposition\u2003\u2003(5)\n\nvelocity\u2003\u2003(6)\n\nacceleration\u2003\u2003(7)\n\nposition\u2003\u2003(8)\n\nvelocity\u2003\u2003(9)\n\nacceleration\u2003\u2003(10)\n\nposition\u2003\u2003(11)\n\nvelocity\u2003\u2003(12)\n\nacceleration\u2003\u2003(13)\n","Where Jis the \u201cjerk\u201d or first derivative of acceleration in the direction N.","In most game applications the velocity, acceleration and even the jerk) are limited to a specific range of values which constrain the viewpoint motion.","The value dT is the timestep used in the calculation. The ping time (i.e., round-trip-time) between the visibility event server and client units effectively represents a minimum value of the timestep that can be realistically employed to estimate a future value of a navigational parameter from a current value. This is because if the server unit is determining a future value of a navigational parameter using a \u201ccurrent\u201d value received from a client unit, the current value is at least dT time units old, where dT is the ping time between the server and the client. In alternative embodiments, if the determination of the navigational parameter is made by the client and sent to the server, the result used by the server unit is still at least dT units later than the client's actual value.","Thus, the ping time or latency between the client and the server creates an uncertainty in position, velocity, and acceleration which can decrease the accuracy of the navigation prediction algorithm.","In some embodiments, given current values for position, velocity, and acceleration a future value of position can be computed assuming that velocity and\/or acceleration remains constant. This is the basis of dead reckoning navigational predictions used in multiplayer games to predict a future position of a player from a previously known position. In such applications, predictions are used to reduce the frequency at which actual player states are transmitted across the network, thereby reducing the network traffic. The basic method of simple navigation-prediction by dead-reckoning is described in the publication \u201cSimplified Aerial Navigation by Dead Reckoning, McMullen, J. Charles Griffin & Co. 1933\u201d incorporated herein by reference.","In the present embodiment directed to navigation-based prefetch, navigation prediction is not used solely as a substitute for receiving actual position, velocity, and acceleration data from a client unit (which typically consumes a small amount of bandwidth), but primarily to predict the probability of a viewcell being penetrated in the future given current values of the navigational parameters. Increasing the precision of this prediction decreases the amount of data that is prefetched, since a highly predictable navigation generally uses a smaller cache to insure a high hit rate (low incidence of late packet arrival).",{"@attributes":{"id":"p-1021","num":"1034"},"figref":"FIG. 36A"},"In the following discussion, in some embodiments, it is assumed that viewcells are located in the navigable space of the modeled environment. The method accommodates embodiments in which viewcells are equivalent to navigation cells. It also accommodates embodiments in which the viewcells are contained within navigation cells and vice versa.","In some embodiments, viewcells are restricted to the empty navigable space of the environment (i.e. regions that do not contain objects). In other embodiments viewcells may contain modeled objects, in which case the modeled objects contained in the viewcell are considered to be part of the potentially visible set for the corresponding viewcell. Navigation cells, in some embodiments, are located in the navigable space of the environment. In some embodiments, such as those embodiments in which collision detection employs voxel representation, navigation cells are also restricted to the empty navigable space of the modeled environment or can include the boundary between empty or occupied space. In some embodiments navigation cells may contain modeled objects and the collision detection algorithm within a navigation cell uses the modeled objects within the navigation cell.","Consequently, in some embodiments, viewcells and navigation cells, are equivalent.","In step  the viewcell containing the viewer's current viewpoint is determined. The viewcell containing the current viewpoint is identified using the reference variable C_VIEWCELL.","In step , the viewcells adjacent to C_VIEWCELL are identified. In exemplary embodiments, a viewcell is a parallelepiped having six faces. A parallelepiped is a box-like structure which may have a cube-like or rectangular shape. Other convex viewcell arrangements are possible. Adjacent viewcells, in some embodiment, include those viewcells sharing a face with C_VIEWCELL. For example, a C_VIEWCELL that is surrounded by viewcells that are all the same size as C_VIEWCELL has 6 adjacent face-connected neighbors. If the neighboring viewcells are smaller than this number may be greater. Adjacent viewcells are identified using the set of viewcell data structures, from which contains cell adjacent information. Alternate embodiments consider viewcells to be adjacent if they share a face or part of an edge. Using this definition of adjacency, for example, a viewcell surrounded by viewcells of the same size has 18 adjacent neighbors. If vertex connected neighbors are also considered to be connected if they share a vertex, then a viewcell surrounded by viewcells of the same size has 26 adjacent neighbors.","In subsequent decision step , it is determined, for an a adjacent viewcell, if the visibility event packet corresponding to the viewcell-to-viewcell transition between C_VIEWCELL and the adjacent viewcell has already been fetched. If, in decision step , the visibility event packet for the adjacent viewcell has already been fetched, then process flow proceeds to step , in which the next adjacent viewcell is processed.","If, in decision step , it is determined that the visibility event packet corresponding to the viewcell-to-viewcell transition between C_VIEWCELL and the adjacent viewcell has not already been fetched, then processing process flow proceeds to step .","In step , the time until the current viewpoint penetrates the boundary between C_VIEWCELL and the adjacent viewcell is estimated. This time period is a prediction based on the current viewpoint location, velocity and possible acceleration vectors. This prediction employs, in some embodiments, a variety of methods which have different computational costs and which lead to solutions having different degrees of precision.","In one embodiment, the estimation of viewpoint penetration time determined in step  is made assuming constant velocity vector over time. This is a simple dead reckoning linear solution. Alternative embodiments employ an assumption of constant acceleration, in which the viewpoint motion path is described by a quadratic space curve, or a piecewise linear approximation to the space curve.","The aforementioned techniques in which constant velocity or acceleration are assumed lead to a single solution for an estimated penetration time. Alternate embodiments employ more refined predictions which do not assume that velocity or acceleration are constant during the prediction period. In one embodiment, the prediction employs a set of values for initial viewpoint position, velocity, and acceleration that are possible during the prediction period. In this technique, the set of values used follows a distribution curve reflecting the probability that a specific value of initial position, velocity, and acceleration occurs during the prediction period. This distribution reflects the motion constraints imposed by the maximum and minimum values of velocity and acceleration. Additionally, the distribution can be weighted by such factors as the predicted intentionality of the user based on statistics gathered from many user's navigating the same viewcell.","Using a distribution of possible navigational parameters during prediction in a Monte Carlo simulation of possible viewpoint motion curves produces a variety of results for predicted penetration time between C_VIEWCELL and the adjacent viewcell.","In step  the relative probability that a specific viewcell adjacent to C_VIEWCELL is penetrated is determined using both the number of viewpoint motion curves that intersect the adjacent viewcell and the estimated time to penetration of these space curves. This probability is computed, in some embodiments, for each of the adjacent viewcells for which the corresponding visibility event packet has not yet been fetched, and the probabilities are compared to determine a relative probability that a specific adjacent viewcell will be the next viewcell penetrated. Conducting a Monte Carlo-type prediction of a viewpoint motion at runtime can incur a high computational cost. The present method includes embodiments in which these predictions are determined offline, and the results stored in a look-up table for a given set of initial position, velocity and acceleration values. Exemplary data for one embodiment of such a look-up table for a single viewcell\/navcell is given in  and discussed in connection with that figure. This table reflects the probability of penetration of adjacent viewcells given a starting viewcell and the initial parameter values. Since the arrangement of adjacent viewcells around a starting viewcell defines a finite number of patterns that recur frequently in the viewcell adjacency graph of the model, a limited number of tables need to be precomputed and stored. In subsequent step , the visibility event packets corresponding to adjacent viewcell penetrations are fetched using a priority or streaming rate that is proportional to the estimated probability that the corresponding adjacent viewcell will be penetrated.","The method accommodates prefetch of visibility event packets corresponding to non-adjacent viewcell transitions by using a specific adjacent viewcell as a starting point, determining the relative probability of penetrating the viewcells adjacent to the specific adjacent viewcell and then multiplying the probabilities.","In one embodiment, each visibility event packet is completely streamed to the client, but the order of packets streamed is selected to be proportional to the packet's relative probability of penetration. In some embodiments, the priority or order of packets streamed are weighted to be proportional to the size of the individual packet.","Other embodiments are possible in which data from multiple packets are streamed simultaneously by selecting subpackets or bytes from each packet and assigning a streaming rate to each substream that is proportional to the relative probability of penetration of the corresponding viewcell. In some embodiments, the streaming rate for a visibility event packet is weighted to be proportional to the size of the visibility event packet. For example, an equation determining the bandwidth to be assigned to streaming a subpacket is Equation 14:\n\n()+\u2003\u2003Equation 14\n","where\n\n","In step  cached visibility event packets and corresponding PVS data corresponding to viewcells having a low probability of penetration are selected for replacement if cache replacement is required to control cache size.",{"@attributes":{"id":"h-0040","num":"0000"},"figref":"FIG. 36B"},"As previously indicated, the probability that a specific neighboring viewcell of a viewcell containing an actual or predicted viewpoint position will be penetrated is a function of several parameters. The probability of penetration (POP) of a neighboring viewcell is determined by factors including the current position of the actual or predicted viewpoint in the starting viewcell, the velocity vector, intrinsic restrictions to navigation at the viewcell boundaries, navigational decisions made by the same user or multiple other users navigating the starting viewcell and many other factors.","In one embodiment, the aggregate effect of these factors is determined (including, in some embodiments, as data mined from many navigational experiences of many client users) is compiled into a multidimensional look-up table.","An exemplary sub-table for such a navigation-prediction table is displayed as . In  the relative POP of the six faces of viewcell VC[][][] is expressed as a function of the velocity vector of the actual or predicted camera\/viewpoint on entry to viewcell VC[][][]. In some embodiments, sub-tables for the camera\/viewpoint position are also included.","The use of look-up tables, in some embodiments, requires less cpu resources than other methods of navigation-prediction, but may have more storage requirements.","Method of Adaptively Imposing Navigational Constraints to Prevent Late Packet Arrival","The parameter Pof Equation 4, predictability of navigation, reflects the fact that from any given viewcell, the number of reachable adjacent viewcells may be limited. These navigation constraints can be caused by objects in the environment which restrict navigation on the basis of collision, e.g. with walls, curbs, guardrails etc.",{"@attributes":{"id":"p-1044","num":"1062"},"figref":"FIG. 37"},"In decision step , it is determined if, given the current states of the client visibility event cache and the current viewpoint position, velocity, and acceleration vectors, as well as the arrangement of the viewcell adjacency graph for the currently occupied viewcell, late packet arrival is possible.","Late packet arrival is predicted if a probability-weighted and packet-size-weighted navigation model (Equation 14) using dead-reckoning (Equations 5-13) or probabilistic dead reckoning indicates that a visibility event packet will arrive at a client unit after the viewpoint crosses the viewcell boundary corresponding to that visibility event packet.","If, in decision step , it is determined that late packet arrival is not possible, then process flow proceeds to step , in which the nominal prefetch mode is continued.","If, in decision step , it is determined that late packet arrival is predicted, then process flow proceeds to step .","In step , the system imposes a navigational constraint at one or more of the current view's boundaries with adjacent viewcells. In one embodiment, this constraint causes one or more viewcell boundaries to act as collision objects that prevent the viewpoint from penetrating an adjacent viewcell.","According to Equation 1, causing such a restriction to navigation lowers the bandwidth requirement for prefetch. By closing off navigation between the current viewcell and one or more adjacent viewcells, the available transmission bandwidth can be dedicated to the non-restricted packets corresponding to the remaining non-restricted viewcell boundaries. This presents the advantage of decreasing the bandwidth requirement and tends to prevent late packet arrival for the unrestricted viewcell boundaries.","In some embodiments, viewcell boundaries chosen for restriction are those for which restriction produces the greatest reduction in the probability of late packet arrival, and also tends to cause the least disruption of intended navigation.","In some embodiments, boundaries having the following characteristics are good candidates for adaptively imposing navigational restriction:\n\n","The imposition of navigational restriction at viewcell boundaries which do not corresponding to natural collision surfaces in the modeled environment tends to increase the precision of the navigation-prediction algorithm, and thereby increase the hit rate of the navigation-based prefetch algorithm, which can prevent late packet arrival. However, the imposition of such navigational constraints can also reduce the freedom of movement of a client-user, which diminishes the level of interactivity and immersion in the modeled environment. The method of selecting boundaries with low relative probability of penetration for navigation restriction tends to decrease the frequency with which the viewer tends to encounter one of these seemingly arbitrary navigational restrictions.","An optional technique to minimize the arbitrariness of the imposed navigational restrictions and thereby make navigation more predictable to the client-user is cause the imposed navigational constraint to be represented to the user. This representation includes, in some embodiments, a translucent boundary which indicates the navigational restriction. This optional step is shown as step .","Method of Navigation-Based Prefetch of Visibility Event Data in which Viewcells\/Navigation Cells on a Defined Roadway or Other Path are Assigned Weights Giving a Higher Probability of Penetrations than Cells not on Path.","The bandwidth requirements for navigation-based prefetch of visibility event (VE) packets is determined, in some embodiments, by several factors including the size of visibility event packets. The size of a visibility event packet is a function of the visibility coherence at the corresponding visibility event boundary as well as the precision of the corresponding PVSs. As previously described, VE packets or subpackets are also encoded at a plurality of levels-of-detail, with the lower level-of-detail packets having a smaller size.","In addition to the size of the VE packets, bandwidth requirement for VE prefetch is also determined by the velocity of navigation (i.e. number of visibility event boundaries actually crossed by a viewpoint\/camera position in a given time period) as well as the predictability of navigation.","A more predictable, restricted navigation path, e.g. navigation constrained to a roadway or a footpath, will require maintenance of a smaller visibility event cache, since the number of VE boundaries potentially reachable in a specific period of time is limited by such navigational constraints.","As previously discussed,  is a flowchart showing a method of adaptively decreasing bandwidth requirement by imposing a restriction on navigation.",{"@attributes":{"id":"p-1059","num":"1079"},"figref":"FIG. 38"},"Turning now to , in decision step  it is determined if the viewpoint (which may also include a predicted viewpoint location) is on or near a road or other defined path. Exemplary embodiments are possible in which the path is a footpath, flyway, waterway or other navigation path.","If, in decision step  it is determined that the viewpoint is on or near a defined path, then processing proceeds to step . In step , viewcells in the vicinity of the current viewpoint and located on the same path as the current viewpoint are assigned a relatively higher probability of penetration that viewcells that are not on the same path. By increasing the relative probability of penetration of viewcells on the path, the prefetch method of , step  will tend to preferentially prefetch VE packets for path viewcells by dedicating a higher streaming rate or priority order than VE packets corresponding to non-path viewcells.","By preferentially prefetching VE packets corresponding to path viewcells, the method of  allows a higher viewpoint velocity if the viewpoint navigation is restricted to the path, since at any given available streaming bandwidth more of the available streaming is dedicated to delivering VE packets corresponding to path viewcells.","In many common instances, users naturally tend to follow defined paths through a modeled environment. The method of  will tend to allow higher navigational velocities on such paths.","If, in decision step , it is determined that the viewpoint is not on a defined path, then processing proceeds to step . In this case, the predictability of navigation is lower, since the degrees of freedom in moving to adjacent viewcells is increased. This situation generally requires the maintenance of a larger VE cache which, in turn, requires more bandwidth. If the required bandwidth exceeds the available bandwidth then late packet arrival may occur as the actual viewpoint location outruns the viewcells corresponding to cached VE packets. In this case, the velocity of viewpoint motion can be reduced (as described in conjunction with the method shown in exemplary flow diagram B the co-pending application.)","In optional step , a visual indication of off-path regions is displayed to the user which indicates that a lower maximum viewpoint velocity may occur in these regions. In one exemplary embodiment the off-path regions contain shrubs or other objects which naturally tend to reduce navigational velocity. The display of these types of objects in off-path areas will signal the user that velocities may be limited in these regions.","Method of Imposing Navigational Constraints to Prevent Late Packet Arrival in which the Navigational Constraint is Accompanied by a Visual Indication of Loss of Stealth","The method of imposing navigational constraints to prevent late packet arrival described in the exemplary flowchart of  admits any restriction to navigation including those caused by collision or other impediment to navigational progress.","Method of Restricting Navigation During Loss of Stealth Status Coincident with Decreased Visibility Coherence",{"@attributes":{"id":"p-1067","num":"1087"},"figref":"FIG. 39A"},"Imposed navigational constraints should not interrupt the continuity or interactivity of the experience delivered by visibility event data stream in an arbitrary or unpredictable way. The method of the exemplary flowchart allows the user to predict locations in which navigational velocity may be restricted (e.g., by display of such navigational restrictions in step  of , which can occur before the navigational restriction).","In some embodiments, the method of exemplary flow diagram , the predictability of navigational constraints, is not tied to existing restrictions previously displayed in the environment, but a navigational model in which a stealth status of the user or user operated vehicle or avatar is tied to the user's movements in the environment.","In one example embodiment, stealth is compromised with sustained rapid movement in an environment. In this regard, sustained rapid movement increases the likelihood of late packet arrival since the bandwidth required for the client device associated with the vehicle or avatar has increased.","Other examples are possible in which stealth is also compromised when a viewer movement into a region causes the viewer to become exposed to more reflected light from surfaces in the environment. Since the amount of surface area that the viewer is exposed to is a function of the size of the PVS, the rate of change of exposed surface area inversely correlates with local visibility coherence of the model.","Both rapid motion and a high rate of exposure of new surfaces can contribute to low temporal visibility coherence which can threaten late arrival of visibility event packets. By restricting navigation under these conditions using a navigational model linked to stealth, late packet arrival can be prevented in a way that is logical and predictable to the user.","Turning now to , in decision step  it is determined if late packet arrival is predicted. If late packet arrival is not predicted, then process flow proceeds to step . In step , an indication that the display is not compromised is displayed. In exemplary embodiments, this indication of stealth can include displaying an avatar or vehicle as translucent, transparent or otherwise difficult to see. An indicator gauge indicating level of stealth can also be displayed. Embodiments employing other displayed indications of compromised stealth are possible.","If, in decision step , it is determined that late packet arrival is predicted then process flow proceeds to step . In step  an indication that stealth status is compromised is displayed. As previously described, this indication can include, in various embodiments, a change in translucency, transparency, stealth gauge or other representation to the user that stealth status is compromised.","In step , navigation is restricted. This restriction may include an imposed limitation of navigation caused by detection by an enemy or adversary, for example. In one exemplary embodiment, a loss of stealth results in consequences of detection which rapidly restricts navigation by engaging the player in an attack or other hindrance. For example, loss of stealth may trigger an AI attack which impedes forward progress. Other embodiments are possible in which the velocity of the user's avatar or vehicle is automatically slowed in order to prevent detection.","In step , the cause of the navigational restriction is displayed. Exemplary embodiments this cause may include engaging enemies, traps, stasis fields linked to the stealth model or other possible restrictions.",{"@attributes":{"id":"h-0045","num":"0000"},"figref":["FIG. 39B","FIG. 39C"]},"The methods of , , , and , in some embodiments, result in a client-user experiencing limited navigation during periods of low spatiotemporal visibility coherence. This limited navigation may take the form or reduced velocity (e.g.  and ) and\/or actual restrictions which prevent movement into certain areas of the modeled environment (e.g.  and ). The predetermined and adaptive imposition of limits on user navigation, according to the described methods, reduce the probability of late visibility event packet arrival. By preventing late packet arrival, these methods prevent the visually disruptive effect of missing graphical objects which can suddenly \u201cpop\u201d or appear in the scene some time later than when they actually should have become exposed.","In some embodiments, these methods of adaptively imposing navigational limits are only implemented if the methods of sending low level-of-detail packets (e.g.  and ), or small visibility event packets (e.g. ) do not prevent the late packet arrival. Although these methods of imposing navigational limits can prevent the visual disruption caused by late packet arrival, the resulting limitation of user camera\/control can disrupt a client user's sense of control and immersion.","One method of preserving the sense of control and predictability during imposed limits on navigation is to display an indicator to the client user that such limits are or soon will be experienced by the user.","As described in step , for , one circumstance in which navigational limits may logically imposed is in a stealth game or training application in which high velocities in certain areas of the modeled environment (e.g. terrain which creates noise when traversed, or which is occupied by motion detectors) results in loss of stealth. In some embodiments, this loss of stealth results in detection which produces consequences that limit navigation (e.g., attacks).","In some embodiments, the stealth level of a user avatar or vehicle is set to be a function of the probability of late packet arrival which is determined, in some embodiments, as a function of the spatiotemporal visibility coherence of the visibility event data stream and\/or the state of the visibility event cache of the decoder-client process. A gauge reflecting the probability of late packet arrival is displayed as a gauge or other indicator of stealth status.",{"@attributes":{"id":"p-1082","num":"1102"},"figref":"FIG. 39B"},"In other types of games, such as Assassin's Creed (Ubisoft Inc.), the user's camera\/viewpoint moves in an alternate space-time in which the navigation of the user avatar is subject to maintaining a type of \u201csynchronization\u201d which allows the space-time translocation to continue.","In one embodiment of the method of , the ability of a user to navigate without limits is dependent on a navigational model is set to be an inverse function of the rate of exposure of the avatar or vehicle to new surfaces in the modeled environment. In this navigational model, high user velocities in regions of low spatial visibility coherence intrinsically compromise \u201csynchronization\u201d or other parameter describing the ability to navigate unrestricted in a space-time region. In this embodiment, the underlying navigational model is set to have the same performance characteristics as the visibility event delivery codec, namely the performance of both are a function of the spatiotemporal visibility coherence of the visualization.",{"@attributes":{"id":"p-1085","num":"1105"},"figref":"FIG. 39C"},{"@attributes":{"id":"h-0046","num":"0000"},"figref":"FIG. 40A"},{"@attributes":{"id":"p-1086","num":"1106"},"figref":"FIG. 40A","b":["1","1","1","1"]},"The viewcells which contain viewcell-viewcell transitions that correspond to visibility event packets currently cached by the decoder-client process are shown in  for the camera\/viewpoint location labeled VP. The prefetch of visibility event packets is determined by the current camera\/viewpoint position as well as velocity and, in some embodiments, acceleration vectors using simple dead reckoning methods of navigation prediction or, in some embodiments, using the method of navigation prediction described in conjunction with  and . In the exemplary case illustrated in , viewcells in the second column of the viewcell matrix other than Viewcell [, , ] and Viewcell [, , ] are not shown since the navigation prediction used in this example has determined that the probability that Viewcell [, , ] and beyond will be penetrated in a specified time in the future is low. The visibility event packets corresponding to the undisplayed viewcells have not been sent by the decoder-sever, and are consequently not represented in the visibility event cache corresponding to client viewpoint position VP.","Given the set of potentially visible sets (PVSs) that can be assembled from the visibility event packets (including delta-PVS data and, in some embodiments, labeled silhouette contour information) cached for the exemplary camera\/viewpoint location VP, only graphical objects  and  are potentially visible. That is, for the viewcells shown in , only objects  and  are potentially visible.",{"@attributes":{"id":"h-0047","num":"0000"},"figref":["FIG. 40C","FIG. 40A"]},{"@attributes":{"id":"h-0048","num":"0000"},"figref":"FIG. 40B"},{"@attributes":{"id":"p-1089","num":"1109"},"figref":"FIG. 40B","b":["2","2","2","2"]},"The viewcells which contain viewcell-viewcell transitions that correspond to visibility event packets currently cached by the decoder-client process are shown in  for the camera\/viewpoint location labeled VP. The prefetch of visibility event packets is determined by the current camera\/viewpoint position as well as velocity and, in some embodiments, acceleration vectors using simple dead reckoning methods of navigation prediction or, in some embodiments, using the method of navigation prediction described in conjunction with  and . In the exemplary case illustrated in , viewcells in the second column of the viewcell matrix other than Viewcell [, , ] and Viewcell [, , ] are not shown since the navigation prediction used in this example has determined that the probability that Viewcell [, , ] and beyond will be penetrated in a specified time in the future is low. Moreover the visibility event packets associated with Viewcell [, , ] and Viewcell [, , ] which were cached for the earlier camera\/viewpoint location VP are no longer present in the cache for VP. In this exemplary case, the previously cached visibility event packets Viewcell [, , ] and Viewcell [, , ] have been replaced using a cache replacement method. In some embodiments, a cache replacement method such as LRU (least recently used) or another replacement method which determines the currently cached visibility event packets having corresponding viewcell transitions that are least likely to be penetrated in a specified time given the position and velocity vectors of the current camera\/viewpoint location. The method, in some embodiments, employs prior-art techniques of cache replacement, as described in the exemplary publication \u201cWeb Caching and its Applications, Nagaraj, S. 2004 Kluwer Academic Publishers, ISBN 1-420-8050-6\u201d, incorporated herein by reference. The cached visibility event packets corresponding to VP include information that, in some embodiments, is used to construct the PVSs for one or more of the viewcells shown in . These PVSs corresponding to the visibility event cache for VP include objects  and . As shown by their absence in , objects  and  are not included in the PVSs corresponding to the visibility event cache for VP.","Object  is a motorcycle which, in some embodiments, corresponds to a cash store object that can be purchased and used by a client-user. In some embodiments, the exemplary motorcycle object  is purchased by clicking on the object using a cursor or crosshair, or otherwise selecting the object. In some embodiments, the purchased cash store object is then made available for use during the game or SIM program (e.g. the motorcycle becomes available to ride).","As the motorcycle object  is not visible from the viewcells corresponding to the visibility event cache of viewpoint location VP, but becomes visible from the viewcells corresponding to the visibility event cache of a later viewpoint location VP, the graphical object  is delivered to the client-decoder process by a client-server process after the time corresponding to VP and at or before a time corresponding to VP. The client-server process delivering this cash store item is, in some embodiments, a decoder-server process that is operating on a separate server device than the corresponding decoder-server process that is delivering the non-cash store items (e.g.,  and ), as shown in the  separate decoder-sever processes (,,) delivering unique cash store variable visibility event packets to the corresponding decoder-client processes during a broadcast of the same visibility event content delivered from a separate decoder-server process  to all decoder-client processes.","Object  is a billboard containing a variable message which, in some embodiments, corresponds to an advertising object that can be selected by a client-user. In some embodiments, the exemplary billboard object  is selected by clicking on the object using a cursor or crosshair, or otherwise selecting the object. In some embodiments selecting the advertising objects also makes available a cash store object purchased cash store for use during the game or SIM program.","As the billboard object  is not visible from the viewcells corresponding to the visibility event cache of viewpoint location VP, but becomes visible from the viewcells corresponding to the visibility event cache of a later viewpoint location VP, the graphical object  is delivered to the client-decoder process by a client-server process after the time corresponding to VP and at or before a time corresponding to VP. The client-server process delivering this advertising object is, in some embodiments, a decoder-server process that is operating on a separate server device than the corresponding decoder-server process that is delivering the non-advertising content objects (e.g.  and ). As shown in the , separate decoder-sever processes (,,) delivering unique advertising variable visibility event packets to the corresponding decoder-client processes during a broadcast of the same visibility event content delivered from a separate decoder-server process  to all decoder-client processes.","The delivery of variable, advertising, cash-store, or messaging objects, as visibility event packets makes very efficient use of transmission bandwidth and rendering resources, since, in some embodiments, these variable objects are delivered to specific decoder-client units only just before they will become visible to a specific client-user. This is more efficient than existing methods of in-game advertising delivery, in which advertising objects, which are not customized for, or targeted to an individual client-unit, are distributed a-priori throughout the modeled environment and rendered without the benefit of a highly precise PVS maintained by visibility event streaming.",{"@attributes":{"id":"h-0049","num":"0000"},"figref":["FIG. 40D","FIG. 40B"]},{"@attributes":{"id":"h-0050","num":"0000"},"figref":["FIG. 40E","FIG. 40D"]},"As described in conjunction with , , , and , in some embodiments, the maximum velocity of navigation is restricted in certain viewcells\/navigation cells that are not on specified navigational paths (also called preferred navigational paths). See step  of .","In , a region of the navigable space of the modeled environment in which the maximum velocity of navigation is decreased is shown as the hatched region labeled . In some embodiments such a region would correspond to a ground region that, in some embodiments, includes shrubs, mud or other modeled hindrances that would logically decrease the maximum velocity of navigation. The display of these modeled hindrances is described in step  of the process illustrated in .","As illustrated in , the number of viewcells for which visibility event data is cached is greater in the column of viewcells that do not include the hatched hindrance region labeled . For example, visibility event data for viewcell [,,] is cached, whereas visibility event data for viewcell [, , ] is not cached even though viewcell [, , ] is closer to VP than viewcell [,,]. This exemplary cache state is effected, in some embodiments, by the process illustrated in step  of , in which, during the conduct of a navigation prediction algorithm, predicted penetration of a viewcell is assigned a higher probability if the viewcell is on a preferred path. In the example of , those viewcells not including a hindrance region are included in the preferred path.",{"@attributes":{"id":"h-0051","num":"0000"},"figref":["FIG. 40F","FIG. 40E","FIG. 40E"]},"As illustrated in the exemplary embodiment illustrated in , and according to the method of ; the state of visibility event cache for the current viewpoint VP, as reflected in the displayed viewcells of , illustrates that there are more cached viewcells in the region of unrestricted navigation than in the region of restricted navigation. (See step )",{"@attributes":{"id":"p-1100","num":"1120"},"figref":"FIG. 40F"},"Compared to the example of restricted navigation illustrated for , the number of visibility event packets corresponding to viewcells in column viewcell [, , N] is increased, reflecting the lack of navigational hindrance in this column.","Conversely, the number of visibility event packets corresponding to viewcells in column viewcell [, , N] is decreased, reflecting the additional packets that are added to column viewcell [, , N]. Clearly, the removal of the navigational hindrance and the attendant decrease in the weights of probability of penetration for column viewcell [, , N] can increase the probability of late packet arrival when navigating in column viewcell [. , N]. Of course, predicted late packet arrival can be prevented (and\/or actual late packet arrival mitigated) by the previously described method of down-modulating level-of-detail under these conditions as described in conjunction with .","The method of assigning lower weights to relative probability of penetration for viewcell boundaries in navigationally hindered regions can be applied, in some embodiments, at runtime in the case of predicted late packet arrival; as shown in the step  in the exemplary flowcharts of  and . The relative restriction of navigation at runtime would, in one example, correspond to the change in cache state illustrated in the figure sequence moving from  back to , which shows that navigation in the preferred path column viewcell [, , N] is enhanced by placing the relative navigational restriction on column viewcell [, , N]. Consequently, the chance of late packet arrival is decreased in column viewcell [, , N] and\/or the allowable maximum velocity in this preferred path region can be increased.",{"@attributes":{"id":"p-1104","num":"1124"},"figref":["FIG. 41","FIG. 31C","FIG. 41"]},"The two dimensional array of indices to an array of DeltaGplussubmesh data structures for a specific viewcell transition (i.e., a specific visibility event) is shown in  as DeltaGplussubmesh_subpackets[M][N] (where M=10 and N=5 in this example). In this 2D array, the first index dimension refers to one of 10 sub-packets including the visibility event packet. The second index dimension references a specific version of each sub-packet, wherein the version is encoded at one of 5 levels-of-detail in this exemplary embodiment. In the sub-packet data storage scheme of , all sub-packets having the same M index represent the same geometric objects. All sub-packets having the same N index represent a version of a visibility event packet at a level-of-detail N.","Method of Using Subpackets at a Plurality of Levels-of-Detail to Prevent Late Packet Arrival","The co-pending PCT patent application number PCT\/US2011\/042309 specifies a set of methods in which visibility event packets are encoded at multiple levels-of-detail and in which low level-of-detail packets are sent during periods of threatened late packet arrival.","In this method, each visibility event packet contains all of the data for the corresponding viewcell transition at a specific level-of-detail. For each viewcell transition, multiple visibility event packets are used, each corresponding to the complete data for the viewcell transition, encoded at a specific level-of-detail.",{"@attributes":{"id":"p-1108","num":"1128"},"figref":"FIG. 42A"},"Turning now to , in decision step , it is determined if the streaming of visibility event data employs sub-packets. If, in decision step , it is determined that the streaming of visibility event packets does not employ sub-packets, then process flow proceeds to step . In step , the adaptive modulation of level-of-detail for visibility event packets employs the methods of , , , and  of the co-pending PCT patent application number PCT\/US2011\/042309 in which the next visibility packet selected for sending is one having a lower level-of-detail.","If, in decision step  it is determined that the streaming of visibility event data employs sub-packets, then process flow proceeds to step .","In one embodiment of the method of prefetch streaming of visibility event packets using sub-packets, each sub-packet is completely streamed to the client before the next sub-packet is streamed and the order of sub-packet streaming is selected to reflect the probability that the corresponding visibility event boundary will be penetrated. In this embodiment, the order of the sub-packets can also be selected to reflect the size of the overall visibility event packet, with sub-packets from larger visibility event packets being weighted to be selected more frequently.","In another embodiment, data from several sub-packets are streamed simultaneously, with the streaming rate for a specific sub-packet selected to be proportional to the probability that the corresponding visibility event boundary will be penetrated. In this embodiment, the data rate for sub-packet streaming is selected to be proportional to the overall size of the corresponding visibility event packet (e.g., the sub-packet is 50% of the visibility event packet).","In decision step , it is determined if late packet arrival is predicted. If in decision step  it is determined that late packet arrival is not predicted, then process flow proceeds to step .","In step  sub-packets are streamed at the currently selected level-of-detail.","If, in decision step , it is determined that late packet arrival is predicted, then process flow proceeds to step .","In step , the next sub-packet transmitted is selected to have a level-of-detail version (e.g. N value in the 2D index array of ) having a lower level-of-detail than the currently selected level-of-detail.","The use of sub-packets allows a more rapid switch to a lower level-of-detail packet which tends to reduce the occurrence of late packet arrival. In addition, since the sub-packets previously transmitted at a higher level-of-detail can be used by the client in this method, the overall time-to-transmit a complete set of sub-packets for a given visibility event packet is reduced. This also reduces the probability of late packet arrival.","In step , the switch to a lower level-of-detail subpacket can be made for the current subpacket being transmitted or for the next subpacket to be transmitted. If the time to finish transmitting the current subpacket at the current level-of-detail is less than the time required to transmit the entirety of the corresponding subpacket at a lower level-of-detail, then the transmission of the current subpacket at the current level-of detail should be continued; and the switch to a lower level-of-detail subpacket made for the next subpacket scheduled to be transmitted. Otherwise, the transmission of the current subpacket should be abandoned and the transmission of the corresponding lower level-of-detail packet started immediately. Another advantage of employing visibility event sub-packets, and\/or otherwise streaming data from multiple visibility event packets at the same time, at a rate proportional to the probability of penetration, is that as the viewpoint moves through the set of viewcells\/navigation cells, the probability of penetration for viewcell boundaries changes. These changes can often make the previously determined prefetch solution less precise. Using smaller sub-packets or other finer packets of smaller granularity allows earlier, less precise prefetch solutions to be adjusted more rapidly based on more current navigational state data. The finer granularity of these adjustments allows the dynamic maintenance of a visibility event cache that more accurately reflects the current navigational parameters. This more precise visibility event cache decreases the change of late packet arrival and tends to reduce the bandwidth required to maintain the cache.","Method of Completing a PVS by Determining a Sequence of Viewcell Boundaries and Corresponding Visibility Event Packets Having a Minimum Total Time to Transmit",{"@attributes":{"id":"p-1119","num":"1139"},"figref":"FIG. 42B"},"The methods of visibility event data prefetch and caching described in the co-pending PCT patent application number PCT\/US2011\/042309 and in the present embodiments use predictive methods which decrease the incidence of late packet arrival, which occurs when a viewpoint enters a viewcell before the visibility event packets required to construct a complete PVS for the viewcell have arrived.","If the predictive methods result in late packet arrival, then the viewpoint enters a viewcell before all of the visibility event data needed to complete the PVS for that viewcell has arrived. This corresponds to a case of actual late packet arrival. In this case, the relative probability of penetration for the current viewcell is set to 1.0, indicating certain penetration. In this case, the current method adapts, in some embodiments, to actual late packet arrival by shifting completely to a demand-fetch mode and dedicating all available visibility event streaming bandwidth to complete the current viewcell PVS. If the PVS of at least one adjacent viewcell (e.g., the last viewcell occupied by the viewpoint) is complete, then this demand streaming corresponds to completing the visibility event packet corresponding to the boundary between the last viewcell occupied and the current viewcell.","However, under conditions of low spatiotemporal visibility coherence it is possible that the PVS for the last viewcell occupied is also incomplete. Under these circumstances, in order to complete the PVS for the current viewcell, the PVS for the last viewcell occupied must also be completed.","In some cases, it may be faster to complete the PVS of an adjacent viewcell other than the last viewcell occupied and then transmit a visibility event packet from this adjacent viewcell to the current viewcell. This can occur because earlier simultaneous (e.g., multiplexed) streaming of visibility event packet or sub-packet data to multiple adjacent viewcells results in a situation in which the state of visibility event packet transmission completeness combined with the state of adjacent and current viewcell PVS completeness can make it faster to use visibility event packets from an adjacent viewcell other than the last viewcell occupied by the viewpoint.","In one technique, processing determines which adjacent viewcell should be used to result in the most rapid completion of the current viewcell PVS. This determination is made by examining the state of completeness of adjacent viewcell's PVSs and the size of the untransmitted portion of the visibility event packets corresponding to their boundaries with the current viewcell.","Under conditions of very low spatiotemporal visibility coherence, it is possible that a number of previously occupied viewcells are left with incomplete PVSs. In this case, the preceding technique is applied recursively, working from the current viewcell backwards to determine the sequence of visibility event packets must be transmitted to form a chain of completed PVSs and visibility event packets having the lowest total time to transmit and which allows the PVS of the current viewcell to be completed the fastest.","Turning now to , in decision step  it is determined if there has been actual late packet arrival defined as an incomplete PVS for the viewcell currently occupied by the viewpoint.","If, in decision step , it is determined that actual late packet arrival has not occurred then process flow proceeds to step . In step , the previously described method of streaming visibility event data for adjacent viewcells at a rate that is proportional to the probability that the viewcell will be penetrated is maintained.","If, in decision step , it is determined that actual late packet arrival has occurred for the currently occupied viewcell, then process flow proceeds to step .","In step  all available transmission bandwidth and processing is dedicated to completing the PVS of the current viewcell. Streaming of visibility event data to viewcells that may be penetrated in the future is suspended.","In step , the size of un-transmitted visibility event packets for adjacent viewcells is determined.","In step , the completeness state of the PVS for adjacent viewcells is determined. As previously described, in situations of low spatiotemporal visibility coherence the PVS of previously occupied viewcells and other adjacent viewcells may also be incomplete.","In step , both the state of PVS completeness and the size of untransmitted visibility event packets for adjacent viewcells is used to estimate the time to transmit a visibility event packet that would complete the PVS of the viewcell currently occupied by the viewpoint.","An illustration of the application of the method is shown in the exemplary . For example, during the movement of a camera\/viewpoint position from VP shown in  to position VP, shown in , the visibility event packets required to construct a complete PVS for the viewcell [, , ] may not have been received by the decoder-client process in their entirety. Consequently, any movement of the camera\/viewpoint into this cell for the current cache state could result in visibility errors caused by late visibility event packet arrival. If the camera\/viewpoint suddenly penetrates viewcell [, , ] during this incomplete cache state then, using the method of , step  the size of the untransmitted portion of the visibility event packets for adjacent viewcells (VE packet corresponding to transition from viewcell [, , ] to viewcell [, , ]; and VE packet corresponding to transition from viewcell [, , ] to viewcell [, , ]) and the completeness of the PVS for these adjacent viewcells (in this example viewcell [, , ] and viewcell [, , ] (in step ) to determine which visibility event packet to use to complete the uncompleted PVS (e.g. viewcell [, , ]). In the example of , the viewpoint motion vector VECTOR, together with the fact that viewcell [, , ] is located in a region of unhindered navigation (and thus, in some embodiments, is assigned a higher weight for probability of penetration) will, in some embodiments, tend to result in a PVS for viewcell [, , ] that is more complete than the PVS for viewcell [, , ]. In this case the step  of  will determine that the visibility event packet corresponding to the transition from viewcell [, , ] to viewcell [, , ] should be used to complete the incomplete PVS for viewcell [, , ] if the untransmitted portion of this VE packet is not too large.","According to some embodiments, the process of  is applied recursively in the case that the PVS of an adjacent viewcell is incomplete.","In one alternate embodiment, the visibility event encoding includes complete PVS packets for some viewcells in the set of encoded viewcells. These complete PVS sets may be streamed if the time to transmit the complete PVS and use it as a starting set to complete the PVS of the currently occupied viewcell using related visibility event packets is faster than using the existing set of partially complete PVSs.",{"@attributes":{"id":"h-0054","num":"0000"},"figref":"FIG. 43"},"The block diagram\/flowchart of  shows additional detail, in another embodiment, of the client-server process of delivering delta visibility information. In some embodiments, these steps are if the client and server are implemented in different physical devices connected by a network connection, or if the client and server processes are implemented on the same physical device.","In , the block diagram separates the client and server functions into different processes, without regard to whether these processes are executed on different physical devices.","The block labeled  contains the client process and the block labeled  contains the server process.","In a first step, the client process obtains and stores the current viewpoint in data location . Data giving the current viewpoint location can be modified by an input device connected to the client process including mouse, joystick, game controller pad or other input device to modify the current viewpoint location. In alternate embodiments, the viewpoint location may be determined by a prescripted camera path.","The client process provides access to data location  to the server process, which copies the data into data location . This copying may occur in a local memory or across a network.","In step , the server process uses the current viewpoint data to predict future locations of the viewpoint. In some embodiments, this prediction employs dead-reckoning and collision constraints, velocity and acceleration limits, and other methods of predicting future viewpoint locations from the current viewpoint location, such as prior-art methods of pathfinding commonly used to determine the navigation of non-player combatants or artificial intelligence agents in computer games or other simulations. (See \u201cTactical Pathfinding Using A*\u201d, Van Der Sterren, William in Game Programming Gems 3, Charles River Media 2002, the entire contents of which are incorporated herein by reference).","In step , the predicted future viewpoint locations are used to determine predicted viewcell transitions, a viewcell transition being a crossing of a viewcell boundary. This step employs both the data from step  and the known arrangement of viewcells for the model. Delta visibility data (also called visibility event information) corresponding to each crossed viewcell boundary, i.e. each viewcell transition, will be read by the server process in later steps.","In step , the minimum time for the current viewpoint to reach the viewcell boundary corresponding to the predicted viewcell transition being processed is determined. This time is a function of the current viewpoint location and velocity, maximum possible acceleration and velocity, as well as local collision and other navigational constraints. This value is stored in the variable PENETRATETIME.","In step , the maximum time to read and transmit the delta visibility information corresponding to the future viewcell transition being processed is determined. This time is determined based on the size of the corresponding delta visibility packet, available transmission bandwidth to client, and possible other factors. This value is stored in the variable TRANSMITTIME.","In subsequent decision step , it is determined if the variable TRANSMITTIME is greater than the variable PENETRATETIME. If, in decision step , it is determined that the variable PENETRATETIME is greater than TRANSMITTIME then process flow proceeds to step .","In step , instructions are sent to the client unit which indicate that the client unit should render the low level-of-detail objects (sent in the subsequent step ) using a filter which makes the low level of geometric, texture, or shading detail less visually apparent or objectionable to the viewer. In one embodiment, a blurring filter is used. In alternate embodiments, the instructions sent in step  cause the client rendering unit to render obscuring objects in front of the low level-of-detail objects sent in step  and or in regions of the viewport occupied by the low level-of-detail objects sent in step . Embodiments are possible in which these obscuring objects include liquids or solids (e.g. water, oil, mud, ice, mist) on a windshield or visor that is positioned between the viewpoint and objects in the modeled environment. Other embodiments are possible in which the client unit  automatically renders reduced level-of-detail objects using filters and\/or obscuring objects without receiving explicit instructions to do so.","In step  a low level-of-detail (LOD) version of the delta visibility data (e.g., deltaG+submesh data) is read from the out-of-core storage and made available to the client process, which uses this data to update visibility map\/PVS data corresponding to the viewcell and stored in data location labeled . In embodiments, this relatively low level-of-detail delta visibility data includes data having a relatively lower level of geometric detail, lower level of texture resolution, or other methods of reduced detail. As an example, a regular model may include a polygon mesh composed of over one million mesh triangles. In some embodiments, the low level-of-detail version of the regular image would include only 10% of the mesh triangles. For existing methods of generating models at multiple levels of detail see Leubke, David; Reddy, Martin et. al, \u201cLevel of Detail for 3D Graphics\u201d Morgan Kauffman 2003, ISBN: 1-55860-838-9, the entire contents of which are incorporated herein by reference.","If, in decision step , it is determined that the variable TRANSMITTIME is not greater than the variable PENETRATETIME then process flow proceeds to step .","In step , a higher level-of-detail version of the delta visibility data (e.g. deltaG+submesh data) is read from the out-of-core storage and made available to the client process, which uses this data to update visibility map\/PVS data corresponding to the viewcell and stored in data location labeled .","The steps  through  insure that relatively low level-of-detail geometric and\/or texture information in delta visibility packets are employed only when the current viewpoint is likely to enter a viewcell before all of the delta visibility information needed to update the PVS for that viewcell has been received by the client process.","The use of lower level-of-detail information in this situation reduces the time required to read and transmit the delta visibility information to the client process; and thereby reduces the chance that the viewpoint will enter a viewcell before all of the delta visibility information required to incrementally construct the PVS for that viewcell has been read\/transmitted.","Moreover, the use of this lower level-of-detail information in this situation is not likely to be detected by the user. This is because the delta visibility information supplied (e.g., deltaG+submesh information) correspond to newly exposed surfaces, which, in this situation have been exposed for a very brief time; and it is well known that the human visual system has low visual acuity for surfaces that have been exposed for brief times.","Thus, the present method is a method of initially using relatively low level-of-detail delta visibility information to update the PVS of viewcells for which the current viewpoint is likely to penetrate before the corresponding higher level-of-detail could be supplied. In embodiments, the relatively higher level-of-detail delta visibility information is sent at a later time as the exposure time of the deltaG+submesh surfaces increases, corresponding to an exposure time for which the time-dependent spatial acuity of the user's visual system would allow the user to appreciate the higher level-of-detail geometry and\/or texture information. In some embodiments, the time to send the higher level-of-detail visibility information is bounded by a human's ability to perceive objects.","Low level-of-detail graphics information is delivered more quickly to the client unit than higher level-of-detail information. By delivering relatively low level-of-detail information during transient periods when late arrival of graphics information to the client is threatened, the present method substantially prevents late arrival of graphics information. Late arrival of graphics information can be considered a perceptually undesirable failure of a visibility-based prefetch system, since the missing graphic elements (or their sudden reappearance later on) is easily perceived by a viewer and is visually objectionable.","By initially delivering low level-of-detail graphics information during these periods and subsequently sending additional graphics information that can be used to increase the level-of-detail of the initial graphics information, the method can prevent late arrival of graphics information and still achieve a perceptually lossless performance degradation if the substitution schedule matches or exceeds the visual acuity vs. exposure time curve for human dynamic visual acuity.","If the substitution schedule approximates the visual acuity vs. exposure time curve for human dynamic visual acuity, then a perceptually graceful performance degradation will be experienced by the user. This performance degradation will be experienced as a discrete substitution event during the substitution of higher level-of-detail graphics information for lower level-of-detail graphics information. As the substitution schedule approaches the visual acuity vs. exposure time curve for human dynamic visual acuity, this substitution event will become more difficult for the user to perceive. In any case, the substitution event is preferable to the late arrival of graphics information (and subsequent sudden appearance) that would be caused by late arrival of graphics information.","Consequently, this method produces a perceptually lossless or perceptually graceful degradation of performance in the case of transiently low spatiotemporal visibility coherence during the visualization. This transiently low spatiotemporal visibility coherence of the visualization may be caused by transiently high viewpoint velocity or locally low visibility coherence in regions of the model where the visibility tends to change at a high rate (e.g. regions near doors moving indoors to outdoors). This method also is useful in dealing with situations in which the available transmission bandwidth is transiently diminished, or server performance is transiently diminished.","In one important situation, the situation of high viewpoint velocity, the exposure time of deltaG+submesh surfaces may not increase to a duration which allows the user to fully resolve the newly exposed surfaces. This is because at high viewpoint velocity, the time during which the newly exposed surfaces remain in the viewport can be brief.",{"@attributes":{"id":"p-1159","num":"1179"},"figref":"FIG. 44"},"As described earlier in this specification, and in additional detail in the PCT patent application number PCT\/US2011\/042309, first-order wedges constructed using the methods of wedge construction and visibility map simplification detailed in  to  of the present application are used, in some embodiments, to generate delta-PVS or visibility event data including the polygons or fragments of polygons that become potentially visible when a viewpoint moves from one viewcell to an adjacent viewcell. This delta-PVS includes deltaG+submesh and deltaG\u2212 data that, when respectively added to and subtracted from a PVS for one viewcell produces a PVS for another viewcell, e.g. for a contiguous viewcell.","The methods described in  to  of the present application, describe an exemplary embodiment of the method of effectively encoding the delta-PVS for many child viewcells contained in a parent viewcell by labeling certain seed triangles and silhouette edges of polygon meshes included in the PVS for the parent viewcell. These labeled seed triangles are used to initiate a runtime breadth-first traversal of certain sub-regions of the PVS of the parent viewcell, which terminates at occlusion boundaries that are also generated at runtime from the labeled silhouette contours ( and ). This traversal can be used to generate delta-PVS corresponding to a transition from a parent to a child viewcell (deltaG\u2212 data), or to generate the PVS of the child viewcell directly from the parent viewcell PVS.","Since the labeled silhouette contour information (or stored occlusion boundary information) has very low storage requirements, this encoding process, in some embodiments, decreases the storage requirements for delta-PVS data.","Moreover, since the labeled silhouette contour (or stored occlusion boundary information) generally requires much less bandwidth to transmit than all of the delta-PVS data for an entire set of children viewcells, this encoding decreases transmission bandwidth requirements for visibility event streaming by transmitting information that can be used to generate the delta-PVS and\/or PVS data for a large number of child viewcells by sending only the delta-PVS data for the corresponding parent viewcell plus the labeled silhouette contour (or stored occlusion boundary) data.","The following discussion assumes, in exemplary embodiments, that parent and parent are two contiguous viewcells which intersect only at a viewcell boundary, and that a child viewcell is contained in viewcell .","The use of this encoding generally results in sending a relatively large deltaG+submesh data packet (e.g. for a parent-parent viewcell transition) as well as a much smaller amount of labeled silhouette contour (or stored occlusion boundary) data which is used to generate, on the client unit, multiple delta-PVS or PVS for a large number of child viewcells (e.g. the children of parent). When the decoder-server process that delivers these packets (using navigation-based prefetch) is operating in the nominal prefetch mode, sending packets encoded in this way is very efficient. This is because in prefetch mode, if the predictability of navigation is relatively low, then a relatively large cache PVS data corresponding to viewcells in the region of the current viewpoint is required to be maintained. This large cache is efficiently maintained with visibility event packets corresponding to viewcell transitions between large parent viewcells (e.g. large parent-large parent delta-PVS data+labeled silhouette contour metadata for parent's children viewcells).","Conversely, if the transmission of visibility event information is operating in a demand mode, for example in the case of predicted or actual late packet arrival (e.g. that results from the actual-user viewpoint arriving at a viewcell for which no corresponding visibility event data has been received by the client) then the large parent-parent viewcell packets (plus labeled silhouette contour or occlusion boundary) can actually contribute to late packet arrival because the large packet has a long transmission time.","In some embodiments, in demand mode, the decoding process of generating small child PVS (or delta-PVS) packets from a parent PVS and labeled silhouette contour information (or stored occlusion boundary information) is actually performed on the server unit. This results in smaller packets (compared to the larger parent-parent packets) that, in demand mode (e.g. condition of predicted late packet arrival) require less transmission time and are therefore more effective in preventing or minimizing late packet arrival.","Turning now to , a server unit is labeled SERVER. The server unit transmits visibility event information (including delta-PVS information). In some embodiments this transmission is driven by navigation-based prefetch based on a client-user's input controlling viewpoint motion. In other embodiments the transmission is driven by navigation-based prefetch driven by a predetermined camera\/viewpoint motion path. In either case, the prefetch may incur predicted or actual late packet arrival, for example resulting from low spatiotemporal visibility coherence of the visualization relative to the available transmission bandwidth between the server and the client.","In decision step  it is determine if late packet arrival is predicted for a particular visibility event packet corresponding to the transition from viewcell parent to a contiguous viewcell parent.","If, in decision step  it is determined that late packet arrival is predicted then process flow proceeds to step .","In step , a visibility event packet smaller than a parent-parent visibility event packet is generated using the decoding process that is, in one embodiment, illustrated in the exemplary flow diagrams of . This decoding can produce a visibility event packet that is much smaller than the parent-parent VE packet, the smaller packet corresponding to, in some embodiments, the PVS of the child viewcell that is predicted to be penetrated by the user viewpoint. In some embodiments, this PVS can be generated from the larger PVS of the parent viewcell by the decoding method illustrated in  through , which culminate with a rapid traversal (e.g. employing the process of ) of connected components of the parent PVS that defines the child PVS. This method exploits the fact that the PVS of the children viewcells of parent viewcell are subsets of the PVS for parent.","In some embodiments the packet generated is the delta-PVS between parent viewcell and the child viewcell of contiguous parent. The latter packet, in some embodiments, is generated from the deltaG+submesh (visibility event packet for parent-parent viewcell transition) by conducting the processes illustrated in  through , including the rapid, hinted traversal process of . This method exploits the fact that the delta-PVS for the viewcell transition between parent and a child viewcell of parent is a subset of the delta-PVS for the viewcell transition between parent and parent viewcells.","In step , this smaller packet is transmitted to the client. The smaller packet requires less transmission time than the larger parent-parent packet and is consequently more effective in preventing or minimizing late packet arrival than the larger packet. While the decoding step  can add to the latency of transmission for the smaller packet, in the demand mode, transmission bandwidth, not server processing, is generally the bottleneck to packet delivery. Moreover, if the packet is transmitted before decoding (as in the prefetch mode of step ), the decoding step, in some embodiments, is performed on the client unit prior to processing. (In some embodiments, this decoding of the larger parent-parent packets can be deferred during threatened late packet arrival; the deferral reducing the time elapsed before rendering can begin, but increasing the frame rendering time, i.e. decreased frame rate, because the entire parent PVS must be rendered instead of the smaller child PVS).","In  a visibility event client unit is labeled CLIENT. In decision step , it is determined if late packet arrival is predicted for the parent-parent viewcell transition. If, in decision step , it is determined that late packet arrival is predicted then process flow proceeds to step .","In step  the child PVS (or, in some embodiments, parent-child delta-PVS) is received by the client unit.","In some embodiments, in step  this small PVS is used directly. In alternate embodiments, the small parent-child delta-PVS is used to generate the child PVS.","If, in decision step  it is determined that late packet is not predicted and that visibility event information including the delta-PVS for the exemplary parent-parent viewcell transition plus labeled silhouette contour information (or occlusion boundary information) has been transmitted to the client unit then process flow proceeds to step , in which this information is received.","In subsequent step , the labeled silhouette contour information (or, in some embodiments, the stored occlusion boundary information) transmitted along with the delta-PVS data for the parent-parent viewcell transition is subsequently used (by, in some embodiments, methods including methods of -) to generate the PVS of child viewcells of viewcell , depending upon a viewpoint's predicted penetration of these child viewcells.",{"@attributes":{"id":"h-0055","num":"0000"},"figref":["FIG. 45A","FIG. 44"]},"In some embodiments, the processes illustrated in  occur on a server unit labeled SERVER in .","In decision step  it is determine if late packet arrival is predicted for a particular visibility event packet corresponding to the transition from viewcell parent to a contiguous viewcell parent.","If, in decision step  it is determined that late packet arrival is predicted then process flow proceeds to step .","In step  the deltaG+submesh data (delta-PVS information) for the exemplary parent-parent viewcell transition is selected for processing. In some embodiments, this selection includes reading the data from a data store including pre-stored visibility event information including both deltaG+submesh data and labeled silhouette contour information (and\/or, in some embodiments, stored occlusion boundary information). This data store is labeled .","In subsequent step , the visibility event information read in step  is used to generate occlusion boundary segments from labeled silhouette contours (in some embodiments using the processes illustrated by the exemplary flow diagrams of  and ). In other embodiments the occlusion boundary information is stored directly as part of the visibility event information for the viewcell transition, and can be used without the processes of  and . In step , the labeled silhouette contour information and\/or stored occlusion boundary data used is specific for a particular viewcell-viewcell transition, in this example the transition between viewcells parent and child.","In subsequent step , information indicating a seed triangle belonging to the visibility event packet corresponding to the specific viewcell-viewcell transition (e.g. in this case belonging to the deltaG+submesh for the parent-child viewcell transition) is selected. This data is part of the stored visibility event information, labeled .","The resulting occlusion boundary information is used, in some embodiments together with the labeled silhouette contour information in a subsequent step,  in which a traversal of the larger set (e.g. in this exemplary case deltaG+submesh for the parent-parent viewcell transition) to generate a smaller deltaG+submesh set(s) which can be transmitted to the client with lower latency. The traversal of step , in some embodiments employs the fast, hinted traversal process illustrated in the exemplary flow diagrams of , , and related figures.","In subsequent step the traversed polygons and, in some embodiments, polygon fragments traversed in the step , are assembled into the smaller deltaG+submesh for the parent-child viewcell transition. This set is labeled TT.","In subsequent step this smaller visibility event packet is transmitted to the client unit. Process flow for the server-decoder process terminates at step . Process flow proceeds from  to  () via process -.","If, in decision step , it is determined that late packet arrival is not predicted then process flow proceeds to step . In step  the visibility event data including the larger deltaG+submesh for the parent-parent viewcell transition are read from the data store  and transmitted TO. In some embodiments the information read and transmitted in step  also includes labeled silhouette contour information (and\/or, in some embodiments stored occlusion boundary information) corresponding to one or more viewcell transition between parent and the child viewcells of parent. Process flow proceeds from  to  () via process -.",{"@attributes":{"id":"h-0056","num":"0000"},"figref":["FIG. 45B","FIG. 44"]},"In some embodiments, the processes illustrated in  occurs on a client unit labeled CLIENT UNIT in .","In decision step  it is determine if late packet arrival is predicted for a particular visibility event packet corresponding to the transition from viewcell parent to a contiguous viewcell parent.","If, in decision step  it is determined that late packet arrival is predicted then process flow proceeds to step .","In step  the decoder-client process receives the smaller visibility event packet TT (the smaller deltaG+submesh for the parent-child viewcell transition) sent in the step , performed by the server.","In step  the PVS for the exemplary child viewcell is generated by adding the received TT data to the previously cached parent PVS data. In some embodiments this addition includes the process of exemplary flowchart , in which the new submesh(deltaG+submesh for the parent-child transition) is attached to the existing submeshes for the parent PVS.","This completes the child PVS labeled CHILDPVS, which is available for rendering by a rendering subprocess of the client unit.","The decoder-client process terminates at step .","If, in decision step  it is determined that late packet arrival is not predicted then process flow proceeds to step .","In step  the decoder-client process receives the larger exemplary parent-parent deltaG+submesh data together with the labeled silhouette contour information (and\/or, in some embodiments stored occlusion boundary information) sent in the step , executed by the decoder-server process. This labeled silhouette contour information or stored occlusion boundary information includes, in some embodiments, significantly occluding silhouette contours (or stored occlusion boundaries) corresponding to one or more viewcell transitions from parent to one or more child viewcells of parent.","In subsequent step , the labeled silhouette contour information (or, in some embodiments, the stored occlusion boundary information) transmitted along with the delta-PVS data for the parent-parent viewcell transition is subsequently used (by, in some embodiments, methods including methods of -) to generate the PVS of child viewcells of viewcell , depending upon a viewpoint's predicted penetration of these child viewcells.","Process flow terminates following step .","Method of employing navigational-based prefetch of visibility event data in which prefetch is controlled by client unit's actual location.","The exemplary flow diagram of  shows a method of navigation-based prefetch of visibility event data in which prefetch is driven by the location of the client unit in a real environment, as determined by GPS (or other positioning modality) or as computed by a 3D map-matching method.","In some embodiments, modeled environments based on actual environments are constructed from data acquired using LIDAR or other ranging modalities, from conventional 3D modeling, or a combination of both acquisition and hand modeling.","Increasingly, 3D data acquired using LIDAR or other ranging technologies is used to generate highly detailed 3D graphical models of complex, densely occluded actual mission environments such as urban theaters. Real-time graphics display technologies, including game engine technologies, can be used to display these models for purposes of analysis, mission planning, mission rehearsal, and to display the evolution of an actual mission in real time.","Unfortunately, these 3D models generally have very large file sizes. Consequently, using existing file download approaches, it can take a very long time to transmit these files from a central command and control server to networked client units, especially in battlefield environments where available transmission bandwidth is frequently limited. The methods of visibility event encoding (-), transmission (-), and decoding (-), in some embodiments, implemented using existing game engine software modules (e.g. ) allows the progressive, incremental delivery of massive 3D datasets to network client units without the long download periods.","Moreover, since these models generally represent complex, densely occluded environments, they can be difficult to render at interactive frame rates using the limited graphics hardware that is often available to forward-deployed assets. The present methods of visibility event encoding, transmission, and decoding progressively and incrementally deliver such massive models as visibility event data in which most of the occluded geometry (for the currently relevant viewcells) is not transmitted and not rendered. As described in conjunction with , and in additional detail in the PCT patent application number PCT\/US2011\/042309, this reduction in graphical overdraw, in some embodiments, allows the rendering of such massive 3D models at interactive frame rates using inexpensive graphics hardware that is available on forward-deployed client units.","In this way, the visibility event codec, which performs, in some embodiments the methods of -, , -, enables the rapid delivery of tactical information in a format that is very useable for purposes of tactical visualization.","Another use of the visibility event encoding, transmission-control, and decoding methods described by the exemplary figures enumerated in the previous paragraph, is to deliver 3D data representing a real environment to mobile client units navigating under the control of a human operator of the vehicle or navigating autonomously, under the control of an autonomous navigation algorithm.","If the modeled environment is based on an actual environment, then the streamed visibility event information includes a virtual, interactive representation of the real environment that is streamed to the client unit, based on the client unit's location and movement in the real environment. A modeled environment can be reconstructed from an actual environment using LIDAR acquisition, multiple-view geometry methods, hand modeling from photographs, and a combination of these and\/or other methods. The actual location of a client in a real environment can be determined using such modalities as GPS, inertial navigation and other positioning methods. In addition, the actual location of a client in a modeled environment can also be determined by solving the simultaneous location and mapping (SLAM) problem which includes a variety of important methods in the field of autonomous navigation. (See Montermerlo, M., Thrun, S., FastSLAM, A Scalable Method for the Simultaneous Localization and Mapping Problem in Robotics, Springer Verlag, 2007. ISBN-10-3-540-46399-2.)","The SLAM problem is an important problem in autonomous or robotic navigation. Robotic vehicles navigating autonomously frequently use data from external sensors (e.g. LIDAR, see below) to construct a representation (e.g. a 3D map) of the surrounding environment. In some cases, e.g. while operating in GPS-denied environments, or anywhere an independent method of determining position is not available, the robotic vehicle's autonomous navigation algorithm must determine the robot's position within the environment using the representation of the environment developed from the sensors. In these cases, the autonomous navigation algorithm is said to be computing a solution to the SLAM problem, because the algorithm simultaneously constructs a representation (e.g. a 3D map) and uses this constructed representation, together with sensor data, to determine the robot's position in the actual environment.","As discussed below, if an autonomous navigation algorithm has access to a pre-stored 3D representation of the actual environment, then the solution to the position problem is accelerated because the algorithm does not have to construct the entire 3D map in real -time. Map-matching is a general class of algorithms for determining position in an actual environment using pre-acquired sensor data which has already been precomputed into a representation of the scanned environment and which is made available to the autonomous navigation algorithm.","In one exemplary embodiment, the client unit is an autonomous (i.e., unmanned) vehicle or aircraft operating in a GPS-denied environment. In this embodiment, the actual position of the client unit is determined by solving the SLAM problem using a 3D map -matching algorithm. Map matching algorithms compare a stored model of the environment (the 3D map or represented model) to data (e.g., LIDAR) acquired in real-time in the vehicle's local environment. (See Olesk, A., Wang, J. Geometric and Error Analysis for 3D Map Matching, 2009. Holiday Inn Surfers Paradise, Old, Australia. Dec. 1-3, 2009, the entire contents of which are incorporated herein by reference.) Map matching algorithms compare the stored data representing the environment to the data acquired in real time in order to determine the vehicle's location in the environment. Map matching algorithms allow a particularly efficient solution to the SLAM problem because the stored map data can be compared to the data acquired in real time. This is intrinsically more computationally efficient than the approach used by many other SLAM algorithms, which involve acquiring and processing all of the data required to completely construct a useful 3D map (i.e. 3D model) of the environment in real time. In some cases, a fast SLAM solution can be used to augment or increase the precision of a GPS solution which has limited precision (e.g. because of factors contributing to \u201cdilution of precision\u201d).","The methods of visibility event encoding (-), transmission (-), and decoding (-), used in conjunction with the method of exemplary flow diagram  in which the \u201cuser's location\u201d in step  is the location of an autonomous vehicle determined using map matching, provide an efficient method of delivering useful map data to the autonomous vehicle. This efficiency results in part because using the streaming prefetch and caching methods specified in the co-pending PCT patent application number PCT\/US2011\/042309 and further specified in the current application, the visibility event packets can be delivered over intermittent and limited networks (\u201cTactical Edge Characterization Framework\u201d Volume 3, Evolution and Application of the Framework. MITRE Technical Report 080310, Sep. 2008) which often occurs in battlefield environments. Additional efficiency in using the data results because the visibility event packets are decoded into PVS data. Like rendering algorithms, map matching algorithms need only use that portion of the map which is potentially visible from the vehicle to compare to the ranging data acquired in real time. Since the PVS data provides the map matching algorithm with this precise subset of the overall modeled environment (which is typically much larger than any single PVS) the map matching algorithm needs to process much less data.","Turning now to , a server unit is labeled  while a client unit is labeled . In the intrinsically cyclic process involving a bidirectional communications link between the client unit and the server unit, for purposes of exposition, processing starts at decision step .","In exemplary embodiments, the client unit may be a manned vehicle (including aircraft). In such embodiments, the visibility event data streamed to the client unit allows display of a high fidelity 3D representation of the represented environment to a human user.","In other exemplary embodiments, the client unit may be an unmanned vehicle for which navigation is controlled completely or partially by autonomous navigation methods, including map matching.","In decision step , it is determined if the position\/velocity of the client unit is acquired by GPS or other modality that does not require a 3D map matching solution. If, in decision step  it is determined that the position and\/or velocity of the client unit is acquired by GPS or other modality that does not require a 3D map matching solution, then process flow proceeds to step .","In step , the actual position and or velocity, determined by GPS or other modality that does not require a 3D map matching solution is transmitted to the server unit.","In step , the server unit receives and uses the information transmitted by the client unit in the step  to drive navigation-based prefetch of visibility event data stored on the server unit in the data store labeled . This navigation-based prefetch employs the methods specified in the co-pending PCT patent application no. PCT\/US2011\/042309 and may further employ the methods of , , , , , , and  of the present specification.","The navigation-based prefetch of step  causes visibility event data or delta-PVS data stored in data store  to be read and sent to the client unit as a visibility event data stream labeled .","In step , the visibility event packets transmitted from the server unit to the client unit are used to maintain one or more PVSs, corresponding to view regions in the vicinity of the client's current location.","If the client unit is a manned vehicle, then this data can be rendered and displayed to persons operating the vehicle. This displayed data can provide a 3D representation of the environment under conditions where optical, thermal, LIDAR or other imaging is restricted.","If, in decision step , it is determined that the position and\/or velocity of the client unit is determined by map matching or other SLAM solution that benefits from a pre-acquired model or map of the 3D environment, then process flow proceeds to .","In step , the PVS data (labeled ), assembled from the visibility event data in step , is used, together with LIDAR, multiple-view geometry or other ranging data, acquired by the client and stored in data store , to determine the position of the client unit using a map matching or other method which compares stored data representing the surrounding environment (data store ) with ranging data acquired by the client unit in real-time or near real-time.","In step , the client position and\/or velocity data determined by the map matching algorithm is sent to the server unit.","Step  employs the position and\/or velocity data determined in step  to a driver for the navigation-based prefetch described in conjunction with step . In one exemplary embodiment of the method of , the stored visibility event data  is developed from data acquired by reconnaissance assets using LIDAR or other ranging data. This data is converted from a point cloud representation to a boundary representation using polygons and the visibility event packets are determined and encoded using the from region visibility precomputation methods specified in the co-pending PCT patent application number PCT\/US2011\/042309.","This exemplary embodiment allows a large number of relatively inexpensive reconnaissance assets to be deployed shortly before an engagement mission which will use a less expendable attack asset. The reconnaissance assets can be remotely operated or may employ non-map matching navigational algorithms which are less computationally efficient and which can limit the velocity of the reconnaissance vehicles.","The data acquired by the reconnaissance assets is transmitted to an encoder which uses the from-region visibility precomputation methods specified in the co-pending PCT patent application number PCT\/US2011\/042309 to generate visibility event data stored in data store .","In some embodiments, the visibility event data store of  is determined using the methods of from-region visibility determination employing conservative linearized umbral event surfaces (CLUES) as described in conjunction with -. In some embodiments the CLUES data is used to generate visibility event data which includes delta-PVS data and additional metadata (labeled significantly occluding silhouette contours) which is used to generate PVS data at runtime on the client unit. The method of including labeled silhouette contour data, as described in the encoding process of  and , and the decoding process of -, decreases the bandwidth requirements for visibility event packet transmission. This improved bandwidth efficiency can be very useful in battlefield environments where high bandwidth connections are often not available.","This visibility event data can then be efficiently streamed to the attack asset, even as the attack asset is en route to the mission area. As previously described, the PVS data that is constructed from the streamed visibility event data (in step ) can accelerate map matching algorithms executed by the attack asset, since only small subsets of the 3D map (the PVS) need to be compared to the real-time acquired data in the region of the attack asset. This acceleration of the autonomous navigation solution can enable the attack asset to accurately navigate complex, densely occluded environments at velocities greater than those possible under the control of a human operator.","Method of accessing visibility event stream from cached data on local units when visibility event stream from primary server is inaccessible.","When the visibility event codec is employed in the manner shown in  in which the user's location specified in step  is an actual location of the client unit in a real environment then the codec realizes a streaming, interactive, 3D virtual map that is updated based on the client's position.","The visibility event data in this case is supplied, in some embodiments, by a primary server that is located at some distance from the client. For example, the connection between this primary server and the client unit employs satellite communications.",{"@attributes":{"id":"p-1231","num":"1251"},"figref":"FIG. 47"},"In decision step , it is determined if the visibility event stream with the primary server is intact. If, in decision step  it is determined that the visibility event stream is not accessible from the primary server, then process flow proceeds to step .","In step , the current client accesses cached visibility event data from other clients. Other client units in the geographic vicinity of the current client are likely to have the visibility event information used by the current client stored in cache. This method of using client units as secondary servers implements a fully-connected network topology that improves the reliability of transmission in the case where communication with the primary server is lost.","If, in decision step , it is determined that the visibility event packets are accessible from the primary server then, process flow proceeds to step , in which the client continues to receive visibility event packets from the primary server.",{"@attributes":{"id":"p-1235","num":"1255"},"figref":"FIG. 48"},{"@attributes":{"id":"p-1236","num":"1256"},"figref":["FIG. 48","FIG. 46"]},"In some embodiments, units labeled - represent reconnaissance assets or vehicles which acquire 3D data about an actual environments. The 3D data acquired by the reconnaissance assets - is acquired, in some embodiments, using LIDAR, multiple-view geometry (e.g. Multiple View Geometry in Computer Vision Second Edition, Hartley, R., Zisserman A. Cambridge University Press 2003. ISBN 0521 54051 8, the entirety of which is incorporated herein by reference) or other methods of obtaining data that can be used to construct a three dimensional representation of an actual environment. The resulting 3D data is stored, in some embodiments as point cloud data. In some embodiments, the point cloud data is further processed into a polygon or polygon mesh representations.","The use of multiple reconnaissance units allows a more rapid acquisition of 3D data which can be assembled to a more complete 3D representation (also called a 3D reconstruction) than is possible using a single reconnaissance unit. The completeness of a 3D representation is dependent upon acquiring 3D data from multiple camera\/sensor locations, which is facilitated by using multiple reconnaissance units. In some embodiments, reconnaissance units include relatively inexpensive, expendable assets include remotely operated or autonomous small aircraft or ground vehicles which may operate in restricted spaces including inside buildings and other densely occluded environments. In the exemplary block diagram\/flowchart of , the actual environment that is the subject of reconnaissance (e.g., for the purposes of conducting an engagement or attack mission) is labeled .","The data acquired by the reconnaissance units - is, in some embodiments, sent to a visibility event encoder process, labeled  in . In some embodiments, the 3D data is streamed from the reconnaissance assets to the visibility event encoding unit is streamed in real-time or near real-time, whereby loss of a reconnaissance vehicle does not result in significant loss of data.","In some embodiments the raw point-cloud data or polygon data received from the reconnaissance assets is first converted to polygon mesh or manifold polygon mesh data. Commodity software for converting point cloud date to polygon mesh data is available, for example VRMesh software application from VirtualGrid company, Bellevue City, WA. The visibility event encoder, in some embodiments, employs the methods of  (as well as, in some embodiments, other visibility event encoding and compression methods described in the U.S. Provisional Application 61\/382,056) to create visibility event packets that are streamed to an attack asset via the visibility event server.","The resulting visibility event data is stored in data store  of . This visibility event data is, in some embodiments, the same as the delta-PVS\/visibility event data stored in data store  OF , and data store  of .","In some embodiments, the visibility event encoder process  a visibility event decoder-server process are implemented using computing resources of a command center asset, labeled , which includes a command and control unit in which data from multiple reconnaissance assets and different types of reconnaissance assets is integrated into a unified 3D reconstruction of reconnoitered actual environment. In some embodiments, the visibility event encoder process  and the visibility event decoder-server process are hosted on different devices.","The visibility event data stored in data store is read by a visibility event decoder-server process, labeled  using navigation-based prefetch which is, in some embodiments, driven by the movement of an attack or engagement asset, labeled . In some embodiments, the actual engagement asset is autonomous and employs GPS, 3D map matching, or other methods to determine position. The engagement asset position data is sent from the engagement asset to the visibility event decoder-server process , as illustrated by the dashed connecting line . If the position or other navigational data is determined using map-matching, then the communication channel  is, in some embodiments, equivalent to the communications channel indicated by the unidirectional connecting line connecting process  (client unit) to process  (server unit) of the . If the position or other navigational data is determined using a method other than map-matching, then the communication channel  is, in some embodiments, equivalent to the communications channel indicated by the unidirectional connecting line connecting process  (client unit) to process  (server unit) of the .","In some embodiments, the position and other navigational data received by the visibility event decoder-server process , is used to drive navigational-based prefetched of visibility event data  from a visibility event decoder-server process labeled  to a visibility event decoder-client process labeled , wherein process  is implemented on the engagement asset . In the exemplary block diagram\/flowchart of , the delivery of this visibility event data is illustrated as communication channel labeled .","As described in conjunction with the description of , for embodiments in which the decoder-client unit position is determined using a 3D map-matching solution to the SLAM problem, the method of supplying a 3D map as visibility event data allows for a particularly efficient solution since the resulting 3D map is delivered in the form of PVS data which eliminates the need to process irrelevant data (i.e. not belonging to a PVS for a corresponding viewcell). The computational efficiency gained by using PVS data in 3D map -matching allows higher velocities during autonomous navigation, which can enable a more effective engagement mission.","The exemplary embodiments illustrated in  and , not only enable higher velocities during autonomous navigation missions, but also can increase the dispatch of such missions by allowing a real-time delivery of 3D map data acquired in the vicinity of the objective target while the engagement asset is en-route. Moreover, in some embodiments, any additional or updated reconnaissance data that is acquired by the reconnaissance assets (-) and transmitted to and processed by the visibility event encoder () during the engagement mission is streamed to the decoder-client unit . This additional data can be used to improve the precision of the transmitted 3D map and consequently enhance the accuracy of the engagement asset's mission.","The visibility event data stored in data store , in some embodiments, is also used for analysis, tactical visualization of the objective target area, and mission planning. Mission planning, in some embodiments, includes robotic path planning. In this phase of a mission, preferred navigational paths for autonomously navigating engagement asset(s) are selected. In some embodiments, the visibility event data stream delivered to the decoder-client unit is determined by the preferred primary or secondary navigational paths decided during this mission planning phase. Path planning, in some embodiments, is dependent upon the amount of 3D data acquired during the reconnaissance phase in specific regions of the target environment.","In some embodiments, for training and entertainment purposes, visibility event data, includes camera\/viewpoint motion data that is used to control a virtual camera. For example, in the exemplary SIM embodiment of , data describing a prescripted camera motion path is sent from a visibility event decoder-server unit to a visibility event decoder-client unit, wherein the data drives the virtual camera during the display of visibility event content stream having restricted interactivity. In an exemplary embodiment of , the camera motion control is replaced, in some embodiments, with vehicular position\/velocity controls instructions which are used to effect the navigation of the engagement asset along a desired path. In such embodiments the unit , functions as an integrated command and control system supporting tactical visualization, analysis, robotic mission planning, and mission execution through a bidirectional visibility event data stream driven by navigation-based prefetch of visibility event packets.",{"@attributes":{"id":"h-0059","num":"0000"},"figref":"FIG. 49A"},"Objects labeled  and  are reconnaissance assets which, in some embodiments, acquire 3D data about an actual environment. In some embodiments, the reconnaissance assets  and  are small, relatively inexpensive aircraft that are remotely operated or navigate using an autonomous navigation solution employing SLAM without map-matching, and therefore operating at a lower velocity than is possible with map matching. This lower velocity makes the targets more vulnerable.","In  objects of an actual environment including buildings  and , as well as a target vehicle  are illustrated. The vehicle  is in this example an enemy vehicle which is the target\/subject of a reconnaissance assets  and  (as well as the target of attack asset , described in ).","In the example of , the reconnaissance assets send 3D data about the actual environment (e.g., obtained using LIDAR, multiple-view geometry, or other method) to a command center unit for encoding as visibility event data using the encoder process , described in conjunction with . In some embodiments the encoder process is located far from the reconnaissance region, for example in a command and control unit. Data is delivered to the visibility event encoder through the communication channels labeled \u201c3D DATA TO ENCODER\u201d in .","Also illustrated in  is a set of viewcells corresponding to viewcells for which visibility event data has been encoded (for example by process  of ).",{"@attributes":{"id":"p-1253","num":"1273"},"figref":"FIG. 49B"},"Objects labeled , , and  are modeled objects, based on objects in an actual environment and modeled by 3D reconstruction using 3D data acquired by reconnaissance assets  and  of , and encoded by the process of  of .","Object  is a depiction of an attack asset. In some embodiments, the attack asset is an autonomous vehicle such as the Northrop Grumman MQ-8 Firescout rotary wing aircraft.","In  attack asset object  is shown receiving a visibility event data stream and using the PVS data decoded from the visibility event stream to compute an efficient SLAM solution using map-matching. This allows high-speed autonomous navigation by the attack asset. The high operating velocity makes the attack asset less vulnerable to an offensive or defensive objective target, for example, the object labeled  in .",{"@attributes":{"id":"p-1257","num":"1277"},"figref":"FIG. 49B"},"Using the efficient visibility event encoding process described in -, 3D data acquired during a reconnaissance mission depicted in  is, in some embodiments, rapidly converted to visibility event data which is streamed with little delay to an attack asset depicted in , even while the attack asset is en-route to the objective target. Efficient visibility event encoding also enables data obtained during ongoing reconnaissance acquisition to be rapidly delivered to the attack asset, providing a frequently updated representation of the tactical situation in the region of the objective target. Thus, the attack asset, from this 3D data, is able to determine is actual position in an environment and any other hazards that are posed to the attack asset, even in a GPS-denied environment.","In some embodiments, the visibility event encoding process (e.g. process  of ) as well as the decoder-server process ( of ) and the decoder-client process ( of ) are all conducted by processing means located on a physical engagement asset ( of ).",{"@attributes":{"id":"p-1260","num":"1280"},"figref":"FIG. 50","b":["5000","5000","5000"]},"In one embodiment, the processor  includes a CPU  which processes data and instructions stored in main memory  and\/or ROM . The CPU  also processes information stored on the disk  or CD-ROM . As an example, the CPU  is an IBM System X from IBM of America employing at least one Xenon processor from Intel of America or an Opteron processor from AMD of America. Thus, instructions corresponding to a process in a mobile device are stored on any one of the disk , CD-ROM , main memory  or ROM .","In one embodiment, the processor  also includes a network interface , such as an Intel Ethernet PRO network interface card from Intel Corporation of America, a display controller , such as a NVIDIA GeForce GTX graphics adaptor from NVIDIA Corporation of America for interfacing with a display , such as a Hewlett Packard HP L2445w LCD monitor. The processor  also includes an I\/O interface  for interfacing with a keyboard  and pointing device , such as a roller ball or mouse. According to some embodiments, the disk controller  interconnects disk , such as a hard disk drive or FLASH memory drive, and CD-ROM  or DVD drive with bus , which is an ISA, EISA, VESA, PCI, or similar for interconnecting all of the components of the server . A description of the general features and functionality of the display , keyboard  and pointing device , as well as the display controller , disk controller , network interface  and I\/O interface  is also omitted for brevity as these features are well known. Of course, other processor and hardware vendors and types are known in the art such as Freescale ColdFire, i.MX and ARM processors from Freescale Corporation of America.","The example processor  of  is a hardware platform of a computing device, such as a PC, and CPU  is an Intel Pentium Processor, or any other desired processor known in the art. The computer-readable instructions stored on any one of the main memory , ROM , disk  or CD-ROM  is provided as a utility application, background daemon, or component of an operating system, or combination thereof, executing in conjunction with CPU  and an operating system such as Microsoft VISTA, UNIX, Solaris, LINUX, Apple MAC-OS and other systems known to those skilled in the art.","Main memory  is a random access memory (RAM), FLASH memory, EEPROM memory, or the like, while ROM  is Read Only Memory, such as PROMs. Further descriptions of the main memory  and the ROM  are omitted for brevity as such memory is well known.","In additional embodiments, the client device is a smartphone, such as an iPhone, that is configured to perform any of the above methods performed by the client device. In further embodiments, the client device is a tablet computer, such as an iPad, that is configured to perform any one of the above methods performed by the client device. As an example, an application may be downloaded to an iPhone\u00ae or iPad\u00ae, where the application permits the iPhone or iPad to communicate with a visibility event server and perform anyone of the above methods performed by the client device.","Obviously, numerous modifications and variations of the present advancements are possible in light of the above teachings. It is therefore to be understood that within the scope of the appended claims, the advancements may be practiced otherwise than as specifically described herein."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0045","num":"0044"},"figref":"FIG. 2","b":["1","1","10","1"]},{"@attributes":{"id":"p-0046","num":"0045"},"figref":["FIG. 3","FIG. 3","FIG. 1"],"b":"110"},{"@attributes":{"id":"p-0047","num":"0046"},"figref":["FIG. 4A","FIG. 4A","FIG. 1"],"b":"116"},{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIG. 4B","b":"1"},{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIG. 4C"},{"@attributes":{"id":"p-0050","num":"0049"},"figref":"FIG. 4D"},{"@attributes":{"id":"p-0051","num":"0050"},"figref":"FIG. 4E"},{"@attributes":{"id":"p-0052","num":"0051"},"figref":"FIG. 5A"},{"@attributes":{"id":"p-0053","num":"0052"},"figref":["FIG. 5B","FIG. 6A"]},{"@attributes":{"id":"p-0054","num":"0053"},"figref":"FIG. 5C"},{"@attributes":{"id":"p-0055","num":"0054"},"figref":"FIG. 6A"},{"@attributes":{"id":"p-0056","num":"0055"},"figref":"FIG. 6B"},{"@attributes":{"id":"p-0057","num":"0056"},"figref":"FIG. 7A"},{"@attributes":{"id":"p-0058","num":"0057"},"figref":["FIG. 7B","FIG. 7B"]},{"@attributes":{"id":"p-0059","num":"0058"},"figref":"FIG. 7C","b":["7","1"]},{"@attributes":{"id":"p-0060","num":"0059"},"figref":"FIG. 7D"},{"@attributes":{"id":"p-0061","num":"0060"},"figref":"FIG. 7E"},{"@attributes":{"id":"p-0062","num":"0061"},"figref":"FIG. 7F"},{"@attributes":{"id":"p-0063","num":"0062"},"figref":["FIG. 7G","FIG. 7C"]},{"@attributes":{"id":"p-0064","num":"0063"},"figref":["FIG. 7H","FIG. 7D"]},{"@attributes":{"id":"p-0065","num":"0064"},"figref":["FIG. 7I","FIG. 7E"]},{"@attributes":{"id":"p-0066","num":"0065"},"figref":["FIG. 7J","FIG. 7I"]},{"@attributes":{"id":"p-0067","num":"0066"},"figref":["FIG. 7K","FIG. 7J"]},{"@attributes":{"id":"p-0068","num":"0067"},"figref":"FIG. 8A"},{"@attributes":{"id":"p-0069","num":"0068"},"figref":"FIG. 8B"},{"@attributes":{"id":"p-0070","num":"0069"},"figref":"FIG. 8C"},{"@attributes":{"id":"p-0071","num":"0070"},"figref":"FIG. 8D"},{"@attributes":{"id":"p-0072","num":"0071"},"figref":["FIG. 8E","FIG. 8A"]},{"@attributes":{"id":"p-0073","num":"0072"},"figref":["FIG. 8F","FIG. 8B"]},{"@attributes":{"id":"p-0074","num":"0073"},"figref":["FIG. 8G","FIG. 8C"]},{"@attributes":{"id":"p-0075","num":"0074"},"figref":"FIG. 8H"},{"@attributes":{"id":"p-0076","num":"0075"},"figref":"FIG. 8I"},{"@attributes":{"id":"p-0077","num":"0076"},"figref":"FIG. 9A"},{"@attributes":{"id":"p-0078","num":"0077"},"figref":["FIG. 9B","FIG. 9A"]},{"@attributes":{"id":"p-0079","num":"0078"},"figref":"FIG. 9C"},{"@attributes":{"id":"p-0080","num":"0079"},"figref":["FIG. 9D","FIG. 9C"]},{"@attributes":{"id":"p-0081","num":"0080"},"figref":"FIG. 10A"},{"@attributes":{"id":"p-0082","num":"0081"},"figref":["FIG. 10B","FIG. 10A"]},{"@attributes":{"id":"p-0083","num":"0082"},"figref":"FIG. 11A"},{"@attributes":{"id":"p-0084","num":"0083"},"figref":["FIG. 11B","FIG. 11A"]},{"@attributes":{"id":"p-0085","num":"0084"},"figref":"FIG. 12A"},{"@attributes":{"id":"p-0086","num":"0085"},"figref":["FIG. 12B","FIG. 12A"]},{"@attributes":{"id":"p-0087","num":"0086"},"figref":["FIG. 12C","FIG. 12B"]},{"@attributes":{"id":"p-0088","num":"0087"},"figref":"FIG. 12D"},{"@attributes":{"id":"p-0089","num":"0088"},"figref":["FIG. 12E","FIG. 12D"]},{"@attributes":{"id":"p-0090","num":"0089"},"figref":"FIG. 13A"},{"@attributes":{"id":"p-0091","num":"0090"},"figref":["FIG. 13B","FIG. 13A"]},{"@attributes":{"id":"p-0092","num":"0091"},"figref":["FIG. 13C","FIG. 13B"]},{"@attributes":{"id":"p-0093","num":"0092"},"figref":["FIG. 13D","FIG. 13B"]},{"@attributes":{"id":"p-0094","num":"0093"},"figref":"FIG. 14A"},{"@attributes":{"id":"p-0095","num":"0094"},"figref":["FIG. 14B","FIG. 14A"]},{"@attributes":{"id":"p-0096","num":"0095"},"figref":["FIG. 15","FIG. 14B"],"sub":["O","E"]},{"@attributes":{"id":"p-0097","num":"0096"},"figref":"FIG. 16A"},{"@attributes":{"id":"p-0098","num":"0097"},"figref":["FIG. 16B","FIG. 16A"]},{"@attributes":{"id":"p-0099","num":"0098"},"figref":"FIG. 17A"},{"@attributes":{"id":"p-0100","num":"0099"},"figref":["FIG. 17B","FIG. 17A"]},{"@attributes":{"id":"p-0101","num":"0100"},"figref":["FIG. 17C","FIG. 17B"]},{"@attributes":{"id":"p-0102","num":"0101"},"figref":["FIG. 17D","FIG. 17C"]},{"@attributes":{"id":"p-0103","num":"0102"},"figref":"FIG. 18A"},{"@attributes":{"id":"p-0104","num":"0103"},"figref":["FIG. 18B","FIG. 18A"]},{"@attributes":{"id":"p-0105","num":"0104"},"figref":"FIG. 19A"},{"@attributes":{"id":"p-0106","num":"0105"},"figref":["FIG. 19B","FIG. 19A"]},{"@attributes":{"id":"p-0107","num":"0106"},"figref":"FIG. 20"},{"@attributes":{"id":"p-0108","num":"0107"},"figref":"FIG. 21"},{"@attributes":{"id":"p-0109","num":"0108"},"figref":"FIG. 22"},{"@attributes":{"id":"p-0110","num":"0109"},"figref":"FIG. 23"},{"@attributes":{"id":"p-0111","num":"0110"},"figref":"FIG. 24A"},{"@attributes":{"id":"p-0112","num":"0111"},"figref":"FIG. 24B"},{"@attributes":{"id":"p-0113","num":"0112"},"figref":"FIG. 25A"},{"@attributes":{"id":"p-0114","num":"0113"},"figref":["FIG. 25B","FIG. 25A"]},{"@attributes":{"id":"p-0115","num":"0114"},"figref":["FIG. 26A","FIG. 25A","FIG. 25B"]},{"@attributes":{"id":"p-0116","num":"0115"},"figref":["FIG. 26B","FIG. 26A"]},{"@attributes":{"id":"p-0117","num":"0116"},"figref":["FIG. 26C","FIG. 26A"]},{"@attributes":{"id":"p-0118","num":"0117"},"figref":["FIG. 26D","FIG. 26C"]},{"@attributes":{"id":"p-0119","num":"0118"},"figref":["FIG. 26E","FIG. 26D"]},{"@attributes":{"id":"p-0120","num":"0119"},"figref":"FIG. 27A"},{"@attributes":{"id":"p-0121","num":"0120"},"figref":"FIG. 27B"},{"@attributes":{"id":"p-0122","num":"0121"},"figref":"FIG. 27C"},{"@attributes":{"id":"p-0123","num":"0122"},"figref":"FIG. 27D"},{"@attributes":{"id":"p-0124","num":"0123"},"figref":"FIG. 28A"},{"@attributes":{"id":"p-0125","num":"0124"},"figref":"FIG. 28B"},{"@attributes":{"id":"p-0126","num":"0125"},"figref":"FIG. 28C"},{"@attributes":{"id":"p-0127","num":"0126"},"figref":"FIG. 29"},{"@attributes":{"id":"p-0128","num":"0127"},"figref":"FIG. 30"},{"@attributes":{"id":"p-0129","num":"0128"},"figref":"FIG. 31"},{"@attributes":{"id":"p-0130","num":"0129"},"figref":"FIG. 32A"},{"@attributes":{"id":"p-0131","num":"0130"},"figref":["FIG. 32B","FIG. 32A"]},{"@attributes":{"id":"p-0132","num":"0131"},"figref":"FIG. 33A"},{"@attributes":{"id":"p-0133","num":"0132"},"figref":["FIG. 33B","FIG. 33A"]},{"@attributes":{"id":"p-0134","num":"0133"},"figref":"FIG. 34"},{"@attributes":{"id":"p-0135","num":"0134"},"figref":"FIG. 35A"},{"@attributes":{"id":"p-0136","num":"0135"},"figref":["FIG. 35B","FIG. 35A"]},{"@attributes":{"id":"p-0137","num":"0136"},"figref":"FIG. 35C","b":"1"},{"@attributes":{"id":"p-0138","num":"0137"},"figref":"FIG. 36A"},{"@attributes":{"id":"p-0139","num":"0138"},"figref":"FIG. 36B"},{"@attributes":{"id":"p-0140","num":"0139"},"figref":"FIG. 37"},{"@attributes":{"id":"p-0141","num":"0140"},"figref":"FIG. 38"},{"@attributes":{"id":"p-0142","num":"0141"},"figref":"FIG. 39A"},{"@attributes":{"id":"p-0143","num":"0142"},"figref":"FIG. 39B"},{"@attributes":{"id":"p-0144","num":"0143"},"figref":"FIG. 39C"},{"@attributes":{"id":"p-0145","num":"0144"},"figref":"FIG. 40A"},{"@attributes":{"id":"p-0146","num":"0145"},"figref":"FIG. 40B"},{"@attributes":{"id":"p-0147","num":"0146"},"figref":["FIG. 40C","FIG. 40A"]},{"@attributes":{"id":"p-0148","num":"0147"},"figref":["FIG. 40D","FIG. 40B"]},{"@attributes":{"id":"p-0149","num":"0148"},"figref":["FIG. 40E","FIG. 40D"]},{"@attributes":{"id":"p-0150","num":"0149"},"figref":["FIG. 40F","FIG. 40E","FIG. 40E"]},{"@attributes":{"id":"p-0151","num":"0150"},"figref":"FIG. 41"},{"@attributes":{"id":"p-0152","num":"0151"},"figref":"FIG. 42A"},{"@attributes":{"id":"p-0153","num":"0152"},"figref":"FIG. 42B"},{"@attributes":{"id":"p-0154","num":"0153"},"figref":"FIG. 43"},{"@attributes":{"id":"p-0155","num":"0154"},"figref":"FIG. 44"},{"@attributes":{"id":"p-0156","num":"0155"},"figref":["FIG. 45A","FIG. 44"]},{"@attributes":{"id":"p-0157","num":"0156"},"figref":["FIG. 45B","FIG. 44"]},{"@attributes":{"id":"p-0158","num":"0157"},"figref":"FIG. 46"},{"@attributes":{"id":"p-0159","num":"0158"},"figref":"FIG. 47"},{"@attributes":{"id":"p-0160","num":"0159"},"figref":"FIG. 48"},{"@attributes":{"id":"p-0161","num":"0160"},"figref":"FIG. 49A"},{"@attributes":{"id":"p-0162","num":"0161"},"figref":"FIG. 49B"},{"@attributes":{"id":"p-0163","num":"0162"},"figref":"FIG. 50"}]},"DETDESC":[{},{}]}
