---
title: Facilitating face detection with user input
abstract: One or more techniques and/or systems are disclosed for improving face detection in an image. A user may select a first eye location while viewing the image (e.g., per red-eye reduction) and a first indication of user input, comprising the location selected by the user, can be received. The first eye location in the image can then be used to determine a face location in the image (and a second user indicated eye location can be used as well). The location of the face can be identified in the image, and the image with the identified face location can be provided to a face detection and/or recognition operation, for example.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09111130&OS=09111130&RS=09111130
owner: Microsoft Technology Licensing, LLC
number: 09111130
owner_city: Redmond
owner_country: US
publication_date: 20110708
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["In a computing environment, users commonly collect and manage digital images. For example, a user may capture an image using a camera, download the image to a computer, and save descriptive information with the image (e.g., metadata). Image management applications allow users to make adjustments to an image, such as by adjusting image characteristics to enhance image quality (e.g., brightness, contrast, and\/or color in the image). A user may also enhance the image by adding features that were not found in the captured subject, and\/or to mitigate undesirable effects or eliminate undesired features. Further, some image management applications may be able to detect features in an image, such as human face detection, which may also be used to retrieve data associated with image features, such as names associated with recognized faces.","This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key factors or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter.","Currently, several image management applications use a face detection system to find faces in images and then utilize a face recognition system to compare detected faces to known examples. As an example, if a detected face is recognized to have a close resemblance to a known face, then a recommendation for descriptive information, such as a name, corresponding to the known face can be provided to the user (e.g., in a UI identifying the detected face). For example, an image management application may scan images stored by a user to detect faces, then run detected faces through facial recognition that compares detected face image data to known face image data (e.g., stored locally or remotely). In this example, tags that comprise information associated with a known face can be provided when a match is identified for the detected face.","Face detection operations often do not detect a face even when present in the image (e.g., false negatives). For example, the characteristics for the undetected face may not meet threshold requirements for face detection for a variety of reasons. For example, the undetected face may be oriented in a manner that does not allow for detection, the face may be obscured by an object or shadow, and\/or the image may be unfocused. Further, when the face is not detected by an automated face detection operation, for example, then a name-suggestion for the face may not be made using face recognition. However, humans can typically detect a human face in an image, even when a facial detection application cannot.","Accordingly, one or more techniques and\/or systems are disclosed where human interaction with an image may provide input that can be used to improve face detection in the image. When a user interacts with the image to enhance or adjust it, they may be providing indications of a location of an undetected face in the image. For example, users often apply a red-eye reduction operation to the image, when the red-eye effect causes glowing pupils (e.g., from a camera flash reflecting off of blood vessels in the back of the eye). In this example, when the user selects or interacts with an eye to apply the red-eye reduction, this interaction can identify a location of an eye in the image. Because eyes are typically associated with faces, a location of the face may be inferred from the user interaction, for example. The face location information may, for example, be used in a subsequent face detection operation to detect a previously undetected face and\/or a facial recognition operation.","In one embodiment of improving face detection in an image, a first indication of user input is received, where the first indication comprises a first eye location in the image. Further, the first location of the eye, comprised in the first indication, may be used to determine a face location in the image. Additionally, the location of the face can be indicated in the image, and the image with the indicated face location can be provided for a face detection operation.","To the accomplishment of the foregoing and related ends, the following description and annexed drawings set forth certain illustrative aspects and implementations. These are indicative of but a few of the various ways in which one or more aspects may be employed. Other aspects, advantages, and novel features of the disclosure will become apparent from the following detailed description when considered in conjunction with the annexed drawings.","The claimed subject matter is now described with reference to the drawings, wherein like reference numerals are generally used to refer to like elements throughout. In the following description, for purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of the claimed subject matter. It may be evident, however, that the claimed subject matter may be practiced without these specific details. In other instances, structures and devices are shown in block diagram form in order to facilitate describing the claimed subject matter.","A method may be devised that provides for facilitating face detection in digital images, for example, which may lead to improved face recognition. Currently, some image management applications provide for tagging (e.g., applying associated data, such as a name, to) a portion of an image, such as a face. As an example, a user of an image management application may review an image, select a portion of the image comprising a face, and enter a name associated with the face (e.g., the person's name). Further, some image management applications can automatically detect a facial region in an image, for example, proving the user with an enhanced experience when deciding whether to tag the people in the image.","Additionally, some image management applications can automatically recognize the face detected by the detection process, for example, and can provide the associated data (e.g., the tag) to the user without the user having to enter information. However, the facial detection operation may not always succeed in detecting a face in the image, for example, which may also prevent the face recognition operation from proceeding as expected by the user. This may lead to a diminished user experience, for example, requiring the user to manually select regions in the image, and manually apply tags.",{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 1","b":["100","100","102"]},"In one embodiment, when the user applies the red-eye reduction (e.g., manually or using an image manipulation application) to the image they identify a location of one or more eyes. In this embodiment, the indication of the user input can comprise the identification of the eye location by the red-eye reduction application. In other embodiments, the eye location may be identified by the user in other ways. For example, the user may wish to change a color of the pupil of the eyes, and\/or may wish to make the eyes shine more. In one embodiment, an application used to manipulate an image can identify when the user interacts with an eye, thereby identifying its location. For example, the user may select an operation in the application that is used merely for eyes in the image (e.g., red-eye reduction, and\/or other enhancements), and subsequently select the location of an eye in the image. Alternatively, other operations and\/or applications that are not specifically related to, limited to, etc. one or more eyes may nevertheless yield eye location, such as via user input, for example.","A  in the exemplary method , using the first indication, a face location is identified in the image. As an example, because an eye is known to be associated with a face, and the eye is known to be located in a particular position relative to an area comprising the face, a face location in the image may be determined from the location of the first eye. In one embodiment, an estimated location of the face in the image may be inferred based on known geometries of a human face, such as the location of eyes relative to the location of the face. In one embodiment, the estimated location of the face may comprise a default area size (e.g., comprising a fixed, predetermined, etc. pixel area) comprising the location of the first eye identified from the first indication.","For example, the first indication may comprise a user selected area (e.g., covering a plurality of pixels) of the image identifying the first eye, or may comprise merely a location selected by the user (e.g., comprising merely a pixel) identifying the location of the first eye in the image. In this example, an estimated location of the face can comprise a default sized area (e.g., merely one pixel, or a square of pixels of a desired size) of the image, where the eye location is situated at a center (or an offset) of the default area. Because an orientation of the face may not be readily inferred from merely one eye location, for example, placing the eye location in the center of the estimated face location area may be able to encompass at least a desired amount of the face in the image. In this way, in this example, regardless of the actual location of the face in the image, it (e.g., or a portion thereof) will likely be comprised in the identified face location.","At  in the exemplary method , the face location is indicated in the image for a face detection operation on the image. For example, the face location identified from the first eye location can be provided to a face detection application. In this example, the face detection application can use the face location indicated in the image as a starting point to attempt to detect the face. While face detection applications have improved they commonly miss detecting a face in an image, for example, where the particular parameters used by the application for detecting a face are not met.","As an example, detection of a face is commonly missed when the orientation of the face is not within a specified alignment (e.g., rotated), the face is partially in profile, a portion of the face is obscured (e.g., by shadow or an object), and\/or if partially out of focus. In one embodiment, a second (e.g., or third, etc.) face detection operation attempt can be performed on the image comprising the indicated face location, for example, thereby providing additional information to the application that may not have been available during a first attempt at face detection. In this way, for example the face detection operation may be more likely to detect the face by using the indicated location of the face. It may be appreciated that one or more different thresholds, parameters and\/or values, etc. may be used for a face detection operation when an indication of face location is provided (as opposed to when an indication of face location is not provided). For example, if a face detection operation was performed on an image and no faces were detected, then running that same operation on merely a portion of the image (e.g., indicated as potentially comprising an face based upon user input, such as red-eye reduction) may produce the same results (e.g., false negative). However, if requirements for detecting a face were altered (e.g., lowered) because user input provides an indication of a location in the image where a face is likely located due to user interaction with the image for red-eye reduction purposes, for example, then a face may more likely be detected (e.g., fewer false negatives). That is, less stringent face detection requirements may be acceptable, for example, in view of preliminary evidence (e.g., from a user) that a face exists at a certain location in the image (e.g., the user input tempers the likelihood of false positives (of detecting a face where no face exists) that may otherwise occur if lesser face detection standards were implemented).","Having indicated the face location in the image for a face detection operation on the image, the exemplary method  ends at .",{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 2","b":["200","202"]},"Further, the area of the image selected by the user can be used to determine a size of the eye, such as using known human face geometry. For example, the user may select an ellipse that comprises the pupil (e.g., or the eye) in the image. In this example, the area of the selected ellipse can be determined (e.g., in pixels, or some measurement units) and the size of the eye may be inferred based on known human (e.g., or non-human) eye size. In this way, the first indication of user input can comprise both the first eye location and the first eye size.","At , the first eye location can be compared with known face regions in the image to determine whether the first eye location is comprised in a previously detected facial region. For example, the image may have previously been run through a face detection operation (e.g., prior to the user selecting the first eye), which may have identified one or more face regions in the image. If the selected first eye is located in a previously detected facial region (YES at ) no further action may be taken, at . For example, the user may be performing image enhancement operations on previously detected facial regions, selecting the eye for red-eye reduction. In this example, because the face has already been detected, no further face detection may be necessary.","If the selected first eye is not located in a previously detected facial region (NO at ), information regarding a second eye may be identified. In one embodiment, a second indication of user input may be received that comprises a second eye location in the image. Further, in one embodiment, the second indication may comprise a second eye size. As described above for the first eye, for example, the user may select the second eye in the image to perform some image enhancement operations. In this example, the eye location, and\/or the eye size may be determined by the user selection of the second eye.","At , if the second eye is selected, and the location (e.g., and size) of the second eye is indicated (YES at ), the location of a face region in the image may be determined using the location and size of the first and second eyes, along with a distance between the first and second eyes, at . In one embodiment, the face location may be determined using the first indication and the second indication if a distance between the first eye location and the second eye location meets a desired distance threshold. For example, in order for appropriate face location identification to occur, the first and second eyes should be from the same face.","In this embodiment, for example, a threshold distance may be used to determine whether the first and second eyes are associated with the same face. For example, based on known human face geometry, a typical eye distance (e.g., or distance range) may be identified that can be used for a threshold to help determine whether the first and second eyes belong to the same face. In one embodiment, eye size may be utilized when determining the desired distance threshold to use for comparison to an indentified distance between the first and second yes.","In this embodiment, a first eye size comprised in the first indication and a second eye size comprised in the second indication, for example, can be used to identify the desired distance threshold based on known human face geometry. That is, for example, because the sizes of the eyes are known, a typical distance between the eyes for eyes of that size may be identified and used as a threshold. Further, as an example, the distance between the first and second eyes can be compared to the identified distance threshold, to determine whether the first and second eyes are comprised on the same face. Also, the size of the respective eyes may indicate whether the eyes are from the same or a different face. For example, if one eye is substantially larger than the other, then that may be indicative of two separate faces: one in a foreground (or closer to an image capturing device) and one in a background (or further away from an image capturing device).","In one aspect, if the first and second eyes are found to be from the same face, for example, a face region location can be determined based on the eye size and eye location. In one embodiment, when the size and location of both eyes are now known, an aspect (e.g., approximate size) of the face can be more accurately approximated based on known geometries of a human (or non-human) face, for example, by comparing the eye size, eye location, and eye distance to the known geometries. Further, the orientation of the face may be identified based on the orientation of both eyes in the face. In this way, for example, an estimated face region location may be identified, which can comprise a group of pixels in the image, for example.","At , if the second eye is not selected (NO at ), the second eye may be detected, at . In one embodiment, a second eye may be detected within a desired distance threshold of the first eye location in the image. In this embodiment, using the first eye size, a desired distance threshold may be determined (as described above), for example, within which the second eye may be found in the image. For example, the size of the first eye may be compared with the known human face geometries to identify a typical range of distance, within which the second eye may be found. In this example, a region in the image surrounding the first eye, equivalent to the distance threshold, may be scanned for the second eye. In this embodiment, for example, the second eye may be detected if an eye is identified within the distance threshold. If the second eye is detected (YES at ), the face region location may be determined based upon the first indication and the detected second eye, at , as described above.","If the second eye is not detected (NO at ), the face region location may be determined using the location and size of the first eye, at . In one aspect, an estimated face region area and location may be identified using information for merely one eye. In this aspect, while the area identified using merely one eye may not comprise the accuracy provided when using both eyes of the face, for example, a general location and approximate face area size may still be determined that is sufficient to improve face detection.","In one embodiment, in this aspect, the size of the first eye may be compared with the known human face geometries to identify a corresponding, approximate face size. In this embodiment, for example, the identified, approximate face size may be used to determine a face region location in the image, where the face region comprises the first eye location. As an example, a typical distance from the first eye to an edge of the face (e.g., determined by the known face geometries) may be used as a radius for the approximate face size. In this way, in this example, even though an orientation of the face may not be known, the approximate face size may incorporate a region in the image that comprises at least a portion of the face, thereby providing the face detection operation with a location from which to work.","At  in the example embodiment , the face location can be provided to a face detection operation, where face detection can be performed for the image. In one embodiment, the face detection operation may be provided with an indication of the face region location, indicating an area in the image that potentially comprises (an image of) a face. For example, when a face detection application identifies a face in an image, a portion of the image (e.g., that comprises the face region) may be \u201chighlighted\u201d (e.g., cropped, outlined, modified) in a user interface (UI). In this embodiment, for example, the indication of the face region location provided to the face detection operation may comprise a \u201chighlighted\u201d area of the image that comprises an image of the identified potential face location. Alternately, merely a region of the image may be identified to the face detection operation, such as a pixel region or plotted location.","At , after the face detection operation is run on the image, comprising the indicated face region location, a face recognition operation may be run on the image. For example, if the face detection operation detected a face in the provided face region location, detected face data may be provided to the face recognition operation. In this example, the face recognition operation may compare the detected face data with data comprising known faces to determine whether the detected face matches a known face. Further, in one embodiment, the known face may be associated with related data (e.g., tags, such as names, locations, descriptive comments), which may be retrieved should a match\/recognition occur.","In one embodiment, an indication of a face region location, identified at , may be provided to a face recognition component, where the face recognition operation may be run, at . For example, a face region location may be identified in the image using the first and second indications, as described above, where the face region location provides information that is sufficient for face recognition. In this way, in one embodiment, the face detection operation may be skipped.","A system may be devised that facilitates face detection, and\/or face recognition in an image. For example, face detection operations (e.g., applications) are not always able to detect a face in an image, due to image characteristics and\/or a subject's orientation, for example. However, a human viewing the image is more likely to detect a face in the image. Using the ability of humans to detect a face in an image, user input indicated by common image interaction may be used to improve face detection operations.",{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 3","b":["300","302","300","304","304","350","350","352","352","352"]},"Further, the user can identify a face in the image , where the face comprises at least one eye. The user may select the eye for an enhancement operation, such as red-eye reduction, and the selection of the eye can comprise the indication of the user input, comprising the first eye location in the image . The face locator component  may be able to use this first indication to determine the location of the face in the image , for example, because the user has selected the location of the first eye while interacting with the image  during the enhancement operation.","A face indication component  is operably coupled with the face locator component . The face indication component  is configured to identify the face location area  in the image , that may be used, for example, for a face detection operation on the image . For example, a first face detection operation may be run on an image, comprising one or more faces, resulting in some, all or none of the faces detected. In this example, if at least one face is not detected, the face location component  may be able to determine the face location area  of the at least one face, using the first indication, and the face indication component  can provide the image to the face detection operation with face location area  indicated in the image. In this way, for example, the face detection operation can perform a second face detection, focusing on the indicated face location area  (e.g., that is otherwise overlooked or remains unremarkable relative to the rest of the image).",{"@attributes":{"id":"p-0043","num":"0042"},"figref":["FIG. 4","FIG. 3","FIG. 3"],"b":["400","414"]},"For example, when an image  has been previously run through a face detection operation, one or more faces may have been detected. In this example, a user may select a first eye in the image, where the first eye may be located in one of the one or more detected faces (e.g., for red-eye reduction). The eye location examination component  can determine if the user selected first eye, comprised in a first indication , for example, is located within one of these previously detected face regions. If the eye location examination component  determines that the user selected first eye is located in a previously detected face region, then, in one embodiment, no further action (e.g., additional face detection) may be performed for the first indication .","An eye detection component  can be configured to detect a second eye in the image  based at least in part upon the first indication . For example, the first indication  can comprise a location of the first eye in the image  (e.g., which is not found in a previously recognized face area), and, in one embodiment, may also comprise a first eye size. In this embodiment, the eye detection component  may use the first eye location and size information to determine whether the second eye may be found within a desired threshold distance of the first eye. For example, the desired threshold distance can be used to identify that an identified second eye is from a same face as the first eye (e.g., not too far away, an appropriate size, etc.).","In one embodiment, the face locator component  can be configured to determine the face location area  based on the first indication  of user input and a second indication  of user input, which comprises a second eye location in the image . For example, instead of detecting the second eye, using the eye detection component , the user may provide input that identifies the location and\/or the size of the second eye in the image . The user input identifying the second eye can be comprised in the second indication , and, in combination with the first indication , may be used to determine the face area location , for example, by comparing first and second eye data with known human face geometry data.","A face detection component  can be configured to detect a face in the image . The face detection component  may utilize the identified face location area  in the image  and\/or an adjusted facial recognition threshold to detect the face. In one embodiment, the image  may have been previously run through the face detection component , for example, where zero or more faces may have been detected.","In this embodiment, if the face indication component  indicates a face location  in the previously run image , the face detection component  may still not detect the face if it uses a same facial recognition threshold as was used for the previous run on the image . In one embodiment, the facial recognition threshold may be adjusted for use by the face detection component , for example, where the adjustment provides for a lowered threshold for face detection. As an example, when face detection is first run on the image , there may be \u201ccut-off\u201d constraints that are used to eliminate identified areas in the image  as not comprising a face. In this embodiment, for example, when re-detecting, one or more of the cut-off constants (thresholds) that are used to detect a face can be reduced so that the face area location  may not be missed a second time. It will be appreciated that lowering one or more thresholds in this manner should not have deleterious effects (e.g., false positives) because the face area location  provides a starting point wherein it is much more likely to detect a face (e.g., because a user has identified an eye in this area).","In the example embodiment , a face recognition component  can be configured to identify data  associated with an image of a face in the identified face location area  in the image . For example, the face recognition component  may compare image data from the face location area  (e.g., or a face detected by the face detection component ) with image data of known faces, where the known faces comprise some associated data, such as a person's name or some other identifying information.","As an example, image data for a plurality of known faces (e.g., previously identified) may be stored in a database, and the face recognition component  can compare the image data from the identified face area location  to the image data in the database. In this example, if a match is found in the database, the face recognition component  may retrieve the associated data  from the database, and provide it to the user. In this way, for example, the user experience may be improved by automatically providing interesting information to the user (e.g., name of someone next to their face in the image), without the user having to locate the face, and enter the associated information manually. It may be appreciated that such a comparison (of image data) is meant to be interpreted broadly. For example, comparing identified face area image data to database image data may comprise a much more sophisticated comparison than merely comparing two numbers to one another, although it may also or alternatively comprise a less sophisticated\/complicated comparison as well.","Still another embodiment involves a computer-readable medium comprising processor-executable instructions configured to implement one or more of the techniques presented herein. An exemplary computer-readable medium that may be devised in these ways is illustrated in , wherein the implementation  comprises a computer-readable medium  (e.g., a CD-R, DVD-R, or a platter of a hard disk drive), on which is encoded computer-readable data . This computer-readable data  in turn comprises a set of computer instructions  configured to operate according to one or more of the principles set forth herein. In one such embodiment , the processor-executable instructions  may be configured to perform a method, such as at least some of the exemplary method  of , for example. In another such embodiment, the processor-executable instructions  may be configured to implement a system, such as at least some of the exemplary system  of , for example. Many such computer-readable media may be devised by those of ordinary skill in the art that are configured to operate in accordance with the techniques presented herein.","Although the subject matter has been described in language specific to structural features and\/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather, the specific features and acts described above are disclosed as example forms of implementing the claims.","As used in this application, the terms \u201ccomponent,\u201d \u201cmodule,\u201d \u201csystem\u201d, \u201cinterface\u201d, and the like are generally intended to refer to a computer-related entity, either hardware, a combination of hardware and software, software, or software in execution. For example, a component may be, but is not limited to being, a process running on a processor, a processor, an object, an executable, a thread of execution, a program, and\/or a computer. By way of illustration, both an application running on a controller and the controller can be a component. One or more components may reside within a process and\/or thread of execution and a component may be localized on one computer and\/or distributed between two or more computers.","Furthermore, the claimed subject matter may be implemented as a method, apparatus, or article of manufacture using standard programming and\/or engineering techniques to produce software, firmware, hardware, or any combination thereof to control a computer to implement the disclosed subject matter. The term \u201carticle of manufacture\u201d as used herein is intended to encompass a computer program accessible from any computer-readable device, carrier, or media. Of course, those skilled in the art will recognize many modifications may be made to this configuration without departing from the scope or spirit of the claimed subject matter.",{"@attributes":{"id":"p-0055","num":"0054"},"figref":["FIG. 6","FIG. 6"]},"Although not required, embodiments are described in the general context of \u201ccomputer readable instructions\u201d being executed by one or more computing devices. Computer readable instructions may be distributed via computer readable media (discussed below). Computer readable instructions may be implemented as program modules, such as functions, objects, Application Programming Interfaces (APIs), data structures, and the like, that perform particular tasks or implement particular abstract data types. Typically, the functionality of the computer readable instructions may be combined or distributed as desired in various environments.",{"@attributes":{"id":"p-0057","num":"0056"},"figref":["FIG. 6","FIG. 6"],"b":["610","612","612","616","618","618","614"]},"In other embodiments, device  may include additional features and\/or functionality. For example, device  may also include additional storage (e.g., removable and\/or non-removable) including, but not limited to, magnetic storage, optical storage, and the like. Such additional storage is illustrated in  by storage . In one embodiment, computer readable instructions to implement one or more embodiments provided herein may be in storage . Storage  may also store other computer readable instructions to implement an operating system, an application program, and the like. Computer readable instructions may be loaded in memory  for execution by processing unit , for example.","The term \u201ccomputer readable media\u201d as used herein includes computer storage media. Computer storage media includes volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information such as computer readable instructions or other data. Memory  and storage  are examples of computer storage media. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, Digital Versatile Disks (DVDs) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by device . Any such computer storage media may be part of device .","Device  may also include communication connection(s)  that allows device  to communicate with other devices. Communication connection(s)  may include, but is not limited to, a modem, a Network Interface Card (NIC), an integrated network interface, a radio frequency transmitter\/receiver, an infrared port, a USB connection, or other interfaces for connecting computing device  to other computing devices. Communication connection(s)  may include a wired connection or a wireless connection. Communication connection(s)  may transmit and\/or receive communication media.","The term \u201ccomputer readable media\u201d may include communication media. Communication media typically embodies computer readable instructions or other data in a \u201cmodulated data signal\u201d such as a carrier wave or other transport mechanism and includes any information delivery media. The term \u201cmodulated data signal\u201d may include a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal.","Device  may include input device(s)  such as keyboard, mouse, pen, voice input device, touch input device, infrared cameras, video input devices, and\/or any other input device. Output device(s)  such as one or more displays, speakers, printers, and\/or any other output device may also be included in device . Input device(s)  and output device(s)  may be connected to device  via a wired connection, wireless connection, or any combination thereof. In one embodiment, an input device or an output device from another computing device may be used as input device(s)  or output device(s)  for computing device .","Components of computing device  may be connected by various interconnects, such as a bus. Such interconnects may include a Peripheral Component Interconnect (PCI), such as PCI Express, a Universal Serial Bus (USB), firewire (IEEE 1394), an optical bus structure, and the like. In another embodiment, components of computing device  may be interconnected by a network. For example, memory  may be comprised of multiple physical memory units located in different physical locations interconnected by a network.","Those skilled in the art will realize that storage devices utilized to store computer readable instructions may be distributed across a network. For example, a computing device  accessible via network  may store computer readable instructions to implement one or more embodiments provided herein. Computing device  may access computing device  and download a part or all of the computer readable instructions for execution. Alternatively, computing device  may download pieces of the computer readable instructions, as needed, or some instructions may be executed at computing device  and some at computing device .","Various operations of embodiments are provided herein. In one embodiment, one or more of the operations described may constitute computer readable instructions stored on one or more computer readable media, which if executed by a computing device, will cause the computing device to perform the operations described. The order in which some or all of the operations are described should not be construed as to imply that these operations are necessarily order dependent. Alternative ordering will be appreciated by one skilled in the art having the benefit of this description. Further, it will be understood that not all operations are necessarily present in each embodiment provided herein.","Moreover, the word \u201cexemplary\u201d is used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as \u201cexemplary\u201d is not necessarily to be construed as advantageous over other aspects or designs. Rather, use of the word exemplary is intended to present concepts in a concrete fashion. As used in this application, the term \u201cor\u201d is intended to mean an inclusive \u201cor\u201d rather than an exclusive \u201cor\u201d. That is, unless specified otherwise, or clear from context, \u201cX employs A or B\u201d is intended to mean any of the natural inclusive permutations. That is, if X employs A; X employs B; or X employs both A and B, then \u201cX employs A or B\u201d is satisfied under any of the foregoing instances. Further, at least one of A and B and\/or the like generally means A or B or both A and B. In addition, the articles \u201ca\u201d and \u201can\u201d as used in this application and the appended claims may generally be construed to mean \u201cone or more\u201d unless specified otherwise or clear from context to be directed to a singular form.","Also, although the disclosure has been shown and described with respect to one or more implementations, equivalent alterations and modifications will occur to others skilled in the art based upon a reading and understanding of this specification and the annexed drawings. The disclosure includes all such modifications and alterations and is limited only by the scope of the following claims. In particular regard to the various functions performed by the above described components (e.g., elements, resources, etc.), the terms used to describe such components are intended to correspond, unless otherwise indicated, to any component which performs the specified function of the described component (e.g., that is functionally equivalent), even though not structurally equivalent to the disclosed structure which performs the function in the herein illustrated exemplary implementations of the disclosure. In addition, while a particular feature of the disclosure may have been disclosed with respect to only one of several implementations, such feature may be combined with one or more other features of the other implementations as may be desired and advantageous for any given or particular application. Furthermore, to the extent that the terms \u201cincludes\u201d, \u201chaving\u201d, \u201chas\u201d, \u201cwith\u201d, or variants thereof are used in either the detailed description or the claims, such terms are intended to be inclusive in a manner similar to the term \u201ccomprising.\u201d"],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 4","b":"400"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 6"}]},"DETDESC":[{},{}]}
