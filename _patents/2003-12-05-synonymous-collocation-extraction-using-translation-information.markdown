---
title: Synonymous collocation extraction using translation information
abstract: A method of automatically extracting synonymous collocations from monolingual corpora and a small bilingual corpus is proposed. The methodology includes generating candidate synonymous collocations and selecting synonymous collocations as a function of translation information, including collocation translations and probabilities. Candidate synonymous collocations with similarity scores that exceed a threshold are extracted as synonymous collocations. The extracted collocations can be used later in language generation by substituting synonymous collocations for applications such as writing assistance programs.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07689412&OS=07689412&RS=07689412
owner: Microsoft Corporation
number: 07689412
owner_city: Redmond
owner_country: US
publication_date: 20031205
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF ILLUSTRATIVE EMBODIMENTS","Collocation Extraction","Candidate Synonymous Collocations","Language Model and Construction","Collocation Translation from Source to Target Language","Translation Probabilities and Translation Model","Feature Vectors and Similarity Calculation"],"p":["The present invention generally relates to natural language processing. More particularly, the present invention relates to natural language processing including synonymous collocations. A collocation refers to a lexically restricted word pair with a certain syntactic relation that can take the form: <head, relation-type, modifier>. For instance, a collocation such as <turn on, OBJ, light> is a collocation with a verb-object syntactic relation. Collocations are useful in helping to capture the meaning of a sentence or text, which can include providing alternative expressions for similar ideas or thoughts.","A synonymous collocation pair refers to a pair of collocations that are similar in meaning, but not identical in wording. For example, <turn on, OBJ, light> and <switch on, OBJ, light> are considered synonymous collocation pairs due to their similar meanings. Generally, synonymous collocations are an extension of synonymous expressions, which include synonymous words, phrases and sentence patterns.","In natural language processing, synonymous collocations are useful in applications such as information retrieval, language generation such as in computer-assisted authoring or writing assistance, and machine translation, to name just a few. For example, the phrase \u201cbuy book\u201d extracted from user's query should also match \u201corder book\u201d indexed in the documents. In language generation, synonymous collocations are useful in providing alternate expressions with similar meanings. In the bilingual context, synonymous collocations can be useful in machine translation or machine-assisted translation by translating a collocation in one language to a synonymous collocation pair in a second language.","Therefore, information relating to synonymous expressions and collocations is considered important in the context of natural language processing. Attempts have been made to extract synonymous words from monolingual corpora that have relied on context words to develop synonyms of a particular word. However, these methods have produced errors because many word pairs are generated that are similar but not synonymous. For example, such methods have generated word pairs such as \u201ccat\u201d and \u201cdog\u201d which are similar but not synonymous.","Other work has addressed extraction of synonymous words and\/or patterns from bilingual corpora. However, these methods are limited to extracting synonymous expressions actually found in bilingual corpora. Although these methods are relatively accurate, the coverage of the extracted expressions has been quite low due to the relative unavailability of bilingual corpora.","Accordingly, there is a need for improving techniques of extracting synonymous collocations particularly with respect to improving coverage without sacrificing accuracy.","A method of generating synonymous collocations that uses monolingual corpora of two languages and a relatively small bilingual corpus. The methodology includes generating candidate synonymous collocations and selecting synonymous collocations as a function of translation information, including collocation translations and probabilities. In some embodiments, the similarity of two collocations is estimated by computing the similarity of their feature vectors using the cosine method. Candidate synonymous collocations with similarity scores that exceed a threshold are extracted as synonymous collocations.","The generated collocations can be used later in language generation. In some embodiments, language generation includes parsing an input sentence into collocations, obtaining stored synonymous collocations, and substituting synonymous collocations into the input sentence to generate another sentence. In other embodiments, an input sentence in a source language can be translated by substituting synonymous collocations in a target language to generate a target language sentence.","Automatic extraction of synonymous collocations is an important technique for natural language processing including information retrieval, writing assistance, machine translation, question\/answering, site search, and the like. Collocations are critical because they catch the meaning of a sentence, which is important for text understanding and knowledge inference. Further, synonymous collocations can be difficult for non-English speakers to master. Aspects of the present invention can help users use appropriate alternative or different expressions to express similar ideas and to avoid repetition. Also, users can often ask the same question with different phrases or collocations (e.g. paraphrases) in question\/answering systems or query systems used for instance in obtaining information such as a site search used in a wide or local area network.","One aspect of the present invention provides for a method of obtaining synonymous collocation information of a source language such as English by using translation information from a target language such as Chinese. Another aspect of the present invention provides a method for processing an input sentence or text to generate another sentence or text in the same language using synonymous collocations. In still another aspect, the present invention provides a method of translating a source language sentence or text by selecting from target language synonymous collocations to generate a target language sentence or text.","In one view, aspects of the present invention are based on the assumption that two collocations are correlated if their translations are similar. Dependency triples or collocations are used to identify alternative expressions, which allow longer phrases to be captured that might be effective synonymous expressions for a shorter inputted phrase. Large monolingual corpora of different languages are used because they are relatively economical and easily obtained. A relatively small bilingual corpus is also used, especially for training purposes. Since the present invention primarily utilizes unsupervised training, human resources needed to develop manually labeled training data are minimized.","Before addressing further aspects of the present invention, it may be helpful to describe generally computing devices that can be used for practicing the invention.  illustrates an example of a suitable computing system environment  on which the invention may be implemented. The computing system environment  is only one example of a suitable computing environment and is not intended to suggest any limitation as to the scope of use or functionality of the invention. Neither should the computing environment  be interpreted as having any dependency or requirement relating to any one or combination of components illustrated in the exemplary operating environment .","The invention is operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of well-known computing systems, environments, and\/or configurations that may be suitable for use with the invention include, but are not limited to, personal computers, server computers, hand-held or laptop devices, multiprocessor systems, microprocessor-based systems, set top boxes, programmable consumer electronics, network PCS, minicomputers, mainframe computers, telephone systems, distributed computing environments that include any of the above systems or devices, and the like.","The invention may be described in the general context of computer-executable instructions, such as program modules, being executed by a computer. Generally, program modules include routines, programs, objects, components, data structures, etc. that perform particular tasks or implement particular abstract data types. Tasks performed by the programs and modules are described below and with the aid of figures. Those skilled in the art can implement the description and\/or figures herein as computer-executable instructions, which can be embodied on any form of computer readable media discussed below.","The invention may also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment, program modules may be located in both local and remote computer storage media including memory storage devices.","With reference to , an exemplary system for implementing the invention includes a general-purpose computing device in the form of computer . Components of computer  may include, but are not limited to, processing unit , system memory , and system bus  that couples various system components including the system memory to processing unit . System bus  may be any of several types of bus structures including a memory bus or memory controller, a peripheral bus, and a local bus using any of a variety of bus architectures. By way of example, and not limitation, such architectures include Industry Standard Architecture (ISA) bus, Micro Channel Architecture (MCA) bus, Enhanced ISA (EISA) bus, Video Electronics Standard Association (VESA) local bus, and Peripheral Component Interconnect (PCI) bus also known as Mezzanine bus.","Computer  typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by computer  and includes both volatile and non-volatile media, removable and non-removable media. By way of example, and not limitation, computer readable media may comprise computer storage media and communication media. Computer storage media includes both volatile and non-volatile, removable and non-removable media implemented in any method or technology for storage of information such as computer readable instructions, data structures, program modules or other data. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices. Communication media typically embodies computer readable instructions, data structures, program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term \u201cmodulated data signal\u201d means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media includes wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, RE, infrared and other wireless media. Combinations of any of the above should also be included within the scope of computer readable media.","System memory  includes computer storage media in the form of volatile and\/or non-volatile memory such as read only memory (ROM)  and random access memory (RAM) . Basic input\/output system  (BIOS), containing the basic routines that help to transfer information between elements within computer , such as during start-up, is typically stored in ROM . RAM  typically contains data and\/or program modules that are immediately accessible to and\/or presently being operated on by processing unit . By way of example, and not limitation,  illustrates operating system , application programs , other program modules , and program data .","The computer  may also include other removable\/non-removable, and volatile\/non-volatile computer storage media. By way of example only,  illustrates hard disk drive  that reads from or writes to non-removable, non-volatile magnetic media, magnetic disk drive  that reads from or writes to removable, non-volatile magnetic disk , and optical disk drive  that reads from or writes to removable, non-volatile optical disk  such as a CD ROM or other optical media. Other removable\/non-removable, volatile\/non-volatile computer storage media that can be used in the exemplary operating environment include, but are not limited to, magnetic tape cassettes, flash memory cards, digital versatile disks, digital video tape, solid state RAM, solid state ROM, and the like. Hard disk drive  is typically connected to system bus  through a non-removable memory interface such as interface , and magnetic disk drive  and optical disk drive  are typically connected to system bus  by a removable memory interface, such as interface .","The drives and their associated computer storage media discussed above and illustrated in , provide storage of computer readable instructions, data structures, program modules and other data for computer . In , for example, hard disk drive  is illustrated as storing operating system , application programs , other program modules , and program data . Note that these components can either be the same as or different from operating system , application programs , other program modules , and program data . Operating system , application programs , other program modules , and program data  are given different numbers here to illustrate that, at a minimum, they are different copies.","A user may enter commands and information into computer  through input devices such as keyboard , microphone , and\/or pointing device , such as a mouse, trackball or touch pad. Other input devices (not shown) may include a joystick, game pad, satellite dish, scanner, or the like. These and other input devices are often connected to processing unit  through user input interface  that is coupled to the system bus, but may be connected by other interface and bus structure, such as a parallel port, game port or a universal serial bus (USB). Monitor  or other type of display device is also connected to system bus  via an interface, such as a video interface . In addition to the monitor, computers may also include other peripheral output devices such as speakers  and printer , which may be connected through output peripheral interface .","Computer  may operate in a networked environment using logical connections to one or more remote computers, such as remote computer . Remote computer  may be a personal computer, a hand-held device, a server, a router, a network PC, a peer device or other common network node, and typically includes many or all of the elements described above relative to computer . The logical connections depicted in  include local area network (LAN)  and wide area network (WAN) , but may also include other networks. Such networking environments are commonplace in offices, enterprise-wide computer networks, intranets and the Internet.","When used in a LAN networking environment, computer  is connected to LAN  through a network interface or adapter . When used in a WAN networking environment, computer  typically includes modem  or other means for establishing communications over WAN , such as the Internet. Modem , which may be internal or external, may be connected to system bus  via the user input interface , or other appropriate mechanism. In a networked environment, program modules depicted relative to computer , or portions thereof, may be stored in a remote memory storage device. By way of example, and not limitation,  illustrates remote application programs  as residing on remote computer . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.",{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 2","b":["200","200","202","204","206","208","210"]},"Memory  is implemented as non-volatile electronic memory such as random access memory (RAM) with a battery back-up module (not shown) such that information stored in memory  is not lost when the general power to mobile device  is shut down. A portion of memory  is preferably allocated as addressable memory for program execution, while another portion of memory  is preferably used for storage, such as to simulate storage on a disk drive.","Memory  includes operating system , application programs  as well as object store . During operation, operating system  is preferably executed by processor  from memory . Operating system , in one preferred embodiment, is a WINDOWS\u00ae CE brand operating system commercially available from Microsoft Corporation. Operating system  is preferably designed for mobile devices, and implements database features that can be utilized by applications  through a set of exposed application programming interfaces and methods. The objects in object store  are maintained by applications  and operating system , at least partially in response to calls to the exposed application programming interfaces and methods.","Communications interface  represents numerous devices and technologies that allow mobile device  to send and receive information. The devices include wired and wireless modems, satellite receivers and broadcast tuners to name a few. Mobile device  can also be directly connected to a computer to exchange data therewith. In such cases, communication interface  can be an infrared transceiver or a serial or parallel communication connection, all of which are capable of transmitting streaming information.","Input\/output components  include a variety of input devices such as a touch-sensitive screen, buttons, rollers, and a microphone as well as a variety of output devices including an audio generator, a vibrating device, and a display. The devices listed above are by way of example and need not all be present on mobile device . In addition, other input\/output devices may be attached to or found with mobile device  within the scope of the present invention.",{"@attributes":{"id":"p-0037","num":"0036"},"figref":["FIG. 3","FIGS. 4 and 5"],"b":"300"},"Referring to , lexical knowledge base construction module  performs step  in method  to augment lexical knowledge base  (shown in ). Lexical knowledge base construction module  augments or provides lexical knowledge base  with synonymous collocation information used later to perform step  (shown in ) to generate a sentence or text using synonymous collocations. Step  is discussed in greater detail below in conjunction with . Briefly, in step , lexical knowledge base construction module  can augment lexical knowledge base  with information such as collocation databases, a language model of collocations, and a translation model of collocations.","Lexical knowledge base construction module  receives source language data illustrated as English language corpus  necessary to augment lexical knowledge base . In one embodiment, the source language data comprises \u201cunprocessed\u201d data, such as data that can be obtained from newspapers, books, publications and journals, web sources and the like. The unprocessed source language data can be received from any of the input devices described above as well as from any of the data storage devices described above. It is important to note that use of English as source language is illustrative only. Lexical knowledge base construction module  can be an application program  executed on computer  or stored and executed on any of the remote computers in the LAN  or the WAN  connections. Likewise, lexical knowledge base  can reside on computer  in any of the local storage devices, such as hard disk drive , or on an optical CD, or remotely in the LAN  or the WAN  memory devices.","Source or English language corpus  is provides as an input to source or English collocation extraction module  having parser . As noted above, a collocation comprises a word pair that has some syntactical relation, such as <verb, OBJ, noun>, also known as a dependency triple or \u201ctriple.\u201d Sentences in English language corpus  are parsed into component dependency triples using suitable parser . Parser output can be a phrase structure parse tree or a logical form represented with dependency triples. For example, the sentence \u201cShe owned this red coat.\u201d can be parsed into the following four triples: <own, SUBJ, she>, <own, OBJ, coat>, <coat, DET, this>, and <coat, ATTR, red>. Generally, these triples are represented in the form of <head w, relation-type r, modifier w> as is well known.","One measure or value used to extract or define collocations, from the parsed triples is called weighted mutual information (WMI) discussed in \u201cA Technical Word- and Term-Translation Aid Using Noisy Parallel Corpora Across Language Groups\u201d by P. Fung and K. McKeown in Machine Translation, Vol. 1-2(special issue), pp. 53-87 and which can be expressed as the following equation:",{"@attributes":{"id":"p-0042","num":"0041"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"WMI","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"w","mn":"1"},{"mi":"w","mn":"2"}],"mo":[",",","],"mi":"r"}}},{"mrow":{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"w","mn":"1"},{"mi":"w","mn":"2"}],"mo":[",",","],"mi":"r"}}},"mo":["\u2062","\u2062","\u2062"],"mi":"log","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mfrac":{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"w","mn":"1"},{"mi":"w","mn":"2"}],"mo":[",",","],"mi":"r"}}},{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"w","mn":"1"},"mo":"\u2758","mi":"r"}}},{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"w","mn":"2"},"mo":"\u2758","mi":"r"}}},{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"r"}}],"mo":["\u2062","\u2062"]}]}}],"mo":"="}},{"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"1."}}]}}}},"br":{},"sub":["1","2","1","2","1","1 ","2","2 "],"b":["401","414","416"]},"Similarly, lexical knowledge base construction module  receives unprocessed target language or Chinese language corpus  necessary to augment lexical knowledge base . Target language data can be provided from any of the input devices described above as well as from any of the data storage devices described above. It is also noted that use of Chinese is illustrative only and that other target languages can be used. In addition, aspects of the present invention are not limited to only one target language. For example, it can be advantageous to use one target language for some types of collocation relation-types and another target language for other relation-types.","Lexical knowledge base construction module  further comprises a target language or Chinese collocation extraction module  having parser . Parser  parses or segments Chinese language corpus  into dependency triples (\u201ctriples\u201d) such as <verb, OBJ, noun>. Chinese collocation extraction module  extracts parsed Chinese triples such as by selecting those triples that have WMI values larger than a given or selected threshold as described above.","The total number and unique source language collocations (e.g. English) extracted by module  are stored in an appropriate database . Similarly, target or Chinese collocations extracted by module  are stored in a database .","In actual experiments, English collocations for three kinds of collocations were extracted from the Wall Street Journal (1987-1992). The extracted English collocations are shown the table below:",{"@attributes":{"id":"p-0047","num":"0046"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"84pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"77pt","align":"center"}}],"thead":{"row":[{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]},{"entry":[{},"Class","Type","Tokens"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"84pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"77pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Verb, OBJ, noun","506,628","7,005,455"]},{"entry":[{},"Noun, ATTR, adj.","333,234","4,747,970"]},{"entry":[{},"Verb, MOD, adv.","40,748","483,911"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]}}]}},"br":{}},{"@attributes":{"id":"p-0048","num":"0047"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"84pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"77pt","align":"center"}}],"thead":{"row":[{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]},{"entry":[{},"Class","Type","Tokens"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"84pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"77pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Verb, OBJ, noun","1,579,783","19,168,229"]},{"entry":[{},"Noun, ATTR, adj.","311,560","5,383,200"]},{"entry":[{},"Verb, MOD, adv.","546,054","9,467,103"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]}}]}},"br":{},"b":"420"},"English collocations extracted at English collocation extraction module  are input or received by candidate synonymous collocation generation module , which generates candidate synonymous collocations or \u201ccandidates\u201d from the extracted English collocations. Candidates are generated based on the following assumption: For a given collocation in the form: <head, relation-type, modifier>, a synonymous collocation or expression usually takes the same form, i.e. <head, relation-type, modifier>. Sometimes, however, synonymous expressions can comprise a single word or sentence pattern.","Candidate synonymous collocation generation module  expands a given English collocation by generating one or more synonyms for each of the \u201chead\u201d and\/or \u201cmodifier\u201d using any known means of generating word synonyms, such as an English language thesaurus. In one embodiment, candidate synonymous collocation generation module  accesses thesaurus  such as WordNet 1.6, which was developed at Princeton University of Princeton, N.J. and is available publicly, to generate head and modifier synonyms. In WordNet, for example, one synonym set or \u201csynset\u201d comprises several synonyms representing a single sense or denotation. Polysemous words such as \u201cturn on\u201d can occur in more than one synset each having a different sense. Synonyms of a given word are generated or obtained from all the synsets including the given word. For illustration, the word \u201cturn on\u201d is a polysemous word and is included in several synsets. For the sense \u201ccause to operate by flipping a switch\u201d, \u201cswitch on\u201d is one of its synonyms. For the sense \u201cbe contingent on\u201d, \u201cdepend on\u201d is one of its synonyms. Both \u201cdepend on\u201d and \u201cswitch on\u201d are generated as synonyms of \u201cturn on\u201d. However, the generated candidate set contains some errors because, for example, \u201cdepend on\u201d is generated as a synonym of \u201cturn on\u201d and \u201cillumination\u201d is generated as a synonym of \u201clight\u201d. However, the triple <depend on, OBJ, illumination> is not a synonymous collocation of the triple <switch on, OBJ, light>.","Formally, suppose Cindicates the synonym set of a word w and U denotes the English collocation set extracted in English collocation extraction module . The following table represents a method or algorithm that can be used to generate candidate synonymous collocations in module .",{"@attributes":{"id":"p-0052","num":"0051"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"203pt","align":"left"}}],"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["(1)","For each collocation Col=<head w, relation-type R,"]},{"entry":[{},"modifier w> \u2208 U, do the following steps."]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"175pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"a.","Expand the Head and Modifier using thesaurus 412"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"168pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"to obtain synonym sets Cand C."]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"175pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"b.","Generate a candidate set of its synonymous"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"168pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"collocations:"]},{"entry":[{},"\u2002S= {Col} = {< w, R, w>}|w\u2208 {Head} \u222a C&"]},{"entry":[{},"\u2002w\u2208 {Modifier} \u222a C& < w, R, w>\u2208 U &"]},{"entry":[{},"\u2002< w, R, w >\u2260 Col"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["(2)","Generate the candidate set of synonymous collocation"]},{"entry":[{},"pairs:"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"147pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"SC = {(Col,Col)|Col}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}}},"Referring back to Chinese collocation extraction module , extracted Chinese or target language collocations stored in Chinese collocation database  are input or received by Chinese language model construction module . Chinese language model construction module  constructs a language model of probability information for Chinese collocations in Chinese collocation database . In some embodiments, an interpolation method can smooth the language model to mitigate data sparseness problems.","Generally, the probability of a given Chinese collocation occurring in Chinese language corpus  is approximated by the following:",{"@attributes":{"id":"p-0055","num":"0054"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["c","col"]}}},"mo":"=","mfrac":{"mrow":{"mi":"count","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["c","col"]}}},"mi":"N"}}},{"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"2."}}]}}}},"br":[{},{},{}],"sub":["col","col","1","c","2","1 ","2 ","c","col","1","c","2","c","c"],"b":"419","in-line-formulae":[{},{}],"i":["p","c","p","c","|r","p","c","|r","p","r"]},{"@attributes":{"id":"p-0056","num":"0055"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"mrow":{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"c","mn":"1"},{"mi":["r","c"]}],"mo":"\u2758"}}},"mo":"=","mfrac":{"mrow":[{"mrow":{"mi":"count","mo":"(","mrow":{"msub":[{"mi":"c","mn":"1"},{"mi":["r","c"]}],"mo":[",",","]}},"mo":"\u2062","mi":"*)"},{"mi":"count","mo":"\u2062","mrow":{"mi":["(*","*)"],"mo":["\u2062","\u2062","\u2062"],"mrow":{"mo":[",","\u2062",","],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["r","c"]}},"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}]}},"mo":","}},{"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"4."}}]},{"mtd":[{"mrow":{"mrow":{"mrow":{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"c","mn":"2"},{"mi":["r","c"]}],"mo":"\u2758"}}},"mo":"=","mfrac":{"mrow":[{"mi":"count","mo":"\u2062","mrow":{"mi":"(*","mo":"\u2062","mrow":{"mrow":{"mo":[",","\u2062",",","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msub":[{"mi":["r","c"]},{"mi":"c","mn":"2"}]},"mo":")"}}},{"mi":"count","mo":"\u2062","mrow":{"mi":["(*","*)"],"mo":["\u2062","\u2062","\u2062"],"mrow":{"mo":[",","\u2062",","],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["r","c"]}},"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}]}},"mo":[",","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mi":"and"}},{"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"5."}}]},{"mtd":[{"mrow":{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["r","c"]}}},{"mfrac":{"mrow":{"mi":"count","mo":"\u2062","mrow":{"mi":["(*","*)"],"mo":["\u2062","\u2062","\u2062"],"mrow":{"mo":[",","\u2062",","],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["r","c"]}},"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},"mi":"N"},"mo":"."}],"mo":"="}},{"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"6."}}]}]}}}},"Further, count(c,r,*) is the frequency or count of collocations with cas the head and ras the relation type; count(*,r,c) is the frequency or count of collocations with ras the relation type and cas the modifier; and count(*,r,*) is the frequency or count of collocations having ras the relation type. The symbol * denotes any word that forms part of a particular collocation. In other embodiments, the language model can be smoothed by interpolating in order to mitigate data sparseness as follows:",{"@attributes":{"id":"p-0058","num":"0057"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["c","col"]}}},{"mrow":[{"mi":"\u03bb","mo":"\u2062","mfrac":{"mrow":{"mi":"count","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["c","col"]}}},"mi":"N"}},{"mrow":[{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","mi":"\u03bb"}},{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"c","mn":"1"},{"mi":["r","c"]}],"mo":"\u2758"}}},{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"c","mn":"2"},{"mi":["r","c"]}],"mo":"\u2758"}}},{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["r","c"]}}}],"mo":["\u2062","\u2062","\u2062"]}],"mo":"+"}],"mo":"="}},{"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"7."}}]}}}},"br":{}},"Thus, Chinese language model construction module  generates, estimates or calculates probabilities p(c) for each Chinese collocation using the above equations and the set Chinese collocations such as in database  to build target language model . Language model  is used later to generate translation probabilities in module  described in further detail below.","Referring back to module , collocation translation module  receives candidate synonymous collocations or candidates  from module . Candidates  are to be translated from English to one or more languages such as Chinese to form Chinese collocation translation set . Candidate synonymous collocations each in the form of <head, relation-type, modifier> are translated by translating each corresponding \u201chead\u201d and \u201cmodifier\u201d using a bilingual English-Chinese lexicon or dictionary  accessed by collocation translation module .","In other words, candidates or English language collocations in the form e=<e,r,e> are translated into target or Chinese language collocations in the form c=<c,r,c> using English-Chinese dictionary . If the Chinese translation sets of eand eare represented as CSand CS, respectively, the Chinese collocation translation set  can be represented as:\n\n\u2003\u2003Eq. 8.\n\nwhere R denotes a relation set of similar relation-types. Typically, eand eeach have multiple translations listed in English-Chinese dictionary .\n","Next, it is necessary to calculate translation probability information p(c|e) indicated at translation probability module . In some embodiments, bilingual corpus  is used to calculate translation probabilities described in greater detail below. Given an English collocation e=<e,r,e> and a Chinese collocation c=<c,r,c>, the probability that eis translated into cis calculated using Baye's Theorem as follows:",{"@attributes":{"id":"p-0063","num":"0062"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["c","col"]},{"mi":["e","col"]}],"mo":"\u2758"}}},"mo":"=","mfrac":{"mrow":[{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["e","col"]},{"mi":["c","col"]}],"mo":"\u2758"}}},{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["c","col"]}}}],"mo":"\u2062"},{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["e","col"]}}}]}}},{"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"9."}}]}}}},"br":[{},{}],"sub":["col","col","col","col","col","col","col","col","col ","max ","max","col","col","col"],"b":["436","422","409","406","428"],"in-line-formulae":[{},{}],"i":["c","p","e","|c","p","c"]},"However, if the equation for p(e|c) were used directly, there can be accuracy problems due to data sparseness. Therefore, the equation for p(e|c) can be simplified using the following assumptions.","Assumption 1: For a Chinese collocation cand r, it is assumed that eand eare conditionally independent. Therefore, the translation model can be rewritten or approximated as follows:\n\n()=()=()()()\u2003\u2003Eq. 11.\n","Assumption 2: Given a Chinese collocation <c,r,c>, it is assumed that the translation probability p(e|c) only depends on eand c(i=1,2), and p(r|c) only depends or rand r. Equation 11 can them be rewritten or approximated as:\n\n()=()()()=()()()\u2003\u2003Eq. 12.\n\nEquation 12 is equivalent to the word translation model if the relation-type is considered as another element such as a word.\n","Assumption 3: Assume that one type of English collocation can only be translated into the same type of Chinese collocation then p(r|r)=1 and Equation 12 simplifies to:\n\n()=()()\u2003\u2003Eq. 13.\n","In other words, the collocation translation probability is approximated as the product of the individual translation probabilities of component words. The probabilities p(e|c) and p(e|c) can be calculated using a word translation model constructed with unparallel or parallel bilingual corpus.","In some embodiments, translation model construction module  constructs translation model  using bilingual corpus  and target-source language or Chinese-English lexicon or dictionary  to align the bilingual corpus, such as described in \u201cFinding Target Language Correspondence for Lexicalized EBMT System,\u201d by Wang et al., In Proc. Of the Sixth Natural Language Processing Pacific Rim Symposium. However, in the present invention other known methods of calculating or estimating word translation probabilities can be used, such as described in \u201cEstimating Word Translation Probabilities from Unrelated Monolingual Corpora using the EM Algorithm,\u201d by P. Koehn and K. Knight, National Conference on Artificial Intelligence (AAAI 2000) and \u201cThe mathematics of statistical machine translation: parameter estimation\u201d by Brown et al., Computational Linguistics, 19(2), pp. 263-311 which are herein incorporated by reference in their entirety.","Further, the language model and the translation model can be combined to obtain the collocation translation model in equation 9 as follows:",{"@attributes":{"id":"p-0071","num":"0070"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["c","col"]},{"mi":["e","col"]}],"mo":"\u2758"}}},{"mfrac":{"mrow":[{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"e","mn":"1"},{"mi":"c","mn":"1"}],"mo":"\u2758"}}},{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"e","mn":"2"},{"mi":"c","mn":"2"}],"mo":"\u2758"}}}],"mo":"\u2062"},{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["e","col"]}}}]},"mo":"*","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"\u03bb","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mfrac":{"mrow":{"mi":"count","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["c","col"]}}},"mi":"N"}},{"mrow":[{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","mi":"\u03bb"}},{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"c","mn":"1"},{"mi":["r","c"]}],"mo":"\u2758"}}},{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"c","mn":"2"},{"mi":["r","c"]}],"mo":"\u2758"}}},{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["r","c"]}}}],"mo":["\u2062","\u2062","\u2062"]}],"mo":"+"}}}],"mo":"="}},{"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"13.1"}}]}}}},"br":{}},"In some embodiments, in order to mitigate the problem with data sparseness, simple smoothing is conducted by adding 0.5 to the counts of each word translation pair as follows:",{"@attributes":{"id":"p-0073","num":"0072"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["e","c"],"mo":"\u2758"}}},"mo":"=","mfrac":{"mrow":{"mrow":{"mi":"count","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["e","c"],"mo":","}}},"mo":"+","mn":"0.5"},"mi":"N"}}},{"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"14."}}]}}}}},"Chinese collocation translation sets  and corresponding values for p(c|e) generated in module  can be used to construct feature vectors  for each English collocation among pairs of candidate synonymous collocations generated in module . Feature vectors can be represented as follows:\n\n=<(), (), . . . , ()\u2003\u2003Eq. 15.\n\nwhere i=1, 2 for each pair of candidate synonymous collocations and m is the number of collocations in Chinese collocation translation set  for a given English collocation. In some embodiments, however, m can be a selected number to limit the number of features in each feature vector while ensuring adequate accuracy.\n","Feature vectors associated with individual English collocations are received by synonymous collocation pair selection module . Synonymous collocation pair selection module  comprises similarity calculation module  that calculates similarity between collocations e,eusing their feature vectors. The assumption behind this method is that two collocations are similar if their context is similar. In one embodiment, module  calculates sim(e,e) using a method called the cosine method. The similarity of e,eusing the cosine method is given as follows:",{"@attributes":{"id":"p-0076","num":"0075"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"mi":"sim","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["e","col"],"mn":"1"},{"mi":["e","col"],"mn":"2"}],"mo":","}}},{"mi":"cos","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["Fe","col"],"mn":"1"},{"mi":["Fe","col"],"mn":"2"}],"mo":","}}}],"mo":["=","\u2062"],"mi":{}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mfrac":{"mrow":{"munder":{"mo":"\u2211","mrow":{"msubsup":[{"mi":["c","col"],"mrow":{"mn":"1","mo":"\u2062","mi":"i"}},{"mi":["c","col"],"mrow":{"mn":"2","mo":"\u2062","mi":"i"}}],"mo":"="}},"mo":"\u2062","mrow":{"msubsup":[{"mi":["p","col"],"mrow":{"mn":"1","mo":"\u2062","mi":"i"}},{"mi":["p","col"],"mrow":{"mn":"2","mo":"\u2062","mi":"j"}}],"mo":"*"}},"msqrt":{"mrow":{"munder":{"mo":"\u2211","mi":"i"},"mo":"\u2062","mrow":{"msup":{"mrow":{"mo":["(",")"],"msubsup":{"mi":["p","col"],"mrow":{"mn":"1","mo":"\u2062","mi":"i"}}},"mn":"2"},"mo":"*","msqrt":{"mrow":{"munder":{"mo":"\u2211","mi":"j"},"mo":"\u2062","msup":{"mrow":{"mo":["(",")"],"msubsup":{"mi":["p","col"],"mrow":{"mn":"2","mo":"\u2062","mi":"j"}}},"mn":"2"}}}}}}}}}}]}},{"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"16"}}]}}}},"br":{}},"Similarity values calculated in module  are compared to a threshold value at threshold decision module . Collocation pairs that exceed a threshold value are selected at module  as synonymous collocations. It is noted, however, that the threshold value for different types of collocations can be different. For example, synonymous collocations in the form <verb, OBJ, noun> potentially can have a different threshold value than synonymous collocations in the form <noun, ATTR, adjective>. Synonymous collocation pair selection module generates synonymous collocations  which can be stored as a database to augment lexical knowledge base  used later in the sentence generation phase as indicated on .","Referring to , sentence generation module  performs step  in method  to generate a sentence or text using synonymous collocations received from lexical knowledge base  illustrated on . Sentence generation module  can be an application program  executed on computer  or stored and executed on any of the remote computers in the LAN  or the WAN  connections.","Sentence generation module  receives input sentence, or portion thereof, indicated and herein referred to as \u201cinput sentence \u201d from any of the input devices or storage devices described above. Input sentence  can be a sentence or text that can be selectively modified using synonymous collocations. For instance, a user could input a source or English language sentence . In one embodiment, sentence generation module  can automatically modify input sentence  using synonymous collocations. In other embodiments, sentence generation module  can provide as an output one or more synonymous collocations that can be selected to modify input sentence  for various natural language processing applications as described above.","Sentence generation module  comprises collocation recognition module  which receives input sentence . Collocation recognition module  comprises triple parser  which can be the same or similar to parser  illustrated on . Triple parser  parses received input sentence  into dependency triples. Collocation recognition module  recognizes or selects which of the parsed triples are collocations in the same or similar manner as English collocation extraction module .","Parsed sentence  generated by collocation recognition module  is received by substitution module . Substitution module  substitutes synonymous collocations in place of collocations recognized at module . In some embodiments, the substitutions can be automatic. In other embodiments, the substitutions are selectable.","In still other embodiments, sentence generation module  can be a sentence translation module. In these embodiments, input sentence  is a sentence, which will be translated into another language using the English language collocations in lexical knowledge base . In these embodiments, sentence generation module  receives input sentence  in a language such as Chinese. Collocation recognition module  recognizes Chinese collocations in input sentence  using parser . Parser  can be the same or similar as parser  illustrated in . Collocation recognition module  generates parsed sentence  which is received by substitution module . Substitution module  substitutes English language collocations in lexical knowledge base  to generate output sentence .","A grammar module can be included in sentence generation module  to ensure that output sentence  is grammatically correct after receiving each of the substituted synonymous collocations from substitution module .",{"@attributes":{"id":"p-0084","num":"0083"},"figref":["FIG. 6","FIG. 6"],"b":["600","404","602","604","602","604"]},"Step  and step  together process unprocessed target or Chinese language corpus to extract or generate Chinese collocations. At step , unprocessed Chinese language corpus is obtained or received from any of the input or storage devices described above. At step , Chinese language corpus is parsed into dependency triples. Dependency triples that are recognized as collocations are extracted or generated as described above. The Chinese language collocations extracted at step  can be stored in databases for further processing.","At step , candidate synonymous collocations are identified or generated using, for example, a source language thesaurus as is described in greater detail above. Generally, an extracted English language collocation in the form <head, relation-type, modifier> is expanded with synonyms of the head and the modifier to generate candidate synonymous collocations. A thesaurus can be used to provide synonyms for the expansions.","At step , a language model of the extracted target or Chinese language collocations is constructed. The language model provides probability information of the extracted Chinese language collocations and is used later in estimating translation probabilities for Chinese collocations in translation sets generated at step  below.","At step , each candidate synonymous collocations generated at step  is translated into a Chinese collocation translation set. A source-target language dictionary, such as an English-Chinese dictionary is used to translate each of the head and the modifier to generate the Chinese language translation sets.","At step , a word translation model is constructed to provide translation information of component words used later in estimating translation probabilities for Chinese collocations in translation sets generated at step .","At step , translation probabilities, p(c|e) for Chinese collocations in the Chinese collocation translation sets are calculated using the language model constructed at step  and the translation model constructed at step .","At step , feature vectors Fe,Feare constructed for candidate synonymous collocations identified in step . The feature vectors are in the form\n\n=<(), (), . . . , ()>\u2003\u2003Eq. 17\n\nwhere i equals 1 or 2 for a candidate English collocation pair and m is the number of Chinese collocations in a Chinese collocation translation set corresponding with a particular candidate English collocation. Generally, the Chinese collocations can be ranked from most to least probable.\n","At step , similarity information for candidate English language collocations is calculated or generated using the feature vectors. In some embodiments, the cosine method is used to calculate similarities. However, other known methods of calculating similarity can be used as described above.","At step , English language collocations having a similarity value exceeding a selected threshold are selected as synonymous collocations. In some embodiments, the selected threshold can differ for collocations having different relation-types.","At step , a lexical knowledge base is augmented with the generated or selected synonymous collocations that can be used later in desired applications such as language generation.",{"@attributes":{"id":"p-0095","num":"0094"},"figref":"FIG. 7","b":["700","702","704","706"]},"At step , synonymous collocations are substituted for collocations in the input sentence. The substituting can occur automatically or be selectable. At step , an output sentence is generated having synonymous collocations.","Although the present invention has been described with reference to particular embodiments, workers skilled in the art will recognize that changes may be made in form and detail without departing from the spirit and scope of the invention.","Although the present invention has been described with reference to particular embodiments, workers skilled in the art will recognize that changes may be made in form and detail without departing from the spirit and scope of the invention."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 7"}]},"DETDESC":[{},{}]}
