---
title: Method and apparatus for transducer-based text normalization and inverse text normalization
abstract: A method and apparatus are provided that perform text normalization and inverse text normalization using a single grammar. During text normalization, a finite state transducer identifies a second string of symbols from a first string of symbols it receives. During inverse text normalization, the context free transducer identifies the first string of symbols after receiving the second string of symbols.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07630892&OS=07630892&RS=07630892
owner: Microsoft Corporation
number: 07630892
owner_city: Redmond
owner_country: US
publication_date: 20040910
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF ILLUSTRATIVE EMBODIMENTS"],"p":["The present invention relates to text processing. In particular, the present invention relates to transforming between different forms of text.","In many speech recognition systems, the speech recognition is limited to word sequences defined in a context free grammar. Authoring such grammars can be complex because the author must take into consideration all the different ways that written text can be spoken. For example, the written numbers \u201c123\u201d can be pronounced \u201cone two three\u201d, \u201cone twenty-three\u201d, or \u201cone hundred twenty-three\u201d.","In addition, speech recognizers are designed to provide spoken forms of the words as output. Before displaying these spoken words, it is common to perform an inverse text normalization to convert the spoken form of the word into a written or display form. For example, the words \u201cone two three\u201d would be converted into \u201c123\u201d.","In the past, either hard-coded rules or a context free grammar has been used to perform the inverse text normalization. The hard-coded rules are time-consuming to construct and the context free grammar is very limited in that it can only be used on complete words, and it cannot handle inverse text normalizations in which the order of the symbols in the display text is different than the order in the spoken text. For example, context free grammars of the prior art cannot convert \u201cten to twelve\u201d into \u201c11:50\u201d.","The context free grammar for performing inverse text normalization under the prior art also only provides one output candidate per input spoken form. Since there is often more than one way to display a spoken word, this limited response is undesirable. In addition, the parsing system used to parse an input text using the context free grammar of the prior art is not as fast as desired.","Text normalization, in which the written form of a word or speech sound is converted into its spoken form has largely been performed by hand as part of forming the context free grammar for the speech recognition engine. As a result, text normalization and inverse text normalization have been treated as separate problems that have been addressed using separate solutions. Thus, the current state of the art has required that two separate systems be built in order to provide both text normalization and inverse text normalization.","A method and apparatus are provided that perform text normalization and inverse text normalization using a single grammar. During text normalization, a context free transducer identifies a second string of symbols from a first string of symbols it receives. During inverse text normalization, the context free transducer identifies the first string of symbols after receiving the second string of symbols.",{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 1","b":["100","100","100","100"]},"The invention is operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of well-known computing systems, environments, and\/or configurations that may be suitable for use with the invention include, but are not limited to, personal computers, server computers, hand-held or laptop devices, multiprocessor systems, microprocessor-based systems, set top boxes, programmable consumer electronics, network PCs, minicomputers, mainframe computers, telephony systems, distributed computing environments that include any of the above systems or devices, and the like.","The invention may be described in the general context of computer-executable instructions, such as program modules, being executed by a computer. Generally, program modules include routines, programs, objects, components, data structures, etc. that perform particular tasks or implement particular abstract data types. The invention is designed to be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment, program modules are located in both local and remote computer storage media including memory storage devices.","With reference to , an exemplary system for implementing the invention includes a general-purpose computing device in the form of a computer . Components of computer  may include, but are not limited to, a processing unit , a system memory , and a system bus  that couples various system components including the system memory to the processing unit . The system bus  may be any of several types of bus structures including a memory bus or memory controller, a peripheral bus, and a local bus using any of a variety of bus architectures. By way of example, and not limitation, such architectures include Industry Standard Architecture (ISA) bus, Micro Channel Architecture (MCA) bus, Enhanced ISA (EISA) bus, Video Electronics Standards Association (VESA) local bus, and Peripheral Component Interconnect (PCI) bus also known as Mezzanine bus.","Computer  typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by computer  and includes both volatile and nonvolatile media, removable and non-removable media. By way of example, and not limitation, computer readable media may comprise computer storage media and communication media. Computer storage media includes both volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information such as computer readable instructions, data structures, program modules or other data. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by computer . Communication media typically embodies computer readable instructions, data structures, program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term \u201cmodulated data signal\u201d means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media includes wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, RF, infrared and other wireless media. Combinations of any of the above should also be included within the scope of computer readable media.","The system memory  includes computer storage media in the form of volatile and\/or nonvolatile memory such as read only memory (ROM)  and random access memory (RAM) . A basic input\/output system  (BIOS), containing the basic routines that help to transfer information between elements within computer , such as during start-up, is typically stored in ROM . RAM  typically contains data and\/or program modules that are immediately accessible to and\/or presently being operated on by processing unit . By way of example, and not limitation,  illustrates operating system , application programs , other program modules , and program data .","The computer  may also include other removable\/non-removable volatile\/nonvolatile computer storage media. By way of example only,  illustrates a hard disk drive  that reads from or writes to non-removable, nonvolatile magnetic media, a magnetic disk drive  that reads from or writes to a removable, nonvolatile magnetic disk , and an optical disk drive  that reads from or writes to a removable, nonvolatile optical disk  such as a CD ROM or other optical media. Other removable\/non-removable, volatile\/nonvolatile computer storage media that can be used in the exemplary operating environment include, but are not limited to, magnetic tape cassettes, flash memory cards, digital versatile disks, digital video tape, solid state RAM, solid state ROM, and the like. The hard disk drive  is typically connected to the system bus  through a non-removable memory interface such as interface , and magnetic disk drive  and optical disk drive  are typically connected to the system bus  by a removable memory interface, such as interface .","The drives and their associated computer storage media discussed above and illustrated in , provide storage of computer readable instructions, data structures, program modules and other data for the computer . In , for example, hard disk drive  is illustrated as storing operating system , application programs , other program modules , and program data . Note that these components can either be the same as or different from operating system , application programs , other program modules , and program data . Operating system , application programs , other program modules , and program data  are given different numbers here to illustrate that, at a minimum, they are different copies.","A user may enter commands and information into the computer  through input devices such as a keyboard , a microphone , and a pointing device , such as a mouse, trackball or touch pad. Other input devices (not shown) may include a joystick, game pad, satellite dish, scanner, or the like. These and other input devices are often connected to the processing unit  through a user input interface  that is coupled to the system bus, but may be connected by other interface and bus structures, such as a parallel port, game port or a universal serial bus (USB). A monitor  or other type of display device is also connected to the system bus  via an interface, such as a video interface . In addition to the monitor, computers may also include other peripheral output devices such as speakers  and printer , which may be connected through an output peripheral interface .","The computer  is operated in a networked environment using logical connections to one or more remote computers, such as a remote computer . The remote computer  may be a personal computer, a hand-held device, a server, a router, a network PC, a peer device or other common network node, and typically includes many or all of the elements described above relative to the computer . The logical connections depicted in  include a local area network (LAN)  and a wide area network (WAN) , but may also include other networks. Such networking environments are commonplace in offices, enterprise-wide computer networks, intranets and the Internet.","When used in a LAN networking environment, the computer  is connected to the LAN  through a network interface or adapter . When used in a WAN networking environment, the computer  typically includes a modem  or other means for establishing communications over the WAN , such as the Internet. The modem , which may be internal or external, may be connected to the system bus  via the user input interface , or other appropriate mechanism. In a networked environment, program modules depicted relative to the computer , or portions thereof, may be stored in the remote memory storage device. By way of example, and not limitation,  illustrates remote application programs  as residing on remote computer . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.",{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 2","b":["200","200","202","204","206","208","210"]},"Memory  is implemented as non-volatile electronic memory such as random access memory (RAM) with a battery back-up module (not shown) such that information stored in memory  is not lost when the general power to mobile device  is shut down. A portion of memory  is preferably allocated as addressable memory for program execution, while another portion of memory  is preferably used for storage, such as to simulate storage on a disk drive.","Memory  includes an operating system , application programs  as well as an object store . During operation, operating system  is preferably executed by processor  from memory . Operating system , in one preferred embodiment, is a WINDOWS\u00ae CE brand operating system commercially available from Microsoft Corporation. Operating system  is preferably designed for mobile devices, and implements database features that can be utilized by applications  through a set of exposed application programming interfaces and methods. The objects in object store  are maintained by applications  and operating system , at least partially in response to calls to the exposed application programming interfaces and methods.","Communication interface  represents numerous devices and technologies that allow mobile device  to send and receive information. The devices include wired and wireless modems, satellite receivers and broadcast tuners to name a few. Mobile device  can also be directly connected to a computer to exchange data therewith. In such cases, communication interface  can be an infrared transceiver or a serial or parallel communication connection, all of which are capable of transmitting streaming information.","Input\/output components  include a variety of input devices such as a touch-sensitive screen, buttons, rollers, and a microphone as well as a variety of output devices including an audio generator, a vibrating device, and a display. The devices listed above are by way of example and need not all be present on mobile device . In addition, other input\/output devices may be attached to or found with mobile device  within the scope of the present invention.","The present invention provides a context free transducer that allows for both text normalization and inverse text normalization using the same grammar. An example of such a grammar is shown in .","In , the grammar is written using mark-up language tags, such as tags ,  and . In general, the tags appear in pairs with a starting tag, such as <translate> tag  and an ending tag such as <\/translate> tag .","A number of different tags can be used with the present invention. For example, a <rule> tag such as starting <rule> tag  and ending <\/rule> tag  define a set of paths that can be traversed to satisfy a rule. A path in a rule is formed by a sequence of transitions, were a transition is designated by either a pair of <translate> tags, such as <translate> tags  and , or by a <ruleref> tag, such as <ruleref> tag .","Each transition that is defined by a pair of <translate> tags includes a pair of <in> tags such as <in> tags  and , and a pair of <out> tags, such as <out> tags  and . Each pair of <in> tags includes a sequence of symbols representing a written or display form of a word. Each pair of <out> tags includes a sequence of symbols representing a spoken form of a word. During text normalization, a sequence of symbols in the input string must match a sequence of symbols between the <in> tags in order for the transition to be followed by the transducer. During inverse text normalization, a sequence of symbols in the input string must match a sequence of symbols between the <out> tags in order for the transition to be followed by the transducer.","Each <out> tag and each <in> tag may have an optional \u201ccost\u201d property that indicates the cost of matching the text between the tags. This provides a cost to each transition that can be used to form a cost for the complete path through a rule. Each <out> tag and each <in> tag may also have an optional \u201ccase=\u2018insensitive\u2019\u201d property, that allows a match to words between the tags regardless of the capitalization form of the words.","The <out> and <in> tags can also be provided with additional properties that describe further aspects of the text that must match the property of the tag in order to match the words within the tag. For example, the gender, number or person associated with an input text can be matched against a \u201cgender\u201d, \u201cnumber\u201d, or \u201cperson\u201d property listed in the <out> or <in> tag. A possible output will be generated for a tag only if the text and the properties of the text match the text between the tags and the properties defined in tags. This helps to keep the transducer from over-generating possible normalizations or inverse normalizations.","Other examples of possible properties include name entity properties such as \u201cpersonal names\u201d, \u201cplace names\u201d, and \u201ccompany names\u201d. Using these properties, it is possible to apply the correct capitalization during inverse text normalization.","Transitions marked by a <ruleref> tag are followed if the input string can match a complete path through the rule referenced in the <ruleref> tag. For example, in order for the transition marked by <ruleref> tag  to be traversed, the input string must complete a path through the \u201ccardinal-tens\u201d, which is defined between <rule> tags  and . In this context, the rule that is identified in the <ruleref> tag is the referenced rule, and the rule that contains the <ruleref> tag is the referencing rule.","To allow for multiple parallel input words to be associated with multiple parallel output words, more than one pair of <in> tags and more than one pair of <out> tags may be present between a pair of <translate> tags. When more than one pair of <in> tags or <out> tags is found between <translate> tags, the text in those tags are treated as alternative written and spoken forms, respectively.","Under the grammar of the present invention, multiple parallel transition paths may be defined between two states using a pair of <one-of> tags such as <one-of> tags  and . Each translate tag within a pair of <one-of> tags represents a separate parallel transition path. Sequences of transitions may be grouped together using a pair of <item> tags.","The tags described above are samples of possible tags under the present invention. In other implementations, other tags may be used in their place.","Because the same grammar can be used for text normalization and inverse text normalization, the present invention reduces the amount of work that must be performed to achieve both text normalization and inverse text normalization. In addition, because they are defined in the same grammar, the results produced by text normalization and inverse text normalization are more consistent with each other than in the prior art.","Before the grammar can be used by a transducer of the present invention, it must be compiled into a context free transducer network. As shown in , the text normalization\/inverse text normalization grammar  is complied by complier  into a context free transducer network . This forms a network of states that are connected together by transitions. For example, rule  of  would be compiled into network  of . In , two states  and  are connected together by a set of parallel paths including paths  and . During text normalization, the transducer may transition across one of the paths if the input string matches the text between <in> tags of the grammar. For example, for path , the input text would have to include the symbol \u201c1\u201d in order for the transducer to transition along path . During inverse text normalization, the transducer may transition across one of the paths if the input string matches text between <out> tags associated with the transition.","Under one embodiment of the present invention, the compilation of the grammar into the context free transducer network includes the formation of initial transition tables for each rule. Each initial transition table consists of a set of symbol sequences that represent the symbol sequences associated with the transitions from the first state in the rule. Under one embodiment, this table includes one set of entries for text normalization and one set of entries for inverse text normalization. Under other embodiments, the tables are only constructed for text normalization.","Because of the ability to have rule references within rules, nested rules that extend from the first state must be examined to develop the initial transition table for the rule.  provides a hierarchical depiction of a set of nested rules , , , ,  and . Rule  consists of an initial transition that is associated with symbol sequence A, and other subsequent transitions that are not shown. Since rule  only has one initial transition, only symbol sequence A is stored in initial transition table  for rule . Similarly, rule  has only a single initial transition, and as such, initial transition table  for rule  includes only a single symbol sequence D.","Rule  includes two parallel initial transitions, which are associated with symbols sequences E and F, respectively. As such, initial transition table  for rule  includes both symbol sequence E and symbol sequence F.","Rule  is referenced by rule  in parallel with an initial transition associated with symbol sequence C. As such, initial transition table  for rule  includes symbol sequence C as well as the initial transition symbol sequence A of rule .","Rule  has two initial transitions, which respectively contain rule references for rules  and . As such, initial transition table  for rule  includes the symbol sequences of initial transition table  for rule  and initial transition table  of rule . Thus, initial transition table  includes symbol sequences D, E and F.","Rule  includes two initial <ruleref> transitions that are in parallel with each other and that reference rules  and , respectively. As such, initial transition table  for rule  includes the symbol sequences from initial transition table  and the symbol sequences from initial transition table . As such, initial transition table  includes symbol sequences A, C, D, E and F.","As will be discussed further the below, the initial transition tables associated with the rules allow rules to be removed from consideration without expanding all of the sub-rules that are nested within the rule simply. This is done by determining if the next word in the input is found in the initial transition table for the rule. If the next word in the input is not found in the initial transition table, the rule does not need to be expanded for the current word.","Once context free transducer network  has been compiled, it is provided to a transducer , which parses an input text  using the context free transducer network to form a parse tree . When the transducer network is provided to transducer , the transducer is instructed to perform either text normalization or inverse text normalization, since the context free transducer network can be used for both.",{"@attributes":{"id":"p-0058","num":"0057"},"figref":"FIG. 7","b":["700","412","404"]},"At step , a token is selected from queue  and is examined at step  to determine if it is complete. The token is complete when a path from the beginning state to the ending state of the rule associated with the token has been traversed based on the input string.","If the token is complete, the token is used to advance any incomplete tokens that this token fits into at step . A list of incomplete tokens is kept in an incomplete tokens list . Advancing an incomplete token involves advancing across the transition associated with the rule for the complete token and creating a new token based on this advance. The new token, which represents a rule complete to the state after the traversed transition is added to the queue.","The complete token that was selected at step  is also stored in a complete token list  at step . By storing the completed tokens, the present invention avoids re-computing rules that are already known to match a segment of the input string.","If the selected token is not complete at step , each rule reference at each transition from the current state of the token is examined. If there exists a token in the complete token table that fits it, a new token is generated based on this advance. If the transition is a ruleref transition but there is no corresponding complete token that fits it, a new token may be generated to explore the sub-rule further. The rule is eliminated from further investigation\\search and no token is generated for this rule transition if the current symbol sequence in the input string is not in the initial transition table of the referenced rule at step . Thus, a new token for a rule reference transition is only created if the current symbol sequence in the input string would satisfy an initial transition for the rule. By examining the initial transition table of the referenced rule, the present invention avoids having to expand all of the rules that may be encapsulated within the referenced rule if the current symbol sequence would never satisfy the rule.","At step , each word transition from the current state in the rule for the token is compared against the current symbol sequence in the input string. If the symbol sequence in the input string matches a symbol sequence associated with the transition, a new token is generated that represents the current rule extended to the next state in the rule. This new token is added to the token queue .","At step , the method determines if there are more tokens in queue . If there are more tokens, the process returns to step  to select a next token from the queue. Step , ,  and  are then repeated for the new token. When there are no further tokens in the queue at step , the best completed tokens that span the longest portion of the input string are selected at step . A parse tree is then formed for each selected token at step . Each parse tree is formed by placing the word transitions that matched the input at the leaf nodes of the parse tree and the rules that connect those transitions as nodes within the parse tree. Thus, the rule associated with the selected token forms the root node of the parse tree. After the parse tree has been formed, the method determines if there are more symbol sequences in the input string at step . If there are more symbol sequences, the pointer is advanced to the position right after the portion which has been parsed (or the next space delimited symbol sequences if there was no parse) at step  and steps  through  are performed for the new symbol sequence. When there are no more symbols in the input string, the process ends at step .","Parse tree  is provided to a lattice construction unit  in . Lattice construction unit  constructs an output lattice , which represents the possible text normalizations or inverse text normalizations that can be formed from input text . Unlike the prior art, the present invention is able to provide multiple different possible text normalizations and inverse text normalizations for the same input text. Under some embodiments, each possible output path through the output lattice  includes a cost that is defined in grammar . The cost of the various paths through the output lattice  can be used to select a single path through the output lattice.","The default behavior of lattice construction unit  is to form the output lattice by traversing the leaves of parse tree  in a left to right manner. At each leaf node, a single symbol sequence or a lattice of symbol sequences is constructed. During text normalization, the text between the <out> tags associated with the transition of the leaf node is used to form the output symbol sequence. If there is only one pair of <out> tags for the transitions, a single symbol sequence is formed. However, if there are multiple pairs of <out> tags, a lattice of parallel symbol sequences is formed.","For example, if the transition was defined in the grammar as:",{"@attributes":{"id":"p-0068","num":"0067"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"154pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"<translate>"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"77pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"140pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"<in>1<\/in>"]},{"entry":[{},"<out>one<\/out>"]},{"entry":[{},"<out>one hundred<\/out>"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"154pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"<translate>"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}},"br":{}},"During inverse text normalization, the text between the <in> tags is used to form the single symbol sequence or the lattice of symbol sequences for the leaf node.","If multiple parse trees were formed for multiple complete rules, lattice construction unit  constructs a separate lattice for each parse tree and then combines the lattices in parallel to form the final output lattice .","Under one embodiment of the present invention, the default lattice construction performed by lattice construction unit  can be overridden using a script that is embedded in text normalization grammar . During the parse tree formation, this script is placed in the leaf node. When lattice construction unit  encounters a script, it constructs a complete script from all of the leaf nodes of the parse tree and then calls a script interrupter  to execute the script. As part of this execution, instructions are provided for modifying the output lattice. These instructions can include incorporating new portions into the lattice, re-organizing the lattice or overwriting parts of the lattice that were previously formed. By allowing this scripting, the present invention allows the grammar author to overcome the ordering limitation of context free transducers, which requires that the order of the input words match the order of the output words.",{"@attributes":{"id":"p-0072","num":"0071"},"figref":["FIG. 8","FIG. 8"],"b":"400"},"In , the <tag> tags are used to delimit script code. The script code can work with a set of variables to modify the output lattice. In particular, the variable \u201c$.Network[i]\u201d is an array that holds the output lattice. The index, i, to the array indicates the left-to-right position in the lattice. The $.Network[i] variable has a lattice property $.Network[i].latt, which can accept an array for position [i] in the output lattice and a string property $.Network[i].OutStr, which can accept a single string for position [i] in the output lattice.","In addition, the grammar of  provides a shorthand for referring to a variable associated with a rule. In particular, within a rule \u201c$.xyz\u201d can be used to refer to variable \u201cxyz\u201d. To access a variable from another rule, the format $rname.xyz is used, where \u201crname\u201d is the rule name where the xyz variable is set.","In , Number rule  provides conversions between the spoken form and the written form of numbers. Within each pair of <out> tags and <in> tags, is a <tag> that includes one scripting instruction. That instruction sets the $.val variable equal to the number in the <in> tag. Note that the $.Val variable is referred to as $Number.Val outside of the Number rule.","Minute rule  and Hour rule  each reference Number rule . Minute rule  includes scripting instructions between <tag> tags  and  and <tag> tags  and  that assign the value of the $number.val variable to the $.val variable, which is referenced outside of Minute rule  as $Minute.val. Hour rule  includes similar scripting instructions that assign the $number.val variable to the $.val variable, which is referenced outside of Hour rule  as $Hour.val.","Time rule  includes references to Minute rule  and Hour rule  and is triggered by phrases such as \u201cten to eleven\u201d or \u201cfive to two\u201d. Time rule  also includes <translate> tags  and , which delimit <tag> tags  and . <tag> tags  and  delimit scripting instructions. These instructions include code for performing a function TimeFormat that takes an hour and a minute value and returns a time as \u201chour:minute\u201d. The instructions also assign the value provided by this function to the $.Network[0].outstr property, and thereby set the value in the output lattice.","When the transducer is traversing the context free transducer network, it does not consider information in between the <tag> tags. As such, the information between <translate> tags  and  is treated as an epsilon transition and does not have to match the input. During parsing, the script found between <tag> tags is stored as a leaf node associated with the rule. Thus, the script between tags  and  is stored as a leaf node for the node for Time rule . Similarly, the instructions between <tag> tags for rules  and  are stored as leaf nodes for the nodes for those rules.","After the default lattice has been constructed, the script is assembled in a bottom up manner beginning with scripting instructions in the leaf nodes of the lowest nodes of the tree and adding instructions upward until reaching the leaf nodes of the root node of the parse tree. By doing this, any values that must be evaluated by a substructure will be determined in the script before the script reaches script from the root portion of the rule.","Constructing the script in this manner using the grammar of  and the input string \u201cten to eleven\u201d, the script of  would be generated. In , the $number.val variable becomes an array of variables, since the number rule is invoked more than once. As such, in line , the variable is referred to as $number[1].val and in line , the variable is referred to as $number[2].val. The variable $.network(0).outstring is an array that holds values for the output lattice. The index of the network variable indicates the position in the output lattice that is to receive the value of the \u201ctime format\u201d function.","Once the script has been constructed from the parse tree, it is executed by a script interpreter. This script interpreter receives a copy of the default output lattice and creates a new version of the output lattice based on the instructions in the script. This new version of the output lattice is then returned and used to overwrite output lattice .","By allowing for scripting in the grammar, the present invention overcomes one of the limitations of a transducer network thereby providing the flexibility found in hard coded text normalization systems while providing the ease of a single grammar that can be used for text normalization and inverse text normalization.","In prior art context free grammars, the grammar is written so that each transition in the context free grammar is associated with a complete word. For morphologically rich languages like French, Spanish and Italian, and agglutinating languages like German and Dutch, this places a great burden on the grammar author because they must include every variation of a word, such as variations for gender, person, number, and case when constructing the grammar. For example, in German, there are multiple forms for the word meter including \u201cmeter\u201d, \u201cmeters\u201d, and \u201cmetern\u201d. In order to provide coverage for all of the prefixes and that can be combined with word meter to form the different metric denominations, the grammar author of the prior art would need to write a transition for every different combination, such as \u201cterameter\u201d, \u201cterameters\u201d, \u201cterametern\u201d, \u201cgigameter\u201d, \u201cgigameters\u201d, \u201cgigametern\u201d, etc.","In one embodiment of the present invention, this burden is removed by allowing the grammar author to write grammar rules for portions of words instead of for the entire word. Thus, one grammar rule can be written for the prefixes of the words and a second grammar rule can be written for the suffixes of the words. A special property value known as \u201cglue\u201d is placed in the <out> tags to indicate that the transition listed in the grammar forms part of a complete transition.",{"@attributes":{"id":"p-0085","num":"0084"},"figref":"FIG. 10","b":["1000","1002","1000","1004","1006","1008","1004","1006","1008","1000"]},"When forming the context free transducer network , compiler  forms a separate transition for each portion of the words and annotates each transition with the \u201cglue\u201d property. During initial formation of the output lattice, lattice construction unit  places the output text and the \u201cglue\u201d property in the lattice.","A pass through the lattice is then made to merge transitions that have the \u201cglue\u201d property with their neighboring transitions. For example, if the \u201cglue\u201d property of a transition is \u201cglue-L\u201d, the transition is combined with each parallel transition to its immediate left in the output lattice. If the \u201cglue\u201d property of a transition is \u201cglue-R\u201d, the transition is combined with each parallel transition to its immediate right in the output lattice.","If there are parallel transitions with the \u201cglue\u201d property, a separate set of transitions is formed for each transition. For example, if there were three parallel transitions with the \u201cglue-L\u201d property and there were four parallel transitions to the left of these transitions in the output lattice, twelve transitions would be formed that would replace these seven transitions. Note that a similar procedure is performed for transitions with the \u201cglue-R\u201d property except that the transitions are merged with transitions to their right in the output lattice.","In other embodiments, the transitions are combined at the time of compiling the context free transducer network  instead of modifying the output lattice. Specifically, when forming the context free transducer network , compiler  forms a transition in the context free transducer network  for each possible combination of a transition that includes the \u201cglue\u201d property with the transitions of the rule that the \u201cglue\u201d property is directed to. For example, when compiler  encounters line  in , it creates a transition for each transition in MetricPrefixExpanded rule .","For each transition that is created, compiler  forms new <in> values for the transition by combining the text in the <in> tags of the transitions of the two rules that were used to form the new transition and forms new <out> values by combining the text in the <out> tags of the transitions of the two rules that were used to form the new transition. For example, the compiler combines the text in the <out> tags of line  with the text in the <out> tags of each transition of MetricPrefixExpanded rule  to form separate values for the <out> tags of each new transition.","In , there are three transitions with the glue property and fourteen transitions in MetricPrefixExpanded rule . As such, compiler  would construct forty-two different transitions in the finite state transition network.","Note that for transitions  and  in MetricPrefixExpanded rule , there are two sets of <in> tags. In such a case, a set of <in> tags is formed for each combined transition formed for transitions  and . For example, for one transition formed from transition  there would be two pairs of <in> tags, one for <in>kmeter<\/in> and one for <in>kilometer<\/in>. For a second transition, there be <in>kmetern<\/in> and <in>kilometern<\/in>.","Thus, compiler  uses the glue property to automatically form every combination of prefixes and suffixes, thereby alleviating the burden from the grammar author.","The context free transducer system of the present invention may be used to perform text normalization to assist in expanding a speech recognition grammar as shown in .","In , a speech recognition grammar  is authored by the designer of an application. This grammar defines the allowed sequences of words that may be recognized by a speech recognition engine . Upon receiving grammar , speech recognition engine  compiles the grammar using a lexicon  to identify the pronunciation of words in speech recognition grammar . These pronunciations will be used by the speech recognition engine to determine which words have been spoken.","During the compilation process, speech recognition engine  encounters symbol sequences  that are not in lexicon . When this occurs, speech recognition engine  provides symbol sequences  as input text to a context free transducer system such as context free transducer system  of . Context free transducer system  produces an output lattice  through a process as described above for .","Under one embodiment of the present invention, speech recognition engine  may access and retrieve the output lattice  through an interface  known as iTextNormMultiResult. This interface represents an interface to programming object or module  and supports a collection of methods that allows speech recognition engine  to obtain specific portions of output lattice  as well as exposing a data structure that contains the entire output lattice. In one particular embodiment, iTextNormMultiResult interface supports the following methods:","GetTopResult: which returns a string containing the highest scoring path through output lattice ","GetBestTokens: which returns the highest scoring token found in the parse tree","GetTopResults: which takes as an argument the number, n, of results to be returned and which returns an array of strings representing the top n paths through the output lattice  based on cost","IsLinear: which returns a Boolean value to indicate if output lattice  only includes one traversal.","AcceptsTraversal: which takes a string as input and returns a Boolean indicating whether this string was present in output lattice .","GetParseDepth: which returns the number of items parsed from the input text","Serialize: which returns the output lattice  in a serial format.","Deserialize: which receives as input a previous serialized form of the output lattice, and returns an array for the lattice.","The data structure containing the lattice that is exposed by ItextNormMultiResult consists of a value holding the size of the structure, a value holding the number of strings within the structure, an array of the strings starting position indices, and a value containing the strings concatenated together with null separators. The starting position indices indicate the location in the input string where each string in the output string begins.","In further embodiments of the invention, iTextNormMultiResult interface  also supports a number of methods that can be called by the finite state transducer system to perform the parsing of the input text.","The context free transducer system of  may also be used to perform inverse text normalization to convert recognized text into displayable text.  shows a block diagram of a system for converting speech recognition results into displayable text.","In , a speech recognition engine  produces recognized text . This text is passed to context free transducer system  while indicating that the context free transducer system should perform an inverse text normalization on the recognized text. The results of this inverse text normalization is an output lattice , which corresponds to output lattice  in . Output lattice  includes the inverse text normalized form of recognized text , including the displayable form such as the numbers \u201c123\u201d for the words \u201cone two three\u201d.","Using the iTextNormMultiResult interface  of programming module , a display control  selects one or more of the inverse text normalized results of output lattice  to display on a display . Under one embodiment, display control  retrieves the best scoring inverse text normalized strings to display. In a further embodiment, lower scoring inverse text normalized strings are provided as alternatives to the user when the user highlights the displayed string. This allows the user to correct the displayed string if the best scoring inverse text normalized string is not the string that they desired.","Although the present invention has been described with reference to particular embodiments, workers skilled in the art will recognize that changes may be made in form and detail without departing from the spirit and scope of the invention."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 12"}]},"DETDESC":[{},{}]}
