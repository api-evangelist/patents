---
title: Optimized packing of loose data in a graphics queue
abstract: A data queue optimized for receiving loosely packed graphics data and suitable for use in a computer graphics system is described. The data queue operates on first-in-first-out principals, and has a variable width input and output. The variable width on the input side facilitates the reception and storage of loosely packed data. The variable width output allows for the single-cycle output of multi-word data. Packing of the data occurs on the write-side of the FIFO structure.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=06847369&OS=06847369&RS=06847369
owner: Sun Microsystems, Inc.
number: 06847369
owner_city: Santa Clara
owner_country: US
publication_date: 20020130
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS"],"p":["1. Field of the Invention","This invention relates generally to the field of computer graphics and, more particularly, to render pipelines.","2. Description of the Related Art","With each new generation of graphics system, there is more image data to process and less time in which to process it. This consistent increase in data and data rates places additional burden on the interconnect systems tasked with moving data between graphics sub-systems. In addition to the volume of graphics data and the speed with which it must be handled, there are often problems encountered associated with the synchronicity of data. Frequently it is found that individual components of the graphics stream have varying processing times associated with them, making the use of a synchronous stream difficult or inefficient.","In order to move large amounts of high-speed data from one sub-system to another, wide, high-speed buses are often constructed. One example of such a bus may be represented by UPA (Ultra Port Architecture). Due to the non-continuous nature of some types of graphics data, the bus may not be fully utilized. The unused bus capacity may be observed as empty or invalid data words embedded in the stream. These empty data words may adversely affect the optimal performance of down-stream elements in the render pipe-line if these elements are designed to operate synchronously, and if they are dependent on multiple word data.","The use of FIFO type memories is effective in regulating streams where the data rate may be variable. However, the use of a FIFO may be problematic where the interfacing bus is wide, and contains sporadically placed empty data words. Therefore, for these reasons, a system and method for improving the packing of data in a graphics queue, is highly desired. A system and method is also desired for improved retrieval of data from a graphics queue, wherein ordering of the graphics data is maintained during retrieval of the graphics data.","The problems set forth above may at least in part be solved in some embodiments by a system or method for improving the packing of data in a graphics queue. In one embodiment, the system may include a bus interface configured to receive parallel graphics data and an integer number of data enables on a wide, high-speed bus. The parallel graphics data may be subdivided into an integer number of data words, and one or more data enables may be associated with each data word. The data enables may be asserted to indicate which portions of the bus contain active or valid data.","The bus interface may also comprise a plurality of input multiplexers, configured to select any one of the multiple data words transferred on the bus. A plurality of FIFO (first-in-first-out) memories may be coupled to the input multiplexers, and configured to receive and store the data words output by the input multiplexers. A write controller may maintain a next write register, which indicates the next FIFO to receive to data. The write controller may also generate control signals for the input multiplexers and FIFOs based upon the contents of the next write register and the data enables.","The system may further comprise a plurality of output multiplexers coupled to the FIFOs, and each configured to output a data word retrieved from one of the FIFOs. The FIFO read-side control signals and the output multiplexer controls may be generated by a read controller, configured to handle requests for graphics data. The read controller may maintain a next read register which indicates the next FIFO to retrieve data from.","As noted above, a method for improving the packing of data in a graphics queue is also contemplated. In one embodiment, the method includes receiving graphics data, represented by a plurality of parallel data words. The parallel data words may be examined to determine if any data words are not enabled or do not contain valid data. Each of the enabled data words may be selected and stored in a FIFO memory. The FIFO memories may be accessed in repetitive, circular fashion, and a write position pointer may be maintained to indicate the FIFO least recently accessed by a write operation. A read controller may maintain a similar read position pointer, indicating the FIFO least recently accessed by a read operation. The read controller may receive requests for graphics data, and retrieve the data from the FIFOs according to the contents of the read position pointer. The outputs of the FIFOs containing the requested data may be reordered in order to maintain the original sequential interrelationship of the data words, and output to the requesting processing block.","While the invention is susceptible to various modifications and alternative forms, specific embodiments thereof are shown by way of example in the drawings and will herein be described in detail. It should be understood, however, that the drawings and detailed description thereto are not intended to limit the invention to the particular form disclosed, but on the contrary, the intention is to cover all modifications, equivalents, and alternatives falling within the spirit and scope of the present invention as defined by the appended claims. Note, the headings are for organizational purposes only and are not meant to be used to limit or interpret the description or claims. Furthermore, note that the word \u201cmay\u201d is used throughout this application in a permissive sense (i.e., having the potential to, being able to), not a mandatory sense (i.e., must).\u201d The term \u201cinclude\u201d, and derivations thereof, mean \u201cincluding, but not limited to\u201d. The term \u201cconnected\u201d means \u201cdirectly or indirectly connected\u201d, and the term \u201ccoupled\u201d means \u201cdirectly or indirectly connected\u201d.","Computer System","Referring now to , one embodiment of a computer system  that includes a graphics system that may be used to implement one embodiment of the invention is shown. The graphics system may be comprised in any of various systems, including a computer system, network PC, Internet appliance, a television, including high definition television (HDTV) systems and interactive television systems, personal digital assistants (PDAs), virtual reality systems, and other devices which display 2D and or 3D graphics, among others.","As shown, the computer system  comprises a system unit  and a video monitor or display device  coupled to the system unit . The display device  may be any of various types of display monitors or devices including cathode ray tube (CRT), liquid crystal display (LCD) or gas-plasma display. Various input devices may be connected to the computer system, including a keyboard  and\/or a mouse , or other input device (e.g., a trackball, digitizer, tablet, six-degree of freedom input device, head tracker, eye tracker, data glove, or body sensors). Application software may be executed by the computer system  to display graphical objects on display device .","Referring now to , a simplified block diagram illustrating the computer system of  is shown. Elements of the computer system that are not necessary for an understanding of the present invention are not shown for convenience. As shown, the computer system  includes a central processing unit (CPU)  coupled to a high-speed memory bus or system bus  also referred to as the host bus . A system memory  may also be coupled to high-speed bus .","Host processor  may comprise one or more processors of varying types, e.g., microprocessors, multi-processors and CPUs. The system memory  may comprise any combination of different types of memory subsystems, including random access memories, (e.g., static random access memories or \u201cSRAMs,\u201d synchronous dynamic random access memories or \u201cSDRAMs,\u201d and Rambus dynamic random access memories or \u201cRDRAM,\u201d among others) and mass storage devices. The system bus or host bus  may comprise one or more communication or host computer buses (for communication between host processors, CPUs, and memory subsystems) as well as specialized subsystem buses.","In , a graphics system  is coupled to the high-speed memory bus . The 3-D graphics system  may be coupled to the bus  by, for example, a crossbar switch or other bus connectivity logic. It is assumed that various other peripheral devices, or other buses, may be connected to the high-speed memory bus . It is noted that the graphics system may be coupled to one or more of the buses in computer system  and\/or may be coupled to various types of buses. In addition, the graphics system may be coupled to a communication port and thereby directly receive graphics data from an external source, e.g., the Internet or a network. As shown in the figure, one or more display devices  may be connected to the graphics system  comprised in the computer system .","Host CPU  may transfer information to and from the graphics system  according to a programmed input\/output (I\/O) protocol over host bus . Alternately, graphics system  may access the memory subsystem  according to a direct memory access (DMA) protocol or through intelligent bus mastering.","A graphics application program conforming to an application programming interface (API) such as OpenGL or Java 3D may execute on host CPU  and generate commands and data that define a geometric primitive (graphics data) such as a polygon for output on display device . As defined by the particular graphics interface used, these primitives may have separate color properties for the front and back surfaces. Host processor  may transfer this graphics data to memory subsystem . Thereafter, the host processor  may operate to transfer the graphics data to the graphics system  over the host bus . In another embodiment, the graphics system  may read in geometry data arrays over the host bus  using DMA access cycles. In yet another embodiment, the graphics system  may be coupled to the system memory  through a direct port, such as the Advanced Graphics Port (AGP) promulgated by Intel Corporation.","The graphics system may receive graphics data from any of various sources, including the host CPU  and\/or the system memory , other memory, or from an external source such as a network, e.g., the Internet, or from a broadcast medium, e.g., television, or from other sources.","Note while graphics system  is depicted as part of computer system , graphics system  may also be configured as a stand-alone device (e.g., with its own built-in display). Graphics system  may also be configured as a single chip device or as part of a system-on-a-chip or a multi-chip module. Additionally, in some embodiments, certain elements of the illustrated graphics system  may be implemented in software.","Graphics System","Referring now to , a functional block diagram illustrating one embodiment of graphics system  is shown. Note that many other embodiments of graphics system  are possible and contemplated. Graphics system  may comprise one or more media processors , one or more hardware accelerators , one or more texture buffers , one or more frame buffers , and one or more video output processors . Graphics system  may also comprise one or more output devices such as digital-to-analog converters (DACs) , video encoders , flat-panel-display drivers (not shown), and\/or video projectors (not shown). Media processor  and\/or hardware accelerator  may be any suitable type of high performance processor (e.g., specialized graphics processors or calculation units, multimedia processors, digital signal processors (DSPs), or general purpose processors).","In some embodiments, one or more of these components may be removed. For example, the video output processor may be excluded from an embodiment that does not provide video output signals to drive a display device. In other embodiments, all or part of the functionality implemented in either or both of the media processor or the graphics accelerator may be implemented in software.","In some embodiments, media processor  and hardware accelerator  may be comprised within the same integrated circuit. In other embodiments, portions of media processor  and\/or hardware accelerator  may be comprised within separate integrated circuits.","As shown, graphics system  may include an interface to a host bus such as host bus  in  to enable graphics system  to communicate with a host system such as computer system . More particularly, host bus  may allow a host processor to send commands to the graphics system . In one embodiment, host bus  may be a bi-directional bus.","Each functional block of graphics system  is described in more detail below.","Media Processor",{"@attributes":{"id":"P-00040","num":"00040"},"figref":"FIG. 4","b":["14","14","112","80","112","80","14"]},"Transformation refers to manipulating an object and includes translating the object (i.e., moving the object to a different location), scaling the object (i.e., stretching or shrinking), and rotating the object (e.g., in three-dimensional space, or \u201c3-space\u201d).","Lighting refers to calculating the illumination of the objects within the displayed image to determine what color and or brightness each individual object will have. Depending upon the shading algorithm being used (e.g., constant, Gourand, or Phong), lighting may be evaluated at a number of different locations. For example, if constant shading is used (i.e., each pixel of a polygon has the same lighting), then the lighting need only be calculated once per polygon. If Gourand shading is used, then the lighting is calculated once per vertex. Phong shading calculates the lighting on a per-pixel basis.","As illustrated, media processor  may be configured to receive graphical data via host interface . A graphics queue  may be included in media processor  to buffer a stream of data received via the accelerated port of host interface . The received graphics data may comprise one or more graphics primitives. As used herein, the term graphics primitive may include polygons, parametric surfaces, splines, NURBS (non-uniform rational B-splines), sub-divisions surfaces, fractals, volume primitives, voxels (i.e., three-dimensional pixels), and particle systems. In one embodiment, media processor  may also include a geometry data preprocessor  and one or more microprocessor units (MPUs) . MPUs  may be configured to perform vertex transform and lighting calculations and programmable functions and to send results to hardware accelerator . MPUs  may also have read\/write access to texels (i.e. the smallest addressable unit of a texture map, which is used to \u201cwallpaper\u201d a three-dimensional object) and pixels in the hardware accelerator . Geometry data preprocessor  may be configured to decompress geometry, to convert and format vertex data, to dispatch vertices and instructions to the MPUs , and to send vertex and attribute tags or register data to hardware accelerator .","As shown, media processor  may have other possible interfaces, including an interface to a memory. For example, as shown, media processor  may include direct Rambus interface  to a direct Rambus DRAM (DRDRAM) . A memory such as DRDRAM  may be used for program and data storage for MPUs . DRDRAM  may also be used to store display lists and\/or vertex texture maps.","Media processor  may also include interfaces to other functional components of graphics system . For example, media processor  may have an interface to another specialized processor such as hardware accelerator . In the illustrated embodiment, controller  includes an accelerated port path that allows media processor  to control hardware accelerator . Media processor  may also include a direct interface, such as bus interface unit (BIU) , which provides a direct port path to memory  and to hardware accelerator  and video output processor  via controller .","Hardware Accelerator","Referring now to , one embodiment of the hardware accelerator  is shown. One or more hardware accelerators  may be configured to receive graphics instructions and data from media processor  and then to perform a number of functions on the received data according to the received instructions. For example, hardware accelerator  may be configured to perform rasterization, 2D or 3D texturing, pixel transfers, imaging, fragment processing, clipping, depth cueing, transparency processing, set-up, and\/or screen space rendering of various graphics primitives occurring within the graphics data.","Clipping refers to the elimination of graphics primitives or portions of graphics primitives that lie outside of a 3D view volume in world space. The 3D view volume may represent that portion of world space that is visible to a virtual observer (or virtual camera) situated in world space. For example, the view volume may be a solid truncated pyramid generated by a 2D view window and a viewpoint located in world space. The solid truncated pyramid may be imagined as the union of all rays emanating from the viewpoint and passing through the view window. The viewpoint may represent the world space location of the virtual observer. In most cases, primitives or portions of primitives that lie outside the 3D view volume are not currently visible and may be eliminated from further processing. Primitives or portions of primitives that lie inside the 3D view volume are candidates for projection onto the 2D view window.","Set-up refers to mapping primitives to a three-dimensional viewport. This involves translating and transforming the objects from their original \u201cworld-coordinate\u201d system to the established viewport's coordinates. This creates the correct perspective for three-dimensional objects displayed on the screen.","Screen-space rendering refers to the calculation performed to generate the data used to form each pixel that will be displayed. For example, hardware accelerator  may calculate \u201csamples.\u201d Samples are points have color information but no real area. Samples allow hardware accelerator  to \u201csuper-sample,\u201d or calculate more than one sample per pixel. Super-sampling may result in a higher quality image.","Hardware accelerator  may also include several interfaces. For example, in the illustrated embodiment, hardware accelerator  has four interfaces. Hardware accelerator  has an interface  (referred to as the \u201cNorth Interface\u201d) to communicate with media processor . Hardware accelerator  may also be configured to receive commands from media processor  through this interface. Additionally, hardware accelerator  may include an interface  to bus . Bus  may connect hardware accelerator  to boot PROM (programmable read-only memory)  and\/or video output processor . Boot PROM  may be configured to store system initialization data and\/or control code for frame buffer . Hardware accelerator  may communicate with texture buffer  using an eight-way interleaved texel bus that allows hardware accelerator  to read from and write to texture buffer . Hardware accelerator  may also interface to a frame buffer . For example, hardware accelerator  may be configured to read from and\/or write to frame buffer  using a four-way interleaved pixel bus.","The vertex processor  may be configured to use the vertex tags received from the media processor  to perform ordered assembly of the vertex data from the MPUs . Vertices may be saved in and\/or retrieved from a mesh buffer .","The render pipeline  may be configured to receive vertices and convert them to fragments. The render pipeline  may be configured to rasterize 2D window system primitives (e.g., dots, fonts, Bresenham lines, polygons, rectangles, fast fills, and BLITs (Bit Block Transfers, which move a rectangular block of bits from main memory into display memory, which may speed the display of moving objects on screen)) and 3D primitives (e.g., smooth and large dots, smooth and wide DDA (Digital Differential Analyzer) lines, triangles, polygons, and fast clear) into pixel fragments. The render pipeline  may be configured to handle full-screen size primitives, to calculate plane and edge slopes, and to interpolate data down to pixel tile resolution using interpolants or components such as r, g, b (i.e., red, green, and blue vertex color); r, g, b (i.e., red, green, and blue specular color from lit textures); a (alpha); and z, s, t, r, and w (texture components).","In embodiments using supersampling, the sample generator and evaluator  may be configured to generate samples from the fragments output by the render pipeline  and to determine which samples are inside the rasterization edge. Sample positions may be defined in loadable tables to enable stochastic sampling patterns.","Hardware accelerator  may be configured to write textured fragments from 3D primitives to frame buffer . The render pipeline  may send pixel tiles defining r, s, t and w to the texture address unit . The texture address unit  may determine the set of neighboring texels that are addressed by the fragment(s), as well as the interpolation coefficients for the texture filter, and request texels from the texture buffer  (as described in greater detail below). The texture buffer  may be interleaved to obtain as many neighboring texels as possible in each clock. The texture filter  may perform bilinear, trilinear or quadlinear interpolation. The texture environment  may apply texels to samples produced by the sample generator and evaluator . The texture environment  may also be used to perform geometric transformations on images (e.g., bilinear scale, rotate, flip) as well as to perform other image filtering operations on texture buffer image data (e.g., bicubic scale and convolutions).","Fragment processor  may be used to perform standard fragment processing operations such as the OpenGL fragment processing operations. For example, the fragment processor  may be configured to perform the following operations: fog, area pattern, scissor, alpha\/color test, ownership test (WID), stencil test, depth test, alpha blends or logic ops (ROP), plane masking, buffer selection, pick hit\/occlusion detection, and\/or auxiliary clipping in order to accelerate overlapping windows.","Texture Buffer","Texture buffer  may include several SDRAMs. Texture buffer  may be configured to store texture maps, image processing buffers, and accumulation buffers for hardware accelerator . Texture buffer  may have many different capacities (e.g., depending on the type of SDRAM included in texture buffer ). In some embodiments, each pair of SDRAMs may be independently row and column addressable.","Frame Buffer","Graphics system  may also include a frame buffer . In one embodiment, frame buffer  may include multiple 3DRAM64s. Frame buffer  may be configured as a display pixel buffer, an offscreen pixel buffer, and\/or a supersample buffer. Furthermore, in one embodiment, certain portions of frame buffer  may be used as a display pixel buffer, while other portions may be used as an offscreen pixel buffer and supersample buffer.","Video Output Processor","In some embodiments, a video output processor  may buffer and process pixels output from frame buffer . For example, video output processor  may be configured to read bursts of pixels from frame buffer . Video output processor  may also be configured to perform double buffer selection (dbsel) if the frame buffer  is double-buffered, overlay transparency, plane group extraction, gamma correction, pseudocolor or color lookup or bypass, and\/or cursor generation. In one embodiment, frame buffer  may include multiple 3DRAM64 devices that include the transparency overlay function and all or some of the lookup tables. Video output processor  may also be configured to support two video output streams to two displays using the two independent video raster timing generators. For example, one raster (e.g., A) may drive a 1280\u00d71024 CRT while the other (e.g., B) may drive a NTSC or PAL device with encoded television video.","In one embodiment, the video output processor  may directly output digital pixel data in lieu of analog video signals. This may be useful when a display device is based on a digital technology (e.g., an LCD-type display or a digital micro-mirror display).","DAC and Encoder","In some embodiments, the video output processor  may be configured to output a stream of digital video data to a DAC (digital to analog converter) . The DAC  may, in turn be configured to provide a high resolution RGB analog video output at dot rates of 240 MHz. This analog video output may be used to drive a display device such as a cathode ray tube (CRT) monitor. In some embodiments, the video output processor  may also output a stream of digital video data to one or more encoders . Each encoder  may be configured to supply an encoded video signal to a display (e.g., encoded NTSC or PAL video).","Vertex Processor","Turning now to , a simplified block diagram of one embodiment of a vertex processor is shown. In the illustrated embodiment, the bus interface  may be configured to receive information communicated on a bus representing a UPA (Ultra Port Architecture) architecture variant. The information transmitted on the bus may take the form of packets, where each packet may be introduced (i.e., message preamble) with a tag. The tag may contain information concerning the nature of any data following the tag, such as how many data words are expected, or where the data words are to be stored.","Information received by the bus interface  may be buffered and distributed to the two queues. The width of the bus may be 64 binary bits, and may be logically subdivided into four 16-bit halfwords, each halfword being further divisible into two bytes. In parallel with the bus data, two byte enable bits may be sent for each halfword (i.e., 8 byte enable bits). The byte enable bits may be used to indicate which bytes contain valid data during data transfer cycles. In one embodiment, the data transmitted on the bus may be aligned on halfword (even byte address) boundaries, and may be sent in byte pairs (i.e., halfwords are sent). In other embodiments, the transmitted data may be aligned on word boundaries (i.e. byte address that is an exact multiple of 4).","The tag queues  may be configured to receive and store the tags from the bus interface . These tags may be justified, reordered, and then pushed onto FIFO (first-in-first-out) storage structures until the tag unpack state machine  is ready to process them. (The internal operation of the tag queues  is described in greater detail below.) The tags may contain information regarding any associated data transmitted after the tag, this information may identify the data as being one of three basic types; register write data, vertex data, or attribute data.","A register write tag received in the tag queues may indicate that the next two 16-bit halfwords received are to be treated as register data. The two halfwords may therefore be received and pushed onto the tag queues. As the corresponding tag is unpacked and decoded by the tag unpack state machine , the data (6 bytes of data, 2B register tag plus 4B register data) may be removed from the tag queues  and transferred to the vertex processor registers . The register receiving the data may be specified by a register address embedded in the tag. In this way, the media processor  may control and configure the vertex processor . Additionally, information transmitted in this manner may allow for the media processor  to order 2d and 3d primitives.","Vertex data tags (halfword size, 2B) received in the tag queues may indicate that a series of words, doublewords or quadwords is to follow which are descriptive of one vertex of a geometric primitive. There may also be information embedded within the tag which describes how the vertex data is to be processed (i.e., push the vertex onto one of the data queues , push vertex onto the mesh buffer , etc.). In some embodiments, a vertex data tag may introduce a variable-length stream of information associated with one vertex. In these cases, the stream may be subdivided into several different components, each component conveying unique information (e.g., X, Y, Z and W coordinates, front face RGB values, back face RGB values, specular values, texture coordinates, etc.). The data received in the stream may be temporarily stored in one of the data queues until the vertex accumulation buffers  are ready to process it. In one embodiment, the receipt of a pre-defined component may be used to terminate the stream. In this case, the terminating component may be selected by writing the component type to one of the vertex processor registers .","In one embodiment, tags received which correspond to attribute data may introduce a series of doublewords or quadwords targeted for the vertex processor registers . The attribute data may be received and pushed onto the data queues , where the data may be temporarily stored until transfer to the appropriate registers may be accomplished. Each packet of the attribute data may contain a target register address and the data to be written into the register. In one embodiment, the attribute data may be terminated by a write to a reserved register address. Since one tag may be associated with a multiplicity of attribute data, this method may be more efficient for writing to large blocks of vertex processor registers  than the register tag method described above.","In some embodiments, the tag unpack state machine  may be configured to maintain status information of the individual queues within the tag queues , track pushes and pops onto the queues, and the location of tags within the queue structure. In other embodiments, a subset of these functions may be performed within the tag queues . The tag unpack state machine  may examine each tag as it is conveyed from the tag queues , and extract sequencing information embedded within the tag. The sequencing information may be decoded, and any additional, associated tags may be popped off the tag queues  (e.g., in the case where an examined tag is determined to indicate a register data transfer, two additional halfwords may be popped off the tag queues and routed to the vertex processor registers ). In one embodiment, the tag unpack state machine  may convey an encoded operation code to the data transfer state machine  in response to determining the nature of the tag, this operation code may contain information regarding the source and target locations of data to be transferred throughout the vertex processor . In other embodiments, the tag may be popped off the tag queues , and transferred directly to the data transfer state machine  by the tag unpack state machine .","In one embodiment, the data transfer state machine  may be configured to receive tags from the tag unpack state machine . The data transfer state machine may decode the tags, determine the implied data transfers, and issue the appropriate control signals the functional blocks of the vertex processor . Through the control signals, the data transfer state machine  may initiate the transfer of vertex data (e.g., from the vertex accumulation buffers  to the mesh buffer  or to the time sort buffers ), and affect updates to the vertex processor registers .","In some embodiments, there may be a large number of vertex processor registers , ranging in size from a single bit to 32 bits in width. The contents of the vertex processor registers  may be altered directly through the use of register write tags, and alternately, attribute tags may be used for modifying large blocks of registers. The function of an individual register may vary, it may be a hardware control function (e.g., setting the high-water mark for the tag queues  and data queues ), a transfer control function (e.g., specifying the number of vertices to be included in a packet), or attribute data to be applied to one or more vertices (e.g., color and transparency values).","In some embodiments, the data queues  may be configured to receive and provide short-term storage for vertices and attribute data. The data queues  may be a small FIFO memory structure, and in some embodiments, more than one data queue  may be available. In cases where there is more than one data queue  in the vertex processor , and more than one MPU  in the media processor , each data queue  may be associated with a single MPU .","In one embodiment, vertices may be built in the vertex accumulation buffers  from the constituent elements. This building process may involve combining data from the data queues  with attribute information stored in the vertex processor registers . The width of the vertex accumulation buffers  may be configured to accommodate all the information associated with a vertex before lighting is applied. This information may include some or all of the following; X, Y and Z coordinates, clipping information, texture coordinates, color values for both front and back faces, and transparency (alpha) values.","In one embodiment, the next vertex buffer  may receive vertex information from either the vertex accumulation buffers , or directly as a result of register write operations. An input multiplexer within the next vertex buffer  may choose between the two input sources, and may be controlled by signals received from the data transfer state machine . Vertex information stored temporarily in the next vertex buffer  may be routed to either the mesh buffer  or the time sort buffers .","In some embodiments, individual vertices used more than once may be temporarily stored in a mesh buffer . If, for example, an area to be displayed is composed of triangles, then one vertex may be common to two or more adjacent triangles. In these cases, saving the common vertices in the mesh buffer  may mitigate redundant data transfers, with geometric primitives assembled from a combination of inbound vertices, and vertices stored in the mesh buffer .","In one embodiment, the concept of a geometric primitive first becomes realized in the time sort buffers . The time sort buffers  may be configured to receive vertex information from the next vertex buffer  or the mesh buffer . The source of the vertex information may be controlled by signals received from the data transfer state machine . Within the time sort buffers , entire primitives may be built from the selected vertices and then output to the render pipeline ","Tag Queues","Turning now to , one embodiment of the tag queues  is illustrated. Each of the four MUXs  may be configured to receive a full bus width of information (i.e., 64 bits), and to output a single halfword (i.e., 16 bits). Thus, any of the four FIFOs  may receive any one of the four halfwords received from the bus. The write controller  may receive byte enable information from the bus interface  indicative of the active bytes within the four halfwords, and may combine this information with historical status information of the four FIFOs  (i.e., the last FIFO written to). From the combination of these two pieces of information, the write controller  may generate the multiplexer control signals, write enables, and write controls appropriate for pushing the pending tag data onto the FIFOs . As the tag data is pushed onto the queue, the write controller  may update the historical status information, indicating the last FIFO  device written to as part of the most recent operation. Additionally, the write controller  may check the FIFOs  for available space and stall the media processor  upon determination that a predetermined high-water mark has been reached. The write controller  may also signal the tag unpack state machine  that tag data is available in the tag queue  for processing.","In the illustrated embodiment, the read controller  may receive control signals from the tag unpack state machine  requesting data. These control signals may include information regarding the amount of data requested. In response to a request for data, the read controller  may utilize historical status information (i.e., the last FIFO read from) to generate read controls which may then be conveyed to the FIFOs . Additionally, the read controller  may generate the appropriate control signals for the output MUXs . Each of the output MUXs  may be directed to output a halfword from a single FIFO , selecting the FIFOs  properly so that the original ordering of the halfwords is retained. In response to a request for data, one, two or three halfwords may be transferred to the tag unpack state machine . In one embodiment, the write controller  and the read controller  may be coupled in order to synchronize the starting write and read positions.","Thus, the tag queues  may be configured to perform in a fashion similar to a circular buffer, where the width of the read and write data may be independently variable. Hence by pushing only active halfwords onto the FIFOs , the tag queues  may potentially offer the same level of functionality as a much larger fixed word width FIFO (i.e., a 64 bit wide FIFO).","Turning now to , a flow diagram representing one embodiment of a method for storing tag data in the tag queues  is illustrated. The illustrated method may be entered upon system power up or some other convenient board level reset, and begin by initializing the next write position to zero (step ). The next write position may be a two bit address indicating which of the four FIFOs  is to receive the next halfword. The next read position may then be likewise initialized to 0 (step ) and the flag indicating that there is data available in the queue may be de-asserted (step ). The process may then stall while waiting for any inbound tag data (step ).","Once inbound tag data is detected, the byte enables associated with the tag data and the next write position may be combined to generate the input multiplexer signals (step ). In some embodiments, the data is received by the tag queues  in byte pairs (halfwords), and may be aligned on halfword boundaries. In these cases therefore, it may be necessary to examine only four of the eight byte enable signals (i.e., the enables corresponding to the first byte of each quadword). This, coupled with the fact that the next write position may be implemented with two bits yields a total of 2or 64 possible conditions affecting the state of the input multiplexers. Therefore, the generation of the input multiplexer control lines may be implemented in combinatorial logic, or a small, hard-wired look-up table (LUT).","Next, the byte enables and the next write position may be combined and utilized to generate the FIFO  write controls (step ). Similar to the generation of the input multiplexer controls, the generation of the FIFO write controls may also be implemented in combinatorial logic or a small LUT, there being only 16 possible conditions to consider.","Once the data has been transferred to the FIFOs , the next write position may be updated to reflect the most recent write operation (step ). This may be accomplished by summing the value of the next write position with the number of halfwords written, and truncating the result of the summation to two bits. Next, a counter indicating the amount of available data in the tag queues  (i.e., the number of valid halfwords available) may be incremented by the number of halfwords written (step ). The data count may then be tested for equality to zero (step ), and if it is determined that the data count is not equal to zero (i.e., valid halfwords available in the queue), then the flag indicating available data may be asserted (step ). If, however, the data count is equal to zero, then the flag indicating available data may be de-asserted (step ). Once the flag has been updated, the process may branch back to step .","Referring now to , a flow diagram representing one embodiment of a method for retrieving tag information from the tag queues  is illustrated. This method may be used in conjunction with the method illustrated in FIG.  and described above, to manage the flow of tag data through the tag queues . Some features of this method for retrieving tag information (i.e., available data flag, data count, and next read position) are also employed in the method for storing tag data as described above. Hence, referring to the preceding description of a method for storing tag data will aid in understanding the following description.","The process may initially be stalled in an idle state or idle loop while waiting for valid data to become available in the tag queues (step ). Once the presence of valid tag data is indicated by the data ready flag, a single tag word may be removed from the tag queues  (step ). The next read position may be used in generating the correct output multiplexer control signals, affecting the justification and the output of the tag (step ).","The first tag may be decoded, and information extracted or inferred from tag data which may indicate that the tag includes more than one halfword (e.g. register write tag uses three halfwords) (step ). In some embodiments, this information as well as the number of words contained in the packet may be available in one of the vertex processor registers , or a separate queue. If it is determined that the tag includes more than one halfword (step ), then the additional halfwords comprising the tag may also be removed from the FIFOs , and the appropriate output multiplexer control signals generated (step ).","Once the initial tag halfword, and any additional halfwords required to complete the tag are removed from the queue, the next read position may be incremented by the number of halfwords removed (step ). Finally, the data count may be decremented by the number of halfwords removed from the queue (step ). The data count, as described earlier, may be incremented as data is stored in the FIFOs , and decremented as data is removed, thus providing an indication not only of valid data present in the queue, but also of the amount of available storage space remaining in the queue. Testing the data count against a predetermined high-water mark (not shown in ) may then allow for stalling the media processor  and averting a potential overrun condition.","Although the embodiments above have been described in considerable detail, other versions are possible. Numerous variations and modifications will become apparent to those skilled in the art once the above disclosure is fully appreciated. It is intended that the following claims be interpreted to embrace all such variations and modifications. Note the headings used herein are for organizational purposes only and are not meant to limit the description provided herein or the claims attached hereto."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The foregoing, as well as other objects, features, and advantages of this invention may be more completely understood by reference to the following detailed description when read together with the accompanying drawings in which:",{"@attributes":{"id":"P-00013","num":"00013"},"figref":"FIG. 1"},{"@attributes":{"id":"P-00014","num":"00014"},"figref":"FIG. 2"},{"@attributes":{"id":"P-00015","num":"00015"},"figref":"FIG. 3"},{"@attributes":{"id":"P-00016","num":"00016"},"figref":["FIG. 4","FIG. 3"]},{"@attributes":{"id":"P-00017","num":"00017"},"figref":["FIG. 5","FIG. 3"]},{"@attributes":{"id":"P-00018","num":"00018"},"figref":["FIG. 6","FIG. 5"]},{"@attributes":{"id":"P-00019","num":"00019"},"figref":["FIG. 7","FIG. 6"]},{"@attributes":{"id":"P-00020","num":"00020"},"figref":"FIG. 8"},{"@attributes":{"id":"P-00021","num":"00021"},"figref":"FIG. 9"}]},"DETDESC":[{},{}]}
