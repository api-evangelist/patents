---
title: Methods and appratuses for atomic storage operations
abstract: A method and apparatus for storing data packets in two different logical erase blocks pursuant to an atomic storage request is disclosed. Each data packet stored in response to the atomic storage request comprises persistent metadata indicating that the data packet pertains to an atomic storage request. In addition, a method and apparatus for restart recovery is disclosed. A data packet preceding an append point is identified as satisfying a failed atomic write criteria, indicating that the data packet pertains to a failed atomic storage request. One or more data packets associated with the failed atomic storage request are identified and excluded from an index of a non-volatile storage media.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08725934&OS=08725934&RS=08725934
owner: Fusion-io, Inc.
number: 08725934
owner_city: Salt Lake City
owner_country: US
publication_date: 20111222
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","FIELD OF THE INVENTION","BACKGROUND","Description of the Related Art","BRIEF SUMMARY","DETAILED DESCRIPTION"],"p":["This application claims priority to U.S. Provisional Patent Application No. 61\/579,627 entitled \u201cMETHODS AND APPARATUSES FOR ATOMIC STORAGE OPERATIONS\u201d and filed on Dec. 22, 2011 for Ashish Batwara et. al, which is incorporated herein by reference.","The disclosure relates to data storage and, more particularly, to methods and apparatuses for atomic storage operations.","Ensuring the integrity of data written to a storage media poses a number of significant challenges. These challenges increase given the diverse circumstances and events that may affect the storage media. For example, power failures or other types of invalid shutdowns or system restarts may have a substantial impact on data integrity, particularly if a shutdown occurs when data is being written to the storage media.","The following presents a simplified summary of the disclosed embodiments in order to provide a basic understanding of such embodiments. This summary is not an extensive overview of all contemplated embodiments, and is intended to neither identify key or critical elements nor delineate the scope of such embodiments. Its sole purpose is to present some concepts of the disclosed embodiments in a simplified form as a prelude to the more detailed description that is presented later.","In one embodiment, a method for servicing an atomic storage request to store data on a non-volatile solid-state storage device is disclosed. The non-volatile solid-state storage device may comprise one or more solid-state storage elements, each solid-state storage element partitioned into a plurality of physical erase blocks.","In one embodiment, storing the data of an atomic storage request comprises storing a first data packet and a second data packet on a non-volatile solid-state storage device in a log-based sequential format. The first data packet and the second data packet may be stored on different logical erase blocks. Each logical erase block may comprise two or more physical erase blocks.","Persistent metadata may be stored within each data packet of the atomic storage request. The persistent metadata indicates that the data pertains to the atomic storage request. The persistent metadata indicating that the data pertains to an atomic storage request may comprise a single bit within each data packet. Completion of the atomic storage request may also be acknowledged.","In one embodiment, the persistent metadata and data corresponding to the persistent metadata for each data packet are stored in a single write operation to the non-volatile solid-state storage device.","One embodiment may further comprise queuing atomic and non-atomic storage requests for the non-volatile solid-state storage device in an ordered queue. The atomic and the non-atomic storage requests may be processed in an order of arrival at the ordered queue.","The data stored on the non-volatile solid-state storage device pursuant to the atomic storage request may comprise an endpoint. In one embodiment, grooming operations within an erase block of the non-volatile solid-state storage device associated with the endpoint are prohibited.","One embodiment may further comprise receiving the atomic storage request by way of a single application programming interface call. The single application programming interface call may be identified as an atomic storage request by a flag parameter. The single application programming call may comprise a vector that identifies storage locations, which may be contiguous or noncontiguous, related to each of one or more storage operations associated with the atomic storage request.","An apparatus for servicing atomic storage requests is also disclosed. The apparatus may comprise a non-volatile solid-state storage device having one or more solid-state storage elements, each solid-state storage element partitioned into a plurality of physical erase blocks. The apparatus may further comprise a storage layer. The storage layer may be configured to store data of an atomic storage request comprising a first data packet and a second data packet on the non-volatile solid-state storage device in a log-based sequential format. The first data packet and the second data packet may be stored on different logical erase blocks. The persistent metadata indicating that the data pertains to an atomic storage request may comprise a single bit within each data packet.","The storage layer may be further configured to store persistent metadata within each data packet of the atomic storage request. The persistent metadata indicates that the data pertains to the atomic storage request. The storage layer may be further configured to acknowledge completion of the atomic storage request.","In one embodiment, the storage layer is further configured to store the persistent metadata and data corresponding to the persistent metadata for each data packet in a single write operation to the non-volatile solid-state storage device.","The storage layer may further comprise an ordered queue for queuing atomic and non-atomic storage requests for the non-volatile solid-state storage device. In one embodiment, the ordered queue processes the atomic and the non-atomic storage requests in order of arrival at the ordered queue. The apparatus may further comprise a storage layer. The storage layer may be configured to access the non-volatile storage device at the append point.","In one embodiment, a method for restart recovery for a non-volatile storage device is also disclosed. The non-volatile storage device may be configured to accept atomic and non-atomic storage requests.","The method may comprise accessing a non-volatile storage device at an append point. The non-volatile storage device may be configured to store a plurality of data packets to solid-state storage media by sequentially appending the data packets at the append point to a log-based structure of the solid-state storage media. The data packets are associated with different logical identifiers that belong to a logical address space that is independent of physical storage locations on the solid-state storage media.","The method may also comprise identifying a failed atomic storage request in response to a data packet preceding the append point comprising a persistent indicator that satisfies a failed atomic write criteria. One or more data packets associated with the failed atomic storage request may also be identified. The one or more data packets may be positioned sequentially within the log-based structure.","The method may also comprise excluding from an index each data packet associated with the failed atomic storage request. The index maps the logical identifiers to physical locations of the data packets on the solid-state storage media.","In one embodiment, the method may further comprise reading from the solid-state storage media during a power-on operation to construct the index. Exclusion of the one or more packets from the index may occur during the power-on operation and before normal input-output operations commence for the non-volatile storage device.","Excluding from the index, in one embodiment, further comprises bypassing each data packet associated with the failed atomic storage request during a scan of the log-based structure used to create the index.","Excluding from the index may further comprise removing each logical identifier that maps to each data packet associated with the failed atomic storage request from the index created by way of a scan of the log-based structure. Excluding from the index may further comprise erasing each data packet associated with the failed atomic storage request from the solid-state storage media by way of a storage space recovery operation.","In another embodiment, excluding from the index further comprises erasing each erase block of the solid-state storage media comprising one or more data packets associated with the failed atomic storage request and transferring valid data packets from each erase block to a different location on the solid-state storage media.","Erasing each erase block may comprise assigning a subsequence number to a destination erase block configured to store the transferred data packets. The subsequence number may be configured to maintain an ordered sequence among erase blocks of the log-based structure such that an ordered sequence of storage operations completed on the solid-state storage media is preserved on the solid-state storage media.","Erasing each erase block may further comprise in response to identifying a first erase block having a sequence number and second erase block having a subsequence number derived from the sequence number of the first erase block, grooming the first erase block and excluding each data packet associated with the failed atomic storage request from the index.","An apparatus for restart recovery for a non-volatile storage device configured to accept atomic and non-atomic storage requests is also disclosed.","The apparatus may comprise a non-volatile storage device configured to store a plurality of data packets to solid-state storage media by sequentially appending the data packets at an append point to a log-based structure of the solid-state storage media. The data packets associated with different logical identifiers belonging to a logical address space that is independent of physical storage locations on the solid-state storage media.","The apparatus may further comprise a virtual storage layer. The virtual storage layer may be configured to access the non-volatile storage device at the append point.","The storage layer may further be configured to identify a failed atomic storage request in response to a data packet preceding the append point comprising a persistent indicator that satisfies a failed atomic write criteria.","The storage layer may also be configured to identify one or more data packets associated with the failed atomic storage request. The one or more data packets may be positioned sequentially within the log-based structure.","The storage layer may additionally be configured to exclude from an index each data packet associated with the failed atomic storage request. The index maps the logical identifiers to physical locations of the data packets on the solid-state storage media.","In one embodiment, the storage layer is configured to read from the solid-state storage media during a power-on operation to construct the index. Exclusion of the one or more packets from the index may occur during the power-on operation and before normal input-output operations commence for the non-volatile storage device.","Excluding the packets from the index may further comprise bypassing each data packet associated with the failed atomic storage request during a scan of the log-based structure used to create the index.","Excluding the packets from the index, in one embodiment, comprises removing each logical identifier that maps to each data packet associated with the failed atomic storage request from the index created by way of a scan of the log-based structure.","To the accomplishment of the foregoing and related ends, one or more embodiments comprise the features hereinafter fully described and particularly pointed out in the claims. The following description and the annexed drawings set forth in detail certain illustrative aspects of the disclosed embodiments. These aspects are indicative, however, of but a few of the various ways in which the principles of various embodiments may be employed. Further, the disclosed embodiments are intended to include all such aspects and their equivalents.","Reference throughout this specification to features, advantages, or similar language does not imply that all of the features and advantages that may be realized with the present invention should be or are in any single embodiment of the invention. Rather, language referring to the features and advantages is understood to mean that a specific feature, advantage, or characteristic described in connection with an embodiment is included in at least one embodiment of the present invention. Thus, discussion of the features and advantages, and similar language, throughout this specification may, but do not necessarily, refer to the same embodiment.","Furthermore, the described features, advantages, and characteristics of the invention may be combined in any suitable manner in one or more embodiments. One skilled in the relevant art will recognize that the invention may be practiced without one or more of the specific features or advantages of a particular embodiment. In other instances, additional features and advantages may be recognized in certain embodiments that may not be present in all embodiments of the invention. These features and advantages of the present invention will become more fully apparent from the following description and appended claims, or may be learned by the practice of the invention as set forth hereinafter.","Many of the functional units described in this specification have been labeled as modules, in order to more particularly emphasize their implementation independence. For example, a module may be implemented as a hardware circuit comprising custom VLSI circuits or gate arrays, off-the-shelf semiconductors such as logic chips, transistors, or other discrete components. A module may also be implemented in programmable hardware devices such as field programmable gate arrays, programmable array logic, programmable logic devices, or the like.","Modules may also be implemented in software for execution by various types of processors. An identified module of executable code may, for instance, comprise one or more physical or logical blocks of computer instructions which may, for instance, be organized as an object, procedure, or function. Nevertheless, the executables of an identified module need not be physically located together, but may comprise disparate instructions stored in different locations which, when joined logically together, comprise the module and achieve the stated purpose for the module.","Indeed, a module of executable code may be a single instruction, or many instructions, and may even be distributed over several different code segments, among different programs, and across several memory devices. Similarly, operational data may be identified and illustrated herein within modules, and may be embodied in any suitable form and organized within any suitable type of data structure. The operational data may be collected as a single data set, or may be distributed over different locations including over different storage devices, and may exist, at least partially, merely as electronic signals on a system or network. Where a module or portions of a module are implemented in software, the software portions are stored on one or more computer readable media.","Reference throughout this specification to \u201cone embodiment,\u201d \u201can embodiment,\u201d or similar language means that a particular feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment of the present invention. Thus, appearances of the phrases \u201cin one embodiment,\u201d \u201cin an embodiment,\u201d and similar language throughout this specification may, but do not necessarily, all refer to the same embodiment.","Reference to a computer readable medium may take any form capable of storing machine-readable instructions on a digital processing apparatus. A computer readable medium may be embodied by a compact disk, digital-video disk, a magnetic tape, a Bernoulli drive, a magnetic disk, a punch card, flash memory, integrated circuits, or other digital processing apparatus memory device.","Furthermore, the described features, structures, or characteristics of the invention may be combined in any suitable manner in one or more embodiments. In the following description, numerous specific details are provided, such as examples of programming, software modules, user selections, network transactions, database queries, database structures, hardware modules, hardware circuits, hardware chips, etc., to provide a thorough understanding of embodiments of the invention. One skilled in the relevant art will recognize, however, that the invention may be practiced without one or more of the specific details, or with other methods, components, materials, and so forth. In other instances, well-known structures, materials, or operations are not shown or described in detail to avoid obscuring aspects of the invention.","The schematic flow chart diagrams included herein are generally set forth as logical flow chart diagrams. As such, the depicted order and labeled steps are indicative of one embodiment of the presented method. Other steps and methods may be conceived that are equivalent in function, logic, or effect to one or more steps, or portions thereof, of the illustrated method. Additionally, the format and symbols employed are provided to explain the logical steps of the method and are understood not to limit the scope of the method. Although various arrow types and line types may be employed in the flow chart diagrams, they are understood not to limit the scope of the corresponding method. Indeed, some arrows or other connectors may be used to indicate only the logical flow of the method. For instance, an arrow may indicate a waiting or monitoring period of unspecified duration between enumerated steps of the depicted method. Additionally, the order in which a particular method occurs may or may not strictly adhere to the order of the corresponding steps shown.",{"@attributes":{"id":"p-0068","num":"0067"},"figref":"FIG. 1","b":["100","100","114","102","114","114"]},"The host computing system  stores data in the storage device  and communicates data with the storage device  via a communications connection. The storage device  may be internal to the host computing system  or external to the host computing system . The communications connection may be a bus, a network, or other manner of connection allowing the transfer of data between the host computing system  and the storage device . In one embodiment, the storage device  is connected to the host computing system  by a PCI connection such as PCI express (\u201cPCI-e\u201d). The storage device  may be a card that plugs into a PCI-e connection on the host computing system .","The storage device , in the depicted embodiment, performs data storage operations such as reads, writes, erases, etc. In certain embodiments, a power connection and the communications connection for the storage device  are part of the same physical connection between the host computing system  and the storage device . For example, the storage device  may receive power over PCI, PCI-e, serial advanced technology attachment (\u201cserial ATA\u201d or \u201cSATA\u201d), parallel ATA (\u201cPATA\u201d), small computer system interface (\u201cSCSI\u201d), IEEE 1394 (\u201cFireWire\u201d), Fiber Channel, universal serial bus (\u201cUSB\u201d), PCIe-AS, or another connection with the host computing system .","The storage device  provides nonvolatile storage for the host computing system .  shows the storage device  as a non-volatile storage device  comprising a storage controller , a write data pipeline , a read data pipeline , and nonvolatile non-volatile storage media . The storage device  may contain additional components that are not shown in order to provide a simpler view of the storage device .","The non-volatile storage media  stores data such that the data is retained even when the storage device  is not powered. In some embodiments, the non-volatile storage media  comprises a solid-state storage media, such as flash memory, nano random access memory (\u201cNRAM\u201d), magneto-resistive RAM (\u201cMRAM\u201d), dynamic RAM (\u201cDRAM\u201d), phase change RAM (\u201cPRAM\u201d), Racetrack memory, Memristor memory, nanocrystal wire-based memory, silicon-oxide based sub-10 nanometer process memory, graphene memory, Silicon-Oxide-Nitride-Oxide-Silicon (\u201cSONOS\u201d), resistive random-access memory (\u201cRRAM\u201d), programmable metallization cell (\u201cPMC\u201d), conductive-bridging RAM (\u201cCBRAM\u201d), and the like. While, in the depicted embodiment, the storage device  includes non-volatile storage media , in other embodiments, the storage device  may include magnetic media such as hard disks, tape and the like, optical media, or other nonvolatile data storage media. The storage device  also includes a storage controller  that coordinates the storage and retrieval of data in the non-volatile storage media . The storage controller  may use one or more indexes to locate and retrieve data, and perform other operations on data stored in the storage device . For example, the storage controller  may include a groomer for performing data grooming operations such as garbage collection, as will be explained below.","As shown, the storage device , in certain embodiments, implements a write data pipeline  and a read data pipeline , an example of which is described in greater detail below. The write data pipeline  may perform certain operations on data as the data is transferred from the host computing system  into the non-volatile storage media . These operations may include, for example, error correction code (ECC) generation, encryption, compression, and others. The read data pipeline  may perform similar and potentially inverse operations on data that is being read out of non-volatile storage media  and sent to the host computing system .","In one embodiment, the host computing system  includes one or more other components in addition to the storage device , such as additional storage devices, graphics processors, network cards, and the like. Those of skill in the art, in view of this disclosure, will appreciate the different types of components that may be in a host computing system . The components may be internal or external to the host computing system . In one embodiment, some of the components may be PCI or PCI-e cards that connect to the host computing system  and receive power through the host computing system .","In some embodiments, the driver , or alternatively the storage interface , is an application program interface (\u201cAPI\u201d) and acts to translate commands and other data to a form suitable to be sent to a storage controller . In another embodiment, the driver  includes one or more functions of the storage controller . For example, the driver  may include all or a portion of the modules described below and may include one or more indexes or maps for the storage devices . The driver , one or more storage controllers , and one or more storage devices  comprising the storage system  have a storage interface  connection to a file system\/file server and allocation traditionally done in a file system\/file server, which is advantageously pushed down (i.e., offloaded) to the storage system .","A logical identifier, as used in this application, is an identifier of a data unit that differs from a physical address where data of the data unit is stored. A data unit, as used in this application, is any set of data that is logically grouped together. A data unit may be a file, an object, a data segment of a redundant array of inexpensive\/independent disks\/drives (\u201cRAID\u201d) data stripe, or other data set used in data storage. The data unit may be executable code, data, metadata, directories, indexes, any other type of data that may be stored in a memory device, or a combination thereof. The data unit may be identified by a name, by a logical address, a physical address, an address range, or other convention for identifying data units. A logical identifier includes data unit identifiers, such as a file name, an object identifier, an inode, Universally Unique Identifier (\u201cUUID\u201d), Globally Unique Identifier (\u201cGUID\u201d), or other data unit label, and may also include a logical block address (\u201cLBA\u201d), cylinder\/head\/sector (\u201cCHS\u201d), or other lower level logical identifier. A logical identifier generally includes any logical label that can be mapped to a physical location.","In some embodiments, the storage device  stores data in a sequential log-based format on the non-volatile storage media . For example, when a data unit is modified, data of the data unit is read from one physical storage location, modified, and then written to a different physical storage location. The order and sequence of writing data to the data storage device  may comprise an event log of the sequence of storage operations performed on the non-volatile storage device . By traversing the event log (and\/or replaying the sequence of storage operations), and storage metadata, such as a forward index can be constructed or reconstructed. During traversal of the event log from oldest operation moving towards newest operation, data on the log for a given LBA is recognized as valid until a version of the data for the given LBA is located later on the event log. The data later on the event log then becomes the valid version and older data on the event log is recognized as invalid.","In a typical random access device, logical identifiers have almost a one-to-one correspondence to physical addresses of the random access device. This one-to-one mapping in a typical random access device (excluding a small number of physical addresses on the random access device reserved for bad block mapping) also correlates to a near one-to-one relationship between storage capacity associated with logical identifiers and physical capacity associated with physical addresses. For example, if a logical identifier is a logical block address (\u201cLBA\u201d), each logical block associated with an LBA has a fixed size. A corresponding physical block on the random access device is typically the same size as a logical block. This enables a typical file server \/file system to manage physical capacity on the random access device by managing logical identifiers, such as LBAs. This continuity of LBA to physical block address (\u201cPBA\u201d) mapping is generally depended upon and utilized by file systems to defragment the data stored on the data storage device. Similarly, some systems may use this continuity to locate the data on specific physical tracks to improve performance as is the case of a technique called \u201cshort stroking\u201d the disk drive. The highly predictable LBA to PBA mapping is essential in certain applications to indirectly manage the storage of the data in the physical storage space through direct management of the logical address space.","However, the storage system  may be a log structured file system such that there is no \u201cfixed\u201d relationship or algorithm to determine the mapping of the LBA to the PBA, or in another embodiment, may be random access, but may be accessed by more than one client  or file server \/file system such that the logical identifiers allocated to each client  or file server \/file system represent a storage capacity much larger than the one-to-one relationship of logical to physical identifiers of typical systems. The storage system  may also be thinly provisioned such that one or more clients  each has an allocated logical address range that is much larger than the storage capacity of the storage devices  in the storage system . In embodiment, the storage system  manages and allocates logical identifiers such that there is no pre-determined one-to-one or near one-to-one relationship between logical identifiers and physical identifiers.","The system  is advantageous because it allows more efficient management of storage capacity than typical storage systems. For example, for typical random access devices accessible by a number of clients , if each client is allocated a certain amount of storage space, the storage space typically will exist and be tied up in the allocations even if the actual amount of storage space occupied is much less. The system  is also advantageous because the system  reduces complexity of standard thin provisioning systems connected to storage devices . A standard thin provisioning system has a thin provisioning layer comprising a logical-to-logical mapping between logical identifiers in the logical address space and physical storage locations. The system  is more efficient because multiple layers of mapping are eliminated and thin provisioning (logical-to-physical mapping) is done at the lowest level.",{"@attributes":{"id":"p-0081","num":"0080"},"figref":"FIG. 2A","b":["200","204","106","108","102","204","0","104","110","0","104","104","110","0","104","110","104","110","204","104","110","104","110","104","104","110","110","104","110"],"i":["a","n","a ","n","a","n","a ","a ","n ","n ","a ","a","a","n ","a","n","a","n\u2212","a","n\u2212","n","n"]},"In one embodiment, at least one non-volatile controller  is a field-programmable gate array (\u201cFPGA\u201d) and controller functions are programmed into the FPGA. In a particular embodiment, the FPGA is a Xilinx\u00ae FPGA. In another embodiment, the storage controller  comprises components specifically designed as a storage controller , such as an application-specific integrated circuit (\u201cASIC\u201d) or custom logic solution. Each storage controller  typically includes a write data pipeline  and a read data pipeline , which are described further in relation to . In another embodiment, at least one storage controller  is made up of a combination FPGA, ASIC, and custom logic components.","The non-volatile storage media  is an array of non-volatile storage elements , , , arranged in banks , and accessed in parallel through a bi-directional storage input\/output (\u201cI\/O\u201d) bus . The storage I\/O bus , in one embodiment, is capable of unidirectional communication at any one time. For example, when data is being written to the non-volatile storage media , data cannot be read from the non-volatile storage media . In another embodiment, data can flow both directions simultaneously. However bi-directional, as used herein with respect to a data bus, refers to a data pathway that can have data flowing in only one direction at a time, but when data flowing one direction on the bi-directional data bus is stopped, data can flow in the opposite direction on the bi-directional data bus.","A non-volatile storage element (e.g., SSS . ) is typically configured as a chip (a package of one or more dies) or a die on a circuit board. As depicted, a non-volatile storage element (e.g., ) operates independently or semi-independently of other non-volatile storage elements (e.g., ) even if these several elements are packaged together in a chip package, a stack of chip packages, or some other package element. As depicted, a row of non-volatile storage elements , , is designated as a bank . As depicted, there may be \u201cn\u201d banks -and \u201cm\u201d non-volatile storage elements -, -, -per bank in an array of n\u00d7m non-volatile storage elements , ,  in a non-volatile storage media . Of course, different embodiments may include different values for n and m. In one embodiment, a non-volatile storage media includes twenty non-volatile storage elements -per bank  with eight banks . In one embodiment, the non-volatile storage media includes twenty-four non-volatile storage elements -per bank  with eight banks . In addition to the n\u00d7m storage elements -, -, -, one or more additional columns (P) may also be addressed and operated in parallel with other non-volatile storage elements , , for one or more rows. The added P columns in one embodiment, store parity data for the portions of an ECC chunk (i.e., an ECC codeword) that span m storage elements for a particular bank. In one embodiment, each non-volatile storage element , ,  is comprised of single-level cell (\u201cSLC\u201d) devices. In another embodiment, each non-volatile storage element , ,  is comprised of multi-level cell (\u201cMLC\u201d) devices.","In one embodiment, non-volatile storage elements that share a common line on the storage I\/O bus (e.g., , , ) are packaged together. In one embodiment, a non-volatile storage element , ,  may have one or more dies per package with one or more packages stacked vertically and each die may be accessed independently. In another embodiment, a non-volatile storage element (e.g., SSS . ) may have one or more virtual dies per die and one or more dies per package and one or more packages stacked vertically and each virtual die may be accessed independently. In another embodiment, a non-volatile storage element SSS . may have one or more virtual dies per die and one or more dies per package with some or all of the one or more dies stacked vertically and each virtual die may be accessed independently.","In one embodiment, two dies are stacked vertically with four stacks per group to form eight storage elements (e.g., SSS .-SSS .) , . . . , each in a separate bank , . . . . In another embodiment, twenty-four storage elements (e.g., SSS .-SSS .) , , . . . form a logical bank so that each of the eight logical banks has twenty-four storage elements (e.g., SSS.-SSS .) , , . Data is sent to the non-volatile storage media  over the storage I\/O bus  to all storage elements of a particular group of storage elements (SSS .-SSS .) , , . The storage control bus is used to select a particular bank (e.g., Bank  ) so that the data received over the storage I\/O bus  connected to all banks  is written just to the selected bank ","In one embodiment, the storage I\/O bus  is comprised of one or more independent I\/O buses (\u201cIIOBa-m\u201d comprising -. . . -) wherein the non-volatile storage elements within each column share one of the independent I\/O buses that are connected to each non-volatile storage element , ,  in parallel. For example, one independent I\/O bus of the storage I\/O bus may be physically connected to a first non-volatile storage element , , of each bank -. A second independent I\/O bus of the storage I\/O bus may be physically connected to a second non-volatile storage element , , of each bank -. Each non-volatile storage element , , in a bank (a row of non-volatile storage elements as illustrated in ) may be accessed simultaneously and\/or in parallel. In one embodiment, where non-volatile storage elements , ,  comprise stacked packages of dies, all packages in a particular stack are physically connected to the same independent I\/O bus. As used herein, \u201csimultaneously\u201d also includes near simultaneous access where devices are accessed at slightly different intervals to avoid switching noise. Simultaneously is used in this context to be distinguished from a sequential or serial access wherein commands and\/or data are sent individually one after the other.","Typically, banks -are independently selected using the storage control bus . In one embodiment, a bank  is selected using a chip enable or chip select. Where both chip select and chip enable are available, the storage control bus  may select one package within a stack of packages. In other embodiments, other commands are used by the storage control bus  to individually select one package within a stack of packages. Non-volatile storage elements , ,  may also be selected through a combination of control signals and address information transmitted on storage I\/O bus  and the storage control bus .","In one embodiment, each non-volatile storage element , ,  is partitioned into erase blocks and each erase block is partitioned into pages. An erase block on a non-volatile storage element , ,  may be called a physical erase block or \u201cPEB.\u201d A typical page is 2048 bytes (\u201c2 kB\u201d). In one example, a non-volatile storage element (e.g., SSS .) includes two registers and can program two pages so that a two-register non-volatile storage element , ,  has a capacity of 4 kB. A bank  of twenty non-volatile storage elements , , would then have an 80 kB capacity of pages accessed with the same address going out the independent I\/O buses of the storage I\/O bus .","This group of pages in a bank  of non-volatile storage elements , , . . . of 80 kB may be called a logical page or virtual page. Similarly, an erase block of each storage element , , . . . of a bank may be grouped to form a logical erase block (which may also be called a virtual erase block). In one embodiment, an erase block of pages within a non-volatile storage element is erased when an erase command is received within the non-volatile storage element. Whereas the size and number of erase blocks, pages, planes, or other logical and physical divisions within a non-volatile storage element , ,  are expected to change over time with advancements in technology, it is to be expected that many embodiments consistent with new configurations are possible and are consistent with the general description herein.","Typically, when a packet is written to a particular location within a non-volatile storage element , wherein the packet is intended to be written to a location within a particular page which is specific to a particular physical erase block of a particular storage element of a particular bank, a physical address is sent on the storage I\/O bus  and is followed by the packet. The physical address contains enough information for the non-volatile storage element  to direct the packet to the designated location within the page. Since all storage elements in a column of storage elements (e.g., SSS .-SSS N. , , . . . ) are connected to the same independent I\/O bus (e.g., .) of the storage I\/O bus , to reach the proper page and to avoid writing the data packet to similarly addressed pages in the column of storage elements (SSS .-SSS N. , , . . . ), the bank that includes the non-volatile storage element SSS . with the correct page where the data packet is to be written is selected by the storage control bus and other banks . . . of the non-volatile storage media are deselected.","Similarly, satisfying a read command on the storage I\/O bus  requires a signal on the storage control bus  to select a single bank and the appropriate page within that bank . In one embodiment, a read command reads an entire page, and because there are multiple non-volatile storage elements , , . . . in parallel in a bank , an entire logical page is read with a read command. However, the read command may be broken into subcommands, as will be explained below with respect to bank interleave. Similarly, an entire logical page may be written to the non-volatile storage elements , , . . . of a bank in a write operation.","An erase block erase command may be sent out to erase an erase block over the storage I\/O bus  with a particular erase block address to erase a particular erase block. Typically, storage controller may send an erase block erase command over the parallel paths (independent I\/O buses --) of the storage I\/O bus  to erase a logical erase block, each with a particular erase block address to erase a particular erase block. Simultaneously, a particular bank (e.g., Bank  ) is selected over the storage control bus  to prevent erasure of similarly addressed erase blocks in non-selected banks (e.g., Banks -N -). Alternatively, no particular bank (e.g., Bank  ) is selected over the storage control bus  (or all of the banks are selected) to enable erasure of similarly addressed erase blocks in all of the banks (Banks -N -) in parallel. Other commands may also be sent to a particular location using a combination of the storage I\/O bus  and the storage control bus . One of skill in the art will recognize other ways to select a particular storage location using the bi-directional storage I\/O bus  and the storage control bus .","In one embodiment, packets are written sequentially to the non-volatile storage media . For example, storage controller streams packets to storage write buffers of a bank of storage elements  and, when the buffers are full, the packets are programmed to a designated logical page. Storage controller then refills the storage write buffers with packets and, when full, the packets are written to the next logical page. The next logical page may be in the same bank or another bank (e.g., ). This process continues, logical page after logical page, typically until a logical erase block is filled. In another embodiment, the streaming may continue across logical erase block boundaries with the process continuing, logical erase block after logical erase block.","In a read, modify, write operation, data packets associated with requested data are located and read in a read operation. Data segments of the modified requested data that have been modified are not written to the location from which they are read. Instead, the modified data segments are again converted to data packets and then written sequentially to the next available location in the logical page currently being written. The index entries for the respective data packets are modified to point to the packets that contain the modified data segments. The entry or entries in the index for data packets associated with the same requested data that have not been modified will include pointers to original location of the unmodified data packets. Thus, if the original requested data is maintained, for example to maintain a previous version of the requested data, the original requested data will have pointers in the index to all data packets as originally written. The new requested data will have pointers in the index to some of the original data packets and pointers to the modified data packets in the logical page that is currently being written.","In a copy operation, the index includes an entry for the original requested data mapped to a number of packets stored in the non-volatile storage media . When a copy is made, a new copy of the requested data is created and a new entry is created in the index mapping the new copy of the requested data to the original packets. The new copy of the requested data is also written to the non-volatile storage media  with its location mapped to the new entry in the index. The new copy of the requested data packets may be used to identify the packets within the original requested data that are referenced in case changes have been made in the original requested data that have not been propagated to the copy of the requested data and the index is lost or corrupted.","Beneficially, sequentially writing packets facilitates a more even use of the non-volatile storage media  and allows the solid-storage device controller  to monitor storage hot spots and level usage of the various logical pages in the non-volatile storage media . Sequentially writing packets also facilitates a powerful, efficient garbage collection system, which is described in detail below. One of skill in the art will recognize other benefits of sequential storage of data packets.","In various embodiments, the non-volatile storage device controller  also includes a data bus , a local bus , a buffer controller , buffers -N -, a master controller , a direct memory access (\u201cDMA\u201d) controller , a memory controller , a dynamic memory array , a static random memory array , a management controller , a management bus , a bridge  to a system bus , and miscellaneous logic , which are described below. In other embodiments, the system bus  is coupled to one or more network interface cards (\u201cNICs\u201d) , some of which may include remote DMA (\u201cRDMA\u201d) controllers , one or more central processing unit (\u201cCPU\u201d) , one or more external memory controllers  and associated external memory arrays , one or more storage controllers , peer controllers , and application specific processors , which are described below. The components - connected to the system bus  may be located in the host computing system  or may be other devices.","Typically, the storage controller(s)  communicate data to the non-volatile storage media  over a storage I\/O bus . In a typical embodiment where the non-volatile storage is arranged in banks  and each bank  includes multiple storage elements , , accessed in parallel, the storage I\/O bus  is an array of busses, one for each column of storage elements , ,  spanning the banks . As used herein, the term \u201cstorage I\/O bus\u201d may refer to one storage I\/O bus  or an array of independent data busses wherein individual data busses of the array independently communicate different data relative to one another. In one embodiment, each storage I\/O bus  accessing a column of storage elements (e.g., , , ) may include a logical-to-physical mapping for storage divisions (e.g., erase blocks) accessed in a column of storage elements , , . This mapping (or bad block remapping) allows a logical address mapped to a physical address of a storage division to be remapped to a different storage division if the first storage division fails, partially fails, is inaccessible, or has some other problem.","Data may also be communicated to the storage controller(s)  from a requesting device  through the system bus , bridge , local bus , buffer(s) , and finally over a data bus . The data bus  typically is connected to one or more buffers -controlled with a buffer controller . The buffer controller  typically controls transfer of data from the local bus  to the buffers  and through the data bus  to the pipeline input buffer  and output buffer . The buffer controller  typically controls how data arriving from a requesting device can be temporarily stored in a buffer  and then transferred onto a data bus , or vice versa, to account for different clock domains, to prevent data collisions, etc. The buffer controller  typically works in conjunction with the master controller  to coordinate data flow. As data arrives, the data will arrive on the system bus , be transferred to the local bus  through a bridge .","Typically, the data is transferred from the local bus  to one or more data buffers  as directed by the master controller  and the buffer controller . The data then flows out of the buffer(s)  to the data bus , through a non-volatile controller , and on to the non-volatile storage media  such as NAND flash or other storage media. In one embodiment, data and associated out-of-band metadata (\u201cmetadata\u201d) arriving with the data is communicated using one or more data channels comprising one or more storage controllers -1 and associated non-volatile storage media -1 while at least one channel (storage controller , non-volatile storage media ) is dedicated to in-band metadata, such as index information and other metadata generated internally to the non-volatile storage device .","The local bus  is typically a bidirectional bus or set of busses that allows for communication of data and commands between devices internal to the non-volatile storage device controller  and between devices internal to the non-volatile storage device  and devices - connected to the system bus . The bridge  facilitates communication between the local bus  and system bus . One of skill in the art will recognize other embodiments such as ring structures or switched star configurations and functions of buses , , ,  and bridges .","The system bus  is typically a bus of a host computing system  or other device in which the non-volatile storage device  is installed or connected. In one embodiment, the system bus  may be a PCI-e bus, a Serial Advanced Technology Attachment (\u201cserial ATA\u201d) bus, parallel ATA, or the like. In another embodiment, the system bus  is an external bus such as small computer system interface (\u201cSCSI\u201d), FireWire, Fiber Channel, USB, PCIe-AS, or the like. The non-volatile storage device  may be packaged to fit internally to a device or as an externally connected device.","The non-volatile storage device controller  includes a master controller  that controls higher-level functions within the non-volatile storage device . The master controller , in various embodiments, controls data flow by interpreting object requests and other requests, directs creation of indexes to map object identifiers associated with data to physical locations of associated data, coordinating DMA requests, etc. Many of the functions described herein are controlled wholly or in part by the master controller .","In one embodiment, the master controller  uses embedded controller(s). In another embodiment, the master controller  uses local memory such as a dynamic memory array  (dynamic random access memory \u201cDRAM\u201d), a static memory array  (static random access memory \u201cSRAM\u201d), etc. In one embodiment, the local memory is controlled using the master controller . In another embodiment, the master controller  accesses the local memory via a memory controller . In another embodiment, the master controller  runs a Linux server and may support various common server interfaces, such as the World Wide Web, hyper-text markup language (\u201cHTML\u201d), etc. In another embodiment, the master controller  uses a nano-processor. The master controller  may be constructed using programmable or standard logic, or any combination of controller types listed above. One skilled in the art will recognize many embodiments for the master controller .","In one embodiment, where the storage device\/non-volatile storage device controller  manages multiple data storage devices\/non-volatile storage media -, the master controller  divides the work load among internal controllers, such as the storage controllers -. For example, the master controller  may divide an object to be written to the data storage devices (e.g., non-volatile storage media -) so that a portion of the object is stored on each of the attached data storage devices. This feature is a performance enhancement allowing quicker storage and access to an object. In one embodiment, the master controller  is implemented using an FPGA. In another embodiment, the firmware within the master controller  may be updated through the management bus , the system bus  over a network connected to a NIC  or other device connected to the system bus .","In one embodiment, the master controller , which manages objects, emulates block storage such that a host computing system  or other device connected to the storage device\/non-volatile storage device  views the storage device\/non-volatile storage device  as a block storage device and sends data to specific physical addresses in the storage device\/non-volatile storage device . The master controller  then divides up the blocks and stores the data blocks as it would objects. The master controller  then maps the blocks and physical address sent with the block to the actual locations determined by the master controller . The mapping is stored in the object index. Typically, for block emulation, a block device application program interface (\u201cAPI\u201d) is provided in a driver in a computer such as the host computing system , or other device wishing to use the storage device\/non-volatile storage device  as a block storage device.","In another embodiment, the master controller  coordinates with NIC controllers  and embedded RDMA controllers  to deliver just-in-time RDMA transfers of data and command sets. NIC controller  may be hidden behind a non-transparent port to enable the use of custom drivers. Also, a driver on a host computing system  may have access to a computer network through an I\/O memory driver using a standard stack API and operating in conjunction with NICs .","In one embodiment, the master controller  is also a redundant array of independent drive (\u201cRAID\u201d) controller. Where the data storage device\/non-volatile storage device  is networked with one or more other data storage devices\/non-volatile storage devices , the master controller  may be a RAID controller for single tier RAID, multi-tier RAID, progressive RAID, etc. The master controller  also allows some objects to be stored in a RAID array and other objects to be stored without RAID. In another embodiment, the master controller  may be a distributed RAID controller element. In another embodiment, the master controller  may comprise many RAID, distributed RAID, and other functions as described elsewhere. In one embodiment, the master controller  controls storage of data in a RAID-like structure where parity information is stored in one or more storage elements , ,  of a logical page where the parity information protects data stored in the other storage elements , ,  of the same logical page.","In one embodiment, the master controller  coordinates with single or redundant network managers (e.g., switches) to establish routing, to balance bandwidth utilization, failover, etc. In another embodiment, the master controller  coordinates with integrated application specific logic (via local bus ) and associated driver software. In another embodiment, the master controller  coordinates with attached application specific processors  or logic (via the external system bus ) and associated driver software. In another embodiment, the master controller  coordinates with remote application specific logic (via the computer network ) and associated driver software. In another embodiment, the master controller  coordinates with the local bus  or external bus attached hard disk drive (\u201cHDD\u201d) storage controller.","In one embodiment, the master controller  communicates with one or more storage controllers  where the storage device\/non-volatile storage device  may appear as a storage device connected through a SCSI bus, Internet SCSI (\u201ciSCSI\u201d), fiber channel, etc. Meanwhile the storage device\/non-volatile storage device  may autonomously manage objects and may appear as an object file system or distributed object file system. The master controller  may also be accessed by peer controllers  and\/or application specific processors .","In another embodiment, the master controller  coordinates with an autonomous integrated management controller to periodically validate FPGA code and\/or controller software, validate FPGA code while running (reset) and\/or validate controller software during power on (reset), support external reset requests, support reset requests due to watchdog timeouts, and support voltage, current, power, temperature, and other environmental measurements and setting of threshold interrupts. In another embodiment, the master controller  manages garbage collection to free erase blocks for reuse. In another embodiment, the master controller  manages wear leveling. In another embodiment, the master controller  allows the data storage device\/non-volatile storage device  to be partitioned into multiple logical devices and allows partition-based media encryption. In yet another embodiment, the master controller  supports a storage controller  with advanced, multi-bit ECC correction. One of skill in the art will recognize other features and functions of a master controller  in a storage controller , or more specifically in a non-volatile storage device .","In one embodiment, the non-volatile storage device controller  includes a memory controller , which controls a dynamic random memory array  and\/or a static random memory array . As stated above, the memory controller  may be independent or integrated with the master controller . The memory controller  typically controls volatile memory of some type, such as DRAM (dynamic random memory array ) and SRAM (static random memory array ). In other examples, the memory controller  also controls other memory types such as electrically erasable programmable read only memory (\u201cEEPROM\u201d), etc. In other embodiments, the memory controller  controls two or more memory types and the memory controller  may include more than one controller. Typically, the memory controller  controls as much SRAM  as is feasible and by DRAM  to supplement the SRAM .","In one embodiment, the object index is stored in memory ,  and then periodically off-loaded to a channel of the non-volatile storage media or other non-volatile memory. One of skill in the art will recognize other uses and configurations of the memory controller , dynamic memory array , and static memory array .","In one embodiment, the non-volatile storage device controller  includes a DMA controller  that controls DMA operations between the storage device\/non-volatile storage device  and one or more external memory controllers  and associated external memory arrays  and CPUs . Note that the external memory controllers  and external memory arrays  are called external because they are external to the storage device\/non-volatile storage device . In addition, the DMA controller  may also control RDMA operations with requesting devices through a NIC  and associated RDMA controller .","In one embodiment, the non-volatile storage device controller  includes a management controller  connected to a management bus . Typically, the management controller  manages environmental metrics and status of the storage device\/non-volatile storage device . The management controller  may monitor device temperature, fan speed, power supply settings, etc. over the management bus . The management controller  may support the reading and programming of erasable programmable read only memory (\u201cEEPROM\u201d) for storage of FPGA code and controller software. Typically, the management bus  is connected to the various components within the storage device\/non-volatile storage device . The management controller  may communicate alerts, interrupts, etc. over the local bus  or may include a separate connection to a system bus  or other bus. In one embodiment, the management bus  is an Inter-Integrated Circuit (\u201cI2C\u201d) bus. One of skill in the art will recognize other related functions and uses of a management controller  connected to components of the storage device\/non-volatile storage device  by a management bus .","In one embodiment, the non-volatile storage device controller  includes miscellaneous logic  that may be customized for a specific application. Typically, where the non-volatile device controller  or master controller  is\/are configured using a FPGA or other configurable controller, custom logic may be included based on a particular application, customer requirement, storage requirement, etc.",{"@attributes":{"id":"p-0118","num":"0117"},"figref":["FIG. 2B","FIG. 2A","FIG. 2B"],"b":["0","214","110","0","214","216","216","216","216","1","216","0","205","1","207","2","209","3","211","4","213","215","0","217","0","205","216","217"],"i":["a ","a ","a ","a","b","m","a","m ","a ","a","a","a","a","a","a","a ","a","m ","a","m","a","z "]},{"@attributes":{"id":"p-0119","num":"0118"},"figref":["FIG. 3","FIG. 2A"],"b":["300","104","106","108","122","102","300","203","206","208","204","106","302","304","106","306","308","310","312","314","316","108","318","320","108","328","322","324","326","330","108","332","316","334","336","338","104","340","342","344","346","348","350","104","106","108","110","308","328"]},"The write data pipeline  includes a packetizer  that receives a data or metadata segment to be written to the non-volatile storage, either directly or indirectly through another write data pipeline  stage, and creates one or more packets sized for the non-volatile storage media . The data or metadata segment is typically part of a data structure such as an object, but may also include an entire data structure. In another embodiment, the data segment is part of a block of data, but may also include an entire block of data. Typically, a set of data such as a data structure is received from a computer such as the host computing system , or other computer or device and is transmitted to the non-volatile storage device  in data segments streamed to the non-volatile storage device . A data segment may also be known by another name, such as data parcel, but as referenced herein includes all or a portion of a data structure or data block.","Each data structure is stored as one or more packets. Each data structure may have one or more container packets. Each packet contains a header. The header may include a header type field. Type fields may include data, attribute, metadata, data segment delimiters (multi-packet), data structures, data linkages, and the like. The header may also include information regarding the size of the packet, such as the number of bytes of data included in the packet. The length of the packet may be established by the packet type. The header may include information that establishes the relationship of the packet to a data structure. An example might be the use of an offset in a data packet header to identify the location of the data segment within the data structure. One of skill in the art will recognize other information that may be included in a header added to data by a packetizer  and other information that may be added to a data packet.","Each packet includes a header and possibly data from the data or metadata segment. The header of each packet includes pertinent information to relate the packet to the data structure to which the packet belongs. For example, the header may include an object identifier or other data structure identifier and offset that indicate the data segment, object, data structure or data block from which the data packet was formed. The header may also include a logical address used by the storage bus controller  to store the packet. The header may also include information regarding the size of the packet, such as the number of bytes included in the packet. The header may also include a sequence number that identifies where the data segment belongs with respect to other packets within the data structure when reconstructing the data segment or data structure. The header may include a header type field. Type fields may include data, data structure attributes, metadata, data segment delimiters (multi-packet), data structure types, data structure linkages, and the like. One of skill in the art will recognize other information that may be included in a header added to data or metadata by a packetizer  and other information that may be added to a packet.","The write data pipeline  includes an ECC generator  that generates one or more error-correcting codes (\u201cECC\u201d) for the one or more packets received from the packetizer . The ECC generator  typically uses an error-correcting algorithm to generate ECC check bits, which are stored with the one or more data packets. The ECC codes generated by the ECC generator  together with the one or more data packets associated with the ECC codes comprise an ECC chunk. The ECC data stored with the one or more data packets is used to detect and to correct errors introduced into the data through transmission and storage. In one embodiment, packets are streamed into the ECC generator  as un-encoded blocks of length N. A syndrome of length S is calculated, appended, and output as an encoded block of length N+S. The value of N and S are dependent upon the characteristics of the ECC algorithm, which is selected to achieve specific performance, efficiency, and robustness metrics. In one embodiment, there is no fixed relationship between the ECC blocks and the packets; the packet may comprise more than one ECC block; the ECC block may comprise more than one packet; and a first packet may end anywhere within the ECC block and a second packet may begin after the end of the first packet within the same ECC block. In one embodiment, ECC algorithms are not dynamically modified. In one embodiment, the ECC data stored with the data packets is robust enough to correct errors in more than two bits.","Beneficially, using a robust ECC algorithm allowing more than single bit correction or even double bit correction allows the life of the non-volatile storage media  to be extended. For example, if flash memory is used as the storage medium in the non-volatile storage media , the flash memory may be written approximately 100,000 times without error per erase cycle. This usage limit may be extended using a robust ECC algorithm. Having the ECC generator  and corresponding ECC correction module  onboard the non-volatile storage device , the non-volatile storage device  can internally correct errors and has a longer useful life than if a less robust ECC algorithm is used, such as single bit correction. However, in other embodiments the ECC generator  may use a less robust algorithm and may correct single-bit or double-bit errors. In another embodiment, the non-volatile storage device  may comprise less reliable storage such as multi-level cell (\u201cMLC\u201d) flash in order to increase capacity, which storage may not be sufficiently reliable without more robust ECC algorithms.","In one embodiment, the write pipeline  includes an input buffer  that receives a data segment to be written to the non-volatile storage media  and stores the incoming data segments until the next stage of the write data pipeline , such as the packetizer  (or other stage for a more complex write data pipeline ) is ready to process the next data segment. The input buffer  typically allows for discrepancies between the rate data segments, which are received and processed by the write data pipeline  using an appropriately sized data buffer. The input buffer  also allows the data bus  to transfer data to the write data pipeline  at rates greater than can be sustained by the write data pipeline  in order to improve efficiency of operation of the data bus . Typically, when the write data pipeline  does not include an input buffer , a buffering function is performed elsewhere, such as in the non-volatile storage device  but outside the write data pipeline , in the host computing system , such as within a network interface card (\u201cNIC\u201d), or at another device, for example when using remote direct memory access (\u201cRDMA\u201d).","In another embodiment, the write data pipeline  also includes a write synchronization buffer  that buffers packets received from the ECC generator  prior to writing the packets to the non-volatile storage media . The write synchronization buffer  is located at a boundary between a local clock domain and a non-volatile storage clock domain and provides buffering to account for the clock domain differences. In other embodiments, synchronous non-volatile storage media  may be used and synchronization buffers   may be eliminated.","In one embodiment, the write data pipeline  also includes a media encryption module  that receives the one or more packets from the packetizer , either directly or indirectly, and encrypts the one or more packets using an encryption key unique to the non-volatile storage device  prior to sending the packets to the ECC generator . Typically, the entire packet is encrypted, including the headers. In another embodiment, headers are not encrypted. In this document, encryption key is understood to mean a secret encryption key that is managed externally from a storage controller .","The media encryption module  and corresponding media decryption module  provide a level of security for data stored in the non-volatile storage media . For example, where data is encrypted with the media encryption module , if the non-volatile storage media  is connected to a different storage controller , non-volatile storage device , or server, the contents of the non-volatile storage media  typically could not be read without use of the same encryption key used during the write of the data to the non-volatile storage media  without significant effort.","In a typical embodiment, the non-volatile storage device  does not store the encryption key in non-volatile storage and allows no external access to the encryption key. The encryption key is provided to the storage controller  during initialization. The non-volatile storage device  may use and store a non-secret cryptographic nonce that is used in conjunction with an encryption key. A different nonce may be stored with every packet. Data segments may be split between multiple packets with unique nonces for the purpose of improving protection by the encryption algorithm.","The encryption key may be received from a host computing system , a server, key manager, or other device that manages the encryption key to be used by the storage controller . In another embodiment, the non-volatile storage media  may have two or more partitions and the storage controller  behaves as though it was two or more storage controllers , each operating on a single partition within the non-volatile storage media . In this embodiment, a unique media encryption key may be used with each partition.","In another embodiment, the write data pipeline  also includes an encryption module  that encrypts a data or metadata segment received from the input buffer , either directly or indirectly, prior sending the data segment to the packetizer , the data segment encrypted using an encryption key received in conjunction with the data segment. The encryption keys used by the encryption module  to encrypt data may not be common to all data stored within the non-volatile storage device  but may vary on an per data structure basis and received in conjunction with receiving data segments as described below. For example, an encryption key for a data segment to be encrypted by the encryption module  may be received with the data segment or may be received as part of a command to write a data structure to which the data segment belongs. The solid-sate storage device  may use and store a non-secret cryptographic nonce in each data structure packet that is used in conjunction with the encryption key. A different nonce may be stored with every packet. Data segments may be split between multiple packets with unique nonces for the purpose of improving protection by the encryption algorithm.","The encryption key may be received from a host computing system , another computer, key manager, or other device that holds the encryption key to be used to encrypt the data segment. In one embodiment, encryption keys are transferred to the storage controller  from one of a non-volatile storage device , host computing system , computer, or other external agent, which has the ability to execute industry standard methods to securely transfer and protect private and public keys.","In one embodiment, the encryption module  encrypts a first packet with a first encryption key received in conjunction with the packet and encrypts a second packet with a second encryption key received in conjunction with the second packet. In another embodiment, the encryption module  encrypts a first packet with a first encryption key received in conjunction with the packet and passes a second data packet on to the next stage without encryption. Beneficially, the encryption module  included in the write data pipeline  of the non-volatile storage device  allows data structure-by-data structure or segment-by-segment data encryption without a single file system or other external system to keep track of the different encryption keys used to store corresponding data structures or data segments. Each requesting device  or related key manager independently manages encryption keys used to encrypt only the data structures or data segments sent by the requesting device .","In one embodiment, the encryption module  may encrypt the one or more packets using an encryption key unique to the non-volatile storage device . The encryption module  may perform this media encryption independently, or in addition to the encryption described above. Typically, the entire packet is encrypted, including the headers. In another embodiment, headers are not encrypted. The media encryption by the encryption module  provides a level of security for data stored in the non-volatile storage media . For example, where data is encrypted with media encryption unique to the specific non-volatile storage device , if the non-volatile storage media  is connected to a different storage controller , non-volatile storage device , or host computing system , the contents of the non-volatile storage media  typically could not be read without use of the same encryption key used during the write of the data to the non-volatile storage media  without significant effort.","In another embodiment, the write data pipeline  includes a compression module  that compresses the data or metadata segment prior to sending the data segment to the packetizer . The compression module  typically compresses a data or metadata segment using a compression routine known to those of skill in the art to reduce the storage size of the segment. For example, if a data segment includes a string of 512 zeros, the compression module  may replace the 512 zeros with code or token indicating the 512 zeros where the code is much more compact than the space taken by the 512 zeros.","In one embodiment, the compression module  compresses a first segment with a first compression routine and passes along a second segment without compression. In another embodiment, the compression module  compresses a first segment with a first compression routine and compresses the second segment with a second compression routine. Having this flexibility within the non-volatile storage device  is beneficial so that computing systems  or other devices writing data to the non-volatile storage device  may each specify a compression routine or so that one can specify a compression routine while another specifies no compression. Selection of compression routines may also be selected according to default settings on a per data structure type or data structure class basis. For example, a first data structure of a specific data structure may be able to override default compression routine settings and a second data structure of the same data structure class and data structure type may use the default compression routine and a third data structure of the same data structure class and data structure type may use no compression.","In one embodiment, the write data pipeline  includes a garbage collector bypass  that receives data segments from the read data pipeline  as part of a data bypass in a garbage collection system. A garbage collection system (also referred to as a \u201cgroomer\u201d or grooming operation) typically marks packets that are no longer valid, typically because the packet is marked for deletion or has been modified and the modified data is stored in a different location. At some point, the garbage collection system determines that a particular section (e.g., an erase block) of storage may be recovered. This determination may be due to a lack of available storage capacity, the percentage of data marked as invalid reaching a threshold, a consolidation of valid data, an error detection rate for that section of storage reaching a threshold, or improving performance based on data distribution, etc. Numerous factors may be considered by a garbage collection algorithm to determine when a section of storage is to be recovered.","Once a section of storage has been marked for recovery, valid packets in the section typically must be relocated. The garbage collector bypass  allows packets to be read into the read data pipeline  and then transferred directly to the write data pipeline  without being routed out of the storage controller . In one embodiment, the garbage collector bypass  is part of an autonomous garbage collector system that operates within the non-volatile storage device . This allows the non-volatile storage device  to manage data so that data is systematically spread throughout the non-volatile storage media  to improve performance, data reliability and to avoid overuse and underuse of any one location or area of the non-volatile storage media  and to lengthen the useful life of the non-volatile storage media .","The garbage collector bypass  coordinates insertion of segments into the write data pipeline  with other segments being written by computing systems  or other devices. In the depicted embodiment, the garbage collector bypass  is before the packetizer  in the write data pipeline  and after the depacketizer  in the read data pipeline , but may also be located elsewhere in the read and write data pipelines , . The garbage collector bypass  may be used during a flush of the write pipeline  to fill the remainder of the logical page in order to improve the efficiency of storage within the non-volatile storage media  and thereby reduce the frequency of garbage collection.","Grooming may comprise refreshing data stored on the non-volatile storage media . Data stored on the non-volatile storage media  may degrade over time. The storage controller  may comprise a groomer that identifies \u201cstale\u201d data on the non-volatile storage device  (data that has not been modified and\/or moved for a pre-determined time), and refreshes the stale data by re-writing the data to a different storage location.","In some embodiments, the garbage collection system, groomer, and\/or garbage collection bypass  may be temporarily disabled to allow data to be stored contiguously on physical storage locations of the non-volatile storage device . Disabling the garbage collection system and\/or bypass  may ensure that data in the write data pipeline  is not interleaved with other data. For example, and discussed below, garbage collection and\/or the garbage collection bypass  may be disabled when storing data pertaining to an atomic storage request.","In some embodiments, the garbage collection and\/or groomer may be restricted to a certain portion of the physical storage space of the non-volatile storage device. For example, storage metadata, such as the reverse index described below, may be periodically persisted to a non-volatile storage location. The garbage collection and\/or grooming may be restricted to operating on portions of the non-volatile storage media that correspond to the persisted storage metadata.","In one embodiment, the write data pipeline  includes a write buffer  that buffers data for efficient write operations. Typically, the write buffer  includes enough capacity for packets to fill at least one logical page in the non-volatile storage media . This allows a write operation to send an entire logical page of data to the non-volatile storage media  without interruption. By sizing the write buffer  of the write data pipeline  and buffers within the read data pipeline  to be the same capacity or larger than a storage write buffer within the non-volatile storage media , writing and reading data is more efficient since a single write command may be crafted to send a full logical page of data to the non-volatile storage media  instead of multiple commands.","While the write buffer  is being filled, the non-volatile storage media  may be used for other read operations. This is advantageous because other non-volatile devices with a smaller write buffer or no write buffer may tie up the non-volatile storage when data is written to a storage write buffer and data flowing into the storage write buffer stalls. Read operations will be blocked until the entire storage write buffer is filled and programmed. Another approach for systems without a write buffer or a small write buffer is to flush the storage write buffer that is not full in order to enable reads. Again, this is inefficient because multiple write\/program cycles are required to fill a page.","For depicted embodiment with a write buffer  sized larger than a logical page, a single write command, which includes numerous subcommands, can then be followed by a single program command to transfer the page of data from the storage write buffer in each non-volatile storage element , ,  to the designated page within each non-volatile storage element , , . This technique has the benefits of eliminating partial page programming, which is known to reduce data reliability and durability, while freeing up the destination bank for reads and other commands while the buffer fills.","In one embodiment, the write buffer  is a ping-pong buffer where one side of the buffer is filled and then designated for transfer at an appropriate time while the other side of the ping-pong buffer is being filled. In another embodiment, the write buffer  includes a first-in first-out (\u201cFIFO\u201d) register with a capacity of more than a logical page of data segments. One of skill in the art will recognize other write buffer  configurations that allow a logical page of data to be stored prior to writing the data to the non-volatile storage media .","In another embodiment, the write buffer  is sized smaller than a logical page so that less than a page of information could be written to a storage write buffer in the non-volatile storage media . In the embodiment, to prevent a stall in the write data pipeline  from holding up read operations, data is queued using the garbage collection system that needs to be moved from one location to another as part of the garbage collection process. In case of a data stall in the write data pipeline , the data can be fed through the garbage collector bypass  to the write buffer  and then on to the storage write buffer in the non-volatile storage media  to fill the pages of a logical page prior to programming the data. In this way, a data stall in the write data pipeline  would not stall reading from the non-volatile storage device .","In another embodiment, the write data pipeline  includes a write program module  with one or more user-definable functions within the write data pipeline . The write program module  allows a user to customize the write data pipeline . A user may customize the write data pipeline  based on a particular data requirement or application. Where the storage controller  is an FPGA, the user may program the write data pipeline  with custom commands and functions relatively easily. A user may also use the write program module  to include custom functions with an ASIC; however, customizing an ASIC may be more difficult than with an FPGA. The write program module  may include buffers and bypass mechanisms to allow a first data segment to execute in the write program module  while a second data segment may continue through the write data pipeline . In another embodiment, the write program module  may include a processor core that can be programmed through software.","Note that the write program module  is shown between the input buffer  and the compression module , however, the write program module  could be anywhere in the write data pipeline  and may be distributed among the various stages -. In addition, there may be multiple write program modules  distributed among the various states - that are programmed and operate independently. In addition, the order of the stages - may be altered. One of skill in the art will recognize workable alterations to the order of the stages - based on particular user requirements.","The read data pipeline  includes an ECC correction module  that determines if a data error exists in ECC blocks a requested packet received from the non-volatile storage media  by using ECC stored with each ECC block of the requested packet. The ECC correction module  then corrects any errors in the requested packet if any error exists and the errors are correctable using the ECC. For example, if the ECC can detect an error in six bits but can only correct three bit errors, the ECC correction module  corrects ECC blocks of the requested packet with up to three bits in error. The ECC correction module  corrects the bits in error by changing the bits in error to the correct one or zero state so that the requested data packet is identical to when it was written to the non-volatile storage media  and the ECC was generated for the packet.","If the ECC correction module  determines that the requested packets contains more bits in error than the ECC can correct, the ECC correction module  cannot correct the errors in the corrupted ECC blocks of the requested packet and sends an interrupt. In one embodiment, the ECC correction module  sends an interrupt with a message indicating that the requested packet is in error. The message may include information that the ECC correction module  cannot correct the errors or the inability of the ECC correction module  to correct the errors may be implied. In another embodiment, the ECC correction module  sends the corrupted ECC blocks of the requested packet with the interrupt and\/or the message.","In one embodiment, a corrupted ECC block or portion of a corrupted ECC block of the requested packet that cannot be corrected by the ECC correction module  is read by the master controller , corrected, and returned to the ECC correction module  for further processing by the read data pipeline . In one embodiment, a corrupted ECC block or portion of a corrupted ECC block of the requested packet is sent to the device requesting the data. The requesting device  may correct the ECC block or replace the data using another copy, such as a backup or mirror copy, and then may use the replacement data of the requested data packet or return it to the read data pipeline . The requesting device  may use header information in the requested packet in error to identify data required to replace the corrupted requested packet or to replace the data structure to which the packet belongs. In another embodiment, the storage controller  stores data using some type of RAID and is able to recover the corrupted data. In another embodiment, the ECC correction module  sends an interrupt and\/or message and the receiving device fails the read operation associated with the requested data packet. One of skill in the art will recognize other options and actions to be taken as a result of the ECC correction module  determining that one or more ECC blocks of the requested packet are corrupted and that the ECC correction module  cannot correct the errors.","The read data pipeline  includes a depacketizer  that receives ECC blocks of the requested packet from the ECC correction module , directly or indirectly, and checks and removes one or more packet headers. The depacketizer  may validate the packet headers by checking packet identifiers, data length, data location, etc. within the headers. In one embodiment, the header includes a hash code that can be used to validate that the packet delivered to the read data pipeline  is the requested packet. The depacketizer  also removes the headers from the requested packet added by the packetizer . The depacketizer  may be directed to not operate on certain packets but pass these forward without modification. An example might be a container label that is requested during the course of a rebuild process where the header information is required for index reconstruction. Further examples include the transfer of packets of various types destined for use within the non-volatile storage device . In another embodiment, the depacketizer  operation may be packet type dependent.","The read data pipeline  includes an alignment module  that receives data from the depacketizer  and removes unwanted data. In one embodiment, a read command sent to the non-volatile storage media  retrieves a packet of data. A device requesting the data may not require all data within the retrieved packet and the alignment module  removes the unwanted data. If all data within a retrieved page is requested data, the alignment module  does not remove any data.","The alignment module  re-formats the data as data segments of a data structure in a form compatible with a device requesting the data segment prior to forwarding the data segment to the next stage. Typically, as data is processed by the read data pipeline , the size of data segments or packets changes at various stages. The alignment module  uses received data to format the data into data segments suitable to be sent to the requesting device  and joined to form a response. For example, data from a portion of a first data packet may be combined with data from a portion of a second data packet. If a data segment is larger than a data requested by the requesting device , the alignment module  may discard the unwanted data.","In one embodiment, the read data pipeline  includes a read synchronization buffer  that buffers one or more requested packets read from the non-volatile storage media  prior to processing by the read data pipeline . The read synchronization buffer  is at the boundary between the non-volatile storage clock domain and the local bus clock domain and provides buffering to account for the clock domain differences.","In another embodiment, the read data pipeline  includes an output buffer  that receives requested packets from the alignment module  and stores the packets prior to transmission to the requesting device . The output buffer  accounts for differences between when data segments are received from stages of the read data pipeline  and when the data segments are transmitted to other parts of the storage controller  or to the requesting device . The output buffer  also allows the data bus  to receive data from the read data pipeline  at rates greater than can be sustained by the read data pipeline  in order to improve efficiency of operation of the data bus .","In one embodiment, the read data pipeline  includes a media decryption module  that receives one or more encrypted requested packets from the ECC correction module  and decrypts the one or more requested packets using the encryption key unique to the non-volatile storage device  prior to sending the one or more requested packets to the depacketizer . Typically, the encryption key used to decrypt data by the media decryption module  is identical to the encryption key used by the media encryption module . In another embodiment, the non-volatile storage media  may have two or more partitions and the storage controller  behaves as though it was two or more storage controllers  each operating on a single partition within the non-volatile storage media . In this embodiment, a unique media encryption key may be used with each partition.","In another embodiment, the read data pipeline  includes a decryption module  that decrypts a data segment formatted by the depacketizer  prior to sending the data segment to the output buffer . The data segment may be decrypted using an encryption key received in conjunction with the read request that initiates retrieval of the requested packet received by the read synchronization buffer . The decryption module  may decrypt a first packet with an encryption key received in conjunction with the read request for the first packet and then may decrypt a second packet with a different encryption key or may pass the second packet on to the next stage of the read data pipeline  without decryption. When the packet was stored with a non-secret cryptographic nonce, the nonce is used in conjunction with an encryption key to decrypt the data packet. The encryption key may be received from a host computing system , a client, key manager, or other device that manages the encryption key to be used by the storage controller .","In another embodiment, the read data pipeline  includes a decompression module  that decompresses a data segment formatted by the depacketizer . In one embodiment, the decompression module  uses compression information stored in one or both of the packet header and the container label to select a complementary routine to that used to compress the data by the compression module . In another embodiment, the decompression routine used by the decompression module  is dictated by the device requesting the data segment being decompressed. In another embodiment, the decompression module  selects a decompression routine according to default settings on a per data structure type or data structure class basis. A first packet of a first object may be able to override a default decompression routine and a second packet of a second data structure of the same data structure class and data structure type may use the default decompression routine and a third packet of a third data structure of the same data structure class and data structure type may use no decompression.","In another embodiment, the read data pipeline  includes a read program module  that includes one or more user-definable functions within the read data pipeline . The read program module  has similar characteristics to the write program module  and allows a user to provide custom functions to the read data pipeline . The read program module  may be located as shown in , may be located in another position within the read data pipeline , or may include multiple parts in multiple locations within the read data pipeline . Additionally, there may be multiple read program modules  within multiple locations within the read data pipeline  that operate independently. One of skill in the art will recognize other forms of a read program module  within a read data pipeline . As with the write data pipeline , the stages of the read data pipeline  may be rearranged and one of skill in the art will recognize other orders of stages within the read data pipeline .","The storage controller  includes control and status registers  and corresponding control queues . The control and status registers  and control queues  facilitate control and sequencing commands and subcommands associated with data processed in the write and read data pipelines , . For example, a data segment in the packetizer  may have one or more corresponding control commands or instructions in a control queue  associated with the ECC generator . As the data segment is packetized, some of the instructions or commands may be executed within the packetizer . Other commands or instructions may be passed to the next control queue  through the control and status registers  as the newly formed data packet created from the data segment is passed to the next stage.","Commands or instructions may be simultaneously loaded into the control queues  for a packet being forwarded to the write data pipeline  with each pipeline stage pulling the appropriate command or instruction as the respective packet is executed by that stage. Similarly, commands or instructions may be simultaneously loaded into the control queues  for a packet being requested from the read data pipeline  with each pipeline stage pulling the appropriate command or instruction as the respective packet is executed by that stage. One of skill in the art will recognize other features and functions of control and status registers  and control queues .","The storage controller  and or non-volatile storage device  may also include a bank interleave controller , a synchronization buffer , a storage bus controller , and a multiplexer (\u201cMUX\u201d) .","In some embodiments, a storage layer provides an interface through which storage clients perform persistent operations. The storage layer may simplify data storage operations for storage clients and expose enhanced storage features, such as atomicity, transactional support, recovery, and so on.  depicts one embodiment of a system comprising a storage layer (SL)  that presents a logical address space  of the non-volatile storage device  to storage client applications  operating on a computing device . The computing device  may comprise a processor, non-volatile storage, memory, human-machine interface (HMI) components, communication interfaces (for communication via the network ), and the like.","The non-volatile storage device  may comprise a single non-volatile storage device, may comprise a plurality of non-volatile storage devices, a cluster of storage devices, or other suitable configurations. The storage layer  may comprise a driver, a user-space application, or the like. In some embodiments, the storage layer  is implemented in conjunction with the driver  described above. The storage layer  and\/or the storage clients  may be embodied as instructions stored on a non-volatile storage device.","The SL  may maintain and present a logical address space to  to the storage clients  via one or more interfaces and\/or APIs provided by the SL  (SL interface ). The storage clients  may include, but are not limited to: operating systems, virtual operating systems (e.g., guest operating systems, hypervisors, etc.), file systems, database applications, server applications, general-purpose applications, and the like. In some embodiments, one or more storage clients  operating on a remote computing device to access the VSL  via a network .","The SL  is configured to perform persistent storage operations on the non-volatile storage device , which may comprise a non-volatile storage device as described above. The VSL  communicates with the non-volatile storage device  via a communication bus , which may include, but is not limited to: a PCE-e bus, a network connection (e.g., Infiniband), a storage network, Fibre Channel Protocol (FCP) network, HyperSCSI, or the like. The storage operations may be configured according to the capabilities and\/or configuration of the nonvolatile storage device . For example, if the non-volatile storage device  comprises a write-once, block-erasable device, the SL  may be configured to perform storage operations accordingly (e.g., storage data on initialized or erased storage locations, etc.).","In some embodiments, the SL  accesses storage metadata  to maintain associations between logical identifiers (e.g., blocks) in the logical address space  and physical storage locations on the non-volatile storage device . As used herein, a physical storage location may refer to any storage location of the non-volatile storage device , which may include, but are not limited to: storage divisions, erase blocks, storage units, pages, logical pages, logical erase blocks, and so on.","The SL  maintains \u201cany-to-any\u201d assignments between logical identifiers in the logical address space  and physical storage locations on the non-volatile storage device . The SL  may cause data to be written or updated \u201cout-of-place\u201d on the non-volatile storage device . In some embodiments, data is stored sequentially and in a log-based format. Storing data \u201cout-of-place\u201d provides wear-leveling benefits and addresses \u201cerase-and-program-once\u201d properties of many non-volatile storage devices. Moreover, out-of-place writing (and writing data in logical storage locations as opposed to individual pages) addresses asymmetric properties of the non-volatile storage device . Asymmetric properties refers to the idea that different storage operations (read, write, erase) take very different amounts of time. For example, it may take ten times as long to program data on a non-volatile storage media  as it takes to read data from the solid-state storage element media . Moreover, in some cases, data may only be programmed to physical storage locations that have first been initialized (e.g., erased). An erase operation may take ten times as long as a program operation (and by extension one hundred times as long as a read operation). Associations between logical identifiers in the logical address space  and physical storage locations on the non-volatile storage device  are maintained in the storage metadata .","In some embodiments, the SL  causes data to be persisted on the non-volatile storage  in a sequential, log-based format. Sequential, log-based storage may comprise persisting the order of storage operations performed on the non-volatile storage device . In some embodiments, data is stored with persistent metadata that is persisted on the non-volatile storage device  with the data itself. For example, a sequence order of storage operations performed may be maintained using sequence indicators (e.g., timestamps, sequence numbers, or other indicators) that are stored on the non-volatile storage device  and\/or the current storage location (e.g., append point, discussed below) of the non-volatile storage device .","Persisting data in a sequential, log-based format may comprise persisting metadata on the non-volatile storage device  that describes the data. The persistent metadata may be stored with the data itself (e.g., in the same program and\/or storage operation and\/or in the smallest write unit supported by the non-volatile storage device ); the persistent metadata may, therefore, be guaranteed to be stored with the data it describes. In some embodiments, data is stored in a container format (e.g., a packet, ECC codeword, etc.). Persistent metadata may be included as part of the packet format of the data (e.g., as a header, footer, or other field within the packet). Alternatively, or in addition, portions of the persistent metadata may be stored separately from the data it describes.","The persistent metadata describes the data and may include, but is not limited to: a logical identifier (or other identifier) of the data, security or access control parameters, sequence information (e.g., a sequence indicator), a persistent metadata flag (e.g., indicating inclusion in an atomic storage operation), a transaction identifier, or the like. The persistent metadata may comprise sufficient information to reconstruct the storage metadata and\/or replay the sequence of storage operations performed on the non-volatile storage device .","The sequential, log-based data may comprise an \u201cevent log\u201d of storage operations that are performed on the non-volatile storage device . Accordingly, the SL  may be capable of replaying a sequence of storage operations performed on the non-volatile storage device  by accessing the data stored on the non-volatile storage media  in a particular order that matches the order of the event log. The sequential, log-based data format enables the SL  to reconstruct the storage metadata , as well as other data, in the event of an invalid shutdown (or other failure condition). Examples of apparatus, systems, and methods for crash recovery and\/or data integrity despite invalid shutdown conditions are described in U.S. Provisional Patent Application No. 61\/424,585, entitled, \u201cAPPARATUS, SYSTEM, AND METHOD FOR PERSISTENT MANAGEMENT OF DATA IN A CACHE DEVICE,\u201d filed Dec. 17, 2010, and in U.S. Provisional Patent Application No. 61\/425,167, entitled, \u201cAPPARATUS, SYSTEM, AND METHOD FOR PERSISTENT MANAGEMENT OF DATA IN A CACHE DEVICE,\u201d filed Dec. 20, 2010, which are hereby incorporated by reference in their entirety. In some embodiments, the non-volatile storage device  comprises a secondary power source  (e.g., battery, capacitor, etc.) to power the storage controller  and\/or non-volatile storage media  in the event of an invalid shutdown. The non-volatile storage device  (or controller ) may, therefore, comprise a \u201cprotection domain\u201d or \u201cpowercut safe domain\u201d (defined by the secondary power source ). Once data is transferred to within the protection domain, of the non-volatile storage device, it may be guaranteed to be persisted on the non-volatile storage media . Alternatively, or in addition, the storage controller  may be capable of performing storage operations independent of the host computing device .","A primary power source  is also disclosed. The primary power source  is the primary source of electrical power for the non-volatile storage device . The primary power source  may be coupled directly to the computing device , which, in turn, supplies power to the non-volatile storage device . In an alternative embodiment (not illustrated), the primary power source  is directly coupled to the non-volatile storage device .","The sequential, log-based storage format implemented by the SL  provides crash-recovery and\/or data integrity for the data stored on the non-volatile storage  as well as the storage metadata . After an invalid shutdown and reconstruction operation, the SL  may expose the reconstructed storage metadata  to storage clients . The storage clients  may, therefore, delegate crash-recovery and\/or data integrity to the SL , which may significantly simplify the storage clients  and\/or allow the storage clients  to operate more efficiently. For example, a file system storage client  may require crash-recovery and\/or data integrity services for some of its metadata, such as I-node tables, file allocation tables, and so on. The storage client  may have to implement these services itself, which may impose significant overhead and\/or complexity on the storage client . The storage client  may be relieved from this overhead by delegating crash recovery and\/or data integrity to the SL . As described above, the SL  stores data in a sequential, log-based format. As such, in the event of an invalid shutdown, the SL  is capable of reconstructing the storage metadata  and\/or identifying the \u201ccurrent\u201d version of data using the sequential, log-based formatted data on the non-volatile storage device . The SL  provides access to the reconstructed storage metadata  and\/or data via the SL interface . Accordingly, after an invalid shutdown, a file system storage client  may access crash-recovered file system metadata and\/or may ensure the integrity of file data accessed through the SL .","The logical address space  may be \u201csparse\u201d meaning the logical address space  is large enough that allocated\/assigned logical identifiers are non-contiguous and separated by sections of one or more unallocated\/unassigned addresses, and, as such, may comprise a logical capacity that exceeds the physical storage capacity of the non-volatile storage device . Accordingly, the logical address space  may be defined independent of the non-volatile storage device ; the logical address space  may present a larger address space than the physical storage capacity of the non-volatile storage device , and may present different storage location partitions and\/or block sizes than provided by the non-volatile storage device , and so on. Associations between the logical address space  and the non-volatile storage  are managed by the SL  (using the storage metadata ). Storage clients  may leverage the SL interface , as opposed to a more limited block-storage layer and\/or the other storage interface provided by a particular non-volatile storage device .","In some embodiments, the logical address space  may be very large, comprising a 64-bit address space referenced by 64-bit logical identifiers (LIDs). Each 64-bit logical identifier in the logical address space  (e.g., 64-bit address) references a respective virtual storage location. As used herein, a virtual storage location refers to a block of logical storage capacity (e.g., an allocation block). The SL  may be configured to implement arbitrarily sized virtual storage locations; typical sizes range from 512 to 4086 bytes (or even 8 kb to 16 kb depending on the needs of the storage clients ); the disclosure, however, is not limited in this regard. Since the logical address space  (and the virtual storage locations therein) is independent of the physical storage capacity and\/or storage partitioning of the non-volatile storage device , the logical address space  may be tailored to the requirements of the storage clients .","The SL  may manage allocations within the logical address space using storage metadata . In some embodiments, the SL  maintains storage metadata  that tracks allocations of the logical address space  using a forward index. The SL  may allocate ranges within the logical address space  for use by particular storage clients . Logical identifiers may be allocated for a particular storage client  to persist a storage entity. As used herein, a storage entity refers to any data or data structure in the logical address space  that is capable of being persisted to the non-volatile storage device ; accordingly, a storage entity may include, but is not limited to: file system objects (e.g., files, streams, I-nodes, etc.), a database primitive (e.g., database table, extent, or the like), streams, persistent memory space, memory mapped files, or the like. A storage entity may also be referred to as a Virtual Storage Unit (VSU). A file system object refers to any data structure used by a file system including, but not limited to: a file, a stream, file attributes, file index, volume index, node table, or the like.","As described above, allocating a logical identifier refers to reserving a logical identifier for a particular use or storage client. A logical identifier may refer to a set or range of the logical address space  (e.g., a set or range of virtual storage locations). The logical capacity of an allocated logical identifier may be determined by the size of the virtual storage locations of the logical address space . As described above, the logical address space  may be configured to present virtual storage locations of any pre-determined size. The size of the virtual storage locations may be configured by one or more storage clients , the SL , or the like.","An allocated logical identifier, however, may not necessarily be associated with and\/or assigned to physical storage locations on the non-volatile storage device  until required. In some embodiments, the SL  allocates logical identifiers comprising large, contiguous ranges in the logical address space . The availability of large, contiguous ranges in the logical address space is enabled by the large address space (e.g., 64-bit address space) presented by the SL VSL . For example, a logical identifier allocated for a file may be associated by the SL  with an address range of 2^32 contiguous virtual storage locations in the logical address space  for data of the file. If the virtual storage locations (e.g., allocation blocks) are 512 bytes each, the allocated logical identifier may represent a logical capacity of two (2) terabytes. The physical storage capacity of the non-volatile storage device  may be smaller than two (2) terabytes and\/or may be sufficient to store only a small number of such files, such that if logical identifier allocations were to cause equivalent assignments in physical storage space, the VSL  would quickly exhaust the capacity of the non-volatile storage device . Advantageously, however, the SL  is configured to allocate large, contiguous ranges within the logical address space  and to defer assigning physical storage locations on the nonvolatile storage device  to the logical identifiers until necessary. Similarly, the SL  may support the use of \u201csparse\u201d allocated logical ranges. For example, a storage client  may request that a first data segment be persisted at the \u201chead\u201d of an allocated logical identifier and a second data segment be persisted at the \u201ctail\u201d of an allocated logical identifier. The SL  may assign only those physical storage locations on the non-volatile storage device  that are needed to persist the first and second data segments. The SL  may not assign or reserve physical storage locations on the non-volatile storage device  for allocated logical identifiers that are not being used to persist data.","The SL  maintains storage metadata  to track allocations in the logical address space and to track assignments between logical identifiers in the logical address space  and physical storage locations on the non-volatile storage media . In some embodiments, the SL  track both logical allocations and physical storage location assignments using a single metadata structure. Alternatively, or in addition, the SL  may be configured to track logical allocations in logical allocation metadata and to track assigned physical storage locations on the non-volatile storage media  using separate, physical reservation metadata.","Storage clients  may access the SL  via the SL interface . In some embodiments, storage clients  may delegate certain functions to the SL. For example, and as described above, storage clients  may leverage the sequential, log-based data format of the SL  to delegate crash recovery and\/or data integrity functions to the SL . In some embodiments, storage clients may also delegate allocations in the logical address space  and\/or physical storage reservations to the SL .","Typically, a storage client , such as a file system, tracks the logical addresses and\/or physical storage locations that are available for use. The logical storage locations available to the storage client  may be limited to the physical storage capacity of the underlying non-volatile storage device (or partition thereof). Accordingly, the storage client  may maintain a set of logical addresses that \u201cminors\u201d the physical storage locations of the non-volatile storage device. For example, and as shown in , a storage client  may identify one or more available logical block addresses (LBAs) for a new file. Since the LBAs map directly to physical storage locations in conventional implementations, the LBAs are unlikely to be contiguous; the availability of contiguous LBAs may depend upon the capacity of the underlying block storage device and\/or whether the device is \u201cfragmented.\u201d The storage client  then performs block-level operations to store the file through, inter alia, a block storage layer (e.g., a block-device interface). If the underlying storage device provides a one-to-one mapping between logical block address and physical storage locations, as with conventional storage devices, the block storage layer performs appropriate LBA-to-physical address translations and implements the requested storage operations. If, however, the underlying non-volatile storage device does not support one-to-one mappings (e.g., the underlying storage device is a sequential, or write-out-of-place device, such as a non-volatile storage device, in accordance with embodiments of this disclosure), another redundant set of translations is needed (e.g., a Flash Translation Layer, or other mapping). The redundant set of translations and the requirement that the storage client  maintain logical address allocations may represent a significant overhead for storage operations performed by the storage client  and may make allocating contiguous LBA ranges difficult or impossible without time-consuming \u201cdefragmentation\u201d operations.","In some embodiments, storage clients  delegate allocation functionality to the SL . Storage clients  may access the SL interface  to request logical ranges in the logical address space . The SL  tracks the allocation status of the logical address space  using the storage metadata . If the SL  determines that the requested logical address range is unallocated, the SL  allocates the requested logical address range for the storage client . If the requested range is allocated (or only a portion of the range is unallocated), the SL  may return an alternative range in the logical address space  and\/or may return a failure. In some embodiments, the SL  may return an alternative range in the logical address space  that includes contiguous range of logical addresses. Having a contiguous range of logical addresses often simplifies the management of the storage entity associated with this range of logical addresses. Since the SL  uses the storage metadata  to maintain associations between the logical address space  and physical storage locations on the non-volatile storage device , no redundant set of address translations is needed. Moreover, the SL  uses the storage metadata  to identify unallocated logical identifiers, which frees the storage client  from this overhead.","In some embodiments, the SL  makes allocations within the logical address space  as described above. The SL  may access an index comprising allocated logical address ranges (e.g., forward index of ) to identify unallocated logical identifiers, which are allocated to storage clients  upon request. For example, the SL  may maintain storage metadata  comprising a range-encoded tree data structure, as described above; entries in the tree may represent allocated logical identifiers in the logical address space , and \u201choles\u201d in the tree represent unallocated logical identifiers. Alternatively, or in addition, the SL  maintains an index of unallocated logical identifiers that can be allocated to storage clients (e.g., without searching a forward index).","In one embodiment, the SL  may comprise an ordered queue . The ordered queue  may receive both atomic storage requests (such as an atomic storage request  discussed below in connection with ) and non-atomic storage requests for the non-volatile storage device . In one configuration, the atomic and the non-atomic storage requests are processed based on an order of arrival at the ordered queue . The ordered queue  may simplify processing of storage requests and obviate the need, for example, for an inflight index  (disclosed below in connection with ) because storage requests do not potentially conflict with pending requests as all requests are processed in a specific order. Consequently, certain embodiments may include the ordered queue  and not the inflight index . In addition, embodiments that use the ordered queue  avoids potential problems that may be caused by interleaving of data packets, which may occur if multiple atomic requests are processed simultaneously. As will be explained below in connection with , if data packets for each atomic request are stored contiguously (without interleaving packets associated with other write requests), a single bit within each data packet may be utilized to identify whether an atomic write was successfully completed. Accordingly, in certain embodiments, the ordered queue  may provide significant advantages by mitigating the metadata stored on the storage media  in connection with atomic write operations.","In an alternative embodiment, the ordered queue  may process either atomic storage request or non-atomic storage requests but not both. As an additional alternative, there may be a first ordered queue for atomic storage requests and a second ordered queue for non-atomic storage requests.",{"@attributes":{"id":"p-0189","num":"0188"},"figref":"FIG. 5","b":["504","106","504","504","430","404","118"]},"In the  example, the data structure  is implemented as a range-encoded B-tree. The disclosure is not limited in this regard, however; the forward index  may be implemented using a suitable data structure including, but not limited to: a tree, a B-tree, a range-encoded B-tree, a radix tree, a map, a content addressable map (CAM), a table, a hash table, or other suitable data structure (or combination of data structures).","The forward index  comprises a plurality of entries  (entries A-G), each representing one or more logical identifiers in the logical address space. For example, the entry B references logical identifiers  (LIDs -). Data may be stored sequentially or \u201cout-of-place\u201d on the non-volatile storage device and, as such, there may be no correspondence between logical identifiers and the physical storage locations. The forward index  maintains assignments between allocated logical identifiers and physical storage locations (e.g., using physical storage location references ). For example, the reference B assigns the logical identifiers  (LIDs -) to one or more physical storage locations of the non-volatile storage device. In some embodiments, the references  comprise a physical address on the non-volatile storage device. Alternatively, or in addition, the references  may correspond to a secondary datastructure (e.g., a reverse index), or the like. The references  may be updated in response to changes to the physical storage location of data (e.g., due to grooming operations, data refresh, modification, overwrite, or the like).","In some embodiments, one or more of the entries  may represent logical identifiers that have been allocated to a storage client, but have not been assigned to any particular physical storage locations (e.g., the storage client has not caused data to be written to the logical identifiers). The physical storage location reference  of an unassigned entry  may be marked as \u201cnull\u201d or not assigned.","The entries  are arranged into a tree data structure by the edges . In some embodiments, the entries  are indexed by logical identifier, which provides for fast and efficient entry  lookup. In the  example, the entries  are arranged in logical identifier order such that the entry C references the \u201clowest\u201d logical identifiers and G references the \u201clargest\u201d logical identifiers. Particular entries  are accessed by traversing the edges  of the forward index . In some embodiments, the forward index  is balanced, such that all leaf entries  are of a similar depth within the tree.","For clarity, the  example depicts entries  comprising numeric logical identifiers. However, the disclosure is not limited in this regard, and one of skill in the art will recognize that the entries  could comprise any suitable logical identifier representation, including, but not limited to: alpha-numerical characters, hexadecimal characters, binary values, text identifiers, hash codes, or the like.","The entries  of the index  may reference logical identifiers of variable size and\/or length; a single entry  may reference a plurality of logical identifiers (e.g., a set of logical identifiers, a logical identifier range, a noncontiguous set of logical identifiers, or the like). For example, the entry B represents a contiguous range of logical identifiers -. Other entries of the index  may represent a noncontiguous set of logical identifiers; entry G represents logical identifiers - and -, each assigned to respective physical storage locations by respective references G and G. The forward index  may represent logical identifiers using any suitable technique; for example, the entry D references logical identifier  and length , which corresponds to a range of logical identifiers -.","In some embodiments, the entries  comprise and\/or reference metadata , which may comprise metadata pertaining to the logical identifiers, such as age, size, logical identifier attributes (e.g., client identifier, data identifier, file name, group identifier), the underlying physical storage location(s), or the like. The metadata  may be indexed by logical identifier (through association with the respective entries ) and, as such, the metadata  may remain associated with entry  regardless of changes to the location of the underlying physical storage locations of the data.","The index  may be used to efficiently determine whether the non-volatile storage device comprises a particular logical identifier. In one example, a storage client may request allocation of a particular logical identifier. If the index  comprises and entry  that includes the requested logical identifiers, the logical identifier(s) associated with the request may be identified as being already allocated. If the logical identifiers are not in the index, they may be allocated to the requester by creating a new entry  in the index . In another example, a storage client requests data of a particular logical identifier. The physical storage location of the data is determined by accessing the reference  to the physical storage location of the entry  comprising the logical identifier. In another example, a client modifies data pertaining to a logical identifier. In another example, a storage client modifies existing data of a particular logical identifier. The modified data is written sequentially to a new physical storage location on the non-volatile storage device, and the physical storage location reference  of the entry  in the index  is updated to reference the physical storage location of the new data. The obsolete data may be marked as invalid for reclamation in a grooming operation.","The forward index  of  maintains a logical address space and, as such, is indexed by logical identifier. As discussed above, entries  in index  may comprise references  to physical storage locations on a non-volatile storage device. In some embodiments, the references  may comprise physical addresses (or address ranges) of the physical storage locations. Alternatively, or in addition, the references  may be indirect (e.g., reference a secondary datastructure, such as a reverse index).",{"@attributes":{"id":"p-0199","num":"0198"},"figref":["FIG. 6","FIG. 6"],"b":["622","622","622","622","504","622","504","622","620","622","624","626","628","626","410","630","632","634","632","636","622","626"]},"The reverse index  may be organized according to the configuration and\/or layout of a particular non-volatile storage device. Accordingly, the reverse index  may be arranged by storage divisions (e.g., erase blocks), physical storage locations (e.g., pages), logical storage locations, or the like. In the  example, the reverse index  is arranged into a plurality of erase blocks (, , and ), each comprising a plurality of physical storage locations (e.g., pages, logical pages, or the like).","The entry  comprises metadata pertaining to the physical storage location(s) comprising data of the entry F of . The entry  indicates that the physical storage location is within erase block n . Erase block n  is preceded by erase block n\u22121  and followed by erase block n+1 . (The contents of erase blocks n\u22121 and n+1 are not shown).","The entry ID  may be an address, a virtual link, or other data to associate entries in the reverse index  with entries in the forward index  (or other storage metadata). The physical address  indicates a physical address on the non-volatile storage device (e.g., non-volatile storage media ). The data length  associated with the physical address  identifies a length of the data stored at the physical address . Together, the physical address  and data length  may be referred to as destination parameters .","The logical identifier  and data length  may be referred to as source parameters . The logical identifier  associates the entry with a logical identifier of the logical address space. The logical identifier  may be used to associate an entry in the reverse index  with an entry  of the forward index . The data length  refers to the length of the data in the logical address space (e.g., from the perspective of the storage client). The source parameter  data length  may be different from the source parameter  data length  due to, inter alia, data compression, header overhead, encryption overhead, or the like. In the  example, the data associated with the entry  is highly compressible and was compressed from 64 blocks in the logical address space to 1 block on the non-volatile storage device.","The valid tag  indicates whether the data mapped to the entry  is valid. In this case, the data associated with the entry  is valid and is depicted in  as a \u201cY\u201d in the row of the entry . As used herein, valid data refers to data that is up-to-date and has not been deleted and\/or made obsolete (overwritten or modified). The reverse index  may track the validity status of each physical storage location of the non-volatile storage device. The forward index  may comprise entries corresponding to valid data only. In the  example, entry \u201cQ\u201d  indicates that data associated with the entry  is invalid. Note that the forward index  does not include logical addresses associated with entry Q . The entry Q  may correspond to an obsolete version of the data of entry C (overwritten by data now stored at entry \u201cC\u201d).","The reverse index  may maintain entries for invalid data so that valid and invalid data can be quickly distinguished for storage recovery (e.g., grooming). In some embodiments, the forward index  and\/or the reverse index  may track dirty and clean data in a similar manner to distinguish dirty data from clean data when operating as a cache.","In some embodiments, the reverse index  may omit the source parameters . For example, if the source parameters  are stored with the data, possibly in a header of the stored data, the reverse index  may identify a logical address indirectly by including a physical address  associated with the data and the source parameters  could be identified from the stored data.","The reverse index  may also include other miscellaneous data , such as a file name, object name, source data, storage client, security flags, atomicity flag, transaction identifier, or the like. One of skill in the art will recognize other information useful in a reverse index . While physical addresses  are depicted in the reverse index , in other embodiments, physical addresses , or other destination parameters , may be included in other locations, such as in the forward index , an intermediate table or data structure, or the like.","The reverse index  may be arranged by erase block or erase region (or other storage division) so that traversing a section of the index allows a groomer to identify valid data in a particular storage division (e.g., erase block ) and to quantify an amount of valid data, or conversely invalid data, therein. The groomer may select storage divisions for recovery based, in part, on the amount of valid and\/or invalid data in each division.","In some embodiments, the groomer and\/or garbage collection processes are restricted to operating within certain portions of the physical storage space. For example, portions of the storage metadata  may be periodically persisted on the non-volatile storage device , and the garbage collector and\/or groomer may be limited to operating on the physical storage locations corresponding to the persisted storage metadata . In some embodiments, storage metadata  is persisted by relative age (e.g., sequence), with older portions being persisted, while more current portions are retained in volatile memory. Accordingly, the groomer and\/or garbage collection systems may be restricted to operating in older portions of the physical address space and, as such, are less likely to affect data of an in process atomic storage request. Therefore, in some embodiments, the garbage collection system and\/or groomer may continue to operate while an atomic storage request is serviced. Alternatively, or in addition, the garbage collection system and\/or groomer may access the storage metadata and\/or inflight index (discussed below) to prevent interference with atomic storage operations.","Referring back to , the non-volatile storage device  may be configured to store data on the non-volatile storage media  in a sequential, log-based format. The contents of the non-volatile storage device may, therefore, comprise an ordered \u201cevent log\u201d of storage operations on the non-volatile storage media . The sequential ordering of storage operations may be maintained by appending data at an append point within the physical storage space of the non-volatile storage device . Alternatively, or in addition, sequence information may be maintained through persistent data stored on the non-volatile storage device . For example, each storage division on the storage device may comprise a respective indicator (e.g., timestamp, sequence number, or other indicator), to indicate an order of the storage division within the event log.",{"@attributes":{"id":"p-0211","num":"0210"},"figref":"FIG. 7A","b":["700","700","712"]},"Each physical storage location may be assigned a respective physical address ranging from zero (0) to N. Data is stored sequentially at an append point . The append point  moves sequentially through the physical storage space . After storing data at the append point , the append point advances sequentially to the next available physical storage location. As used herein, an available physical storage location refers to a physical storage location that has been initialized and is ready to store data (e.g., has been erased). Some non-volatile storage media, such as non-volatile storage media , can only be programmed once after erasure. Accordingly, as used herein, an available physical storage location may refer to a storage location that is in an initialized (or erased) state. If the next storage division in the sequence is unavailable (e.g., comprises valid data, has not been erased or initialized, is out of service, etc.), the append point  selects the next available physical storage location. In the  example, after storing data on the physical storage location , the append point  may skip the unavailable storage division , and continue at the next available location (e.g., physical storage location  of storage division ).","After storing data on the \u201clast\u201d storage location (e.g., storage location N  of storage division ), the append point  wraps back to the first division  (or the next available storage division if  is unavailable). Accordingly, the append point  may treat the physical address space as a loop or cycle. As depicted in , the append point  sequentially cycles through the storage locations of the non-volatile storage device.","As discussed above, storing data in a sequential, log-based format may comprise persisting metadata on the non-volatile storage device  that describes the data stored thereon. The persistent metadata may comprise the logical identifier associated with the data and\/or provide sequence information pertaining to the sequential ordering of storage operations performed on the non-volatile storage device. Accordingly, the sequential, log-based data may represent an \u201cevent log\u201d that tracks the sequence of storage operations performed on the non-volatile storage device .",{"@attributes":{"id":"p-0215","num":"0214"},"figref":"FIG. 8","b":["810","810","812","812","812","812","812"]},"The packet  includes persistent metadata  that is stored on the non-volatile storage device. In some embodiments, the persistent metadata  is stored with the data segment  (e.g., as a packet header, footer, or the like). The persistent metadata  may include a logical identifier indicator  that identifies the logical identifier(s) to which the data segment  pertains. The logical identifier indicator  may be used to reconstruct the storage metadata, such as the forward index (e.g., forward index ) and\/or reverse index (e.g., reverse index ). The persistent metadata  may further comprise one or more metadata flags . As discussed below, the flags  may be used to support atomic storage operations, transactions, or the like.","In some embodiments, the packet  is associated with a sequence indicator . The sequence indicator  may be persisted on the storage location (e.g., page) with the packet  and\/or on the storage division (e.g., erase block) of the packet . Alternatively, the sequence indicator  may be persisted in a separate storage location. In some embodiments, a sequence indicator is applied when a storage division is made available for use (e.g., when erased, when the first or last storage location is programmed, or the like). The sequence indicator  may be used to determine the temporal sequential ordering of storage operations on the non-volatile storage device.","Referring back to , the sequential, log-based format disclosed herein enables the SL  to reconstruct the storage metadata , as well as other data, in the event of an invalid shutdown (or other failure condition).","The storage metadata  (e.g., the forward index  of ) maintains assignments between logical identifiers and physical storage locations on the non-volatile storage device. Accordingly, there may be no pre-determined mapping between logical identifiers and physical storage locations; data of a logical identifier may be stored on any arbitrary physical storage location of the non-volatile storage device. Moreover, since data is stored in sequentially and in a log-based format, when data is overwritten or modified, previous versions of the data may be retained (until recovered in a grooming operation).","Referring back to , the letters A-L represent data stored on physical storage locations of a non-volatile storage device. Data A is initially stored at a physical storage location . When the data A is persisted at location , the physical storage location reference  of the corresponding forward index entry  is updated to reference the physical storage location . In addition, a reverse index entry  may be updated to indicate that the physical storage location  comprises valid data and\/or to associate the physical storage location  with logical identifiers - (not shown). (For clarity, other portions of the forward index and\/or reverse index are omitted from .)","When the data A is modified and\/or overwritten, the updated data may not be stored in the original physical storage location . Instead, the updated data A\u2032 is stored sequentially (out-of-place) at storage location  (at the current position of the append point ). The storage metadata is updated accordingly. The forward index entry  is updated to associate the logical identifiers - with the physical storage location  comprising A\u2032. The entry  of the reverse index is updated to mark physical storage location  as invalid and to indicate that the physical storage location  comprises valid data. Marking the physical storage location  as invalid may allow the storage location  to be reclaimed in a grooming and\/or garbage collection operation, as described above.","The data A\u2032 is further modified and\/or overwritten with data A\u2033. The updated data A\u2033 is stored at the current append point  (physical storage location ). The storage metadata is updated, as described above: the forward index entry  is updated to associate the entry with the physical storage location , and a reverse index entry  is updated to indicate that the physical storage address  comprises valid data (and that the physical address  comprises invalid data).","The \u201cobsolete\u201d versions A and A\u2032 may be retained on the non-volatile storage device until the corresponding physical storage locations  and\/or  are reclaimed (e.g., erased) in a grooming operation.","The data A, A\u2032, and A\u2033 may be stored in the sequential, log-based format (an \u201cevent-log\u201d format) described above. Storage metadata, such as the forward index  of  may be reconstructed from the sequential, log-based formatted data. The logical identifier indicator of the persistent metadata stored with data A, A\u2032, and\/or A\u2033 may indicate that the data stored at the physical storage locations , , and  corresponds to logical identifiers -. A sequence indicator of the data A, A\u2032, and\/or A\u2033 (and\/or the position of the append point ) indicates that the physical storage location  comprises the current, valid copy of the data. Therefore, the forward index entry  may be reconstructed to associate the logical identifiers - with the physical storage location . In addition, the reverse index entries , , and\/or  may be reconstructed to indicate that the physical storage locations  and  comprise invalid data, and that the physical storage location  comprises valid data.","The storage metadata and sequential, log-based data storage disclosed herein may be leveraged to implement efficient atomic operations. Many applications (e.g., user applications ) rely on atomic storage operations. Atomic storage operations may be limited to a relatively small, fixed-sized data (e.g., a single sector within a block storage device). Atomic storage operations may require a \u201ccopy on write\u201d operation to ensure consistency (e.g., to allow the atomic storage operation to be rolled back, if necessary), which may significantly impact the performance of the atomic storage operations. Moreover, support for atomic storage operations may typically be provided by a layer that maintains its own, separate metadata pertaining to atomic storage operations, resulting in duplicative effort, increased overhead, and\/or decreased performance.","In some embodiments, the storage metadata  is leveraged and\/or extended to provide efficient atomic storage operations through the SL interface . Consistency of the storage metadata  may be maintained by deferring updates until the one or more storage operations comprising the atomic storage request are complete. Metadata pertaining to storage operations that are \u201cin process\u201d (e.g., ongoing operations that are not yet complete) may be maintained in separate \u201cinflight\u201d metadata, described below. Accordingly, in certain embodiments, the state of the storage metadata  is maintained until the atomic storage operation successfully completes, obviating the need for extensive post-failure \u201croll back\u201d operations.","The sequential, log-based data format provides an \u201cevent log\u201d of storage operations on the non-volatile storage device . The sequential, log-based storage retains multiple copies of data (e.g., previous versions of the data) on the non-volatile storage device . The previous versions may be retained until the data is marked as invalid in the storage metadata  and\/or the data is recovered in a grooming operation.","As discussed above, the storage metadata  may be reconstructed from the sequential, log-based data stored on the non-volatile storage device . The up-to-date version of data is identified based upon the location of the append point and\/or sequence indicators associated with the data. During reconstruction, data pertaining to an incomplete atomic storage request may be identified (and discarded) using persistent metadata associated with the data, as depicted in .","In some embodiments, the SL  provides atomic storage operations by storing data in a sequential, log-based format, storing data pertaining to an atomic storage request together with persistent metadata on the non-volatile storage device, and\/or acknowledging completion of the atomic storage request when the one or more storage operations are complete. The logical identifiers of the atomic storage request may be noncontiguous. Completion of a storage request may comprise transferring data to a write buffer, a protection domain, a powercut safe domain, and\/or storing the data on a non-volatile storage device . The persistent metadata may be stored with data of the atomic storage request in a single storage operation. The storage metadata  may be preserved while an atomic storage operation is in process by deferring updates to the storage metadata  until the atomic storage operation is complete. Metadata pertaining to an atomic storage operation that is in progress may be maintained in a separate index (inflight index). In some embodiments, a first persistent metadata flag identifies data pertaining to the atomic storage request, and a first persistent metadata flag in a second state indicates completion of the atomic storage request. An incomplete atomic storage request is identified when the non-volatile storage device  comprises the first flag, but not the second flag. Alternatively, the persistent metadata flags may comprise an identifier (e.g., transaction or atomic storage request identifier). Storage operations of an atomic storage request may be completed despite invalid shutdown conditions, such as a failure of a host computing device , power loss or the like. Assignments between logical identifiers and physical storage locations may be preserved until the atomic storage operation completes. Metadata pertaining to in-process atomic storage operations may be maintained in an inflight index, which may be separate from other storage metadata. The inflight index may be accessed to identify read and\/or write hazards pertaining to the atomic storage request.",{"@attributes":{"id":"p-0230","num":"0229"},"figref":"FIG. 9A","b":["434","904","950","504","904","904","930"]},"An atomic storage request  may comprise a request to store data atomically to a set of one or more noncontiguous, contiguous, or combination of contiguous and noncontiguous logical identifiers. In the  example, the atomic storage request  comprises atomically storing to two noncontiguous logical identifier ranges (- and -), portions of which overwrite existing data in the forward index . The existing data is referenced by entries B and E of the forward index . The entries B and E may comprise references to physical storage locations of the data and\/or may reference the physical storage locations  and  of the data using the entries  and  of a reverse index  (for clarity, only a portion of the reverse index  and reverse index entries are depicted). As illustrated in , the atomic storage request expands the logical identifier range of - to -. Servicing the atomic storage request may, therefore, comprise allocating additional logical identifiers in the logical address space. The new logical identifiers may be allocated in the forward index  (in an unassigned entry (not shown)), or, as depicted in  in the inflight datastructure .","As discussed above, the storage metadata  may be updated as data is stored on the non-volatile storage device . The updating may comprise updating one or more entries in the forward index  to assign logical identifiers to updated physical storage locations. The updating may further comprise updating the reverse index  to invalidate previous versions of overwritten\/modified data and to track the physical storage locations of the updated data. This updating changes the state of the storage metadata , which may make it difficult to \u201croll back\u201d a failed atomic storage operation. Moreover, the updates may cause previous versions of the data to be removed from the non-volatile storage device  by a groomer, garbage collection system, or other process, such as cache manager or the like; as discussed above, storage locations comprising invalid data as indicated by absence from the forward index  and\/or marking the data as invalid in the reverse index , may be removed. In one embodiment, these problems may be avoided or mitigated by prohibiting the groomer from accessing certain logical erase blocks, such as a logical erase block in which the final packet of an atomic write operation is situated. Removal of the previous version of data overwritten by a data of an atomic storage request may make it difficult or impossible to roll back the atomic storage request in the event of a failure.","Use of the inflight index\/datastructure  may provide additional advantages over tracking in-process storage operations using the forward index  alone. For example, as a storage request is performed, the inflight datastructure  may be updated via an \u201cexclusive\u201d or \u201clocked\u201d operation. If these updates were performed in the forward index  (or other shared metadata), the lock may preclude other storage requests from being completed. Isolating these updates in a separate datastructure may \u201cfree up\u201d the storage metadata to service other, potentially concurrent, requests. In addition, the inflight index  may track in-process operations that may be rolled back in the event of failure (e.g., atomic storage operations). Furthermore, isolating the in-process metadata within the inflight index  allows the other metadata  to be maintained in a consistent state (until the storage request is fully complete), and may allow for more efficient rollback of failed and\/or incomplete storage requests.","In some embodiments, the state of the storage metadata  is preserved until completion of an atomic storage request. The progress of an atomic storage request (e.g., request ) may be tracked in a separate datastructure, such as an inflight index . Modifications to the inflight index  may be applied to the storage metadata (forward index  and\/or reverse index ) upon completion of the atomic storage request (and\/or upon reaching a point after which the atomic storage operation is guaranteed to complete).","The inflight index  depicted in  may comprise a separate datastructure from the forward index . The disclosure is not limited in this regard; in other embodiments, the inflight index  may be implemented within the forward index  (using special-purpose entries in the index ), as metadata entries of the forward index entries, or the like.","The inflight index  may comprise any suitable datastructure (e.g., tree, B-tree, radix tree, map, etc.). In the  example, the inflight index  is implemented using a range encoded tree. The entries  in the inflight index  may be indexed by logical identifier, as described above.","Entries B and E are added to the inflight index  in response to the atomic storage request . The entries B and E identify logical identifiers pertaining to the atomic storage operation. As illustrated in , the atomic storage request  comprises two noncontiguous logical identifier ranges. The inflight index  comprises respective entries B and E for each logical identifier range. The disclosure is not limited in this regard, however, and could be adapted to generate entries for each logical identifier, for sub-ranges of logical identifiers in the request, and so on.","The inflight index  is updated in response to completion of one or more portions of the atomic storage request .  depicts the inflight index  after storing a first portion of the data of the atomic storage request . The entry E indicates that the data corresponding to logical identifiers - has been successfully stored at physical storage locations -. Alternatively, or in addition, the physical storage locations may be referenced using a secondary datastructure, such as a separate reverse index or the like. The forward index  and reverse index  remain unchanged.","The inflight index  is further updated in response to completion of other portions of the atomic storage request .  depicts the inflight index  as the atomic storage request is completed. The inflight index entry B is updated to assign physical storage locations to the logical identifiers -. The forward index  and\/or reverse index  remain unchanged.","The storage metadata  may be updated in response to detecting completion of the atomic storage request  and\/or determining that the atomic storage request  will successfully complete (e.g., data of the atomic storage request has been received at a write data pipeline or write buffer of the non-volatile storage device ).",{"@attributes":{"id":"p-0241","num":"0240"},"figref":["FIG. 9D","FIG. 9D"],"b":["434","901","906","906","950","922","924","925","926","927","905","905","904","901","926","927","950","072","83","072","120","904"]},"In some embodiments, the inflight index  is used to avoid write and\/or read hazards. As shown in , a storage request  pertaining to a logical identifier of an atomic storage request may be received after or concurrent with the atomic storage request , but before completion of the atomic storage request . For example, the storage request may pertain to logical identifiers - that are to be overwritten by the atomic storage request . If the request  is to read data of -, the request may pose a read hazard (e.g., read before write), since reading the physical storage location  of the entry B will return obsolete data. The read hazard may be identified in the inflight index , which indicates that the target of the request  is in the process of being modified. The request  may be delayed until completion or failure of the atomic storage request  (and removal of the in-process entry B from the inflight index ). A write hazard may be detected and addressed similarly.","The inflight index  may also be used to prevent a subsequent storage request from writing data to the logical identifiers of the atomic storage request. For example, the entry B of the inflight index  may be accessed to prevent another storage client from allocating logical identifiers -.","Referring back to , data may be stored on the non-volatile storage device  in an \u201cevent log;\u201d data is stored in a sequential log-based format, wherein data is appended to the non-volatile storage media  at an append point  which moves sequentially (and cyclically) through the physical storage space of the non-volatile storage device . In the event of an invalid shutdown, the storage metadata  may be reconstructed from the contents of the non-volatile storage device . This reconstruction is enabled by the sequential, log-based format of the data; data is stored in conjunction with persistent metadata that associates the data with one or more logical identifiers from which a forward and\/or reverse index may be derived. Up to date, valid data may be distinguished from obsolete or invalid data based upon the ordering of storage operations (e.g., relative to the position of the append point and\/or sequence identifiers associated with the data).","Partially completed atomic storage operations should be identifiable during reconstruction. Otherwise, data pertaining to a failed atomic storage operation may appear to be the most up-to-date version of data. This potential issue is illustrated in . Data A, B, C are stored on physical storage locations , , and  respectively. Other data D is subsequently stored within the physical storage space of a non-volatile storage device . The data A, B, and C is modified (overwritten) in a subsequent atomic storage request. The atomic storage request stores a portion of the atomic storage request, the updated data A\u2032, at physical storage location  and updated B\u2032 at , but a failure occurs (with the append point  at physical storage location ) before the atomic storage operation is completed (before writing C\u2032 at physical storage location ). The failure may require the storage metadata (e.g., forward index and\/or reverse index through power loss or data corruption) to be reconstructed.","As discussed above, the forward index may be reconstructed from the \u201cevent log\u201d of sequential log-based data on the non-volatile storage device. The event log is accessed from the last known append point , which corresponds to the most recent operations in the log. In some embodiments, the append point  location is periodically stored on a non-volatile storage device. Alternatively, or in addition, the append point  may be determined using sequence indicators associated with storage divisions (e.g., erase blocks) of the non-volatile storage device. The metadata is reconstructed by traversing the event log in a pre-determined order (e.g., from storage operation performed furthest in the past to the most recent storage operations (tail to head) or from the most recent storage operations to older storage operations (head to tail)).","As discussed above, data is stored on the non-volatile storage device  in a sequential log-based format, in which the data is stored with persistent metadata. , discussed above, illustrates an exemplary sequential log-based data format  comprising a data segment  and persistent metadata . The persistent metadata  may include a logical identifier indicator  that identifies the logical identifier(s) to which the data segment  is assigned. A sequence indicator  (included as part of the data format , on the same storage division (e.g., erase block), or the like), may be used to determine the relative order of the data  in the event log.","Referring back to , based on the event log (the sequential log-based data stored on the non-volatile storage device ), the data A\u2032 at  and B\u2032  of the failed atomic storage request may appear to comprise the most up-to-date versions of the data A and B (rendering obsolete the previous versions(s) of A at , and B at ). However, the atomic storage request should have been rolled back to preserve the original data A, B, and C. If the failed atomic storage request is not identified and reconciled, this may result in reconstructing invalid entries A and B in the forward index  that associate the A and B logical identifiers with data of the failed atomic storage request (e.g. storage locations  and\/or ). The reverse index  may comprise entries  and  that improperly invalidate A data at  and B data at , and entries  and  that improperly indicate that the data of the failed atomic storage request at  and  is valid.","In some embodiments, persistent indicators stored on the non-volatile media are used to track in-process storage requests on the non-volatile storage device and\/or to account for loss of storage metadata. As used herein, a persistent indicator refers to an indicator that is stored (persisted) on the media of the non-volatile storage device with the data to which the indicator pertains. In some embodiments, the persistent indicators are persisted with the data (e.g., as a packet header associated with the data or the like). The persistent indicators are preferably stored with the data in a single storage operation and\/or in the smallest write unit supported by the non-volatile storage device . Accordingly, persistent storage indicators will be available when the storage metadata is reconstructed from the contents of the non-volatile storage device. The persistent indicators may identify incomplete and\/or failed atomic storage requests despite an invalid shutdown and\/or loss of storage metadata .","Referring back to , in some embodiments, the persistent metadata  of the sequential log-based data format is used to identify failed atomic storage requests. The persistent metadata flag(s)  may identify data  pertaining to an atomic storage request and\/or indicate completion of an atomic storage request. The persistent metadata flag(s)  may be stored with the data segment  in a single storage operation (e.g., single program operation, write buffer programming operation, or the like).","In some embodiments, data pertaining to an atomic storage operation is stored with a first persistent metadata flag in a first state  (e.g., a single bit \u201c1\u201d). Data that does not pertain to an atomic storage operation, or indicates completion of an atomic storage operation, is stored with the first persistent metadata flag in a second state  (e.g., a single bit \u201c0\u201d).  illustrate the progression of persistent metadata flags in an exemplary atomic storage operation.","In , the event log (sequential log-based data)  comprises data pertaining to logical identifiers - stored on respective physical storage locations -. The append point  is prepared to store data at the next, sequential physical storage location . A forward index  associates logical identifiers , , and  with respective physical storage locations as described above. The forward index  may include other entries, not shown here for clarity.","An atomic storage request  is received to store data in association with a noncontiguous set of logical identifiers (LIDs , , and ). In some embodiments, an atomic storage request  is formed by aggregating one or more sub-requests. The sub-requests may be combined into a single atomic storage request that is implemented as a whole.","In some embodiments, data of the atomic storage request  is stored contiguously in the event log , such that data that does not pertain to the atomic storage request  is not interleaved with data of the atomic storage request. The logical identifiers of the atomic storage request , however, may be noncontiguous, out of order, or the like. Accordingly, while data of the atomic storage request  is being stored on event log , other data that does not pertain to the request , such as garbage collection bypass data, grooming data (e.g., data refresh), other data requests, and the like, may be suspended. In one embodiment, suspension is not required if write requests, including grooming, are processed utilizing the ordered queue .",{"@attributes":{"id":"p-0255","num":"0254"},"figref":["FIG. 11B","FIG. 11B"],"b":["1114","1150","1103","1101","4","6","1103","1150","4","6","26","27","1101"]},"The persistent metadata flag  stored with the data on physical storage locations  and  indicates that the physical storage locations  and  comprise data pertaining to an incomplete atomic storage operation because the first encountered persistent metadata flag  is a \u201c0\u201d rather than a \u201c1,\u201d reading in reverse sequence order (reading to the left from the append point , as illustrated in ). If the first persistent metadata flag  preceding the append point  is set to a \u201c1\u201d (as shown in ), this indicates that the atomic storage operation was successfully completed. The persistent metadata flag  may be stored with the data on the physical storage locations  and .","If a failure were to occur, the persistent metadata flags  are used, together with the contiguous placement of data for the atomic storage request , to identify data pertaining to the failed atomic storage request . As discussed above in conjunction with , storage metadata is reconstructed using the event log of sequential log-based data. When the event log  of  is traversed in reverse sequence order (e.g., right to left as shown in  or, in other words, from the tail to the head of the sequence), the first persistent metadata flag  will be a \u201c0,\u201d indicating that the data pertains to a failed atomic storage request. The data at storage location  may, therefore, be invalidated and may not result in reconstructing invalid storage metadata  as in the  example. The data may continue to be invalidated or ignored, until a \u201c1\u201d flag is encountered at physical storage location . As will be appreciated by one of skill in the art, this approach relies on data of the atomic storage request  being stored contiguously within the event log . If data comprising a \u201c1\u201d persistent metadata flag  were interleaved with the atomic storage data (before completion of the atomic storage request ), the data at  and\/or  could be misidentified as being valid (e.g., pertaining to a complete atomic storage request ).",{"@attributes":{"id":"p-0258","num":"0257"},"figref":"FIG. 11C","b":["1101","1101","1101","1104","4","6","8","26","27","28","4","6","8","1101"]},"If a failure were to occur subsequent to persisting the data at physical storage location , the storage metadata  could be correctly reconstructed. When traversing the event log  in reverse sequence (e.g., moving left from the append point), the first persistent metadata flag  encountered would be the \u201c1\u201d flag on the physical storage location , indicating that the data at physical storage locations  and  pertain to a successfully completed atomic storage request.","In some embodiments, the data of such an atomic storage request may be limited by storage boundaries of the non-volatile storage device (e.g., page boundaries, logical page boundaries, storage divisions, erase blocks, logical erase blocks, etc.). Alternatively, the size of the data for an atomic storage request may require that the atomic storage request wait until the append point is on a storage division with sufficient free space to fit the atomic storage request before reaching a logical erase block boundary. Accordingly, the size of an atomic storage request may be limited to a logical page size. Additionally, in some embodiments, atomic storage requests do not cross logical erase block boundaries.","In another example, the persistent metadata flag  may comprise an identifier, which may allow data to be interleaved with atomic storage requests and\/or allow atomic storage requests to be serviced concurrently.",{"@attributes":{"id":"p-0262","num":"0261"},"figref":"FIG. 12","b":["1203","1215","1203","1","2","1","4","5","9","2","6","7"]},"The ID_ persistent metadata flag  on physical storage locations  and  identifies data pertaining to the atomic storage operation ID that has not yet been completed. The persistent metadata flag  ID_ on the physical storage location  indicates successful completion of the atomic storage operation ID. Another persistent metadata flag  ID_ identifies data pertaining to a different, interleaved atomic storage operation. The persistent metadata flag  ID_ of physical storage location  indicates successful completion of the atomic storage request ID. Data that does not pertain to an atomic storage operation may comprise a \u201c1\u201d persistent metadata flag  or other, pre-determined identifier. When reconstructing storage metadata from the event log , if an atomic storage request identifier comprising a \u201c0\u201d flag (e.g., ID_) is encountered before (or without) encountering a completion persistent metadata flag  (e.g., ID_), all data associated with the persistent metadata flag  ID may be invalidated. By contrast, after encountering the ID_ flag, all data associated with the ID persistent metadata flag  may be identified as pertaining to a completed atomic storage request. Although the extended persistent metadata flags  of  may provide for more robust support for atomic storage operations, they may impose additional overhead.","Each logical erase block -comprises two or more physical erase blocks (e.g., blocks  -shown in ). A logical erase block boundary  separates each logical erase block -. The logical erase block boundary  may comprise a virtual or logical boundary (i.e., a virtual boundary) between each logical erase block -.","As illustrated in , two data packets -are stored in a first logical erase block and two different data packets -are stored in a second logical erase block . In the illustrated embodiment, all four of the data packets -are stored as a result of a single atomic storage request. As indicated above, the append point  indicates where additional data may be written to the storage media .","Each logical erase block -comprises two or more physical erase blocks (e.g., blocks  -shown in ). A logical erase block boundary  separates each logical erase block -. The logical erase block boundary  may comprise a virtual or logical boundary (i.e., a virtual boundary) between each logical erase block -","As illustrated in the embodiment of , each data packet -includes a header -. Each header -may comprise persistent metadata related to data  within each packet -. The data  may comprise user data to be stored on and potentially retrieved from the storage media  in response to requests by, for example, storage clients  (shown in ). In one embodiment, a header and its associated data  are both stored to the storage media  in a single write operation (i.e., as a single unit or collection of data).","In , a header of a first data packet is illustrated. The header may comprise persistent metadata including various flags -. For example, one or more bits of the header may comprise a data packet flag that, when set to a particular value, indicates when an associated data packet -comprises user data. The position and number of the bits for each data packet flag within the header may be varied within the scope of the disclosed subject matter. Also, in one embodiment, the data packet flags may be located in the same position (i.e., the same bit position) within each header -of each data packet -","The illustrated headers -also include either a first persistent metadata flag in a first state or the first persistent metadata flag in a second state . The first persistent metadata flag -may comprise a single bit within each header -. For example, the first persistent metadata flag in the first state may comprise a particular bit position (such as the 56th bit) within a header set to a high value (a \u201c1\u201d), while the first persistent metadata flag in the second state may comprise the same bit position in a different header set to a low value (a \u201c0\u201d). Alternatively, the first persistent metadata flag in the first state may comprise a particular bit position within the header set to a low value, while the first persistent metadata flag in the second state may comprise the same bit position in a different header set to a high value. In one embodiment, the first persistent metadata flag in the first or second state -may each comprise a pattern of multiple bits or separate and distinct bit positions. Use of a single bit within each packet -, when data packets -associated with an atomic storage request are stored contiguously, provides the advantage that a very small amount of data is used on the storage media  to indicate whether an atomic write operation failed or succeeded.","As illustrated in , each header of the first three data packets -comprises the first persistent metadata flag in the first state , while the last data packet comprises the first persistent metadata flag in the second state . In one embodiment, each of data packets -, except the last data packet , stored on the storage media  pursuant to an atomic storage request comprises the first persistent metadata flag in the first state . As illustrated, the last packet includes the first persistent metadata flag in the second state , which signals the end or completion of data written pursuant to an atomic write request. This embodiment is advantageous in that only one bit within each packet -is needed to signal whether an atomic storage request was completed successfully. The first persistent metadata flags in the first and second states -indicate not only that the data  of these packets -pertain to an atomic storage request, but also identify a beginning and end, or successful completion, of the data associated with the atomic storage request.","However, a problem may arise if the third and fourth data packets -of the second logical erase block are erased. Some background information may be helpful to understand this problem. For example, during a recovery or other process an event log  could be created to define a logical sequence of logical erase blocks -(e.g., from head to tail). This may be achieved through a scan of the erase blocks -and, in particular, through examination and processing of metadata and sequence indictors stored in the erase block headers -to form an event log . The logical sequence of erase blocks -and\/or event log  may be formulated before performing recovery following an invalid shutdown or a restart operation (such as a shutdown resulting from a power failure) using either a forward or reverse sequence scan of the logical erase blocks -stored on the media . After the logical sequence of erase blocks -and\/or event log  has been formulated, reverse sequence scanning the event log  or logical sequence of logical erase blocks -based on the event log  from the append point  (i.e., the tail) in reverse sequence toward the head or beginning of the log , in certain embodiments, is initiated to identify failed atomic requests. In such a case (if third and fourth data packets -of the second logical erase block are erased), the reverse sequence scanning from an append point  could erroneously identify the first and second data packets -as being associated with a failed atomic storage request because the first encountered packet does not include the first persistent metadata flag in the second state . Accordingly, in one embodiment, grooming or deletion of a logical erase block that includes an endpoint  is prohibited.","As used in this application, an endpoint  may comprise the point immediately after the last packet , which may be stored or identified in a volatile memory. Alternatively, the final or last packet of an atomic write operation may comprise the endpoint.","As an alternative to prohibiting grooming or deletion of a logical erase block that includes an endpoint , an incorrect determination that the first and second data packets -relate to a failed atomic storage request is avoided by reference to sequence indicators (such as the sequence indicators  illustrated in ). As noted above, the sequence indicators  identify or specify an ordered sequence of erase blocks -. In particular, in one embodiment, sequence indicators -of each erase block header -comprise monotonically increasing numbers spaced at regular intervals. In view of the foregoing, if, a sequence indicator for a next logical erase block in the event log , moving from left to right (from the head to the tail of logical chain of erase blocks, as specified by the event log ), is not a next sequence number in the sequence, then, for example, the SL  recognizes that prior logical erase block does not end with a failed atomic request, i.e., the first and second packets -do not comprise a part of a failed atomic write.",{"@attributes":{"id":"p-0274","num":"0273"},"figref":["FIG. 14","FIG. 14","FIG. 14","FIG. 14"],"b":["1402","1442","1488","1488","1488","1488","406","1410","1440","1410","1440","1410","1417","1410","1317","1402","1420","1410","1410","1404","1415","1423","1410","1402","1404","1434","1402"],"i":["a","b ","a ","c ","b","a","c ","a","c ","b","a","c ","a","c ","a","c "]},"As used in this application, restart recovery comprises the act of a system, apparatus, or computing device, commencing processing after an event that can cause the loss of data stored within volatile memory of the system, apparatus, or computing device, (e.g., a power loss, reset, etc.). Restart recovery may also comprise power cycle recovery, such as commencing processing after an invalid shutdown, hard reset, or disconnection or separation of the powered device from a power supply (such as physically disconnecting a power supply for the device).","In one embodiment, excluding from the index  may comprise bypassing each data packet -associated with the failed atomic storage request during a scan of a log-based structure (e.g., the event log  illustrated in  or the ordered sequence of logical erase blocks -specified by the log ) used to create the index . In another embodiment, excluding from the index  may further comprise removing each logical identifier  that maps to each data packet -associated with the failed atomic storage request from the index  created by way of a scan of the log-based structure. In yet another embodiment, excluding from the index  may further comprise erasing each data packet -associated with the failed atomic storage request from the storage media  by way of a storage space recovery operation (which will be explained further below). Of course, one or more of the foregoing embodiments may be combined or used with other embodiments for excluding the data packets -from the index .",{"@attributes":{"id":"p-0277","num":"0276"},"figref":["FIG. 15","FIG. 15","FIG. 14"],"b":["1588","1588","1588","1510","1510","1502","1540","1103","1534","1502","1520","1540","1540","1103","1520","1510","1510","1510","1540","1510","1540","1510","1417","1510","1510","1510","1510","1417"],"i":["a ","b","a ","d","e","f","i ","a","c ","a","c","a","b ","d","e","f","i ","d","e ","a ","f","i ","b ","i","a","d","e","f","h ","d","e","f","h ","a. "]},"Thereafter, a recovery grooming operation  may be initiated to transfer the valid data packets -(but not the invalid data packets -) from the first logical erase block to the third logical erase block . More specifically, the grooming operation , for example, may involve transfer of valid packets -from the first logical erase block to the third logical erase block with a newly assigned sequence number (e.g., a logical erase block immediately after the append point ), while data packets -, -associated with a failed atomic write are not transferred to the logical erase block with the newly assigned sequence number.","At this point, a brief background describing one technique for utilization of sequence numbers -may be useful. As noted above, a sequence number -may be assigned to each erase block -. The sequence numbers -may be stored in logical erase block headers -, as illustrated in , or at another location on the non-volatile solid-state storage media . The sequence numbers -are utilized to create an ordered sequence of the logical erase blocks -. The ordered sequence may be identified or specified by the log . The sequence numbers -for each logical erase block -, in one embodiment, are spaced at regular intervals. For example, a consecutive series of logical erase blocks -may be assigned the following sequence numbers: 1, 65, 129, 193, 257, 321, 385 and 449. When it is determined that a new logical erase block needs be to utilized for the storage of data, the new logical erase block may be assigned the next available sequence number -in the series of sequence numbers -. Accordingly, in such an embodiment, if the last sequence number assigned to a logical erase block is the sequence number , a newly assigned erase block may be assigned the sequence number . Of course, in alternative embodiments, spacing between the sequence numbers -may be at an interval other than 64 (such as 32) or at irregular or varying intervals. Also, the sequence numbers -may be assigned in the cyclic fashion such that when the highest sequence number is utilized (given the number of bits of metadata  allocated for the sequence numbers -), the lowest sequence number no longer in use may be assigned to a newly identified erase block ","In view of this background, as illustrated in , during the recovery grooming operation , which is intended to transfer the valid data packs -from the first logical erase block to the third logical erase block, a second power failure may occur resulting in a failure of the grooming operation . Accordingly, a technique for identification of such a failure would be helpful to prevent use of the invalid or partially written data -saved in the third logical erase block or confusion as to whether the data in the first logical erase block or the third logical erase block should be utilized.","One such technique involves assigning a subsequence number  (rather than a sequence number -to the logical erase block to which the valid data -will be or is intended to be transferred. As indicated above, in one embodiment, the sequence numbers -are spaced at regular intervals, such as at intervals of 64 or at intervals of 32, as illustrated in . For example, consecutive sequence numbers may increment the most significant bits -of a fixed size sequence number by a particular increment, while leaving the least significant bits -unchanged. The subsequence number  may be derived from a sequence number by incorporating the most significant bits of the sequence number from which the subsequence number  is derived and altering (such as incrementing or decrementing) the least significant bits of the sequence number . As illustrated in , the subsequence number  may incorporate the most significant bits of the first sequence number and increment the least significant bits of the first sequence number , to yield the subsequence number  (i.e., 1010001000001). By assigning the subsequence number  to the third logical erase block , the sequencing order of the erased blocks -is maintained because the subsequence number  is greater than the first sequence number from which the subsequence number  is derived, but the subsequence number  is less than a next sequence number . Accordingly, the subsequence number  maintains an ordered sequence among logical erase blocks -of the log-based structure (e.g., the log  illustrated in ) such that an ordered sequence of storage operations completed on the storage media  is preserved on the storage media .","It should also be noted that a subsequence number  may be derived in various ways from a sequence number . For example, a subsequence number  could decrement the most significant bits of the first sequence number from which the subsequence number  is derived and increment the least significant bits of the sequence number from which the subsequence number  is derived.","In due course, all of the data packets -, -of the first logical erase block will be erased, including erase block header , from the storage media  if the grooming operation  were completed successfully. However, erasure of the data packets -, -and the erase block header of the first logical erase block may not occur immediately if the grooming operation  is completed successfully. Moreover, if second power failure occurs during the grooming (e.g., transferring) of the valid data -from the first logical erase block to the third logical erase block , the data packets -in the third logical erase block could potentially be corrupt or incomplete.","Accordingly, during a power-on operation following the second power failure , a restart recovery process may be initiated. During the restart recovery process, the log  will be created to formulate an ordered sequence of the logical erase blocks -. During this process, it may be determined that the first logical erase block has been assigned the first sequence number and the third logical erase block has been assigned the subsequence number  derived from the first sequence number . As explained above, this may indicate that either the data of the first logical erase block was not erased or that a grooming operation was interrupted. In either case, the data packets -of the third logical erase block are potentially corrupted or incomplete and should not be relied on as being valid. As a result, the data packets -, erase block header , and any other data stored in the third logical erase block should be erased or scheduled for erasure and should be excluded from the index . (As indicated previously, the index  maps logical identifiers  to physical locations or addresses  and may comprise or be based on metadata  stored on the media .)","Thereafter, the append point  would be positioned immediately to the right of invalid data packet , as shown in . Reverse sequence scanning of the non-volatile storage media  from the append point  would be commenced and would identify data packets -of the first logical erase block and data packets -of the second logical erase block as comprising a portion of a failed atomic write operation as a result of the first power failure . The valid data packets -of first logical erase block will be groomed  to the third logical erase block without transferring the invalid data packets -to the third logical erase block . In one embodiment, when the valid data packets -are groomed  to the third logical erase block , the first persistent metadata flag for each of the valid data packets -is set to a second state ","In view of the foregoing, it should also be observed that excluding from the forward or logical index  during a restart recovery may comprise erasing each logical erase block -of the non-volatile solid-state storage media  comprising one or more data packets -, -associated with the failed atomic storage request and transferring data packets -(e.g., valid data packets) from the each logical erase block -to a different location or logical erase block on the storage media . Also, erasing each logical erase block during restart recovery may comprise assigning a subsequence number  to a destination logical erase block configured to store transferred data packets -(i.e., valid data -). Further, erasing each logical erase block -during a restart recovery process may comprise, in response to identifying a first logical erase block having a sequence number and a third logical erase block having a subsequence number , grooming  the first logical erase block and, as described above, excluding each data packet -of the first logical erase block associated with the failed atomic storage request from the index . Again, the invalid data packets -of the first logical erase block may immediately or eventually be erased from the media  after the grooming operation  is performed.","The recovery grooming operation  if completed before normal input-output operations commence, in one embodiment, avoids a scenario in which data packets -, -associated with a failed atomic write operation could be considered valid because those data packets are removed from the media  by the recovery grooming operation . The following example illustrates this point.","First, a failed atomic write operation commences and is interrupted, resulting in the invalid data packets -, -being stored on the storage media . Second, a power-on operation is performed and, through a scan, the event log  is formulated without engaging in the recovery grooming operation  such that the invalid data packets -, -are included in the event log  and forward index . Third, a second atomic write operation is commenced and successfully completed. Finally, a reverse-sequence scan from the append point  (which is positioned after the data packets associated with the second successful atomic write operation) is subsequently initiated to identify packets associated with a failed atomic write operation. In this scenario, the invalid packets -, -will not be identified and removed from the storage media . This is because the reverse sequence scanning from the append point  will encounter the packets associated with the second successful atomic write operation, and determine that the second atomic write operation was successfully completed. In certain embodiments, identifying the second successful atomic write operation may result in termination of the reverse sequence scanning and the invalid data packets -, -will not be identified as being associated with a failed atomic write operation. Accordingly, the invalid data packets -, -will not be removed, or otherwise excluded, from the forward index  or from the storage media .",{"@attributes":{"id":"p-0289","num":"0288"},"figref":["FIG. 16","FIG. 16"],"b":["1694","1694","413","412","430","1696","1694","1694","1696","1694","1696"],"i":["a","d ","a","d ","a","d "]},"A file descriptor parameter of the call  identifies the file to which the API call  relates using, for example, a file identification number. The IO_Vector parameter may identify one or more storage operations to be performed on contiguous or noncontiguous blocks of storage media, using various parameters such as the source address, length of the data, and a destination address for each storage operation. IO_Count may identify the number of storage operations encapsulated within the IO_Vector . The flag parameter may identify the type of storage operation to be performed, such as an atomic write, a trim or discard request, a delete request, a format request, a patterned write request of a specific pattern of bits, a write zero request, or an atomic write operation with verification request. The atomic write operation with verification request completes the atomic write operation and then verifies that the data of the request was successfully written to the storage media.","The ability to utilize a single call  to make changes to noncontiguous blocks of the storage media may minimize the number of calls that need to be sent in order to perform a set of operations. Also, a number of storage requests may be aggregated into a single API call  utilizing such a format. In addition, the use of a flag parameter provides flexibility such that the API call  may be utilized for various purposes, such as atomic writes, a trim or discard request, a delete request, a format request, a patterned write request, a write zero request, or an atomic write operation with verification request.",{"@attributes":{"id":"p-0292","num":"0291"},"figref":["FIG. 17","FIG. 4","FIG. 17","FIG. 17","FIG. 17"],"b":["1730","1702","1721","1730","430","401","412","1730","1702","1730","1702","1730","1702","1730","1739"]},"The SL  may include an ordered queue . The ordered queue  is analogous to the ordered queue  illustrated in . The ordered queue  may receive non-atomic and\/or atomic storage requests and process pending requests in a sequential fashion, such as in the order the requests are received at the queue . In addition, the SL  may include multiple ordered queues (not illustrated), such as an ordered queue for atomic storage requests and an ordered queue for non-atomic requests. As explained above, the ordered queue  may obviate the need for an inflight index  (disclosed above in connection with ) and may avoid potential problems associated with interleaving of packets associated with different atomic write operations.","The SL  may also comprise a storage module . The storage module enables storage of user data  and metadata (e.g., a first persistent metadata flag in a first state or a second state) -on the non-volatile storage media  of the non-volatile storage device . For example, the storage module  enables storage of user data  and associated persistent metadata in each packet stored on the non-volatile storage media . In one embodiment, the persistent metadata stored in each packet indicates that the data pertains to atomic storage request. As explained above, the persistent metadata may comprise a single bit within each data packet. Further, the storage module  may store data packets associated with a single atomic write request in different logical erase blocks -. Each logical erase block -may comprise two or more physical erase blocks (e.g., block  of ).","The SL  may further comprise an acknowledgment module  that transmits or records acknowledgment of completion of a non-atomic or atomic storage request. Acknowledgment module  may transmit acknowledgment asynchronously via a callback or other mechanism. Alternatively, an acknowledged atomic storage request  may be synchronous and may comprise returning from asynchronous function or method call. The acknowledgment module  may send acknowledgment after the data has actually been saved or when it is certain that the data of the request  will be saved, as will be explained in further detail in connection with the flowchart shown in .","The SL  may further comprise a restart recovery module . The restart recovery module  recovers (e.g., removes data packets -, -associated with a failed atomic storage operations from the media ) the non-volatile storage media  following a failed atomic write operation which may be caused by a power failure. The restart recovery module  may comprise one or more of the following modules: an access module , an identification of module , and an exclusion module , which may comprise a groomer . The access module  accesses the storage media  at append point  on the non-volatile storage media  using the storage controller . Thereafter, the identification module  may identify a failed atomic request in response to a data packet preceding the append point  comprising a persistent indicator that satisfies a failed atomic write criteria, such as the data packet comprising a first persistent metadata flag in a first state , as explained in connection with .","Thereafter, the exclusion module  may exclude from an index  each data packet -, -associated with the failed atomic storage request. As explained above, the index  maps logical identifiers to physical locations of the data packets on the storage media  (e.g., a non-volatile solid-state storage media).","The exclusion module  excludes from the index , in one embodiment, by bypassing each data packet -, -associated with the failed atomic storage request during a forward or backward scan of the log-based structure used to create the index . The exclusion module  may also exclude from the index  by removing each logical identifier  that maps to each data packet -, -associated with the failed atomic storage request from the index  created by way of a scan of the log-based structure .","The groomer  of the exclusion module  may also exclude from the index  by erasing each data packet -, -associated with the failed atomic storage request from the solid-state storage media  by way of a storage space recovery operation. A storage space recovery operation may comprise, for example, the groomer  transferring valid data -from a first logical erase block to another logical erase block and\/or erasing the data -of the first logical erase block such that the storage space in the first logical erase block is available to store other data, as explained in connection with .","In one embodiment, the groomer  excludes from the index  by erasing each logical erase block of the solid-state storage media comprising one or more data packets -, -associated with the failed atomic storage request and transferring valid data packets -from each logical erase block to a different location on the solid-state storage media . The groomer  may also erase each logical erase block -by assigning a subsequence number  to a destination logical erase block configured to store the transferred data packets -. The subsequence number  may be configured to maintain an ordered sequence among logical erase blocks -of the log-based structure  such that an ordered sequence of storage operations completed on the solid-state storage media  is preserved on the solid-state storage media . Also, during a restart recovery process (such as during a power-on operation), in response to identifying the first logical erase block having a sequence number and the other logical erase block having a subsequence number  derived from the sequence number of the first logical erase block , the groomer  may erase each logical erase block -by grooming  the first logical erase block and excluding each data packet -, -associated with the failed atomic storage request from the index .",{"@attributes":{"id":"p-0301","num":"0300"},"figref":["FIG. 18","FIG. 2B"],"b":["1800","1101","1710","1702","1","216","0","217","1101","1810","1730","1101","1810","1694","1101","1702","1101","1810","1733"],"i":["a ","a"]},"The storage module  may store  data of the atomic storage request and persistent metadata (e.g., the header illustrated in ) in data packets -on different logical erase blocks -of a storage media , as illustrated, for example, in . In one embodiment, the atomic storage request  may involve a plurality of storage operations, each of which may encompass storage operations in a plurality of different logical erase blocks -. The storage module  may store  persistent metadata (such as a header ) and associated user data  within a packet -on the storage media  in a single write operation, i.e., as part of a single operation performed on the storage media .","The acknowledgment module  may then acknowledge  completion of the atomic storage request  to a storage client or the like. The acknowledgment module  may send acknowledgment asynchronously via a callback or other mechanism. Alternatively, the atomic storage request  may be synchronous, and the acknowledgment module  may transmit acknowledgment by a return from a synchronous function or method call.","In some embodiments, acknowledgment is provided as soon as it can be assured that the data of the atomic storage request  will be persisted to the non-volatile storage device , but before the data is actually stored thereon. For example, the acknowledgment module  may send acknowledgment upon transferring data of the atomic storage request  into a buffer of the non-volatile storage device , into a write data pipeline, transferring the data to a storage controller  (e.g., within a protection domain of a storage controller), or the like. Alternatively, acknowledgment  is performed after the data of the atomic storage request  has been persisted on the media .",{"@attributes":{"id":"p-0305","num":"0304"},"figref":["FIG. 19","FIG. 19"],"b":["1900","406","1741","1730","1910","1702","1520","1510","1510","1510","1502","1510","1510","1510","1520","1103","1502","1510","1510","1510","1515","1504","1523","1502"],"i":["a","c","d","e","f","i ","a","c","d","e","f","i ","a","c","d","e","f","i "]},"The identification module  of the SL  identifies  a failed atomic storage request in response to a data packet preceding the append point  comprising a persistent indicator that satisfies a failed atomic write criteria. For example, the persistent indicator may satisfy the failed atomic write criteria if the preceding data packet comprises the first persistent metadata flag in the first state ","The identification module  also identifies  one or more data packets -, -associated with the failed atomic storage request by, for example, identifying data packets including the first persistent metadata flag in a first state . The one or more data packets -, -associated with the failed atomic storage request may be positioned sequentially within the log-based structure . One example of a failed atomic storage request involving sequentially positioned packets is illustrated in , i.e., the data packets -, -of  are associated with the failed atomic storage request and are positioned sequentially in a log-based structure . It should be noted that identifying  the failed atomic storage request and identifying  one or more packets associated with the failed atomic storage request may be performed consecutively or concurrently.","The exclusion module  of the SL  excludes  each data packet -, -associated with the failed atomic storage request from an index, such as a forward index  or a reverse index . The exclusion module  may exclude  bypassing each data packet -, -associated with the failed atomic storage request during a scan of the log-based structure  used to create the index . In addition, the exclusion module  may exclude  by removing each logical identifier  that maps to each data packet -, -associated with the failed atomic storage request from the index  created by way of a scan of the log-based structure .","The groomer  of the exclusion module  may also exclude  by erasing each data packet -, -associated with the failed atomic storage request  from the solid-state storage media  by way of the storage space recovery operation, such as a grooming operation . The groomer  may further exclude  by erasing each logical erase block -of the solid-storage media comprising one or more data packets -, -associated with the failed atomic storage request and transferring data packets -from each logical erase block to a different location on the solid-state storage media , as illustrated, for example, in . The groomer  may also erase by assigning a subsequence number  to a destination logical erase block configured to store the preserved data packets -, as is also illustrated, for example, in . During a power-on operation of the storage device, groomer  may erase by identifying a first logical erase block having a sequence number and another logical erase block having a subsequence number  derived from the sequence number and grooming the first logical erase block , as illustrated in , and excluding each data packet -, -associated with the failed atomic storage request from the index .","The SL  may commence  normal input-output operations after restart recovery is complete. Performing exclusion  before commencing  normal input-output operations, in one embodiment, simplifies the restart recovery process by preventing normal input-output operations from interfering with the restart recovery process and\/or propagating errors in data stored on the media .","It should be noted that the order of the steps of the methods ,  disclosed in  may be varied from the order illustrated in these figures. Also, certain steps may be omitted or added to this disclosed methods."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["In order that the advantages of the invention will be readily understood, a more particular description of the invention briefly described above will be rendered by reference to specific embodiments that are illustrated in the appended drawings. Understanding that these drawings depict only typical embodiments of the invention and are not therefore to be considered to be limiting of its scope, the invention will be described and explained with additional specificity and detail through the use of the accompanying drawings, in which:",{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 2A"},{"@attributes":{"id":"p-0040","num":"0039"},"figref":["FIG. 2B","FIG. 2A"]},{"@attributes":{"id":"p-0041","num":"0040"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0042","num":"0041"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0043","num":"0042"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0045","num":"0044"},"figref":"FIG. 7A"},{"@attributes":{"id":"p-0046","num":"0045"},"figref":"FIG. 7B"},{"@attributes":{"id":"p-0047","num":"0046"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIGS. 9A-E"},{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0050","num":"0049"},"figref":"FIGS. 11A-C"},{"@attributes":{"id":"p-0051","num":"0050"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0052","num":"0051"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0053","num":"0052"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0054","num":"0053"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0055","num":"0054"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0056","num":"0055"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0057","num":"0056"},"figref":"FIG. 18"},{"@attributes":{"id":"p-0058","num":"0057"},"figref":"FIG. 19"}]},"DETDESC":[{},{}]}
