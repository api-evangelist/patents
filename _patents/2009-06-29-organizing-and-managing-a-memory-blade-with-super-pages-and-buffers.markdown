---
title: Organizing and managing a memory blade with super pages and buffers
abstract: A system and method is illustrated wherein a protocol agent module receives a memory request encoded with a protocol, the memory request identifying an address location in a memory module managed by a buffer. Additionally, the system and method includes a memory controller to process the memory request to identify the buffer that manages the address location in the memory module. Further, the system and method includes an address mapping module to process the memory request to identify at least one super page associated with the memory module, the at least one super page associated with the address location.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08645610&OS=08645610&RS=08645610
owner: Hewlett-Packard Development Company, L.P.
number: 08645610
owner_city: Houston
owner_country: US
publication_date: 20090629
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["CROSS REFERENCE TO RELATED APPLICATIONS","BACKGROUND","DETAILED DESCRIPTION"],"p":["This is non-provisional patent application is related to Patent Cooperation Treaty Application Number PCT\/US2008\/069168 entitled \u201cMEMORY SERVER\u201d that was filed on Jul. 3, 2008, and which is incorporated by reference in its entirety.","Repeater buffers are used in the facilitation of data reads and writes from memory. Additionally, these repeater buffers are dedicated to the management a particular type of memory. Further, repeater buffers are used by computer systems in the management of virtual memory to conduct the above referenced reads and writes.","A system and method is illustrated for facilitating memory management of a plurality of disparate memory devices using one or more repeater buffers. These repeater buffers, referenced herein as buffers, reside upon a memory blade. Memory management includes the allocation or de-allocation of virtual memory pages utilized by compute blades. Virtual memory pages, as referenced herein, include super pages comprised of sub pages. A memory blade is a device for managing a plurality of memory devices that may be utilized by a compute blade. A compute blade, as referenced herein, is a computer system with memory to read input commands and data, and a processor to perform commands manipulating that data. Example memory devices, referenced herein as memory modules, include Static Random Access Memory (SRAM), Dynamic Random Access Memory (DRAM), Electrically Erasable Programmable Read-Only Memory (EEPROM) referenced herein as flash memory, other Main Memory implementation (e.g., magnetic, flash or optically based memory), or other suitable memory. A buffer, as used herein, is a device used to temporarily store and process (e.g., encode and decode) data for storage into one or more of the memory devices.","In some example embodiments, the virtual memory pages are managed using a system of mapped addresses. For example, a blade identifier (bladeID) and System Machine Address (SMA) are used by the memory blade to map to a Remote Memory Machine Address (RMMA). Specifically, a map register is indexed using the bladeID, where each entry in the map register (e.g., a base entry and a limit entry) represents the number of super pages managed by the memory blade identified using the bladeID. Further, the base entry and a super page ID parsed from the SMA are used to index into an RMMA map structure. Each entry in the RMMA map represents a super page and the permissions associated with this super page. A super page is a virtual memory page of for example, 16 KB or larger. A sub page is a virtual memory page that is, for example, smaller than 16 KB.","In some example embodiments, a system and method is illustrated for a memory blade design that provides a unified command interface (e.g., Application Programming Interfaces (APIs)) for multiple access granularities and usage models, and using memory modules with heterogeneous density and timing characteristics, and modular composition of multiple memory blades. The API facilitates read and write functionality for the memory blade. Additionally, the API allows for memory on the memory blade to be allocated or de-allocated by a compute blade. Additionally, the API allows for address mappings and permissions to be retrieved and set. This API also allows a single memory blade to be shared by multiple compute blades.",{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 1","b":["100","101","102","103","104","101","102","105","105","105","106","107","108","109","101","102","105"]},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 2","b":["102","201","202","203","201","202","203","207","205","207","205","205","207","202"]},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 3","b":["103","301","302","303","302","305","309","303","310","315","302","304","309","303","310","315","304","309","310","315","316","317","304","317","318","305","318","319","306","319","320","307","320","221","308","321","223","309","324","326","310","326","327","311","327","328","312","328","329","313","329","330","314","330","331","315","103"]},"In some example embodiments, multiple on-board repeater buffers  through  are implemented with each buffer acting as an independent memory controller, or manager, for a subset of the memory modules. As illustrated in , a single buffer may be used to manage 2-4 memory modules. The management duties for a buffer (e.g., buffers  through ) include receiving memory operation commands and data from a central memory controller (e.g., central memory controller ), and forwarding the data to individual memory modules (e.g., memory modules  through ). Responses are sent from one more of the memory modules  through , via the buffers  through , to the central memory controller . Additionally, as will be more fully illustrated below, a buffer may store and use a particular codec to format data for storage in a particular memory module to which the buffer is operatively connected.",{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 4","b":["103","103","301","413","401","101","102","402","403","403","402","404","405","408"]},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 5","b":["304","501","507","501","501","506","501","502","503","502","304","508"]},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 6","b":["600","402","409","410","605","606","605","606","409","601","602","601","602","405","410","603","604","606","406"]},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 7","b":["700","705","601","706","602","705","603","705","603","705","601","706","604","706","602"]},"In some example embodiments, various reliability and serviceability optimizations are implemented for the buffers. For example, in cases where the memory blade  maintains a free list of super pages, memory scrubbing can be initiated for allocated virtual memory pages in the background to avoid unnecessary failures. Scrubbing, as referenced herein, includes the detection and recovery of virtual memory pages that include data errors. Further, because memory allocation information is maintained on the memory blade , the memory module  can map out memory modules (e.g., DRAM ) for which a fault is detected, and use memory modules for which a fault has not been detected. Additionally, memory provisioning, repair and upgrade can be done on a single memory blade, without having to modify individual compute blades. Further, the buffer  and  can also include an indicator Light Emitting Diode (LED) light to indicate the operational status of these memory modules.",{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 8","b":["403","801","802","801","802"]},{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 9","b":["403","901","909","901","801","909","908","910","909","907","911","911","904","902","908","802","906","905","905","802"]},"In some example embodiments, the call sequence for the various tables maintained by the address mapping module  is illustrated as follows. Accessing data on the remote memory involves address mapping, and performing a read or write to a memory blade and memory maintained therein. In one example embodiment, memory is organized and managed in large chunks (e.g., as a plurality of 16 MB super pages) so that one single entry in the RMMA mapping table  corresponds to thousands of contiguous operating system pages (e.g., virtual memory pages). These virtual memory pages are accessed based upon the needs of the compute blades and the applications residing therein.","Some example embodiment include the use of the above referenced map registers  to facilitate RMMA partitioning to identify particular compute blades that are to receive a memory allocation, or that are to have memory de-allocated. An RMMA may be partitioned in a particular manner to enforce a static or dynamic memory allocation. Under a static partitioning regime, each compute blade is assigned a fixed portion of remote memory. This remote memory is allocated at startup time, and remains constant during the compute blade's uptime. Under a dynamic partitioning regime, each compute blade is given an initial allocation of remote memory, and can request additional memory as application demand changes. In some example embodiments, the memory blade  orchestrates memory reallocation by revoking allocations from some compute blades (e.g., compute blade ) to provide capacity to others (e.g., compute blade ). Such changes are recorded as the map register values to reflect the capacity allocation for each compute blade.","In certain cases, RMMA sharing may be implemented by a compute blade. An owner of a RMMA region is defined as the compute blade to which the RMMA region is allocated. This owner can choose to map the RMMA into another blade's SMA, therefore allowing the data stored in this region to be shared among both blades (e.g., compute blade  and ). This is achieved by the mapping and sharing interface illustrated below.",{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 10","b":["103","1001","1001","1002","1002","1003","1003","1004","1004","1004"]},{"@attributes":{"id":"p-0045","num":"0044"},"figref":"FIG. 11","b":["103","1101","1101","1102","1102","1102","1102","1102","1102","1103"]},{"@attributes":{"id":"p-0046","num":"0045"},"figref":"FIG. 12","b":["103","1201","409","1201","1201","405","1201","1201","1201","1201","1201","1201","1201","304","302"]},{"@attributes":{"id":"p-0047","num":"0046"},"figref":"FIG. 13","b":["1300","1301","1001","1302","1002","1303","1003","1304","1004","1305","409","1306","1307","501"]},{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIG. 14","b":["1400","103","1401","1101","1402","1102","1403","1102","1404","1102","1405","1102","1406","1102","1407","1102","1408","1102"]},{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIG. 15","b":["1500","1501","1201","1502","1201","1503","1201","1504","1201","1505","1201","1506","1201","501","1501","1506"]},{"@attributes":{"id":"p-0050","num":"0049"},"figref":"FIG. 16","b":["1600","1602","1603","102","1618","102","1605","1617","103","1609","1611","1616","1614","1606","1612","402","1601","102","1602","1601","102","1602","1603","102","1601","1618","1618"]},"In example cases where decisional operation  evaluates to \u201ctrue,\u201d an operation  is executed. Operation  is executed to processes the read request  using the protocol agent . An operation  is executed to parse the address of the data from the read request . Specifically, the read request  is unpacked, or parsed, and separated from its protocol header. For example, a SMA may be parsed to retrieve a super page address through the use of operation . Operation  is executed to index into the RMMA map table  (see e.g., operation  referenced above). A decisional operation  is executed to determine whether an index within the RMMA map table  has been identified. In cases where decisional operation  evaluates to \u201cfalse,\u201d an exception  is thrown. This exception  may be handled by the hypervisor . In cases where decisional operation  evaluates to \u201ctrue,\u201d a decisional operation  is executed. Decisional operation , when executed, determines whether the appropriate permissions exist to access a particular RMMA map table entry. In cases where decisional operation  evaluates to \u201cfalse,\u201d an exception is thrown as denoted at . Exceptions thrown at  are handled by the hypervisor . In example cases where decisional operation  evaluates to \u201ctrue,\u201d an operation  is executed. Operation  transmits or otherwise sends a read command to a buffer (see e.g., buffer ). Operation  is executed to optionally power up and retrieve decoded data from memory such as DRAM  or DRAM . This memory is represented generically as data store . Operation  is executed to packetized data using the protocol agent . This packetization process includes affixing a header to the decoded data such that the decoded data can be transmitted across a memory channel such as memory channel  or . This data is transmitted as a response message in the form of requested data . Operation  is executed to optionally power down memory in the form of, for example, DRAM  or . In some example embodiments, in parallel with the read, the ECC and dirty bits are fetched and packetized with the data as part of the response message for the read request .","In some example embodiments, one or more optimizations of the memory blade  are implemented. For example, where data is to be read or written locally to the compute blade , power optimization is facilitated through putting the DRAM  on the memory blade  into an active power-down mode. A second example of power optimization is facilitated through placing data within consecutive virtual memory pages on the same DRAM  residing on the memory blade . As to the first example of power optimization, a powering down of the DRAM  may occur whenever the idle period of the DRAM  has been longer than a given threshold, and the buffer (e.g., buffer ) can command the DRAM into active power-down mode. The buffer  may receive a power down command from the compute blade  via another buffer or the memory controller  via a memory channel. A given threshold may be some unit of time. With respect to the second example of power optimization, virtual memory pages are clustered to a reduced number of DRAM such that a fewer number of DRAM is utilized than would otherwise be used to store data. As illustrated above, DRAM not in use is powered down.",{"@attributes":{"id":"p-0053","num":"0052"},"figref":"FIG. 17","b":["1700","1701","1701","1701","1702","1703","1703"]},"In some example embodiments, an API is provided for the memory blade  so as to allow compute blades and other devices to interface with the memory blade. This API may be exposed by the central memory controller  to a compute blade. These APIs provide an interface to utilize the data use modules referenced above. Illustrated are various data access functions. These data access functions include\u2014","data=read (bladeID, bladeMA, size)","write (bladeID, bladeMA, size, data)","The read function takes three parameters that include a blade identifier (bladeID) value, a blade machine address (bladeMA) value, and a size value. A page of virtual memory is returned through the execution of this function as data. The bladeID identifies a particular memory blade from which data is being retrieved. Additionally, the bladeMA value is a logical or physical address for a memory blade. The size value reflects the size of the virtual memory page being retrieved for reading. These various parameters may be integers or some other suitable value. Additionally, as illustrated above, the write function takes the three aforementioned parameters plus a data parameter that may denote the data type of the data being written to memory.","In some example embodiments, the above referenced API includes various capacity management functions that include\u2014","allocate (bladeID, bladeMA, size)","release (bladeID, bladeMA, size)","The capacity management functions allow for a compute blade to request that memory be allocated or de-allocated (e.g., released) on the memory blade . Like the read function illustrated above, both of the capacity management functions take, for example, three parameters (e.g., bladeID, bladeMA, and size). These parameters, however, are used for a purpose distinct from that outlined above with respect to the data access functions. Here, for example, these three parameters are used to identify a particular blade upon which memory is to be allocated. Additionally, the size of the virtual pages to be allocated is also provided.","Some example embodiments of the above referenced API include a mapping and sharing API. The mapping and sharing API has the following form:","RMMA=getMap (bladeID, bladeMA)","setMap (bladeID, bladeMA, RMMA, permission)","The getmap function takes two parameters, and returns an integer in the form of a Remote Memory Machine Address (RMMA). These two parameters are the aforementioned bladeID, and bladeMA. The setMap function takes four parameters that include bladeID and bladeMA, RMMA, and a permission value. The permission value may be a boolean value, integer value, character value, or other suitable values used to denote a particular permission field in the RMMA map table .","In some example embodiments, a VM is migrated from one compute blade to another compute blade such that the memory contents of the migrated compute blade needs to be copied. In cases where address mapping is implemented (see e.g., RMMA Map ) on a memory blade, only local memory for the VM need be migrated through the use of the above referenced setMap function.",{"@attributes":{"id":"p-0061","num":"0060"},"figref":"FIG. 18","b":["1800","103","1802","1808","103","1801","103","1801","1802","1802","401","1801","1803","601","602","1805","1803","1804","1805","1806","1807","1809","504","1808","601","602"]},"In some example embodiments, the operation  is executed to facilitate ECC coding and capacity optimizations on the buffers  and . The buffers can encode the ECC bits on groups of cache block granularity (e.g., 64 bytes), as compared to typical 8-byte granularity, reducing ECC overhead due to encoding 12 bits per 64 bytes. This generates a resulting cost savings when ECC is performed.","In some example embodiments, prior to or while executing the method  various memory allocation and de-allocation operations are performed by the central memory controller . These allocations and de-allocations may be executed by the operation  prior to the writing of data. In one example embodiment, the memory blade  maintains a list of free virtual super pages to track capacity for allocation and de-allocation. In cases where the virtual free pages runs low in the memory blade , the memory blade  coordinates with the hypervisors (e.g., hypervisor ) on individual compute blades (e.g., compute blades  and ) to inflate one or more VM memory balloon drivers (e.g., balloon driver ). The inflating of the VM memory balloon drivers frees pages in the VM address space for use by another compute blade.",{"@attributes":{"id":"p-0064","num":"0063"},"figref":"FIG. 19","b":["1804","1901","1906","1901","601","603","1902","1902","1907","1902","1903","1903","1903","1905","1903","1904","1904","1906"]},{"@attributes":{"id":"p-0065","num":"0064"},"figref":"FIG. 20","b":["2000","2001","2004","2003","402","2001","2002","2003","2004"]},{"@attributes":{"id":"p-0066","num":"0065"},"figref":"FIG. 21","b":["2004","2101","2103","2101","2102","2103","2103","2101","2103"]},{"@attributes":{"id":"p-0067","num":"0066"},"figref":"FIG. 22","b":["2200","2201","201","2201","2200","2201","2202","2204","2203","2205","2204","2207","2204","2206","2204","2204","2211","2209","2209","2211","2211","2208","2212","2210","2213","2214","2215","2211","2216","2216","2217","2218"]},"The SATA port  may interface with a persistent storage medium (e.g., an optical storage devices, or magnetic storage device) that includes a machine-readable medium on which is stored one or more sets of instructions and data structures (e.g., software) embodying or utilized by any one or more of the methodologies or functions illustrated herein. The software may also reside, completely or at least partially, within the SRAM  and\/or within the CPU  during execution thereof by the computer system . The instructions may further be transmitted or received over the 10\/100\/1000 ethernet port , USB port  or some other suitable port illustrated herein.","In some example embodiments, a removable physical storage medium is shown to be a single medium, and the term \u201cmachine-readable medium\u201d should be taken to include a single medium or multiple medium (e.g., a centralized or distributed database, and\/or associated caches and servers) that store the one or more sets of instructions. The term \u201cmachine-readable medium\u201d shall also be taken to include any medium that is capable of storing, encoding or carrying a set of instructions for execution by the machine and that cause the machine to perform any of the one or more of the methodologies illustrated herein. The term \u201cmachine-readable medium\u201d shall accordingly be taken to include, but not be limited to, solid-state memories, optical and magnetic medium, and carrier wave signals.","Data and instructions (of the software) are stored in respective storage devices, which are implemented as one or more computer-readable or computer-usable storage media or mediums. The storage media include different forms of memory including semiconductor memory devices such as DRAM, or SRAM, Erasable and Programmable Read-Only Memories (EPROMs), Electrically Erasable and Programmable Read-Only Memories (EEPROMs) and flash memories; magnetic disks such as fixed, floppy and removable disks; other magnetic media including tape; and optical media such as Compact Disks (CDs) or Digital Versatile Disks (DVDs). Note that the instructions of the software discussed above can be provided on one computer-readable or computer-usable storage medium, or alternatively, can be provided on multiple computer-readable or computer-usable storage media distributed in a large system having possibly plural nodes. Such computer-readable or computer-usable storage medium or media is (are) considered to be part of an article (or article of manufacture). An article or article of manufacture can refer to any manufactured single component or multiple components.","In the foregoing description, numerous details are set forth to provide an understanding of the present invention. However, it will be understood by those skilled in the art that the present invention may be practiced without these details. While the invention has been disclosed with respect to a limited number of embodiments, those skilled in the art will appreciate numerous modifications and variations therefrom. It is intended that the appended claims cover such modifications and variations as fall within the \u201ctrue\u201d spirit and scope of the invention."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["Some embodiments of the invention are described, by way of example, with respect to the following figures:",{"@attributes":{"id":"p-0005","num":"0004"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0006","num":"0005"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0007","num":"0006"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 18"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 19"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 20"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 21"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 22"}]},"DETDESC":[{},{}]}
