---
title: Bitrate video transcoding based on video coding complexity estimation
abstract: A system and method provides content-adaptive bitrate video transcoding of a source video for a video hosting service. The system is coupled to a video coding complexity engine and video rate-distortion modeling engine of the video hosting service. The system is configured to receive the video coding complexity score of the source video and a trained rate-distortion model and a scaling model. A target bitrate estimation module of the system is configured to calculate an initial target bitrate based on the video coding complexity using the trained rate-distortion model. A bitrate refinement module of the system is configured to adjust the initial target bitrate with respect to the resolution and/or frame rate of the transcoded source video. An adaptive video coder of the system is configured to transcode the source video with the adjusted target bitrate.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08897370&OS=08897370&RS=08897370
owner: Google Inc.
number: 08897370
owner_city: Mountain View
owner_country: US
publication_date: 20091130
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["The present invention relates generally to video hosting systems, and more particularly to a video coding system for adaptive transcoding of videos based on video coding complexity.","Video hosting services, such as YOUTUBE, allow users to post videos. Most video hosting services transcode an original source video from its native encoded format (e.g., MOV) into a commonly available format (e.g., ADOBE FLASH or Windows Media Video (WMV)). Transcoding comprises decoding the source video from the native format into an unencoded representation using a codec for the native format and then encoding the unencoded representation with a codec for the commonly available format. Transcoding can be used to reduce storage requirements, and also to reduce the bandwidth requirements for serving the video to clients.","One challenge in designing a video coding system for video hosting services with millions of videos is to transcode and to store the videos with acceptable visual quality and at a reasonable computing cost. A particular problem is the efficient allocation of coding bits to achieve an optimized rate-distortion (R-D) of a source video. Generally, given a target resolution and frame rate, a video's visual quality is determined by its encoding bitrate computed using a rate control algorithm. Conventional video coding systems use traditional rate control algorithms to optimize the allocation of available coding bits within a single video sequence. However, given a large video corpus, in addition to the problem addressed by the traditional rate control algorithms, another problem is how to optimize the allocation of coding bits among different videos. Coding each video with same target resolution and video frame rate does not necessarily produce acceptable video quality in every case. A same encoding bitrate applied to two different videos having content of different complexity leads to different video qualities. A coding bitrate applied to one particular video cannot accurately represent average bitrate of the video corpus. Using a fixed encoding bitrate to encode source videos with varying video content deteriorates visual quality processed by a conventional video coding system for video hosting services.","Another aspect of the challenge in designing the video coding system is resolution transcoding with optimal visual quality. Conventional video transcoders specify a fixed resolution with a target bit rate and encode source videos with a variety of coding parameters (e.g., resolution, frame rate, bitrate) and content to output videos with the fixed resolution. However, conventional video transcoders ignore the impact of video content and coding complexity on transcoding. For example, a large number of user-contributed videos contain simple content, such as slide shows and talking heads with very little motion in the videos. Encoding these simple videos with a fixed small resolution degrades video quality and user experience.","A method, system and computer program product provides adaptive transcoding of videos based on video coding complexity for video hosting services.","In one embodiment, the adaptive transcoding system comprises a video coding complexity engine, a video rate-distortion modeling engine, an adaptive bitrate transcoding sub-system and an adaptive resolution transcoding sub-system. The video coding complexity engine is configured to generate a measure of the video coding complexity of a source video. The video coding complexity measure represents a measure of how complicated it is to encode the source video. The video rate-distortion modeling engine is configured to estimate a rate-distortion (R-D) model and a scaling model for the adaptive transcoding system. The R-D model and scaling model are statistical models trained on a video corpus of the adaptive transcoding system using the video coding complexity information for the videos in the corpus. The adaptive bitrate transcoding sub-system applies the trained R-D model and scaling model to a source video and transcodes a source video with an optimized bitrate and visual quality based on the video coding complexity of the source video. The adaptive resolution transcoding sub-system transcodes a source video with an optimized resolution and visual quality based on the video coding complexity of the source video.","In another embodiment, the adaptive transcoding method receives a source video from client for a video hosting service. The adaptive transcoding method measures the video coding complexity of the source video and provides the video coding complexity measure of the source video to one or more adaptive video transcoders for further processing. The adaptive transcoding method trains a subset of a video corpus and generates a R-D model and scaling model using the video corpus and video coding complexity information for the videos in the corpus. The adaptive video coding method applies the R-D model and the scaling model to the source video based on the video coding complexity measure of the source video and transcodes the source video with an optimized bitrate and visual quality. The adaptive video coding method determines an optimal resolution for the source video based on the video coding complexity measure of the source video and transcode the source video with an optimized resolution and visual quality.","A video coding complexity engine of an adaptive transcoding system generates a measure of the coding complexity of a source video. In one embodiment, the video coding complexity of a source video is proportional to the amount of spatial and motion information contained in the video. The video coding complexity engine is configured to receive a source video in its native format and to encode the video from its native format into an intermediate format. The video coding complexity engine extracts spatial and temporal video features at the frame level, and transforms the frame level video features into features representing the source video as a whole. The video coding complexity engine selects a subset of video features, normalizes selected video features and scales the normalized video features for the coding complexity computation. Based on the normalized and scaled video features, the video coding complexity engine computes a video coding complexity score for the source video. The coding complexity score is used by an adaptive bitrate transcoding sub-system and\/or an adaptive resolution transcoding sub-system for transcoding the source video.","In another embodiment, a video coding complexity measurement method receives a source video in its native format and encodes the source video into an intermediate format. The complexity measurement method extracts video features of the source video, normalizes and scales selected video features. The complexity measurement method further computes a video coding complexity score for the source video based upon the video feature information.","In one embodiment, the video rate-distortion modeling engine is configured to select a set of videos from a video corpus of a video hosting service and to encode the selected set of videos at multiple bitrates, resolutions and frame rates. From the plurality of encoded videos, the video rate-distortion modeling engine collects a plurality of rate-distortion coding data, and trains a rate-distortion model and a scaling model based on the collected rate-distortion coding data. A rate-distortion model defines a relationship between a quantization step size and a bitrate for a video being coded. A scaling model directs adjustment of the bitrate of a video calculated by the rate-distortion model given a frame rate and\/or resolution of the transcoded video. To determine the optimized target bitrate of a video in the video corpus for a given visual quality, the trained rate-distortion model and the scaling model are applied to all videos in the corpus of the video hosting service. The trained rate-distortion model and the scaling model are used in conjunction with a video coding complexity engine and an adaptive bitrate transcoding sub-system to transcode a source video with an optimized bitrate and visual quality based on the video coding complexity measure of the source video.","In another embodiment, a R-D modeling method selects a set of videos from a video corpus of an adaptive transcoding system and encodes the selected set of videos at multiple bitrates, resolutions and frame rates. The R-D modeling method collects multiple pairs of R-D data from the encoding. The R-D data pair describe a relationship between the bitrate of an encoded source video and the distortion of reconstructed source video, so as to provide a model of how the distortion of the reconstructed source video varies with the bitrate of the encoded source video keeping. Based on the collected R-D data, the R-D modeling method estimates a R-D model and a scaling model for encoding the videos of the video corpus.","An adaptive bitrate transcoding sub-system transcodes a source video with an optimized bitrate and visual quality based on the video coding complexity of the source video and the R-D model and scale model provided by the rate-distortion modeling engine. The adaptive bitrate transcoding sub-system is configured to receive a source video and to obtain the video coding complexity score from a video coding complexity engine and an expected target video coding quality (e.g., indicated by a target quantization step size) of the source video. The adaptive bitrate transcoding sub-system calculates an initial target bitrate of the source video based on the video coding complexity score and the target quantization step of the source video using the R-D model. The adaptive bitrate transcoding sub-system uses the scale model to adjust the calculated initial target bitrate with respect to the resolution and\/or the frame rate of the transcoded video. The adaptive bitrate transcoding sub-system transcodes the source video using the adjusted initial target bitrate.","An adaptive bitrate transcoding method receives a source video and obtains the video coding complexity of the source video. The bitrate transcoding method applies the trained R-D model to the source video to calculate an initial target bitrate of the source video based on the video coding complexity. The bitrate transcoding method further adjusts the initial target bitrate with respect to the resolution and\/or frame rate of the transcoded video. The adaptive bitrate transcoding method transcodes the source video with the adjusted target bitrate.","An adaptive resolution transcoding sub-system transcodes a source video with an optimized resolution and visual quality based on the video coding complexity of the source video. The adaptive resolution transcoding sub-system is configured to receive a source video in its native format. The adaptive resolution transcoding sub-system obtains the video coding complexity score of the source video and video coding parameters (e.g., content header information) of the source video from a video coding complexity engine and sets a resolution adjustment level based on the complexity score. Based on the resolution adjustment level, the adaptive resolution transcoding sub-system determines an optimal output resolution for the source video for each video output format supported by the adaptive resolution transcoding sub-system. The optimal output resolution represents the most suitable resolution to transcode the source video based on its coding parameters such as content complexity and its native resolution. Responsive to a user selection of video output format, the adaptive resolution transcoding sub-system determines an optimal output resolution for the source video and encodes the source video with the determined optimal output resolution.","In another embodiment, the adaptive resolution transcoding method receives a source video from a client and obtains the video coding complexity score and video coding parameters (such as content header) of the source video. For each video output format, the adaptive resolution transcoding method sets a resolution adjustment level and determines an optimal output resolution for the source video. Responsive to a user selection of a video output format, the adaptive resolution transcoding method selects an optimal resolution for the source video and encodes the source video with the optimal output resolution for the selected video output format","The features and advantages described in the specification are not all inclusive and, in particular, many additional features and advantages will be apparent to one of ordinary skill in the art in view of the drawings, specification, and claims. Moreover, it should be noted that the language used in the specification has been principally selected for readability and instructional purposes, and may not have been selected to delineate or circumscribe the disclosed subject matter.","The figures depict various embodiments of the present invention for purposes of illustration only, and the invention is not limited to these illustrated embodiments. One skilled in the art will readily recognize from the following discussion that alternative embodiments of the structures and methods illustrated herein may be employed without departing from the principles of the invention described herein.","I. System Overview",{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 1A","b":["100","102","110","100","100","100","110","130","100","110","102","110"]},"Turning to the individual entities illustrated on , each client  is used by a user to request video hosting services. For example, a user uses a client  to send a request for uploading a video for sharing, or playing a video. The client  can be any type of computer device, such as a personal computer (e.g., desktop, notebook, laptop) computer, as well as devices such as a mobile telephone, personal digital assistant, IP enabled video player. The client  typically includes a processor, a display device (or output to a display device), a local storage, such as a hard drive or flash memory device, to which the client  stores data used by the user in performing tasks, and a network interface for coupling to the system  via the network .","A client  also has a video player  (e.g., the Flash\u2122 player from Adobe Systems, Inc., or a proprietary one) for playing a video stream. The video player  may be a standalone application, or a plug-in to another application such as a network browser. Where the client  is a general purpose device (e.g., a desktop computer, mobile phone), the player  is typically implemented as software executed by the computer. Where the client  is dedicated device (e.g., a dedicated video player), the player  may be implemented in hardware, or a combination of hardware and software. All of these implementations are functionally equivalent in regards to the present invention. The player  includes user interface controls (and corresponding application programming interfaces) for selecting a video feed, starting, stopping, and rewinding a video feed. Also, the player  can include in its user interface a video display format selection configured to indicate which video display format (e.g., a standard definition TV or a high-definition TV). Other types of user interface controls (e.g., buttons, keyboard controls) can be used as well to control the playback and video format selection functionality of the player .","The network  enables communications between the clients  and the video hosting service . In one embodiment, the network  is the Internet, and uses standardized internetworking communications technologies and protocols, known now or subsequently developed that enable the clients  to communicate with the video hosting service .","The video hosting service  comprises an adaptive transcoding system , a video server , an ingest server  and a video database . The ingest server  receives user uploaded videos and stores the videos in the video database . The video server  serves the videos from the video database  in response to user video hosting service requests. The video database  stores user uploaded videos and videos transcoded by the adaptive transcoding system . In one embodiment, the video database  stores a large video corpus (e.g., 20,000 user uploaded videos) for the adaptive transcoding system  to train rate-distortion modeling engines.","The adaptive transcoding system  comprises a video coding complexity engine , a video rate-distortion (R-D) modeling engine , an adaptive bitrate transcoding sub-system  and an adaptive resolution transcoding sub-system . For a source video, the video coding complexity engine  generates a measure of the coding complexity of the source video based on spatial and temporal features of the source video. The video rate-distortion modeling engine  generates a rate-distortion model and a scaling model from videos in a video corpus stored in the video database . The adaptive bitrate transcoding sub-system  transcodes a given source video with an optimized bitrate and visual quality based on the video coding complexity of the source video and the rate-distortion model. The adaptive resolution transcoding sub-system  transcodes the source video with an optimized resolution and visual quality based on the video coding complexity of the source video. As a beneficial result, each source video is transcoded to a video  that has an optimized visual quality, bit rate, and resolution.","To further illustrate the system operation of the video hosting service ,  is a flow diagram illustrating of the adaptive transcoding system  within the video hosting service  illustrated in . A user sends a request to the video hosting service  for uploading a source video . The adaptive transcoding system  receives the source video  in its native format and processes the source video  by the video coding complexity (VCC) engine . The source video  in its native format has a plurality video coding parameters including resolution, frame rate and bitrate. These plurality video coding parameters are referred to as \u201csource video coding parameters.\u201d","In one embodiment, the VCC engine  comprises a VCC calculation module  and VCC model training module . The VCC calculation module  comprises an intermediate video encoder, a video feature extraction module and a VCC estimation module. The intermediate video encoder encodes the source video  into an intermediate video format. The VCC estimation module generates a measure of the video coding complexity of the source video . The VCC model training module  comprises a normalization module and a training module. The VCC model training module  may further comprise an intermediate video encoder and a video feature extraction module. The VCC model training module  trains a VCC model using videos selected from a video corpus . The VCC calculation module  generates the measure of the video coding complexity of the source video with reference to the trained VCC model. The VCC engine  sends the plurality of source video coding parameters, such as, resolution, frame rate and content header information of the source video , and the estimated video coding complexity measurement to adaptive transcoders  for further processing. The VCC engine  is further described in , B, C and .","The source video  can be also stored in a video database and becomes a part of the video corpus  stored in the video database. The video corpus  is processed by the video R-D modeling engine . In one embodiment, the video R-D modeling engine  comprises a R-D model estimation module  and a scaling model estimation module . The video R-D modeling engine  trains the video corpus  off-line using the R-D model estimation module  and the scaling model estimation module  to generate a rate-distortion model and a scaling model from videos in the video corpus . The video R-D modeling engine  is further described in .","The adaptive transcoders  in one embodiment comprises the adaptive bitrate transcoding sub-system  and the adaptive resolution transcoding sub-system  illustrated in . The adaptive bitrate transcoding sub-system  communicates with the VCC engine  to obtain the video coding complexity and one or more video coding parameters of the source video . The adaptive bitrate transcoding sub-system  applies the R-D model and scaling model obtained from the video R-D modeling engine  to the source video and generates an optimal bitrate for the source video  based on the video coding complexity of the source video . The adaptive bitrate transcoding sub-system  transcodes the source video  with the optimal bitrate and visual quality. The adaptive bitrate transcoding sub-system  is further described in .","The adaptive resolution transcoding sub-system  communicates with the VCC engine  to obtain the video coding complexity and one or more video coding parameters of the source video . The adaptive resolution transcoding sub-system  estimates an optimal output resolution for the source video  based on the video coding complexity of the source video , and transcodes the source video  with the optimal output resolution and visual quality. The optimal output resolution represents the most suitable resolution to transcode the source video based on its coding parameters such as content complexity and its native resolution. The adaptive resolution transcoding sub-system  is further described in . Other embodiments of the adaptive transcoders  may comprise additional transcoding sub-systems.","II. Video Coding Complexity Measurement","Varying contents in scenes captured by video sequences lead to various amount of information contained in the video sequences. Given a large video corpus of a video hosting service, coding each video with same target resolution and video frame rate does not necessarily produce acceptable video quality in every case. Applying same coding bitrate to different video content leads to different video qualities. A coding bitrate being applied to a particular video sequence cannot accurately represent average bitrate of the video corpus. Furthermore, ignoring the impact of video content and coding complexity on transcoding a video sequence degrades the visual quality of the transcode video and user experience. To transcode a source video with acceptable video quality needs effectively evaluates the video coding complexity of the source video.",{"@attributes":{"id":"p-0045","num":"0044"},"figref":["FIG. 2A","FIG. 2B","FIG. 2C"],"b":["200","200","210","250","210","150","150","250","160","250","210","150","210","250"]},{"@attributes":{"id":"p-0046","num":"0045"},"figref":"FIG. 2B","b":["210","200","210","220","230","240","200","150","150","210","150"]},"The intermediate video encoder  is configured to encode the source video  from its native format into an intermediate format at a high target encoding bitrate (e.g., 20 megabits per second for a 640\u00d7360 video sequence). To encode the source video  into an intermediate format allows the VCC calculation module  to easily extract the spatial and temporal features from the source video and the extracted spatial and temporal features from the source video closely represent the amount of spatial and temporal (e.g., motion) information contained in the source video . The VCC calculation module  uses the extracted features to measure the coding complexity of the source video .","To simplify the description of the intermediate video encoder , the input video  in its native format is referred to as the \u201coriginal source video\u201d and the encoded input video in the intermediate video format is referred to as \u201cencoded source video\u201d. The intermediate format may be same as the native format or different from the native format. Practically, the intermediate video encoder  is allocated some number of coding bits as needed to encode the source video  such that the encoded source video is close to lossless video quality. The encoding process by the intermediate video encoder  creates a log file that stores the coding information of each video frame of the encoded source video.","For example, the VCC calculation module  receives a source video  having a Moving Picture Experts Group (MPEG) 2 as its native format. The intermediate video encoder  is a flash video encoder, and the flash video encoder encodes the source video  from its native format (e.g., MPEG-2) to an intermediate format (e.g., H.263) using a video encoding program (e.g., FFmpeg). The encoding process by the flash video encoder creates an FFmpeg log file for the encoded source video. Other video encoders can be used as the intermediate video encoder .","The coding information in the log file created by the intermediate video encoder  may include the information about the spatial and temporal features and\/or other information of each frame of the encoded source video. Using the same example above, the FFmpeg log file of the encoded source video may include the following parameters to represent the information of a video frame of the encoded source video:","display_picture_number: picture number of the video frame in display order;","qp: quantization parameter of the video frame;","header_bits: number of bits to encode the header of the video frame;","mv_bits: number of bits to code motion vectors of the video frame;","mc_mb_var_sum: sum of variance of motion-compensated macroblock (MB) residual over the video frame;","mb_var_sum: sum of variance of original source MB over the video frame;","i_tex_bits: number of bits to encode an intra macroblock texture;","p_tex_bits: number of bits to encode a predictive macroblock texture;","i_count: number of intra macroblocks of the video frame;","skip_count: number of skipped macroblocks of the video frame.","Variations in the spatial and temporal characteristics of source videos lead to different coding complexity of the source videos. In one embodiment, the video coding complexity of a source video is proportional to the amount of spatial and temporal (e.g., motion) information contained in the source video. The video feature extraction module  extracts one or more spatial and temporal features of the source video from the log file created by the intermediate video encoder . Extracted spatial features of a video frame characterize the amount of spatial activity of the video frame. In one embodiment, the video feature extraction module  extracts the variance of each MB's luminance value of a source video frame averaged over an entire intra frame, and extracts the variance of each MB's motion-compensated luminance residual over the entire motion-compensated P- or B-frame. The larger the luminance variance\/motion-compensated luminance residual becomes, the larger the spatial activity is contained in the video frame.","Using the FFmpeg log file described above as an example, the extracted spatial feature for a video frame is represented by \u201cmb_var_sum\u201d parameter in the FFmpeg log file. For a motion-compensated frame, the extracted spatial feature is represented by \u201cmc_mb_var_sum\u201d parameter which indicates the energy of the motion-compensated luminance residual over the entire video frame.","Extracted temporal features of a video frame, such as the length\/entropy of motion vectors and energy of frame residual, represent the amount of motion present in the video frame. Practically, MBs of a video frame are encoded differently from each other depending on how much motion a MB contains and how much similarity the MB has with the MBs in reference frame(s). A macroblock with zero motion and very low motion prediction errors can be skipped. For a macroblock with high motion prediction errors, the macroblock is best encoded in an intra mode. Other macroblocks between being skipped and being encoded in an intra mode can be encoded as P or B type of macroblocks. In one embodiment, the video feature extraction module  records information from the encoding process, including number of bit to encode motion vectors, number of bits to encode an intra macroblock texture, number of bits to encode a predictive macroblock texture (e.g., P type macroblocks), number of intra macroblocks and number of skipped macroblocks of a video frame as the temporal features.","Using the FFmpeg log file described above as an example, the extracted temporal features include \u201cmv_bits\u201d (for number of bits to encode motion vectors), \u201ci_tex_bits\u201d (for number of bits to encode an intra macroblock texture), \u201cp_tex_bits\u201d (for number of bits to encode a predictive macroblock), \u201ci_count\u201d (for number of intra macroblocks) and \u201cskip_count\u201d (for number of skipped macroblocks of a video frame).","The VCC estimation module  generates a measure of the coding complexity of the source video  using the spatial and\/or temporal features extracted by the video feature extraction module . The measure of the coding complexity of the source video  represents a measure of how complicated it is to encode the entire source video . The spatial and temporal features extracted by the video feature extraction module  represent the spatial and temporal activity of the source video frame at frame level. In one embodiment, to measure the coding complexity of the entire source video, the VCC estimation module  selects a sub-set of the extracted features from each encoded source frame (e.g., frame-level features) and transfers the selected sub-set of frame-level features into features for the entire video sequence (e.g., sequence-level features). The VCC estimation module  processes the sequence-level features and computes a VCC score for the source video  based on the processed sequence-level features.","Generally, the more bits needed to encode a source video, the more complicated the source video is. In other words, a higher bitrate of the encoded source video indicates a more complex video being encoded. To effectively represent the overall coding complexity of the source video, the extracted features selected to compute the VCC score of the source video should have a high correlation with the bitrate of the encoded source video. In one embodiment, a model training module  of the VCC engine  can perform an off-line correlation analysis using the videos in the video corpus  illustrated in . Based on the correlation analysis from encoding the videos in the video corpus , the VCC estimation module  selects frame-level spatial variance, residual energy, number of skipped macroblocks and number of bits to encode the motion vector of a predictive MB of the source video  to compute the VCC score. The off-line correlation analysis by the VCC model training module  is described below with reference to .","To transfer the frame-level spatial and temporal features into the sequence-level ones, the VCC estimation module  calculates mean spatial variance, mean residual energy, percentage of skipped macroblocks and average bits to encode the motion vector of a predictive MB of the source video  using the following equations:",{"@attributes":{"id":"p-0068","num":"0067"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":["mean_spatial","_variance","_per","_frame"],"mo":["\u2062","\u2062","\u2062"]},"mo":"=","mfrac":{"mrow":{"mi":["mb_var","_sum"],"mo":"\u2062"},"mi":"mb_num"}}},{"mrow":{"mo":["(",")"],"mn":"1"}}]}}}},"br":{}},{"@attributes":{"id":"p-0069","num":"0068"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":["mean_spatial","_variance"],"mo":"\u2062"},"mo":"=","mfrac":{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"0"},"mi":"k"},"mo":"\u2062","mrow":{"mi":["mean_spatial","_variance","_per","_frame"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mrow":{"mo":["(",")"],"mi":"i"}}},"mi":"k"}}},{"mrow":{"mo":["(",")"],"mn":"2"}}]}}}},"br":{},"b":"150"},{"@attributes":{"id":"p-0070","num":"0069"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":["mean_residual","_energy","_per","_frame"],"mo":["\u2062","\u2062","\u2062"]},"mo":"=","mfrac":{"mrow":{"mi":["mc_mb","_var","_sum"],"mo":["\u2062","\u2062"]},"mi":"mb_num"}}},{"mrow":{"mo":["(",")"],"mn":"3"}}]}}}},"br":{}},{"@attributes":{"id":"p-0071","num":"0070"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":["mean_residual","_energy"],"mo":"\u2062"},"mo":"=","mfrac":{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"0"},"mi":"k"},"mo":"\u2062","mrow":{"mi":["mean_residual","_energy","_per","_frame"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mrow":{"mo":["(",")"],"mi":"i"}}},"mi":"k"}}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}}}}},{"@attributes":{"id":"p-0072","num":"0071"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":{"mi":["percentage_skip","_mb"],"mo":"\u2062"},"mo":"=","mfrac":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"0"},"mi":"k"},"mo":"\u2062","mrow":{"mi":"skip_count","mo":"\u2062","mrow":{"mo":["(",")"],"mi":"i"}}},{"mi":["k","mb_num"],"mo":"*"}]}},"mo":";"}},{"mrow":{"mo":["(",")"],"mn":"5"}}]}}}},"br":{}},{"@attributes":{"id":"p-0073","num":"0072"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":["mean_mv","_bits"],"mo":"\u2062"},"mo":"=","mfrac":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"0"},"mi":"j"},"mo":"\u2062","mrow":{"mi":"mv_bits","mo":"\u2062","mrow":{"mo":["(",")"],"mi":"i"}}},{"mi":["j","mb_num"],"mo":"*"}]}}},{"mrow":{"mo":["(",")"],"mn":"6"}}]}}}},"br":{},"b":"150"},"The VCC estimation module  processes the sequence-level spatial and\/or temporal features for generating the coding complexity score of the source video . As a source video can be potentially transcoded into various resolutions and frame rates, the coding complexity of the source video is computed at a predetermined reference resolution and frame rate in order to represent the complexity of the video content of the source video. In one embodiment, the VCC estimation module  normalizes the sequence-level spatial and temporal features using 320\u00d7240 pixels as the predetermined reference resolution and 29.97 frames per second (fps) as the reference frame rate. In practice, the VCC estimation module  may simply normalize the sequence-level spatial and temporal features based on the reference frame rate because different resolutions do not cause significant changes to the sequence-level spatial and temporal features calculated by the equations (1) to (6) above.","For example, given a frame rate of the source video , fps(source), and the reference frames rate of 29.97 fps, the VCC estimation module  normalizes the mean number of bits to encode the motion vectors of the source video  using the following equation:\n\nnormalized_mean_bits=mean_bits*(source)\/29.97.\u2003\u2003(7)\n","The VCC estimation module  also normalizes texture energy of the source video  using the reference frame rate. The texture energy of the source video  represents the spatial activity of the source video  and can be represented by the mean_spatial_variance calculated by equation (2) above or the mean_residual_energy calculated by equation (4) above. Given a frame rate of the source video , fps(source), and the reference frames rate of 29.97 fps, the VCC estimation module  normalizes the texture energy of the source video  as following:\n\nnormalized_texture_energy_spatial=mean_spatial_variance*(source)\/129.97;\u2003\u2003(8a)\n\nnormalized_texture_energy_residual=mean_residual_energy*(source)\/29.97.\u2003\u2003(8b)\n","To keep the coding complexity estimation process efficient, such as linear computing time of complexity estimation, the VCC estimation module  applies a log function (e.g., log) to the normalized texture energy and generates scaled texture energy. To further limit the scaled texture energy to be within a maximum threshold, the VCC estimation module  applies a clip function (e.g., clip_T, where T is the maximum threshold value) to the scaled texture energy. The clip function sets the value of the scaled texture energy to its original values if the scaled texture energy is smaller or equal to the maximum threshold value T, otherwise, the clip function set the value of the scaled texture energy to T.","The spatial activity of the source video  can be represented by the normalized_texture_energy_residual or the normalized_texture_energy_spatial of the source video . The VCC estimation module  classifies the encoding mode of the source video  into two modes: inter-dominant mode and intra-dominant mode. The encoding mode classification is based on the comparison between normalized_texture_energy_residual and the normalized_texture_energy_spatial of the source video . Responsive to the normalized_texture_energy_spatial is larger than normalized_texture_energy_residual, the encoding mode of the source video  is classified as inter-dominant mode, otherwise, the encoding mode of the source video  is classified as intra-dominant mode.","Responsive to the source video  being encoded into the intermediate video format in inter-dominant mode, the VCC estimation module  computes the VCC score of the source video  using the following equation (9a):",{"@attributes":{"id":"p-0080","num":"0079"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"mi":["vcc_score","_inter"],"mo":"\u2062"},{"msub":{"mi":"\u03b1","mn":"1"},"mo":["+","\u2062"],"mi":{},"mrow":{"msub":[{"mi":"\u03b1","mn":"2"},{"mi":"clip_T","mn":"1"}],"mo":["*","\u2062"],"mrow":{"mo":"(","mrow":{"msub":{"mi":"log","mn":"10"},"mo":"(","mrow":{"mi":["normalized_texture","_energy","_"],"mo":["\u2062","\u2062"]}}}}}],"mo":"="}}},{"mtd":{"mrow":{"mrow":{"mi":{},"mo":["\u2062",")"],"mrow":{"mi":"residual","mo":"+","mn":"1"}},"mo":")"}}},{"mtd":{"mrow":{"mrow":[{"mo":["+","\u2062"],"mi":{},"msub":{"mi":"\u03b1","mn":"3"}},{"mo":["(",")"],"mrow":{"mi":["normalized_mean","_mv","_bits"],"mo":["\u2062","\u2062"]}}],"mo":["*","\u2062"],"msub":{"mi":"clip_T","mn":"2"}}}},{"mtd":{"mrow":{"mrow":{"mrow":{"mo":["+","\u2062"],"mi":{},"msub":{"mi":"\u03b1","mn":"4"}},"mo":["*","\u2062"],"mi":["percentag_skip","_mb"]},"mo":";"}}}]}}},"br":{},"sub":["1","2","3","4","1","2 ","1 ","2 ","1","2","1","2","3","4","1","2","3","4 "],"b":["160","200"],"figref":"FIG. 1B"},"Responsive to the source video  being encoded into the intermediate video format in intra-dominant mode, the VCC estimation module  computes the VCC score of the source video  using the following equation (9b):",{"@attributes":{"id":"p-0082","num":"0081"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"mi":["vcc_score","_intra"],"mo":"\u2062"},{"msub":{"mi":"\u03b2","mn":"1"},"mo":["+","\u2062"],"mi":{},"mrow":{"msub":[{"mi":"\u03b2","mn":"2"},{"mi":"clip_T","mn":"3"}],"mo":["*","\u2062"],"mrow":{"mo":"(","mrow":{"msub":{"mi":"log","mn":"10"},"mo":"(","mrow":{"mi":["normalized_texture","_"],"mo":"\u2062"}}}}}],"mo":"="}}},{"mtd":{"mrow":{"mrow":{"mi":{},"mo":["\u2062",")"],"mrow":{"mi":"engery_spatial","mo":"+","mn":"1"}},"mo":")"}}},{"mtd":{"mrow":{"mrow":[{"mo":["+","\u2062"],"mi":{},"msub":{"mi":"\u03b2","mn":"3"}},{"mo":["(",")"],"mrow":{"mi":["normalized_mean","_mv","_bits"],"mo":["\u2062","\u2062"]}}],"mo":["*","\u2062"],"msub":{"mi":"clip_T","mn":"4"}}}},{"mtd":{"mrow":{"mrow":{"mrow":{"mo":["+","\u2062"],"mi":{},"msub":{"mi":"\u03b2","mn":"4"}},"mo":["*","\u2062"],"mi":["percentag_skip","_mb"]},"mo":";"}}}]}}},"br":{},"sub":["1","2","3","4","3","4 ","3 ","4 ","3","4","1","2","3","4","1","2","3","4 "],"b":["160","200"],"figref":"FIG. 1B"},"To effectively represent the overall coding complexity of a source video, the extracted features from the source video selected to compute the VCC score of the source video should have a high correlation with the bitrate of the encoded source video. For example, the VCC engine  can encode videos in the video corpus  and analyze the correlations between each of the spatial and temporal features of the encoded videos in the video corpus and their corresponding bitrates.  illustrates a VCC model training module  of the video coding complexity engine . The VCC model training module  is configured to select a video set, such as 20,000 videos, from the video corpus  and to train a VCC model using a linear regression algorithm. The VCC model training module  comprises an intermediate video encoder , a video feature extraction module , a normalization module  and a training module .","The intermediate video encoder  is similar as the video encoder  of , but encodes the source video into an output video at a reference resolution (e.g., 320\u00d7240) and a reference frame rate (e.g., 29.97 fps). The video feature extraction module  is similar as video feature extraction module  of . The normalization module  receives the encoding bitrate of a video encoded by the intermediate video encoder  and normalizes the encoding bitrate to a predetermined range. For example, the predetermined range of the normalization is between a numeric zero and one. The normalized bitrate by the normalization module is used by the training module  to approximate the VCC score of a source video calculated by the VCC calculation module .","The training module  receives video features extracted by the video feature extraction module  and the normalized bitrate by the normalization module for a video encoded by the intermediate video encoder . In one embodiment, a linear regression algorithm is used to model the relationship between the selected video feature and the normalized bitrate. For example, the training module  uses the \u201cregress\u201d function provided by MatLab software. The result from the training module  is a trained VCC model that is used by the VCC calculation module  to calculate the VCC score of a given source video.","Turning now to ,  is a flow chart of estimating video coding complexity of a source video using the video coding complexity engine  illustrated in . Initially, the video coding complexity engine  receives  a source video in a native video format (e.g., MPEG-2) and encodes  the source video by the intermediate video encoder  into an intermediate video format (e.g., H.263). The intermediate video encoder  also creates a log file to store the coding information of each video frame of the encoded source video and\/or the video coding parameters of the original source video. The video feature extraction module  reads the log file and extracts  frame-level spatial and\/or temporal features from the encoded source video. The VCC estimation module  transfers  the frame-level video features to sequence-level features. The VCC estimation module  selects  a sub-set of sequence-level video features to be used for the VCC score calculation. The VCC estimation module  further normalizes  the selected video features based on a references resolution (e.g., 320\u00d7240 pixels) and\/or a reference frame rate (e.g., 29.97 fps). To keep the VCC score estimation process efficient and bounded within a predetermined maximum threshold, the VCC estimation module  scales and clips  the normalized video features. Finally, the VCC score estimation module  computes  the video coding complexity score based on a determination of encoding mode classification (e.g., inter-dominant or intra-dominant). The computed VCC score of the source video is used by other modules of the adaptive transcoding system  for transcoding the source video.","III. VCC-Based Rate-Distortion Modeling","One challenge in designing an effective adaptive transcoding system for a large video corpus with millions of video containing various contents, is to efficiently distribute bits among the videos in the video corpus within an acceptable cost limit. Given a target resolution and frame rate, a video's encoding quality is determined by its bitrate. However, as the coding complexity of each video in a large video corpus is very likely to be different from each other, the same bitrate applied to the videos in the video corpus leads to different encoding quality of the videos. To optimize the average encoding quality of the video corpus under a total egress bandwidth constraint (e.g., the average bitrate of the video corpus), the adaptive transcoding system  is configured to determine the target bitrate of each video in the video corpus using statistical learning and modeling on the video corpus based upon the use of a video coding complexity metric, such as VCC.",{"@attributes":{"id":"p-0088","num":"0087"},"figref":["FIG. 5","FIG. 4"],"b":["400","410","400","510","420","400","520","530","540","420","400","560","570"]},{"@attributes":{"id":"p-0089","num":"0088"},"figref":["FIG. 4","FIGS. 1A and 1B"],"b":["400","102","400","410","420","430","440","400","108","430","440","400","108"]},"The R-D modeling manager  randomly selects a video set, for example, 20,000 videos, from the video corpus stored in the video database  and provides the selected video set to the video encoder . The video encoder  encodes each of the video in the selected video set into multiple different versions at multiple different bitrates, resolutions or frame rates. For each encoded video, the video encoder  collects multiple coding parameters (e.g., quantization step size) and multiple pairs of rate-distortion data corresponding to a bitrate, resolution or frame rate. The collected rate-distortion data are sent by the video encoder  to train the R-D model estimation module  and the scaling model estimation module . In one embodiment, the video encoder  is a H.264 video encoder. Other video codecs known to those of ordinary skills in the art are readily available for encoding the selected video set.","Given a source video, a pair of rate-distortion data defines a trade-off between minimizing the bitrate of the encoded video and keeping the distortion in the reconstructed (i.e., encoded and decoded) video small. The distortion measures the difference between the reconstructed video and the original source video. For example, assuming that the source video follows Laplacian distribution, the distortion measure of a pixel of the source video D can be defined as D(x-x\u2032)=abs(x-x\u2032), where x is a pixel of the source video, x\u2032 is the reconstructed pixel of the original pixel x, and abs(x-x\u2032) is the absolute difference between the reconstructed pixel x\u2032 and the original pixel x.","The bitrate of a video being encoded is defined as the average number of bits used to represent the length of the video for a second. The bitrate required to encode a video into a pre-defined quality depends on both the video content complexity of the video and the sampling characteristics (e.g., resolutions and frame rates) of the video. The pre-defined quality of a video can be established by the peak-signal-to-noise-ratio (PSNR) of the reconstructed video. A higher PSNR of a video indicates a better quality of the reconstructed source video. However, a particular value of PSNR of a video does not necessarily equate to an \u201cabsolute\u201d subjective quality of the reconstructed video. Alternatively, quantization parameter, such as quantization step size, of a video is often used by various video codecs to define the quality of the reconstructed video. A quantizer maps a signal of the source video with a range of values to a quantized signal with a reduced range of values.","Given a target resolution and frame rate of a video, the tradeoff between distortion and rate in coding the video can be represented by a rate-distortion function R(D). The rate distortion function R(D) specifies the lowest bitrate at which a video can be encoded while keeping the distortion less than or equal to D. For example, assuming that the video follows Laplacian distribution, the distortion measure of the video D being defined as D(x-x\u2032)=abs(x-x\u2032), a closed-form solution for the R(D) function is:\n\n()=ln(1),\u2003\u2003(10)\n\nwhere a is a pre-determined constant. Expanded to a Taylor series, the R(D) function of equation (10) for encoding a single unit, such as a frame or a MB of the video, becomes:\n\n()=+ . . . \u2003\u2003(11)\n","The R(D) function of equation (11) modified in terms of quantization step size and absolute difference between the video and the reconstructed video is defined by the following equation:\n\n()=\u2003\u2003(13)\n\nwhere Q is the quantization step size for each frame of the video, s is the predicted mean absolute difference between a frame or MB of the video and the frame or MB of the reconstructed video, h denotes the bits used to encode header and motion vectors of each frame of the video.\n","To estimate the bitrate to encode an entire source video based on the video coding complexity of the source video, the R(D) function of equation (13) is modified to include the video coding complexity as follows:\n\n()=()*\u2003\u2003(14)\n\nwhere Q\u2032 is the average quantization step size across the encoded output of the source video, VCC indicates the video coding complexity of the source video, and a the b are pre-determined modeling parameters. Compared with s of the R(D) function of equation (13) for coding complexity of a frame or MB of the source video, VCC of the R(Q\u2032,VCC) function of equation (14) captures the coding complexity of the source video. Further, rate R of equation (14) to encode a source video is linearly proportional to the video coding complexity of the whole source video and inversely proportional to the averaged quantization step size of the source video. In other words, more bits are required to encode a more complicated source video and more bits are required to encode a source video with a smaller quantization step size.\n","For example, a source video is encoded using a H.264 codec with H.264 main profile as the compression algorithm using the R(D) function of equation (14). For each frame of the video, the quantization step size Q is calculated as Q=0.25*2, where qp is the quantization parameter defined in the H.264 coding standard. The average quantization step size of the entire video Q\u2032 is the quantization step size Q of each frame of the video averaged over the entire video. VCC is the coding complexity score of the video calculated by the video coding complexity engine  illustrated in . The pre-determined R-D model parameters a and b are a=2238.79 and b=193.33 in one embodiment.","The R-D model estimation module  uses the multiple pairs of rate-distortion data corresponding to a bitrate, resolution or frame rate collected by the video encoder  to train a R-D model at a reference resolution and reference frame rate. Specifically, the R-D model estimation module  uses a multi-variance linear regression algorithm to train the R-D model given in equation (14) above to estimate the model parameters a and b. In one embodiment, the multi-variance linear regression algorithm is ordinary least squares (OLS) algorithm. Other embodiments may use other multi-variance linear regression algorithms.","Similarly, the scaling model estimation module  uses the multiple pairs of rate-distortion data corresponding to a bitrate, resolution or frame rate collected by the video encoder  to train a scaling model that scales the bitrate from encoding at a reference resolution\/reference frame rate to a different target resolution\/frame rate. One or more modeling parameters for the scaling model are obtained through the training described below.","The scaling model estimation module  communicates with the R-D model estimation module  to scale the bitrates calculated by the R-D model estimation module  with a reference resolution and\/or frame rate to a different target resolution and\/or frame rate. In one embodiment, the reference resolution is 320\u00d7240 pixels, and the reference frame rate is 29.97 fps. Other embodiments can use other reference resolutions and\/or reference frame rates.","Given a reference bitrate, reference_bitrate, calculated by the R-D model estimation module , a reference resolution, reference_resolution, and a target resolution of the transcoded video, target_resolution, the scaling model estimation module  scales the reference bitrate to obtain a scaled bitrate for the source video as follows:\n\nresolution_ratio=target_resolution\/reference_resolution\u2003\u2003(15a)\n\nbitrate(resolution)=reference_bitrate*resolution_ratio,\u2003\u2003(15b)\n\nwhere c is a modeling parameter and c=0.7.\n","Similarly, given a reference frame rate, reference_fps and an output frame rate of the transcoded video, target_fps, the scaling model estimation module  scales the reference bitrate to obtain a scaled bitrate for the source video as follows:\n\n_ratio=target\/reference\u2003\u2003(16a)\n\nbitrate()=reference_bitrate*(\u03b1\/(_ratio*_ratio)+\u03b1_ratio+\u03b1)\u2003\u2003(16b)\n\nwhere \u03b1, \u03b1, \u03b1are modeling parameters obtained through the training of the scaling model estimation module . In one embodiment, \u03b1=\u22120.0164, \u03b1=\u22120.0182, \u03b1=1.0330.\n\nIV. Adaptive Bitrate Transcoding Sub-System\n",{"@attributes":{"id":"p-0102","num":"0101"},"figref":["FIG. 6","FIGS. 1A and 1B"],"b":["600","102","600","602","602","600","602","602","600","602","602","600","602","602","400","600","602","602","602","602","602","600"]},"In one embodiment, the adaptive bitrate transcoding sub-system  comprises a target bitrate estimation module , a bitrate refinement module  and an adaptive video encoder . The adaptive bitrate transcoding sub-system  communicates with the video coding complexity engine  and the video R-D modeling engine . The video coding complexity engine  computes the video coding complexity score of the source video . For example, the video coding complexity engine  encodes the source video  into an intermediate video format and extracts a plurality of frame-level spatial and\/or temporal features (e.g., sum of variance of intra frames and motion-compensated frames) from a log file created from encoding the source video . The video coding complexity engine  further transfers the frame-level spatial and\/or temporal features to sequence-level ones using equations such as (1)-(6). Depending on which encoding mode (e.g., inter-dominant or intra-dominant) is used to encode the source video, the video coding complexity engine  computes the video coding complexity score of the source video  using equation (9a) or (9b).","The video R-D modeling engine  provides a trained R-D model and a scaling model for the source video  to the adaptive bitrate transcoding sub-system . The R-D model and the scaling model are trained over multiple sets of videos selected from a large video corpus by the video R-D modeling engine . For example, the video R-D modeling engine  provides a R-D model defined by the R(D) function of equation (14) R(Q\u2032,VCC)=(a\/Q\u2032+b)*VCC, where the coding constants a and b are obtained through the model training and a=2238.79 and b=193.33. The video R-D modeling engine  also provides a scaling model defined by the equations (15a-b) and (16a-b). The target coding parameters, such as target quantization step size, frame rate and resolution, of the source video  to the adaptive bitrate transcoding sub-system , in one embodiment, are design parameters. Frame rate and resolution can also be adaptively adjusted using an adaptive resolution transcoding subsystem, such as the subsystem  in .","The target bitrate estimation module  receives the video complexity score of the source video  from the video coding complexity engine  and\/or the target quantization step size of the source video  from the video R-D modeling engine  and computes the initial target bitrate Rusing the equation (14) as follows:\n\n()=()*\n\nwhere the coding constants a=2238.79 and b=193.33, VCC is the video coding complexity score of the source video  and Q\u2032 is the target quantization step size of the source video .\n","The bitrate refinement module  refines the initial target bitrate Rby scaling the initial target bitrate Rwith respect to the target resolution of the source video  and a reference resolution using the equations (15a-b) as follows:\n\nresolution_ratio=target_resolution\/reference_resolution\n\n(resolution)=*resolution_ratio\n\nwhere target_resolution is the output (or target) resolution of the source video  after transcoding and the reference_resolution is 320\u00d7240 pixels.\n","Similarly, the bitrate refinement module  refines the initial target bitrate Rby scaling the initial target bitrate Rwith respect to the target frame rate of the source video  and a reference frame rate using the equations (16a-b) as follows: as follows:\n\n_ratio=target\/reference\n\n()=*(\u03b1\/(_ratio*_ratio)+\u03b1_ratio+\u03b1)\n\nwhere \u03b1, \u03b1, \u03b1are scaling constants and \u03b1=\u22120.0164, \u03b1=\u22120.0182, \u03b1=1.0330. target-fps is the output frame rate of the source video  after transcoding and the reference_fps is 29.97 fps.\n","The bitrate refinement module  can refine the initial target bitrate sequentially with both target resolution and frame rate of the source video . In one embodiment, the bitrate refinement module  refines the initial target bitrate first with the target resolution of the source video  followed by the target frame rate of the source video . In another embodiment, the bitrate refinement module  refines the initial target bitrate first with the target frame rate of the source video  followed by the target resolution of the source video . The adaptive video encoder  receives the refined initial bitrate from the bitrate refinement module  and encodes the source video  using the refined initial bitrate.",{"@attributes":{"id":"p-0109","num":"0108"},"figref":["FIG. 7","FIG. 6"],"b":["600","600","710","720","600","730","400","600","740","600","750","600","760","600","770","780","750","760"]},"V. Adaptive Resolution Transcoding Sub-System",{"@attributes":{"id":"p-0110","num":"0109"},"figref":["FIG. 8","FIGS. 1A and 1B"],"b":["800","102","800","802","802","800","802","800","802","802","200","802","802","800","802","800","802","802","800","802","802"]},"In one embodiment, the adaptive resolution transcoding sub-system  comprises a resolution determination module  and an adaptive video encoder . The adaptive resolution transcoding sub-system  communicates with the video coding complexity engine . The resolution determination module  receives the video coding complexity score of the source video  and the content header information of the source video  from the video coding complexity engine . For each video output format supported by the adaptive resolution transcoding sub-system , the resolution determination module  sets a resolution adjustment level for the source video  based on the video coding complexity score and the content header information of the source video , and selects a resolution for the source video  from a predetermined list of resolutions based on the resolution adjustment level. The determination module  compares the selected resolution with the original resolution of the source video and determines the optimal resolution of the source video based on the comparison. The resolution determination module  further receives a user selection of a user input , such as user selection of a video output format of the source video , and selects the optimal resolution for the source video  based on the user selection and outputs the selected optimal resolution for the source video  to the adaptive video encoder . The adaptive video encoder  encodes the source video  with the received optimal resolution.","To further illustrate the functionality of the resolution determination module , the following pseudo-code represents one embodiment of setting the resolution adjustment level for a source video based on its video coding complexity and selects a resolution for the source video based on the resolution adjustment level:",{"@attributes":{"id":"p-0113","num":"0112"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"7pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"203pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"7pt","align":"left"}}],"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\/\/predetermined list of resolutions\/\/static struct TargetVideoResolution {",{}]},{"entry":[{},"int width;",{}]},{"entry":[{},"int height;",{}]},{"entry":[{},"} kTargetVideoResolution {",{}]},{"entry":[{},"\u2003\u2003\u2003{320, 240},",{}]},{"entry":[{},"\u2003\u2003\u2003{480, 360},",{}]},{"entry":[{},"\u2003{640, 480},",{}]},{"entry":[{},"};",{}]},{"entry":[{},"\/\/setting resolution adjustment levels\/\/",{}]},{"entry":[{},"int TranscoderUtil::AdjustMaxTargetResoltion {",{}]},{"entry":[{},"float VCC;",{}]},{"entry":[{},"int32 video_output_format;",{}]},{"entry":[{},"int rLevel;",{}]},{"entry":[{},"switch (video_output _format) {",{}]},{"entry":[{},"\u2003case (contentHeader::FLV_h264_HQ):",{}]},{"entry":[{},"\u2003\u2003IF 0 \u2266 VCC \u2266 0.2 {",{}]},{"entry":[{},"\u2003\u2003\u2003rLevel = 2;",{}]},{"entry":[{},"\u2003\u2003\u2003RETURN rLevel;",{}]},{"entry":[{},"\u2003\u2003\u2003\u2003}",{}]},{"entry":[{},"\u2003ELSE IF VCC \u2266 0.3 {",{}]},{"entry":[{},"\u2003\u2003\u2003rLevel = 1;",{}]},{"entry":[{},"\u2003\u2003\u2003RETURN rLevel;",{}]},{"entry":[{},"\u2003}",{}]},{"entry":[{},"\u2003break;",{}]},{"entry":[{},"\u2003\u2003\u2003case (contentHeader::FLV_320_Normal):",{}]},{"entry":[{},"\u2003IF 0 \u2266 VCC \u2266 0.1 {",{}]},{"entry":[{},"\u2003\u2003\u2003rLevel =2;",{}]},{"entry":[{},"\u2003\u2003\u2003RETURN rLevel;",{}]},{"entry":[{},"\u2003}",{}]},{"entry":[{},"\u2003ELSE IF VCC \u2266 0.2 {",{}]},{"entry":[{},"\u2003\u2003\u2003rLevel = 1;",{}]},{"entry":[{},"\u2003\u2003\u2003RETURN rLevel;",{}]},{"entry":[{},"\u2003}",{}]},{"entry":[{},"\u2003break;",{}]},{"entry":[{},"\u2003DEFAULT:",{}]},{"entry":[{},"\u2003\u2003\u2003Break;",{}]},{"entry":[{},"\u2003\u2003\u2003}",{}]},{"entry":[{},"\u2003rLevel = 0;",{}]},{"entry":[{},"\u2003RETURN rLevel;",{}]},{"entry":[{},"}"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}}}},"The parameter video_output_format represents one of the video output formats supported by the adaptive resolution transcoding sub-system , including high resolution and high quality video format such as high definition (HD of 1280\u00d7720 pixels resolution) and normal resolution and normal quality video format such as standard definition (SD of 640\u00d7480 pixels resolution). The parameter contentHeader contains the content header information of the source video. In the exemplary pseudo-code above, contentHeader can be FLV_h264_HQ corresponding to high resolution and high quality video output format, or FLV320_Normal corresponding to default normal resolution and quality video output format. Threshold values (e.g., 0.1, 0.2 and 0.3) compared with the video coding complexity score above are predetermined. In one embodiment, the threshold values are obtained from experiments on a large video corpus of a video hosting service. The list of the target video resolutions, kTargetVideoResolution, is illustrated as an example. Higher resolutions, such as 720p (i.e., 1280\u00d7720 pixels, progressive), can also be included in the list.","For example, the resolution determination module  receives a source video having a video coding complexity score of 0.4, original resolution in its native format of 640\u00d7480 pixels and content header of contentHeader::FLV320_Normal. The video coding complexity score of 0.4 indicates that there is relatively high complexity to encode the source video. The resolution determination module  sets the resolution adjustment level to level 0 based on the coding complexity score and the content header information, and selects the resolution of 320\u00d7240 pixels from the list of target resolutions based on the resolution adjustment level. The resolution determination module  compares the selected resolution (320\u00d7240 pixels) with the original resolution (640\u00d7480 pixels) of the source video, and sets the optimal resolution of the source video as the selected resolution (320\u00d7240 pixels).","The resolution determination module  determines an optimal output resolution for a source video based on the native resolution, content header information and the VCC score of the source video. The optimal output resolution represents the most suitable resolution to transcode the source video based on its coding parameters such as content complexity and its native resolution. To further illustrate the resolution determination module , Table I lists some examples of the optimal output resolution for a source video in its native resolution from the resolution determination process.",{"@attributes":{"id":"p-0117","num":"0116"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE I"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Optimal Output Resolution"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"56pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"49pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Content Header of",{},"Source Video ","Optimal Output"]},{"entry":["Source Video","VCC score","Native Resolution","Resolution"]},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}},{"entry":["FLV_h264_HQ","0 \u2266 VCC \u2266 0.2","{320, 240}","{320, 240)"]},{"entry":[{},{},"{480, 360}","{480, 360)"]},{"entry":[{},{},"{640, 480}","{640, 480}"]},{"entry":[{},"0.2 \u2266 VCC \u2266 0.3","{320, 240}","{320, 240}"]},{"entry":[{},{},"{480, 360}","{480, 360}"]},{"entry":[{},{},"{640, 480}","{480, 360)"]},{"entry":["FLV_320_Normal","0 \u2266 VCC \u2266 0.1","{320, 240}","{320, 240}"]},{"entry":[{},{},"{480, 360}","{480, 360}"]},{"entry":[{},{},"{640, 480}","{640, 480}"]},{"entry":[{},"0.1 \u2266 VCC \u2266 0.2","{320, 240}","{320, 240}"]},{"entry":[{},{},"{480, 360}","{480, 360}"]},{"entry":[{},{},"{640, 480}","{480, 360}"]},{"entry":["Default","VCC","{320, 240}","{320, 240}"]},{"entry":[{},{},"{480, 360}","{320, 240}"]},{"entry":[{},{},"{640, 480}","{320, 240}"]},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}}]}}]}}},"To simplify an implementation of the adaptive resolution transcoding sub-system , a source video with original resolution in its native format that is lower than the optimal resolution determined by the resolution determination module  is encoded with its original resolution. For example, the resolution determination module  receives a source video having a video coding complexity score of 0.1, original resolution in its native format of 320\u00d7240 pixels and content header of contentHeader::FLV_h264_HQ. The video coding complexity score of 0.1 indicates that the source video has simple content, such as a talking head against a static background whiteboard. The resolution determination module  sets the resolution adjustment level to level 2 based on the coding complexity score and selects the resolution of 640\u00d7480 pixels from the list of target resolutions based on the resolution adjustment level. The resolution determination module  compares the original resolution (e.g., 320\u00d7240 pixels) of the source video with the selected resolution (640\u00d7480 pixels). The original resolution is lower than the selected resolution. The resolution determination module  sets the optimal resolution for the source video as the original resolution (320\u00d7240 pixels). The source video is encoded with its original resolution.",{"@attributes":{"id":"p-0119","num":"0118"},"figref":["FIG. 9","FIG. 8"],"b":["800","800","910","920","200","800","930","810","800","940","810","950","960","810","970","820","820","980"]},{"@attributes":{"id":"p-0120","num":"0119"},"figref":["FIG. 10","FIG. 9"],"b":["950","810","1010","1020","810","1030","810","1040","810","1050","810","1060"]},{"@attributes":{"id":"p-0121","num":"0120"},"figref":"FIG. 11","b":["1110","800","800","800","1110","1120","1130","1110","1120"]},"The above description is included to illustrate the operation of the preferred embodiments and is not meant to limit the scope of the invention. The scope of the invention is to be limited only by the following claims. From the above discussion, many variations will be apparent to one skilled in the relevant art that would yet be encompassed by the spirit and scope of the invention.","The present invention has been described in particular detail with respect to one possible embodiment. Those of skill in the art will appreciate that the invention may be practiced in other embodiments. First, the particular naming of the components, capitalization of terms, the attributes, data structures, or any other programming or structural aspect is not mandatory or significant, and the mechanisms that implement the invention or its features may have different names, formats, or protocols. Further, the system may be implemented via a combination of hardware and software, as described, or entirely in hardware elements. Also, the particular division of functionality between the various system components described herein is merely exemplary, and not mandatory; functions performed by a single system component may instead be performed by multiple components, and functions performed by multiple components may instead performed by a single component.","Some portions of above description present the features of the present invention in terms of algorithms and symbolic representations of operations on information. These algorithmic descriptions and representations are the means used by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art. These operations, while described functionally or logically, are understood to be implemented by computer programs. Furthermore, it has also proven convenient at times, to refer to these arrangements of operations as modules or by functional names, without loss of generality.","Unless specifically stated otherwise as apparent from the above discussion, it is appreciated that throughout the description, discussions utilizing terms such as \u201cprocessing\u201d or \u201ccomputing\u201d or \u201ccalculating\u201d or \u201cdetermining\u201d or \u201cdisplaying\u201d or the like, refer to the action and processes of a computer system, or similar electronic computing device, that manipulates and transforms data represented as physical (electronic) quantities within the computer system memories or registers or other such information storage, transmission or display devices.","Certain aspects of the present invention include process steps and instructions described herein in the form of an algorithm. It should be noted that the process steps and instructions of the present invention could be embodied in software, firmware or hardware, and when embodied in software, could be downloaded to reside on and be operated from different platforms used by real time network operating systems.","The present invention also relates to an apparatus for performing the operations herein. This apparatus may be specially constructed for the required purposes, or it may comprise a general-purpose computer selectively activated or reconfigured by a computer program stored on a computer readable medium that can be accessed by the computer. Such a computer program may be stored in a computer readable storage medium, such as, but is not limited to, any type of disk including floppy disks, optical disks, CD-ROMs, magnetic-optical disks, read-only memories (ROMs), random access memories (RAMs), EPROMs, EEPROMs, magnetic or optical cards, application specific integrated circuits (ASICs), or any type of media suitable for storing electronic instructions, and each coupled to a computer system bus. Furthermore, the computers referred to in the specification may include a single processor or may be architectures employing multiple processor designs for increased computing capability.","The algorithms and operations presented herein are not inherently related to any particular computer or other apparatus. Various general-purpose systems may also be used with programs in accordance with the teachings herein, or it may prove convenient to construct more specialized apparatus to perform the method steps. The structure for a variety of these systems will be apparent to those of skill in the, along with equivalent variations. In addition, the present invention is not described with primary to any particular programming language. It is appreciated that a variety of programming languages may be used to implement the teachings of the present invention as described herein, and any reference to specific languages are provided for disclosure of enablement and best mode of the present invention.","The present invention is well suited to a wide variety of computer network systems over numerous topologies. Within this field, the configuration and management of large networks comprise storage devices and computers that are communicatively coupled to dissimilar computers and storage devices over a network, such as the Internet.","Finally, it should be noted that the language used in the specification has been principally selected for readability and instructional purposes, and may not have been selected to delineate or circumscribe the inventive subject matter. Accordingly, the disclosure of the present invention is intended to be illustrative, but not limiting, of the scope of the invention, which is set forth in the following claims"],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE FIGURES","p":[{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 1A"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":["FIG. 1B","FIG. 1A"]},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 2A"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 2B"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 2C"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":["FIG. 3","FIG. 2"]},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":["FIG. 5","FIG. 4"]},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":["FIG. 7","FIG. 6"]},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":["FIG. 9","FIG. 8"]},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 11"}]},"DETDESC":[{},{}]}
