---
title: Nonvolatile media journaling of verified data sets
abstract: The storage of data sets in a storage set (e.g., data sets written to hard disk drives comprising a RAID array) may diminish the performance of the storage set through non-sequential writes, particularly if the storage devices promptly write data sets that are followed by sequentially following data sets. Additionally, storage sets may exhibit inconsistencies due to non-atomic writes of data sets and verifiers (e.g., checksums) and an intervening failure, such as an occurrence of the RAID write hole. Instead, data sets and verifiers may first be written to a stored on the nonvolatile media of a storage device before being committed to the storage set. Such writes may be sequentially written to the journal, irrespective of the locations of the data sets in the storage set; and recovery of a failure may simply involve re-committing the consistent records in the journal to correct incomplete writes to the storage set.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09229809&OS=09229809&RS=09229809
owner: Microsoft Technology Licensing LLC
number: 09229809
owner_city: Redmond
owner_country: US
publication_date: 20110911
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION","A. Introduction","B. Presented Techniques","C. Exemplary Embodiments","D. Variations","E. Computing Environment","F. Usage of Terms"],"p":["Within the field of computing, many scenarios involve the storage of data on one or more nonvolatile storage devices (e.g., platter-based magnetic and\/or optical hard disk drives, solid-state storage devices, and nonvolatile memory circuits). Many details of the data storage may vary, such as the word size, the addressing method, the partitioning of the storage space of the storage device into one or more partitions, and the exposure of allocated spaces within the storage device as one or more volumes within a computing environment.","In many such storage scenarios, techniques may be utilized to detect unintended changes to the data. For example, an error in the reading or storing logic of the device, a buffer underrun or overrun, a flaw in the storage medium, or an external disruption (such as a cosmic ray) may occasionally cause an inadvertent change in the data stored on the storage medium or in the reading of data from the storage medium. Therefore, in many such scenarios, the data is stored on the storage devices according to an error detection scheme involving a verifier (e.g., a parity bit or checksum) computed for respective data sets (e.g., different words, sectors, regions, or other sets of data). The verifier may be used to confirm that the contents of the data set have been validly stored to and\/or read from the storage device. As one such example, in the context of storing a data set comprising a set of bits, an exclusive OR (XOR) operation may be applied to the bits, resulting in a parity bit that may be stored and associated with this data set. When the data set is later read, another XOR operation may be applied thereto, and the result may be compared with the parity bit. A change of any one bit results in a mismatch of these XOR computations, indicating that the data has been incorrectly stored, altered, or incorrectly read from the storage device. Many types of verifiers may be identified, which may vary in some features (e.g., ease of computation, a capability of identifying which bit of the data set has changed, and an error-correction capability whereby an incorrectly read portion of data may be corrected).","Error detection schemes are often utilized in Redundant Array of Inexpensive Disks (RAID) arrays, such as a set of hard disk drives that are pooled together to achieve various aggregate properties, such as improved throughput and automatic data mirroring. As one such example, a RAID 4 array involves a set of two or more disks, where one disk is included in the array not to store user data, but to store verifiers of the data stored on the other disks. For example, for a RAID 4 array involving four disks each storing one terabyte of data, the capacity of the first three disks is pooled to form a three-terabyte storage space for user data, while the fourth disk is included in the array to hold verifiers for data sets stored on the first three disks (e.g., for every three 64-bit words respectively stored on the other three disks, the fourth disk includes a 64-bit verifier that verifies the integrity of the three 64-bit words). The RAID array controller comprises circuitry that is configured to implement the details of a selected RAID level for a provided set of drives (e.g., upon receiving a data set, automatically apportioning the data across the three user data disks, calculating the verifier of the data set, and storing the verifier on the fourth disk). The RAID techniques used may also enable additional protections or features; e.g., if any single storage device in a RAID 4 array fails, the data stored on the failed device may be entirely reconstructed through the use of the remaining storage devices.","This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key factors or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter.","The writing of data to a storage device may present several sources of inefficiency and potential problems. As a first example, a data set written to a storage device may involve a sequence of data, such as data written to a sequence of physical addresses on a storage device. By writing the data set in accordance with this sequence (e.g., as a sequential write of a contiguous block of data), the storage device may achieve faster seek times, higher throughput, and\/or reduced power consumption and physical wear due to the reduction of seek times and write operations. However, due to various circumstances, a storage device may write the sequence of data as two or more sub-sequences and may fail to achieve these efficiencies. As a first example, the request to write the data set may comprise two or more requests to write a portion of the sequence (e.g., a first request to write addresses 1,000-1,015 and a second request to write addresses 1,016 to 1,031), and the storage device may separately commit each portion of the sequence, rather than committing the entire sequence together. As a second example, the storage device may receive several write requests, and between writing a first portion of the sequence and a second portion of the sequence, may store a different data set at a different location, thereby causing two additional seeks between the first portion and the second portion. These and other circumstances may be identified as missed opportunities for efficiency gains in the performance, power efficiency, and longevity of the storage device.","A second problem that may arise while storing verifier-based storage sets involves the delay between storing a data set and its verifier (or vice versa). As a first example, many storage devices only support a write to one location at a time (e.g., the location underneath the write head of a hard disk drive, or the location specified by an address register in a solid-state storage device), and the sequential storing of data involves writing the data set before the verifier, or vice versa. As a second example, if the data set and verifier are stored on different storage devices, it may be difficult to synchronize the moment that the first storage device stores the data set with the moment that the second storage device stores the verifier of the data set. In these and other examples, storing a data set and a corresponding verifier may occur not a synchronous manner, but in a sequential manner. However, a failure of the storage device(s), such as power loss, a hardware failure, a software crash, or an unanticipated removal of a storage device from the array, may occur after storing a data set and before storing the verifier. Consequently, the verifier does not match the data represented by the verifier. This problem caused by non-atomic writes, sometimes identified as the RAID write hole, may manifest in many resulting consequences. For example, it may be difficult for the storage device(s) to determine how to remedy this error, e.g., whether the mismatch represents an incorrect verifier, an unintended change to the data set (e.g., a manifestation of the bit error rate (BER) of the storage set), or an incorrect read of either the data set or the verifier. This lack of information may even jeopardize the confidence in the accuracy of a portion of the data set that have not recently been written. For example, if one of the storage devices fails, an attempt to recover the data on the storage device from the remaining storage devices (using the incorrect verifier) may result in incorrect data reconstruction. For instance, in order to recover data from a particular volume that has been lost or corrupted and substituted with a repaired or replacement storage device, each word of data on the missing storage devices may be computed by XORing together the words at the same location (e.g., the same physical or logical address on the other devices) and the checksum for the set of words at the address, and the result yields the missing word. However, if the checksum has inadvertently changed, the XOR operation results in an incorrect result and the replacement of the word on the substituted volume with incorrect data. As another example, if a verifier C is stored for a data set comprising portions A and B, and a catastrophic failure arises while updating A and C, the computer may be able to identify a mismatch between the data set [A, B] and verifier C. This inability may undermine the confidence not only in A and C, which were participating in the writing at the moment of catastrophic failure, but also B, which may not even have been accessed in a long time.","Presented herein are techniques for reducing the risks of data loss and the protracted recovery time caused by problems such as the RAID write hole, as well as improving the efficiency of the storage set. In accordance with these techniques, on one or more storage devices of the storage set, a journal may be generated that is configured to store data sets that are to be committed to the storage set. The journal may comprise, e.g., a sequence of records structured as a loop array, where each record has capacity to store a data set and a verifier computed for the data set. All data sets to be written to the storage set may first be stored in the journal, in sequential order of receipt, along with a verifier computed for the data set.","These techniques may reduce the consequences of the RAID write hole by providing a mechanism whereby non-atomic writes may be stored on the nonvolatile memory of a storage device before being committed to the location in the storage set. If a failure occurs while the data set is being written to the journal, the version of the data set stored in the storage set remains intact; and if a failure occurs while the data set is being written to the storage set, the failure may be recovered by reinitiating the write of the data set from the journal to the storage set. Additionally, the use of a journal may improve the performance of the storage device by promoting sequential writes. As a first example, non-sequential data sets are first written sequentially to the journal, providing rapid, sequential write throughout even for non-sequential data sets. As a second example, the journal may operate as a write buffer between the write requests and the storage set, thereby enabling a coalescence of write requests comprising a sequence of data stored in contiguous physical locations of the storage device. Still further performance improvements may be achieved, e.g., by generating a volatile memory representation of the journal stored on the nonvolatile storage device, where the volatile memory representation serves as a reach cache and\/or a write buffer. These and other advantages may be achievable through the use of the techniques presented herein.","To the accomplishment of the foregoing and related ends, the following description and annexed drawings set forth certain illustrative aspects and implementations. These are indicative of but a few of the various ways in which one or more aspects may be employed. Other aspects, advantages, and novel features of the disclosure will become apparent from the following detailed description when considered in conjunction with the annexed drawings.","The claimed subject matter is now described with reference to the drawings, wherein like reference numerals are used to refer to like elements throughout. In the following description, for purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of the claimed subject matter. It may be evident, however, that the claimed subject matter may be practiced without these specific details. In other instances, structures and devices are shown in block diagram form in order to facilitate describing the claimed subject matter.","Within the field of computing, many scenarios involve the storage of a storage set, comprising a series of data sets, on a set of one or more storage devices. For example, a user may wish to create an archive across a set of hard disk drives, and may store within the archive one or more data sets (e.g., bytes, words, blocks or sequences of data, files, or records). In some scenarios, the storage device(s) may be entirely allocated for the storage of data; e.g., a two-terabyte hard disk drive may be configured to provide a two-terabyte storage set. In other scenarios, the storage set may be redundantly stored on the storage devices in a manner that promotes data accessibility and\/or recovery; e.g., a one-terabyte data set may be identically stored on two one-terabyte hard disk drives in order to provide a backup in case either copy becomes corrupt. Multiple storage devices may be configured to interoperate in various ways to store the storage set.","Many such storage schemes and features are included in variations of a Redundant Array of Inexpensive Disks (RAID). As a second example, in a RAID 0 storage scheme, the entire storage space of two or more hard disk drives may be allocated to the storage of data, such that a first hard disk drive may provide access to data stored in one portion of the storage set while a second hard disk drive provides access in parallel to data stored in another portion of the storage set, effectively doubling of the rate of access to the data sets (and possibly comprising a further multiplication for portions of the storage set stored on other hard disk drives). As a second example, in a RAID 1 storage scheme, a first hard disk drive may be entirely allocated to store a storage set, and additional disk drives, operating as mirrors, may store identical copies of the storage set. Performance improvements may be achieved through concurrent access to the storage set provided by different hard disk drives. Additionally, a complete copy of the storage set may be accessed on any hard disk drive, and any hard disk drive that fails (e.g., due to corruption, unresponsiveness, absence, or damage) may be replaced without jeopardizing the availability of the data contained therein. However, RAID 1 schemes significantly reduce the capacity of the storage set (e.g., the addition of hard disk drives does not increase the capacity of the storage set). Additional RAID variations may balance the accessibility, performance, and fault recovery properties of the RAID 0 and RAID 1 arrays while maximizing the capacity of the storage set. For example, in a RAID 4 array comprising a set of hard disk drives of a particular size, the full capacity of all but one of the hard disk drives may provide storage space, while the reserved hard disk drive may store parity information (e.g., an exclusive OR (XOR) calculation for each of the data sets stored on the other hard disk drives). This configuration maximizes storage space (e.g., a RAID 4 array comprising four one-terabyte hard disk drives provides three terabytes of storage space), while also tolerating a one-drive failure; e.g., if any one of the hard disk drives completely fails, it may be replaced with a replacement hard disk drive, and the data on the failed hard disk drives may be reconstructed using the data stored on the remaining hard disk drives. For example, a failed parity hard disk drive may be reconstructed simply by recomputing the XOR values for respective data sets stored on the hard disk drives; and the data stored on a failed one of the other hard disk drives may be reconstructed through the use of the available data sets and the XOR parity value.","However, within scenarios involving the storage of a storage set comprising various data sets on one or more storage devices, various inefficiencies and problems may arise that affect the performance and\/or reliability of the storage set.  depict examples of two such problems that may be addressed by the techniques presented herein.","In the exemplary scenario  of , a storage set , comprising a series of data sets  (e.g., various bytes, words, data blocks, files, or records of the storage set ), may be stored on a storage device  (e.g., a hard disk drive). A process generating or accessing the storage set  may generate a set of read and\/or write requests involving various data sets , and may be received and fulfilled by the storage device . For example, a hard disk drive may comprise a read\/write head  that is suspended over a rotating physical medium, and that is capable of reading the data stored under any sector (e.g., radial line of data) of the physical medium that is rotated under the read\/write head . Thus, when a first data set  is received that is to be stored at a first location  in the storage set , the hard disk drive may rotate the physical medium until the physical location matching the location  in the storage set  is rotated under the read\/wrote head , and may then write the data set  to the physical medium. However, the performance of such hard disk drives is often limited by the delay while rotating the physical medium to the appropriate position. This delay may be mitigated through sequential accesses; e.g., three data sets  comprising a sequence of physical locations on the physical medium may be written in succession, thereby reducing the number of rotational delays in correctly positioning the locations under the read\/write head  from three to one. Because the rotational delay is often the rate-limiting factor in the throughput of a hard disk drive, sequential accesses may significantly improve the throughput of the storage device . Additionally, for each data set , a verifier  (represented in the exemplary scenario  of  as a parity byte computed for each four-byte data set ) is computed and stored with the data set , and may be used to verify the integrity of the data set . The exemplary scenario  of  presents an efficiency improvement by appending the verifier  to the data set  in the sector, such that the data set  and the verifier  may be stored by performing one rotational seek and write request.",{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 1","b":["110","104","102","106","100","104","106","114","104","110","102","106","108","104","112","116","104","110","102","106","108","104","112","108","110","104","104"]},"However, the exemplary scenario  of  depicts some sources of inefficiency in the writing of data sets  to the storage set . As a first example, the first data set  and the third data set  comprise a sequence (e.g., data sets  stored at consecutive locations  in the storage set ), as do the second data set  and the fourth data set . For example, a first process may request a first sequence of writes to the storage set  while a second process concurrently requests a second sequence of writes to a different portion of the storage set , and the storage device  may receive the requests in an interleaved manner. However, in this exemplary scenario , the storage device  writes the data sets  to the physical medium in sequential order of receipt, and thus performs four rotational seeks in order to write the data sets . While this write process may strictly preserve the order in which the write requests are received, an improvement in throughput may have been achieved by performing a first rotational seek to store the first data set  and the third data , and a second rotational seek to store the second data set  and the fourth data set .","A second source of inefficiency depicted in  arises from unnecessarily writing data to the storage set  that is promptly overwritten by a subsequent write. As a first example, the second data set  and the fourth data set  are both written to the same location  in the storage set . A recognition of the overwrite may provide an opportunity to improve the performance of the storage device  by only writing the latest write  (particularly if the overwrite writes the same data as the earlier write). However, the storage device  in this exemplary scenario  fails to achieve this recognition, and unnecessarily performs two writes of the same data set  to the same location  in the storage set . As a second example, a first verifier  may be computed for a first set of data sets  (e.g., locations 0x0044-0047), including the first data set  and the third data set , and a second verifier  may be computed for a second set of data sets  (e.g., locations 0xA0F0-A0F3), including the second data set  and the fourth data set . Because of the sequence in which the write requests are received, the storage device  computes and writes each verifier  twice (e.g., a first computation of the first verifier  is performed for and stored with the first data set ; a first computation of the second verifier  is performed for and stored with the second data set ; a recomputation of the first verifier  is performed for and stored with the third data set ; and a recomputation of the second verifier  is performed for and stored with the fourth data set ). These recomputations may have been avoided, thereby reducing the number of computations and writes, by computing the first verifier  once for the first data set  and the third data set  and computing the second verifier  once for the second data set  and the fourth data set . These and other inefficiencies may arise from the inability of the storage device  to identify opportunities to reduce the computations and\/or writes involved for write requests for data sets  stored sequentially in the storage set .",{"@attributes":{"id":"p-0030","num":"0029"},"figref":["FIG. 2","FIG. 2"],"b":["200","102","200","106","102","112","106","104","110","106","112","104","104","112","200","104","104","104","112","106","104","200","106","104","106","112","106","106","106","106","106","104","106","204","202","104","106","112","106","106","202","204","106","202","112","206"]},"The imperfect synchrony of storage devices  depicted in the exemplary scenario  of  may create an inconsistency in the event of a failure of the storage service. For example, at a third time point , another write  may be requested to both a data set  stored by the third storage device  and the corresponding verifier  stored by the fourth storage device . However, a failure  of the computing environment (e.g., a power failure or a hardware or software crash) may occur during this write process. While storage devices  and storage arrays are often designed to withstand many such failures , this failure  may occur after the third storage device  has completed the write of the updated data set , but before the fourth storage device  has written the updated verifier . At a fourth time point , when the storage devices  are again accessible (e.g., when power is restored), the update of the data set  stored by the third storage device  and the failure of the fourth storage device  to update the verifier  present an inconsistency: the verifier  no longer matches the corresponding data. A similar scenario may occur with the use of a single storage device ; e.g., redundant copies of a data set  stored on a storage device  with a single read\/write head  may present an inconsistency if a failure  arises between updating a first copy and a second copy of the data set .","This inconsistency, sometimes identified as the \u201cRAID write hole,\u201d may result in several problems. As a first example, it may not be possible to identify which of the one or more data sets  and\/or the verifier  is incorrect (e.g., the failure may have similarly occurred after the fourth storage device  updated the verifier  but before the third storage device  completed updating the data sets ), thereby jeopardizing the integrity of all of the data sets  represented by the verifier \u2014even the data sets  stored on the first and second storage devices  that were not even involved in the write . As a second example, this inconsistency may not be promptly discovered, but may linger within the storage set . Subsequently, if a storage device  becomes unavailable (e.g., if the first storage device  completely fails or is removed), an attempt to reconstruct the data on the first storage device  may utilize the data on the other storage devices, but the inconsistency may result in an incorrect reconstruction of the data. The array therefore fails to provide the expected capability to recover from the failure of a single storage device . These and other problems may arise from the imperfect synchrony in the interoperation of the storage devices  while storing related data sets  in the storage set .","Presented herein are techniques for addressing some of the problems and\/or inefficiencies that may arise in storage scenarios, possibly including those illustrated in the exemplary scenarios of  and . In accordance with these techniques, a journal may be generated on one or more of the storage devices  storing the storage set . Data sets  and verifiers  to be written to the storage set  may first be written to the journal. Moreover, the journal may be structured as a sequence of data sets  structured in the sequence of the write order of the data sets , such that a storage device  that receives a stream of requests to write data sets  in various (non-sequential) locations  of the storage set  may first store the data sets  sequentially in the journal. Additionally, in order to commit data to the requested locations  in the storage set , a number of data sets may be selected that may be written as a batch. For example, the journal may sequentially enqueue data sets  to be written to the storage set , and may periodically select, from the front of the queue, a set of data sets  that may be written in the same batch.",{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 3","b":["330","104","112","102","302","106","300","106","104","102","302","104","112","102","302","300","304","104","110","104","102","112","104","304","302","306","104","304","308","104","304","310","302","308","306","304","104","102","106","104","302","308","304","104","304","312","104","302","308","104","304","302","314","106","112","104","112","302","104","302","102","316","106","318","104","102","392","318","104","104","112","102","304","104","102","302","306","302","104","102"]},"The depiction in  of an exemplary use of some of the techniques presented herein illustrates some potential advantages that may be achievable thereby. As a first example, the batching of data sets may coalesce a sequence two or more data sets  that may be written to a continuous sequence of locations  in the storage set , even if the write requests for the data sets  comprising the sequence are interleaved with other data sets  to be stored in other locations  of the storage set ; even if such data sets  are not received in strict sequential order; and\/or even if brief delays occur between the requests to write the data sets . As a second example, the batching of data sets  may improve the efficiency of the storage device  by reducing overwrites of the same data that are received in a short time frame. For example, multiple requests to overwrite a particular data set  may be grouped into a batch, and may be fulfilled through a single write of the data set  to the location  in the storage set . For example, the selection of the batch  in the exemplary scenario  of  omits a data set  that is near the tail pointer, but that is overwritten by a subsequent write  stored in the journal . As a third example, separate requests to write data sets  represented by the same verifier , if grouped into the same batch, may result in a single computation and write of the verifier  instead of several separate updates of the verifier . As a fourth example, by selecting the batch  conservatively (e.g., not aggressively emptying the journal , but leaving some records  therein), the techniques may identify and achieve opportunities for future efficiency gains. For example, the data set  to be written to location 0x03C0 is not selected for the batch , as it has been recently received and may be promptly followed by requests to write additional data sets . Thus, when a subsequent request to write a data set  to location 0x03C1 is received and stored in the journal , both data sets  may be selected for a future batch , thereby committing both data sets  together in sequence rather than issuing two separate writes .","A second potential advantage of the presently disclosed techniques illustrated in the exemplary scenario  of  is a reduction of the incidence and consequences of the RAID write hole. Committing requests to update a data set  first to the journal on the nonvolatile medium of the storage device , and then moving the data set  and verifier  to the storage set , may avoid an inconsistent therebetween. For example, if a failure  occurs while the data set  and\/or verifier  are being written to the journal, a recovery process may detect that the journal was incompletely written, and may discard the incomplete portion of the journal. While this discarding may result in a loss of writes , such writes  were not yet committed to the storage set  and were not confirmed to the processes requesting such writes , and thus may be safely lost. Additionally, the consistency of the storage set  is not compromised by the incomplete write involved in the RAID write hole. Further, if a failure  occurs while the contents of the journal are being committed to the storage set , the storage device  may recover from the failure  by re-committing the data sets  stored in the journal to the storage set . In this manner, writes  to the storage set  may be fulfilled with improved performance and\/or with a reduced incidence of inconsistencies caused by problems such as the RAID write hole. These and other advantages may be achievable through the storage of data sets  to a storage set  according to the techniques presented herein.",{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 4","b":["400","104","102","106","400","400","402","404","406","106","302","104","112","104","110","102","408","104","302","410","318","104","302","102","102","412","104","318","414","112","416","112","302","418","104","112","104","102","418","104","112","104","102","104","302","104","102","400","420"]},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 5","b":["500","104","102","106","500","500","502","504","506","106","302","304","308","306","508","104","110","102","510","308","302","304","512","104","304","514","104","306","302","102","516","104","518","112","104","520","112","104","302","522","104","112","104","102","524","306","302","304","104","104","102","500","526"]},{"@attributes":{"id":"p-0039","num":"0038"},"figref":["FIG. 6","FIG. 4","FIG. 5"],"b":["600","602","612","610","610","602","104","102","106","400","500"]},"The techniques discussed herein may be devised with variations in many aspects, and some variations may present additional advantages and\/or reduce disadvantages with respect to other variations of these and other techniques. Moreover, some variations may be implemented in combination, and some combinations may feature additional advantages and\/or reduced disadvantages through synergistic cooperation. The variations may be incorporated in various embodiments (e.g., the first exemplary method  of  and the second exemplary method  of ) to confer individual and\/or synergistic advantages upon such embodiments.","D1. Scenarios","A first aspect that may vary among embodiments of these techniques relates to the scenarios wherein such techniques may be utilized. As a first variation, these techniques may be used to manage many types of storage sets  and data sets , including one or more volumes respectively comprising a set of values stored at respective addresses; file systems respectively comprising a set of files; databases respectively comprising a set of records; media libraries respectively comprising a set of media objects; application sets respectively comprising a set of applications; and computing environment servers respectively comprising a set of volumes and\/or memories of virtual machines. Additionally, the identification of a data set  within a storage set  may vary in granularity among different scenarios; e.g., a storage set  comprising a volume may utilize these techniques to journal and commit to the storage set  data sets  comprising bits, bytes, words of various lengths, data blocks of various lengths, or sectors.","As a second variation, these technique may be used to manage the storage of storage sets  and data sets  on various types of volatile and nonvolatile storage devices , including hard disk drives, solid-state storage devices, magnetic or optical tape storage drives, and magnetic or optical discs. The number of storage devices  involved in storing the storage set  may also vary; e.g., these techniques may be used to manage the storage of a storage set  on a single storage device , on a small and tightly integrated set of storage devices  (e.g., a RAID array), or a loosely integrated set of storage devices  that may be potentially large and\/or potentially widely distributed (e.g., a set of storage devices  deployed in different areas of the world and communicating over the internet). As but one example, these techniques may be adapted for use with different RAID levels implemented in various types of RAID arrays of storage devices . Moreover, the storage devices  storing the storage set  may also be of mixed types, and may be organized according to various hierarchical arrangements (e.g., the storage set  may first be stored on a comparatively high-performance primary storage device  that is backed up to a comparatively low-performance offsite archive storage device ). The techniques may be also be implemented in view of and attuned to various properties of the storage set  and storage devices , including cost, availability, reliability, performance demands, and sensitivity and security measures applied to the storage set , and the capabilities of the storage devices .","A third variation of this first aspect relates to the relationships of journals  with storage devices , particularly where the storage set  spans multiple storage devices . As a first such example, a journal  may be stored exclusively on one storage device  for a storage set  allocated across one or more separate storage devices . Alternatively, the journal  may be stored on the same storage device  as part or all of the storage set . As a second such example, multiple journals  may be generated among the storage devices . For example, for a storage set  spanning several storage devices , a journal  may be generated on each storage device  for the data sets  store in the portion of the storage set  stored on the storage device . Alternatively, journals  on separate storage devices  may not be associated with particular locations in the storage set ; e.g., a data set  to be written to the storage set  may be stored in any journal before being committed to the storage set . This variation may provide a decentralized journaling process; e.g., a data set  may be written to the journal  of the storage device  having the shortest I\/O queue, or, for a storage set  shared among a geographically distributed set of storage devices , to the storage device  presenting the highest accessibility to the writing process (e.g., the storage device  that is geographically closest to the process and\/or featuring the lowest latency or highest bandwidth while communicating with the writing process). As a third such example, a journal  may be redundantly stored as two or more copies on the same storage devices , may be stored as mirror copies on two or more storage devices , or distributed (e.g., by striping) across two or more storage devices , in order to confer upon the journal  similar fault-tolerance features as provided by various RAID storage schemes.","A fourth variation of this first aspect relates to the types of verifiers  used to verify the integrity of respective data sets . In some scenarios, a comparatively simple verifier  may be used, such as a parity bit computed as an XOR of the data sets , or a sum or hashcode of the data sets . A simple verifier  may be suitable for comparatively low-value storage sets , comparatively low-powered storage devices  (e.g., a storage device on a portable device having comparatively slow hardware, limited-capacity memory, and limited battery life), and\/or storage sets  for which performance is highly significant, such that more rapidly computable verifiers  may be advantageous. In other scenarios, a comparatively complex verifier  may be used that may provide additional data security features. For example, an error-correcting verifier , such as a Hamming code, may be used to determine not only whether the data sets  are accurate, but also whether an inconsistency has been caused by a change to one of the data sets  and\/or the verifier . Moreover, different types of verifiers  may be utilized for different sets or types of data sets  in the storage set  (e.g., more complex but durable verifiers  may be utilized for more valuable data). Those of ordinary skill in the art may implement the techniques presented herein in many scenarios having these and other types of variations and details.","D2. Elements","A second aspect that may vary among embodiments involves variations of the elements of these techniques. As a first variation, many techniques may be used while generating  the journal . For example, the journal  may comprise many types of data structures, such as an array, a linked list, a table, a database, a stack, a queue, a heap, or a binary tree. Different implementations may present various advantages and disadvantages (e.g., performance, ease of updating, space efficiency, computing economy, and compatibility with the characteristics of the storage device  and\/or storage set ). Different types of journals  may also be implemented on different storage devices  storing the storage set  and\/or for different types of data sets . For example, a journal  structured as an array, comprising a head pointer  and a tail pointer , may provide the advantages of rapid index (e.g., O(1) access time) to any record  of the journal , efficient allocation and re-use of records  through manipulation of the head pointer  and the tail pointer , and efficient storage of data sets  in the journal  (e.g., by simply appending the new data set  to the sequence of records  comprising the journal ).","As a second variation of this second aspect, the selecting  of batches  to be committed to the storage set  may be performed in many ways. As a first example, the selecting  may be initiated by many types of events. For example, a device , storage device , or other type of device implementing these techniques may initiate the selecting  of batches  upon detecting many types of commit events. Some examples of such commit events (comprising an exemplary commit event set) include a journal capacity event involving a capacity of the journal  (e.g., the journal  becoming full); a duration event involving a duration of the data sets  stored in the journal  (e.g., data sets  older than a certain age, such as data sets  stored in the journal  more than a minute ago); a commit request event involving a request to commit at least one data set  in the journal  to the storage set  (e.g., a process that requested the write  of a data set  may request a commitment of the data set  to the storage set ); and a storage device workload event involving a workload of at least one storage device  of the storage set  (e.g., a storage device  may detect an idle moment of input\/output work and may use the idle moment to flush some data sets  from the journal ). Many other types of events may prompt an initiation of the process of committing data sets  to the storage set .","As a second example of this second variation of this second aspect, the selection of a batch  of data sets  to be committed to the storage set  may be performed in many ways. For example, it may be advantageous to defer the committing of a first data set  to the storage set  for a brief duration after receiving the write request, in case subsequent writes  specify an overwriting of the first data set  and\/or provide additional data sets  that sequentially follow the first data set  and that therefore may be written together to the storage set  (e.g., as depicted at the fourth time point  in the exemplary scenario  of ). However, it may be disadvantageous to defer the committing of a data set  for an extended period of time, when the value of the reduced probability of imminently receiving a sequentially following second data set  is outweighed by the cost and complexity involved in storing the data set  as a record  of the journal . Additionally, it may be advantageous to select the data sets  comprising a batch  in order to improve the efficiency of the commitments to the storage set . As a first such example, when a first data set  stored in the journal  is selected for inclusion in a batch  that is to be stored at a first location  in the storage set , an embodiment may also select for inclusion in the batch  a second data set  that is also stored in the journal  and that is to be stored at a second location  that is near the first location  in the storage set  (e.g., data sets  that are consecutive or at least physically nearby on the physical medium of the storage device , and that may efficiently be written together in the same batch ). As a second such example, an embodiment of these techniques may omit from a batch  a first data set  that is stored in the journal  and that is to be stored at a location  in the storage set , if the embodiment determines that the journal  also includes a second data set  that is newer than the first data set , and that is to be stored at the same location  in the storage set  (i.e., a subsequent overwrite). Rather than including the data set  in the batch , the embodiment may simply remove the older data set  from the journal .","As a third variation of this second aspect, the computing  of verifiers  may occur in many ways. As a noted variation of the first aspect, many types of verifiers  may be utilized in such scenarios, but additionally, the verifier  may be computed from the available data in various ways. As a first example, the verifier  may be entirely recalculated based on the current data sets  represented thereby. However, as a second example, when a verifier  represents several data sets  of which a subset of data sets  changes, it may be possible, and occasionally more efficient, to remove the stale data sets  from the verifier  and include the updated data sets  in the verifier  than to recompute the verifier  from the current data sets , which may involve retrieving the remainder of the data set  from the storage set .",{"@attributes":{"id":"p-0051","num":"0050"},"figref":"FIG. 7","b":["700","112","702","104","112","102","104","112","102","704","104","104","102","112","104","302","102","112","104","104","704","112","104","706","112","104","104","202","104","302","104","302","102","112","104","708","112","104","112","104","112","112","104","104","112","104","104","302","104","302","102","104","102","112","104","104","112","104"]},"As a fourth variation of this second aspect, in the event of a failure  of the storage set  (e.g., a power failure or a software crash) and\/or one or more storage devices  (e.g., an interruption of communication with the storage device , a hardware, firmware, or driver failure of the storage device , or a removal of or damage to the storage device , followed by a reestablishment of communication or a replacement of the storage device ), an embodiment of these techniques may utilize the journal  in many ways to recover from the failure . As a first example of this fourth variation of this second aspect, an embodiment of these techniques may simply review the journal , discard any incomplete or inconsistent records  in the journal  (e.g., records that were incompletely written at the moment of the failure ), and then recommence committing data sets  from the journal  to the storage set . In the process, any data sets  that may have been incompletely written to the storage set  may be correctly rewritten during the recovery process, even without detecting the incomplete writing of the data set  to the storage set .","As a second example of this fourth variation of this second aspect, the recovery from a failure  may be performed in a phased manner. For example, it may be advantageous to recover from a failure  as rapidly as possible (e.g., in order to reduce the downtime of a service utilizing the storage set ), while also ensuring that accesses to the storage set  provide valid and consistent data. Accordingly, during a first phase of the recovery, an embodiment of these techniques may first read the contents of a journal  (e.g., the locations  within the storage set  where a data set  is stored in the journal ), in order to determine whether accesses to the storage set  are to be fulfilled from the journal  or from the storage set . The recovery may then proceed to a second phase involving recommencing the commitment of data sets  from the journal  to the storage set  in order to correct incompletely and\/or inconsistently written data sets  caused by the failure . Thus, the embodiment may block","Additional exemplary variations of the recovery process may involve, e.g., scanning part or all of the storage set  to verify the integrity thereof; applying the recovery process only to the storage devices  involved in the failure (e.g., only rewriting data sets  from the journal  to the storage device  that was temporarily removed); and applying different recovery processes for different storage devices  and\/or different data sets  (e.g., applying the recovery process to a first journal  stored on a first storage device , and completing the recovery thereof, before applying the recovery process to a second journal  stored on a second storage device ). Those of ordinary skill in the art may devise many such ways of varying the elements of the techniques presented herein for application in different scenarios.","D3. Volatile Memory Representation of the Journal","A third aspect that may vary among embodiments of these techniques involves the generation, in a volatile memory of a device  implementing such techniques, of a volatile memory representation of the journal . For example, in addition to the journal  generated on the nonvolatile medium of a storage device , an embodiment of these techniques may generate a volatile memory representation that also stores the data sets  stored in the journal , and that is kept in sync with the journal . While the generation and maintenance of a volatile memory representation to the journal  may add complexity and consume additional computing resources, the volatile memory representation may provide many potential uses and advantages in embodiments of these techniques. As a first exemplary advantage, the volatile memory representation may serve as a write buffer to the journal ; e.g., instead of writing individual data sets  to the journal , an embodiment may initially store the data sets  in the volatile memory representation, and may commit a block of data sets  to the journal , thereby extending the efficiency gain of sequential writes  of the data sets  to the journal  with the batching of writes  to the journal . As a second exemplary advantage, the volatile memory representation may serve as a read cache of recently written data sets ; e.g., instead of reading a recently written data set  from the journal  stored on the comparatively slow storage device , an embodiment may provide the data set  from the volatile memory representation. An embodiment of these techniques may therefore endeavor to retrieve a requested data set  according to its availability in the journal  and the volatile memory representation. For example, the embodiment may, upon determining that the data set  is stored in the volatile memory representation of the journal  in the volatile memory, retrieve and present the data set  stored in the volatile memory representation; upon determining that the data set  is stored in the journal  on a storage device , retrieve and present the data set  stored in the journal ; and may otherwise retrieve and present the data set  stored in the storage set  on the storage device . A data set  may also span two or more of these sources; e.g., a first portion of a requested data set  may exist in and be retrieved from the journal , while a second portion of the requested data set  may exist in and be retrieved from the volatile memory representation (irrespective of whether this second portion is present in the less accessible journal  and\/or storage set ). As a third exemplary advantage, decisions pertaining to the data sets  stored in the journal , such as the selecting  of batches , may be more efficiently performed by evaluating the contents of the volatile memory representation, which often provides more rapid access, than evaluating the contents of the journal . These advantages of accessibility of the data sets  in the rapid but volatile memory of the device  may be achieved in parallel with the durability of the data sets  through the storage thereof in the journal  on the nonvolatile storage device .","As a first variation of this third aspect, the volatile memory representation may be structured similarly to the journal , or may be generated in a different manner. For example, while it may be advantageous to structure the journal  to promote sequential writes on a storage device  such as a hard disk drive, this advantage may be diminished in a memory circuit that provides comparatively equivalent sequential and random access; thus, the volatile memory representation may be generated in another manner, such as indexed according to the locations  in the storage set  where the data sets  are to be stored, such as a hashtable or a B-tree such as an Adelson-Velskii-Landis tree.","As a second variation of this third aspect, the volatile memory representation may store the same data sets  stored in the journal , or may store different storage sets . As a first such example, the volatile memory representation may temporarily accumulate new data sets  to be written together to the journal  in the manner of a write buffer. As a second such example, the volatile memory representation may retain data sets  that are removed from the journal , in furtherance of the use of excess capacity of the volatile memory representation as a volatile memory read cache. For example, after a data set  is committed to the storage set  and removed from the journal  (and perhaps even overwritten), the volatile memory representation may retain the data set  in memory, due to the comparatively high probability that a process may request the recently written data set . This retention of data sets  in the volatile memory representation (following the committing of the data set  to the journal  and\/or the storage set ) may continue as long as capacity remains in the volatile memory representation, and the volatile memory representation may evict previously committed data sets  in order to provide capacity for newly received and as-yet uncommitted data sets . In this scenario, it may be advantageous for the volatile memory representation to differentiate data sets  that have been committed to the journal  and\/or the storage set  from uncommitted data sets . For example, upon storing a data set  in the journal , an embodiment may store the data set  in the volatile memory representation of the journal  and mark the data set  as unremovable; and upon committing a data set  stored in the journal  to the storage set , the embodiment may mark the data set  stored in the volatile memory representation as removable. Subsequently, in order to free capacity in the volatile memory representation, the embodiment may safely remove from the volatile memory representation of the journal  only the data sets  that are marked as removable. This variation maintains the synchrony of the journal  and the volatile memory representation while advantageously utilizing the spare capacity of the volatile memory representation as a read cache.","Conversely, and as a third variation of this third aspect, it may be advantageous not to exhaust the capacity of the volatile memory representation in storing committed or uncommitted data sets , but to reserve sufficient capacity in the volatile memory representation in the volatile memory to store incoming data sets . In particular, sufficient capacity may be reserved for a buffer configured to store data sets  to be stored in the storage set  while the journal  is occupied with committing other data sets  to the journal . This variation further utilizes the volatile memory representation as a write buffer in order to accept incoming data sets  without interrupting the storage device  from the task of committing data sets  from the journal  to the storage set .","As a fourth variation of this third aspect, a recovery of a failure  may also involve the rebuilding of the volatile memory representation  of the journal . For example, the recovery process may begin by reading the journal  to regenerate the volatile memory representation . Beginning the rebuilding in this manner may be advantageous, e.g., by reestablishing the read cache and\/or write buffer features of the volatile memory representation , and thereby reducing the read\/write workload of the storage device  storing the journal  and facilitating the task of committing data sets  in the journal  to the storage set  in order to overwrite incomplete or inconsistent writes  caused by the failure .",{"@attributes":{"id":"p-0061","num":"0060"},"figref":["FIG. 8","FIG. 8","FIG. 8"],"b":["800","302","106","802","302","610","800","802","110","104","302","804","302","802","104","806","104","302","104","802","302","806","104","302","102","106","104","302","306","302","104","802","104","802","800","802","104","104","810","104","802","302","102","812","104","802","302","302","802","302","802","302","102","800","802","302","802","302"]},"D4. Interoperation with Write Buffer","A fourth aspect that may vary among embodiments of these techniques relates to the inclusion and utilization of a write buffer in a storage device  storing the storage set . In many cases, a storage device  may advantageously utilize a write buffer to improve performance, e.g., by batching writes  of data sets  in a volatile memory until a flush request is initiated, and then committing all of the data sets  to the storage set  stored on the storage device . However, the operation of a write buffer on a storage device  may diminish the performance of the techniques presented herein, and in fact may cause some problems. For example, if a request to store a data set  in the journal  results is delayed in the volatile write buffer, then the data sets  may be lost if a failure  occurs. In particular, the write buffer is often implemented in a transparent manner, such that the operating system or processes may have difficulty determining whether data sets  have actually been committed to the  journal (unless a flush operation is affirmatively requested and verified as complete), or even whether or not a write buffer exists for the storage device . Thus, when a process requests to write a data set  to the journal , the storage device  may promptly indicate to the process that the request has been fulfilled, even if the write is stored in the volatile write buffer instead of in the nonvolatile storage of the journal . The application may therefore incorrectly operate as if the data set  had been committed, and inconsistencies and unexpected data loss may arise if a failure  occurs before the storage device  flushes the data set  from the write buffer. Similarly, the operation of the write buffer between the journal  and the storage set  may cause the journal  to operate incorrectly as if the data sets  had been persistently stored; e.g., the journal may remove data sets  that have not yet been committed to the storage set , thereby resulting in incomplete and inconsistent data sets  in the event of a failure  before the write buffer is flushed. Moreover, the advantages that the write buffer may propose (e.g., batched writes , coalescence of sequential writes , and reduction of overwrites) are already provided by other components of the techniques presented herein. Thus, it may be appreciated that the presence and operation of the write buffer causes added complexity, increased expense, potential performance degradation, and unexpected results, and yet provides few or no advantages that are not already achieved by the techniques presented herein.","In view of these potential disadvantages, an embodiment of these techniques may be adjusted in view of the presence of the write buffer. As a first variation of this fourth aspect, an embodiment of these techniques may avoid the use and effects of the write buffer in various ways. As a first example of this first variation, when writing data sets  and verifiers  to the journal , bypass the write filter, e.g., by issuing the write to the journal  as a write-through request, or by simply disabling the write buffer on the storage device . As a second example of this first variation, the embodiment may negate the effects of the write buffer by issuing a flush request after each write  to the journal  and\/or the storage set  stored on the storage device . Although a frequent issuing of flush requests may diminish the performance of the storage device , the loss of performance may be reduced in various ways; e.g., if the storage set  and\/or journal  are distributed over a set of storage devices  that respectively may or may not comprise a write buffer, an embodiment of these techniques may be configured to issue flush requests only to the storage devices  storing the recently written data sets .","As a second variation of this fourth aspect, an embodiment of these techniques may interoperate with the write buffer, and may coordinate the operation of the write buffer with the operation of the journal  and\/or the in-memory representation  of the journal . As a first example of this second variation, when a storage device  storing a journal  is flushed, a flush point of the journal  may be identified that represents the data sets  that have been flushed to the journal  (as contrasted with the data sets  for which a write request has been issued to the journal , but that may remain in the write buffer). For example, in an embodiment featuring a volatile memory representation  of the journal , the data sets  stored in the volatile memory representation  may initially be marked as unremovable, and may remain so marked until the flush point of the journal  is moved past the data set , at which point the volatile memory representation  may mark the data set  as removable.",{"@attributes":{"id":"p-0066","num":"0065"},"figref":"FIG. 9","b":["900","802","902","106","302","900","104","102","802","302","102","902","106","302","104","302","902","210","104","302","902","104","904","104","302","802","104","106","104","106","106","906","106","104","104","302","104","902","906","902","104","106","104","104","106","908","802","902","902","104","302","910","106","104","104","106","912","106","104","802","104","302","802","104","902","106"]},"As a third variation of this fourth aspect, a write buffer  may also intermediate, and may interfere with, the commitment of data sets  from the journal  to the storage set . In similar manner, the status of the data sets  stored in the volatile memory representation  and\/or the journal  may indicate whether the data sets  have been flushed from the journal  to the storage set . For example, an embodiment of these techniques may, upon detecting a commitment of a data set  from the write buffer  to the storage set  (e.g., an acknowledgment of a flush request), mark the data set  in the journal  and\/or the volatile memory representation  as committed, and may remove from the journal  and\/or the volatile memory representation  only the data sets  that are marked as having been committed to the storage set .",{"@attributes":{"id":"p-0068","num":"0067"},"figref":"FIG. 10","b":["1000","302","902","106","102","1000","302","104","102","106","902","104","106","102","302","104","302","102","1002","318","104","302","104","106","104","106","104","302","104","102","902","106","104","302","1004","302","106","106","106","106","104","1004","106","302","104","302","1008","308","306","104","306","102","1010","302","104","308","1008","104","102","302","902","106","102","902"]},{"@attributes":{"id":"p-0069","num":"0068"},"figref":"FIG. 11","b":"1102"},{"@attributes":{"id":"p-0070","num":"0069"},"figref":["FIG. 11","FIG. 11"],"b":["1100","1102","1102","1106","1108","1108","1104"]},"In some embodiments, device  may include additional features and\/or functionality. For example, device  may include one or more additional storage components , including, but not limited to, a hard disk drive, a solid-state storage device, and\/or other removable or non-removable magnetic or optical media. In one embodiment, computer-readable and processor-executable instructions implementing one or more embodiments provided herein are stored in the storage component . The storage component  may also store other data objects, such as components of an operating system, executable binaries comprising one or more applications, programming libraries (e.g., application programming interfaces (APIs), media objects, and documentation. The computer-readable instructions may be loaded in the memory component  for execution by the processor .","The computing device  may also include one or more communication components  that allows the computing device  to communicate with other devices. The one or more communication components  may comprise (e.g.) a modem, a Network Interface Card (NIC), a radiofrequency transmitter\/receiver, an infrared port, and a universal serial bus (USB) USB connection. Such communication components  may comprise a wired connection (connecting to a network through a physical cord, cable, or wire) or a wireless connection (communicating wirelessly with a networking device, such as through visible light, infrared, or one or more radiofrequencies.","The computing device  may include one or more input components , such as keyboard, mouse, pen, voice input device, touch input device, infrared cameras, or video input devices, and\/or one or more output components , such as one or more displays, speakers, and printers. The input components  and\/or output components  may be connected to the computing device  via a wired connection, a wireless connection, or any combination thereof. In one embodiment, an input component  or an output component  from another computing device may be used as input components  and\/or output components  for the computing device .","The components of the computing device  may be connected by various interconnects, such as a bus. Such interconnects may include a Peripheral Component Interconnect (PCI), such as PCI Express, a Universal Serial Bus (USB), firewire (IEEE 1394), an optical bus structure, and the like. In another embodiment, components of the computing device  may be interconnected by a network. For example, the memory component  may be comprised of multiple physical memory units located in different physical locations interconnected by a network.","Those skilled in the art will realize that storage devices utilized to store computer readable instructions may be distributed across a network. For example, a computing device  accessible via a network  may store computer readable instructions to implement one or more embodiments provided herein. The computing device  may access the computing device  and download a part or all of the computer readable instructions for execution. Alternatively, the computing device  may download pieces of the computer readable instructions, as needed, or some instructions may be executed at the computing device  and some at computing device .","Although the subject matter has been described in language specific to structural features and\/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather, the specific features and acts described above are disclosed as example forms of implementing the claims.","As used in this application, the terms \u201ccomponent,\u201d \u201cmodule,\u201d \u201csystem\u201d, \u201cinterface\u201d, and the like are generally intended to refer to a computer-related entity, either hardware, a combination of hardware and software, software, or software in execution. For example, a component may be, but is not limited to being, a process running on a processor, a processor, an object, an executable, a thread of execution, a program, and\/or a computer. By way of illustration, both an application running on a controller and the controller can be a component. One or more components may reside within a process and\/or thread of execution and a component may be localized on one computer and\/or distributed between two or more computers.","Furthermore, the claimed subject matter may be implemented as a method, apparatus, or article of manufacture using standard programming and\/or engineering techniques to produce software, firmware, hardware, or any combination thereof to control a computer to implement the disclosed subject matter. The term \u201carticle of manufacture\u201d as used herein is intended to encompass a computer program accessible from any computer-readable device, carrier, or media. Of course, those skilled in the art will recognize many modifications may be made to this configuration without departing from the scope or spirit of the claimed subject matter.","Various operations of embodiments are provided herein. In one embodiment, one or more of the operations described may constitute computer readable instructions stored on one or more computer readable media, which if executed by a computing device, will cause the computing device to perform the operations described. The order in which some or all of the operations are described should not be construed as to imply that these operations are necessarily order dependent. Alternative ordering will be appreciated by one skilled in the art having the benefit of this description. Further, it will be understood that not all operations are necessarily present in each embodiment provided herein.","Moreover, the word \u201cexemplary\u201d is used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as \u201cexemplary\u201d is not necessarily to be construed as advantageous over other aspects or designs. Rather, use of the word exemplary is intended to present concepts in a concrete fashion. As used in this application, the term \u201cor\u201d is intended to mean an inclusive \u201cor\u201d rather than an exclusive \u201cor\u201d. That is, unless specified otherwise, or clear from context, \u201cX employs A or B\u201d is intended to mean any of the natural inclusive permutations. That is, if X employs A; X employs B; or X employs both A and B, then \u201cX employs A or B\u201d is satisfied under any of the foregoing instances. In addition, the articles \u201ca\u201d and \u201can\u201d as used in this application and the appended claims may generally be construed to mean \u201cone or more\u201d unless specified otherwise or clear from context to be directed to a singular form.","Also, although the disclosure has been shown and described with respect to one or more implementations, equivalent alterations and modifications will occur to others skilled in the art based upon a reading and understanding of this specification and the annexed drawings. The disclosure includes all such modifications and alterations and is limited only by the scope of the following claims. In particular regard to the various functions performed by the above described components (e.g., elements, resources, etc.), the terms used to describe such components are intended to correspond, unless otherwise indicated, to any component which performs the specified function of the described component (e.g., that is functionally equivalent), even though not structurally equivalent to the disclosed structure which performs the function in the herein illustrated exemplary implementations of the disclosure. In addition, while a particular feature of the disclosure may have been disclosed with respect to only one of several implementations, such feature may be combined with one or more other features of the other implementations as may be desired and advantageous for any given or particular application. Furthermore, to the extent that the terms \u201cincludes\u201d, \u201chaving\u201d, \u201chas\u201d, \u201cwith\u201d, or variants thereof are used in either the detailed description or the claims, such terms are intended to be inclusive in a manner similar to the term \u201ccomprising.\u201d"],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 11"}]},"DETDESC":[{},{}]}
