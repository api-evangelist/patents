---
title: System and method of descriptively specifying memory placement in a computer system
abstract: In a computer system, a descriptive memory allocation system is described having a memory policy allocation module for setting memory allocation policies by an operating system in response to descriptive resource use requirements provided by an application requesting access to a specified address range in memory. The descriptive memory allocation system includes a descriptive resource allocator that uses descriptive memory use advice provided by an application to decide how to allocate memory to the application. The descriptive resource allocator includes memory allocation policies that may be set by the operating system after the operating system has determined the appropriate allocation scheme to implement based on an allocation advice provided by a requesting after the application. The application in providing its descriptive memory use information does not specify a specific allocation policy the operating system should use to allocate memory to it.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07149863&OS=07149863&RS=07149863
owner: Sun Microsystems, Inc.
number: 07149863
owner_city: Santa Clara
owner_country: US
publication_date: 20031008
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD OF THE INVENTION","BACKGROUND ART","SUMMARY OF INVENTION","DESCRIPTION OF THE PREFERRED EMBODIMENTS"],"p":["The present claimed invention relates generally to the field of resource allocation in a computer system. More particularly, embodiments of the claimed invention relate to a descriptive method for specifying memory placement in a computer system.","Multiprocessor computers by definition contain multiple processors that can execute multiple parts of a computer program and\/or multiple distinct programs simultaneously, in a manner known as parallel computing. In general, multiprocessor computers execute multithreaded-programs and\/or single-threaded programs faster than conventional single processor computers, such as personal computers (PCs), that must execute programs sequentially. The actual performance advantage is a function of a number of factors, including the degree to which parts of a multi-threaded program and\/or multiple distinct programs can be executed in parallel and the architecture of the particular multiprocessor computer at hand.","Multiprocessor computers may be classified by how they share information among the processors and also whether they share memory or not. Shared-memory multiprocessor computers offer a common physical memory address space that all processors can access. Multiple processes and\/or multiple threads within the same process can communicate through shared variables in memory that allow them to read or write to the same memory location in the computer. Message passing multiprocessor computers, in contrast, have a separate memory space for each processor or set of processors, requiring processes in such a system to communicate through explicit messages to each other.","Shared-memory multiprocessor computers may further be classified by whether all physical memory can be accessed by all CPUs in the same amount of time. The two classes of shared memory multiprocessors based on memory access time are called Uniform Memory Access (UMA) and Non-Uniform Memory Access (NUMA) machines. NUMA machines are often organized into multiple nodes with one or more processors per node. Although all of the memory is globally accessible in a NUMA machine, a processor can access memory in its local node faster than memory in a remote node and depending on the architecture, multiple levels of nodes may exist.","In UMA machines, all of the processors can access all of the physical memory in the same amount of time. Both forms of memory organization typically use high-speed cache memory in conjunction with main memory to reduce execution time.",{"@attributes":{"id":"p-0007","num":"0006"},"figref":["FIG. 1","FIG. 1"],"b":["100","120","130","107","105","122","123","132","133","150","150","121","131","107","107","120","130","100"]},"In the computer system  shown in , the operating system  usually allocates memory for an application such that it will perform well without much or any knowledge of the application itself. This is straightforward on UMA machines where the latency to all memory is the same from the CPUs \u2013 in the system . With UMA machines, the operating system  can indiscriminately allocate memory from almost anywhere and provide the same performance for the application . In contrast, this becomes more difficult on machines with asymmetric memory hierarchies such as NUMA where the latency isn't the same to all memory from all CPUs \u2013 in the system. On such systems, the operating system may need to allocate the memory near the CPUs \u2013 that application  runs on for optimal performance.","Without knowing more about the application itself, the operating system in  becomes an unintelligent conduit to memory  and  and can only do so much. On NUMA systems, the operating system can try to always allocate memory near the application, but this may not be the right thing to do all the time for all applications. For example, this might only be good to do for a portion of the application's address space or the needs of the application may change over time. Thus, there are applications that could achieve more performance through better memory placement than what the operating system provides by default. Given this, the problem is coming up with the best way to inform the operating system of how the memory should be allocated to get the best performance in the application. Furthermore, placement optimization requires restricting locations of the processes.","In a prior art memory allocation system, such as system , an inflexible prescriptive memory allocation scheme is the way an application can effect how the operating system should allocate memory for the application. In such a memory allocation scheme, the application has to know memory characteristics of the particular computer system that the application is running on and the semantics of the memory allocation policies implemented by the operating system in order to optimally allocate the prescribed memory. This inflexible method of allocating memory restricts the ability of certain computer systems to run certain applications. And in the event that these computer systems run out of memory, the prescribed memory range is unavailable or the prescription may not apply to the computer that the application is currently running on rendering ineffective the applications memory prescriptive capabilities. The prescriptive method of allocating memory to applications may also result in a degradation of the overall performance of the underlying computer system.","Because of these issues and the relative costs of the various solutions approaches of the prior art, a simple solution which is viable to implement with minimal expense and which provides minimal complexity and maximum usability to the end user is needed. This solution should provide a platform independent scheme of allocating memory based on how an application uses memory rather than a rigid prescribed memory allocation policy.","Accordingly, there is provided a multi-processor computer system having a descriptive Application Programming Interface (API) that allows the underlying operating system to optimally allocate resources for the applications on whatever computer the application is running on. For example, on a NUMA machine, the operating system may use this descriptive advice to set the memory allocation policy rather than having the application programs running in the computer system explicitly set these policies as is prevalent in the conventional art.","What is described herein is an approach of dynamically allocating resources, such as memory, in a multi-processor computer system environment based on a described memory access behavior of an application program in the computer system. An underlying computer operating system allocates resources based on how a particular application intends to use memory, etc., in the computer system.","According to an example of the invention, a system interface to the operating system allows an application to describe how it will use memory in a computer system and allows the underlying operating system to decide how best to allocate resources on a given computer system in a platform independent manner. With the descriptive information provided by the application, the operating system is able to optimally allocate resources for the application. In Non Uniform Memory Access machines, this results in the operating system choosing an appropriate memory allocation policy for the specified memory given the application's described behavior. This, in contrast to having the application specify the memory allocation policy directly as exists in the prior art.","This frees applications from having to know details of how the underlying computer system hardware or operating system works. It also frees applications from having to inform the operating system what exactly to do and allows the operating system to interpret an application's memory access characteristics to determine how to optimally allocate memory to the application in a computer platform independent manner. In one embodiment of the present invention, the descriptive information provided by an application may be interpreted by the operating system and applied to any computer or reapplied if the underlying computer hardware changes.","Embodiments of the present invention include an application interface that enables a requesting application to descriptively provide its memory access advice to the underlying computer system operating system and a separate module for the operating system to determine the optimal memory allocation policy to allocate memory to the application. The memory access advice provided by an application includes advice to indicate that the next thread to touch a specified memory range will access that particular memory range heavily. The memory access advice further includes advice to indicate that many threads will access a specified memory range heavily.","Embodiments of the present invention include application request interpretation logic that interprets the descriptive request from the application and determines the appropriate allocation policy. On a NUMA machine, the invention relies on the application request interpretation logic to interpret the descriptive memory use information provided by the application. The invention then selects the memory allocation policy for the specified memory range.","Embodiments of the invention further include a policy module that provides a variety of memory allocation policies to optimize performance of an application based on the memory access advice given by it. The policy module includes a latency aware module for generating memory allocation policies for memory access to NUMA machines and a cache conflict avoidance module for generating a memory allocation policy for memory access to non-NUMA machines.","Embodiments of the latency aware module include a \u201cnext-touch\u201d memory allocation policy for specifically allocating a specified range of memory to a requesting application from (or near) the node where the next reference of the memory is made. This policy is used by the operating system when the application advises that the next thread to touch the specified address range will access that particular range most heavily, so it should try to allocate the memory and other resources for this range near the thread accordingly.","Embodiments of the latency aware module also include \u201crandom\u201d memory allocation policy that implements a random memory allocation when the application describes its behavior as having many threads or processes access the given memory range heavily. This policy has the operating system allocate the specified range of memory randomly from all the nodes on a NUMA machine.","Embodiments of the latency aware module further include default memory allocation policy specified by the operating system in which memory allocation for a specified address range is implemented based on a default manner of the underlying computer system.","Embodiments of the cache conflict avoidance module include cache avoidance policy for deciding whether a cache avoidance policy should be set for a given range of memory.","These and other objects and advantages of the present invention will no doubt become obvious to those of ordinary skill in the art after having read the following detailed description of the preferred embodiments which are illustrated in the various drawing figures.","Reference will now be made in detail to the preferred embodiments of the invention, examples of which are illustrated in the accompanying drawings. While the invention will be described in conjunction with the preferred embodiments, it will be understood that they are not intended to limit the invention to these embodiments.","On the contrary, the invention is intended to cover alternatives, modifications and equivalents, which may be included within the spirit and scope of the invention as defined by the appended Claims. Furthermore, in the following detailed description of the present invention, numerous specific details are set forth in order to provide a thorough understanding of the present invention. However, it will be obvious to one of ordinary skill in the art that the present invention may be practiced without these specific details. In other instances, well-known methods, procedures, components, and circuits have not been described in detail as not to unnecessarily obscure aspects of the present invention.","The embodiments of the invention are directed to a system, an architecture, subsystem and method to process data in a multi-processor computer system. In accordance with an aspect of the invention, a system for dynamically allocating specified memory address range based on descriptive memory access information provided by an application to an underlying computer operating system in the multi-processor environment.",{"@attributes":{"id":"p-0035","num":"0034"},"figref":["FIG. 2","FIG. 2"],"b":["200","200","215","220","225","210","210","230","250","250","200","210","210","200","250","250","210","210","200","200"]},"The operating system  communicates with the application programs  via system call interfaces. In one embodiment of the invention, a descriptive memory allocation system may be implemented in the operating system .","In one embodiment of the invention, a system interface to the operating system  allows an application to describe how it will use memory  in a computer system . With this information, the operating system  is able to optimally allocate memory  for the applications . On NUMA machines, this results in the operating system  choosing the appropriate memory allocation policy for the specified memory given in the application's described behavior. This is in contrast to having the application specify the memory allocation policy directly as exists in the prior art.",{"@attributes":{"id":"p-0038","num":"0037"},"figref":["FIG. 3","FIG. 3"],"b":["220","220","300","310","320","330","300","305","307"]},"The Advice Interpretation Module (AIM)  receives advice on how a particular application intends to use memory and passes the advice to the appropriate policy module in the Policy Module (PM)  given the kind of computer system on which the application is running.","The Locality Aware Module (LAM)  determines, sets and helps to implement the memory allocation policies to improve locality by scheduling threads to run on certain CPUs, allocating memory nearby whenever possible, and balancing load across nodes at the same time to provide lowest latency. In one embodiment, the LAM  determines the appropriate memory allocation policy on NUMA machines to set for a given memory range to a particular application.","In one embodiment of the invention, the LAM  comprises three memory allocation policies that include a \u201cnext touch\u201d memory policy, a random memory allocation policy and a default memory allocation policy. In one embodiment of the invention, the operating system uses the DRA  to interpret the descriptive advice provided by an application to set memory allocation policies to allocate a specified memory address range for the application.","The \u201cnext touch\u201d memory allocation policy is invoked by the DRA  when the application specifies that the next thread to touch a specified address range will access to that particular range heavily. This allows the operating system to try to allocate memory and other resources for the specified range and the thread accordingly. In one embodiment on NUMA machines, the memory is allocated in the node for which the thread has affinity.","The random memory allocation policy is invoked by the DRA  when the application specifies that multiple processes or threads will access the specified address range heavily. This enables the operating system to try to allocate memory and other resources randomly to the specified range. In one embodiment of the present invention, the operating system allocates memory for the specified range from all the nodes in NUMA machine.","In a third memory allocation scheme, the default memory allocation policy is invoked by the DRA  when the application wants to set the expectation of how the specified address range will be accessed to the default. In one embodiment of the present invention, the default memory allocation policy for shared memory is the random memory allocation policy on NUMA machines. For private memory, the default allocation policy is the next touch policy.","Once the memory allocation policy for a requesting application is determined and set, the Memory Allocation Module (MAM)  sets the memory allocation policy given by the Policy Module and allocates memory according the given policy. The MAM  may further establish a mapping between a process's virtual address space and the desired object (e.g., physical memory in the present invention). In one embodiment of the invention, the MAM  is the virtual memory subsystem in the operating system. The CPU Scheduling Module (CSM)  schedules threads to run on CPUs. In one embodiment of the invention, this is the scheduling subsystem of the operating system.",{"@attributes":{"id":"p-0046","num":"0045"},"figref":["FIG. 4","FIG. 4"],"b":["220","420","430","421","422","431","432","423","433","220"]},"In the environment depicted in , the underlying computer operating system is locality aware (e.g., the operating system knows which CPU, memory, and other resources are close to each other). In the example illustrated in , the DRA  enables multiple applications (e.g., application ) to perform efficiently on any supported computing platform. The DRA  allocates resources for an application to provide good performance given its described memory access behavior.","In the environment shown in , the application  has a portion of its virtual memory  (e.g., V) from addresses \u201cA\u201d to \u201cB\u201d mapped to physical memory \u201cM\u201d in memory . In the implementation illustrated in , the application  advises the DRA  that the next thread to touch virtual memory \u201cV\u201d in range \u201cA\u201d to \u201cB\u201d will access it heavily. The AIM  receives this advice from the application  and determines that the application is running on a NUMA machine and passes the advice to the LAM .","The LAM  then determines that the next touch memory allocation policy should be set for virtual memory V in address range \u201cA\u201d to \u201cB\u201d and informs the MAM  to set the policy. The MAM  subsequently sets the next touch allocation policy and arranges for memory to be near the next thread to touch the specified memory range. When the next thread accesses virtual memory \u201cV\u201d, the MAM  migrates memory \u201cM\u201d from the remote node  to memory \u201cM\u201d in the local node  where the thread is running. The MAM  then establishes mapping from virtual \u201cV\u201d in memory  to local memory \u201cM\u201d in memory .",{"@attributes":{"id":"p-0050","num":"0049"},"figref":["FIG. 5","FIG. 5","FIG. 5","FIG. 5","FIG. 5"],"b":["500","520","530","521","522","531","532","523","533","220","500","220","410","510","501","1","1","523","510","220","1"]},"The AIM  receives this advice from the application  and determines that the application is running on a NUMA machine and passes the advice onto the LAM . The LAM  then determines that the next touch memory allocation policy should be set for virtual memory \u201cV\u201d in address range \u201cA\u201d to \u201cB\u201d and informs the MAM  to set the policy. The MAM  subsequently sets the next touch allocation policy and arranges for memory to be near the next thread to touch the specified memory range.","When the next thread attempts to access the virtual memory \u201cV\u201d in memory , the MAM  determines whether the memory \u201cM\u201d in the remote node  is locked. If the remote memory \u201cM\u201d is locked, the MAM  informs the LAM  to assign the thread to run on the remote node  whenever possible and the LAM  makes the thread have an affinity for the remote node . The CSM  then schedules the thread to run on the remote node .",{"@attributes":{"id":"p-0053","num":"0052"},"figref":["FIG. 6","FIG. 6"],"b":["1","610","220","610","1","601","310","610","310","307","307","1","320","320","623","1","1"]},"Still referring to , in a second stage (e.g., 2), the application  further advises the DRA  that the next thread to touch the virtual memory \u201cV\u201d in range \u201cC\u201d to \u201cD\u201d will access it heavily. The DRA  receives the advice and determines that cache conflict avoidance may be implemented on the underlying computer system in which the applications  is running. The DRA  then passes the advice provided by the applications  to the CCAM . The CCAM  then determines that cache conflict avoidance memory allocation policy should be set for virtual memory \u201cV\u201d and informs the MAM  to set the policy. The MAM  sets the cache conflict avoidance policy and tries to allocate memory \u201cM\u201d to get different cache color \u201cC\u201d to avoid cache conflicts between heavily accessed virtual memory \u201cV\u201d and \u201cV\u201d as advised by the application .",{"@attributes":{"id":"p-0055","num":"0054"},"figref":"FIG. 7"},"At step , the application gives descriptive advice to the DRA  specifying that the given memory will be heavily accessed and whether it will be accessed by one or many threads.","At step , the DRA  interprets the advice provided by the application and passes the advice to the appropriate policy module. The policy module determines the appropriate allocation policy and sets the policy at step . The DRA  then arranges for resources to be allocated according the policy at step .","The foregoing descriptions of specific embodiments of the present invention have been presented for purposes of illustration and description. They are not intended to be exhaustive or to limit the invention to the precise forms disclosed, and obviously many modifications and variations are possible in light of the above teaching. The embodiments were chosen and described in order to best explain the principles of the invention and its practical application, to thereby enable others skilled in the art to best utilize the invention and various embodiments with various modifications are suited to the particular use contemplated. It is intended that the scope of the invention be defined by the Claims appended hereto and their equivalents."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The accompanying drawings, which are incorporated in and form a part of this specification, illustrate embodiments of the invention and, together with the description, serve to explain the principles of the invention:",{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":["FIG. 5","FIG. 4"]},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 7"}]},"DETDESC":[{},{}]}
