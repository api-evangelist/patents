---
title: Method of, system for, and computer program product for providing efficient utilization of memory hierarchy through code restructuring
abstract: Code restructuring or reordering based on profiling information and memory hierarchy is provided by constructing a Program Execution Graph (PEG) corresponding to a level of the memory hierarchy, partitioning this PEG to reduce estimated memory overhead costs below an upper bound, and constructing a PEG for a next level of the memory hierarchy from the partitioned PEG. The PEG is constructed from control flow and frequency information from a profile of the program to be restructured. The PEG is a weighted undirected graph comprising nodes representing basic blocks and edges representing transfer of control between pairs of basic blocks. The weight of a node is the size of the basic block it represents and the weight of an edge is the frequency of transition between the pair of basic blocs it connects.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=06839895&OS=06839895&RS=06839895
owner: International Business Machines Corporation
number: 06839895
owner_city: Armonk
owner_country: US
publication_date: 20001010
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS REFERENCES TO RELATED APPLICATIONS","STATEMENT AS TO RIGHTS TO INVENTIONS MADE UNDER FEDERALLY SPONSORED RESEARCH AND DEVELOPMENT","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DESCRIPTION OF THE PREFERRED EMBODIMENTS"],"p":["This application is a Continuation of application Ser. No. 08\/987,911, filed Dec. 9, 1997, entitled \u201cMETHOD OF, SYSTEM FOR, AND COMPUTER PROGRAM PRODUCT FOR PROVIDING EFFICIENT UTILIZATION OF MEMORY HIERARCHY THROUGH CODE RESTRUCTURING\u201d, which application is incorporated herein by reference.\u201d","A portion of the Disclosure of this patent document contains material which is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure, as it appears in the Patent and Trademark Office patent file or records, but otherwise reserves all copyright rights whatsoever.","1. Field of the Invention","This invention relates generally to optimizing compilers for development of computer programs for use on a computer, and more particularly to code restructuring.","2. Description of the Related Art","Advances in processor designs have provided very high speed instruction processing. In order to sustain this speed, processors need very effective usage of the memory hierarchy of the machines. Ensuring effective usage is difficult for large programs like compilers or database engine codes that have complex control flow. This occurs, for example, as a result of having to consider inputs of different types or handle error conditions appropriately. Consequently, code generated for large complex programs consists of sections that are heavily executed intermingled with sections that are infrequently executed, resulting in poor utilization of the memory hierarchy. Code restructuring may improve memory hierarchy utilization and, consequently, performance.","The state of the art of code restructuring techniques is described in Simons (B. Simons, \u201cCode Positioning for Procedures and Basic Blocks,\u201d IBM Technical Report ADTI-1994-006; also available as IBM Santa Teresa Laboratory Technical Report TR 03.580.). Two representative techniques described by Simons include the IBM Heatshrink and the IBM AOPT products. Heatshrink, a trace-directed program restructuring (TDPR) tool, was designed and implemented by Randy R Heisch. A more detailed description of Heatshrink is given in Heisch (R. R. Heisch, \u201cTrace-Directed Program Restructuring for AIX Executables\u201d, vol. 38, September 1994.). AOPT, a stand-alone tool developed by Itai Nahshon, attempts to locate a \u201chot path\u201d in the code and then move the entire path as a single entity, rather than move individual basic blocks. Profile information, used by both Heatshrink and AOPT, is obtained by AOPT. Profiling involves running code using sample or representative inputs. Based on the behavior of the code on the given inputs, one makes assumption about its behavior on all inputs. Heatshrink and AOPT both perform global reordering of basic blocks and are bundled together as part of FDPR (Feedback Directed Program Restructuring tool), an IBM Licensed Program Product for IBM AIX Operating System, Version 4.1.","Pettis et al. (K. Pettis and R. C. Hansen, \u201cProfile Guided Code Positioning,\u201d 90 (), pp. 16-27, June 1990.) follow a different approach, that also relies on profiling information, but that moves executed basic blocks only within a procedure in addition to moving procedures. Basic blocks that are not executed according to the profile data are moved to a separate region at the end of an ordering.","These and other conventional techniques for code restructuring attempt to keep frequently executed code close together in memory. Intuitively, such an approach tries to retain the working set of frequently executed code in cache. If the working set fits in the cache, this approach is reasonable, on the other hand, if the cache is small or if the effective portion of the cache available to an application is small due to multiprogramming, conventional code restructuring techniques may not perform well. A major flaw of these conventional approaches is that \u201ccloseness\u201d in memory does not actually relate to cache or paging performance. Two basic blocks may be \u201cclose\u201d together in memory, and yet there may still be a cache miss or page fault transitioning between the two basic blocks, or they may be very far apart and still cause the same effect of a cache miss or page fault. Thus, conventional code restructuring techniques fail to realize that blocks interacting frequently should be kept in the same cache line or page rather than \u201cclose\u201d to each other in memory. Conventional code restructuring techniques also fail to realize that memory hierarchy hardware parameters such as cache line size, page size, cache associatively size and translation lookaside buffer (TLB) size may be considered in determining the code structure of an application.","Thus, the conventional code restructuring techniques fail to consider memory hierarchy. As such, there is a need for a method of, system for, and computer program product for, providing code restructuring dependent upon a memory hierarchy.","The invention disclosed herein comprises a method of a system for, and an article of manufacture for providing code restructuring based on profiling information and memory hierarchy, that is also scalable to arbitrary levels of memory hierarchy.","The present invention reorders code to reduce communications across cache line and across page boundaries. The present invention also reorders code to reduce the likelihood that chunks of code that communicate with each other and that have the same cache or TLB associativity set will be assigned to different cache lines or pages. The present invention may be generalized to deal with multiple levels of cache or other types of memory hierarchy.","The present invention accomplishes this by reordering code such that chunks of code that communicate extensively with each other are on the same cache line, page, or other level of memory hierarchy. Reordering is performed by first clustering the nodes, performing first level padding of unfilled clusters with basic blocks that were not accessed in profiling, and then performing additional padding using no-operation instructions (NOP or NO-OP) to fill out any unfilled portion of that level of the memory hierarchy. For example, to reorder code for a first level cache line, nodes are clustered to reduce communications across cache lines, any of these clusters which are unfilled clusters are padded with basic blocks that were not accessed in profiling, and then any unfilled cache lines are further padded with NOPs to fill out the cache lines.","The present invention avoids set collisions by considering both memory hierarchy size and associativity. Associativity means that there is a connection between a memory location and the place in the memory hierarchy to which the data for that memory location is assigned. 4-way associativity, for example, means that the cache is divided into four sets, and each memory location is uniquely mapped into one of those four sets. This mapping may be done by using two bits of the memory location address. If a cache line is 128 bytes in size, and if a memory word contains 4 bytes, then 32 words will fit on one cache line. A primary objective of the associativity mapping is to have each consecutive chunk of 32 words map to different associativity sets. If bits  and  of the memory location address are used to determine to which associativity set a 32 bit chunk maps, then adjacent 32 word chunks map to different sets. A set collision occurs when all the cache lines in a particular set are full. This may occur even if one of the other sets has space. As conventional processor implementations hard wire the associativity between memory and cache sets, data may not arbitrarily occupy a cache line in an associativity set that differs from the one to which the particular memory is assigned. The present invention considers both memory hierarchy size and associativity to avoid such associativity set collisions by dividing the cache size by its associativity, i.e., if an 8K cache is 4-way set associative, then n2=2048 bytes. This n2 falls between the cache line size, n1, of 128 bytes and the page size, n3, of 4096 bytes. Similarly, the TLB size divided by its associativity is considered. Alternative embodiments of the present invention may be modified to ignore various levels of the memory hierarchy; for example, an alternative embodiment may ignore associativity.","The present invention restructures code by constructing a Program Execution Graph (PEG) corresponding to a level of the memory hierarchy, partitioning this PEG to reduce estimated memory overhead costs, and constructing a PEG for a next level of the memory hierarchy from the partitioned PEG. The present invention may be performed by:\n\n","In accordance with one aspect of this invention, a Program Execution Graph constructor constructs a Program Execution Graph (PEG), a weighted undirected graph comprising nodes representing basic blocks or clusters of basic blocks and edges representing transfer of control between pairs of basic blocks where each basic block in the pair is in a different cluster, wherein a weight of a node of the PEG is a size of a represented basic block, and a weight of an edge is a frequency of transition between a pair of basic blocks that the edge connects or a frequency of transition between elements of two clusters that the edge connects.","In accordance with another aspect of this invention, the PEG, a weighted undirected graph G=(V,E) with edge and node weights, is partitioned into clusters such that the sum of the weights of the edges whose endpoints are in different partitions is minimized and the sum of weights of nodes in each set is no greater than a given upper bound.","In accordance with another aspect of this invention, a PEG for level i of a memory hierarchy, G=(V, E), is partitioned into clusters, and these clusters from level i are the nodes for G=, (V, E, a PEG for level i+1 of the memory hierarchy.","In accordance with another aspect of this invention, if u and v\u03b5V, there is edge (u,v) between u and v if and only if there is an edge between some pair of nodes in G, one of which is in the cluster corresponding to u and one of which is in cluster corresponding to v.","In accordance with another aspect of this invention, the weight of edge (u,v) in Gis the sum of the weights of the edges between component nodes of u and v in G.","In accordance with another aspect of this invention, the upper bound for a level of the memory hierarchy may be equal to the size of that level of the memory hierarchy.","In accordance with another aspect of this invention, the upper bound for a level of the memory hierarchy may be equal to a multiple of the size of that level of the memory hierarchy.","The present invention has the advantage of restructuring code responsive to a memory hierarchy.","The present invention has the further advantage of reducing memory hierarchy access cost of restructured code.","The present invention has the further advantage of reducing cache misses between basic blocks of restructured code.","The present invention has the further advantage of reducing page faults between basic blocks of restructured code.","The present invention has the further advantage of restructuring code responsive to multiple levels of cache or other types of memory hierarchy.","The present invention has the further advantage of being extendable to deal with additional memory hierarchies, such as second level (L) or higher caches that may be introduced in future machines.","Referring now to FIG.  through , sample Program Execution Graphs are shown to illustrate the practice of the present invention. The control flow and frequency information of the profiled program is organized in the form of a Program Execution Graph (PEG) which is a weighted undirected graph. For example,  illustrates a PEG G, generally referred to as , where G=(V, E). A node in the set of nodes Vcorresponds to a basic block that has been executed at least once during profiling. For G, the set of nodes Vcomprises the set {A , B , C , D , E , F , G , H , I , J , K , L , M , N , O , P , and Q }. An edge (u, v), in the set of edges E, between nodes u and v implies that there is some transition that occurs during profiling between the two basic blocks represented by u and v. For G, the set of nodes Ecomprises the set {(A, B) , (B, C) , (C, F) , (F, K) , (K, L) , (L, N) , (N, O) , (O, P) , (O, Q) , (B, N) , (F, J) , (J, K) , (B, E) , (E, F) , (B, D) , (D, E) , (E, G) , (G, I) , (I, L) , (G, H) , (H, I) 177, (H, M) , (M, N) , and (D, M) }. The weight of a node v, written W(v), is the size (in bytes) of the basic block that v represents. In G, node A  has a weight W(A) of 114 bytes, node B  has a weight W(B) of 36 bytes, C  has a weight W(C) of 8 bytes, and so on. The weight of edge e=(u,v), written W(e), is the total number of transitions from basic block u to basic block v plus the total number of transitions from basic block v to basic block u. In G, edge (A, B)  has a weight W,((A, B)) of 556 transitions, edge (B,C)  has a weight W,((B,C)) of 745 transitions, edge (C, F)  has a weight W((C, F)) of 745 transitions, and so on.","Using the initial PEG Gfor a program and given all the parameters for a n-level memory hierarchy, the present invention performs code restructuring in n or fewer steps. The istep constructs a new PEG G, by clustering PEG G, wherein graph partitioning is performed on Gto produce the clustering. Clustering and graph partitioning are discussed in the following paragraphs.","A cluster, written Cl, is defined to be a set of nodes. A clustering of a graph G with upper bound N, written Clust(G,N), is a partitioning of the nodes in G into clusters so that the sum of the node weights of the nodes in any cluster is no greater than N. Each cluster in a partitioned PEG represents a unit in the level of the hierarchy for which clustering is performed. For example, in the first stage, a unit represents the size of a cache line (in bytes) or some integral multiple of this size (to allow for basic blocks that are larger than a cache line).","The goal of a graph partitioning heuristic is to create clusters of nodes from the PEG such that the sum of the weights of the edges that have endpoints in different clusters is minimized, subject to the total weight of the nodes in each cluster not exceeding N. This goal is modeled by using a cost function C( ). For edge e, e has zero cost in Clust(G,N), written C(e)=0, if both endpoints of e are in the same cluster for Clust(G,N). Otherwise, e has a cost in Clust(G,N), C(e)=1. Those skilled in the art recognize that costs other than zero or one may be assigned by the cost function for the purposes of partitioning the graph. The goal is to compute a clustering that minimizes the sum of C(e) times W(e) for all edges e in E. The result of graph partitioning is to create the clusters, i.e., to assign values to C(e) for all edges e in E. If C(e)=0, then the endpoints of edge e are in the same cluster; otherwise, the endpoints of edge e are in different clusters.","Each cluster represents a unit in the stage of the hierarchy for which clustering is performed. Because a basic block may be larger than a cache line, the first stage of the heuristic allows a cluster to be the size of some multiple of the number of bytes in a cache line. At each subsequent stage, the clusters are limited to the size of the portion of the memory hierarchy being considered, for example, a memory page size. However, if for some reason this cluster size is not preferred, the heuristic may be easily modified to assign a different value to the cluster size.","The cost function is an approximation of the memory hierarchy overhead incurred by the restructured program on a real machine. Formally, the code restructuring problem may be defined as a first minimization of \u03a6, then a minimization of \u03a6, then a minimization of \u03a6, . . . and a final minimization of \u03a6, where k is the number levels in the memory hierarchy, and where: \n\n","The objective is to construct a clustering Clust(G,N) so that \u03a6is the summation of the product of C(e) and W(e) for all e in E; where C(e)=0 if both endpoints of e are in the same cluster and C(e)=1 otherwise; where the sum of the node weights of the nodes in each cluster in Clust(G,N) is no greater than N, and where Nis some multiple of the cache line size.","For example, if a cache line size is 128 bytes, Nmay be set to 128 bytes, and Gmay be partitioned into clusters such that the sum of the node weights of the nodes in each cluster is no greater than 128 bytes where Nis a multiple of one times the cache line size. Such a partitioning of Gyields a clustering Clust(G, N)={{A  }{B , C , F }, {J }, {K , L }, {D , E }, {G , H ,I }, {M , N }, {O , P , Q }}. Clustering Clust(G, N) is illustrated in  where duster  comprising {A } has a node weight sum of 114 bytes; cluster  comprising {B , C , F } has a node weight sum of 126 bytes; cluster  comprising {J } has a node weight sum of 92 bytes; duster  comprising {K , L } has a node weight sum of 94 bytes; cluster  comprising {D , E } has a node weight sum of 86 bytes; cluster  comprising {G , H ,I } has a node weight sum of 120 bytes, duster  comprising {M , N } has a node weight sum of 80 bytes; and cluster  comprising {O , P , Q } has a node weight sum of 104 bytes.","Once a clustering on Gis constructed, \u201cpadding\u201d is performed by adding basic blocks that were not executed during the profiling to dusters that have capacity, i.e. where the sum of the weights of the nodes in the cluster is less than Nand where adding the un-executed basic block will not cause the sum of the cluster's weights to exceed N. If there are still basic blocks remaining that have not been added to any of the clusters, and if there is no more cluster capacity to absorb these basic blocks, then they are grouped into additional clusters of their own. The technique of adding the unexecuted basic block nodes to the clusters or of putting them into their own clusters is called bin packing. A greedy bin packing heuristic may be used for performing the bin backing.","Although the example of  does not contain any un-executed basic blocks that may be used for padding (as the initial PEG does not include un-executed basic blocks in its node set), if there is an additional unexecuted basic block whose size is less than 48 bytes, it may be padded into the cluster  with node M  and node N  yielding a padded cluster whose sum of node weights does not exceed N=128. In a similar manner, a greedy bin packing heuristic may pad the other clusters.","Once all of the basic blocks have been put into some cluster, then the next stage is performed which is the construction of a graph G=V, E). Each node in Vcorresponds to a cluster in G, and each node in Vhas a weight of one. This weight corresponds to the assignment of Nbytes to the basic blocks in each cluster. An edge e=(u,v) exists in Eif and only if both u and v are in V, u and v correspond to different clusters, and there is some edge in Efrom one of the nodes in the cluster represented by it to some node in the duster represented by v, or visa versa. The weight of e is the sum of the weights of all edges in Ethat have endpoints in the two clusters (u and v).","Referring next to , a graph G=(E) constructed from the clusters Clust(G, N) of Gis illustrated. Gis constructed by creating a node corresponding to each cluster of Clust(G, N) such that node A  corresponds to cluster , node BCF  corresponds to cluster , node J  corresponds to duster , node KL  corresponds to cluster , node DE  corresponds to cluster , node GHI  corresponds to cluster , node MN  corresponds to cluster , and node OPQ  corresponds to cluster . Each of the nodes V={A , BCF , J , KL , DE , GHI , MN , OPQ } is assigned a weight of one. Edges Eare then constructed between various nodes Vwhere an edge e=(u,v) exists in Eif and only if both u and v are in V, u and v correspond to different clusters, and there is some edge in Efrom one of the nodes in the cluster represented by u to some node in the cluster represented by v, or visa versa. For example, edge (BCF, DE)  is created as there is an edge (B, D)  between node B  and node D , and as node B  and node D  are in different clusters, node B  being in cluster  and node D  being in cluster . In a similar manner, edges (A, BCF) , (BCF, KL) , (BCF, J) , (BCF, MN) , (DE, MN) , (DE, GHI) , (GHI, KL) , J, KL) , (KL, MN) , and (MN, OPQ)  are created. Weights are then assigned to the edges of Ewhere the weight of an edge (u, v) in Eis the sum of the weights of all edges in Ethat have endpoints in the two clusters u and v. For example, edge (BCF, DE)  is assigned a weight of , the sum of the edge weights of edge (B, D) , edge (B, E) , and edge (E, F) , all of these edges having endpoints in cluster  and cluster . In a similar manner, edge weights of , , , , , , , , , , and , respectively, are assigned to edges (A, BCF) , (BCF, KL) , (BCF, J) , (BCF, MN) , (DE, MN) , (DE, GHI) , (GHI, MN) , (GHI, KL) , (J, KL) , (KL, MN) , and (MN, OPQ) .","Let nbe the size of the next level of the memory hierarchy, typically the page size in bytes. Set N=n\/N. Then the same clustering heuristic is applied to G, using the same cost function C, i.e. counting only those edges that have end points in different clusters in G, This partitioning of Gis used to construct graph G=(N, E) by making each node in Vcorrespond to a cluster in V, and having an edge in Eif and only if there is some edge in Ethat has end points in nodes in the two clusters represented by the endpoints of the Eedge. Again the nodes have a weight of one, and the edges have weights corresponding to the sum of the weights on the edges in Gthat run between some node in cluster u and some node in cluster v.","For example, if n, the size of the next level of the memory hierarchy, is 512 bytes, then set N=n\/N=512\/128=4. Gmay be partitioned into clusters such that the sum of the node weights of the nodes in each cluster is no greater than 4. Such a partitioning of Gyields a clustering Clust(G,N)={ABCFJKL  and DEGHIMNOPQ } as illustrated in  graph G=V, E) may be constructed from the clusters Clust(G, N) of Gas illustrated in is constructed by creating a node corresponding to each cluster of Clust(G, N) such that node ABCFJKL  corresponds to cluster , and node DEGHIMNOPQ  corresponds to cluster . Each of the nodes V={ABCFJKL , DEGHIMNOPQ } is assigned a weight of one. Edges Eare then constructed between various nodes Vwhere an edge e=(u,v) exists in Eif and only if both u and v are in V, u and v correspond to different clusters, and there is some edge in Efrom one of the nodes in the cluster represented by u to some node in the cluster represented by v, or visa versa. For example, edge (ABCFJKL, DEGHIMNOPQ)  is created as there is an edge (BCF, DE)  between node BCF  and node DE  which are in different clusters, node BCF  being in cluster  and node DE  being in cluster . Weights are then assigned to the edges of E_where the weight of an edge (u, v) in Eis the sum of the weights of all edges in Ethat have endpoints in the two clusters u and v. For example, edge (ABCFJKL, DEGHIMNOPQ)  is assigned a weight of 1110, the sum of the edge weights of edge (BCF, DE) , edge (KL, GHI) , edge (KL, MN) , and edge (BCF, MN) , all of these edges having endpoints in cluster  and cluster .","This process of graph partitioning may be repeated for each level of the memory hierarchy for i>1. For level i of the memory hierarchy, let nbe the size of the memory hierarchy, and set N=n\/N. Then the same clustering heuristic is applied to G, using the same cost function C, i.e. counting only those edges that have end points in different clusters in G. This partitioning of Gis used to construct graph G=(N, E) by making each node in Vcorrespond to a cluster in V, and having an edge in Eif and only if there is some edge in Ethat has end points in nodes in the two clusters represented by the endpoints of the Eedge. Again the nodes have a weight of one, and the edges have weights corresponding to the sum of the weights on the edges in Gthat run between some node in cluster u and some node in cluster v.","One of the advantages of the present invention is that it may be generalized to deal with memory hierarchies with an arbitrary number of levels. In particular, the present invention may be applied to fixture improvements to memory hierarchy hardware.","Another advantage of the present invention is that it may be modified to deal with special cases. An example of such a special case occurs when a pair of basic blocks need to be close in memory. In this case, the edge connecting the two basic blocks may be given an exceedingly large weight, thereby forcing the two basic blocks into the same cluster if together they don't exceed the upper bound on the cluster size. This approach may also be generalized to the case in which several basic blocks are required to be close together in memory.","The present invention may also be modified to deal with the case of a program with a few very large basic blocks. Assume basic blocks B, B, and Bhave weights larger than the size of the cache line, and that the weights of all the other basic blocks are no greater than the cache line size. Define G\u2032, =G\u2212{B, B, B}, i.e. the PEG that is obtained by removing B, B, and Bfrom the problem input. Let N=the cache line size, and let Clust(G\u2032, N) be the clustering that is computed by the heuristic. Clust(G\u2032, N)+{B, B, B}may then be used as the input node set to the next stage of the heuristic.","As the nodes representing B, B, and Bhave weights greater than the value used for Nin the first stage of the partitioning, the weight of the node in Vthat corresponds to Bis p, where pis the smallest integer such that p*N>=the number of bytes in B. Values pand pfor Band B, respectively, are similarly computed. The heuristic may then be applied to G, with the above modifications. Note that this assumes that none of B, B, and Bis larger in bytes than n,. If that is not the case, then again the very large basic block(s) may be removed from consideration and reintegrated at the next stage of the heuristic.","Referring next to FIG.  and , flowcharts illustrating operations preferred in carrying out the present invention are shown. In the flowcharts, the graphical conventions of a diamond for a test or decision and a rectangle for a process or function are used. These conventions are well understood by those skilled in the art, and the flowcharts are sufficient to enable one of ordinary skill in the art to write code in any suitable computer programming language.","Referring now to , the process of the invention, generally referred to as , begins at process block . Thereafter, process block  determines memory hierarchy parameters such as the number of levels in the memory hierarchy and the size of each such level, and process block  profiles the target program. The profiler gathers information comprising identification of basic blocks, control flow between basic blocks, and frequencies of transition between basic blocks. The present invention assumes a program is divided into basic blocks, where a basic block is a contiguous sequence of instructions with a single entry into and a single exit out of this sequence. In addition, the present invention assumes information about the transfer of control between basic blocks and the frequency with which the transfer occurs during actual program execution. This information may be made available through the use of profiling techniques well known to those skilled in the art such as the teachings summarized in Simons (B. Simons, \u201cCode Positioning for Procedures and Basic Blocks,\u201d IBM Technical Report ADTI-1994-006; also available as IBM Santa Teresa Laboratory Technical Report TR 03.580.). Responsive to the profiler provided information, process block , a Program Execution Graph constructor, constructs a Program Execution Graph (PEG), a weighted undirected graph comprising nodes representing basic blocks and edges representing transfer of control between pairs of basic blocks. In this PEG produced by the Program Execution Graph constructor, a weight of a node of the PEG is a size of a represented basic block, and a weight of an edge is a frequency of transition in the profiling between a pair of basic blocks that the edge connects. Process block  then initializes a memory hierarchy level i to one, and sets nequal to a smallest power of 2 that is a multiple of the cache line size and that is greater than the sizes of all basic block sizes. Thereafter, process block  applies a heuristic H to partition Ginto Clust(G, N), a partitioning of the nodes in Ginto dusters so that the sum of the node weights of the nodes in any cluster is no greater than N. The heuristic H solves the relaxed graph partitioning problem, and such heuristics are well known to those skilled in art such as the teachings of Barnard et al. (S. T. Barnard and H. D. Simon, \u201cFast Multilevel Implementation of Recursive Spectral Bisection for Partitioning Unstructured Problems,\u201d vol.6(2), pp.101-117, April 1994.). After clustering is performed on G, process block  pads the clusters with unexecuted nodes by adding basic blocks that were not executed during the profiling to clusters that have capacity, i.e. where the sum of the weights of the nodes in the cluster is less than Nand where adding the un-executed basic block will not cause the sum of the cluster's weights to exceed N. This padding is performed to reduce unused memory. If there are still basic blocks remaining that have not been added to any of the clusters, and if there is no more cluster capacity to absorb these basic blocks, then process block  groups these unexecuted nodes into additional clusters of their own. The technique of adding the unexecuted basic block nodes to the clusters or of putting them into their own clusters is called bin packing. A greedy bin packing heuristic, which is well known to those skilled in art such as the teachings of Garey et al. (Garey, Michael and Johnson, David S. Computers and Intractability, A Guide to the Theory of NP Completeness, pages 124-127, W. H. Freeman & Co., San Francisco, Calif., 1979) may be used to perform the bin backing. Process block  then creates the next level PEG Gfrom the level 1 partitioned PEG Clust(G, N). In this graph G=(V, E) constructed by process block , each node in Vcorresponds to a cluster in G, and each node in Vhas a weight of one. This weight corresponds to the assignment of Nbytes to the basic blocks in each cluster. An edge e=(u, v) exists in Eif and only if both, and v are in V, u and v correspond to different clusters, and there is some edge in Efrom one of the nodes in the cluster represented by u to some node in the cluster represented by v, or visa versa. The weight of e is the sum of the weights of all edges in Ethat have endpoints in the two clusters (u and v).","Higher levels of the memory hierarchy, i greater than 1, are processed by a loop comprising decision block  and process blocks , , , and . Decision block  begins the loop for each level i of the memory hierarchy. As the loop begins, decision block  determines if level i is less than or equal to a maximum number of levels. If so, then process block  increments level i, and then process block  sets n, equal to the size of the memory hierarchy for level i, i.e., to the page size in bytes for the page level of the memory hierarchy. Thereafter, process block  applies a heuristic H to partition Ginto clusters having sizes no greater than N=\/Nto produce Clust(G, n\/N) . Process block  may perform this clustering in the same manner as process block  or it may use a different partitioning heuristic. Thereafter, process block  creates the next level PEG Gfrom the level i partitioned PEG Clust(Gn\/N). In this graph G=(V, E) constructed by process block , each node in Vcorresponds to a cluster in G, and each node in Vhas a weight of one. An edge e=(u,v) exists in Eif and only if both u and v are in V, u and v correspond to different clusters, and there is some edge in Efrom one of the nodes in the cluster represented by u to some node in the cluster represented by v, or visa versa. The weight of e is the sum of the weights of all edges in Ethat have endpoints in the two clusters (u and v). Process block  performs this PEG construction in the same manner as process block . Thereafter, processing loops back to decision block , the beginning of the loop, to process the next level i+1.","Returning now to decision block , if level i is not less than or equal to a maximum number of levels, then process block  reorders basic blocks in memory by grouping all of the nodes of a cluster in an adjacent order, process block  adds NOPs to fill out the unused portion of memory for that memory hierarchy level for each level of the memory hierarchy; process block  stores the reordered and padded basic blocks in memory beginning with the first memory location after a boundary for all the memory levels; and then the process ends at process block .","Referring now to , an expansion of process block  is illustrated After process block  partitions the PEG, then process block , process block , process block , and process block  construct a Program Execution Graph Gfor the next higher level i+1 wherein the nodes Nof Gare constructed from the clusters Clust(G, n\/N) of G. Process block  assigns a weight of one to each node in N. Process block  assigns edges Ein Gby placing an edge (u,v) between u and v, for each u and v of N, if and only if there is an edge between a pair of nodes in G, one of which is in the cluster corresponding to u and one of which is in cluster corresponding to v. Process block  assigns weights to the edges Ein G; the weight of an edge (u,v) in G, is the sum of the weights of the edges between component nodes in Gof the clusters, corresponding to u and v. Thereafter, processing loops back to decision block , the beginning of the loop, to process the next level i+1.","In an alternative embodiment of the present invention, only a single loop may be used for processing all levels of the memory hierarchy. In this alternative embodiment, process block  may be modified to include the functionality of process block  so that process block  partitions all PEG's. Process block  may also be modified to include the functionality of process block  so that it creates the next level PEG from all partitioned PEG's. Process block  may be moved within the loop to pad the first or additional level clusters.","After the clustering is completed, the basic blocks are reordered in memory by grouping all of the nodes of a cluster in an adjacent order (process block ), although the ordering of the basic blocks within this adjacent grouping may be varied. Because clusters must not cross boundaries of memory hierarchies, NOPs are added to fill out the portion of a memory hierarchy level that is not filled by the clusters. For memory hierarchy level 1, NOPs are added to the end of any clusters that have size less than some multiple of the cache line size so that the amount of memory required for each cluster is some integral multiple of the cache line size. For each subsequent level of the memory hierarchy, the minimum number of NOPs necessary to fill out the unused portion of memory for that memory hierarchy level are added (process block ). After the processing for the last memory hierarchy is completed yielding the final PEG, the reordered basic blocks are stored in memory starting at a boundary of all the memory levels so that all of the basic blocks from one cluster are adjacent, i.e. interleaving from different clusters is not allowed (process block ). This is true for every level of the hierarchy.","Referring now to , a block diagram illustrates a computer system  used in performing the method of the present invention, forming part of the apparatus of the present invention, and which may use the article of manufacture comprising a computer-readable storage medium having a computer program embodied in said medium which may cause the computer system to practice the present invention. The computer system  includes a processor , which includes a central processing unit (CPU) , a memory , and a multi-level memory cache  (level 1 cache (L)  through level n cache (Ln) ). Additional memory, in the form of a hard disk file storage  and a computer-readable storage device , is connected to the processor . Computer-readable storage device  receives a computer-readable storage medium  having a computer program embodied in said medium which may cause the computer system to implement the present invention in the computer system . The computer system  includes user interface hardware, including a mouse  and a keyboard  for allowing user input to the processor  and a display  for presenting visual data to the user. The computer system may also include a printer .","Although the present invention has been particularly shown and described with reference to a preferred embodiment, it will be understood by those skilled in the art that various changes in form and detail may be made without departing from the spirit and the scope of the invention."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["For a more complete understanding of the present invention and the advantages thereof, reference is now made to the Detailed Description in conjunction with the attached Drawings, in which:",{"@attributes":{"id":"P-00037","num":"00037"},"figref":"FIG. 1","sub":"1 "},{"@attributes":{"id":"P-00038","num":"00038"},"figref":"FIG. 2","sub":"1 "},{"@attributes":{"id":"P-00039","num":"00039"},"figref":"FIG. 3","sub":["2 ","1 "]},{"@attributes":{"id":"P-00040","num":"00040"},"figref":"FIG. 4","sub":"2 "},{"@attributes":{"id":"P-00041","num":"00041"},"figref":"FIG. 5","sub":["3 ","2 "]},"FIG.  and  are flowcharts illustrating the operations preferred in carrying out the present invention; and",{"@attributes":{"id":"P-00043","num":"00043"},"figref":"FIG. 8"}]},"DETDESC":[{},{}]}
