---
title: Generation of parallel code representations
abstract: A generated grouped representation of existing source code can define regions of the existing source code. A set of the regions that can run in parallel can be identified based on the grouped representation. The grouped representation can be converted into a modified representation, such as modified source code or a modified intermediate compiler representation, which can be configured to be resolved or executed to self-schedule the set of regions to run in parallel as a set of tasks. Additionally, the source code can include one or more exception handling routines, and user input can be received to identify in one or more lambda expressions one or more regions of the source code to be run in parallel as one or more tasks.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08667474&OS=08667474&RS=08667474
owner: Microsoft Corporation
number: 08667474
owner_city: Redmond
owner_country: US
publication_date: 20090619
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["Multiple core computer architectures have become more common in recent times. Such architectures allow code to be run in parallel. For example, in a dual core architecture, a first core can execute a first set of code in a first thread at the same time that a second core is executing a second set of code in a second thread. The first thread could be running a block of code from a first program and a second thread could be running a block of code from a second program. In addition, it is possible for the two threads to be running two blocks of code from the same program. To allow such parallelism, programs can include parallel code, which includes self-scheduling code that specifies code that can be run in parallel during partial or completely overlapping time periods. A compiler can model control and data dependencies for regions of code in an existing program. The compiler can also codify those dependencies and inject new code back into the program so that when a region of the program completes execution, the program can update a set of dependencies that are relevant to successor regions, if any, in the program. If all the dependencies of a successor code region are fulfilled, then the self-scheduling code can initiate that successor to run as a task. This code that initiates successor regions in the program is referred to herein using the terms \u201cself-scheduling code,\u201d \u201ccode that is configured to self-schedule,\u201d or similar terms. Many existing programs are written as sequential programs that do not include parallel code.","Whatever the advantages of previous code generation tools and techniques, they have neither recognized the parallel code generation tools and techniques described and claimed herein, nor the advantages produced by such tools and techniques. For example, the tools and techniques can perform acts that can include translating existing source code to self-scheduling parallel source code; displaying potentially parallel regions with their dependencies in a visual editor; and generating self-scheduling parallel executable object code. In some implementations, all these acts may be done automatically and may be done at once. In other implementations, only some of these acts may be done, or they may not be done at once. For example, a self-scheduling parallel intermediate compiler representation can be generated from existing source code. That intermediate compiler representation may be used for different purposes, such as to generate parallel object code, to generate parallel source code, and\/or to provide a data structure for visual editing.","Accordingly, the tools and techniques described herein can allow existing source code to be converted to modified representations of the existing source code. The modified representations can include parallel code that is configured to self-schedule regions of the code to run in parallel as tasks. The representations can take various forms, such as modified source code, intermediate compiler formatted code that can be grouped to represent the source code at a higher level, etc.","In one embodiment, the tools and techniques can include abstracting existing source code and representing the source code at a higher level (e.g., as functional blocks). This higher level representation can be analyzed to identify regions of the source code (e.g., functional blocks or groups of functional blocks) that can run in parallel. Moreover, self-scheduling code can be inserted into the regions to schedule the regions to run in parallel as tasks. A modified higher level representation with such inserted code can be translated back into modified source code that is configured to self-schedule the regions to run in parallel as a set of tasks.","The higher level representation can be referred to as a grouped representation of existing source code. As used herein, a grouped representation of source code is a representation that identifies groups of the source code, such as basic blocks, loops, exception handling routines, user-defined regions, and\/or single entry single exit regions. The grouped representation can define regions of the existing source code. A set of the regions that can run in parallel can be identified based on the grouped representation. The grouped representation can be converted into modified source code that is configured to self-schedule the set of regions in parallel as a set of tasks.","In another embodiment of the tools and techniques, existing source code that includes one or more exception handling routines can be received. The existing source code can be automatically converted into a modified representation of the existing source code that is configured to self-schedule a set of regions from the existing source code as a set of tasks to run in parallel.","In yet another embodiment of the tools and techniques, user input can identify in one or more lambda expressions a set of regions of existing source code. The one or more lambda expressions can identify the regions as regions that can run in parallel. Moreover, a modified representation of existing source code can be produced. The modified representation can be configured to self-schedule the set of regions to run in parallel as a set of tasks. A lambda expression defines and constructs one or more unnamed function objects, which behave like handwritten function objects. Lambda expressions will be discussed in more detail in the Written Description section below.","This Summary is provided to introduce a selection of concepts in a simplified form. The concepts are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter. Similarly, the invention is not limited to implementations that address the particular techniques, tools, environments, disadvantages, or advantages discussed in the Background, the Detailed Description, or the attached drawings.","Embodiments described herein are directed to techniques and tools for improved generation of parallel representations (i.e., representations of existing source code that are configured to self-schedule a set of code regions to run as a set of tasks in parallel when the representations are resolved or executed, possibly after being compiled) from source code. Such improvements may result from the use of various techniques and tools separately or in combination.","Such techniques and tools may include converting sequential source code into a modified parallel representation of the source code, which can include self-scheduling code to schedule regions of code to run in parallel as tasks. Grouped representations of the existing sequential source code may be generated and analyzed to identify these regions of code that can run in parallel. For example, sequential code may be abstracted to generate one or more grouped representations, such as a control flow graph and a hierarchical task graph. The grouped representations can be analyzed to identify regions of the code that can run in parallel. Self-scheduling code can be injected into the regions in an intermediate compiler representation of the sequential code to form a parallel intermediate compiler representation. That intermediate compiler representation can be translated into a parallel source code representation that includes such self-scheduling code to schedule the regions to run in parallel as tasks. In addition, the parallel source code representation and\/or the parallel intermediate compiler representation may be compiled into object code that includes the self-scheduling code. The generation of parallel representations may account for exception handling routines, such as by keeping such routines together, rather than splitting them across multiple tasks or threads. In addition, optimization techniques can be employed when generating parallel representations. For example, variables can be privatized to allow for additional parallelism in the parallel representations. These techniques can be completely or partially automated, such as by being performed automatically in a compiler computing environment. Thus, the techniques and tools described herein can allow sequential code to be effectively and efficiently converted to parallel representations that may be translated back into source code and\/or compiled into object code that can reap the benefits of multi-core computer architectures.","Accordingly, one or more substantial benefits can be realized from the parallel representation generation tools and techniques described herein. However, the subject matter defined in the appended claims is not necessarily limited to the benefits described herein. A particular implementation of the invention may provide all, some, or none of the benefits described herein. Although operations for the various techniques are described herein in a particular, sequential order for the sake of presentation, it should be understood that this manner of description encompasses rearrangements in the order of operations, unless a particular ordering is required. For example, operations described sequentially may in some cases be rearranged or performed concurrently. Techniques described herein with reference to flowcharts may be used with one or more of the systems described herein and\/or with one or more other systems. Moreover, for the sake of simplicity, flowcharts may not show the various ways in which particular techniques can be used in conjunction with other techniques.","I. Exemplary Computing Environment",{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 1","b":"100"},"The computing environment () is not intended to suggest any limitation as to scope of use or functionality of the invention, as the present invention may be implemented in diverse general-purpose or special-purpose computing environments.","With reference to , the computing environment () includes at least one processing unit () and memory (). In , this most basic configuration () is included within a dashed line. The processing unit () executes computer-executable instructions and may be a real or a virtual processor. In a multi-processing system, multiple processing units execute computer-executable instructions to increase processing power. The memory () may be volatile memory (e.g., registers, cache, RAM), non-volatile memory (e.g., ROM, EEPROM, flash memory), or some combination of the two. The memory () stores software () implementing generation of parallel representations from source code.","Although the various blocks of  are shown with lines for the sake of clarity, in reality, delineating various components is not so clear and, metaphorically, the lines of  and the other figures discussed below would more accurately be grey and fuzzy. For example, one may consider a presentation component such as a display device to be an I\/O component. Also, processors have memory. The inventors hereof recognize that such is the nature of the art and reiterate that the diagram of  is merely illustrative of an exemplary computing device that can be used in connection with one or more embodiments of the present invention. Distinction is not made between such categories as \u201cworkstation,\u201d \u201cserver,\u201d \u201claptop,\u201d \u201chandheld device,\u201d etc., as all are contemplated within the scope of  and reference to \u201ccomputer,\u201d \u201ccomputing environment,\u201d or \u201ccomputing device.\u201d","A computing environment () may have additional features. In , the computing environment () includes storage (), one or more input devices (), one or more output devices (), and one or more communication connections (). An interconnection mechanism (not shown) such as a bus, controller, or network interconnects the components of the computing environment (). Typically, operating system software (not shown) provides an operating environment for other software executing in the computing environment (), and coordinates activities of the components of the computing environment ().","The storage () may be removable or non-removable, and may include magnetic disks, magnetic tapes or cassettes, CD-ROMs, CD-RWs, DVDs, or any other medium which can be used to store information and which can be accessed within the computing environment (). The storage () stores instructions for the software ().","The input device(s) () may be a touch input device such as a keyboard, mouse, pen, or trackball; a voice input device; a scanning device; a network adapter; a CD\/DVD reader; or another device that provides input to the computing environment (). The output device(s) () may be a display, printer, speaker, CD\/DVD-writer, network adapter, or another device that provides output from the computing environment ().","The communication connection(s) () enable communication over a communication medium to another computing entity. Thus, the computing environment () may operate in a networked environment using logical connections to one or more remote computing devices, such as a personal computer, a server, a router, a network PC, a peer device or another common network node. The communication medium conveys information such as data or computer-executable instructions or requests in a modulated data signal. A modulated data signal is a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media include wired or wireless techniques implemented with an electrical, optical, RF, infrared, acoustic, or other carrier.","The tools and techniques can be described in the general context of computer-readable storage media. Computer-readable storage media are any available media that can be accessed within a computing environment. By way of example, and not limitation, with the computing environment (), computer-readable storage media include memory (), storage (), and combinations of the above.","The tools and techniques can be described in the general context of computer-executable instructions, such as those included in program modules, being executed in a computing environment on a target real or virtual processor. Generally, program modules include routines, programs, libraries, objects, classes, components, data structures, etc. that perform particular tasks or implement particular abstract data types. The functionality of the program modules may be combined or split between program modules as desired in various embodiments. Computer-executable instructions for program modules may be executed within a local or distributed computing environment. In a distributed computing environment, program modules may be located in both local and remote computer storage media.","For the sake of presentation, the detailed description uses terms like \u201cdetermine,\u201d \u201cchoose,\u201d \u201cadjust,\u201d \u201cgenerate,\u201d and \u201coperate\u201d to describe computer operations in a computing environment. These and other similar terms are high-level abstractions for operations performed by a computer, and should not be confused with acts performed by a human being, unless performance of an act by a human being (such as a \u201cuser\u201d) is explicitly noted. The actual computer operations corresponding to these terms vary depending on the implementation.","II. General Parallel Code Representation Generation System and Environment",{"@attributes":{"id":"p-0047","num":"0046"},"figref":["FIG. 2","FIG. 2","FIG. 1","FIG. 2"],"b":["200","200","100","200"]},"The generation environment () can include a source code compiler front end (), which can receive existing source code (). For example, the source code () may be entered by a user with a user input device and\/or transmitted to the front end () in some other manner. The source code () can be standard sequential source code, such as standard source code in C++ format, or it may already include some parallel code.","The front end () can produce an intermediate transmittal representation () of the existing source code (), which can be passed to a source code compiler back end (). The front end () and the back end () can both be part of standard compiler program, such as the compiler in Microsoft's Visual Studio\u00ae Dev10 development system. However, the compiler, and especially the back end (), can include the additional tools and techniques described herein for generating parallel representations of existing source code. Other implementations of the compiler may be possible, such as a compiler that does not have a clear boundary between front end and back end tools and techniques.","The back end () can generate a back end intermediate representation of the existing source code (). For example, the back end representation may be a standard control flow graph (). Such a flow graph () can group code into basic blocks, and can represent the flow between those basic blocks. Using the flow graph (), the back end () can generate one or more region graphs (), which can also represent the existing source code (), and can further group together the basic blocks of the flow graph () into regions. For example, the region graphs () can include a loop graph (), which can identify regions corresponding to loops (for loops, while loops, etc.) in the existing source code (). As another example, the region graphs () can include a single entrance single exit graph (), which can identify regions of the existing source code () that have a single entry point and a single exit point. As will be explained more below, such single entry single exit regions can correspond to groups of one or more basic blocks that have a control dependence on a common set of one or more other basic blocks. As yet another example, the region graphs () can include an exception handling graph (), which can indicate regions of exception handling routines (e.g., try and catch routines) in the existing source code (). It can be useful to identify each of these types of regions so that they can be identified as task regions in a task graph (). Other types of region graphs can also be used. For example, a graph of user-defined regions can be used. Such user-defined regions could include code that is wrapped in user-defined lambda expressions in the existing code. Indeed, some of those lambda expressions in the existing source code () could already be configured to schedule some existing code to run in parallel, and additional parallelism could be identified and scheduled using the tools and techniques discussed herein. Lambda expressions will be explained below.","A control dependence graph () and a data dependence graph () can both be generated by the back end (). The control dependence graph () can map control dependencies of the task regions in the task graph (), and the data dependence graph () can map the data dependencies of the task regions in the task graph (). The dependence graphs ( and ) can be formed as various different types of data structures, such as annotations of the task graph () and\/or as one or more tables.","Using the task graph (), the control dependence graph (), and the data dependence graph (), the back end () can produce a modified parallel representation of the existing source code () that is configured to self-schedule the set of task regions of the existing source code () as tasks. The modified parallel representation can include code that can track dependencies and can schedule a task region for execution as a task, possibly in parallel with another task region if such parallel execution can be done consistently with the data dependencies of the task regions. For example, the modified representation may be modified parallel source code () or a modified parallel intermediate compiler representation of the existing source code (). Such a modified intermediate compiler representation or the modified parallel source code () may be compiled into modified parallel object code (). For example, the modified intermediate representation may be translated into the modified parallel source code (), modified parallel object code (), or both, using standard compiler techniques to compile intermediate representations into object code, and to translate code between intermediate representations such as graphs and source code. Also, the modified parallel source code () may be edited by an automated tool or a user prior to being compiled into object code. For example, a user could edit the source code to break dependencies between some task regions and allow for additional parallelism in the resulting code.","III. Generating a Task Graph","A. General Discussion of Generating a Hierarchical Task Graph","As noted above, a control flow graph can be generated in a standard manner. A hierarchical task graph (HTG) can be generated using the flow graph. The HTG can be a directed-acyclic-graph or DAG, which contains no cycles. At the lowest level of abstraction, each node in the HTG can contain (by incorporating, representing, be linked to, etc.) 1 to N nodes, or blocks, in the flow graph. At the next level in the hierarchy, each region node can contain one or more region nodes from the previous level. Successively higher levels of the HTG can be generated until a root node that contains the entire program is generated.","The initial level of the HTG can include task regions, which represent regions of flow graph nodes. For example, the task regions can include call site regions, single entrance single exit regions, loop regions, and exception handling regions. The task regions of the HTG could also represent other types of regions, such as user-defined regions. This first level region HTG can form a DAG. Data-dependence and control dependencies between nodes can be computed at this level.","Another higher level graph can be generated. Each task region in the higher level graph can include one or more of the initial HTG task nodes, including \u201ccompound\u201d task regions that reduce the number of nodes and edges (i.e., links between nodes) from those at the previous level. Control and data dependencies can be computed at this level. This technique of adding additional levels with reduced numbers of nodes can be repeated until a level is generated with only one node that includes all the task regions.","When this HTG is complete, it can define task regions that can be scheduled at program execution time as tasks when the dependencies (i.e., prerequisites) for each task region have been completed. The code to check that the dependencies have been honored and the code to schedule the task region itself can be emitted by the compiler, as will be discussed more below. Thus, the compiler can be involved in scheduling task regions to run as tasks.","B. Examples of Generating a Task Graph","An HTG can be generated to identify and handle exception handling routines (e.g., C++ and structured exception handling), loops, single entrance single exit regions, and user specified regions (such as those specified with lambda constructs discussed below). Exception handling routines can complicate flow graphs with try bodies, handlers and asynchronous edges that model flow that \u201cmay\u201d happen in the event of a fault at runtime. An exception handling region tree can model containment and adjacency, and can ensure that one entire try with all of its associated handlers is not split across threads.","The HTG can be formed in one traversal of the control flow graph, from the following 3 region graphs: (1) loop region graph (loop graph); (2) exception handling region graph (EH graph); and (3) single entrance single exit region graph (SESE graph). Other region graphs may also be used, such as user-defined region graphs. Such region graphs can be used in the same way as these three region graphs when generating the HTG. The SESE, EH and Loop graphs are referred to as the initial independent region graphs, or region graphs.","The region graphs can be built on top of an ordinary flow graph (such as by including indications of the three types of regions in a tuple associated with each basic block of a flow graph). Thus, each flow graph node can have one edge pointing back into a containing region in the SESE graph; possibly one edge pointing back into a containing region in the EH graph; and possibly one edge pointing back into the loop graph. Typically, all flow graphs nodes will be members of at least one identically control dependent region. It is optional that a flow graph node would be contained in a loop or an exception handling region. Indeed, the existing source code may not include any loops or exception handling routines.","The following sections will provide examples of loops the associated loop graph, single entrance single exit regions and the associated SESE graph, and exception handling routines and the associated EH graph.","Each of the region graphs can form a DAG, and the three DAGs can be used to form one unified region graph for scheduling parallel code: the HTG. Accordingly, after examples of the region graphs are discussed, an example of a technique for walking the flow graph (whose nodes are annotated with pointers to the three region graphs) and generating the HTG will be explained. The HTG can be used to form task regions that can be dispatched as machine code tasks at runtime and\/or used to generate parallel source code, such as parallel C++ source code.","1. Loops and the Associated Loop Graph","Loops and loop graphs will be explained with reference to the following C++ source code and the associated loop graph () illustrated in .",{"@attributes":{"id":"p-0066","num":"0065"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"168pt","align":"left"}}],"thead":{"row":[{"entry":[{},"TABLE 1"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"void main ( )"]},{"entry":[{},"{"]},{"entry":[{},"\u2003int i, j, k, A[1000], B[1000], C[1000];"]},{"entry":[{},"\u2003for (i = 0; i < 1000; i++) \/\/ loop 1"]},{"entry":[{},"\u2003{"]},{"entry":[{},"\u2003\u2003for (j = 0; j < 1000; j++) \/\/ loop 2"]},{"entry":[{},"\u2003\u2003{"]},{"entry":[{},"\u2003\u2003\u2003\u2009A[i] += B[j];"]},{"entry":[{},"\u2003\u2003}"]},{"entry":[{},"\u2003\u2003\u2009for (j = 0; j < 1000; j++) \/\/ loop 3"]},{"entry":[{},"\u2003\u2003{"]},{"entry":[{},"\u2003\u2003\u2003\u2009C[i] += A[j];"]},{"entry":[{},"\u2003\u2003}"]},{"entry":[{},"\u2003}"]},{"entry":[{},"\u2003for (k = 0; k < 1000; i++) \/\/ loop 4"]},{"entry":[{},"\u2003{"]},{"entry":[{},"\u2003\u2003\u2003\u2009A[i] = B[i] + C[i];"]},{"entry":[{},"\u2003}"]},{"entry":[{},"\u2009}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"As can be seen, the loop graph () can be arranged in a hierarchical manner, where a root node () includes loop  node (), which includes a loop  node () and a loop  node (). The loop  node () corresponds to loop  above, and the loop  and  nodes correspond to loops  and  above, which are nested in loop . Additionally, a loop  node () is also included in the root node (). The loop graph () reveals that loop  and loop  might be able to run in parallel, or loop  and loop  might be able to run in parallel. Accordingly, the loop graph () can begin to reveal information about possible ways to partition the program to exploit parallelism. Additional information and analysis can be used to determine what parallelism actually exists in the example. This additional information and analysis could include determining whether there is any exception handling in or around these loops, and whether there is any control dependence that determines whether a particular loop will ever execute. The following discussion will explain some of this analysis, as it relates to exception handling and control dependence.","2. Exception Handling and Associated EH Graph","Different types of exception handling routines may exist in code. Two examples are structured exception handling and C++ exception handling. Such exception handling routines will now be discussed, followed by a discussion of techniques for dealing with such exception handling routines using an EH graph.","Following this paragraph is an example of structured exception handling (SEH). Note that control flow may go to the handler, or it may flow around the handler. Accordingly, the control-flow-graph for this C++ function could be complicated by the exception handling, such as by having an edge type that captures the semantic of a possible rare event like divide by zero.",{"@attributes":{"id":"p-0071","num":"0070"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"thead":{"row":[{"entry":[{},"TABLE 2"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"BOOL SafeDiv(INT32 dividend, INT32 divisor, INT32 *pResult)"]},{"entry":[{},"{"]},{"entry":[{},"\u2003_try"]},{"entry":[{},"\u2003{"]},{"entry":[{},"\u2003\u2003*pResult = dividend \/ divisor;"]},{"entry":[{},"\u2003}"]},{"entry":[{},"\u2003_except(GetExceptionCode( ) =="]},{"entry":[{},"\u2003EXCEPTION_INT_DIVIDE_BY_ZERO ?"]},{"entry":[{},"\u2003\u2003\u2003EXCEPTION_EXECUTE_HANDLER :"]},{"entry":[{},"\u2003\u2003\u2003EXCEPTION_CONTINUE_SEARCH)"]},{"entry":[{},"\u2003{"]},{"entry":[{},"\u2003\u2003Printf(\u201cI\u2019m in the handler becuase divisor was zero,"]},{"entry":[{},"\u2003\u2003*pResult is garbage\\n\u201d);"]},{"entry":[{},"\u2003}"]},{"entry":[{},"\u2003return TRUE;"]},{"entry":[{},"}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"Following is an example of C++ exception handling (CEH), where \u201cMyFunction( )\u201d might throw an object of a certain type, where the object would be caught by a catch routine.",{"@attributes":{"id":"p-0073","num":"0072"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"196pt","align":"left"}}],"thead":{"row":[{"entry":[{},"TABLE 3"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"int main( )"]},{"entry":[{},"{"]},{"entry":[{},"\u2003cout << \u201cIn main.\u201d << endl;"]},{"entry":[{},"\u2003try"]},{"entry":[{},"\u2003{"]},{"entry":[{},"\u2003\u2003cout << \u201cIn try block, calling MyFunc( ).\u201d << endl;"]},{"entry":[{},"\u2003\u2003MyFunc( );"]},{"entry":[{},"\u2003}"]},{"entry":[{},"\u2003catch( CTest E )"]},{"entry":[{},"\u2003{"]},{"entry":[{},"\u2003\u2003cout << \u201cIn catch handler.\u201d << endl;"]},{"entry":[{},"\u2003\u2003cout << \u201cCaught CTest exception type: \u201c;"]},{"entry":[{},"\u2003\u2003cout << E.ShowReason( ) << endl;"]},{"entry":[{},"\u2003}"]},{"entry":[{},"\u2003catch( char *str )"]},{"entry":[{},"\u2003{"]},{"entry":[{},"\u2003\u2003cout << \u201cCaught some other exception: \u201c<< str << endl;"]},{"entry":[{},"\u2003}"]},{"entry":[{},"\u2003cout << \u201cBack in main. Execution resumes here.\u201d << endl;"]},{"entry":[{},"\u2003return 0;"]},{"entry":[{},"}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"In the above example, the handler that is chosen for an exception in the try body is dependent on the \u201ctype\u201d of the exception. Thus, the underlying flow graph for the function main( ){ . . . } can be even more complex because control may: (1) transition from the try body to the lexically first handler and then the last cout and return; (2) transition from the try body to the lexically second handler and then the last cout and return; (3) transition from the try body, never throw an exception object, and then execute the last cout( ) and return 0.","These types of exception handling routines can get more complicated if a handler can re-throw an exception, causing more complex flow graphs with more possible edges between basic blocks. For example, a throw expression with no operand can re-throw the exception currently being handled, where the re-thrown exception object is the original exception object, rather than a copy. Such a re-throw expression typically appears only in a catch handler or in a function called from within a catch handler. Following is an example of an exception handling routine with such a re-throw expression:",{"@attributes":{"id":"p-0076","num":"0075"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":[{"entry":"TABLE 4"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"Try"},{"entry":"{"},{"entry":"\u2003throw CSomeOtherException( );"},{"entry":"}"},{"entry":"catch(...) \u2003\/\/ Handle all exceptions"},{"entry":"{"},{"entry":"\u2003\/\/ Respond (perhaps only partially) to exception"},{"entry":"\u2003throw; \u2003\/\/ re-throw, passing same exception to some other handler"},{"entry":"}"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}}}},"Because exception handling routines are common in system programs and commercial applications, it can be useful to deal with the resulting complexities of such routines in order to provide a general technique for generating parallel representations of source code. Accordingly, an EH region graph with an arbitrarily complex C++ exception handling construct reproduced below will be discussed with reference to .",{"@attributes":{"id":"p-0078","num":"0077"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"196pt","align":"left"}}],"thead":{"row":[{"entry":[{},"TABLE 5"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"int main( )"]},{"entry":[{},"{"]},{"entry":[{},"\u2003cout << \u201cIn main.\u201d << endl;"]},{"entry":[{},"\u2003try \u2003\/\/ \u2002TRY_1"]},{"entry":[{},"\u2003{"]},{"entry":[{},"\u2003\u2003cout << \u201cIn try block, calling MyFunc( ).\u201d << endl;"]},{"entry":[{},"\u2003\u2003try \u2003\/\/ \u2002TRY_2"]},{"entry":[{},"\u2003\u2003{"]},{"entry":[{},"\u2003\u2003\u2003MyFunc( );"]},{"entry":[{},"\u2003\u2003}"]},{"entry":[{},"\u2003\u2003catch (...)"]},{"entry":[{},"\u2003\u2003{"]},{"entry":[{},"\u2003\u2003\u2003throw;"]},{"entry":[{},"\u2003\u2003}"]},{"entry":[{},"\u2003}"]},{"entry":[{},"\u2003catch( CTest E )"]},{"entry":[{},"\u2003{"]},{"entry":[{},"\u2003\u2003cout << \u201cIn catch handler.\u201d << endl;"]},{"entry":[{},"\u2003\u2003cout << \u201cCaught CTest exception type: \u201c;"]},{"entry":[{},"\u2003\u2003cout << E.ShowReason( ) << endl;"]},{"entry":[{},"\u2003}"]},{"entry":[{},"\u2003catch( char *str )"]},{"entry":[{},"\u2003{"]},{"entry":[{},"\u2003\u2003cout << \u201cCaught some other exception: \u201c<< str << endl;"]},{"entry":[{},"\u2003\u2003try \u2003\/\/ \u2002TRY_3"]},{"entry":[{},"\u2003\u2003{"]},{"entry":[{},"\u2003\u2003}"]},{"entry":[{},"\u2003\u2003catch (...)"]},{"entry":[{},"\u2003\u2003{"]},{"entry":[{},"\u2003\u2003}"]},{"entry":[{},"\u2003}"]},{"entry":[{},"\u2003try \u2003\/\/ \u2002TRY_4"]},{"entry":[{},"\u2003{"]},{"entry":[{},"\u2003}"]},{"entry":[{},"\u2003catch (...)"]},{"entry":[{},"\u2003{"]},{"entry":[{},"\u2003}"]},{"entry":[{},"\u2003cout << \u201cBack in main. Execution resumes here.\u201d << endl;"]},{"entry":[{},"\u2003return 0;"]},{"entry":[{},"}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"The example above includes four try routines, labeled TRY_, TRY_, TRY_, and TRY_. This example can be modeled by the EH region graph () of . The EH graph () includes a root node (). The root node () can include basic blocks that are not nested within try bodies or handlers. The root node can thus include a TRY_ node () corresponding to the try routine labeled TRY_ above because the TRY_ routine is not nested within other try bodies or handlers. The TRY_ node () can include a TRY_ node () corresponding to the TRY_ routine above because the TRY_ routine is nested within the TRY_ routine. A catch routine is nested within the TRY_ routine, so the TRY_ node () can include a corresponding catch node (). Additionally, two catch routines are nested within the TRY_ routine above, so the TRY_ node () can include corresponding catch nodes ( and ). One of those catch nodes () can also include a TRY_ node () corresponding to the TRY_ routine above because that try routine is nested within the corresponding catch routine above. Additionally, the TRY_ node () can include a catch node () corresponding to a catch routine nested within the TRY_ routine above. Also, the root node () can include a TRY_ node (), and a catch node () nested within the TRY_ node (). Basic blocks outside of a try body or a handler (e.g., a catch routine) can be considered members of the root node (). The control flow graph for the example above can have pointers from the basic blocks back into the corresponding nodes in this EH graph ().","Useful information can be gleaned from the EH graph (). For example, it can be seen that the TRY_ and TRY_ are adjacent, that TRY_ is nested in a handler that is associated with TRY_, and that TRY_ is the only try body in the example that has 2 possible handlers. When generating the HTG, the EH graph can be used to pull off threads that are buried in exception handling routines (exception handling regions, such as try routines and handling (e.g., catch) routines) for inclusion as task regions in the HTG.","3. Single Entrance Single Exit Regions and Associated SESE Graph","The previous explanations of the loop graph and the EH graph were illustrated with source code, and showing how the graphs are generated from nested loops or exception handling routines in that source code. For illustrating the single entrance single exit regions in an SESE graph, a flow graph and a corresponding control dependence graph will be illustrated and discussed. To generate the SESE graph, the compiler can walk the control dependence graph (which can be used for code generation) and then form the single entrance single exit regions from that walk.","To state control dependence informally, a basic block Y is directly control dependent on basic block X if: (1) following one edge out of X will eventually execute Y; (2) the only way out after taking this edge is through Y; and (3) choosing some other edge out of X may avoid Y. In other words, node X will determine whether node Y will execute. A more formal statement of this relationship is that a basic block Y is directly control dependent on a basic block X if: (1) Y post dominates some successor of X; and (2) Y does not post dominate all successors of X.","Referring now to , a control flow graph () (illustrated on the left side of ) will be discussed along with its associated control dependence graph () (illustrated on the right side of ). The control flow graph () represents the flow of ten basic blocks: block  (), block  (), block  (), block  (), block  (), block  (), block  (), block  (), block  (), and block  (). From the flow graph () it can be seen that block  (), block  (), and block  () will execute if and only if the condition in block  () is true. Accordingly, block  (), block  (), and block  () depend directly from block  () in the control dependence graph (), as illustrated by the arrows extending directly from block  () to block  (), block  (), and block  ().","Block  () contains an iteration test for a loop that includes block  (), block  (), block  () or block  (), block  (), and block  (). Accordingly, block  (), block  (), block  (), and block  () will execute (or execute again) if and only if the condition in block  () is true. Accordingly, block  (), block  (), block  (), and block  () depend directly from block  () in the control dependence graph (). For block  (), the dependence on itself is illustrated in the control dependence graph () by the arrow extending from the bottom of block  () around and to the top of block  ().","Block  () contains an iteration test for a loop that includes block  (), block  () or block  (), and block  (). Accordingly, block  () and block  () will execute again if and only if the condition in block  () is true, and block  () and block  () depend directly from block  () in the control dependence graph (). Note that the inner loop that includes block  (), block  () or block  (), and block  () is a loop, such as a do-while loop, that will execute once before the condition for the loop is evaluated. In contrast, the outer loop that includes block  (), block  (), block  () or block  (), block  (), and block  () is a loop that will not execute at all if the condition in block  () is initially false.","Additionally, block  () will execute if and only if the condition in block  () is false, and block  () will execute if and only if the condition in block  () is true. Accordingly, block  () and block  () both depend directly from block  () in the control dependence graph ().","In addition to the direct dependencies, the control dependence graph can reveal indirect dependencies between blocks. A block X is indirectly control dependant on another block Y if taking one of multiple possible edges out of block Y may, but will not necessarily, lead to the execution of block X. For example, referring to , if the condition in block  () is true, that condition is necessary for block  () to execute, but it is insufficient. This is because the condition in block  () also has to be true for block  () to execute. In the control dependence graph, a second block is indirectly control dependent on a first block if there is no line extending directly from the first block to the second block, but there is a path of lines extending from the first block, through one or more other blocks, and to the second block. For example, a path of lines extend from block  (), through block  (), and to block  (), so block  () is indirectly control dependent on block  ().","Note that block  () does not appear in the control dependence graph () because block  () is the last block and block  () is always executed. Accordingly, no other blocks depend on block  (), and block  () does not depend on any other blocks.","From the control dependence graph, the single entrance single exit regions can be generated. These are regions that all share a common set of control dependence edges. In other words a single entrance single exit region includes a set of blocks, where each block in the set has the same control dependency (directly or indirectly) on at least one common block. Note that each region with this common control dependency can also be a single entrance single exit region of blocks in a corresponding flow graph.","To produce the SESE graph, the control dependence graph can be walked in a depth-first-order, while noting the current edge (with its label) on a state stack. The current stack of control dependence edges (from the root node of the control dependence graph to the current block in the walk) forms a set of labels for the block that the walk is entering. A set of nodes with a common \u201clabel\u201d (i.e., a common edge in the control dependence graph) can form a SESE region.","Referring now to , the control flow graph () and the control dependence graph () of  are shown with single entrance single exit regions being circumscribed with dashed lines or boxes. As is illustrated, a region, SESE  (), includes block  (), block  (), block  (), block  (), block  (), block  (), block  (), and block  (), all of which are control dependent (either directly or indirectly) on the condition in block  () being true. Another region, SESE  (), includes block  (), block  (), block  (), block  (), block  (), and block  (), all of which are control dependent (either directly or indirectly) on the condition in block  () being true. Another region, SESE  (), includes block  (), block  (), block  (), and block  (), all of which are control dependent (either directly or indirectly) on the condition in block  () being true. Another region, SESE  (), includes block  (), which is control dependent on the condition in block  () being false. Yet another region, SESE  (), includes block  (), which is control dependent on the condition in block  () being true.","As one example of how the block labels correspond to the regions, the basic blocks in SESE  () would have basic block labels as follows: {1T, 3T, 7T} for block  (), {1T, 3T, 7T, 4F, 5} for block  (), and {1T, 3T, 7T, 4T, 6} for block  (). In these sets of labels, the lowest common label is 7T, so the region head is 7T (i.e., the condition in block  () being true).","4. Redundancy Between Loops and Single Entrance Single Exit Regions","To reduce complexity, it can be desirable to create an HTG with fewer numbers of nodes and edges. Note that there can be redundancy between the single entrance single exit regions and loop regions. This redundancy can be avoided when generating the HTG by creating a priority and using only the redundant region with the highest priority. For example, exception handling regions can have highest priority, then loop regions, and then single entrance single exit regions.","In understanding why such redundancies can exist, consider that single entrance single exit regions are a property of edges, rather than nodes. Likewise, loops are a property of edges, not nodes. Loops can be identified from a flow graph by identifying an edge whose source is a block with a depth-first-order number that is greater than or equal to the depth-first-order number of the destination block. Single entrance single exit regions can be demarcated by edges as well. Two edges (A and B) demarcate single entrance single exit region if: (1) A dominates B (every execution path to B includes A); (2) B postdominates A (every execution path from A to program exit includes B); and (3) every loop containing A also contains B, and every loop containing B also contains A.","Referring now to , a flow graph is illustrated with block  (), block  (), and block  (). As illustrated on the left side, edge A from block  () to block  () and edge B from block  () to block  () define a single entrance single exit region SESE A-B () that includes only block  (). In addition, as illustrated on the right, edge C from the exit of block  () to the entrance of block  () defines a loop region LOOP C () that also includes only block  (). Accordingly, region SESE A-B () and region LOOP C () are redundant.","The loop and single entrance single exit regions are used to map the regions to task regions, so that the compiler can generate the code to schedule the task regions as tasks or threads at runtime when the proper dependencies have been satisfied. A loop that is not a single entrance single exit region can be difficult to map to a thread. For example, such a loop may have several early exits with variables that are live on those exits. Mapping this onto a thread could be difficult, especially if it also involved exception handling.","If an HTG was being generated from these two regions, the generation could favor region LOOP C () over region SESE A-B () by ignoring region SESE A-B () for purposes of the HTG. Exception handling regions can take priority over loop regions, and loop regions can take priority over single entrance single exit regions. This assumption can eliminate useless concentric regions in the HTG.","Single entrance single exit regions can be useful to find parallelism in functions that contain no loops and no exception handling. Consider the code below and the corresponding flow graph () and SESE region graph () in . (Note that the code in  and the other figures is all upper case solely for formatting purposes.) As can be seen in the flow graph (), the code includes a number of blocks (, , , , , , and ) in straight line code with no loops. However, there are several single entrance single exit regions: R (), R (), R (), R (), R (), R (), R (), and a root region ().",{"@attributes":{"id":"p-0101","num":"0100"},"tables":{"@attributes":{"id":"TABLE-US-00006","num":"00006"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"147pt","align":"left"}}],"thead":{"row":[{"entry":[{},"TABLE 6"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Void foo (MyClass *p) {"]},{"entry":[{},"\u2003if (c1) {"]},{"entry":[{},"\u2003\u2003p->F0( );"]},{"entry":[{},"\u2003\u2003if (c2) {"]},{"entry":[{},"\u2003\u2003\u2003p->F1( );"]},{"entry":[{},"\u2003\u2003\u2003p->F2( );"]},{"entry":[{},"\u2003\u2003}"]},{"entry":[{},"\u2003}"]},{"entry":[{},"\u2003if (c3) {"]},{"entry":[{},"\u2003\u2003p->F3( );"]},{"entry":[{},"\u2003}"]},{"entry":[{},"\u2003p->F4( );"]},{"entry":[{},"}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"Note that it might be possible to run R () and R () in parallel if the inter-procedural information proves data independence between the regions (i.e., if p\u2192F( ) does not write data that p\u2192F( ) touches and vice versa). If R () is data dependent on R (), then it might be possible to go up in the hierarchy and run R () and R () in parallel. That level of parallelism would require knowing the control dependence of \u201cif (c)\u201d and \u201cif (c)\u201d were satisfied. Thus, the example above contains no loops or exception handling, but the single entrance single exit regions can still be useful to determine whether functional level parallelism is present.","5. Hierarchical Task Graph Example","The three region graphs (which can be DAGs) described above (EH graph, loop graph, and SESE graph) can be used to generate the HTG. The HTG can be generated in one pass over the existing compiler structures that are normally built for optimized sequential code generation, such as a flow graph that is annotated with indications of the regions to which each block in the flow graph belongs.","Referring now to , source code () is shown on the left with task regions circumscribed in dashed lines. A corresponding HTG () is shown on the right with regions in a hierarchical configuration. A screen such as  can be displayed to a user, and the user can edit the source code (), such as to break dependencies between task regions to provide for additional parallelism. For example, a screen such as  could be included in a visual editor display, such as a Microsoft Visual Studio\u00ae VC++ IDE screen.","Referring still to , the source code () can be automatically grouped into task regions by the compiler, and the task regions can be represented on the HTG (). The HTG graph () can include a root or start node (). The task regions can include a task region T () that is an exception handling try region on a level below the start node () in the HTG (). Within the task region T (), and one level below the task region T () in the HTG (), are a task region T () that is a loop region, a task region T () that is a single entrance single exit region, and another task region T () that is a loop region. Additionally, below the start node () there can be three single entrance single exit task regions: task regions T (), T () and T (). Also, within the task region T (), there can be two loop task regions T () and T () that are one level below the task region T () in the HTG (). There may also be additional task regions such as single entrance single exit task regions (, , , and ) corresponding to code that is nested within the bodies of the loop task regions T (), T (), T (), and T (), respectively.","Leaf nodes in the hierarchical task graph, such as T (), T (), T (), T (), and T (), can all be run in parallel as tasks if and only if the control and data dependencies for those task regions are honored. For example, if *p is less than 9, then T () cannot be run, but T () and T () may be able to run in parallel as tasks if there are no data dependencies between these two task regions. Modeling control and data dependencies between the nodes, or task regions, in an HTG will be discussed below.","6. Generating the Hierarchical Task Graph from Region Graphs","In each of the three region graphs, each node can be numbered  to N so that each node in the loop graph has its own identifier ( to N), each node in the EH graph has its own identifier ( to N), and each node in the SESE region graph has its own identifier ( to N). Once the flow graph nodes are annotated with pointers back to the three initial region graphs (EH,LOOP,SESE), a function of those basic block links can be used to determine membership of the corresponding blocks in a particular task graph node. A \u201chash\u201d function can be used while performing a depth-first-walk of the original flow graph to detect when any one of the three links changes when walking from one flow graph node to the next. Such a change can correspond to entering a new HTG task region that will be generated or that has been generated. This generation of the HTG will now be described in more detail.","The following code details a depth-first-ordered traversal of the basic blocks in the control flow graph. This code assumes every block has a bit that has been initialized to indicate that the block has not been visited. This bit can be flipped when the traversal visits the corresponding block. Thus, the traversal can be configured to visit every node only once.",{"@attributes":{"id":"p-0111","num":"0110"},"tables":{"@attributes":{"id":"TABLE-US-00007","num":"00007"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":[{"entry":"TABLE 7"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"\u2003void DFO (Block * b) {"},{"entry":"\u2003\u2003Foreach_Succ_Block( succ, b) {"},{"entry":"\u2003\u2003\u2003if ( ! succ->visited ) {"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"84pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"133pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["\u2003\u2003\u2003\u2003Process(b,succ);","\/\/ Build or find the corresponding HTG node"]},{"entry":"and point this block to it"},{"entry":["\u2003\u2003\u2003\u2003DFO(succ);","\/\/ Recurse across a flow graph edge"]},{"entry":[{},"to the next basic block"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"\u2003\u2003\u2003}"},{"entry":"\u2003\u2003}"},{"entry":"\u2003}"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"While recursively traversing the flow graph with the DFO( ) function, the HTG can be incrementally generated using an abstract state machine backed by stacks and the 3 region DAGs. Specifically, the three stacks (corresponding to the three region graphs) can be pushed and popped in the \u201cProcess(b,successor)\u201d call.  illustrates existing source code (), as well as data structures that can be used in generating a hierarchical task graph representing the source code. These structures can include three region graphs: an EH graph (), a loop graph (), and a SESE graph (). Each block () of the flow graph can be annotated with a triplet (), which indicates the lowest level exception handling region from the EH graph (), lowest level loop region from the loop graph (), and lowest level single entrance single exit region of the SESE graph () to which the block () belongs. This triplet can be used as a hash key to find or create an HTG task region to which the block () belongs.","The actual DFO traversal of the flow graph to generate the HTG will now be discussed. During the traversal of the flow graph, it can be useful to map the triplet () to an HTG node that already exists or is created during the traversal.","An example of code for the DFO traversal is set forth above in Table 7. Following is a sketch for the \u201cProcess\u201d function that pushes and pops while creating the HTG. (An example of code for the \u201cProcess\u201d function will be set forth below in Table 10.) This function uses the DAG that is formed when building the three initial region graphs.",{"@attributes":{"id":"p-0115","num":"0114"},"tables":{"@attributes":{"id":"TABLE-US-00008","num":"00008"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":[{"entry":"TABLE 8"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"void Process(Block *source , Block* destination) {"},{"entry":"\u2003\u2003pop_and close_htg_regions(3 stacks) \u2003\/\/ possibly leaving regions"},{"entry":"\u2003\u2003push_and_open_htg_regions(3 stacks) \/\/ possibly entering regions"},{"entry":"\u2003\u2003current_htg_node = LookUp(b, triplet(top_of_stacks(StackPtr))) \/\/"},{"entry":"\u2003\u2003find or create node"},{"entry":"\u2003\u2003b->htg = current_htg_node; \/\/ generate pointer to HTG node"},{"entry":"}"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}}}},"In the Process(b,succ) function (where b is the source block in the flow graph and succ is the destination block in the flow graph), three stacks are maintained during the walk or traversal of the flow graph\u2014one stack for each of the initial region graphs. As is indicated in the sketch above, the Process function can involve popping and closing HTG regions (i.e., HTG task regions) if the traversal leaves those regions when traversing from b to succ. Additionally, the Process function can involve pushing and opening HTG regions if the traversal leaves those regions when traversing from b to succ. As has been discussed above, a HTG region is indicated by a combination of entries from the three stacks. Thus, entering a region can involve pushing one or more corresponding region entries onto the top of the one or more of the three stacks, and leaving a region can involve popping to higher level region(s) by removing one or more corresponding regions from the top of one or more of the three stacks. Accordingly, as the traversal exits and enters different flow graph region boundaries, it can also enter or create the appropriate HTG node or task region being entered. Additionally, a hash key can be formed from the triplet, which includes indicators of the three different types of regions that may contain a basic block. Once the basic block is hashed to the correct HTG node using the triplet, the basic block can be linked to that HTG node.","Accordingly, the three stacks corresponding to the three initial types of regions (exception handling regions, loop regions, and single entry single exit regions) can be used to identify corresponding HTG nodes during the traversal. The top of each stack can form a \u201ctop of stack\u201d triplet (EH #, Loop #, SESE #) that can be used in the Process(b,succ) function as a key to hash a particular basic block to a node in the HTG. That HTG node may already exist or may be generated as it is needed during the traversal.","Mapping a basic block to an HTG node can include a function that looks up and uses the top-of-stack triplet: HierarchicalTaskNode*nodePtr=LookUp (block, top_of_stack(three_stacks)). This will indicate the triplet corresponding to the current block of the flow graph so long as the three types of initial region nodes are correctly pushed and popped while performing the depth first ordered traversal of the flow graph.","7. An Example of a Three-Region Stack for a GOTO Example",{"@attributes":{"id":"p-0120","num":"0119"},"figref":["FIG. 11","FIG. 11"],"b":["1110","1112","1112","1120","1122","1124","1112","1126","1112","1126","1112","1110"]},"Note that in , nodes of the flow graph () are superimposed over the source code. As illustrated, the flow graph includes a first try basic block (), a second nested try basic block (). Additionally, nested within the second try basic block () is an \u201cif\u201d basic block (). If the \u201cif\u201d basic block () returns false, then an \u201ca=b+c\u201d basic block () can execute. However, if the \u201cif\u201d basic block () is true, then a GOTO basic block () can cause an edge to be taken to a LABEL basic block (). The LABEL basic block () can be outside of the try routines and the three nested loops (indicated by the three curved arrows on the left that contain the basic blocks (, , and )). Note that parts of the flow graph () have been omitted in order to focus on maintaining the triplet and the three-region stack () when the traversal takes an edge from the GOTO basic block () to the LABEL basic block ().","If the depth first ordered traversal walks from the GOTO basic block () to the LABEL basic block () and an HTG is being generated while walking the flow graph, then the three-region stack () can capture the scenario that the traversal is leaving two exception handling regions and three loop regions. This can lead to the traversal exiting some corresponding HTG structure that is not yet fully generated (such as the structure for the exception handling regions and loop regions). The traversal can exit those regions, and can later return to those regions and add to them as needed.","The top version of the three-region stack () shows the stack when the traversal is at the GOTO basic block (). The tops () of the three stacks (, , ) form a triplet that indicates that the GOTO basic block () is in an exception handling region T, a loop region L, and a single entry single exit region S. Taking the edge to the LABEL basic block () can result in exiting two exception handling regions T and T and three loop regions L, L, and L. This can be indicated by pushing new root node indicators to the new top () of the exception handling stack () and the loop stack (), as well as pushing a new single entrance single exit region S to the top () of the single entrance single exit stack (). As will be shown below, an alternative could involve removing the exception handling region indicators and the loop region indicators from the stack, and then adding them back in later when the traversal returns to those loop and exception handling regions.","Once the recursion in the depth first ordered traversal function returns back to the split in the flow graph at the \u201cif\u201d basic block (), the function can regain the state for that block (i.e., have the three-region stack () indicate the proper containing regions at that point) so that the function can resume adding basic blocks to the correct HTG node under construction. This can be done by removing the new top () of the stack when the edge is recursively taken back from the LABEL basic block () to the GOTO basic block (), and then resuming normal pushing and popping from that point.","8. An Example of a Hierarchical Task Graph for a Nested Loop Example","To describe the proper maintenance of the three stacks during the DFO routine, an example of the generation of a HTG from an example of loop nested code will be discussed. Consider the following pseudo code for three nested loops that have been bottom tested. The compiler can bottom test a loop in a standard way so that the loop is in a canonical form for loop invariant motion to have a place to always pull invariants, and so that the control flow is optimized for the fewest number of branches upon termination of the loop.",{"@attributes":{"id":"p-0127","num":"0126"},"tables":{"@attributes":{"id":"TABLE-US-00009","num":"00009"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"168pt","align":"left"}}],"thead":{"row":[{"entry":[{},"TABLE 9"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Before Bottom Testing:"]},{"entry":[{},"\u2003while ( i < 100) {"]},{"entry":[{},"\u2003\u2003while (j < 100) {"]},{"entry":[{},"\u2003\u2003\u2003while (k < 100) {"]},{"entry":[{},"\u2003\u2003\u2003\u2003... Loop body ..."]},{"entry":[{},"\u2003\u2003\u2003}"]},{"entry":[{},"\u2003\u2003}"]},{"entry":[{},"\u2003}"]},{"entry":[{},"\u2003A = B + C;"]},{"entry":[{},"Canonical Form After Bottom Testing:"]},{"entry":[{},"\u2003if ( i <100) {"]},{"entry":[{},"L1:"]},{"entry":[{},"\u2003\u2003if (j <100) {"]},{"entry":[{},"\u2002L2:"]},{"entry":[{},"\u2003\u2003\u2003if (k < 100) {"]},{"entry":[{},"\u2003L3:"]},{"entry":[{},"\u2003\u2003\u2003\u2003... Loop body ..."]},{"entry":[{},"\u2003\u2003\u2003\u2003if (k <100) GOTO L3"]},{"entry":[{},"\u2003\u2003\u2003}"]},{"entry":[{},"\u2003\u2003\u2003if (j < 100) GOTO L2"]},{"entry":[{},"\u2003\u2003}"]},{"entry":[{},"\u2003\u2003if (i < 100) GOTO L1"]},{"entry":[{},"\u2003}"]},{"entry":[{},"\u2003A = B + C;"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"Note that loop invariant code can be placed just before L, L, or L, depending on the alias information and data dependencies in each loop. Also note that the branch and test is at the bottom of the loop which can result in the shortest code path for loop termination and iteration.",{"@attributes":{"id":"p-0129","num":"0128"},"figref":["FIG. 12","FIG. 12"],"b":["1210","1212","1210","1212","1210","1","1220","1","2","1222","1","2","3","1224","2","3","4","1226","3","3","3","5","1228","2","6","1230","1","1","1250","1","1252","1250","2","1254","1","1252","3","1256","2","1254","1","1220","1250","2","1222","6","1230","1","1252","3","1224","5","1228","2","1254","4","1226","3","1256","1212"]},"When generating the HTG while performing a random DFO traversal the flow graph (), the function can pop and push nodes on the region stacks. This pushing and popping can help ensure that the HTG is constructed so that generating the final HTG results in a mapping from each basic block to the innermost, or lowest level, HTG node containing that basic block.","As an example, the depth first ordered traversal of the flow graph () can traverse the basic blocks in the following order { (),  (), . . . ,  (),  (),  (),  ()}, where the recursive descent randomly moves out of the loop at basic block  () by going to the successors of block  (). Leaving the loop nest is represented as the \u201c . . . \u201d in the sequence { (),  (), . . . ,  (),  (),  (),  ()}. The recursion can eventually pop back to block  () that was previously visited, and continue on to the unvisited node at block  (), as was discussed in the GOTO example above. The actions to build the HTG during the traversal will be discussed below, without discussing the specifics of leaving the loop nest at block ().","9. Least Common Ancestor and Path Graph Concepts","Because the code to form the HTG can use the concepts of a lowest common ancestor (LCA) of two nodes, and of a path of ancestors from a node to another node, those concepts will now be discussed with reference to .  illustrates a general graph (), such as a control flow graph. The graph () can have a root node 0 () that points down to a node 1 (). In turn, node 1 () can point down to a node 2 (), which can point down to a node 3 (). Node  () can also point down to a node 4 (), which can point down to a node 5 (). In addition, root node 0 () can point down to a node 6 (), which can point down to a node 7 ().","The LCA of two nodes is the ancestor that the two nodes have in common that is lowest on the graph. LCA is commutative. For example, in the graph (), the LCA of node 3 () and node 5 () is node 1 (), i.e., LCA (,)=1. As another example, the LCA of node 3 () and node 7 () is root node 0 (), i.e., LCA (,)=0.","The ancestor path from a source node to a destination node includes the destination node, the LCA of the source and destination nodes, and all nodes in the path between the destination node and LCA of the source and destination nodes. Ancestor path is not commutative. For example, the ancestor path from node 1 () to node 5 () includes node 1 (), node 4 (), and node 5 (), i.e., Path(\u21901)={1,4,5}. As another example, the ancestor path from node 3 () to node 5 () is the same as the path from the LCA of node 3 () and node 5 () (which is node 1 ()) to node 5 (), and that ancestor path includes node 1 (), node 4 (), and node 5 (). In other words, Path(\u21903)=Path(\u2190LCA(,)=Path(\u21901)={1,4,5}. Path(X\u2190Y) can also be expressed as Path(X,Y), i.e., Path(destination,source).","As will be seen below, LCA and Path can be used to simplify expressions that allow the function for generating the HTG to exploit the nature of the region graphs that are encountered while traversing the basic blocks of the flow graph.","10. Code to Form HTG Region Nodes","Following is an example of code that can be used to do the depth first ordered traversal of the flow graph to produce the HTG. This code can include the following code for a DFO driver that drives the depth first ordered traversal from one block to another, and the process code that performs the traversal:",{"@attributes":{"id":"p-0139","num":"0138"},"tables":{"@attributes":{"id":"TABLE-US-00010","num":"00010"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":[{"entry":"TABLE 10"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"DFO Driver, i.e., DFO( ):"},{"entry":"\u2003void DFO (Block * src_block, Block * curr_block) {"},{"entry":"\u2003\u2003Foreach_Succ_Block( succ, curr_block) {"},{"entry":"\u2003\u2003\u2003if ( ! succ->visited ) {"},{"entry":"\u2003\u2003\u2003\u2003Process(curr_block, succ); \/\/ traverse the flow graph edge"},{"entry":"\u2003\u2003\u2003\u2003DFO(curr_block, succ);"},{"entry":"\u2003\u2003\u2003}"},{"entry":"\u2003\u2003}"},{"entry":"\u2003}"},{"entry":"Processing Edges Between Blocks, i.e., Process( ):"},{"entry":"\u2003void Process(Block * src, Block * dest) {"},{"entry":"\u2003\u2003\u2002lca = LCA(dest->regions, src->regions); \/\/ find the lowest"},{"entry":"\u2003\u2003\u2002common ancestors"},{"entry":"\u2003\u2003\u2002pop_to(lca); \u2003\/\/ possibly exiting regions"},{"entry":"\u2003\u2003\u2002push(PATH(dest,lca)); \u2003\/\/ possibly entering regions"},{"entry":"\u2003\u2003\u2002current_htg_node = LookUp(b,triplet(top_of_stacks(StackPtr)));"},{"entry":"\u2003\u2003\u2002\/\/ find or create"},{"entry":"\u2003\u2003\u2002dest->htg = current_htg_node; \u2003\/\/ assign task graph node"},{"entry":"\u2003\u2003\u2002to basic block"},{"entry":"\u2003}"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}}}},"In this code, the DFO Driver can drive the traversal through the edges between blocks in the control flow graph. For each such edge that is taken, the Process function can perform a number of actions. First, the Process function can find the lowest common ancestor regions of regions that contain the source block and the destination block in the EH graph, loop graph, and SESE graph.","The Process function can also pop to the least common ancestor region in each of the three region graphs. This can include removing regions from the top of the three-region stack so that the top of the three-region stack includes the least common ancestor region from each of the three region graphs. As noted in the comment above, this can represent exiting regions in the traversal.","The Process function can also push to the three-region stack the regions on the ancestor path from the least common ancestor regions to the innermost regions containing the destination block. As noted in the comment above, this can represent entering regions in the traversal.","The Process function can also set the current HTG node equal to a hash lookup using the triplet formed by the references to regions in the top of the three-region stack. If no existing HTG node is found, a new HTG node can be generated.","Finally, the Process function can assign the found (existing or new) HTG node to the destination control flow graph block. This can include adding a pointer from the destination block to the corresponding HTG node.","The DFO driver can then drive the Process function to the next edge in the control flow graph, with the old destination block being the new source block in the function, and an unvisited block being the new destination block. In addition, the old destination block can be marked as having been visited, so that the DFO driver can avoid processing it multiple times.","11. Using Code to Form HTG Region Nodes in the Nested Loop Example","Referring now to , an example of generating a hierarchical task graph using the flow graph () and loop graph () of  will be discussed. For the sake of simplification and clarity, this discussion will only consider the loop stack\u2014omitting considerations of exception handling regions and single entry single exit regions. Accordingly, this example will, as a practical matter, map the loop graph onto a HTG.","In each of , the actions of the Process function will be set forth in the box at the top of the figure. The basic blocks of the flow graph () that have been visited will be indicated on the left, along with edges that have been taken in the traversal (i.e., not all edges in the flow graph () will be shown, only those that have been taken in the traversal). In addition, the current HTG () will be illustrated on the right, showing new nodes and links from the blocks of the flow graph () in dashed lines and existing ones in solid lines. The visited basic blocks of the flow graph () and the HTG are illustrated before (above) and after (below) the process actions.","To begin, the DFO routine can traverse basic block  (), as illustrated in , and perform Process (Null,) actions (). These actions () can include finding the LCA of the root loop region ( in ) and null, the LCA being the root loop region ( in ). Of course, no regions are being exited, so there is no need to pop to any regions. However, as  illustrates, block  () is in the root loop region ( in ), so the actions () can include pushing the root loop region onto the top of the three-region stack (not shown). The lookup will not find an HTG task region corresponding to the root loop region ( in ), so the actions () can include generating a root task region () in the HTG () corresponding to the root loop region ( in ). In addition, a new edge can be generated from block  () to the new root task region ().","Referring to , the DFO routine can traverse from block  () to block  () and perform Process (,) actions (). These actions () can include finding the LCA of the loop region L ( in ) to which block  () belongs, and the root loop region ( in ), the LCA being the root loop region ( in ). No regions are being exited, so there is no need to pop to any regions or remove any regions from the three-region stack. However, as  illustrates, block  is in the loop region L ( in ), so the actions () can include pushing the loop region L ( in ) onto the top of the three-region stack (not shown). The lookup will not find an existing HTG task region corresponding to the loop region L ( in ), so the actions () can include generating a new HTG task region T () corresponding to the loop region L ( in ) below the root task region (). In addition, a new edge can be generated from block  () to the new task region T ().","Referring now to , the DFO routine can traverse from block  () to block  () and perform Process (,) actions (). Note that the DFO routine could traverse the successors of block  () before proceeding to block  (), those successors could be traversed after block  () is traversed, or they could be traversed in some other order, but their traversal is not discussed in detail in this example. The actions () can include finding the LCA of the loop region L ( in ) to which block  () belongs and that same loop region L ( in ) to which block  () belongs, the LCA being loop region L ( in ). No regions are being exited, so there is no need to pop to any regions or remove any regions from the three-region stack. Additionally, no regions are being entered because the destination and source blocks are in the same region. The lookup will find existing HTG task region T () corresponding to the loop region L ( in ), so the actions () do not need to generate a new HTG task region. A new edge can be generated from block  () to the found task region T ().","Referring to , the DFO routine can traverse from block  () to block  () and perform Process (,) actions (). These actions () can include finding the LCA of the loop region L ( in ) to which block  () belongs, and the loop region L ( in ) to which block  () belongs, the LCA being loop region L ( in ). No regions are being exited, so there is no need to pop to any regions or remove any regions from the three-region stack. However, as  illustrates, block  is in the loop region L ( in ), so the actions () can include pushing the loop region L ( in ) onto the top of the three-region stack (not shown). The lookup will not find an existing HTG task region corresponding to the loop region L ( in ), so the actions () can include generating a new HTG task region T () corresponding to the loop region L ( in ) below the HTG task region T (). In addition, a new edge can be generated from block  () to the new task region T ().","Referring now to , the DFO routine can traverse from block  () to block  () and perform Process (,) actions (). These actions () can include finding the LCA of the loop region L ( in ) to which block  () belongs and that same loop region L ( in ) to which block  () belongs, the LCA being loop region L ( in ). No regions are being exited, so there is no need to pop to any regions or remove any regions from the three-region stack. Additionally, no regions are being entered because the destination and source blocks are in the same region. The lookup will find existing HTG task region T () corresponding to the loop region L ( in ), so the actions () do not need to generate a new HTG task region. A new edge can be generated from block  () to the found task region T ().","Referring to , the DFO routine can traverse from block  () to block  () and perform Process (,) actions (). These actions () can include finding the LCA of the loop region L ( in ) to which block  () belongs, and the loop region L ( in ) to which block  () belongs, which is L ( in ). No regions are being exited, so there is no need to pop to any regions or remove any regions from the three-region stack. However, as  illustrates, block  () is in the loop region L ( in ), so the actions () can include pushing the loop region L ( in ) onto the top of the three-region stack (not shown). The lookup will not find an existing HTG task region corresponding to the loop region L ( in FIG. ), so the actions () can include generating a new HTG task region T () corresponding to the loop region L ( in ) below the HTG task region T (). In addition, a new edge can be generated from block  () to the new task region T (), thereby completing the HTG ().","12. Using Code to Track the Stack When Generating an HTG in the GOTO Example","Referring now to , the DFO routine for traversing between the GOTO block () and the LABEL block () in the flow graph () of  will be discussed. In , actions in the DFO routine will be set forth in a box at the top of each figure. The flow graph () of  will be illustrated on the left side with block reference numbering corresponding to the block numbering in . Additionally, some pertinent regions are outlined in dashed lines, although not all regions are outlined in the flow graph () of  for the sake of brevity and focus. Particularly single entrance single exit region S () can include the \u201ca=b+c\u201d basic block (), and single entrance single exit region S () can include LABEL basic block (). As noted above with reference to , the flow graph itself is also not entirely complete because it is being used to focus on the DFO routine when traversing between particular blocks. At the right of each figure is a three-region stack (), including an exception handling region stack (), a loop region stack (), and a single entrance single exit region stack (). The top () of the three-region stack () forms a triplet that includes the top entry of each of the three stacks (, , and ) that can be hashed to a task region of a HTG that is being generated with the DFO routine.","Referring now to , the top illustration of the three-region stack () illustrates the stack () when the DFO routine is at the GOTO block () in the traversal of the flow graph (). The bottom illustration of the three-region stack () illustrates the stack () when the DFO routine is at the LABEL block (). The GOTO block () is nested within nested exception handling regions T and T, within nested loop regions L, L, and L, and within nested single entry single exit regions S, S, and S. The LABEL block () is nested within single entrance single exit regions S, S, and S, but is not nested within any exception handling or loop regions.","The text box at the top of  illustrates the Process( ) actions () of the DFO routine when taking the edge from the GOTO block () to the LABEL block (). These actions () can include finding each of the following: the LCA of the destination root exception handling region R and the source exception handling region T, which is the root region R; the LCA of the destination root loop region R and the source loop region L, which is root region R; and the LCA of the destination single entry single exit region S and the source single entry single exit region S, which is the root single entry single exit region R. The actions can include popping to the LCA regions, which are all the root regions, R, R, and R, thereby exiting regions T, T, L, L, L, S, S, and S, and removing those regions from the three-region stack (). In addition, the actions can include pushing regions S, S, and S onto the top of the single entrance single exit region stack () of the three-region stack (), tracking the entry into those regions. This results in the three-region stack () illustrated at the bottom right of , with a top () having a triplet of R in the exception handling region stack (), R in the loop stack (), and S in the single entry single exit stack (). The lookup can return a new HTG task region (not shown) corresponding to exception handling region R, loop region R, and single entry single exit region S. In addition, a new edge can be generated from the LABEL block () to that new HTG task region.","Referring now to , the text box at the top of the figure illustrates the Process( ) actions () of the DFO routine when taking the edge from the LABEL block () as the source block back into the nested loops of the flow graph (). That recursion back to the \u201cif (x<9)\u201d basic block () that is guarding the GOTO block () can then proceed to the other successor of the \u201cif (x<9)\u201d block (), which is the \u201ca=b+c\u201d block () (the new destination block). The actions () can include finding each of the following: the LCA of the destination exception handling region T and the source root exception handling region R, which is the root region R; the LCA of the destination loop region L and the source root loop region R, which is root region R; and the LCA of the destination single entry single exit region S and the source single entry single exit region S, which is the root single entry single exit region R. The actions can include popping to the LCA regions, which are all the root regions, R, R, and R, thereby exiting regions S, S, and S, and removing those regions from the three-region stack (). In addition, the actions can include pushing regions T and T onto the top of the exception handling region stack (); pushing regions L, L, and L onto the top of the loop stack (); and pushing regions S, S, S, and S onto the top of the single entrance single exit region stack (). This can result in the three-region stack () illustrated at the bottom right of , with a top () having a triplet of T in the exception handling region stack (), L in the loop stack (), and S in the single entrance single exit region stack (). The lookup can return a new HTG task region (not shown) corresponding to exception handling region T, loop region L, and single entry single exit region S. In addition, a new edge can be generated from the \u201ca=b+c\u201d block () to that new HTG task region.","Referring now to , an example of a flow graph () is illustrated with basic blocks -. In addition, dashed lines circumscribe the following task regions from a corresponding HTG: A (), a single entry single exit task region containing block ; B (), a single entry single exit task region containing block ; C (), a loop task region containing blocks , , , , and ; D (), a single entry single exit task region containing blocks , , and ; E (), a single entry single exit task region containing block ; F (), a loop task region containing blocks , , , and ; G (), a single entry single exit task region containing block ; H () a single entry single exit task region containing blocks , , , , , and ;  (), a single entry single exit task region containing block ; J (), a single entry single exit task region containing blocks , , , , , and ; K (), a single entry single exit task region containing block ; and L (), a single entry single exit task region containing block .","While generating a hierarchical task graph has been explained with reference to specific examples, it should be clear that this and other similar representations of source code can be generated for a wide variety of source code, even if that source code includes complex control configurations, loops, exception handling routines, and\/or other types of source code configurations.","IV. Representing Dependencies","At runtime, each task region in the HTG can be scheduled to run when its control dependencies and data dependencies have been satisfied. Those task regions that have the same control dependencies and have no data dependencies between them can be scheduled to run in parallel as tasks. To model the control and data dependencies, two graphs (a control dependence (CD) graph and a data dependence (DD) graph) can be generated at the different levels in the HTG hierarchy.","To begin building the overall CD graph and DD graph, a CD graph and a DD graph can be built at the basic block level in the control flow graph to find regions with the same control dependencies. Because all statements within a basic block have the same control dependencies, it would be inefficient, although possible, to redundantly record control dependence edges between the program statements that are contained in each basic block. Also, for the DD graph, multiple edges that represent data dependencies between statements involving the same two basic blocks can be replaced with one edge.","With the minimal graphs having been created for the control flow graph, the graphs for each level of the HTG can be generated by applying the same pruning techniques as for the flow graph level. The CD graph and the DD graph from the previous level can be pruned so that all control dependence edges and all data dependence edges are the minimal set needed between the task nodes at the current level in the hierarchy. The CD graph and the DD graph can take various data structure forms, such as one or more tables that list dependencies for each task region.","Edges in the flow graph can be used to encode control dependencies. If execution of a statement depends on whether a test is true or false, then the associated edge in the flow graph can be used to encode that fact. Referring to an example illustrated in , the control dependencies for BB(3) (basic block 3) can be represented as the union of two scenarios: (1) the edge from BB(7) to BB(3) is taken, or (2) the edge from BB(1) to BB(3) is taken. In other words, CD(BB(3))=((7\u22123)V(1\u22123)).","The nodes and edges of the flow graph can also be used to encode data dependencies. A data dependency can be encoded as being satisfied if execution of the flow graph block containing the source of the data dependency is completed, or an edge in the flow graph is taken which will guarantee that the block containing the source of the data dependency will never be reached. Referring to the example in , the control dependency of BB(4) (basic block ) on BB(2) can be represented as the union of two scenarios: (1) BB(2) is completed, or (2) the edge from BB(1) to BB(3) is taken so that BB(2) will not be reached. In other words, DD(BB(4))=(2V(1\u22123)). Coding dependencies will be discussed more below.","V. Generating Self Scheduling Code","Once the HTG, which can be an acyclic graph, is generated, a compiler can walk the HTG in a depth first order. During this walk of the HTG, an optimizer can perform parallel (and\/or) vector code generation transformations in each region using the minimal set of control and data dependencies that are needed for correct code generation. These transformations are described below.","A. Using Lambda Expressions to Schedule Code Execution","Lambda expressions can define and construct unnamed function objects, which can behave like handwritten function objects. Lambda expressions can be used to schedule code execution, such as to schedule the execution of blocks of code in parallel. C++ lambda expressions are available in Microsoft's Visual Studio\u00ae Dev10 development system. Other lambda expressions could also be used, such as lambda expressions in Visual Basic 9.0 and C#3.0 programming languages.","Table 11 below provides an example of a simple C++ lambda expression using syntax from Microsoft's Visual Studio\u00ae Dev10 development system. This expression is similar to creating a structure and overloading the \u201c( )\u201d operator, or redefining the \u201c( )\u201d as what is included in the \u201c{ }\u201d expression. The expression in Table 11 will return the value of x+y. In this expression, the \u201c[ ]\u201d is part of the syntax of the lambda expression, and is used to define how arguments are passed into the lambda expression. The default is to pass arguments in by value, so that copies of the local variables are stored within the function object. This can allow the lambda function to be used after the end of the lifetime of the original variables that were passed to the lambda function. Arguments can also be passed by reference by using an \u201c&\u201d within the brackets: \u201c[&]\u201d. The default of passing arguments by value can also be explicitly specified by using \u201c[=]\u201d instead of \u201c[&]\u201d.",{"@attributes":{"id":"p-0171","num":"0170"},"tables":{"@attributes":{"id":"TABLE-US-00011","num":"00011"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"161pt","align":"left"}}],"thead":{"row":[{"entry":[{},"TABLE 11"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"[ ] (int x, int y) { return x + y; }"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"Following is a C++ example for performing the same function (adding x+y) using another lambda expression syntax.",{"@attributes":{"id":"p-0173","num":"0172"},"tables":{"@attributes":{"id":"TABLE-US-00012","num":"00012"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"thead":{"row":[{"entry":[{},"TABLE 12"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"struct LambdaFunctor {"]},{"entry":[{},"\u2003\u2003int operator ( ) (int x, int y) { return x + y; }\u2003 \/\/ definition"]},{"entry":[{},"}"]},{"entry":[{},"void foo(int a, int b) {"]},{"entry":[{},"\u2003\u2003return LambdaFunctor(a,b); \u2003\u2002\/\/ use"]},{"entry":[{},"}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"The defaults for specifying how variables are passed into a lambda expression can also be combined with lists of variables. For example, if the user wants to capture most variables by reference, but have one by value, then the user can do the following, where \u201cvalue\u201d is passed by value and \u201ctotal\u201d is passed by reference:",{"@attributes":{"id":"p-0175","num":"0174"},"tables":{"@attributes":{"id":"TABLE-US-00013","num":"00013"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"175pt","align":"left"}}],"thead":{"row":[{"entry":[{},"TABLE 13"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"int total = 0;"]},{"entry":[{},"int value = 5;"]},{"entry":[{},"[&, value] (int x) { total += (x * value) };"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"Lambda expressions can be used in making calls to Microsoft's parallel patterns library (PPL) to form self-scheduling parallel code that schedules task regions to be executed in parallel as tasks.","Following is an example of using a C++ lambda expression and PPL calls to create a parallel (recursive) quicksort. The bodies of the lambda expressions are passed as a function pointer to the g.run( ) calls, which can be scheduled by an operating system to execute in parallel.",{"@attributes":{"id":"p-0178","num":"0177"},"tables":{"@attributes":{"id":"TABLE-US-00014","num":"00014"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"thead":{"row":[{"entry":[{},"TABLE 14"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"void quicksort(int *a, int * temp, int n) {"]},{"entry":[{},"\u2003if(n < serial_threshold) {"]},{"entry":[{},"\u2003\u2003serial_sort(a,n); return"]},{"entry":[{},"\u2003}"]},{"entry":[{},"\u2003\/\/ note: not std::partition"]},{"entry":[{},"\u2003int mid = partition(a[0], a, temp, n);"]},{"entry":[{},"\u2003task_group g;"]},{"entry":[{},"\u2003g.run([&]{quicksort(a, temp, mid);});"]},{"entry":[{},"\u2003g.run([&]{quicksort(a+mid, temp+mid, n\u2212mid);});"]},{"entry":[{},"\u2003g.wait( );"]},{"entry":[{},"}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"The use of lambda expressions in C++ schedule code to run in parallel is just one example of an implementation. Other programming languages can have different ways of scheduling code to run in parallel. Moreover, self-scheduling parallel binary machine code can be generated from the same representations of the source code that are discussed above (such as the HTG and dependency representations). Parallel code can also be generated using different abstractions. For example, machine code can use outlining, and can generate Open MP binary constructs that exist in existing UTC compilers.","B. Reading Lambda Expressions for User-Specified Threads","A compiler can recognize lambda functions that the user has specified, and can add these directly to the HTG by building a lambda graph that forms a DAG. This lambda graph can be generated and used in the same manner as the EH graph, loop graph, and SESE graph discussed above.","In one implementation, the UTC compiler Dev10 implementation can exploit a language defined property of lambda expression in order to recognize them in the UTC tuple stream (i.e., the compiler's intermediate representation that is passed from the front end to the back end). Lambda expressions can be function objects of a compiler-dependent type; this type's name can be available only to the compiler. Thus, the compiler back end can look at its records and peer into the name that the front end gave the symbol and know whether the function being called is a lambda expression.","Consider the following example from a C++ front end dump: \u201cpublic: int_thiscall \u2018anonymous namespace\u2019::<lambda0>::operator( ) (class A &&)const\u201d. For this source level construct, the back end can see the following: \u201c??R<lambda0>@?A0x3f45b300@@QBEH$$EAVA@@@Z\u201d. The \u201c??R<lambda0>\u201d string is not something a user creates. Thus, the backend project can proceed with an ugly strcmp( ) on the symbol name to determine whether the symbol corresponds to a lambda expression, without modifying the compiler front end or the UTC reader.","C. Pulling Threads from Regions Contained in Exception Handling","Once the HTG is generated, parallelism may be able to be extracted in the presence of exception handling by duplicating the exception handling within each resulting thread. The data structures described above can allow the identification of exception handling routines and code within those routines. This can allow a transformation by duplicating the exception handling within each thread that corresponds to a body of code within the exception handling routine. This can work under the assumption that the parallel runtime will guarantee that the application can catch the first exception thrown from some thread, though not in any defined order. Table 15 below includes an example of such a transformation, with the code before the transformation on the top and the code after the transformation on the bottom.",{"@attributes":{"id":"p-0186","num":"0185"},"tables":{"@attributes":{"id":"TABLE-US-00015","num":"00015"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"182pt","align":"left"}}],"thead":{"row":[{"entry":[{},"TABLE 15"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Before Transformation:"]},{"entry":[{},"\u2003_try"]},{"entry":[{},"\u2003{"]},{"entry":[{},"\u2003\u2003for (i = 1; i < 10000; i ++) {"]},{"entry":[{},"\u2003\u2003\u2003\u2003Body_1"]},{"entry":[{},"\u2003\u2003}"]},{"entry":[{},"\u2003\u2003for (j = 1; j < 10000; j++) {"]},{"entry":[{},"\u2003\u2003\u2003\u2003Body_2"]},{"entry":[{},"\u2003\u2003}"]},{"entry":[{},"\u2003}"]},{"entry":[{},"\u2003_except(GetExceptionCode( ))"]},{"entry":[{},"\u2003{"]},{"entry":[{},"\u2003\u2003\u2003exception handler code"]},{"entry":[{},"\u2003\u2003\u2003with limited side effects"]},{"entry":[{},"\u2003}"]},{"entry":[{},"After Transformation:"]},{"entry":[{},"g.run( [&] {"]},{"entry":[{},"_try"]},{"entry":[{},"\u2003{"]},{"entry":[{},"\u2003\u2003for (i = 1; i < 10000; i++) { \u2003\u2003\/\/ Loop 1"]},{"entry":[{},"\u2003\u2003\u2003\u2003Body_1"]},{"entry":[{},"\u2003\u2003}"]},{"entry":[{},"\u2003}"]},{"entry":[{},"\u2003_except(GetExceptionCode( ))"]},{"entry":[{},"\u2003{"]},{"entry":[{},"\u2003} } )"]},{"entry":[{},"g.run( [&] {"]},{"entry":[{},"_try"]},{"entry":[{},"\u2003{"]},{"entry":[{},"\u2003\u2003for (j = 1; j < 10000; j++) {\u2003\u2003 \/\/ Loop 2"]},{"entry":[{},"\u2003\u2003\u2003\u2003Body_2"]},{"entry":[{},"\u2003\u2003}"]},{"entry":[{},"\u2003}"]},{"entry":[{},"\u2003_except(GetExceptionCode( ))"]},{"entry":[{},"\u2003{"]},{"entry":[{},"\u2003} } )"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"D. Automatically Writing Parallel Source Code","Each task region in each level of the HTG can be potentially turned into a C++ lambda expression that can subsequently be passed to a PPL call that schedules the task regions. Following is a discussion of determining the arguments and placement of these self scheduling task regions in modified parallel C++ code that represents the existing code. Of course, other programming languages could be used instead of C++.","1. Code Generation for Dependencies in General","As discussed above, each task region in the HTG is dependent on a set of control dependencies (CD) and data dependencies (DD) that have been encoded in a table or graph, as discussed above. Each task region in the HTG can map to a task that can be wrapped in a C++ lambda expression and scheduled as a lightweight thread (such as a WIN 7 UMT or Dev10 PPL library call).","The dependencies can be encoded to ensure that each lambda expression can execute once all pre-requisite dependencies have been fulfilled by emitting code at the end of every basic block that fulfills a data dependency or a control dependency for another block. Thus, this code can be generated to implement the data and control dependencies. Accordingly, the code that enables a task region in the HTG to execute as a task can be distributed to the blocks that determine when a lambda expression can be scheduled for execution of that task.","Each lambda expression can be scheduled to run a corresponding set of one or more task regions as one or more tasks when the union of the task region's or regions' control dependencies and data dependencies are fulfilled at runtime. The execution conditions of a task region number N is the union of its control dependencies and its data dependencies: Exec(N)=CD(N) U DD(N). The code for Exec(N) can be distributed throughout the HTG so that the last node that determines Exec(N) can then schedule region N. Region N can be wrapped in a lambda expression, as discussed herein, and passed to a PPL library call.","To implement the code for Exec(N), local Boolean variables can be introduced to track whether control dependencies have been satisfied, and local integer variables can be introduced to track whether data dependencies have been satisfied. Referring now to , a local variable CD_COND() can be introduced, and can be set to true along the edge from BB(1) to BB(3) and the edge from BB(7) to BB(3), so that CD_COND()=TRUE if either of those edges is taken. When CD_COND()=true, that indicates that the control dependencies for BB(3) are satisfied.","Referring to , a local variable COUNT_ can be introduced and set to a number equal to the number of data dependencies for BB(4). An atomic fetch and decrement operation F&D( ) can be introduced at each point where a data dependency is satisfied to keep a count of the data dependencies for a node and to decrement that count as the dependences are satisfied at runtime, one-by-one. In the example of , the COUNT_ variable can be decremented by one if the program executes BB(2) to satisfy the data dependency on BB(2) for the variable *P, or if the edge from BB(1) to BB(3) is taken, indicating that BB(2) will not run so that there is no need to wait for the data dependency to be satisfied.","Once the control and data dependencies for a region have been satisfied, then that region can be scheduled for execution. Using the table of conditions (i.e., dependencies between regions) for each node in the HTG, code can be emitted for control dependencies and data dependencies, as well as other optimizing code such as for privatization, as set forth below.","2. Code for Control Dependencies","As has been discussed above, control dependencies for a region can be encoded as the logical OR of all the edges that could be taken to ensure execution of the region N. The label of an edge between BB(x) and BB(y) can be encoded as edge (x\u2212y). That can lead to the following encoding of control dependence in the table of control dependencies: CD(N)=(X\u2212Y)V(X\u2212Y) . . . V(Xn\u2212Yn). This can lead to the following code generation at the end of every block of the control flow graph that ends a task region that determines control dependencies for region N Xi from X . . . Xn as follows:",{"@attributes":{"id":"p-0198","num":"0197"},"tables":{"@attributes":{"id":"TABLE-US-00016","num":"00016"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"thead":{"row":[{"entry":[{},"TABLE 16"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"if (Xi \u2212 Yi) {"]},{"entry":[{},"\u2003if (DD_COUNT(N) = 0)"]},{"entry":[{},"\u2003\u2003g.run( [ ] (int a, int b) { code for region N }); \u2003\/\/ C++ lambda"]},{"entry":[{},"\u2003\u2003and PPL construct"]},{"entry":[{},"\u2003else"]},{"entry":[{},"\u2003CD_COND(N) = True;"]},{"entry":[{},"}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"In this code, \u201cDD_COUNT(N)\u201d is an atomic primitive that returns the current count of data dependencies that are outstanding for region N. The \u201cCD_COND(N)\u201d call is an atomic primitive that returns whether the control dependencies for region N have been fulfilled. As can be seen, by taking Xi\u2212Yi, the control dependencies for N are satisfied. Accordingly, if the data dependencies are satisfied, then the code executes the lambda expression to run the region. If the data dependencies are not satisfied, then the code sets CD_COND(N) to True, so that the region can be run when the data dependencies are satisfied.","3. Code for Data Dependencies","Data dependencies can be encoded in the dependency table as a series of either actual data dependencies, or edges that avoid the execution of the block which is the source of the dependence. A data dependency where the source of the edge depends on the execution of a block BB(Y) is encoded in the table as either BB(Y) or an edge from BB(C) to BB(D) such that BB(Y) is never executed. This can be encoded as the following conjunction: (Y V(C\u2212D)). Thus, the complete set of data dependencies for region N can be encoded as the following in the table: DD(N)=((Y V(C\u2212D) . . . ) and (Y V(C\u2212D) . . . ) . . . and (Yn V(Cn\u2212Dn) . . . )). This can lead to generating code at the end of 2 scenarios for each data dependence:","First, in Scenario 1, the data dependency can execute (not avoided due to runtime control flow). This can lead to the following code generation at the end of every block Yi from Y . . . Yn of the control flow graph that ends a task region where the source of a data dependency for region N exists as follows:",{"@attributes":{"id":"p-0203","num":"0202"},"tables":{"@attributes":{"id":"TABLE-US-00017","num":"00017"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"thead":{"row":[{"entry":[{},"TABLE 17"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Temp_N = F&D(DD_COUNT(N))"]},{"entry":[{},"If (CD_COND(N) = True && Temp_N=1) {"]},{"entry":[{},"\u2003\u2003g.run( [ ] (int a, int b) { code for region N }); \u2003\/\/ C++ lambda"]},{"entry":[{},"\u2003\u2003and PPL construct"]},{"entry":[{},"}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"As can be seen, in this code at the end of each block that satisfies a data dependency for N, the data dependency counter for N, DD_COUNT(N), is decremented. If that decrement results in all the data dependencies having been satisfied, and if the control dependencies have been satisfied (CD_COND(N)=True), then the code can execute the lambda expression to run the region. If not, then the code does not yet execute the lambda expression.","Second, in Scenario 2, an edge is taken that assures that the data dependency will never execute. This can lead to the following code generation at the end of every block Ci from C . . . Cn of the control flow graph that ends a task region where an edge is taken so that the corresponding data dependency source Yi will not be executed.",{"@attributes":{"id":"p-0206","num":"0205"},"tables":{"@attributes":{"id":"TABLE-US-00018","num":"00018"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"thead":{"row":[{"entry":[{},"TABLE 18"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"if (Ci \u2212 Di) {"]},{"entry":[{},"\u2003Temp_N = F&D(DD_COUNT(N)"]},{"entry":[{},"\u2003if (CD_COND(N) == True && Temp_N == 1) {"]},{"entry":[{},"\u2003\u2003g.run( [ ] (int a, int b) { code for region N }); \u2003\/\/ C++ lambda"]},{"entry":[{},"\u2003\u2003and PPL construct"]},{"entry":[{},"\u2003}"]},{"entry":[{},"}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"As can be seen, in this code at the end of each block where an edge is taken that assures that a source of a data dependency for N will not be run, the data dependency counter for N, DD_COUNT(N), is decremented. If that decrement results in all the data dependencies having been satisfied, and if the control dependencies have been satisfied (CD_COND(N)=True), then the code can execute the lambda expression to run the region. If not, then the code does not execute the lambda expression.","In addition, if it is determined that two regions can be initiated in parallel, then when the data and control dependencies for the two regions are satisfied, the code to run the regions in parallel can be executed, such as by including adjacent g.run statements for the two regions, as is illustrated above in Table 14. In addition, wrapping regions in separate g.run statements, as discussed above, can allow different regions to be run in different threads that can execute during partially or completely overlapping time periods (i.e., the regions can run in parallel). Following is an example of code where two loop regions, Region  and Region , from a fast Fourier transform (FFT) routine can be wrapped in a lambda expression and run in parallel.",{"@attributes":{"id":"p-0209","num":"0208"},"tables":{"@attributes":{"id":"TABLE-US-00019","num":"00019"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":[{"entry":"TABLE 19"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"void fourn(float data[ ], unsigned long nn[ ], int ndim, int isign)"},{"entry":"{"},{"entry":"\u2003\u2003int idim;"},{"entry":"\u2003\u2003unsigned long i1,i2,i3,i2rev,i3rev,ip1,ip2,ip3,ifp1,ifp2;"},{"entry":"\u2003\u2003unsigned long ibit,k1,k2,n,nprev,nrem,ntot;"},{"entry":"\u2003\u2003float tempi,tempr;"},{"entry":"\u2003\u2003double theta,wi,wpi,wpr,wr,wtemp;"},{"entry":"\u2003\u2003for (ntot=1,idim=1;idim<=ndim;idim++)"},{"entry":"\u2003\u2003\u2003\u2003ntot *= nn[idim];"},{"entry":"\u2003\u2003nprev=1;"},{"entry":"\u2003\u2003for (idim=ndim;idim>=1;idim\u2212\u2212) {"},{"entry":"\u2003\u2003\u2003\u2003n=nn[idim];"},{"entry":"\u2003\u2003\u2003\u2003nrem=ntot\/(n*nprev);"},{"entry":"\u2003\u2003\u2003\u2003ip1=nprev << 1;"},{"entry":"\u2003\u2003\u2003\u2003ip2=ip1*n;"},{"entry":"\u2003\u2003\u2003\u2003ip3=ip2*nrem;"},{"entry":"\u2003\u2003\u2003\u2003i2rev=1;"},{"entry":"\u2003\u2003\u2003\u2003for (i2=1;i2<=ip2;i2+=ip1) {"},{"entry":"\u2003\u2003\u2003\u2003\u2003\/\/ Beginning of Parallel Region 1"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2003\u2003ip2_private = ip2"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2003i2rev_private = i2rev;"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2003if (i2 < i2rev_private) {"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003for (i1=i2;i1<=i2+ip1\u22122;i1+=2) {"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003for (i3=i1;i3<=ip3;i3+=ip2_priv) {"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003i3rev=i2rev_private+i3\u2212i2_rev_private;"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003SWAP(data[i3],data[i3rev]);"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003SWAP(data[i3+1],data[i3rev+1]);"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003}"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003}"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2003}"},{"entry":"\u2003\u2003\u2003\u2003\u2003\/\/ End of Parallel Region 1 and Beginning of Parallel Region 2"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2003ibit=ip2 >> 1;"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2003while (ibit >= ip1 && i2rev > ibit) {"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003i2rev \u2212= ibit;"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003ibit >>= 1;"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2003}"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2003i2rev += ibit;"},{"entry":"\u2003\u2003\u2003\u2003\u2003\/\/ End of Parallel Region 2"},{"entry":"\u2003\u2003\u2003\u2003}"},{"entry":"\u2003\u2003\u2003\u2003ifp1=ip1;"},{"entry":"\u2003\u2003\u2003\u2003while (ifp1 < ip2) {"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2003ifp2=ifp1 << 1;"},{"entry":"\u2003< ... SNIPPED OUT THE REST OF THIS OUTER LOOP FOR"},{"entry":"\u2003SIMPLICITY...>"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}}}},"4. Code for Privatization","Lambda expressions can be used to provide an elegant way of expressing the results of \u201cprivatization\u201d. Privatization is an optimization that can be performed to break dependencies that force regions of a program to run sequentially, rather than in parallel.","Consider the following example of privatizing a variable Y, which had been thwarting parallelism. COBEGIN and COEND indicate a standard internal compiler pseudo operation that indicates a parallel region between the two indicators. In the parallel region, it is permissible to execute any legal parallelism. However, referring to the first row in the table below, there is an anti-dependence that forces the write of Y in \u201cy=a+b\u201d and the subsequent statements \u201ct=y\u201d and \u201cq=y\/a+c\u201d that use the new value of Y to wait until the last read of the old value of Y in \u201cx=y+2\u201d is completed. This anti-dependence thwarts the running of the first two statements in parallel with the last three statements.","As is shown in the second row of the table below, Y can be privatized by substituting a \u201cprivate\u201d variable \u201cprivate_y\u201d for the variable Y in the last three statements. This privatization breaks the anti-dependence on Y, allowing the first two statements to be run in parallel with the last three statements because there are two independent lifetimes for Y when there had been only one.","As is shown in the third row of the table below, the privatization of Y can be done using a lambda expression. As discussed above, there are different ways of passing values into lambda expressions. The values can be passed, by-reference, by-value, or as explicit arguments to the lambda. The \u201c[=]\u201d syntax in the lambda expressions in the third row of the table below means that Y is copied (passed by value) into the lambda expression. That means that Y is effectively renamed by the argument passing style in this lambda, or in other words that Y is privatized by the lambda expressions. Variables Z and Q are passed into the lambda expressions by reference, so those variables are updated in the lambda, and the updated values can be used later by the program that includes this code. In other words, variables Z and Q are live out of the lambda expressions, so they are passed into the lambda expressions by reference, expecting a side effect.",{"@attributes":{"id":"p-0215","num":"0214"},"tables":{"@attributes":{"id":"TABLE-US-00020","num":"00020"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"thead":{"row":[{"entry":[{},"TABLE 20"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"COBEGIN"]},{"entry":[{},"x = y + 2;"]},{"entry":[{},"z = x * 3;"]},{"entry":[{},"y = a + b;"]},{"entry":[{},"t = y;"]},{"entry":[{},"q = y \/a + c;"]},{"entry":[{},"COEND"]},{"entry":[{},"COBEGIN"]},{"entry":[{},"x = y + 2;"]},{"entry":[{},"z = x * 3;"]},{"entry":[{},"\/\/ The code below this point could be run in parallel with the code"]},{"entry":[{},"\/\/ above this point because privatization has broken the"]},{"entry":[{},"anti-dependence on y."]},{"entry":[{},"\u2003private_y = a + b;"]},{"entry":[{},"\u2003t = private_y;"]},{"entry":[{},"\u2003q = private_y \/a + c;"]},{"entry":[{},"COEND"]},{"entry":[{},"int x,y,z,t,v,a,b,c;"]},{"entry":[{},"\u2003\u2003..."]},{"entry":[{},"g.run( [=,z] ( ) { x = y + 2;"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2009z = x * 3} );"]},{"entry":[{},"g.run( [=,q] ( ) { y = a + b;"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2009t = y;"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003q = y \/a + c} );"]},{"entry":[{},"g.wait( );"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"Besides privatization, the compiler can perform other optimizations, such as optimizations that allow for additional parallelism or optimizations that prevent parallelism when such parallelism would not be efficient (e.g., where the regions that could be run in parallel are small enough that it is not worth the additional overhead to run them in parallel). Such optimizations can be done according to standard techniques that are currently done when manually writing parallel code.","The code described above for control dependencies, data dependencies, and for privatization can be inserted in intermediate compiler format, or IR (intermediate representation) into the intermediate representation of the existing source code to form a modified parallel intermediate compiler format representation of the existing source code. This modified parallel intermediate representation of the existing source code can be traversed to emit parallel source code and\/or parallel executable binary machine code.","VI. Parallel Code Representation Generation Techniques","Several parallel code representation generation techniques will now be described with reference to . Some or all of each technique can be performed automatically, such as by a compiler in a source code compiler environment, as discussed above, or in some other computing environment.","Referring to , a technique for producing parallel source code from existing source code is illustrated. In the technique, a grouped representation of the existing source code can be generated (), where the grouped representation includes regions of the existing source code. For example, this generation () can include generating () a control flow graph representing the existing source code, and generating () a hierarchical task graph from the flow graph. Generating () the hierarchical task graph can include generating () region graphs that group blocks from the control flow graph into regions. The region graphs can include, for example, a single entry single exit region graph, a loop region graph, an exception handling region graph, and even a user-defined region graph (such as where user-defined regions are created from lambda expressions in the existing source code). Generating () the hierarchical task graph can also include unifying () the region graphs into the hierarchical task graph while traversing the control flow graph, which can be done in a single pass of the control flow graph. Also, generating () the grouped representation can include identifying a set of one or more exception handling routines and forming one or more exception handling groups, with each of the exception handling groups including at least one of the one or more exception handling routines.","A set of task regions of the existing source code that can run in parallel as a set of tasks can be identified (). For example, this identification () can include mapping data and control dependencies of the set of task regions in the grouped representation, which can include traversing the grouped representation and forming a data structure representing control and data dependencies between task regions in the grouped representation.","The grouped representation can be converted () into modified source code that is configured to self-schedule the set of task regions of the existing source code in parallel as a set of tasks. This conversion () can include encoding () dependencies between the task regions in the grouped representation, such as encoding the dependencies in a control flow graph and\/or a table corresponding to the task regions. The conversion () can also include traversing () the grouped representation and using the encoded dependencies to generate the modified source code that is configured to self-schedule task regions in parallel. The modified source code can include one or more lambda expressions that are configured to self-schedule the set of task regions representing portions of the existing source code in parallel. In addition, the lambda expression(s) can include at least one lambda expression that is configured to copy at least one variable into the lambda expression to privatize the at least one variable and break a data anti-dependency on the at least one variable.","Also, the set of regions can be represented with their dependencies in a visual editor display, such as in a configuration similar to the illustration in . The display could also represent corresponding source code, as in . The representation of the existing source code may be, for example, a display of the existing source code itself and\/or a display of the modified source code. Such a display can allow a user to make changes to the existing source code and\/or the modified source code, which can result in greater efficiencies in a resulting program, such as where a user breaks dependencies to allow some or more parallelism. In addition, the grouped representation can be converted into object code that is configured to self-schedule the set of regions in parallel as a set of tasks. For example, this could include compiling the modified source code or compiling a modified intermediate compiler representation. The translation of the existing source code to the modified source code using the grouped representation (e.g., by generating the grouped representation and converting it into the modified source code); the representation of the set of regions and their dependencies in a visual editor display; and the conversion of the grouped representation into object code that is configured to self-schedule the set of regions in parallel as a set of tasks can all be done automatically and at once, or separately and possibly with some aid from user input.","Referring now to , a technique for generating a modified parallel representation of existing source code will be described. In the technique, existing source code that includes one or more exception handling routines can be received (). Moreover, the existing source code can be automatically converted () into a modified representation of the existing source code that is configured to self-schedule a set of regions of the existing source code to run in parallel as a set of tasks. The modified representation can include modified source code and\/or an intermediate compiler representation. Additionally, the modified representation can be compiled into object code.","Also, the set of task regions can include at least two task regions that are nested within the exception handling routines. Moreover, converting () the grouped representation into a modified representation can include duplicating in the modified representation at least some of the exception handling routines within which at least one of the task regions is nested.","Referring now to , a technique for generating a modified representation of existing source code will be discussed. In the technique, user input can be received (). The user input can identify a set of regions of existing source code in one or more lambda expressions. The one or more lambda expressions can identify the regions as regions that can run in parallel. A modified representation of the existing source code can be produced (). The modified representation can be configured to self-schedule the set of regions to run in parallel as a set of tasks. The modified representation can include at least one lambda expression that is configured to copy at least one variable into the at least one lambda expression of the modified representation to privatize the at least one variable and break a data dependency (which can be an anti-dependency) on the at least one variable. The modified representation can include modified source code.","Although the subject matter has been described in language specific to structural features and\/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather, the specific features and acts described above are disclosed as example forms of implementing the claims. For example, many of the code examples provided herein have been in the C++ source language for convenience and consistency, but the techniques and tools described herein can be applied to code written in a variety of different source languages, such as C#, visual basic, Ruby, Jscript, etc."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":["FIG. 6","FIG. 5"]},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":["FIGS. 14-19","FIG. 12"]},{"@attributes":{"id":"p-0024","num":"0023"},"figref":["FIGS. 20-21","FIG. 11","FIG. 11"]},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 22"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 23"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 24"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 25"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 26"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 27"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 28"},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 29"}]},"DETDESC":[{},{}]}
