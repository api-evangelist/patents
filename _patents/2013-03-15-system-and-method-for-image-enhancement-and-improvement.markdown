---
title: System and method for image enhancement and improvement
abstract: A method and system for improving picture quality of images by providing a series of frames of a given region of interest. One embodiment for a method includes: determining the value of each pixel at each location within each frame to form a first array of pixel values for each frame; determining the overall intensity of each frame; determining the product of the overall intensity and the array of pixel values for each frame; determining the sum of the products by adding together the products of the overall frame intensity and first array of pixel values for each frame; determining the average of the sum of products by dividing the sum of products by the number of frames in the series of frames; determining the average value of each pixel at each pixel location for the series of frames to form a second array of average pixel values; determining the average overall frame intensity for the series of frames; determining a second product of the second array of average pixel values and the average overall frame intensity; subtracting the second product from the first product to provide an improved image of the region of interest. Other improvement embodiments are also disclosed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08594455&OS=08594455&RS=08594455
owner: The United States of America as represented by the Secretary of the Army
number: 08594455
owner_city: Washington
owner_country: US
publication_date: 20130315
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS REFERENCE TO RELATED APPLICATION(S)","GOVERNMENT INTEREST","FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS","Further Improvement Embodiments"],"p":["This application is a continuation-in-part (CIP) application of U.S. patent application Ser. No. 13\/247,470 filed Sep. 28, 2011, herein incorporated by reference in its entirety.","The invention described herein may be manufactured, used, and\/or licensed by or for the United States Government.","This invention relates in general to a method of image processing, and more specifically relates to image enhancement.","Image processing is a form of signal processing for which the input is an image, such as a photograph or video frame, and the output is either image or a set of characteristics or parameters related to the image. Forms of image processing include face detection, feature detection, medical image processing, computer vision (extraction of information from an image by a computer), microscope image processing, etc.","Image resolution relates to the detail that an image possesses. For satellite images, the resolution generally correlates to the area represented by each pixel. Generally speaking, an image is considered to be more accurate and detailed as the area represented by each pixel is decreased. As used herein, the term images include digital images, electronic images, film images, and\/or other types of images. Cameras taking pictures from great distances, such as aerial photos, may not obtain detailed information about the subject matter. Consequently, subtle or detail information are not present in the images.","When an image is captured by a monochrome camera, a single charge-coupled device (CCD) or complementary metal-oxide semiconductor (CMOS) sensor is used to form an image via the light intensity projected onto the sensor.","In U.S. Pat. No. 7,536,012, to Meyers et al., entitled \u201cEntangled Quantum Communications and Quantum Imaging,\u201d there is disclosed, inter alia, a Quantum Imaging System (see Col. 8, line 50, et seq.) in which the sender sends an image of an image mask using entangled photons and coincidence measurements to a receiver. The system differs from the conventional quantum imaging set-up in that polarization beam splitters are placed in the path of the photons to provide two channels for each of the sender and the receiver, as shown in FIG. 4 of the '012 patent. On the sender's side, a photon beam is split by a beam splitter into first and second sub-beams. The first sub-beam is passed through a mask  which creates the image which is directed through a beam splitter  to bucket detectors , , which are operatively connected to a coincidence circuit. The second sub-beam is transmitted to the receiver without ever passing through the mask . In the embodiment of FIG. 4 of the '012 patent, the receiver receives the second sub-beam and an image of the mask is constructed based upon photon coincident measurements composited from two photon detectors  and , also referred to a bucket detectors. The image of a mask is transmitted via coincidences and the photons transmitting the image have never encountered the image mask. Because of the somewhat puzzling nature or circumstances of the transmission, the process has been dubbed by some as \u201cGhost Imaging,\u201d while others have explained the effects as resulting from the quantum properties of light.","The present invention provides a method and system for the enhancement of images using the quantum properties of light. An embodiment of the present invention increases the image quality of an object or scene as seen by a detector. When a low quality detector is aimed at an object, a high quality image is generated using the quantum properties of light. A low quality detector picks up quantum information on the object shape and its temporal relations to reference fields. The reference fields may be recorded by the same imager (CCD, camera, etc.) that acts as a bucket detector (that is, it does not necessarily contain spatial information).","Current imaging methods are limited to the quality of the detector looking at the object being imaged. A preferred embodiment generates an improved quality image of the object without the object being imaged in high resolution directly. The preferred method may be used in connection with photographs taken during turbulent conditions.","A preferred embodiment comprises at least one processor, at least one memory operatively associated with the at least one processor, the at least one processor operating to perform the following steps not necessarily in the order recited:","(a) providing a series of frames of a given region of interest;","(b) determining the value of each pixel at each location within each frame to form a first array of pixel values for each frame;","(c) determining the overall intensity of each frame;","(d) determining the product of the overall intensity and the array of pixel values for each frame;","(e) determining the sum of the products by adding together the products of the overall frame intensity and first array of pixel values for each frame;","(f) determining the average of the sum of products by dividing the sum of products by the number of frames in the series of frames;","(g) determining the average value of each pixel at each pixel location for the series of frames to form a second array of average pixel values;","(h) determining the average overall frame intensity for the series of frames;","(i) determining a second product of the second array of average pixel values and the average overall frame intensity;","(j) subtracting the second product from the first product to provide an improved image of the region of interest.","An alternate embodiment comprises computing the average overall intensity of a plurality of frames and arranging the frames into two sets. A first set contains the frames having frame intensities greater than the average overall intensity for all frames; the average overall intensity being the summation of the intensities for frames divided by the number of frames. The second set containing frames having an overall intensity less than the average overall intensity. Each of the first and second sets is processed by repeating steps (a) through (i). The result obtained using the second set of frames is then subtracted from the result obtained using the first set of frames to create the image.","Yet another embodiment includes a method for image improvement that comprises: providing a series of frames of a given region of interest; determining the value of each pixel within each frame to form a first array of pixel values; determining the integral of pixel values of each frame to form a second array of overall frame integrals; partitioning the first array into two sets based on a predetermined criteria; partitioning the second array into two sets based on a predetermined criteria; using the first and second sets of the first and second arrays to compute a conditional product term for each of the four combinations of the first and second sets of the first and second arrays; and combining the conditional product terms to provide an improved image of the region of interest.","And yet a further embodiment includes a method for image improvement comprising: providing a series of frames of a given region of interest; determining the value of each pixel at each location within each frame to form a first array of pixel values for each frame; determining the overall intensity of each frame; determining the product of the overall intensity and the array of pixel values for each frame; determining the sum of the products by adding together the products of the overall frame intensity and first array of pixel values for each frame; determining the average of the sum of products by dividing the sum of products by the number of frames in the series of frames; determining the average value of each pixel at each pixel location for the series of frames to form a second array of average pixel values; determining the average overall frame intensity for the series of frames; determining a second product of the second array of average pixel values and the average overall frame intensity; subtracting the second product from the first product to provide an improved image of the region of interest; partitioning the improved image into at least two partitions based on a predetermined criteria; mathematically operating upon the partitioned improved image to increase image contrast or image clarity.","The embodiments herein and the various features and advantageous details thereof are explained more fully with reference to the non-limiting embodiments that are illustrated in the accompanying drawings and detailed in the following description. Descriptions of well-known components and processing techniques are omitted so as to not unnecessarily obscure the embodiments herein. The examples used herein are intended merely to facilitate an understanding of ways in which the embodiments herein may be practiced and to further enable those of skill in the art to practice the embodiments herein. Accordingly, the examples should not be construed as limiting the scope of the embodiments herein.","Rather, these embodiments are provided so that this disclosure will be thorough and complete, and will fully convey the scope of the invention to those skilled in the art. Like numbers refer to like elements throughout. As used herein the term \u201cand\/or\u201d includes any and all combinations of one or more of the associated listed items.","The terminology used herein is for the purpose of describing particular embodiments only and is not intended to limit the full scope of the invention. As used herein, the singular forms \u201ca\u201d, \u201can\u201d and \u201cthe\u201d are intended to include the plural forms as well, unless the context clearly indicates otherwise. It will be further understood that the terms \u201ccomprises\u201d and\/or \u201ccomprising,\u201d when used in this specification, specify the presence of stated features, integers, steps, operations, elements, and\/or components, but do not preclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and\/or groups thereof.","Unless otherwise defined, all terms (including technical and scientific terms) used herein have the same meaning as commonly understood by one of ordinary skill in the art to which this invention belongs. It will be further understood that terms, such as those defined in commonly used dictionaries, should be interpreted as having a meaning that is consistent with their meaning in the context of the relevant art and will not be interpreted in an idealized or overly formal sense unless expressly so defined herein.","The current invention utilizes the ability to increase the image quality of an object as seen by a detector using methods relating to the Quantum nature of light. When a low quality detector is aimed at an object, then a high quality image may be generated based on the quantum properties of light. The high quality image is generated even in the presence of turbulence which might otherwise be disruptive to image clarity. Scattering of quantum particles such as photons off the object carries information of the object shape even when the quantum particles such as photons do not go directly into the camera or detector. An additional low quality bucket detector (i.e. detector lacking spatial information) records quantum information on the object shape and its temporal relations to collocated reference fields. The reference fields are recorded by the same type of imager (CCD, Camera, etc.) that looks at the object and which act like bucket detectors in U.S. Pat. No. 7,536,012, hereby incorporated by reference.","Current Imaging methods are limited to the quality of the detector looking at the object being imaged. This invention enables an image quality improvement by using Ghost Imaging Inspired methods to generate a high quality image of the object without the object being imaged in high resolution directly. One embodiment enables high quality imaging when only low quality images of the object are imaged directly.","Referring now to , in accordance with one embodiment, in Box  a series of frames are inputted into the memory or input of a processor or image processor. The frame may be composed on a plurality of pixels, typically in a two-dimensional (2D) array, that together form an image. Exemplary frames may be electronic image data such a TIFF or JPEG file. As used herein the terminology \u201cprocessor\u201d or \u201cimage processor\u201d as used in the following claims includes a computer, multiprocessor, CPU, minicomputer, microprocessor or any machine similar to a computer or processor which is capable of processing algorithms. The frames may comprise photographs of the same region of interest. The region of interest may be a scene, landscape, an object, a subject, person, or thing. In Box , the frame data or value of each pixel at each pixel location is determined for a frame. In Box , the overall intensity of the frame is determined. The overall intensity correlates to a \u201cbucket value\u201d determination in that overall intensity value does not comprise spatial information. Instead, it correlates to the summation of the light intensity of a frame. In the case of a picture, the overall intensity correlates to the sum of the reflected illumination. In the case of an electronic display formed by pixels, the overall intensity is the summation each pixel value at each pixel location within a given frame. At Box , the values in Box  are multiplied by the value determined in Box . Box  represents the Frame Data\u00d7Intensity Product for the frame. Inasmuch as the Frame Data is an array of pixel values, the Frame Data\u00d7Intensity Product is also an array of values. At Box , the products of Box  (Frame Data\u00d7Intensity Product) are repeated for each frame in a selected plurality of frames. As an example, one hundred frames may be selected. At Box , the summation of the Frame Data\u00d7Intensity Products for the plurality of frames determined in Box  is divided by the number of frames (such as for example one hundred) to determine the Frame Data\u00d7Intensity Product Average for the plurality of frames. As noted in Box , this Product Average is an array containing pixel values at each pixel location within the frame.",{"@attributes":{"id":"p-0068","num":"0067"},"figref":["FIG. 2","FIG. 1","FIG. 2"],"b":["7","8","9","3","3","9","3"]},"Box  represents the multiplication of Boxes  and  to form the Average Frame Data\u00d7Average Intensity Product, which is an array. As shown in the bottom portion of , the Average Frame Data\u00d7Average Intensity Product is subtracted from the Frame Data\u00d7Intensity Product Average to form the refined image of Box .","It is postulated that the preferred methodology in effect subtracts out or negates the effects or errors due to the effects of turbulence or the like. Most fluctuations caused by turbulence occur at the \u201cedges\u201d of objects. The algorithm focuses on the edges of letters, objects, etc. to refine the image edges.",{"@attributes":{"id":"p-0071","num":"0070"},"figref":"FIG. 3","b":["1","3","13","1","14","13","13"]},{"@attributes":{"id":"p-0072","num":"0071"},"figref":["FIG. 4","FIGS. 1 and 2"],"b":["1","1","2","3","4","2","3","5","6","5","7","6","7"]},"In the lower portion of , note that Box A is repeated as shown by the arrow. In Box A, the average frame data (or average value of each pixel at each pixel location) is determined for the first set of frames by averaging the pixel values at each pixel location for the first set of frames to determine an array of average pixel values for the first set. In Box A, the average overall intensity for the first set of frames is determined. This is similar to the determination of Box A except that Box A is a determination for a frame and Box A is an average for a plurality of frames. As stated with respect to Box A, the overall frame intensity correlates to a \u201cbucket value\u201d determination in that overall intensity value does not comprise spatial information. Instead, it correlates to the summation of the light intensity of a frame. In the case of a picture, the overall frame intensity correlates to the sum of the reflected illumination. In the case of an electronic display formed by pixels, the overall intensity is the summation each pixel value at each pixel location within a given frame. The average overall intensity is the summation of the values for a plurality of frames divided by the number of frames.","Box  represents the multiplication of Boxes A and A to form the Average Frame Data\u00d7Average Intensity Product, which is an array. As shown in the bottom portion of , the Average Frame Data\u00d7Average Intensity Product is subtracted from the Frame Data\u00d7Intensity Product Average to form the refined image of Box A.",{"@attributes":{"id":"p-0075","num":"0074"},"figref":"FIG. 5","b":["2","2"]},"The steps are comparable in effect to the similarly numbered frames in FIGS. , , and  as denoted by the addition of a letter \u201cB\u201d suffix to the correlating element number. In Box B, the frame data or value of each pixel at each pixel location is determined for a frame. In Box B, the overall intensity (\u201cbucket value\u201d) of the frame is determined. In the case of a picture, the overall intensity correlates to the sum of the reflected illumination. In the case of an electronic display formed by pixels, the overall intensity is the summation each pixel value at each pixel location within a given frame. At Box , the values in Box B are multiplied by the value determined in Box B. Box B represents the Frame Data\u00d7Intensity Product for the frame. Inasmuch as the Frame Data is an array of pixel values, the Frame Data\u00d7Intensity Product is also an array of values. At Box B, the products of Box B (Frame Data\u00d7Intensity Product) are repeated for each frame in a second set of frames. At Box B, the summation of the Frame Data\u00d7Intensity Products for the plurality of frames determined in Box B is divided by the number of frames (such as for example one hundred) to determine the Frame Data\u00d7Intensity Product Average for the second set of frames. As noted in Box B, this Product Average is an array containing pixel values at each pixel location within the frame.","In the lower portion of , note that Box B is repeated as shown by the arrow. In Box B, the average frame data (or average value of each pixel at each pixel location) is determined for the first set of frames by averaging the pixel values at each pixel location for the first set of frames to determine an array of average pixel values for the first set. In Box B, the average overall intensity for the second set of frames is determined. This is similar to the determination of Box B except that Box B is a determination for a frame and Box B is an average for a plurality of frames. As stated with respect to Box B, the overall frame intensity correlates to a \u201cbucket value\u201d determination in that overall intensity value does not comprise spatial information. Instead, it correlates to the summation of the light intensity of a frame. In the case of a picture, the overall frame intensity correlates to the sum of the reflected illumination. In the case of an electronic display formed by pixels, the overall intensity is the summation each pixel value at each pixel location within a given frame. The average overall intensity is the summation of the values for a plurality of frames divided by the number of frames.","Box  represents the multiplication of Boxes B and B to form the Average Frame Data\u00d7Average Intensity Product, which is an array. As shown in the bottom portion of , the Average Frame Data\u00d7Average Intensity Product is subtracted from the Frame Data\u00d7Intensity Product Average to form the refined image of Box B.",{"@attributes":{"id":"p-0079","num":"0078"},"figref":"FIG. 6","b":"12"},"Another alternate preferred method of the present invention applies the use of techniques from the field of Compressive Imaging or Compressive Sensing. In this embodiment the \u201cbucket\u201d values for each frame of the series is computed by integrating the values of the pixels within each frame. This bucket data is stored for use per Eq. 5 below. The pixel values for each frame of the series are stored as a row in a matrix J. The improved image is computed by application of a Compressive Imaging inversion algorithm such as GPSR to solve Eq. 6. The improved image is returned in the matrix R.","Virtual Ghost Imaging","Virtual Ghost Imaging refers to an imaging process which creates an enhanced image from a series of frames of an imaging subject based on a process related to Ghost Imaging.","Virtual Ghost Imaging in the current instance applies the following process to a series of frames of an imaging subject. Inasmuch as the overall frame intensity value determined in Box  correlates to the \u201cbucket\u201d value, a brief discussion of ghost imaging and reflective ghost imaging follows. Typically ghost imaging uses two detectors, one to observe the light source and the other, single pixel or bucket detector, to observe the light scattering and reflecting from the target object.\n\n()()()()\u2003\u2003(1)\n\nwhere  denotes an ensemble average. If Iand Iare recorded from the same target object, Imay be computed as\n\n()=\u222b()source\u2003\u2003(2)\n\nBasic Virtual Ghost Imaging\n","Results of an experiment conducted through turbulence using chaotic laser or pseudo-thermal light are presented in .  shows the same target computed with data taken using a typical two path configuration.",{"@attributes":{"id":"p-0084","num":"0083"},"figref":["FIG. 8","FIG. 7","FIG. 9","FIG. 8","FIG. 7","FIG. 9"]},"Accelerated (Compressive Imaging) GVirtual Ghost Imaging","A relatively new mathematical field named Compressive Sensing (CS) or Compressive Imaging (CI) can be used to good effect within the context of ghost imaging. The first use of compressive techniques in the context of Ghost Imaging was performed by the Katz group (see O. Katz, et al., \u201cCompressive Ghost Imaging,\u201d Appl Phys. Lett., 95, 131110 (2009)) (hereby incorporated by reference) who demonstrated a ghost like imaging proposal of Shapiro (see J. Shapiro, \u201cComputational Ghost Imaging,\u201d Phys. Rev. A 78 061802(R) (2008)). Their demonstration was limited to a transmission object.","More recently the present inventors have performed experiments of this nature using reflection objects.","The inventors' use of CS and CI is based on finding approximate solutions to the integral equations using the GPSR mathematical methodology where\n\n\u2003\u2003(3)\n\nand\n\n()\u2003\u2003(4)\n\nis the object reflectance. The term J is a matrix, where the rows are the illumination patterns at time k and the B vector:\n\n]\u2003\u2003(5)\n\nrepresents the bucket values. In cases where the system is underdetermined (too few [B]), then Lconstraints are applied to complete the system and sparseness is used:\n",{"@attributes":{"id":"p-0088","num":"0087"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"arg","mo":"\u2062","munder":{"mi":["min","R"]}},{"mrow":[{"mfrac":{"mn":["1","2"]},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":["B","JR"],"mo":"-"}},"mn":["2","2"]}},{"mi":"\u03c4","mo":"\u2062","msub":{"mrow":{"mo":["\uf605","\uf606"],"mi":"R"},"mn":"1"}}],"mo":"+"}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"6"}}]}}}}},"The CS computational strategy takes advantage of the fact that it is normally true in images that not all pixels in an image contain new information and the system is said to be sparse on some basis since fewer degrees of freedom are needed to describe the system than the total number of pixels in the image. Data used to reconstruct an image can be referred to as sparse image data or sparse data. The parameter \u03c4 is often a constant.","CI results for the ARL target are presented using Eq. 2 and varying the \u03c4 parameter.  is an example of a result where \u03c4 is too large and most of the pixel values are driven to 0. One can sense that the letters \u201cARL\u201d are in the figure. Decreasing \u03c4 to a value of 5e7, shown in  more portions of the \u201cARL\u201d letters appear. When \u03c4 is set to 2.5e7 the R is quite clear in  but the appearance of the A and the L are still degraded. Continuing with the examination of the effect of the parameter \u03c4, the value of \u03c4 is set to 1e7. These results are shown in .","Finally, as a lower image quality bound, \u03c4 is set to equal 1e6. The \u201cARL\u201d presented in  is quite degraded. These GPSR calculated Virtual Ghost imaging results highlight the sensitivity of the calculations to an external parameter (\u03c4) which has no connection to the underlying physics.","Air Force Resolution Target","Results were computed using Eq. 1 subject to the self-bucketing concept of Eq. 2. These results are generated from a few hundred shots of the Air Force resolution target imaged at a 100 m distance through turbulence.","A single image from this data set is presented in . This image illustrates the distorting effect of turbulence on imaging. A simple averaging of 335 frames of the dataset that was performed is shown in . This average image has some better qualities that the single frame image of  but one can still only resolve the coarser scale features of the Air Force target.","Using the self-bucket ghost imaging concept on this dataset, an initial result using only 2 frames of the dataset is displayed in . Some of the edges in this image are very distinct and superior to areas in either the instantaneous or the average images. When the entire dataset is used, as presented in , the results are striking. In particular the 4 and 5 on the right side of the target are legible and the small horizontal and vertical bars to the left of the numbers are distinct; whereas those regions in the instantaneous and average images are simply blurs.",{"@attributes":{"id":"p-0095","num":"0094"},"figref":["FIG. 19","FIG. 19"],"b":["500","502","500","502","504","504","502","504","500","502","505","506"]},"This application provides further improvements over the embodiments discussed in the parent application. One key difference is that the improvement embodiments provide a method to partition the values in the measured data sets, i.e. frames, into two or more groups for the frame data (reference fields) and overall frame intensities (bucket values). These groups are then used to compute products, or cross-correlations, between the different groupings. These individual product terms can be mathematically combined, via addition and\/or subtraction processes, to generate improve images of the target or scene. This method further adapts the techniques presented in the parent application. One key advantage to this method is that it is possible with the following embodiments to generate all positive valued images and largely eliminate background and noise effects. Other advantages include the ability to operate on a computed partitioned image using functions such as logarithms and exponentials to further increase contrast or better identify objects and information on their properties.","The following embodiments are predicated on the appreciation that other operations involving the partitioned sets of above average and below average measurements are beneficial to improve image quality in adverse conditions such as turbulence. These operations would include but are not limited to cross-correlations between above average bucket (overall frame intensities) and below average reference fields. Typically four correlation types are available when data is partitioned into two distinct sets such as above the average and below the average values. In a non-normalized form this can be written as",{"@attributes":{"id":"p-0098","num":"0097"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["R","m"]},"mo":"=","mrow":{"mfrac":{"mn":"1","msub":{"mi":["N","M"]}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mn":"1","msub":{"mi":["N","M"]}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":[{"mi":["I","a"]},{"mi":["I","b"]}],"mo":"\u2062"}}}}},{"mrow":{"mo":["(",")"],"mn":"7"}}]}}}},"br":{},"sub":["m ","a ","b","m ","a ","b"]},{"@attributes":{"id":"p-0099","num":"0098"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["RN","m"]},"mo":"=","mfrac":{"msub":{"mi":["R","m"]},"mrow":{"msub":[{"mi":"\u03c3","msub":{"mi":["I","a"]}},{"mi":"\u03c3","msub":{"mi":["I","a"]}}],"mo":"\u2062"}}}},{"mrow":{"mo":["(",")"],"mn":"8"}}]}}}},"br":[{},{},{}],"sub":["a ","b ","a","a"],"in-line-formulae":[{},{}],"i":["I","M","M"]},"The product terms that comprise a particular Rare computed conditionally. The Rcan be called conditional product terms. For instance, Rmay be computed for the set of pixel values Ithat are above the mean for those frames of data with bucket values Ithat are above the mean. For example:",{"@attributes":{"id":"p-0101","num":"0100"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msubsup":{"mi":["R","m"],"mo":"++"},"mo":"=","mrow":{"mfrac":{"mn":"1","msubsup":{"mi":["N","m"],"mo":"+"}},"mo":["\u2062","\u2062","\u2062"],"msubsup":[{"mi":"\u03a3","mn":"1","msubsup":{"mi":["N","m"],"mo":"+"}},{"mi":["I","a"],"mo":"+"},{"mi":["I","b"],"mo":"+"}]}}},{"mrow":{"mo":["(",")"],"mn":"10"}}]}}}},"br":[{},{},{}],"sub":["m ","m ","m","m","m","m"],"in-line-formulae":[{},{}],"i":["R","+R","\u2212R","\u2212R"],"sup":["++","\u2212\u2212","+\u2212","\u2212+"]},"A second alternative embodiment may include computing a Gimproved image. This improved image is then partitioned into pixels that are, for example, above the spatial mean Gand pixels that are below the spatial mean G. These alternative positive\/negative Gpartitioned improved images can display higher contrast and can be further operated upon by using mathematical operations such as logarithms to increase the dynamic range. It is to be recognized that other partitions are possible to tailor results needed for specific applications.","A third embodiment may include computing a Rcorrelation image by calculating the correlation coefficient between the Iand Ipartitions where the Iand Iare not aligned in time or frame. For instance, at a particular pixel i,j there may be 10 frames in which that pixel is above the mean value of that pixel for all frames, and there may only be say 5 frames for which the Ivalues is above the mean of I. A correlation coefficient may be computed between these two sets of data using:",{"@attributes":{"id":"p-0104","num":"0103"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"R","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["i","j"],"mo":","}}},"mo":"=","mfrac":{"mrow":{"mi":"c","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["i","j"],"mo":","}}},"msqrt":{"mrow":{"mrow":[{"mi":"c","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["i","i"],"mo":","}}},{"mi":"c","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["j","j"],"mo":","}}}],"mo":"\u2062"}}}}},{"mrow":{"mo":["(",")"],"mn":"12"}}]}}}},"br":{}},{"@attributes":{"id":"p-0105","num":"0104"},"figref":["FIGS. 20-31","FIG. 20"],"b":["301","303","313","1","314"]},"The method proceeds to . In Box , a series of frames are entered into the memory or input of a processor or image processor. The frames may comprise photographs of the same region of interest. The region of interest may be a scene, landscape, an object, a subject, person, or thing. These frames are the same frames used in Box  and may be reused if they are still available in the memory or input of the processor or image processor. In Box  the Average Frame Data is determined by computing the average value of each pixel at each pixel location for the plurality of frames. In Box  the Deviation Frame Data is determined by subtracting the Average Frame Data from the Frame Data for all pixels in each frame for the plurality of frames.","The method proceeds via path  to  which shows how to generate a third set of data which is referred to here at SET . SET  data includes conditional product terms using frames having a positive overall intensity deviation and positive deviation frame data pixels. SET  may be determined as follows: In Box  frames with a Positive Overall Intensity Deviation multiply the value of the Overall Intensity Deviation by the Positive Frame Data Deviation pixels within that set of frames. In Box  pixel locations (a), the square of the Positive Overall Intensity Deviation (b), the product of the Positive Overall Intensity Deviation\u00d7the Positive Deviation Frame Data pixels and the square of the Positive Deviation Frame Data pixels are recorded and accumulated. In Box  the pre-normalized Positive-Positive Product pixel values, (), are determined by dividing the product of the Positive Overall Intensity Deviation\u00d7the Positive Deviation Frame Data Frame Data pixels by (). The average of the squares of the Positive Overall Intensity is determined by dividing () by (). The average of the squares of the Positive Deviation Frame Data pixels is determined by dividing () by ().","Continuing via path  to , the method proceeds to  In Box  the standard deviation of the Positive Overall Intensity Deviation is determined by taking the square root of the average of the squares of the Positive Overall Intensity Deviation, (). In Box  the standard deviations of the Positive Deviation Frame Data pixels is computed by taking the square root of the average of the squares of the Positive Deviation Frame Data pixels, (). Box  determines the Normalized Positive-Positive Product pixel values by dividing the pre-Normalized Positive-Positive Product, (), by the product of the standard deviation of the Positive Overall Intensity Deviation, , and the standard deviation of the Positive Deviation Frame Data pixels, .","The method proceeds via path  to  which shows how to generate a fourth set of data which is referred to here at SET . SET  frame data includes conditional product terms using frames having a negative overall intensity deviation and negative deviation frame data pixels. SET  may be determined as follows: In Box  frames with a Negative Overall Intensity Deviation multiply the value of the Overall Intensity Deviation by the Negative Frame Data Deviation pixels within that set of frames. In Box  pixel locations (a), the square of the Negative Overall Intensity Deviation (b), the product of the Negative Overall Intensity Deviation\u00d7the Negative Deviation Frame Data pixels and the square of the Negative Deviation Frame Data pixels are recorded and accumulated. In Box  the pre-normalized Negative-Negative Product pixel values, (), are determined by dividing the product of the Negative Overall Intensity Deviation\u00d7the Negative Deviation Frame Data Frame Data pixels by (). The average of the squares of the Negative Overall Intensity is determined by dividing () by (). The average of the squares of the Negative Deviation Frame Data pixels is determined by dividing () by ().","From Box  in , the method can proceed via path  to . In Box  the standard deviation of the Negative Overall Intensity Deviation is determined by taking the square root of the average of the squares of the Negative Overall Intensity Deviation, (). In Box  the standard deviations of the Negative Deviation Frame Data pixels is computed by taking the square root of the average of the squares of the Negative Deviation Frame Data pixels, (). Box  determines the Normalized Negative-Negative Product pixel values by dividing the pre-Normalized Negative-Negative Product, (), by the product of the standard deviation of the Negative Overall Intensity Deviation, , and the standard deviation of the Negative Deviation Frame Data pixels, . At the conclusion of Box , the method proceeds via path  to  for determining an improved image data , or to  for determining alternative improved image data  by an alternative embodiment.","Returning to , the method also can concurrently proceeds via path  to  which shows how to generate a fifth set of data which is referred to here at SET . SET  frame data includes conditional product terms using frames having a negative overall intensity deviation and positive deviation frame data pixels. SET  may be determined as follows: In Box  frames with a Negative Overall Intensity Deviation multiply the value of the Overall Intensity Deviation by the Positive Frame Data Deviation pixels within that set of frames. In Box  pixel locations (a), the square of the Negative Overall Intensity Deviation (b), the product of the Negative Overall Intensity Deviation\u00d7the Positive Deviation Frame Data pixels and the square of the Positive Deviation Frame Data pixels are recorded and accumulated. In Box  the pre-normalized Positive-Negative Product pixel values, (), are determined by dividing the product of the Negative Overall Intensity Deviation\u00d7the Positive Deviation Frame Data Frame Data pixels by (). The average of the squares of the Negative Overall Intensity is determined by dividing () by (). The average of the squares of the Positive Deviation Frame Data pixels is determined by dividing () by ().","From Box  in , the method can proceed via path  to . In Box  the standard deviation of the Negative Overall Intensity Deviation is determined by taking the square root of the average of the squares of the Negative Overall Intensity Deviation, (). In Box  the standard deviations of the Positive Deviation Frame Data pixels is computed by taking the square root of the average of the squares of the Positive Deviation Frame Data pixels, (). Box  determines the Normalized Positive-Negative Product pixel values by dividing the pre-Normalized Positive-Negative Product, (), by the product of the standard deviation of the Negative Overall Intensity Deviation, , and the standard deviation of the Positive Deviation Frame Data pixels, .","At the conclusion of Box  in , the method proceeds via path  to  for determining an improved image data , or to  for determining alternative improved image data  by an alternative embodiment.","Similar as discussed above with respect to the fifth set of data (SET ), returning to , the method also can concurrently proceed via path  to  which shows how to generate a sixth set of data which is referred to here at SET . SET  frame data includes conditional product terms using frames having a positive overall intensity deviation and a negative deviation frame data pixels. SET  may be determined as follows: In Box  frames with a Positive Overall Intensity Deviation multiply the value of the Overall Intensity Deviation by the Negative Frame Data Deviation pixels within that set of frames. In Box  pixel locations (a), the square of the Positive Overall Intensity Deviation (b), the product of the Negative Overall Intensity Deviation\u00d7the Negative Deviation Frame Data pixels and the square of the Negative Deviation Frame Data pixels are recorded and accumulated. In Box  the pre-normalized Negative-Positive Product pixel values, (), are determined by dividing the product of the Positive Overall Intensity Deviation\u00d7the Negative Deviation Frame Data Frame Data pixels by (). The average of the squares of the Positive Overall Intensity is determined by dividing () by (). The average of the squares of the Negative Deviation Frame Data pixels is determined by dividing () by ().","From Box  in , the method can proceed via path  to . In Box  the standard deviation of the Positive Overall Intensity Deviation is determined by taking the square root of the average of the squares of the Positive Overall Intensity Deviation, (). In Box  the standard deviations of the Negative Deviation Frame Data pixels is computed by taking the square root of the average of the squares of the Negative Deviation Frame Data pixels, (). Box  determines the Normalized Negative-Positive Product pixel values by dividing the pre-Normalized Negative-Negative Product, (), by the product of the standard deviation of the Positive Overall Intensity Deviation, , and the standard deviation of the Negative Deviation Frame Data pixels, .","At the conclusion of Box  in , the method proceeds via path  to  for determining an improved image data , or to  for determining alternative improved image data  by an alternative embodiment.",{"@attributes":{"id":"p-0117","num":"0116"},"figref":"FIG. 30","b":["701","3","323","4","405","5","505","6","605"]},{"@attributes":{"id":"p-0118","num":"0117"},"figref":"FIG. 31","b":["702","3","323","4","405","5","805","6","605"]},{"@attributes":{"id":"p-0119","num":"0118"},"figref":"FIG. 32","b":["38","38","38"],"i":["a","b","c"]},"With respect to , SETs - are arbitrarily named, following the reference to SETs  and , described above. However, it should be appreciated that SETs - are distinct and separate from SETs  and  used in the previously discussed embodiments. In fact, SETs - could have renumbered SETs - corresponding to first, second, third and fourth data sets.",{"@attributes":{"id":"p-0121","num":"0120"},"figref":["FIG. 33","FIG. 32"],"sub":["m ","m","m","m","m"],"sup":["++","\u2212\u2212","+\u2212","\u2212+"],"b":["32","39","39","39"],"i":["b","a","c"]},{"@attributes":{"id":"p-0122","num":"0121"},"figref":["FIG. 34","FIG. 38","FIG. 33","FIG. 34"],"sub":["m ","m","m","m","m"],"sup":["++","\u2212\u2212","+\u2212","\u2212+"],"b":["38","40","40","40"],"i":["b","a","c"]},{"@attributes":{"id":"p-0123","num":"0122"},"figref":["FIG. 35","FIG. 21"],"b":["1","1","1","2","2","2","2","2"]},{"@attributes":{"id":"p-0124","num":"0123"},"figref":["FIG. 36","FIG. 30","FIG. 36"]},{"@attributes":{"id":"p-0125","num":"0124"},"figref":["FIG. 37","FIG. 21","FIG. 37","FIG. 38"],"sup":"(2) "},{"@attributes":{"id":"p-0126","num":"0125"},"figref":["FIG. 38","FIG. 21","FIG. 38","FIG. 39"],"sup":"(2) "},{"@attributes":{"id":"p-0127","num":"0126"},"figref":["FIG. 39","FIGS. 37 and 38","FIG. 37","FIG. 38"],"sup":"(2) "},"Potential Applications","The potential extent of possible use of this invention is described in the following. However, this description should not be construed as limited to the statements. Potential applications include high resolution imaging, remote sensing, microscopic sensing, phase-contrast imaging or microscopy, astronomical imaging, physics, chemistry, biology, medical applications, quality control, surveillance, surface tampering detection, imaging partially occluded scenes, spectroscopy, identification of hidden or concealed objects, remote biometrics, design of new sensors and image processing methods, design of new types of stealth technology, design of new types of communications devices.","Speed Traffic Enforcement","Current local governments use traffic enforcement cameras to enforce traffic regulation violations. A traffic enforcement camera (also road safety camera, road rule camera, photo radar, speed camera, Gatso\u2122) is an automated ticketing machine. It may include a camera which may be mounted besides, on, or over a highway or installed in an enforcement vehicle to detect traffic regulation violations, including speeding, vehicles going through a red traffic light, unauthorized use of a bus lane, for recording vehicles inside a congestion charge area and others. The latest automatic number plate recognition (ANPR) systems can be used for the detection of average speeds and use optical character recognition on images to read the license plates on vehicles. There are a number of possible factors that affect the ANPR software performance. One of these important factors is poor image resolution, usually because the plate is too far away but sometimes resulting from the use of a low-quality camera. In the case of camera recording a video (a sequence of images), this invention can process the recorded images to improve image quality of the license plate on vehicle. The enhanced license plate images are used to improve the performance of ANPR software. The invention is especially useful when the images are acquired from a far away distance and\/or from a low-quality camera.","The invention may be utilized in conjunction with large crowd event security and management. Events involving a large crowd, especially the types of events including circuses, sporting events, theatrical events, concerts, rallies, parades, etc., the security task is to prevent, where possible, crimes including theft, vandalism or assault through the deployment of trained and licensed security personnel. Camera monitoring is an important component in this type of event security and management. The invention can be used to improve image details of a human face, nomenclature on a jersey, or a moving object\/vehicle, etc., from a distance, or from the periphery of the event location. Also at football games, a preferred embodiment could be used to enhance the readability of numbers and\/or names on football uniforms.","As used herein, the terminology \u201csubject\u201d means: an area, a scene, an object or objects, a landscape, overhead view of land or an object or objects, or a combination thereof.","As used herein, the terminology \u201cframe\u201d means: a picture, an image or one of the successive pictures on a strip of film or video.","As used herein, the terminology \u201cprocess\u201d means an algorithm, software, subroutine, computer program, or methodology.","As used herein, the terminology \u201calgorithm\u201d means: sequence of steps using computer software, process, software, subroutine, computer program, or methodology.","As used herein, the terminology \u201cimage sensor\u201d means: a camera, bucket detector, CMOS, SPAD, quantum well, LIDAR, LADAR, charge coupled device (CCD), video device, spatial sensor, or range sensor. The image sensor may comprise a device having a shutter controlled aperture that, when opened, admits light enabling an object to be focused, usually by means of a lens, onto a surface, thereby producing a photographic image OR a device in which the picture is formed before it is changed into electric impulses.","The terminology \u201cprocessor\u201d or \u201cimage processor\u201d as used in the following claims includes a computer, multiprocessor, CPU, minicomputer, microprocessor or any machine similar to a computer or processor which is capable of processing algorithms.","The terminology \u201coperations\u201d as used in the following claims includes steps, a series of operations, actions, processes, subprocesses, acts, functions, and\/or subroutines.","As used herein the terminology \u201csuccession\u201d means the act or process of following in order or sequence, but is not limited to sequential order. As used herein the terminology \u201csuccession\u201d refers to a later taken image being compared with an earlier taken image.","As used herein the terminology \u201carray\u201d refers to a systematic arrangement of data in rows and columns. An example of an array is a matrix which is a rectangular array of numbers, symbols, or expressions. Examples of arrays include a matrix which is a rectangular array of numbers, symbols, or expressions and a vector which is a linear array of numbers, symbols or expressions.","As used herein, the terminology \u201cphase\u201d refers to a property of waves that measures the advance of one wave relative to another wave or that wave at an earlier place in space-time. Quantum particles such as photons having some wave properties may exhibit phase properties. Phase differences may be manifest as interference or diffraction fringes. Since images are the result of interaction of quantum particles with reflective or scattering objects they can exhibit interference fringes as signatures of instantaneous or average phase variations. Often objects that exhibit some fringes can be referred to as phase objects. \u201cPhase information\u201d refers to images that provide indications such as interference or diffraction fringes induced by the target and its environment. Phase information can be useful to distinguish features of the target that are generally not as recognizable without the phase information. Phase is discussed in R. E. Meyers, K. S. Deacon, and Y. H. Shih, \u201cTurbulence-free ghost imaging,\u201d App. Phys. Lett. 98, 111115 (2011), R. E. Meyers, K. S. Deacon, and Y. H. Shih, \u201cPositive-negative turbulence-free ghost imaging,\u201d Appl. Phys. Lett. 100, 131114 (2012). As used herein the terminology, \u201cSPAD\u201d refers to an array of Single Photon Counting Detectors that is used for imaging.","As used herein the terminology, \u201csynchronous\u201d means data or frames that are acquired at the time or are time coincident.","As used herein the terminology, \u201casynchronous\u201d means data or frames that are not acquired at the same time or are not time coincident.","As used herein the terminology, \u201clight\u201d is meant to describe electro-magnetic energy of any portion of the electro-magnetic spectrum to include, but not limited to, cosmic rays, x-rays, ultra-violet, visible, infra red, terahertz, microwave, and radio waves. Light may be measured in terms of its photons or fields.","Although various preferred embodiments of the present invention have been described herein in detail to provide for complete and clear disclosure, it will be appreciated by those skilled in the art that variations may be made thereto without departing from the spirit of the invention.","It should be emphasized that the above-described embodiments are merely possible examples of implementations. Many variations and modifications may be made to the above-described embodiments. All such modifications and variations are intended to be included herein within the scope of the disclosure and protected by the following claims. The invention has been described in detail with particular reference to certain preferred embodiments thereof, but it will be understood that variations and modifications can be effected within the spirit and scope of the invention. All references mentioned herein are hereby incorporated by reference herein."],"GOVINT":[{},{}],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The present invention can best be understood when reading the following specification with reference to the accompanying drawings, which are incorporated in and form a part of the specification, illustrate alternate embodiments of the present invention, and together with the description, serve to explain the principles of the invention. In the drawings:",{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":["FIG. 2","FIGS. 1 and 2"]},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 7","sup":"(2) "},{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 8","sup":"(2) "},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 10","sup":["8","\u22126"]},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 11","sup":["7","\u22126"]},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 12","sup":["7","\u22126"]},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 13","sup":["7","\u22126"]},{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 14","sup":["(2) ","6","\u22126"]},{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0041","num":"0040"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0042","num":"0041"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0043","num":"0042"},"figref":"FIG. 18"},{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 19"},{"@attributes":{"id":"p-0045","num":"0044"},"figref":"FIGS. 21-31"},{"@attributes":{"id":"p-0046","num":"0045"},"figref":"FIG. 21"},{"@attributes":{"id":"p-0047","num":"0046"},"figref":"FIG. 22-23"},{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIGS. 24-25"},{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIGS. 26-27"},{"@attributes":{"id":"p-0050","num":"0049"},"figref":"FIGS. 28-29"},{"@attributes":{"id":"p-0051","num":"0050"},"figref":"FIG. 30"},{"@attributes":{"id":"p-0052","num":"0051"},"figref":"FIG. 31"},{"@attributes":{"id":"p-0053","num":"0052"},"figref":"FIG. 32","sup":"(2) "},{"@attributes":{"id":"p-0054","num":"0053"},"figref":["FIG. 33","FIG. 30","FIG. 32"]},{"@attributes":{"id":"p-0055","num":"0054"},"figref":["FIG. 34","FIG. 31","FIG. 33","FIG. 32"]},{"@attributes":{"id":"p-0056","num":"0055"},"figref":"FIG. 35","sup":"(2) "},{"@attributes":{"id":"p-0057","num":"0056"},"figref":"FIG. 36"},{"@attributes":{"id":"p-0058","num":"0057"},"figref":["FIG. 37","FIG. 37","FIG. 38"],"sup":"(2) "},{"@attributes":{"id":"p-0059","num":"0058"},"figref":["FIG. 38","FIG. 37","FIG. 38"],"sup":"(2) "},{"@attributes":{"id":"p-0060","num":"0059"},"figref":["FIG. 39","FIGS. 37 and 38","FIG. 37","FIG. 38"],"sup":"(2) "}]},"DETDESC":[{},{}]}
