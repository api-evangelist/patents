---
title: Computer system, compiler apparatus, and operating system
abstract: A compiler apparatus for a computer system capable of improving the hit rate of a cache memory, which includes a prefetch target extraction device, a thread activation process insertion device, and a thread process creation device. The compiler apparatus creates threads for performing prefetch and prepurge. Prefetch and prepurge threads created by this compiler apparatus perform prefetch and prepurge in parallel with the operation of the main program, by taking into consideration program priorities and the usage ratio of the cache memory.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07424578&OS=07424578&RS=07424578
owner: Matsushita Electric Industrial Co., Ltd.
number: 07424578
owner_city: Osaka
owner_country: JP
publication_date: 20040708
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DESCRIPTION OF THE PREFERRED EMBODIMENTS","First Embodiment","Second Embodiment","INDUSTRIAL APPLICABILITY"],"p":["(1) Field of the Invention","The present invention relates to a computer system, a compiler apparatus, and an operating system, and particularly to a computer system that has a cache memory as well as to a compiler apparatus and an operating system that are used in the computer system.","(2) Description of the Related Art","Recent years have seen a remarkable increase in the computing speed of processors, but the access speed to the main memory has not been increased much, when compared with processors. A widening gap between the processing speeds of processors and main memories has an adverse influence on the improvement in processor performance due to latency in accessing the main memory caused by reading\/writing instructions or data.","With the aim of reducing latency in memory access, recent processors are equipped with a lower-capacity memory, known as cache memory, to which a relatively high-speed access can be made, in addition to the main memory. In a computer with such configuration, it is possible to store, on the cache memory, some of the instructions or data stored in the main memory. Accordingly, it becomes possible to reduce latency that occurs when the processor accesses the main memory and therefore to prevent processor performance from being affected by latency.","If there exists a target instruction or data on the cache memory when a processor accesses the main memory, the processor can read\/write such target instruction or data with a shorter latency than in the case of making an access to the main memory. If a target instruction or data does not exist on the cache memory, on the other hand, the processor reads\/writes such target instruction or data from and to the cache memory after transferring, to the cache memory, some of the instructions or data stored in the memory including the target instruction or data. It takes much time to transfer data from the main memory to the cache memory, but in the general program sequence, it is highly likely that the processor accesses nearby addresses for a certain period of time after making an access to a certain address on the main memory. Because of this fact, it becomes possible to reduce latency if the processor makes an access to the main memory after transferring instructions or data to the cache memory, compared with the case where it makes a direct access to the main memory.","However, when a target instruction or data does not exist on the cache memory, processor processing is required to be suspended while instructions or data are transferred from the main memory to the cache memory, as a result of which the performance of the processor is degraded. In order to prevent such performance degradation, a variety of methods of circumventing cache miss are proposed.","For example, there is disclosed a technique in which a prefetch instruction is inserted into a part of a source program at the time of compiling the source program, and necessary instructions or data are transferred from the main memory to the cache memory before such instructions are executed (See Japanese Laid-Open Patent application No. 11-212802 (FIG. 2) or Japanese Laid-Open Patent application No. 11-306028 (FIG. 1), for example).","However, in the above methods, a prefetch instruction inserted at compile time is executed without exception when a program is executed without taking into consideration a state of the cache memory. This causes a problem that, on a multitasking execution environment, instructions and data used by a program with a higher priority are flushed from the cache memory because of the reason that instructions and data used by a program with a lower priority have been prefetched, and therefore that cache hit rates are lowered.","Furthermore, even when instructions or data used by a program with a lower priority are prefetched, such instructions or data used by the program with a lower priority are flushed from the cache memory while a program with a higher priority is executed, which causes another problem that such prefetch is performed in vain.","The present invention has been conceived in order to solve the above problems, and it is an object of the present invention to provide a computer system, a compiler apparatus, and an operating system that are capable of improving the hit rate of a cache memory.","Moreover, it is also an object of the present invention to provide a computer system, a compiler apparatus, and an operating system that are capable of preventing unnecessary prefetches from a cache memory from being performed.","The computer system according to the present invention is a computer system including a cache memory that stores one or more instructions and data that are stored in a main memory and a cache memory control unit operable to control the cache memory, wherein the cache memory control unit measures a usage ratio of the cache memory, and controls the cache memory according to said measured usage ratio.","This configuration makes it possible for a program being executed by the processor to know the usage ratio of the cache memory. Accordingly, it becomes possible for such program to perform processing of not performing a prefetch, for example, when the usage ratio of the cache memory is high. As a result, it becomes possible to circumvent the case where instructions or data used by a program with a higher priority is flushed from the cache memory, and therefore to increase the hit rate of the cache memory.","More preferably, the cache memory control unit has a counter that measures a number of valid entries in the cache memory for specifying the usage ratio.","This configuration makes it possible to manage the usage status of the cache memory on a line-by-line or an entry-by-entry basis, and to control memory access by a program.","A computer system according to another aspect of the present invention is a computer system including a plurality of cache memories that correspond to a respective plurality of processors and that store one or more instructions and data that are stored in a main memory, a plurality of cache memory control units that correspond to the respective plurality of cache memories, and an inter-processor communication unit operable to interconnect the plurality of processors, and upon receipt of a command from a first processor in the plurality of processors to manipulate a cache memory corresponding to a second processor that is different from the first processor, output a command to manipulate said cache memory to a cache memory control unit corresponding to the second processor, wherein the plurality of cache memory control units manipulate the respective cache memories based on the command from the inter-processor communication unit. More preferably, this computer system further comprises a processor identification unit operable to identify a processor that is executing a program, wherein upon receipt of information for identifying the second processor, the first processor issues, to the inter-processor communication unit, the command to manipulate the cache memory corresponding to the second processor based on the information for identifying the second processor.","This inter-processor communication unit allows a processor to control the cache memory used by another processor.","More preferably, the processor identification unit is implemented as a machine language instruction that returns unique numbers assigned to the respective plurality of processors.","This configuration makes it possible to easily control plural cache memories on the program.","More specifically, the inter-processor communication unit, upon receipt of the command from the first processor, sends a prepurge instruction or a prefetch instruction to the cache memory corresponding to the second processor.","This configuration allows an efficient use of cache memories and therefore to prevent the degradation of processor performance, even for the configuration with plural processors and corresponding cache memories.","Moreover, the compiler apparatus according to further another aspect of the present invention is a compiler apparatus that converts a source program written in a high-level language into an executable program, including a prefetch target extraction unit operable to extract an instruction or data to be prefetched from a main memory to a cache memory for each of predetermined execution groups in the source program, and generate an address list that lists an address of the extracted instruction or data, and a thread process creation unit operable to (i) analyze a control structure of each of the execution groups in the source program, (ii) select, from the address list, the address of the instruction or data to be prefetched according to a result of the analysis, and (iii) create a prefetch thread for prefetching the instruction or data that is stored in a location specified by the selected address.","With the above configuration, a prefetch thread is created in addition to the main task. By adding a prefetch thread to the source program, it becomes possible to makes an efficient use of the cache memory.","Preferably, the thread process creation unit checks a priority of a task that activates the thread process creation unit, and creates the prefetch thread for executing the prefetch, when a task with a priority higher than the priority is not executed on any processors.","According to the above configuration, a prefetch is allowed to be performed if the priority of a task that has activated the prefetch thread is the highest of all. Accordingly, there does not occur any cases where instructions or data of a task with a higher priority are purged by a task with a lower priority, which makes it possible to increase the hit rate of the cache memory.","More preferably, the thread process creation unit creates the prefetch thread for executing the prefetch only when a usage ratio of the cache memory is equal to or lower than a specified value.","By performing a prefetch only when the usage ratio of the cache memory is equal to or lower than a predetermined value, it becomes possible not to perform any prefetches when the usage ratio of the cache memory is high. As a result, there does not occur any cases where a prefetch is performed unnecessarily, due to the fact that instructions or data to be used by a program with a higher priority are purged from the cache memory since instructions or data of a program with a lower priority have been prefetched.","More preferably, this compiler apparatus further comprises a prefetch timing determination unit operable to determine a prefetch start timing at which a prefetch should start on the source program, based on execution time required for an instruction in an execution group of interest and time related to said prefetch of an instruction or data in a next execution group, said prefetch start timing allowing execution of said prefetch to complete before execution of the instruction in the next execution group starts, wherein the thread activation process insertion unit inserts, at the prefetch start timing on the source program determined by the prefetch timing determination unit, a process for activating a prefetch thread for prefetching the instruction or data in the next execution group.","By inserting a prefetch thread at such timing, the prefetch will have been completed before the execution of the next execution group starts. Accordingly, it becomes possible to increase the hit rate of the cache memory, and to achieve high-speed processing.","The operating system according to another aspect of the present invention is an operating system capable of multitasking, the operating system causing a computer to function as a cache usage ratio monitoring unit operable to monitor a usage ratio of a cache memory, and a task control unit operable to control an execution sequence of tasks executed by a processor, according to the usage ratio of the cache memory.","Accordingly, it becomes possible to change execution sequences of tasks to be executed by the processor. Thus, by executing a task with a higher priority in a preferential manner, when the usage ratio of the cache memory is high, it becomes possible to increase the hit rate of the cache memory and to improve program execution speeds.","Preferably, this operating system further causes the computer to function as a priority checking unit operable to check whether or not there is a task with a higher priority than a priority of a task being executed by the processor, wherein when the usage ratio of the cache memory is greater than a predetermined threshold, the task control unit increases execution frequencies so that an execution frequency of a task with a higher priority becomes higher than the case where the usage ratio of the cache memory is equal to or lower than the predetermined threshold.","Accordingly, by executing a task with a lower priority when the usage ratio of the cache memory is high, it becomes possible to prevent instructions or data of a task with a higher priority from being discarded.","The operating system according to another aspect of the present invention is an operating system for a computer system that is equipped with a plurality of processors, wherein the computer system has a plurality of cache memories that correspond to the respective plurality of processors, and the operating system causes a computer to function as a cache usage ratio monitoring unit operable to monitor usage ratios of the respective plurality of cache memories, and a task control unit operable to control an execution sequence of tasks, according to the usage ratios of the respective plurality of cache memories. Preferably, the task control unit preferentially assigns a task to a processor corresponding to a cache memory whose usage ratio is lowest of all the plurality of cache memories.","This configuration makes it possible to assign tasks to a processor whose usage ratio is lower, which consequently enhances the overall computer performance.","More preferably, this operating system further causes the computer to function as a priority checking unit operable to check whether there exists a task with a higher priority than a priority of each task being executed by each of the plurality of processors, wherein when all of the usage ratios of the respective plurality of cache memories are greater than a predetermined threshold, the task control unit increases execution frequencies so that an execution frequency of a task with a higher priority becomes higher than the case where at least one of said usage ratios is equal to or lower than the predetermined threshold.","This configuration allows even a multiprocessor system to control plural cache memories on a task-by-task basis in consideration of task priorities.","Note that not only is it possible to embody the present invention as the computer systems, compiler apparatus, and operating systems described above, but also as a compiler and a program that includes characteristic instructions. It should be also noted that such program can be distributed on recording media such as Compact Disc-Read Only Memory (CD-ROM) and via transmission media such as the Internet.","According to the present invention, it is possible to increase the hit rate of cache memories, prevent unnecessary prefetches from a cache memory from being performed, control memory access from a program executed on the processor, allow a processor to control the cache memory used by another processor in a multiprocessor computer system, make an efficient use of cache memories and to prevent the degradation of processor performance, even when there are plural processors and corresponding cache memories, prevent instructions or data of a task with a higher priority from being flushed from the cache memory. Using the above compiler apparatus, programmers are enabled to develop programs for performing prefetches without needing to be aware of the presence of a cache memory, which facilitates the development of programs with high execution speed and allows a task with a higher priority to make an efficient use of a cache memory, and therefore to facilitate the development of programs with high execution speed.","The disclosure of Japanese Patent Application No. 2003-306437 filed on Aug. 29, 2003 including specification, drawings and claims is incorporated herein by reference in its entirety.","The following gives a detailed description of a computer system according to the first embodiment of the present invention with reference to the drawings.","(1) Hardware Configuration",{"@attributes":{"id":"p-0070","num":"0069"},"figref":["FIG. 1","FIG. 1"],"b":["600","601","604","602","603"]},"The main memory  is a large-capacity storage device, to which an access can be made at a low speed, for storing instructions and data. The cache memory  is a small-capacity storage device, to which an access can be made at a high speed, for temporarily storing instructions and data that are stored in some of the locations on the main memory . The CPU  is an arithmetic unit that executes instructions stored in the main memory  or the cache memory  and that reads and writes data to and from the main memory  or the cache memory . The memory controller  is a control device that controls access between the main memory  and the cache memory  and that controls access between the CPU  and the cache memory  as well as the main memory .",{"@attributes":{"id":"p-0072","num":"0071"},"figref":["FIG. 2","FIG. 1"],"b":["602","602","202","203","204","205"]},{"@attributes":{"id":"p-0073","num":"0072"},"figref":["FIG. 3","FIG. 2"],"b":["602","205","202","203","204","304","301","302","303"]},"Stored in the instruction\/data field  are instructions or data. Stored in the address field  is an address in the main memory  corresponding to the instructions or data stored in the instruction\/data field .","Stored in the validity indication field  is a bit that indicates whether or not the instructions or data stored in the entry are valid. The validity indication field  has a 1-bit element. It indicates that valid instructions or data are stored in the entry if the value of such bit is 1, whereas it indicates that valid instructions or data are not stored in the entry if the value of such bit is 0.","Stored in the writing indication field  is a bit that indicates whether or not writing has been performed to the entry. The writing indication field  has a 1-bit element. It indicates that writing has been performed to the entry if the value of such bit is 1, whereas it indicates that no writing has been performed to the entry if the value of such bit is 0.","The main memory  in  is comprised of 32-bit address spaces, for example, which are divided into \u201clines\u201d on a 16 byte basis. In this case, instructions or data equivalent to one line are stored in one entry in the cache memory . For example, when data stored in the main memory  at an address 0x80000008 is transferred from the main memory  to the cache memory , the memory controller  exercises control so that 16-byte instructions or data stored in the main memory  at addresses from 0x80000000 to 0x8000000F. are transferred all at once to the cache memory .",{"@attributes":{"id":"p-0078","num":"0077"},"figref":["FIG. 4","FIG. 1","FIG. 4"],"b":["604","604","501","501","302","501","302","602","302","602","601","602","501"]},{"@attributes":{"id":"p-0079","num":"0078"},"figref":["FIG. 5","FIG. 5"],"b":["604","601","602","603","604","602","603"]},"Upon receipt of a memory access request from the CPU , the memory controller  checks whether or not there exits any instructions or data in the cache memory  corresponding to an address to which a memory access is requested (S). Stated another way, the memory controller  checks whether or not there is any entry whose address field  stores an address that is the same as the address to which memory access is requested, out of the entries on the cache memory  whose bit in the validity indication field  indicates validity. When there exists an entry that includes the target address (Yes in S), the memory controller  exercises control so that instructions or data are read from or written to the above entry on the cache memory  (S).","When the address field  in none of the valid entries stores an address that matches the target address (No in S), the memory controller  checks whether or not there exists any entry whose bit in the validity indication field  indicates invalidity (S). When there exist entries whose bit in the validity indication field  is invalid (Yes in S), the memory controller  chooses one of such entries, and transfers, to such chosen entry, instructions or data stored in a line that includes the target address on the main memory  (S). At the same time, the memory controller  writes the target address to the address field  of the entry to which instructions or data have been transferred. Furthermore, the memory controller  sets the bit of the validity indication field  of such entry to valid and sets the bit of the writing indication field  of such entry to invalid.","When the bits of the validity indication fields  of all the entries indicate validity, the memory controller  chooses an entry whose contents should be changed (hereinafter referred to as \u201ctarget entry\u201d) by use of the Least Recently Used (LRU) algorithm, and checks whether the bit of the writing indication field  of the target entry indicates validity or not (S). When the writing indication field  indicates validity (Yes in S), the memory controller  transfers, to the main memory , the contents of the instruction\/data field  of the target entry (S). After that, the memory controller  writes instructions or data to the target entry, according to the procedure equivalent to that of the above rewrite processing (S).","When the writing indication field  of the target entry indicates invalidity (No in S), the memory controller  writes instructions or data to the target entry, according to the procedure equivalent to that of the above rewrite processing (S), without performing transfer processing (S) to the main memory .",{"@attributes":{"id":"p-0084","num":"0083"},"figref":"FIGS. 6A and 6B","b":["601","601","602","1900","1903"]},"As  shows, the prefetch instruction , which is represented by an operation code  \u201cPrefetch\u201d, is an instruction for transferring, to the cache memory , a line on the main memory  that includes an address represented by an operand  \u201cAddress\u201d, in accordance with the procedure equivalent to the one that is followed in the case where there has been an access from the CPU  to the main memory .","As  shows, the prepurge instruction , which is represented by an operation code  \u201cPrepurge\u201d, is an instruction for invalidating the validity indication field  of an entry out of entries on the cache memory  whose bits in the validity indication fields  indicate validity, if an address of such entry in the address field  matches the address specified by an operand  \u201cAddress\u201d. However, when the bit of the writing indication field  indicates validity, the validity indication field  is invalidated by this instruction, after data is transferred from the cache memory  to the main memory .","(2) Configuration of Operating System",{"@attributes":{"id":"p-0088","num":"0087"},"figref":"FIG. 7","b":["600","1800","1801","1802","1803","601"]},"The cache usage ratio monitoring device  monitors a usage ratio of the cache memory , by referring to the cache usage amount register  of the memory controller .","The task control device  exercises control so that one task is switched to another task in plural tasks for execution at predetermined time intervals. Each of the tasks being executed is given a priority. The task control device  switches tasks so that a longer execution time is provided to a task with a higher priority. However, when a usage ratio of the cache memory  monitored by the cache usage ratio monitoring device  exceeds a specified value, the task control device  reduces the frequency at which a task with a lower priority is executed, and increases instead the frequency at which a task with a higher priority is executed. Here, the above-mentioned specified value is a value that depends on the type of an application to be supported as well as the type of a program sequence, and therefore it is preferable that an arbitrary numeric value can be set as such value.","The priority notification device  provides a notice indicating whether or not there is a task with a higher priority than that of a task in execution, in response to an inquiry from such task in execution. Note that the priority notification device  may also be an application programming interface (API) that returns a true value if there exists a task with a higher priority than that of the calling task that has made the above inquiry and returns a false value if there is no task with a higher priority.","(3) Configuration of Compiler",{"@attributes":{"id":"p-0093","num":"0092"},"figref":"FIG. 8","b":["601","600","100","101","102","103","104","105","106","601"]},"The compiler apparatus  divides a source program into blocks called \u201cbasic blocks\u201d and performs compilation processing on a basic block basis. Basic block is a group of instructions that include no branch instruction and that therefore operate sequentially without exception. A source code  as shown in , for example, is divided into basic blocks , , , , and  shown in , on which compilation processing is performed on a basic block basis.","The source code analyzing device  reads in a source program written by programmers, performs syntax analysis and semantic analysis on such source program, and generates intermediate codes.","The optimization device  optimizes the intermediate codes generated by the source code analyzing device  so that the sizes and execution times of the executable codes to be generated at the final stage become small and short.","The prefetch target extraction device  extracts a variable to be prefetched. A detailed description of the prefetch target extraction device  is given later.","The thread activation process insertion device  inserts processes for activating a prefetch thread and a prepurge thread. A detailed description of the thread activation process insertion device  is given later.","The thread process creation device  creates a prefetch thread and a prepurge thread. Detailed descriptions of a prefetch thread and a prepurge thread created by the thread process creation device  are given later.","The object code generation device  generates executable codes from the intermediate codes which are generated and optimized respectively by the source code analyzing device  and the optimization device , and to which a prefetch thread and a prepurge thread are inserted by the prefetch target extraction device , the thread activation process insertion device , and the thread process creation device .","Detailed descriptions of the source code analyzing device , the optimization device , and the object code generation device  are not given here, since they are not the main subject of the present invention and their operations are the same as those performed by an existing compiler.",{"@attributes":{"id":"p-0102","num":"0101"},"figref":["FIG. 11","FIG. 8","FIG. 11"],"b":["103","103"]},"The prefetch target extraction device  extracts the address of a line in which instructions included in a basic block are stored, and adds it to the prefetch target list (S). In the case where instructions of a basic block cannot be stored in one line, the prefetch target extraction device  adds the addressees of plural lines to the prefetch target list.","The prefetch target extraction device  checks whether or not the address of a variable used in the basic block is already registered in the prefetch target list (S). If not (No in S), the prefetch target extraction device  registers the address of the variable in the prefetch target list (S). The prefetch target extraction device  iterates such processing for registering the address of a variable to the prefetch target list (S and S) from the top through the end of the basic block (S\u02dcS), and then terminates the processing. However, it is impossible, at compile time, to determine addresses on the memory where variables and instructions are to be placed. Thus, the present invention is configured so that temporary address information is stored in a prefetch target list, which is then rewritten to actual address information at the time of linking object files, when an actual address is determined.",{"@attributes":{"id":"p-0105","num":"0104"},"figref":["FIGS. 12A\u02dc12D","FIG. 10","FIG. 11","FIG. 10"],"b":["801","802","803","805","1001","1002","1003","1004","801","802","803","805","804","2"]},{"@attributes":{"id":"p-0106","num":"0105"},"figref":["FIG. 13","FIG. 8","FIG. 13"],"b":["104","104","1101","1101","104","1102"]},"When there are two or more subsequent basic blocks (No in S), it means that a branch condition exists in the basic block of interest. For this reason, the thread activation process insertion device  judges whether or not a prefetch completes before processing of the subsequent basic block starts, even if such prefetch starts after a branch condition is determined for deciding which basic block is to be executed next (S). When judging that the prefetch completes before processing of the subsequent basic block starts (Yes in S), the thread activation process insertion device  inserts a process for activating a thread for prefetching variables registered in the prefetch target list of the subsequent basic block, depending on subsequent basic block that is determined by the branch condition (S). Note that the thread activation process is inserted immediately after the values of variables used for making a judgment on the branch condition are determined. Accordingly, the prefetch completes before processing of the subsequent basic block starts. For example, taking the basic block , the values of variables a and b are determined after assignment statements \u201ca=x+y;\u201d and \u201cb=x*y;\u201d are executed. At this point of time, a branch condition is determined for deciding which one of the basic blocks  and  is to be executed. Thus, a process for activating a prefetch thread is inserted in a location immediately after where the assignment statement \u201cb=x*y;\u201d is executed.","When judging that the prefetch will not complete before processing of the subsequent basic block starts (No in S), the thread activation process insertion device  inserts a process for activating threads for prefetching variables registered in the prefetch target lists of all the subsequent basic blocks, before a branch condition is determined (S). Note that the thread activation process is inserted into a location that enables the prefetches to complete before processing of the subsequent basic blocks start.","For example, in the case of the source code  shown in  and the basic blocks , , , and  shown in , either the basic block  or the basic block  is executed after the basic block , depending on branch condition that is determined based on the size relationship between the variable a and the variable b. The values of the variables a and b used in the branch condition are determined in the basic block . Thus, assuming that it takes 20 machine cycles from when the value of b is determined to when the execution of either the basic block  or  starts and that it takes 10 machine cycles for a prefetch, the thread activation process insertion device  judges that the prefetch completes before the next basic block, which is either the basic block  or , is to be activated. Therefore, the thread activation process insertion device  inserts a process for activating a prefetch thread before processing for determining the value of the variable b is executed.","On the other hand, assuming that it takes 10 machine cycles from when the value of the variable b is determined until when the execution of the basic block  or  starts and that it takes 20 machine cycles for a prefetch, the thread activation process insertion device  inserts a process for activating prefetch threads corresponding to the respective basic blocks  and . Furthermore, a process for activating a prepurge thread is inserted at the end of a basic block, but a description of this is given later.",{"@attributes":{"id":"p-0111","num":"0110"},"figref":["FIG. 14","FIG. 8","FIG. 14","FIG. 13"],"b":["105","1102","1105"]},"The prefetch thread makes an inquiry to the operating system  about whether or not a task with a higher priority than that of the task which has activated such prefetch thread is being executed (S). If a task with a higher priority than that of the calling task that has made the above inquiry is not being executed (Yes in S), the prefetch thread prefetches, from the main memory , instructions and data of addresses registered in a prefetch target list (S).","If a task with a higher priority is being executed (No in S), the prefetch thread checks the value held in the cache usage amount register  to see whether the value indicating the usage ratio of the cache memory  is equal to or lower than a specified value (S). If the usage ratio of the cache memory  is equal to or lower than the specified value (Yes in S), the prefetch thread prefetches, from the main memory , instructions and data of addresses registered in the prefetch target list (Yes in S). If the usage ratio of the cache memory  exceeds the specified value (No in S), the prefetch thread terminates the processing.","Note that the above-mentioned specified value is a value that depends oh the type of an application to be supported as well as the type of a program sequence, and therefore it is preferable that an arbitrary numeric value can be set as such value.","As described above, by determining whether or not to execute prefetch in consideration of priorities and the usage ratio of the cache memory, it is possible to prevent instructions and data used by a task with a higher priority from being flushed from the cache memory . Accordingly, it becomes possible to increase the hit rate of the cache memory  and therefore to make an efficient use of the cache memory . This consequently allows high-speed processing.",{"@attributes":{"id":"p-0116","num":"0115"},"figref":["FIG. 15","FIG. 8","FIG. 15","FIG. 13"],"b":["105","1104"]},"Here, suppose that there are \u201cn\u201d subsequent basic blocks from basic blocks B to Bn (where \u201cn\u201d is an integer), and that conditions for executing the basic blocks B\u02dcBn are conditions C\u02dcCn, respectively.","In this prefetch thread, as in the case of the prefetch thread shown in , the following processing is performed when there is no task with a higher priority than that of the task that has activated this prefetch thread (Yes in S), and when the usage ratio of the cache memory  is equal to or lower than the specified value although there is a task with a higher priority (No in S and Yes in S).","The prefetch thread checks if any of the conditions C\u02dcCn is satisfied or not (S\u02dcS). Then, the prefetch thread chooses a prefetch target list of the basic block Bi that corresponds to a satisfied condition Ci, and prefetches instructions and data of addresses stored in such prefetch target list (S, S, S, and S).","For example, when the condition C is TRUE (Yes in S), the prefetch thread prefetches instructions and data of addresses registered in the prefetch target list that is associated with the basic block B (S). When the condition C is TRUE (Yes in S), the prefetch thread prefetches instructions and data of addresses registered in the prefetch target list that is associated with the basic block B (S). When the condition C(n\u22121) is TRUE (Yes in S), the prefetch thread prefetches instructions and data of addresses registered in the prefetch target list that is associated with the basic block B(n\u22121) (S). When all of the conditions C to C(n\u22121) are FALSE, the prefetch thread prefetches instructions and data of addresses registered in the prefetch target list that is associated with the basic block Bn () since the condition Cn is TRUE (No in S).","Next, a description is given of a prefetch thread that is activated in a thread activation process.  is a flowchart showing processing performed in a prefetch thread that is activated by a process for activating a prefetch thread to be inserted in the basic block  shown in .","The prefetch thread checks whether a task with a higher priority than that of the task which has activated such prefetch thread is being executed or not (S). If a task with a higher priority is being executed (No in S), the prefetch thread checks the value held in the cache usage amount register  to see whether the value indicating the usage ratio of the cache memory  is equal to or lower than a specified value (S). If there is a task with a higher priority and the usage ratio of the cache memory  exceeds the specified value (No in S and No in S), the prefetch thread terminates the processing without performing prefetch processing.","If there is no prefetch thread with a higher priority (Yes in S), or if the usage ratio of the cache memory  is equal to or lower than the specified value (Yes in S), the prefetch thread makes a judgment about a condition for branching to either the basic block  or the basic block  (S). Stated another way, the prefetch thread compares the sizes of the variable a and the variable b. When the value of the variable a is larger than the value of the variable b (Yes in S), the subsequent basic block is the basic block . Therefore, the prefetch thread prefetches instructions and data, based on addresses registered in the prefetch target list  that corresponds to the basic block  (S).","When the value of the variable a is equal to or lower than the value of the variable b (No in S), the subsequent basic block is the basic block . Therefore, the prefetch thread prefetches instructions and data, based on addresses registered in the prefetch target list  that corresponds to the basic block  (S).","The thread process creation device  creates prepurge threads in addition to the above-described prefetch threads.  is a flowchart showing processing performed in a prepurge thread. In the first loop processing (S\u02dcS), the prepurge thread sequentially chooses addresses included in the prefetch target list corresponding to a basic block to be prepurged. Furthermore, in the second loop processing (S\u02dcS), the prepurge thread sequentially chooses addresses included a basic block to be executed next after the basic block to be prepurged. In the first and second loops, the prepurge thread compares addresses included in the prefetch target list corresponding to the basic block to be prepurged and all addresses included in the prefetch target list corresponding to the next basic block. If the prefetch target list corresponding to the next basic block does not include any addresses that exist on the same line as an address included in the prefetch target list of the basic block to be prepurged (Yes in S), the prepurge thread prepurges, to the main memory , instructions and data of the address included in the prefetch target list corresponding to the basic block to be prepurged (S). If the prefetch target list corresponding to the next basic block includes an address that exists on the same line as an address included in the prefetch target list of the basic block to be prepurged (No in S), the prepurge thread chooses another address included in the prefetch target list corresponding to the basic block to be prepurged (S), and iterates the same processing (S\u02dcS).","For example, using the source code  shown in  and the basic blocks , , , and  shown in , suppose the case where a location used by the basic block  is prepurged. The basic block  or the basic block  is executed next, after the basic block  is executed. Thus, the prepurge thread compares the prefetch target list  with the prefetch target lists  and .","First, the prepurge thread compares addresses stored in the prefetch target list  corresponding to the basic block  with all addresses stored in the prefetch target lists  and . If the prefetch target lists  and  include an address that is on the same line as an address of the basic block  while they do not include any addresses which are the same as those of the basic block , the prepurge thread does not prepurge any addresses of the basic block . On the other hand, if the prefetch target list  and  do not include any addresses on the same line, the prepurge thread performs prepurge.","Since the addresses of the variables a and b exist in the prefetch target lists  and , the prepurge thread does not perform any prepurges. Meanwhile, when the address of the variable x does not exist in the prefetch target lists  or  but an address on the same line exists, the prepurge thread does not prepurge the address of the variable x. Similarly, the prepurge thread does not prepurge the address of the variable y if an address on the same line exists. The thread process creation device  creates a prepurge thread that performs the above processing, and a process for activating such prepurge thread is inserted.","(4) Image at Execution Time",{"@attributes":{"id":"p-0130","num":"0129"},"figref":["FIG. 18","FIG. 9","FIG. 18"],"b":["700","1601","801","1604","802","803","1605"]},"Since thread activation processes are inserted in the program by the thread activation process insertion device  of the compiler apparatus , a prefetch thread  is activated while the basic block  is being executed. The prefetch thread  is a thread created by the thread process creation device  of the compiler apparatus . While operating in parallel with the main thread , the prefetch thread  prefetches instructions and data to be used by one of or both of the basic blocks  and  to be executed next, according to the flowchart shown in one of , , and . The prefetch thread  vanishes upon completion of prefetch processing.","The thread activation process insertion device  of the compiler apparatus  inserts a thread activation process when the processing of the basic block  finishes. For this reason, a prepurge thread  is activated when the execution of the basic block  ends. The prepurge thread  is a thread created by the thread process creation device  of the compiler apparatus . While operating in parallel with the main thread , the prepurge thread  prepurges instructions and data used by the basic block , according to the flowchart shown in . The prepurge thread  vanishes upon completion of prepurge processing.","In an interval , processing of either the basic block  or the basic block  is executed. Usually, instructions to be executed and variables to be used by the basic block  or  do not exist in the cache memory . This makes it impossible to execute a program while instructions and data are transferred from the main memory  to the cache memory . However, in a program complied by the compiler apparatus  according to the present invention, the prefetch thread  transfers, in the interval , instructions and data to be used in the interval  to the cache memory . Accordingly, the CPU  can execute the processing of the next basic block  or  immediately after the execution of the processing of the basic block .","Furthermore, the prepurge thread  purges, from the cache memory , instructions and data which are not to be used in the next basic block  or . Accordingly, it becomes possible to prevent necessary instructions and data from being flushed from the cache memory . Note that prefetch threads and prepurge threads are created and vanish repeatedly as in the above manner.","As described above, in the computer system according to the first embodiment of the present invention, a prefetch thread monitors the usage ratio of the cache memory, and performs no prefetch if the usage ratio is high. Accordingly, by performing a prefetch, it becomes possible to prevent currently used instructions and data from being purged into the main memory. This makes it possible to reduce cache miss occurrence and therefore to improve the speed of executing programs. Moreover, by prefetching instructions or data to be used by a program with a lower priority, there does not occur a case where instructions or data used by a program with a higher priority is discarded and a prefetch is performed in vain.","Furthermore, in the present embodiment, when there are two or more subsequent basic blocks, in the case where a prefetch completes before the subsequent basic block is to be executed even after a branch condition is determined, the prefetch is designed to be performed after such branch condition is determined. Accordingly, it becomes impossible for instructions and data of unnecessary basic blocks to be prefetched.","Moreover, it is also possible to allow a prefetch to be performed when a task that has activated a prefetch thread is the task with the highest priority. Accordingly, it is impossible that instructions or data used by a task with a higher priority are flushed from the cache memory by a task with a lower priority, and therefore to increase the hit rate of the cache memory.","Next, detailed descriptions are given of a computer system according to the second embodiment of the present invention with reference to the drawings. Unlike the computer system in the first embodiment, the computer system according to the present embodiment is equipped with more than one CPU and therefore is capable of parallel execution of processes.",{"@attributes":{"id":"p-0139","num":"0138"},"figref":"FIG. 19","b":["1700","1701","1705","1703","1702","1707","1704","1706","1708"]},"The main memory  has the same configuration as that of the main memory  according to the first embodiment.","Each of the cache memories  and  has the same configuration as that of the cache memory .","The CPU  is an arithmetic unit that executes instructions stored in either the main memory  or the cache memory , and that performs data reading and writing between the main memory  or the cache memory . The CPU  is an arithmetic unit that executes instructions stored in either the main memory  or the cache memory , and that performs data reading and writing between the main memory  or the cache memory .","The memory controller  is a control device that controls access between the main memory  and the cache memory  and that controls access between the CPU  and the cache memory  as well as the main memory . The memory controller  is a control device that controls access between the main memory  and the cache memory  and that controls access between the CPU  and the cache memory  as well as the main memory .","The inter-processor communication device  is a device that connects the CPU  and the CPU  to enable communications to be carried out between the CPUs. Accordingly, it becomes possible for programs running on the respective CPUs to send a command to the other CPU via the inter-processor communication device . More specifically, the CPUs  and  are assigned unique processor IDs so that they can send a command to an arbitrary CPU by specifying its processor ID.",{"@attributes":{"id":"p-0145","num":"0144"},"figref":["FIG. 20A","FIG. 20B","FIG. 20A","FIG. 6A","FIG. 6A"],"b":["2000","2001","2002","2003","2002","2003"]},"Similarly, a prepurge instruction  shown in  causes a CPU with the processor ID indicated by an operand  \u201cCpuid\u201d to prepurge data stored in a location specified by the address indicated by an operand  \u201cAddress\u201d.",{"@attributes":{"id":"p-0147","num":"0146"},"figref":["FIG. 21A","FIG. 20A","FIG. 21B","FIG. 20B"],"b":["2000","2004","1701","1705"]},"Here, assume that a prefetch thread is running on the CPU  and that a program corresponding to a basic block that has called such prefetch thread is running on the CPU . In this case, the prefetch thread running on the CPU  causes the CPU  to perform a prefetch. Consider an example case where the prefetch thread running on the CPU  causes the CPU  to prefetch data stored in the main memory  at the address 0x80000008. In this case, the prefetch thread executes a prefetch instruction  shown in . When the prefetch instruction  is executed, the CPU  issues a command to the inter-processor communication device  to cause the CPU  to prefetch instructions or data stored in the main memory  at the address 0x80000008. Upon receipt of such command, the inter-processor communication device  issues a command to the CPU  to prefetch instructions or data stored in the main memory  at the address 0x80000008. Subsequently, instructions or data stored in the main memory  are transferred to the cache memory , according to the same procedure of the first embodiment.","Next, assume that a prepurge thread is running on the CPU  and that a program corresponding to a basic block that has called such prepurge thread is running on the CPU . In this case, the prepurge thread running on the CPU  causes the CPU  to perform a prepurge. For example, the prepurge thread running on the CPU  executes the prepurge thread  shown in , when prepurging data corresponding to the address 0x80000008 on the cache memory  of the CPU . In this case, a prepurge command is issued to the CPU  according to the same procedure as that for prefetch.",{"@attributes":{"id":"p-0150","num":"0149"},"figref":"FIG. 22","b":["2200","2201","2202"]},"As described above, by including a processor identification instruction as part of the configuration, it becomes possible to previously inform a prefetch thread of the processor ID of a CPU on which the main thread is being executed, when such prefetch thread is activated. This makes it possible for a CPU on which a prefetch thread is running to make a prefetch request to another CPU on which the main thread is running, even when the prefetch thread and the main thread are running on different CPUs. Note that the processor identification instruction  is executed when the execution of the main thread starts, and a processor ID is passed in the form of an argument when the prefetch thread is activated. Here, it is also possible that a processor ID is written in the main memory , which is then passed to the prefetch thread via the main memory . For a prepurge thread too, the processor ID of a CPU on which the main thread is running is passed, as in the case of the prefetch thread.","Detailed descriptions of the other processing are not given here since they are the same as those given in the first embodiment.","Note that in the operating system according to the present embodiment, a task is assigned preferentially to the CPU  () having the cache memory  () whose usage ratio is the lowest. However, as in the case of the computer system  that is equipped with a single processor, when the usage ratios of all the cache memories  and  exceed a specified value, the frequency at which a task with a lower priority is executed is reduced, and the frequency at which a task with a higher priority is executed is increased instead.","According to the present embodiment, in addition to the functions and effects provided by the computer system of the first embodiment, the inter-processor communication device controls access between one processor and another processor. This facilitates the control of more than one cache memory on a program.","Furthermore, by preferentially assigning a task to a processor whose usage ratio is lower, it is possible to improve the overall performance of a computer.","Moreover, the frequency of a task with a higher priority is increased if the usage ratios of all cache memories exceed a specified value. Accordingly, it becomes possible to control plural cache memories on a task-by-task basis in consideration of task priorities.","Note that the compiler apparatus presented in the present embodiment is embodied as a program to be executed on a computer. Therefore, it is possible to store such program on recording media including floppy disk, hard disk, CD-ROM, Magneto-Optical disc (MO), Digital Versatile Disc-Read Only Memory (DVD-ROM). Also, it is possible to store codes in executable form created by the compiler apparatus on these recording media.","Although only some exemplary embodiments of the hardware, operating system, and compiler according to this invention have been described in detail above, those skilled in the art will readily appreciate that many modifications are possible in the exemplary embodiments without materially departing from the novel teachings and advantages of this invention. Accordingly, all such modifications are intended to be included within the scope of this invention.","For example, the computer systems according to the aforementioned embodiments are formed of one or two CPUs, but they may be equipped with three or more CPUs.","Furthermore, in the above embodiments, it is assumed that prepurge processing and prefetch processing are executed for all basic blocks, but such processing does not necessarily have to be performed for all basic blocks, and therefore prefetch processing and prepurge processing may be executed for specific basic blocks. For example, the compiler apparatus may select a basic block for which prefetch and prepurge processing should be performed, based on a loop condition and the like included in a program, execute prefetch processing and prepurge processing only for a basic block for which such processing is effective, based on profile information, and select a basic block for which prefetch processing or prepurge processing should be performed by specifying a compilation option or a program.","Moreover, the compiler apparatus according to the above embodiments creates a prefetch thread and a prepurge thread on a basic block basis, but these threads do not have to be provided for each basic block. For example, a prefetch thread and a prepurge thread may be provided for each function or an arbitrary unit of processing so as to execute prefetch processing and prepurge processing. In this case, it is preferable that units of creating prefetch target lists are changed by the prefetch target extraction device , and locations of inserting thread activation processes are changed by the thread activation process insertion apparatus .","Furthermore, access to the cache memories of the aforementioned embodiments is controlled according to a fully associative scheme, but the significance of the present invention is maintained if access control is performed according to a set associative scheme and a direct-mapped scheme.","Moreover, the prepurge thread shown in  prepurges variables that are not used in the immediately next basic block, but it may also prepurge variables that are not used for two or more subsequent basic blocks. By performing prepurge in consideration of subsequent basic blocks, it is possible to prevent prepurged variables from being prefetched soon after that.","The present invention is applicable to a computer system equipped with a cache memory, as well as to a compiler apparatus and an operating system, and the like used in such computer system."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["These and other objects, advantages and features of the invention will become apparent from the following description thereof taken in conjunction with the accompanying drawings that illustrate a specific embodiment of the invention. In the Drawings:",{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0045","num":"0044"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0046","num":"0045"},"figref":["FIG. 3","FIG. 2"]},{"@attributes":{"id":"p-0047","num":"0046"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIG. 6A"},{"@attributes":{"id":"p-0050","num":"0049"},"figref":"FIG. 6B"},{"@attributes":{"id":"p-0051","num":"0050"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0052","num":"0051"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0053","num":"0052"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0054","num":"0053"},"figref":["FIG. 10","FIG. 9"]},{"@attributes":{"id":"p-0055","num":"0054"},"figref":["FIG. 11","FIG. 8"]},{"@attributes":{"id":"p-0056","num":"0055"},"figref":["FIGS. 12A\u02dc12D","FIG. 10"]},{"@attributes":{"id":"p-0057","num":"0056"},"figref":["FIG. 13","FIG. 8"]},{"@attributes":{"id":"p-0058","num":"0057"},"figref":["FIGS. 14 and 15","FIG. 8"]},{"@attributes":{"id":"p-0059","num":"0058"},"figref":["FIG. 16","FIG. 10"]},{"@attributes":{"id":"p-0060","num":"0059"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0061","num":"0060"},"figref":["FIG. 18","FIG. 9"]},{"@attributes":{"id":"p-0062","num":"0061"},"figref":"FIG. 19"},{"@attributes":{"id":"p-0063","num":"0062"},"figref":"FIG. 20A"},{"@attributes":{"id":"p-0064","num":"0063"},"figref":"FIG. 20B"},{"@attributes":{"id":"p-0065","num":"0064"},"figref":["FIG. 21A","FIG. 20A"]},{"@attributes":{"id":"p-0066","num":"0065"},"figref":["FIG. 21B","FIG. 20B"]},{"@attributes":{"id":"p-0067","num":"0066"},"figref":"FIG. 22"}]},"DETDESC":[{},{}]}
