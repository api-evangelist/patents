---
title: Reflection space image based rendering
abstract: The present invention provides a method, system, and computer program product for reflection space image based rendering of an object at an interactive frame rate. A set of source radiance environment maps associated with a set of source viewing vectors are warped to create a destination radiance environment map associated with a destination viewing vector in a current frame. Blending and weighting operations can also be applied in creating the final destination radiance environment map. An object is then rendered with texture environment mapped from the destination radiance environment map.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07123259&OS=07123259&RS=07123259
owner: Microsoft Corporation
number: 07123259
owner_city: Redmond
owner_country: US
publication_date: 20031027
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS","Table of Contents"],"p":["This application is a continuation application and claims priority under 35 U.S.C. \u00a7120 to U.S. patent application Ser. No. 09\/369,359 filed Aug. 6, 1999, now U.S. Pat. No. 6,697,062 which is herein incorporated by reference in its entirety.","1. Field of the Invention","The present invention relates to the field of computer graphics.","2. Related Art","Offline rendering algorithms have to a great extent conquered physically accurate photo-realism and complex synthetic shading. A result of over twenty years of research, these techniques all solve the lighting or rendering equation in some manner. See, the techniques in Blinn, J. F. and Newell, M. E., 19:542\u2013546 (1976); Cook, R. L., et al., \u201cThe Reyes image rendering architecture,\u201d in (87 .), vol. 21, Stone, M. C., ed., (July 1987), pp. 95\u2013102; Debevec, P., \u201cRendering synthetic objects into real scenes: Bridging traditional and image-based graphics with global illumination and high dynamic range photography,\u201d in 98 ., Cohen, M., ed., Annual Conference Series, ACM SIGGRAPH, Addison Wesley (July 1998), pp. 189\u2013198; He, X. D., et al., \u201cA comprehensive physical model for light reflection,\u201d in (91 .), vol. 25, Sederberg, T. W., ed. (July 1991), pp. 175\u2013186; Jensen, H. W. and Christensen, P. H., \u201cEfficient simulation of light transport in scenes with participating media using photon maps,\u201d in 98 ., Cohen, M., ed., Annual Conference Series, ACM SIGGRAPH, Addison Wesley (July 1998), pp. 311\u2013320; Miller, G. S. and Hoffman, C. R., \u201cIllumination and reflection maps: Simulated objects in simulated and real environments, in 84 (July 1994); Poulin, P. and Fournier, A., \u201cA model for anisotropic reflection,\u201d in (90 .), vol. 24, Baskett, F., ed., (August 1990), pp. 273\u2013284; Veach, E. and Guibas, L. J., \u201cMetropolis light transport,\u201d in 97 ., Whitter, T., ed., Annual Conference Series, ACM SIGGRAPH, Addison Wesley (August 1997), pp. 65\u201376, and the rendering equation in Kajiya, J. T., \u201cThe rendering equation,\u201d (86 .), vol. 20, Evans, D. C. and Athay, R. J., eds., (August 1986), pp. 143\u2013150.","A trade-off between speed and quality exists in off-line rendering and interactive rendering. The outstanding rendering challenge now becomes how to increase the performance of sophisticated shading algorithms without losing the advancements made in quality. This implies that many orders of magnitude in performance improvements must be found. Traditionally, this has been accomplished by vastly simplifying the approximations used in the shading and lighting equations\u2014resulting in a significant loss in complexity and quality.","Environment mapping is one method used to improve the realism of interactive rendering. As originally described by Newell and Blinn (Blinn, J. F. and Newell, M. E., 19:542\u2013546 (1976)), a simple environment map is used to quickly find reflections of distant objects from a perfectly mirrored surface. Other researchers refined this notion by generalizing the BRDF used, though some of these refinements lost the interactivity of simple environment mapping. See, Cabral, B., et al., \u201cBidirectional reflection functions from surface bump maps,\u201d in (87 .), vol. 21, Stone, M. C., ed., (July 1987), pp. 273\u2013281; Greene, N., \u201cApplications of world projections,\u201d in 86, Green, M., ed. (May 1986), pp. 108\u2013114; Miller, G. S. and Hoffman, C. R., \u201cIllumination and reflection maps: Simulated objects in simulated and real environments, in 84 (July 1994); Poulin, P. and Fournier, A., \u201cA model for anisotropic reflection, \u201d in (90 .), vol. 24, Baskett, F., ed., (August 1990), pp. 273\u2013284.","Another method used to bridge the gap between realism and interactivity is image based rendering (McMillan, L. and Bishop, G., \u201cPlenoptic modeling: An image-based rendering system,\u201d in 95 ., Cook, R., ed., Annual Conference Series, ACM SIGGRAPH, Addison Wesley (August 1995), pp. 39\u201346). Image based rendering (IBR) avoids solving the lighting equation during interactive rendering by warping existing photographs or images. These images can be thought of as radiance maps (Gershbein, R., et al., \u201cTextures and radiosity: Controlling emission and reflection with texture maps,\u201d in 94 (Orlando, Fla., Jul. 24\u201329, 1994), Glassner, A., ed., Computer Graphics Proceedings, Annual Conference Series, ACM SIGGRAPH, ACM Press (July 1994), pp. 51\u201358), and generalized to light fields (Levoy, M. and Hanrahan, P., \u201cLight field rendering,\u201d in 96 ., Rushmeier, H., ed., Annual Conference Series, ACM SIGGRAPH, ACM Press (August 1996), pp. 31\u201342) and lumigraphs (Gortler, S. J., et al., \u201cThe lumigraph,\u201d in 96 ., Rushmeier, H., ed., Annual Conference Series, ACM SIGGRAPH, ACM Press (August 1996), pp. 43\u201354). This works well for predefined scenes or images, but not for dynamically changing synthetic objects.","Recently, Debevec (Debevec, P., \u201cRendering synthetic objects into real scenes: Bridging traditional and image-based graphics with global illumination and high dynamic range photography,\u201d in 98 ., Cohen, M., ed., Annual Conference Series, ACM SIGGRAPH, Addison Wesley (July 1998), pp. 189\u2013198) combined captured environment maps and synthetic objects to produce compelling renderings with both synthetic objects and image based environments. His techniques do not work at interactive rates since he computes the lighting equation integration as he renders using RADIANCE (Ward, G. J., \u201cThe RADIANCE lighting simulation and rendering system,\u201d 94 (Orlando, Fla., Jul. 24\u201329, 1994), Glassner, A., ed., Computer Graphics Proceedings, Annual Conference Series, ACM SIGGRAPH, ACM Press (July 1994), pp. 459\u2013572).","What is needed is an interactive photo-realistic rendering algorithm or system.","The present invention provides a method, system, and computer program product for reflection space image based rendering of an object at an interactive frame rate. A set of source radiance environment maps associated with a set of source viewing vectors are warped to create a destination radiance environment map associated with a destination viewing vector in a current frame. Blending and weighting operations can also be applied in creating the final destination radiance environment map. An object is then rendered with texture environment mapping from the destination radiance environment map.","According to one embodiment, at set-up, geometry information for a global set of source viewing vectors and a global set of source radiance environment maps are loaded. A viewpoint tracking loop is then carried out for a current frame. In the viewpoint tracking loop, a destination viewing vector that points along a view direction of the current frame is determined. A subset of source viewing vectors is then determined from the previously loaded global set of source viewing vectors. In one preferred example, the subset of source viewing vectors are a number of viewing vectors (e.g., three source viewing vectors) closest to the destination viewing vector. A set of corresponding weights for the subset of source viewing vectors is also determined.","A warping and blending loop is then performed iteratively on each source radiance environment map corresponding to the subset of source viewing vectors. In one embodiment, each source radiance environment map is warped and blended per texel according to a warping function to a create a destination radiance environment map.","In another preferred embodiment, a fast approximation is made which avoids warping per texel over the entire source radiance environment map. A tessellated mesh with a set of warped texture coordinates is generated. For example, in the case where the source radiance environment maps are sphere maps, the tessellated mesh is simply a tessellated disk. The set of warped texture coordinates at each vertex of the tessellated mesh represent a respective source radiance environment map texel which has been warped according to a warping function evaluated at the destination viewing vector. The mesh is then rendered (also referred to as drawn) using the respective source radiance environment map as a texture map to obtain a texture image. These texture images are also blended with weights corresponding to the respective source viewing vectors. The blended texture images are accumulated in a buffer (such as a frame buffer) to obtain a final destination radiance environment map. Control returns to the viewpoint tracking loop.","The final destination radiance environment map can be further normalized by weight if the weights were not previously normalized. The final destination radiance environment map is then loaded as environment map. An object or scene is then rendered with texture environment mapped from the final destination radiance environment map.","The present invention can be implemented in software, firmware, hardware, or in any combination thereof. The present invention further can be implemented in any graphics architecture or system that supports texture mapping and environment mapping. In one example, not intended to limit the present invention, a routine implementing the present invention is executed on a host processor using a graphics API or library, such as, OpenGL\u00ae.","Thus, the present invention goes beyond the work of Debevec by pre-integrating the lighting equation to allow interactive rendering\u2014in part by constraining the geometry of the global radiance environment maps. This introduces a view-dependence in the generated maps, which is overcome with a new form of IBR in reflection space. The result is interactive rendering of synthetic objects using a wide range of surface reflectances (including but not limited to those described by BRDFs) in arbitrary lighting environments.","There are at least two contributions made by the present invention. The first is the application of IBR techniques in reflection space. This is called reflection space IBR\u2014even though the invention does not necessarily operate on normal images, nor is the result a rendered image. Second is our hybrid rendering algorithm. The algorithm of the present invention and the nature of the source radiance environment maps allows new surface reflectances (e.g., BRDFs) and new geometry not found in the original source images to be used\u2014thus extending the class of renderings possible with interactive IBR algorithms.","In sum, the present invention is an intermediate step toward interactive photo-realistic rendering\u2014an algorithm that uses IBR techniques to approximate the integral lighting equation. Such an interactive algorithm is in demand now and will change the nature and work-flow of artists, designers and scientists.","Further features and advantages of the present invention, as well as the structure and operation of various embodiments of the present invention, are described in detail below with reference to the accompanying drawings.","The present invention is described with reference to the accompanying drawings. In the drawings, like reference numbers indicate identical or functionally similar elements. Additionally, the left-most digit(s) of a reference number identifies the drawing in which the reference number first appears.","0. Overview and Terminology","1. Routine for Reflection-Spaced Image-Based Imaging\n\n","2. Radiance Environment Maps\n\n","3. Reflection Space IBR\n\n","4. Rendering Algorithm","5. Examples","6. Example Implementations","7. Example GUI Computer System","8. Conclusion","0. Overview and Terminology","High quality, physically accurate rendering at interactive rates has widespread application, but is a daunting task. The present invention bridges the gap between high quality offline and interactive rendering by using existing environment mapping hardware in combination with a new Image Based Rendering (IBR) algorithm. One aspect of the present invention lies in performing IBR in reflection space. The present invention can be applied to ordinary environment maps, but for more physically accurate rendering, reflection space IBR is applied to radiance environment maps. In one example, a radiance environment map pre-integrates a Bidirectional Reflection Distribution Function (BRDF) with a lighting environment. A BRDF can be, but is not limited to, an arbitrary four dimension (4D) function that describes how a physical surface (or non-physical surface) reflects light. Using the reflection-space IBR algorithm on radiance environment maps allows interactive rendering of arbitrary objects with a large class of complex BRDFs in arbitrary lighting environments. The ultimate simplicity of the final algorithm suggests that it will be widely and immediately valuable given the ready availability of hardware-assisted environment mapping.","The term \u201cradiance environment map\u201d is used broadly herein to refer to any type of environment map including viewpoint dependent and viewpoint independent maps. A \u201cradiance environment map\u201d can include but is not limited to, any conventional environment map (both pre-integrated and non-pre-integrated). In one example, a radiance environment map can include but is not limited to one which pre-integrates a Bidirectional Reflection Distribution Function (BRDF) with a lighting environment.","1. Routine for Reflection-Spaced Image-Based Imaging","1.1 Set-up",{"@attributes":{"id":"p-0047","num":"0054"},"figref":["FIGS. 1A and 1B","FIG. 1B"],"b":["100","110","120","125","110","120","125","125","130","190","155","155","160","160","162","169"]},"At setup, geometry information of source viewing vectors is loaded (step ). A global set of N source radiance environment maps is also loaded (step ). Each of these steps is similar to conventional radiance environment mapping except that multiple source viewing vectors and multiple source radiance environment maps are used. For example, in the case of spherical source radiance environment maps are used, spherical geometry information is loaded in step . For example, geodesic locations of each of the N source viewing vectors is loaded. Similarly, in step , multiple source radiance environment maps representing spherical environment maps taken at different viewpoints are loaded. In general, each of the source radiance environment maps can be taken from any different viewpoint. In one preferred example, twelve source radiance environment maps are used representing twelve approximately equidistant viewpoints in a three-dimensional space around a sphere.","The present invention can be used with any type of environment map including, but not limited to, radiance environment maps of a spherical or cubic type. In one preferred example, preintegrated radiance environment maps are used of either a spherical or cubic type. An example radiance environment sphere map is shown in . Prior to loading, of course, the multiple source radiance environment maps must be generated off-line. The multiple source radiance environment maps can be generated through photography or computer generated using ray tracing algorithms or other techniques. The generation of the source radiance environment maps is described in further detail below in section 2.","1.2 Viewpoint Tracking Loop","First, destination viewing vector coordinates are computed (step ). These coordinates define the destination viewing vector of the current frame in three-dimensional space. In general, any three-dimensional coordinate system or coordinate space can be used. An example destination viewing vector vin three-dimensional space is illustrated in , as described further below in section 3.4.","Next, a subset of the set of source viewing vectors is determined (step ). This subset has a number of source viewing vectors M, where M is less than N. In one preferred example, the determination involves finding a number of source viewing vectors (e.g., three) in the set of N source viewing vectors which are nearest to the computed destination viewing vector. In this way, the subset of source viewing vectors corresponds to a subset of the previously-loaded source radiance environment sphere maps which were generated for viewpoints closest to the destination viewpoint of the current frame being rendered.","In step , weights are then computed for the subset of viewing vectors determined in step . In the case of a sphere geometry example, each of the weights correspond to respective areas of a spherical patch. For example, as illustrated in  and described in further detail in section 3.4 below, if three viewing vectors v, v, vare determined to be nearest to a particular point of view given by a destination to viewing vector v, then a spherical patch is defined by the three viewing vectors v, v, v. This spherical patch is a spherical triangle formed by the viewing vectors v, v, v. Destination viewing vector Vdefines the line of sight of the viewer and forms three spherical triangles within the largest spherical patch. As shown in , areas a, a, and arepresent three weights for the sphere maps associated with the viewing vectors v, v, and v, respectively. In this way, the contribution of texture from a source environment map at a source viewing vector can be weighted so that its contribution to a final destination source radiance environment map is proportional to how close the source viewing vector is to the destination viewing vector.","1.3 Warping and Blending Loop","In loop , each source radiance environment map corresponding to the subset M of source viewing vectors determined in step  is warped and blended to create a destination radiance environment map at the destination viewpoint of the current frame (step ). In general, each source radiance environment map is warped to create the destination radiance environment map. Blending is used to further improve the quality of the final destination radiance environment map. Blending, of course, could be omitted in certain applications. Also, any type of blending can be used, including, but not limited to the examples of blending described herein.","Specific warping functions for mapping texels from each source radiance environment map to a destination radiance environment map are described in further details in Section 3.2 and 3.3 below. Warping can be expensive if performed per texel over an entire source radiance environment map. Even the case where three source radiance environment maps are warped to create a single, final destination radiance environment map can be expensive and impractical for an interactive application if warping is performed per texel.  shows an example routine that uses a fast, accurate approximation by rendering a final destination radiance environment map as a tessellated mesh. In an example of a destination sphere map, the mesh is a tessellated disk. Texture coordinates at each mesh vertex are then chosen according to a warping function, and the source radiance environment map or image is used as a texture while rendering a tessellated disk into the frame buffer.","An example of this process is shown in the loop  in  (steps \u2013). Loop  is repeated for each viewpoint vin subset M. For example, in the case of three viewing vectors in subset M, loop  is repeated three times (i=0 to M\u22121).","A tessellated mesh is generated with a respective set of warped texture coordinates (step ). In the case of a sphere map, the tessellated mesh corresponds to a tessellated disk. The set of warped texture coordinates corresponds to the respective source radiance environment map warped according to a warping function. In the present invention, a warping function warps a radiance environment map for one view direction to another view direction. Any warping function can be used depending on a particular application and desired effect.","Two examples of warp functions are further described in Section 3.3 below. One warp function is best used where a central reflection is a surface normal as for diffuse objects. The second warp function is for BRDFs where the central reflection direction generally aligns with the mirror reflection direction as is the case in more shiny objects.","An identity warping function can also be used. An identity warping function is one where essentially no warping only blending occurs. This can be used for example in a case of a mirror warp for a cube map (in other words, in an identity pwould equal pin the warp described below).","Next, the mesh is rendered or drawn (step ). The respective warp texture coordinates are used to map texture from the respective source radiance environment map to obtain a textured image. Any conventional texture mapping technique can be used including, but not limited to, mip-mapping. The textured image is further blended at a respective weight and accumulated to obtain a weighted destination radiance environment map in a frame buffer or other accumulation buffer (step ). In step , a check is made to determine if i=M\u22121, that is, whether each source radiance environment map has been warped and blended as described with respect to steps \u2013. Once i=M\u22121 and all source radiance environment maps in subset M have been warped and blended, then control returns to step .","In step , the weighted destination radiance environment map is further normalized according to the M weights. Note that step  is optional. Step  can also be omitted if the weights are first normalized in step . The normalized weighted destination radiance environment map is then loaded into memory as an environment map (step ). An object or scene is then rendered using the normalized destination radiance environment map (step ). Any conventional environmental mapping technique can be used to map texture from the loaded normalized weighted destination environment map to the object during rendering in step  as would be apparent to a person skilled in the art given this description.","After step , viewpoint tracking loop  repeats for each frame at interactive rates. Interactive rates of approximately 10 frames per second and higher are achieved. In one example, on a Silicon Graphics\u00ae Onyx2\u2122 InfiniteReality2\u2122, the interactive viewing program ran at a sustained frame rate of 20 Hz. Such rates are not intended to limit the present invention, however, and in general, faster and slower interactive rates are possible depending on a particular application and hardware support that is provided. Routine  is further described with respect to example radiance environment maps and specific warps in Sections 2 through 3 below. An example of routine  written in pseudo-code is presented in section 4. Examples of routine  applied to rendering an automobile in an environment are further shown below in Section 5. Examples implementations of routine  and software, firmware and\/or hardware and different types of graphics architectures are also described in Section 6 below.","2. Radiance Environment Maps","A traditional environment map records the incident radiance, L, from each direction. The two most common representations are the sphere map and the cube map. A sphere map is a view-dependent representation, equivalent to an orthographic view of a reflective sphere. Note that it is not necessary to render an orthographic view when using a sphere map, for example the sphere mapping in OpenGL\u00ae 1.2 includes the appropriate correction for perspective views. A cube map is a view-independent representation, created by projecting the environment onto the faces of a cube. These examples are illustrative and not intended to limit the present invention. Other representations of an environment in a reflection space are also possible as would be apparent to a person skilled in the art given this description. For example, a parabolic map can be used. See View-independent environment maps; Wolfgang Heidrich, and Hans-Peter Seidel; Proceedings of the 1998 EUROGRAPHICS\/SIGGRAPH workshop on Graphics hardware, 1998, Page 39.","Instead of recording incoming radiance, a radiance environment map records the total reflected radiance, L, for each possible surface orientation. It is defined by the classic lighting equation (Immel, D. S., et al., \u201cA radiosity method for non-diffuse environments,\u201d (86 .), vol. 20, Evans, D. C. and Athay, R. J., eds., (August 1986), pp. 133\u2013142):",{"@attributes":{"id":"p-0067","num":"0074"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msub":{"mi":["L","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["\u03b8","r"]},{"mi":["\u03d5","r"]}],"mo":","}}},{"msub":{"mo":"\u222b","mi":"H"},"mo":"\u2062","mrow":{"mrow":[{"msub":{"mi":["f","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["\u03b8","i"]},{"mi":["\u03d5","r"]}],"mo":[",",","],"mrow":{"msub":[{"mi":["\u03d5","i"]},{"mi":["\u03b8","r"]}],"mo":";"}}}},{"msub":{"mi":["L","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["\u03b8","i"]},{"mi":["\u03d5","i"]}],"mo":","}}},{"mo":"\u2146","msub":{"mi":["\u03c9","i"]}}],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mi":"cos","mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.2em","height":"0.2ex"}}}],"msub":{"mi":["\u03b8","i"]}}}],"mo":"="}}},"br":{},"sub":["r ","i","r","r "],"figref":"FIG. 2"},{"@attributes":{"id":"p-0068","num":"0075"},"figref":["FIG. 2","FIG. 2"],"sub":["r;s","r;s ","i"],"sup":"H(n) "},"The pre-integrated radiance environment map introduces a couple of restrictions for rendering. Since all surface points that have a common normal use the same reflected radiance, only the lighting contribution from a distant environment can be captured, not reflection of local objects or inter-reflection of a single surface. Also, in one preferred example isotopic BRDFs are used (with only a single reflected radiance stored in the map per surface normal it is difficult to record BRDF variation with rotation around the normal).","An approximation is also introduced in the true lighting equation. A radiation environment map is computed with a single view direction, so it is technically incorrect to use it with a perspective view, where the view direction changes from pixel to pixel. While graphics hardware corrects for perspective-induced changes in mirror reflection direction, the correction is not always appropriate for the radiance environment map. Regardless, in the present invention perspective views can be rendered anyway. The usually minor resulting errors, if any, are easily accepted as part of the price for interactivity.","2.1 Obtaining Radiance Environment Maps","Prior to step , a set of radiance environments must first be obtained or generated. In one preferred example, pre-integrated radiance environment maps are first obtained or generated in a pre-processing stage. In general, any conventional method for obtaining a set of radiance environment maps for different viewpoints can be used.","One method to obtain radiance environment maps is to take photographs in the desired environment of a physical sphere whose surface BRDF matches that of the target object. The photographs should be taken with a narrow field view lens to approximate an orthographic projection and to minimize the reflection of the camera. The resulting images are the radiance environment maps, with the integration done by nature. The algorithm of the present invention uses several radiance environment maps, so the inventors use several such images along with the camera orientation for each.","A second method is to compute lighting integral numerically given a desired BRDF and lighting environment. The lighting environment should be known with high dynamic range for good integration results. Such environments can be captured through photographs by the methods of Debevec (Debevec, P. E. and Malik, J., \u201cRecovering high dynamic range radiance maps from photographs,\u201d in 97 ., Cohen, M., ed., Annual Conference Series, ACM SIGGRAPH, Addison Wesley (August 1997), pp. 369\u2013378), or rendered with a package like RADIANCE (Ward, G. J., \u201cThe RADIANCE lighting simulation and rendering system,\u201d 94 (Orlando, Fla., Jul. 24\u201329, 1994), Glassner, A., ed., Computer Graphics Proceedings, Annual Conference Series, ACM SIGGRAPH, ACM Press (July 1994), pp. 459\u2013572). Six photographs or images have been used to represent L, arranged as a cube environment map in the present invention. See also Voorhies, D. and Foran, J., \u201cReflection vector shading hardware,\u201d 94 (Orlando, Fla., Jul. 24\u201329, 1994), Glassner, A., ed., Computer Graphics Proceedings, Annual Conference Series, ACM SIGGRAPH, ACM Press (July 1994), pp. 163\u2013166. Since the BRDF and the environment map, L, are decoupled, the lighting environment can be reused to compute Lfor many different surface types. Results using maps computed in this way are shown in .","3. Reflection Space IBR","With conventional IBR, the light field is sampled by a discrete set of images. For the present invention's algorithm, these samples are a set of radiance environment maps taken from different viewpoints. These maps must be warped to match a new point of view, then blended together.","In addition to matching the viewpoint, the warping correlates features on the different maps. For traditional IBR, the image correlation may require only an affine or projective warp (Seitz, S. M. and Dyer, C. R., \u201cView morphine: Synthesizing 3D metamorphoses using image transforms,\u201d in 96 ., Rushmeier, H., ed., Annual Conference Series, ACM SIGGRAPH, Addison Wesley (August 1996), pp. 21\u201330). For general light fields, it can require gathering light rays in a variety of discontinuous patterns (Levoy, M. and Hanrahan, P., \u201cLight field rendering,\u201d in 96 ., Rushmeier, H., ed., Annual Conference Series, ACM SIGGRAPH, ACM Press (August 1996), pp. 31\u201342).","Since each point in a radiance environment map is an integration of the environment and BRDF, the warp that best correlates features in the environment can vary from BRDF to BRDF. By choosing a warp that models the BRDF well, one can significantly reduce the number of radiance environment maps required for good reconstruction. If the BRDF is a perfect mirror and the warp models it as a perfect mirror, one need only sample well enough to catch the highest frequencies in the environment. If the warp does not match the BRDF, one must sample well enough for the product of the highest frequencies in the BRDF and environment. This is because the lighting integral is essentially a convolution of the BRDF with the environment.","According to a further feature, for BRDFs that are principally reflective, a warp is used that matches the reflection directions of the different maps. So a point on the source image warps to the point on the destination image that reflects in the same direction. Primarily diffuse BRDFs suggest a warp that matches surface normal directions. One can find a well-matched warp for any BRDF that is radially symmetric about a principal direction and does not change shape across the surface. With such a BRDF, the same area of the environment will be integrated for corresponding points from different views.","Lambertian diffuse reflection and Phong specular reflection both satisfy this restriction, but most more realistic BRDFs do not. Fortunately, since the radiance environment maps sample a smooth, continuous function, the present invention can effectively handle a much wider class of BRDFs that are close to the symmetric ideal without requiring a large number of sample maps. For example, the inventors have used a numerically computed BRDF with Fresnel effects and diffuse, specular and backscatter components. For the BRDF, the inventors use a warp that matches mirror reflections. It works because the specular lobe is the only high-frequency component of the BRDF and its shape does not vary too much from texel to texel. The Fresnel effects are naturally handled by the method, and the other components do not require a large number of sample maps because they are low frequency.","Once the sample maps have been warped to the new viewpoint, they must be combined with some reconstruction filter. Oppenheim and Schafer (Oppenheim, A. V. and Schafer, R. W., , Prentice-Hall, Englewood Cliffs, N.J. (1975)) describe many sampling and reconstruction choices. For simplicity and efficiency, linear interpolation is used between neighboring images. The linear interpolation uses a spherical form of barycentric weights, presented in section 3.4. Thus, for any given viewpoint, the three nearby radiance environment maps are warped and blended to create an approximation to the new map (see ).","3.1 Sampling View Directions","Certain environment map representations (e.g., cube maps) are viewpoint independent, while others (e.g., sphere maps) depend on the viewpoint. In contrast, each radiance environment map, whether it is stored in cube map, sphere map or another form, is correct for only a single viewpoint. This is because the radiance environment map captures Fresnel reflection and other view-dependent effects.","As alluded to above, the view-dependence does not limit the use of each map to only one view. This limitation is overcome by precomputing a set of maps\u2014denoted L, j\u2208{0 . . . N\u22121}\u2014at various viewpoints. The unit view vectors can be thought of as points lying on a sphere. The inventors use reflection-space IBR to reconstruct the map for rendering from the Lmaps, but in one example, still require reasonable coverage of the sphere of possible view directions to avoid aliasing artifacts. In one case, the inventors have used one Lfor each viewpoint defined at the vertices of an icosahedron. The number of samples has been sufficient for the environments and BRDF which have been employed and is desirable because its symmetry means that each viewpoints is handled in an unbiased manner.","3.2 Map Warping","Each warp is between a source map L(from the precomputed set L) and a destination map, L(for the current rendering viewpoint). Points in these maps will be called pand prespectively.","For each map point, p, there is a vector r along the central reflection direction of the BRDF. For Phong specular or perfect mirror reflections, r is the geometric reflection vector. For diffuse surfaces, r is the surface normal. To assist in the warp, define an invertible function\n\ng:p\u2192r\n\ng(p) depends on both the BRDF and the map representation. It is most easily defined in terms of a local coordinate system for each map, so the inventors also define a transformation per map to convert the local coordinate system to a common global space\n\nT:r\u2192{right arrow over (r)}\n\nThe composition of these functions defines the full warp from pto p:\n\n()\n\nThis takes a point in L(defined by s and t texture coordinates for a sphere map representation). It is converted first to a 3D reflection vector in the local coordinate system associated with L. This 3D vector is transformed to the global space, then to a vector in the local coordinate system associated with L. Finally, the resulting vector is converted to a point in L(once again given by two texture coordinates if one uses the sphere map representation).\n","3.3 Specific Warps","Two specific warp functions are derived. For clarity, only two warping functions are described in detail. However, other warping functions can be used as would be apparent to a person skilled in the art given this description. Both use a sphere-map representation for Land L. The first is for BRDFs where the central reflection direction is the surface normal. The second is for BRDFs where the central reflection direction aligns with the mirror reflection direction.","For both warps, the local coordinate system associated with each map is aligned with the camera used to create the map. The x-axis points right, the y-axis points up and the z-axis points from the origin toward the camera. Thus transformations Tand Tare defined as 3\u00d73 matrices with columns equal to three axes expressed in global coordinates.",{"@attributes":{"id":"p-0091","num":"0098"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"msub":{"mi":["g","normal"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["s","t"],"mo":","}}},{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":{"mn":"2","mo":"\u2062","mi":"s"},"mo":"-","mn":"1"}}},{"mtd":{"mrow":{"mrow":{"mn":"2","mo":"\u2062","mi":"t"},"mo":"-","mn":"1"}}},{"mtd":{"msqrt":{"mrow":{"mn":"1","mo":["-","+"],"msup":[{"mrow":{"mo":["(",")"],"mrow":{"mrow":{"mn":"2","mo":"\u2062","mi":"s"},"mo":"-","mn":"1"}},"mn":"2"},{"mrow":{"mo":["(",")"],"mrow":{"mi":"t2","mo":"-","mn":"1"}},"mn":"2"}]}}}}]}}],"mo":"="}}},{"mtd":{"mrow":{"mrow":[{"msubsup":{"mi":["g","normal"],"mrow":{"mo":"-","mn":"1"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y","z"],"mo":[",",","]}}},{"mo":["(",")"],"mrow":{"mrow":[{"mfrac":{"mi":"x","mn":"2"},"mo":"+","mn":"0.5"},{"mfrac":{"mi":"y","mn":"2"},"mo":"+","mn":"0.5"}],"mo":","}}],"mo":"="}}}]}}},"br":{},"sub":["mirror","normal"]},{"@attributes":{"id":"p-0092","num":"0099"},"maths":[{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msub":{"mi":["g","mirror"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["s","t"],"mo":","}}},{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mrow":{"mn":"2","mo":"\u2062","mi":"xz"}}},{"mtd":{"mrow":{"mn":"2","mo":"\u2062","mi":"yz"}}},{"mtd":{"mrow":{"mrow":{"mn":"2","mo":"\u2062","msup":{"mi":"z","mn":"2"}},"mo":"-","mn":"1"}}}]}}],"mo":"="}}},{"@attributes":{"id":"MATH-US-00003-2","num":"00003.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msubsup":{"mi":["g","mirror"],"mrow":{"mo":"-","mn":"1"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y","z"],"mo":[",",","]}}},{"msubsup":{"mi":["g","normal"],"mrow":{"mo":"-","mn":"1"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":[",",","],"mrow":{"mi":"z","mo":"+","mn":"1"}}},"msqrt":{"mrow":{"msup":[{"mi":"x","mn":"2"},{"mi":"y","mn":"2"},{"mrow":{"mo":["(",")"],"mrow":{"mi":"z","mo":"+","mn":"1"}},"mn":"2"}],"mo":["+","+"]}}}}}],"mo":"="}}}]},"In one example, since graphics hardware is available to do mirror reflections with a sphere map, the final stage of both warps is modified to use g. The following functional composition chains define the two warps:\n\n()\n\n()\n","Performing three of these warps per texel in the target map for every rendered frame is expensive and impractical for an interactive application. A fast, accurate approximation is possible by rendering the destination sphere map as a tessellated disk. Texture coordinates at each mesh vertex are chosen according to the warping function, and the source image is used as a texture while rendering the disk into the frame buffer. To account for the non-linearity of the warp functions, the mesh is finer toward the edges of the disk and coarser near the center. The 3D coordinate system associated with the destination map changes as the view moves, but the same disk tessellation can always be used. The reflection vectors, r, also remain constant for each mesh vertex and can be precomputed.","The piecewise linear approximation to the warp is accurate for most of the sphere map area. In a sphere map representation, the mirror warp has a singularity at the limit of extreme grazing reflections around the edge of the map\u2014the reflection direction exactly opposite the view vector. The warp equation from Lto Lcan fail at this singularity.","One can locate pfor the singularity by warping the problem reflection direction (the negated source map view vector) into the destination map. Near this point in the destination map, the source map will become pinched and unreliable instead of warping cleanly. Thus, the inventors use a simple distance from pin the destination map to weight our confidence in the warped source image. This weight is used to fade the contribution of each source near its respective singularity.","The textured disk method accelerates the warping operation in two ways. First, s and t are not explicitly calculated for all the points on the sphere map, only at the vertices of the mesh. Second, a major bottleneck in performing the warp is accessing memory associated with the source and destination maps. The inventors leverage the rendering and texturing hardware to solve this memory bottleneck.","3.4 Spherical Barycentric Interpolation","Once the warps have taken place the warped images must be blended. One example interpolation scheme according to the present invention is a spherical variant of classic affine barycentric interpolation, as defined in Farin (Farin, G., , Academic Press, (1990)). Classic barycentric interpolation uses the ratio of the areas of triangles. The inventors instead use the ratio of spherical triangles.",{"@attributes":{"id":"p-0100","num":"0107"},"figref":"FIG. 3","sub":["0","1 ","2 ","d","d ","0","1 ","2 ","0","1 ","2 ","1","1 ","1 ","1"]},"Any given view vector, v, will in general lie within a spherical patch as illustrated in . Each vertex, v, of this spherical triangle is the view vector for one of the source images that have been warped to match v. vis used to form three interior spherical triangles. The weight for the source image at vertex i is a ratio of the areas of the spherical triangle opposite vand the overall spherical triangle. The area of an interior spherical triangle, a, on a unit sphere is given by the spherical excess formula (Bronshtein, I. and Semendyayev, K., , Van Nostrand Reinhold Company, (1985))\n\n=\u03b1+\u03b2+\u03b30,1,2\n\nThe dihedral angles \u03b1, \u03b2, and \u03b3are defined as:\n\n\u03b1=cos(()\u00b7())\n\n\u03b2=cos(()\u00b7())\n\n\u03b3=cos(()\u00b7())\n\nWhere {circumflex over (x)} is the normalized cross product and \u00f8 is an index wrapping operator, defined as\n",{"@attributes":{"id":"p-0102","num":"0109"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":["a","\u2205","b"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},{"mo":"{","mtable":{"mtr":[{"mtd":[{"mrow":{"mi":"b","mo":"-","mn":"1"}},{"mrow":{"mrow":{"mi":["if","a"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},"mo":"<","mn":"0"}}]},{"mtd":[{"mn":"0"},{"mrow":{"mrow":{"mi":["if","a"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},"mo":"\u2265","mi":"b"}}]},{"mtd":[{"mi":"a"},{"mi":"otherwise"}]}]}}],"mo":"="}}}},"4. Rendering Algorithm","The present invention leads to a straightforward interactive rendering algorithm. Pseudo-code for the algorithm is given here. It leverages texture mapping graphics hardware in two ways: once to perform the warping and blending between the sample images; and again using the generated sphere map in the final rendering.",{"@attributes":{"id":"p-0105","num":"0112"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"main ( )"]},{"entry":[{},"\/\/ Set up radiance maps and sphere geometry"]},{"entry":[{},"G  LoadGeodesicLocations( ); \u03bd \u2208 {(\u03b8, \u03c6)...(\u03b8, \u03c6)}"]},{"entry":[{},"Lr,j   LoadSphereMaps (G, L, \u0192);j \u2208 {0...N \u2212 1}"]},{"entry":[{},"\/\/Viewpoint tracking loop"]},{"entry":[{},"loop for each frame"]},{"entry":[{},"\u2003(x, y, v)   ComputeViewCoordinateSystem( )"]},{"entry":[{},"\u2003(v, v, v)   FindSubtendedTriangle(G, v)"]},{"entry":[{},"\u2003(a, a, a)   ComputeWeights( (v, v, v), v)"]},{"entry":[{},"\u2003glClearAccum( 0, 0, 0, 0)"]},{"entry":[{},"\u2003\/\/Warp and blending loop"]},{"entry":[{},"\u2003loop for each of the three vertices, i"]},{"entry":[{},"\u2003\u2003mesh   ComputeMesh(v, v)"]},{"entry":[{},"\u2003\u2003DrawMesh(mesh, L)"]},{"entry":[{},"\u2003\u2003glAccum(GL_ACCUM, a)"]},{"entry":[{},"\u2003end vertex loop"]},{"entry":[{},"\u2003glAccum(GL_RETURN, 1.0\/(a0 + a1 + a2))"]},{"entry":[{},"\u2003LoadNewSphereMap( )"]},{"entry":[{},"\u2003RenderObject ( )"]},{"entry":[{},"end frame loop"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"The interactive rendering program outlined above reads in a set of sphere maps at a prescribed set of geodesic locations along with the associated triangle faces. This decouples the interactive program from any specific choice of sphere map sampling view directions.","5. EXAMPLES","The inventors have validated this technique with several examples.  shows an image of an automobile in an environment generated according to one automobile example of the present invention.  shows example images of source radiance environment maps, corresponding destination radiance environment maps after warping and blending with appropriate spherical barycentric weighting, and a final generated radiance environment map according to an example of the present invention. The outer images are source radiance environment maps for a test environment and a mirror BRDF. The next layer of images show each map warped to the new viewpoint with appropriate spherical barycentric weighting. The center image is the final generated radiance environment map.  shows all of the textures that would be used to render a full model of a car, with separate radiance environment maps generated for the paint, chrome trim, and windows.",{"@attributes":{"id":"p-0109","num":"0116"},"figref":"FIG. 4","i":"Proc. SIGGRAPH '"},"Each of the articles and references cited herein are incorporated in their entirety herein by reference.","6. Example Implementations","Routine  as described above can be implemented in software, firmware, hardware, or in any combination thereof. Several example environments that can support an implementation of routine  are further described with respect to . These examples are illustrative and not intended to limit the present invention.",{"@attributes":{"id":"p-0113","num":"0120"},"figref":"FIG. 7","b":["700","100","700","710","740","100","710","740","710","740","710","720"],"i":["An Object","Oriented ","D Graphics Tool Kit"]},"Layer  represents a graphics application programming interface (graphics API) and\/or other system support (such as, operating system and windowing system). OpenGL\u00ae is an example graphics API. See, e.g., Woo et al., , Second Ed., version 1.1., Addison-Wesley: Mass., 1997 (incorporated in its entirety herein by reference). In one implementation the present invention can be implemented in OpenGL\u00ae. OpenGL\u00ae, of course, otherwise does not need the present invention to operate.","Layer  represents hardware, including graphics hardware. Graphics hardware  can be any graphics pipeline or system including, but not limited to, a computer graphics processor (single chip or multiple chips), high-end to low-end graphics workstations, gaming platforms, systems and consoles, network architectures (e.g., client\/server, local, intermediate or wide area networks), and virtual machine (e.g., a Java-created application). For example, a computer graphics system can include, but is not limited to, Indigo, Indy, Onyx, Onyx2, Infinite Reality, Infinite Reality 2, or Ographics workstations manufactured by Silicon Graphics, Inc.",{"@attributes":{"id":"p-0116","num":"0123"},"figref":"FIG. 8","b":["800","100","800","810","815","805","805","820","830","840","850","855","805","860","860","880"]},"Routine  can be implemented as a program running on host processor  as shown in . Routine  then provides control for graphics system  as described above with respect to  and further indicated schematically by control lines . Apart from the program executing routine  and control lines , each of the components in system  (including graphics subsystem , host processor , memory , frame buffer , and display ) can be a conventional or commercially available off the shelf (COTS) graphics component. Processing functionality for implementing routine  can also be transferred all or in part from host processor  to a separate logic unit (not shown) or to the graphics subsystem  as would be apparent to a person skilled in the art given this description.","7. Example GUI Computer System",{"@attributes":{"id":"p-0119","num":"0126"},"figref":["FIG. 9","FIG. 9"],"b":["100","900","904","900"]},"The processor  is connected to a communications infrastructure  (e.g., a bus, cross-bar switch, or network). Various software embodiments are described in terms of this example computer system. This description is illustrative and not intended to limit the present invention. After reading this description, it will be apparent to a person skilled in the relevant art how to implement the invention using other computer systems and\/or computer architectures.","Computer system  includes a graphics subsystem . Graphics subsystem  can be implemented as one or more processor chips. The graphics subsystem  can be included as part of processor  as shown in  or as a separate graphics engine or processor. Graphics data is output from the graphics subsystem  to the bus . Display interface  forwards graphics data from the bus  for display on the display unit .","Computer system  also includes a main memory , preferably random access memory (RAM), and can also include a secondary memory . The secondary memory  can include, for example, a hard disk drive  and\/or a removable storage drive , representing a floppy disk drive, a magnetic tape drive, an optical disk drive, etc. The removable storage drive  reads from and\/or writes to a removable storage unit  in a well known manner. Removable storage unit  represents a floppy disk, magnetic tape, optical disk, etc., which is read by and written to by removable storage drive . As will be appreciated, the removable storage unit  includes a computer usable storage medium having stored therein computer software and\/or data.","In alternative embodiments, secondary memory  may include other similar means for allowing computer programs or other instructions to be loaded into computer system . Such means can include, for example, a removable storage unit  and an interface . Examples can include a program cartridge and cartridge interface (such as that found in video game devices), a removable memory chip (such as an EEPROM, or PROM) and associated socket, and other removable storage units  and interfaces  which allow software and data to be transferred from the removable storage unit  to computer system .","Computer system  can also include a communications interface . Communications interface  allows software and data to be transferred between computer system  and external devices via communications path . Examples of communications interface  can include a modem, a network interface (such as Ethernet card), a communications port, etc. Software and data transferred via communications interface  are in the form of signals which can be electronic, electromagnetic, optical or other signals capable of being received by communications interface , via communications path . Note that communications interface  provides a means by which computer system  can interface to a network such as the Internet.","Graphical user interface module  transfers user inputs from peripheral devices  to bus . These peripheral devices  can be a mouse, keyboard, touch screen, microphone, joystick, stylus, light pen, voice recognition unit, or any other type of peripheral unit.","The present invention is preferably implemented using software running (that is, executing) in an environment similar to that described above with respect to . In this document, the term \u201ccomputer program product\u201d is used to generally refer to removable storage unit  or a hard disk installed in hard disk drive . These computer program products are means for providing software to computer system .","Computer programs (also called computer control logic) are stored in main memory and\/or secondary memory . Computer programs can also be received via communications interface . Such computer programs, when executed, enable the computer system  to perform the features of the present invention as discussed herein. In particular, the computer programs, when executed, enable the processor  to perform the features of the present invention. Accordingly, such computer programs represent controllers of the computer system .","In an embodiment where the invention is implemented using software, the software may be stored in a computer program product and loaded into computer system  using removable storage drive , hard drive , or communications interface . Alternatively, the computer program product may be downloaded to computer system  over communications path . The control logic (software), when executed by the processor , causes the processor  to perform the functions of the invention as described herein.","In another embodiment, the invention is implemented primarily in firmware and\/or hardware using, for example, hardware components such as application specific integrated circuits (ASICs). Implementation of a hardware state machine so as to perform the functions described herein will be apparent to persons skilled in the relevant art(s).","Description in these terms is provided for convenience only. It is not intended that the invention be limited to application in this example environment. In fact, after reading the following description, it will become apparent to a person skilled in the relevant art how to implement the invention in alternative environments.","8. Conclusion","Interactive photo-realistic rendering is difficult to achieve because solving the lighting integral is computationally expensive. In one example, the inventors pre-compute this integral into several view-dependent radiance environment maps, each of which would allow realistic rendering of arbitrary geometry from one fixed viewpoint. The inventors further dynamically create new maps for new viewpoints by combining the precomputed maps using an application of IBR techniques in reflection space. The resulting algorithm is readily implementable on most texture mapping capable graphics hardware. The technique allows high rendering quality at interactive rates and from arbitrary viewpoints.","Traditional IBR could not have achieved these results; it is limited to rendering geometry contained in the source images. Traditional rendering, even using radiance environment maps, could also not have achieved these results; it could not provide the viewpoint independence without a fast way to create new intermediate maps. As recognized by the inventors, by applying IBR to an intermediate image representation in traditional rendering, it becomes possible to produce new algorithms that combine the strengths of both.","While various embodiments of the present invention have been described above, it should be understood that they have been presented by way of example only, and not limitation. It will be understood by those skilled in the art that various changes in form and details may be made therein without departing from the spirit and scope of the invention as defined in the appended claims. Thus, the breadth and scope of the present invention should not be limited by any of the above-described exemplary embodiments, but should be defined only in accordance with the following claims and their equivalents."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE FIGURES","p":["The accompanying drawings, which are incorporated herein and form part of the specification, illustrate the present invention and, together with the description, further serve to explain the principles of the invention and to enable a person skilled in the pertinent art to make and use the invention.",{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIGS. 1A and 1B"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":["FIG. 7","FIG. 1"]},{"@attributes":{"id":"p-0030","num":"0029"},"figref":["FIG. 8","FIG. 1"]},{"@attributes":{"id":"p-0031","num":"0030"},"figref":["FIG. 9","FIG. 1"]}]},"DETDESC":[{},{}]}
