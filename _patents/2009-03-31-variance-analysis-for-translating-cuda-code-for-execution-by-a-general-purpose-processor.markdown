---
title: Variance analysis for translating CUDA code for execution by a general purpose processor
abstract: One embodiment of the present invention sets forth a technique for translating application programs written using a parallel programming model for execution on multi-core graphics processing unit (GPU) for execution by general purpose central processing unit (CPU). Portions of the application program that rely on specific features of the multi-core GPU are converted by a translator for execution by a general purpose CPU. The application program is partitioned into regions of synchronization independent instructions. The instructions are classified as convergent or divergent and divergent memory references that are shared between regions are replicated. Thread loops are inserted to ensure correct sharing of memory between various threads during execution by the general purpose CPU.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08984498&OS=08984498&RS=08984498
owner: Nvidia Corporation
number: 08984498
owner_city: Santa Clara
owner_country: US
publication_date: 20090331
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION","Variance Analysis","Barrier Synchronization Partitioning","Performance Optimization","Loop Insertion"],"p":["This application claims benefit of U.S. provisional patent application Ser. No. 61\/043,708 filed Apr. 9, 2008, and titled \u201cSystem For Executing GPU-Accelerated Code on Multi-Core Architectures.\u201d The subject material of this related application is hereby incorporated herein by reference.","1. Field of the Invention","Embodiments of the present invention relate generally to compiler programs and more specifically to a translator that retargets an application program written for execution by a multi-core graphics processor for execution by a general purpose processor with shared memory.","2. Description of the Related Art","Modern graphics processing systems typically include a multi-core graphics processing unit (GPU) configured to execute applications in a multi-threaded manner. The graphics processing systems also include memory with portions that shared between the execution threads and dedicated to each thread.","NVIDIA's CUDA\u2122 (Compute Unified Device Architecture) technology provides a C language environment that enables programmers and developers to write software applications to solve complex computational problems such as video and audio encoding, modeling for oil and gas exploration, and medical imaging. The applications are configured for parallel execution by a multi-core GPU and typically rely on specific features of the multi-core GPU. Since the same specific features are not available in a general purpose central processing unit (CPU), a software application written using CUDA may not be portable to run on a general purpose CPU.","As the foregoing illustrates, what is needed in the art is a technique for enabling application programs written using a parallel programming model for execution on multi-core GPUs to run on general purpose CPUs without requiring the programmer to modify the application program.","One embodiment of the present invention sets forth a method for performing variance analysis of application program as part of translating the application program for execution by a general purpose processor. The method includes the steps of receiving the application program written using a parallel programming model for execution on a multi-core graphics processing unit and annotating each statement of the application program with a variance vector to produce a translated application program for execution by the general purpose processor. Each variance vector represents one or more cooperative thread array dimensions that correspond to a number of threads that are concurrently executed by a parallel processor within the multi-core graphics processing unit. The statements of the application program are traversed for each one of the cooperative thread array dimensions while the variance vector for each statement are updated to produce an annotated application program.","One advantage of the disclosed method is that application programs written using a parallel programming model for execution on multi-core GPUs are portable to general purpose CPUs without modification. Portions of the application that rely on specific features of the multi-core GPU are converted by a translator for execution by a general purpose CPU. The application program is partitioned into regions of synchronization independent instructions. The instructions are classified as convergent or divergent and divergent memory references that are shared between regions are replicated. Thread loops are inserted to ensure correct sharing of memory between various threads during execution by the general purpose CPU.","In the following description, numerous specific details are set forth to provide a more thorough understanding of the present invention. However, it will be apparent to one of skill in the art that the present invention may be practiced without one or more of these specific details. In other instances, well-known features have not been described in order to avoid obscuring the present invention.",{"@attributes":{"id":"p-0023","num":"0022"},"figref":["FIG. 1","FIG. 1"],"b":["100","100","102","104","105","105","106","107","107","108","102","106","105","112","105","113","112","110","114","107","116","107","118","120","121","107"]},"CPU  operates as the control processor of computer system , managing and coordinating the operation of other system components. In particular, CPU  issues commands that control the operation of parallel processors  within multithreaded processing subsystem . In some embodiments, CPU  writes a stream of commands for parallel processors  to a command buffer (not shown), which may reside in system memory , subsystem memory , or another storage location accessible to both CPU  and parallel processors . Parallel processors  read the command stream from the command buffer and execute commands asynchronously with respect to the operation of CPU .","System memory  includes an execution image of an operating system, a device driver , and CUDA code  that is configured for execution by multithreaded processing subsystem . CUDA code  incorporates programming instructions intended to execute on multithreaded processing subsystem . In the context of the present description, code refers to any computer code, instructions, and\/or functions that may be executed using a processor. For example, in various embodiments, the code may include C code, C++ code, etc. In one embodiment, the code may include a language extension of a computer language (e.g., an extension of C, C++, etc.).","The operating system provides the detailed instructions for managing and coordinating the operation of computer system . Device driver  provides detailed instructions for managing and coordinating operation of the multithreaded processing subsystem , and in particular parallel processors . Furthermore, device driver  may provide compilation facilities for generating machine code specifically optimized for parallel processors . Device driver  may be provided in conjunction with the CUDA\u2122 framework provided by NVIDIA Corporation.","In one embodiment, the multithreaded processing subsystem  incorporates one or more parallel processors  which may be implemented, for example, using one or more integrated circuit devices such as programmable processors, application specific integrated circuits (ASICs). Parallel processors  may include circuitry optimized for graphics and video processing, including, for example, video output circuitry, and a graphics processing unit (GPU). In another embodiment, the multithreaded processing subsystem  may be integrated with one or more other system elements, such as the memory bridge , CPU , and I\/O bridge  to form a system on chip (SoC). One or more parallel processors  may output data to display device  or each parallel processor  may output data to one or more display devices .","Parallel processors  advantageously implements a highly parallel processor that includes one or more processing cores, each of which is capable of executing a large number of threads concurrently where each thread is an instance of a program, such as code . Parallel processors  can be programmed to execute processing tasks relating to a wide variety of applications, including but not limited to, linear and nonlinear data transforms, filtering of video and\/or audio data, modeling operations (e.g., applying laws of physics to determine position, velocity and other attributes of objects), image rendering operations (e.g., tessellation shader, vertex shader, geometry shader, and\/or pixel shader programs), and so on. Parallel processors  may transfer data from system memory  and\/or local subsystem memory  into local (on-chip) memory, process the data, and write result data back to system memory  and\/or subsystem memory , where such data can be accessed by other system components, including CPU  or another multithreaded processing subsystem .","A parallel processor  may be provided with any amount of subsystem memory , including no subsystem memory , and may use subsystem memory  and system memory  in any combination. For instance, a parallel processor  can be a graphics processor in a unified memory architecture (UMA) embodiment. In such embodiments, little or no dedicated subsystem memory  would be provided, and parallel processor  would use system memory  exclusively or almost exclusively. In UMA embodiments, a parallel processor  may be integrated into a bridge chip or processor chip or provided as a discrete chip with a high-speed link (e.g., PCI-E) connecting the parallel processor  to system memory  via a bridge chip or other communication means.","As noted above, any number of parallel processors  can be included in a multithreaded processing subsystem . For instance, multiple parallel processors  can be provided on a single add-in card, or multiple add-in cards can be connected to communication path , or one or more parallel processors  can be integrated into a bridge chip. Where multiple parallel processors  are present, those parallel processors  may be operated in parallel to process data at a higher throughput than is possible with a single parallel processor . Systems incorporating one or more parallel processors  may be implemented in a variety of configurations and form factors, including desktop, laptop, or handheld personal computers, servers, workstations, game consoles, embedded systems, and the like.","In some embodiments of parallel processors , single-instruction, multiple-data (SIMD) instruction issue techniques are used to support parallel execution of a large number of threads without providing multiple independent instruction units. In other embodiments, single-instruction, multiple-thread (SIMT) techniques are used to support parallel execution of a large number of generally synchronized threads. Unlike a SIMD execution regime, where all processing engines typically execute identical instructions, SIMT execution allows different threads to more readily follow divergent execution paths through a given thread program. Persons skilled in the art will understand that a SIMD processing regime represents a functional subset of a SIMT processing regime. Functional units within parallel processors  support a variety of operations including integer and floating point arithmetic (e.g., addition and multiplication), comparison operations, Boolean operations (AND, OR, XOR), bit-shifting, and computation of various algebraic functions (e.g., planar interpolation, trigonometric, exponential, and logarithmic functions, etc.).","The series of instructions transmitted to a particular processing unit (not shown) within a processing core (not shown) of parallel processors  constitutes a thread, as previously defined herein, and the collection of a certain number of concurrently executing threads across the processing units within one processing core is referred to herein as a \u201cthread group.\u201d As used herein, a \u201cthread group\u201d refers to a group of threads executing the same program on different input data, with each thread of the group being assigned to a different processing unit in a processing core. A thread group may include fewer threads than the number of processing units, in which case some processing units will be idle during cycles when that thread group is being processed. A thread group may also include more threads than the number of processing units, in which case processing will take place over multiple clock cycles.","Since each processing core can support up to G thread groups concurrently, it follows that up to G\u00d7M thread groups can be executing in a processing core at any given time, where M is the number of processing cores in a parallel processor . Additionally, a plurality of related thread groups may be active (in different phases of execution) at the same time within a processing core. This collection of thread groups is referred to herein as a \u201ccooperative thread array\u201d (\u201cCTA\u201d). The size of a CTA is generally determined by the programmer and the amount of hardware resources, such as memory or registers, available to the CTA. The CUDA programming model reflects the system architecture of GPU accelerators. An exclusive local address space is available to each thread and a shared per-CTA address space is used to pass data between threads within a CTA. Processing cores also have access to off-chip \u201cglobal\u201d memory, which can include, e.g., subsystem memory  and\/or system memory .","A host portion of a CUDA application program is compiled using conventional methods and tools, while kernel functions specify CTA processing. At the highest level, the CUDA memory model separates the host and device memory spaces, such that host code and kernel code can only directly access their respective memory spaces. API (application programming interface) functions allow copying of data between the host and device memory spaces. In the shared-memory CPU execution of the CUDA programming model, a controlling CPU thread can executing in parallel with the parallel CTAs without potential data races. The host memory space is defined by the C programming language and the device memory spaces are specified as Global, constant, local, shared, and texture. All threads may access the global, constant, and texture memory spaces. As previously explained, access to the local space is limited to a single thread and access to the shared space is limited to threads in a CTA. This memory model encourages using small memory spaces for low-latency accesses, and encourages wise usage of large memory spaces which typically have longer latency.","A CUDA program, such as code  is typically organized as a set of synchronous or asynchronous executions of CTAs in one, two, or three dimensions, e.g., x, y, and z. A 3-tuple index uniquely identifies threads within a thread block. Thread blocks themselves are distinguished by an implicitly defined 2-tuple variable. The ranges of these indexes are defined at runtime and the runtime environment checks that the indexes conform to any hardware limitations. Each CTA may be executed by a parallel processor  in parallel with other CTAs. Many CTAs may run in parallel with each parallel processor  executing one or more CTAs. The runtime environment is responsible for managing the execution of CUDA code  synchronously or asynchronously as required. Threads within a CTA communicate and synchronize with each other by the use of shared memory and a barrier synchronization primitive called synchthreads( ). CUDA guarantees that threads within a thread block will be live simultaneously, and provides constructs for threads within a thread block to perform fast barrier synchronizations and local data sharing. Distinct thread blocks within a CTA (defined by one or more dimensions) have no ordering imposed on their creation, execution, or retirement. In addition, parallel CTAs are not allowed access to system calls, including I\/O. The CUDA programming model only enforces global synchronization between parallel CTAs, and provides intrinsic atomic operations for limited communication between blocks within a CTA.","The body of each thread, referred to as a kernel, is specified using CUDA which may be represented in standard C using memory model annotations and the barrier synchronization primitive. The semantics of a CUDA program is that each kernel is executed by all the threads in a CTA in an order that respects the memory ordering implied by the barrier synchronization primitive. In particular, all shared memory references within a CTA that occur before a barrier synchronization primitive must be completed before any shared memory references that occur after the barrier synchronization primitive.","Each instance of a barrier synchronization primitive in kernel code conceptually represents a separate logical barrier and should be treated as static. It is illegal to invoke a barrier synchronization primitive in both paths of an if-else construct when CUDA threads may take different branches of the construct. Although all threads within a thread block will reach one of the synchronization primitives, they represent separate barriers, each requiring that either all or none of the threads reach it. Therefore, such a kernel will not execute correctly. More generally, CUDA code is not guaranteed to execute correctly if a synchronization primitive is contained within any control flow construct that behaves differently for different threads within a thread block.",{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 2","b":["200","200","202","204","205","205","106","107","202","210"]},"Multithreaded processing subsystem  is not included in computer system  and CUDA code  is not adapted for execution by a general purpose processor, such as CPU . CUDA code , is adapted for execution by multithreaded processing subsystem , and is translated using translator  to produce translated code  that does not include the barrier synchronization primitive. In order for CPU  to run the program represented by code , code  must first be translated into code . The translated code may then be compiled by compiler  for execution by CPU . Compiler  may perform optimizations that are specific to CPU . Translating the code refers to converting the code written in a first computer language into a second computer language. Compiling the code refers to converting the code written in a computer language (e.g., a source code) into another computer language (e.g., an object code). Translator  is described in conjunction with  and compiler  is described in conjunction with . Compiler  may be included within a device driver  that is configured to interface between code , code  and CPU . A runtime environment  is configured to implement functions for the compiled code, e.g., input and output, memory management, and the like. Runtime environment  also launches compiled code for execution by CPU . Translator  performs optimizing transformations to serialize operations across the fine-grained threads of a CUDA thread group into a single CPU thread, while the runtime environment  schedules thread groups as work units for parallel processing by CPU .","The primary obstacle preventing portability of CUDA applications designed to run on GPUs for execution by general purpose CPUs is the granularity of parallelism. Conventional CPUs do not support the hundreds of hardware thread contexts required for a single CUDA CTA. Therefore, the primary goal of a system implementing the CUDA programming model on a general purpose CPU is to distribute the task-level parallelism to the available CPU cores. At the same time, the system must consolidate the microthreads within a task into a single CPU thread to prevent excessive scheduling overhead and frequent intercore synchronization.",{"@attributes":{"id":"p-0041","num":"0040"},"figref":["FIG. 3A","FIG. 3A"],"b":["101","112","201","202","220","101","220","101","202","201","101","112","225","202","201"]},"In step  translator  receives code  written for execution by a multi-core GPU, such as multithreaded processing subsystem  or a processor including one or more parallel processors , e.g., CUDA code . The code received in step  may be represented as a control flow graph consisting of basic block nodes connected by edges. Each basic block specifies the operations performed by target environment, e.g., CPU . The edges of the control flow graph represent jumps in the control flow. In step  translator  partitions CUDA code  around the barrier synchronization primitives to produce partitioned code. The partitioned code is shown in  and the partitioning process is described in conjunction with those figures. A synchronization partition is a region of code within which the ordering of operations is determined entirely by the control flow and data flow properties of the basic blocks within the partition. A partition has the property that a thread loop can be inserted around a partition to run the parallel threads. The control flow graph may be used to produce a synchronization partition control flow graph by replacing each synchthreads primitive with an edge, separating a basic block node into different partitions.","In step  the partitioned code is classified so that each statement is identified as either convergent or divergent. The partitioned code may include expressions and statements. An expression is a computation which may involve constants, implicit threadIDs, and named variables created by the programmer, but has no side-effects or assignments. A simple statement is defined as a computational expression resulting in a single assignment. A general statement can also represent a barrier, control flow conditional or loop construct, or a sequential block of statements. The CTA dimensions, x, y, and z are propagated through the code to determine whether each operation is dependent on one or more of the CTA dimensions. Operations that reference a threadID (thread identifier) in dimension x, y, and\/or z are considered divergent since a thread that references a CTA dimension may diverge from other threads in the same CTA during execution. For example, an operation that depends on threadID.x is divergent for the x dimension. Another operation that does not depend on threadID.x is convergent in the x dimension. Divergent statements require thread loops for each CTA dimension that they reference.","In step  the partitioned code is optimized for performance using the classification information to produce optimized code. For example, instructions within a partition may be reordered to fuse operations so that those operations with the same classification are grouped together and can fall within the same thread loop that is inserted in step . Operations are ordered such that those operations with fewer threadID dimensions in their variance vector precede the operations that are dependent on more threadID dimensions. This reordering is valid, as a statement must have a variance vector which is a superset of the variance vectors of the statements on which it depends. Thus statements with only one dimension in their variance vector cannot depend on any statement with a different dimension or more than one dimension in their variance vector.","In step  thread-local memory references in the optimized code are promoted to array references as needed, to ensure that each instance of an object has a unique location in which to store a value. In particular, data that is carried from one partition to another needs to be duplicated so that it is available in each partition. A variable that meets one of the following conditions is promoted to an array reference: local variable that has a cross partition dependency (assigned in one partition and referenced in another partition).","In step  translator  promotes thread-local memory references to array references. The program shown in TABLE 1 includes a synchronization barrier primitive and divergent references.",{"@attributes":{"id":"p-0047","num":"0046"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"203pt","align":"left"}}],"thead":{"row":[{"entry":"TABLE 1"},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"_global_ void function( ){"]},{"entry":[{},"\u2003int leftIndex, rightIndex;"]},{"entry":[{},"\u2003SharedMem[threadIdX.x] = ...; \/\/ store value into shared memory"]},{"entry":[{},"\u2003leftIndex = ...threadId.x...;"]},{"entry":[{},"\u2003rightIndex = ...threadId.x;"]},{"entry":[{},"\u2003_synchthreads( );"]},{"entry":[{},"\u2003= ...(SharedMem[leftIndex] + SharedMem[rightIndex])\/2.0;"]},{"entry":[{},"}"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},"The program shown in TABLE 1 is partitioned into a first partition before the synchthreads primitive and a second partition after the synchthreads primitive. The second partition includes references (leftindex and rightindex) that are computed in the first partition and depend from a CTA dimension. If the divergent references are not promoted, the second partition will incorrectly use the values computed by the last iteration of the first partition. The second partition should use the value computed for each corresponding iteration of threadId.x of the first partition. To ensure that the computation is correct, the divergent references are promoted as shown in TABLE 2.",{"@attributes":{"id":"p-0049","num":"0048"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"203pt","align":"left"}}],"thead":{"row":[{"entry":"TABLE 2"},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"void function( ) {"]},{"entry":[{},"\u2003..."]},{"entry":[{},"\u2003for (int tid_x = 0; tid_x < dimblock.X; tid_x++) {"]},{"entry":[{},"\u2003SharedMem[tid.x] = ...; \/\/ store value into shared memory"]},{"entry":[{},"\u2003leftIndexArray[tid_x] = ...threadId.x...;"]},{"entry":[{},"\u2003rightIndexArray[tid_x] = ...threadId.x;"]},{"entry":[{},"\u2003}"]},{"entry":[{},"\u2003for (int tid_x = 0; tid_x < dimblock.X; tid_x++) {"]},{"entry":[{},"\u2003= ...(SharedMem[leftIndexArray[tid_x]] +"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003\u2003SharedMem[rightIndexArray[tid_x]])\/2.0;"]},{"entry":[{},"\u2003}"]},{"entry":[{},"}"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},"In step  the thread loops are generated for those statements which contain threadID dimensions in their variance vectors. Adaptive loop nesting is used to simultaneously evaluate transformations equivalent to loop interchange, loop fission, and loop invariant removal to achieve the best redundancy removal. The nested loops are dynamically generated over values of each dimension of the threadID tuple to best suit the application, rather than assuming a particular loop nesting and evaluating the application based on that nesting. After the statements are ordered in step , loops may be generated for threadID dimensions only around those statements which contain that dimension in their variance vector. To remove loop overhead, translator  may fuse adjacent statement groups where one has a variance vector that is a subset of the other.",{"@attributes":{"id":"p-0051","num":"0050"},"figref":"FIG. 3B","b":["330","350","330","112","331","332","336","331","332","220","330","350","351","331","352","332","353","352","350","351","352","353","352"]},"In step  of , translator  inserts thread loops (such as thread loop ) into the optimized code in order to produce code  that is translated for execution by CPU . Each partition may have a thread loop inserted for each CTA dimension. An example of synchronization partitioning and thread loop insertion is shown in TABLES 3 and 4. A program shown in TABLE 3 is translated into the program shown in TABLE 4.",{"@attributes":{"id":"p-0053","num":"0052"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":[{"entry":"TABLE 3"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"_global_ void function( ){"},{"entry":"\u2003\u2003SharedMem[threadIDX.x] = ...; \/\/ store value into shared memory"},{"entry":"\u2003\u2003_synchthreads( );"},{"entry":"\u2003\u2003=...(SharedMem[threadIDX.x] + SharedMem[threadIdX.x\u22121])\/2.0;"},{"entry":"}"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}}}},"The program in TABLE 3 uses explicit synchronization to ensure correct sharing of memory between various threads in a CTA. Translator  partitions the program into two partitions, each of which is dependent on the x CTA dimension. Therefore, a thread loop is inserted around each of the two partitions to ensure that the translated program performs the operations in the correct order.",{"@attributes":{"id":"p-0055","num":"0054"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"203pt","align":"left"}}],"thead":{"row":[{"entry":"TABLE 4"},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Void function( ){"]},{"entry":[{},"\u2003\u2003for (int tid_x = 0; tid_x < dimblock.X; tid_x++) {"]},{"entry":[{},"\u2003\u2003\u2003SharedMem[tid_x] = ...; \/\/store value into shared memory"]},{"entry":[{},"\u2003\u2003}"]},{"entry":[{},"\u2003\u2003for (int tid_x = 0; tid_x < dimblock.X; tid_x++) {"]},{"entry":[{},"\u2003\u2003\u2003= ...(SharedMem[tid_x] + SharedMem[tid_x \u2212 1])\/2.0;"]},{"entry":[{},"\u2003\u2003}"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},"A simpler technique for translating a program for execution by a general purpose processor inserts explicit thread loops for each CTA dimension, so that it is not necessary to determine the dimension dependency for references within the same partition. For example, the program shown in TABLE 5 is translated into the program shown in TABLE 6. Note that one or more of the thread loops inserted in TABLE 5 may be unnecessary since the program was produced without determining the dimension dependency.",{"@attributes":{"id":"p-0057","num":"0056"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"154pt","align":"left"}}],"thead":{"row":[{"entry":"TABLE 5"},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"_global_ void function( ){"]},{"entry":[{},"\u2003Shared1 = ..."]},{"entry":[{},"\u2003= Shared1"]},{"entry":[{},"}"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},{"@attributes":{"id":"p-0058","num":"0057"},"tables":{"@attributes":{"id":"TABLE-US-00006","num":"00006"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"203pt","align":"left"}}],"thead":{"row":[{"entry":"TABLE 6"},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"void function( ){"]},{"entry":[{},"\u2003for (int tid_x = 0; tid_x < dimblock.X; tid_x++) {"]},{"entry":[{},"\u2003\u2003for (int tid_y = 0; tid_y < dimblock.Y; tid_y++) {"]},{"entry":[{},"\u2003\u2003\u2003\u2003for (int tid_z = 0; tid_z < dimblock.Z; tid_z++) {"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003\u2003Shared1 = ..."]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003\u2003= Shared1"]},{"entry":[{},"\u2003\u2003\u2003\u2003}"]},{"entry":[{},"\u2003\u2003\u2003}"]},{"entry":[{},"\u2003\u2003}"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},{"@attributes":{"id":"p-0059","num":"0058"},"figref":"FIG. 3C","b":["333","360","333","112","334","338","335","334","338","220","333","360","361","334","362","364","365","338"]},"Partition  includes a first portion of instructions that are divergent in a first CTA dimension. Partition  includes a second portion of instructions that are convergent. Partition  includes a third portion of instructions that are divergent in a second CTA dimension. A thread loop  is inserted around partition  to ensure that the synchronization semantic is maintained when partitioned code  is executed by a general purpose processor that does not natively support the synchronization barrier instruction. Thread loop  iterates over the first CTA dimension. A thread loop  is inserted around partition  to iterate over the second CTA dimension.",{"@attributes":{"id":"p-0061","num":"0060"},"figref":"FIG. 4","b":["201","202","400","225","201","405","202","203","201","410","227","203","202","201"]},"Runtime environment  may create a number of operating system (OS) runtime threads, which can be controlled by an environment variable. Be default, the number of cores in the system may be used as the number of OS runtime threads. In step , the number of CUDA threads to be launched may be evaluated and statistically partitioned to the number of runtime threads. Each runtime thread executes a portion of the compiled code sequentially and waits on a barrier. When all runtime threads reach the barrier, the CTA has completed. In step  runtime environment  or device driver  launches the compiled code for execution by CPU .","In order to compile a CUDA program for execution on a processor that does not support direct parallel execution of threads in a CTA, a thread (or vector) loop is generated around program statements that depend on one or more threadIDs, thread dimensions x, y, and z. Variance analysis determines the minimal set of statements that must be considered for thread loop execution. The result of the variance analysis is used to promote thread local memory references to array references in step  of  and to insert thread loops in step  of . Each statement of the CUDA program is annotated for each threadID dimension, e.g., x, y, and z. The annotations are referred to as variance vectors. Implicitly, atomic intrinsics are considered as a \u201cuse\u201d of each dimension of the thread index, as their return value could vary for each CUDA thread.",{"@attributes":{"id":"p-0064","num":"0063"},"figref":["FIG. 5A","FIG. 3A"],"b":["305","500","505"]},{"@attributes":{"id":"p-0065","num":"0064"},"tables":{"@attributes":{"id":"TABLE-US-00007","num":"00007"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"189pt","align":"left"}}],"thead":{"row":[{"entry":"TABLE 7"},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"For each statement node do {"]},{"entry":[{},"\u2003if statement node loads from thread i then"]},{"entry":[{},"\u2003\u2003variance vector(node) += {i};"]},{"entry":[{},"\u2003\u2003worklist += {node};"]},{"entry":[{},"\u2003else if statement node is an atomic intrinsic then"]},{"entry":[{},"\u2003\u2003variance vector(node) += {x, y, z};"]},{"entry":[{},"\u2003\u2003worklist += {node};"]},{"entry":[{},"\u2003else"]},{"entry":[{},"\u2003\u2003variance vector (node) = { };"]},{"entry":[{},"\u2003endif"]},{"entry":[{},"}"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}},"br":{}},"In step  translator  determines if the worklist is empty, and, if so, then the translator proceeds to step  of . Otherwise, in step  translator  removes a node from the worklist. In step  translator  propagates the threadID dimensions as a variance vector through the program. When the variance vector propagation is complete, translator  returns to step  to determine if the variance analysis is complete. The variance analysis is complete when, in step , the worklist is empty. Translator  iterates through every element of the worklist, traversing the data flow and control dependence successors, while updating the variance vectors of each traversed node. When the variance vector of a traversed node changes in step , the node is added to the worklist. The pseudocode shown in TABLE 8 may be used to perform steps , , and , where vvector(n) is the variance vector of node n and the dataflow analysis may be represented by definition-use (def-use) chains. A def-use chain represents a variable and all the uses of that variable that are reachable from the definition, without any other intervening definitions.",{"@attributes":{"id":"p-0067","num":"0066"},"tables":{"@attributes":{"id":"TABLE-US-00008","num":"00008"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"203pt","align":"left"}}],"thead":{"row":[{"entry":"TABLE 8"},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"while (worklist is not empty) do {"]},{"entry":[{},"\u2003pick a node n from the worklist;"]},{"entry":[{},"\u2003worklist \u2212= {n};"]},{"entry":[{},"\u2003if n is an expression in a statement s then"]},{"entry":[{},"\u2003\u2003if merge(vvector(n), vvector(s)) != vvector(s) then"]},{"entry":[{},"\u2003\u2003\u2003\u2003vvector(s) = merge(vvector(n), vvector(s));"]},{"entry":[{},"\u2003\u2003\u2003\u2003worklist += {s};"]},{"entry":[{},"\u2003\u2003endif"]},{"entry":[{},"\u2003endif"]},{"entry":[{},"\u2003if n is statement then"]},{"entry":[{},"\u2003\u2003for each s reached by n in the def-use chains do"]},{"entry":[{},"\u2003\u2003\u2003\u2003if merge(vvecto(n), vvector(s)) != vvector(s) then"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003\u2003vvector(s) = merge(vvector(n), vvector(s));"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003\u2003worklist += {s};"]},{"entry":[{},"\u2003\u2003\u2003\u2003endif"]},{"entry":[{},"\u2003\u2003endfor"]},{"entry":[{},"\u2003endif"]},{"entry":[{},"\u2003if n is an expression in the condition of an if statement then"]},{"entry":[{},"\u2003\u2003for each s in the then and the else part of the if statement do"]},{"entry":[{},"\u2003\u2003\u2003\u2003if merge(vvector(n), vvector(s)) != vvector(s) then"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003\u2003vvector(s) = merge(vvector(n), vvector(s));"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003\u2003worklist += {s};"]},{"entry":[{},"\u2003\u2003\u2003\u2003endif"]},{"entry":[{},"\u2003\u2003endfor"]},{"entry":[{},"\u2003endif"]},{"entry":[{},"\u2003if n is an expression in the condition of a while loop then"]},{"entry":[{},"\u2003\u2003for each s in the body of the while loop do"]},{"entry":[{},"\u2003\u2003\u2003\u2003if merge(vvector(n), vvector(s)) != vvector(s) then"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003\u2003vvector(s) = merge(vvector(n), vvector(s));"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003\u2003worklist += {s};"]},{"entry":[{},"\u2003\u2003\u2003\u2003endif"]},{"entry":[{},"\u2003\u2003endfor"]},{"entry":[{},"\u2003endif"]},{"entry":[{},"\u2003if n is an expression in the condition of a do loop then"]},{"entry":[{},"\u2003\u2003for each s in the increment and the body of the do loop do"]},{"entry":[{},"\u2003\u2003\u2003\u2003if merge(vvector(n), vvector(s)) != vvector(s) then"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003\u2003vvector(s) = merge(vvector(n), vvector(s));"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003\u2003worklist += {s};"]},{"entry":[{},"\u2003\u2003\u2003\u2003endif"]},{"entry":[{},"\u2003\u2003endfor"]},{"entry":[{},"\u2003endif"]},{"entry":[{},"endwhile"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},"Control dependence is used to propagate the threadID dimension dependencies. In the program shown in TABLE 9 the variable i is a function of threadID, after the loop terminates. Since j is always 1 more than i, j also depends on the threadID. The dependence of j on threadID dimension x is accomplished by marking every statement in the body of the loop as dependent on threadID.",{"@attributes":{"id":"p-0069","num":"0068"},"tables":{"@attributes":{"id":"TABLE-US-00009","num":"00009"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"147pt","align":"left"}}],"thead":{"row":[{"entry":"TABLE 9"},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"k = threadId.x \u22121;"]},{"entry":[{},"i = 0"]},{"entry":[{},"j = 1;"]},{"entry":[{},"while (i < threadID.x) {"]},{"entry":[{},"j = j+1;"]},{"entry":[{},"S;"]},{"entry":[{},"i = i+1;"]},{"entry":[{},"}"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},"Generating code for partitions after variance analysis can remove some redundant computation, analogous to loop invariant removal. Each node is identified as dependent on specific dimensions of the threadID value. Therefore, a node without a particular dimension in its variance vector does not need to be contained within a loop iterating through the range of values of the threadID dimension.","In the simplest case, and perhaps the most common, a programmer may intend to only use a subset of the dimensions of the threadID tuple to distinguish threads from each other, implicitly assuming that all of the other dimensions will have a constant value of 1 for all CUDA threads. In this case, the programmer writes a kernel never using one or more dimensions of the threadID except to distinguish the threads from each other. In this case, the variance analysis will discover that the implicitly constant value is never used, and will not have to generate a loop for that dimension of the threadID for any partition.","In order to execute all CUDA threads within a thread group using a single CPU thread, ordering of the computation of the CUDA threads is serialized and scheduled to maintain the semantics of barrier synchronization. A single program multiple data (SPMD) parallelism program that includes synchronization barriers and regular control flow is divided into partitions that include groups of one or more statements without barrier synchronization. Each statement is examined in sequence to determine whether or not it includes a barrier statement. If not, it is included in the current partition. Otherwise, the statement is a barrier statement and therefore defines a partition boundary between the current partition and a new partition. When the statement is a control-flow construct that contains a barrier, then the beginning of the construct is a partition boundary. The internal structure of the control-flow construct is then recursively analyzed to partition the statements within the control-flow construct. Another condition that creates a new partition is the reconvergence point (immediate post-dominator) of a branch where at least two paths of control flow meet, each of which originated from a separate partition. The code beginning with the point of reconvergence is either replicated and appended to each potentially preceding partition or a partition boundary is created to form a new partition.",{"@attributes":{"id":"p-0073","num":"0072"},"figref":["FIG. 5B","FIG. 3A"],"b":["310","530","220","532","220","532"]},"In step  translator  determines if the statement is a barrier synchronization primitive, and, if not, translator proceeds to step . Otherwise, in step  translator  adds the current partition to the output list. In step  translator  begins a new partition since the barrier synchronization primitive defines a partition boundary. In step  translator  determines if the end of a control-flow sequence is reached. If the end of a control-flow sequence is reached, then translator proceeds to step  and determines if another statement exists in the code, and, if not, translator  proceeds to step  and the partitioning process is complete. If, in step  translator  determines that another statement does exist in the code, then translator  returns to step  to continue partitioning with the next statement in the code or control-flow sequence.","If, in step  translator  determines that the end of a control-flow sequence is reached, then translator  proceeds to step  to partition the control-flow construct that includes a barrier. After step  is completed, translator  returns to step  to continue processing the code following the control-flow sequence. The details of step  are described in conjunction with .","If in step  the statement is not a barrier synchronization primitive, then in step  translator  determines if the statement is a simple statement, i.e, not a control-flow construct. If the statement is a simple statement, then in step  translator  adds the statement to the current partition and proceeds to step . Otherwise, in step  translator  determines if the statement is the start of a control-flow construct, and, if not, the statement indicates the start of a sequential block of statements and translator  returns to step  to recursively partition the sequential block of statements. If, in step  translator  determines that the statement represents the start of a control-flow construct, then in step  translator  determines if the control-flow construct, includes a barrier primitive. If the control-flow construct does not include a barrier primitive, then translator proceeds to step  to add the control flow construct to the current partition. Otherwise, translator  proceeds to step  to complete partitioning of the control-flow construct.",{"@attributes":{"id":"p-0077","num":"0076"},"figref":["FIG. 5C","FIG. 5B"],"b":["560","562","220","565","220","2","1","2","1","2","568","220","576","220","1","220","532","1","532","560"]},"If, in step  translator  determines that the statement is an \u201cif\u201d statement, then in step  translator  temporarily defines L as the output list so that the statements in the current partition are collected in L and specifies the \u201cthen\u201d portion of the control-flow construct as the control-flow sequence before proceeding to step  to partition the \u201cthen\u201d portion of the control-flow construct and produce the output list L.","When step  is reached, the then portion of the control-flow construct is complete, and in step  translator  determines if an output list L should be produced. L is produced by partitioning the \u201celse\u201d portion of a control-flow sequence. When translator  determines that the output list L should be produced, in step  translator  sets the output list to L and specifies the \u201celse\u201d portion of the control-flow construct as the control-flow sequence. Translator then proceeds to step  to produce the output list L. If, in step  translator  determines that the output list L has been produced or that L is not needed, then partitioning of the control-flow construct is complete, and in step  translator  sets the output list to the output list of partitions. In step  translator  appends L and L to the output list (output list of partitions) and then returns to step  to start a new partition.","The result of the partitioning process is the output list of partitions that is a list of code partitions that are free of barriers. The partitioning technique shown in  is optimal in that, for structured code, the partitioning technique produces the smallest number of distinct partitions possible, allowing translator  to produce translated code while minimizing replication of regions of code. Translator  may then completely serialize the statements executed by each CUDA thread in the partition, or choose any other interleaving desirable to remove redundancy or for any other beneficial reason.","When translating CUDA programs to run on general purpose processors, such as CPU , the code is transformed so that the local program state that crosses synchronization barriers is scalar expanded (or vectorized). A na\u00efve method would scalar expand every local datum and transform every reference to use the vector reference. Instead, it is possible to optimize the program to use vector form where essential and use scalar form where appropriate while respecting the original CUDA program semantics. This approach yields translated code that executes more efficiently on a general purpose processor.","In order to optimize the performance of the CUDA program, the CUDA program should be partitioned, the variance analysis should completed to classify each program statement as a vector or scalar statement, and data flow information describing dependency information between program statements should be available. The data flow information may be represented in static single assignment (SSA) form, def-use chains, or the like.","When a CUDA program is serialized, the data that is conceptually private to each thread does not necessarily need to be stored in separate memory locations. In particular, values which have a live range completely contained within a partition can potentially avoid replication. Replication is performed by promoting a variable from a scalar variable to a vector variable. Two cases arise in which variable replication should be applied to the output value of an assignment with a non-empty variance vector. The first case is when a variable is used in another partition. The second case is when a variable is used in a different way when a loop over thread indexes is placed around the partition. Thread-local memory assignments which have an empty variance vector, technically never need to write to a replicated location. However, a use with some unreplicated and some replicated definitions reaching it would require a runtime test to determine whether to read the replicated or unreplicated value, depending on the path taken to reach that use. Alternatively, having all definitions reaching a vector variable, write to the vector variable eliminates the need for this additional complexity of a runtime test. Therefore, for any use that is reachable by at least one replicated definition, all its potential definitions must write to the replicated location.",{"@attributes":{"id":"p-0084","num":"0083"},"figref":["FIG. 6","FIG. 3A"],"b":["320","602","220"]},"In step  translator  identifies store and load operations that need vector references. Vector statements that update local variables are traversed. Uses of the local variable are analyzed to determine which partition(s) those uses reside in. Candidate vector statements (statements that modify local state) are traversed in each partition and the load and store operations of the target local variables are classified based on whether or not all uses of the target variable are within a single partition. In step  translator  determines if all uses of a target local variable are within the single partition, and, if so, in step  the store of the target local variable is demoted to a scalar store and all loads or uses of the target local variable remain as scalar loads.","If, in step  translator  determines that all uses of a target local variable are not within the single partition, then in step  the store is changed to write to a vector store. In step  any loads that are in a different partition are promoted to vector loads. Loads that are in the partition remain as scalar loads. This ensures that scalar loads in the same partition read from the original location, while vector loads read from a vector location.","In step  translator  determines if another load or store was identified in step , and, if not, the local variable traversal for vector promotion is complete and translator  proceeds to step . Otherwise, translator  returns to step  to traverse another local variable. In step  code is generated to allocate appropriately dimensioned vectors for the vector stores at the beginning of the translated program. In step  code is generated for all loads requiring access to the vector data. The following code sequence is inserted before the vector load, where X is the local variable.",{"@attributes":{"id":"p-0088","num":"0087"},"tables":{"@attributes":{"id":"TABLE-US-00010","num":"00010"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"161pt","align":"left"}}],"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"X = LOAD Vector(X)[threadID]"]},{"entry":[{},"Load X"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}},"br":{},"b":"622"},{"@attributes":{"id":"p-0089","num":"0088"},"tables":{"@attributes":{"id":"TABLE-US-00011","num":"00011"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"161pt","align":"left"}}],"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"STORE X, VALUE"]},{"entry":[{},"STORE Vector(X)[threadID], X"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},"Finally, in step  of , the thread loops are inserted into the code  by translator  to complete the translation of CUDA code, producing code  for execution by a general purpose processor. Translator  generates loops over thread index dimensions only around those statements which contain that dimension in their variance vector. To remove loop overhead, translator  may fuse adjacent statement groups where one has a variance vector that is a subset of the other. Translator  may use cost analysis techniques to evaluate and perform loop fusion operations. Minimal variable replication and adaptive loop nesting share an interesting interplay in that the maximal fusing of loops can introduce additional cases requiring replication.","TABLE 10 shows an example CUDA kernel and TABLE 11 shows the translation of the CUDA kernel for execution by a general purpose processor. The example kernel multiplies a list of small matrices. Each thread block computes one small matrix multiplication out of the list, while each thread computes one element of the result matrix for its block.",{"@attributes":{"id":"p-0092","num":"0091"},"tables":{"@attributes":{"id":"TABLE-US-00012","num":"00012"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":"TABLE 10"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Example CUDA kernel"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"(1) _global_ small_mm_list(float* A_list, float* B_list, , const"},{"entry":"int size)"},{"entry":"{"},{"entry":"(2) \u2003\u2003float sum;"},{"entry":"(3) \u2003\u2003int matrix_start, col, row, out_index, i;"},{"entry":"(4) \u2003\u2003martrix_start = blockIdx.x*size*size;"},{"entry":"(5) \u2003\u2003col = matrix_start + threadIDx.x;"},{"entry":"(6) \u2003\u2003row = matrix_start + threadIdx.y * size);"},{"entry":"(7) \u2003\u2003sum = 0.0;"},{"entry":"(8) \u2003\u2003for (i = 0; i < size; i++)"},{"entry":"(9) \u2003\u2003sum += A_list[row + i] * B_list[col + (i*size)];"},{"entry":"\u2003\u2003\u2003 \/\/ Synchronize before overwriting input data"},{"entry":"(10) \u2003\u2002_syncthread( );"},{"entry":"(11) \u2003\u2002out_index = matrix_start + (threadIdx.y * size) + threadIdx.x;"},{"entry":"(12) \u2003\u2002A_list[out_index] = sum;"},{"entry":"}"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"Note that the statement at line (9) of TABLE 10 has a variance vector of (x,y) since col is dependent on the x dimension and row is dependent on the y dimension. The z dimension is never used, so no loop is inserted that iterates over z. Typical cost analysis techniques may be used to determine cases such as statements 5 and 6 in the example kernel shown in TABLE 10. As each is only dependent on one threadID dimension, choosing either nesting order of the x and y index loops will force either redundant execution of a statement, or a redundant loop outside the main loop nest of the partition.",{"@attributes":{"id":"p-0094","num":"0093"},"tables":{"@attributes":{"id":"TABLE-US-00013","num":"00013"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":"TABLE 11"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Translated CUDA kernel"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"(1) _global_ small_mm_list(float* A_list, float* B_list, , const int"},{"entry":"size)"},{"entry":"{"},{"entry":"(2) \u2003float sum[ ];"},{"entry":"(3) \u2003int matrix_start[ ], col[ ], row[ ], out_index, i;"},{"entry":"(4) \u2003\u2003matrix_start[threadID] = blockIDx.x*size*size;"},{"entry":"\u2003 \u2003\u2003for(threadID.x = 0; threadID.x < blockDim.x; threadID.x++) {"},{"entry":"(5) \u2003\u2003\u2003col[threadID] = matrix_start + threadIDx.x;"},{"entry":"\u2003 \u2003\u2003\u2003for(threadID.y = 0; threadID.y < blockDim.y; threadID.y++) {"},{"entry":"(6) \u2003\u2003\u2003\u2003row[threadID] = matrix_start[threadID] + (threadIDx.y *"},{"entry":"\u2003 \u2003\u2003\u2003\u2003size);"},{"entry":"(7) \u2003\u2003\u2003\u2003sum[threadID] = 0.0;"},{"entry":"(8) \u2003\u2003\u2003\u2003for (i[threadID] = 0; i < size; i++)"},{"entry":"(9) \u2003\u2003\u2003\u2003sum[threadID] += A_list[row[threadID] + i] *"},{"entry":"\u2003 \u2003\u2003\u2003\u2003\u2003\u2003B_list[col[threadID] + (i*size)];"},{"entry":"\u2003 \u2003\u2003\u2003}"},{"entry":"\u2003 \u2003\u2003}"},{"entry":"(10)"},{"entry":"\u2003 \u2003\u2003for (threadID.x = 0; threadID.x < blockDim.x; threadID.x++) {"},{"entry":"\u2003 \u2003\u2003\u2003for (threadID.y = 0; threadID.y < blockDim.y; threadID.y++) {"},{"entry":"(11) \u2003\u2003\u2003\u2002out_index = matrix_start[threadID] +"},{"entry":"\u2003 \u2003\u2003\u2003\u2003\u2003\u2003(threadID.y * size) + threadID.x;"},{"entry":"(12) \u2003\u2003\u2002A_list[out_index] = sum[threadID];"},{"entry":"\u2003 \u2003\u2003}"},{"entry":"}"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"Translator , compiler , and runtime environment  are used to convert CUDA application programs into code for execution by a generator purpose CPU. The CUDA programming model supports bulk synchronous task parallelism, where each task is composed of fine-grained SPMD threads. Use of the CUDA programming model has been limited to programmers willing to write specialized code for execution by GPUs. This specialized code may be converted for execution by a general purpose CPU without requiring the programmer to rewrite the CUDA application program. The three key abstractions supported by CUDA are SPMD thread blocks, barrier synchronization, and shared memory. Translator  serializes operations across the fine-grained threads of a CUDA thread block into a single CPU thread and performs optimizing transformations to convert a CUDA application program.","While the foregoing is directed to embodiments of the present invention, other and further embodiments of the invention may be devised without departing from the basic scope thereof. For example, aspects of the present invention may be implemented in hardware or software or in a combination of hardware and software. One embodiment of the invention may be implemented as a program product for use with a computer system. The program(s) of the program product define functions of the embodiments (including the methods described herein) and can be contained on a variety of computer-readable storage media. Illustrative computer-readable storage media include, but are not limited to: (i) non-writable storage media (e.g., read-only memory devices within a computer such as CD-ROM disks readable by a CD-ROM drive, flash memory, ROM chips or any type of solid-state non-volatile semiconductor memory) on which information is permanently stored; and (ii) writable storage media (e.g., floppy disks within a diskette drive or hard-disk drive or any type of solid-state random-access semiconductor memory) on which alterable information is stored. Such computer-readable storage media, when carrying computer-readable instructions that direct the functions of the present invention, are embodiments of the present invention. Therefore, the scope of the present invention is determined by the claims that follow."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["So that the manner in which the above recited features of the present invention can be understood in detail, a more particular description of the invention, briefly summarized above, may be had by reference to embodiments, some of which are illustrated in the appended drawings. It is to be noted, however, that the appended drawings illustrate only typical embodiments of this invention and are therefore not to be considered limiting of its scope, for the invention may admit to other equally effective embodiments.",{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 3A"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 3B"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 3C"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":["FIG. 5A","FIG. 3A"]},{"@attributes":{"id":"p-0019","num":"0018"},"figref":["FIG. 5B","FIG. 3A"]},{"@attributes":{"id":"p-0020","num":"0019"},"figref":["FIG. 5C","FIG. 5B"]},{"@attributes":{"id":"p-0021","num":"0020"},"figref":["FIG. 6","FIG. 3A"]}]},"DETDESC":[{},{}]}
