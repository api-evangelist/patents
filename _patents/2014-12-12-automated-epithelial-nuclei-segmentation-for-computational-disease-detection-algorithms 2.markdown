---
title: Automated epithelial nuclei segmentation for computational disease detection algorithms
abstract: In aspects, the subject innovation can comprise systems and methods capable of automatically labeling cell nuclei (e.g., epithelial nuclei) in tissue images containing multiple cell types. The enhancements to standard nuclei segmentation algorithms of the subject innovation can enable cell type specific analysis of nuclei, which has recently been shown to reveal novel disease biomarkers and improve diagnostic accuracy of computational disease classification models.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09626583&OS=09626583&RS=09626583
owner: University of Pittsburgâ€”Of the Commonwealth System of Higher Education
number: 09626583
owner_city: Pittsburgh
owner_country: US
publication_date: 20141212
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS REFERENCE TO RELATED APPLICATIONS","NOTICE ON GOVERNMENT FUNDING","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["This application claims the benefit of U.S. Provisional Patent application Ser. No. 61\/915,234 entitled Automated Epithelial Nuclei Segmentation for Computation Disease Detection Algorithms and filed on Dec. 12, 2013, the entirety of which is incorporated by reference herein.","This invention was made with government support under grant #CA164433 awarded by the National Institutes of Health. The government has certain rights in the invention.","Computational models for disease diagnosis and prognosis applied in a clinical setting can provide unbiased reasoning to assist diagnosis of ambiguous cases, save time by filtering out obvious cases, and help establish degree of disease risk for individual patients. A key component of computational models is identification of nuclei in cell images, on which biomarkers can be measured and related to disease risk. While pathologists have traditionally analyzed nuclei from different cell types according to different criteria, and recent computational findings have uncovered the diagnostic strength of certain cell classes, few automated algorithms exist for categorizing nuclei according to cell-type.","The following presents a simplified summary of the innovation in order to provide a basic understanding of some aspects of the innovation. This summary is not an extensive overview of the innovation. It is not intended to identify key\/critical elements of the innovation or to delineate the scope of the innovation. Its sole purpose is to present some concepts of the innovation in a simplified form as a prelude to the more detailed description that is presented later.","In aspects, the subject innovation can comprise systems and methods capable of automatically labeling cell nuclei (e.g., epithelial nuclei) in tissue images containing multiple cell types. The enhancements to standard nuclei segmentation algorithms of the subject innovation can enable cell type specific analysis of nuclei, which has recently been shown to reveal novel disease biomarkers and improve diagnostic accuracy of computational disease classification models.","In various embodiments, the subject innovation can comprise a method that facilitates automatic labeling of cell nuclei. Such a method may include the acts of receiving an image comprising a plurality of cells and fitting a Gaussian mixture model to an intensity distribution of the image. The model can comprise one or more Gaussian distributions and a background distribution. The model can further include the acts of identifying a first Gaussian component Gg associated with a first Gaussian distribution of the one or more Gaussian distributions corresponding to nuclei of the plurality of cells and defining a nuclei mask as a binary matrix based at least in part on the first Gaussian component Gg. In one embodiment, the nuclei mask can comprise one or more putative nuclei associated with the plurality of cells. Embodiment of the method may include cleaning the nuclei mask, contrast normalizing the nuclei mask, breaking one or more large regions of the nuclei mask into individual nuclei, removing one or more very small regions from the nuclei mask, and expanding at least one of the one or more putative nuclei via a watershed.","To the accomplishment of the foregoing and related ends, certain illustrative aspects of the innovation are described herein in connection with the following description and the annexed drawings. These aspects are indicative, however, of but a few of the various ways in which the principles of the innovation can be employed and the subject innovation is intended to include all such aspects and their equivalents. Other advantages and novel features of the innovation will become apparent from the following detailed description of the innovation when considered in conjunction with the drawings.","The innovation is now described with reference to the drawings, wherein like reference numerals are used to refer to like elements throughout. In the following description, for purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of the subject innovation. It may be evident, however, that the innovation can be practiced without these specific details. In other instances, well-known structures and devices are shown in block diagram form in order to facilitate describing the innovation.","As used in this application, the terms \u201ccomponent,\u201d \u201cmodule,\u201d \u201csystem,\u201d \u201cinterface,\u201d and the like are generally intended to refer to a computer-related entity, either hardware, a combination of hardware and software, software, or software in execution. For example, a component may be, but is not limited to being, a process running on a processor, a processor, an object, an executable, a thread of execution, a program, or a computer. By way of illustration, both an application running on a controller and the controller can be a component. One or more components residing within a process or thread of execution and a component may be localized on one computer or distributed between two or more computers.","Furthermore, the claimed subject matter can be implemented as a method, apparatus, or article of manufacture using standard programming or engineering techniques to produce software, firmware, hardware, or any combination thereof to control a computer to implement the disclosed subject matter. The term \u201carticle of manufacture\u201d as used herein is intended to encompass a computer program accessible from any computer-readable device, carrier, or media. Of course, many modifications may be made to this configuration without departing from the scope or spirit of the claimed subject matter.","Various embodiments of the subject innovation can employ an automated algorithm which can identify epithelial nuclei in esophageal biopsy sample images. Predictions of image pixels belonging to nuclei using the novel nuclei segmentation algorithm of the subject innovation can be employed in connection with esophageal tissue in various embodiments. Next, an optimal labeling of all image pixels as belonging to epithelial nuclei, other nuclei, or background, is discussed wherein each tissue image is modeled as a Markov Random Field (MRF) on putative nuclei. The MRF can incorporate context-based features describing epithelial nuclei. The features disclosed herein were selected under pathologists guidance and parameterized on the training data set. The techniques of the subject innovation was able to identify 97% of nuclei within the data set, and correctly labeled over 90% of those nuclei as epithelial or non-epithelial. The automated epithelial nuclei detector produces a higher quality selection of epithelial nuclei than manual detection, resulting in more accurate biomarker quantification. To establish its utility in a clinical setting, a nano-scale biomarker measured on predicted epithelial nuclei is shown in results discussed herein to vary significantly between Barrett's esophagus patients with three grades of precancerous changes. The ability to measure cell-type specific biomarkers gained through this algorithm can aid identification of novel disease biomarkers and strengthen existing algorithms for computational disease detection.","Methods","Epithelial segmentation according to aspects of the subject innovation can proceed in two phases. In phase one, putative nuclei can be identified in the image. The nuclei method as used for an example data set is outlined below. This nuclei method was designed to obtain accurate nuclei with respect to ground-truth nuclei boundaries. This nuclei method does not seek to minimize the number of false positives (tissue regions mistaken for nuclei), but instead tries to maximize the number of true positives, as the epithelial classification algorithm in phase two is able to identify most false positives, but can suffer when epithelial nuclei are missing from epithelial chains, making global information incorrect. In phase two, nuclei can be labeled as belonging to epithelial or non-epithelial cells using a conditional Markov random field (MRF).","Phase I: Nuclei Segmentation","Phase one can involve a nuclei segmentation method that can identify putative nuclei in stained tissue images. While different image sets\/techniques (e.g., staining, magnification, cell-type, etc.) may require different parameters or perhaps additional steps, the basic method can accurately identify nuclei in several tissue image data sets. Consider segmenting the nuclei shown in the top left image (labeled I) of , which illustrates image denoising and nuclei segmentation in accordance with aspects of the subject innovation. While many nuclei in the image labeled I can be easily identified as black circles, the nuclei on the top left are tightly clustered and hard to resolve, and some nuclei near the bottom right have weaker intensities than the majority of the nuclei. Additionally, there are several dark regions in the image that could be mistaken for nuclei, while they are actually simply variations in cytoplasm\/lumen intensity. Some nuclei can also have intensity variations, causing oversegmentation of the nuclei into several smaller nuclei. These intensity variations can have biological explanations, such as chromosome location within nuclei, but can sometimes also be due to equipment\/experimental error.","First, to reduce intensity variations in cytoplasm regions that can be mistaken for nuclei and variations in nuclei regions leading to oversegmentation, the image I is denoised using total-variance denoising with a range of smoothing factors (\u03bb) to form the denoised image I (e.g., the images in  labeled I with \u03bb=100 and 300). Total variation denoising can minimize the total variation with respect to the true signal xand the observed signal yat pixel i, |x\u2212y|, such that the true values of neighboring pixels are close, where the distance between true values of neighboring pixels i and j is given by (x\u2212x). The smoothing factor \u03bb can control how much weight is given to the total variation term, that is, how much more or less important is the variation of the true signal from the observed signal than the closeness of the true values of neighboring pixels. The denoised solution can be found by optimizing min\u03a3(x\u2212x)+\u03bb\u03a3|x\u2212y|. Using Chin's implementation, built on the fast Laplacian solver, this step can be completed in nearly linear time. Second, an intensity range corresponding to nuclei can be identified for the image. As intensities can vary between tissue images due to staining methods and biological factors, a specific intensity range for nuclei for a given system is not specified. Instead, for each image, one or more Gaussian components can be fit to the distribution of intensities within the image. These can correspond to nuclei, cytoplasm, and stroma\/lumen. This removes the need for normalizing all images in the data set to the same background intensity, thus avoiding normalization artifacts. Additionally, this allows this algorithm to be ported between many systems and tissue types without having to reparametrize intensity thresholds. On each smoothed image I, the following steps can be performed: (1) a gaussian mixture model can be fit to the image's intensity distribution with a plurality of (e.g., three, etc.) gaussians (which can correspond to nuclei, cytoplasm, lumen\/stroma) and a background distribution; (2) using intensities and region sizes of the pixels described by each Gaussian component, the gaussian Gcomponent that most likely corresponds to nuclei can be identified; and (3) the nuclei mask M can be defined as a (n\u00d7n) binary matrix, where M(x, y)=1 if pixel (x, y) is accounted for by at least r % by Gg, and 0 otherwise. The cutoff r was empirically set to be 0.45\u00b7maximal percent that a pixel is accounted for by Gin results presented herein, although greater or lesser values can be used in various aspects of the subject innovation.","Third, at this point, Mis equal to one for any pixel that may be part of a nucleus. Each connected component in Mcan be considered a putative nucleus. However, Mmay contain many large connected regions that are actually made up of several closely neighboring nuclei, and it may be missing pixels that belong to nuclei that were not captured by G, e.g., lighter intensity pixels inside nuclei due to intensity variations. The next few steps work to break up large regions into individual nuclei and smooth out nuclei boundaries:","(1) The mask M can be cleaned up by removing holes and isolated\/bridge pixels;","(2) The mask can be contrast normalized, which can be helpful in finding individual nuclei in large regions;","(3) Thin lines of pixels included in the nuclei mask, which are often caused by \u201cwrinkles\u201d in cytoplasm, can be removed;","(4) Large regions can be subjected to further processing to break them into individual nuclei.","Step four can involve several sub-steps. First, an average size of putative nuclei can be found at this point by determining the median nucleus radius rand setting A=\u03c0r. An upper bound can be set for large regions as any putative nucleus with an area greater than a first constant times A; a first constant of 1.75 was used in results discussed herein and was determined empirically, but in various aspects, greater or lesser values can be used. This bound will cause many nuclei of reasonable size to be included in the group of large regions, but if they are sufficiently uniform in intensity, they will be returned unchanged to the set of putative nuclei after the following steps. Additionally, some statistics on shape (such as eccentricity and convexity) can be computed and applied in various aspects to determine reasonable bounds on nucleus shape.","A second sub-step can involve removing any large region with very high intensity (light in color), by requiring that the darkest pixel in large regions must be at least as dark as the median intensity pixel in small regions.","A third sub-step can be applied wherein, on each large region, anisotropic diffusion can be iteratively performed followed by contrast normalization and thresholding, until the region has been broken into multiple regions. The new regions can be added to the set of large regions, if they are also larger than the first constant multiple of A, or added to the set of putative nuclei. If a large region does not break into multiple regions, but is of reasonable shape and size, it can also be added to the set of putative nuclei, and otherwise discarded.","Finally, at this point, the large regions will all have been broken into smaller regions or deemed to be of reasonable shape and size. The parameters for size (A) and shape can be updated using the revised set of putative nuclei, and the following steps can be performed: (1) very small regions can be removed, defined by any putative nucleus with size less than a second constant (with value between 0 and 1) times A(as used in connection with results presented herein, the size threshold was",{"@attributes":{"id":"p-0035","num":"0034"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mfrac":{"msub":{"mi":["A","med"]},"mn":"3"},"mo":")"},"mo":";"}}},"br":{}},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 2"},"This method yields a putative nucleus mask, M for each smoothed image M (such as the examples seen in the bottom row of ). These masks can be combined so that each pixel can be assigned to the largest putative nucleus across all \u03bb at that pixel, to yield a final putative nucleus mask M, such as the example seen in . The putative nuclei at this point may contain regions that are not actually nuclei, but the second phase of the algorithm can identify these regions as non-epithelial components. Thus, phase one emphasizes having a high True Positive rate, with less concern about achieving a low False Positive Rate. In the bottom row of , nuclei segmentations according to aspects of the subject innovation are shown in yellow, with ground-truth segmentation shown in cyan. Red boxes indicate nuclei that were incorrectly segmented for each \u03bb. In , ground-truth segmentation is again shown in cyan, with merged nuclei segmentation shown in white.","Nuclei Segmentation GUI for Manual Epithelial Classification:","To compare automated epithelial classification with manual epithelial selection, a MatLab GUI was built which allowed a user to hand-pick putative nuclei as epithelial nuclei for phase analysis. As visualization of the putative nuclei boundaries are distracting and can bias the user, the GUI displayed only the raw tissue image. The user could click on a point in the image within a nucleus to select that nucleus. If the nucleus was part of the putative nucleus set, the GUI displayed the boundaries of the putative nucleus at that point. If the nucleus was not part of the set, the GUI used watershed to compute a nucleus at that point and displayed its boundaries to the user. The GUI then allowed the user to grow or shrink the nucleus, merge two nuclei, or split a predicted nucleus into two nuclei. These actions were all performed using steps from the nuclei segmentation algorithm described above. Additionally, the user could hand-trace a nucleus boundary if unsatisfied with the predicted nucleus at that point.","Phase II: Epithelial Classification","Pathologists use context information, in addition to nuclei descriptors, to identify epithelial nuclei in images. For example, while epithelial nuclei in a particular type of tissue are known to have a certain radius, e.g. \u224810 \u03bcm in esophagus epithelium, many other nuclei can have this same size. The location of a nucleus with respect to other nuclei and tissue structures complements this information, allowing pathologists to determine specifically which nuclei make up the epithelium. To analogously combine intrinsic and context information while identifying nuclei, the subject innovation can employ a Markov Random Field (MRF) that can encode unary and binary classifiers.","Unary Classifier:","Unary classifiers give the probability that a nucleus is epithelial, independent of the labels of its neighboring nuclei. A total of 94 features were measured on each putative nucleus, and a classifier was built using AdaBoost to label each putative nucleus with a probability of being epithelial. The feature sets includes descriptors measured on isolated nuclei, such as size, intensity, and convexity, as well as features dependent on the environment, such as distance to cell boundary or next closest nucleus. Used independently, each classifier was only weakly predictive. AdaBoost was used with MatLab's default parameters (binary classifier, learning rate of 1, 100 learners) to combine the set of 94 weak classifiers into a stronger classifier, \u03c8:{right arrow over (x)}\u03b5\u2192[0,1]\u2282, where {right arrow over (x)} is the feature vector for nucleus x.","Pairwise Classifier:","Pairwise classifiers give the probability that a nucleus is epithelial, conditioned on the label (epithelial or non-epithelial) of each of its neighbors. Pathologists use many contextual clues to classify nuclei, e.g. epithelial nuclei tend to form a chain along a lumen region, neighboring epithelial nuclei have similar orientations to the lumen, and size\/shape of neighboring epithelial nuclei are similar.","Epithelial Classification:","Initially, a set of nfeatures encoding such contextual clues were measured on all pairs of nearby nuclei, where the threshold for \u201cnearby\u201d was set to be a function of the median distance between nuclei within an image. However, due to the randomness of individual nuclei, these pair-wise features alone could not distinguish pairs of same-class nuclei from pairs of mixed-class nuclei (epithelial & epithelial, non-epithelial & non-epithelial, or epithelial & non-epithelial). Thus, to encode more global image information, the tissue architecture within the image was captured in terms of a) location of epithelial cell boundaries and b) arrangement of nuclei in a \u201ctree\u201d, with the longest chain of nuclei making up the trunk, as seen in , which shows identification of putative nuclei, modeling with a trunk and branches, and putative cell boundaries. In Phase I, putative nuclei can be predicted. The second row shows a trunk (green) with branches (cyan) built to model the nucleus architecture. The bottom row shows results from a Canny Edge detector that can capture epithelial cell boundaries. These tissue architecture features encode the contextual clues used by pathologists: chains of nuclei (described by the \u201ctree trunk\u201d) along the lumen border (described by the epithelial cell boundaries).","The nuclei pairs were then divided into eight architecture-categories according to their location with respect to the epithelial cell boundaries and their position on the nucleus tree. To find the tree, a greedy algorithm was used which initiated a trunk at the nucleus with highest unary probability of being epithelial, and added nuclei to the trunk in either direction, ensuring that added nuclei were close together, formed a relatively straight line, and had similar unary probabilities, orientation, and size, where parameters for close, straight, and similar were determined empirically. Once no more nuclei can be added while remaining within the restraints specified by the parameters, all remaining nuclei can be added iteratively onto branches, where each nucleus can be simply attached with a branch to its closest neighbor already on the tree. This trunk\/branch model tends to place epithelial cells on the initial trunk, and any other chains of epithelial form branches of the tree. Thus, most nuclei pairs within the same architectural-category are of the same type: nuclei pairs on trunks tend to be epithelial, nuclei pairs at junctures between branches or the trunk and a branch tend to contain mixed nuclei, and nuclei pairs on branches are often either both non-epithelial or both epithelial. This architectural layout largely removes the randomness of individual nuclei pairs that handicapped the classification of pairwise features, when applied to arbitrary nuclei pairs. To determine cell boundaries, a Canny Edge Detector was used, with a Gaussian smoothing factor selected to be 20 \u03bcm, representing twice the length of a nucleus (in various embodiments, greater or lesser values can be used, e.g., when different nuclei sizes are involved). Long, smooth edges representing cell boundaries were formed by first connecting nearby edges with similar slopes at their termini, and then removing short edges. Parameters for short edges, nearby edges, and similar slopes were determined empirically on the training set.","For each of these eight classes, a classifier function was trained using AdaBoost on the initial set of npairwise features. Specifically, for each architecture-category c\u03b5C, the conditional probabilities were learned that a nucleus is epithelial, given that its neighbor is epithelial (\u03a8:({right arrow over (x)},{right arrow over (y)})\u03b5\u00d7\u2192[0,1]\u2282) and the probability that a nucleus is epithelial, given that its neighbor is non-epithelial (\u03a8:({right arrow over (x)},{right arrow over (y)})\u03b5\u00d7\u2192[0,1]\u2282). Here, e denotes epithelial, n denotes non-epithelial, and the probabilities are symmetric (\u03a8({right arrow over (x)},{right arrow over (y)})=\u03a8({right arrow over (x)},{right arrow over (y)}) for the pair of nuclei (x, y) with feature vectors {right arrow over (x)},{right arrow over (y)}\u03b5, analogous for \u03a8.","Conditional Markov Random Field:","Maximization on a conditional random field yields an optimal class labeling (as epithelial or non-epithelial) for the putative nuclei in an image according to that field. Note that the term \u201cputative nuclei\u201d is still carried because some regions assigned to the non-epithelial class may not be nuclei at all; the algorithm only seeks to classify these regions as not being epithelial, regardless of whether or not they are nuclei. An undirected graph was built in which each putative nucleus was a node, and edges placed between nearby nuclei, as defined in the previous section. Let N denote the number of nodes (nuclei) in the graph, E denote the set of epithelial nuclei, and \u0112 denote the set of nonepithelial nuclei. The edge between nodes x and y, belonging to architectural-category c, with feature vectors {right arrow over (x)} and {right arrow over (y)}, can be weighted with the pairwise conditional probability matrix",{"@attributes":{"id":"p-0052","num":"0051"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":[{"mo":["(",")"],"mtable":{"mtr":[{"mtd":[{"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":["x","E"],"mo":"\u2208"},{"mi":["y","E"],"mo":"\u2208"}],"mo":"\u2758"}}}},{"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":["x","E"],"mo":"\u2208"},{"mi":"y","mo":"\u2208","mover":{"mi":["E","_"]}}],"mo":"\u2758"}}}}]},{"mtd":[{"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"x","mo":"\u2208","mover":{"mi":["E","_"]}},{"mi":["y","E"],"mo":"\u2208"}],"mo":"\u2758"}}}},{"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"x","mo":"\u2208","mover":{"mi":["E","_"]}},{"mi":"y","mo":"\u2208","mover":{"mi":["E","_"]}}],"mo":"\u2758"}}}}]}]}},{"mo":["(",")"],"mtable":{"mtr":[{"mtd":[{"mrow":{"msubsup":{"mi":["\u03a8","e","c"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mover":[{"mi":"x","mo":"\u2192"},{"mi":"y","mo":"\u2192"}],"mo":","}}}},{"mrow":{"msubsup":{"mi":["\u03a8","n","c"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mover":[{"mi":"x","mo":"\u2192"},{"mi":"y","mo":"\u2192"}],"mo":","}}}}]},{"mtd":[{"mrow":{"mn":"1","mo":"-","mrow":{"msubsup":{"mi":["\u03a8","e","c"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mover":[{"mi":"x","mo":"\u2192"},{"mi":"y","mo":"\u2192"}],"mo":","}}}}},{"mrow":{"mn":"1","mo":"-","mrow":{"msubsup":{"mi":["\u03a8","n","c"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mover":[{"mi":"x","mo":"\u2192"},{"mi":"y","mo":"\u2192"}],"mo":","}}}}}]}]}}],"mo":"="},"mo":","}}},"br":{},"sup":["T","T","T","T","U","U "],"sub":["i","i","i","i","1","i","1","i","i"],"img":{"@attributes":{"id":"CUSTOM-CHARACTER-00010","he":"3.89mm","wi":"5.67mm","file":"US09626583-20170418-P00002.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},"ul":{"@attributes":{"id":"ul0001","list-style":"none"},"li":{"@attributes":{"id":"ul0001-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0002","list-style":"none"},"li":["(0) Initialize t=0, {right arrow over (v)}={right arrow over (v)}, score={right arrow over (u)}v+\u03c9vBv, score=score+2\u03b5.","(1) While |score\u2212score|>\u03b5\n        \n        "]}}}},"This method is a variant of the power iteration for finding the first eigenpair of a matrix and will converge. As the starting point is usually very close to the optimal solution, the convergence is usually rapid.","Correcting for Isolated Epithelial Nuclei with a Local Smoothing Factor:","If one nuclei in a pair of non-epithelial nuclei has very different features than its neighbor, then the probability that this nuclei is epithelial, conditioned on its neighbor being non-epithelial, can be higher than the probability that both nuclei are non-epithelial, since the probabilities are trained to assign very different neighboring nuclei to different classes. In most cases, the unary probability that this nucleus is non-epithelial is strong enough to overpower the pairwise probability that it is epithelial, and the nucleus is correctly labeled as non-epithelial. However, if the nucleus is in a group of non-epithelial nuclei, and is very different than its neighbors, then the combined pairwise probabilities from all the neighbors that the nucleus is epithelial may outweigh the unary probability that that nucleus is nonepithelial, and the nucleus will be labeled as epithelial. To adjust for this, instead of a single smoothing factor \u03bb; \u03bb can be scaled for each node according to its number of neighbors, specifically: \u03bb:=d(j); where d(j) is the degree of node j. Results provided herein show how a local smoothing factor improves the MRF. Additionally, the classification accuracy can be further improved by following the MRF with a correction step, in which isolated nuclei labeled as epithelial nuclei can be reassigned a label according to their maximal unary probability, as seen in Table 1:",{"@attributes":{"id":"p-0056","num":"0061"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"77pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"70pt","align":"center"}}],"thead":{"row":{"entry":"TABLE 1"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":[{},"Training","Testing"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"1","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"FPR","TPR","accuracy","FPR","TPR","accuracy"]},{"entry":{"@attributes":{"namest":"1","nameend":"7","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"1","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"6","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["MRF, sc. \u03bb","6.7","89.2","91.6","10.4","82.5","86.7"]},{"entry":["MRF, loc. \u03bb","8.3","88.6","90.4","11.6","84.0","86.6"]},{"entry":["MRF, sc. \u03bb + corr.","6.6","89.2","91.7","10.2","82.5","86.8"]},{"entry":["MRF, loc. \u03bb + corr.","5.4","88.2","92.0","8.9","80.9","87.0"]},{"entry":{"@attributes":{"namest":"1","nameend":"7","align":"center","rowsep":"1"}}}]}}]}}},"Choice of Pairwise Classifiers and Parameters:","To determine the most appropriate method for epithelial nuclei detection, a range of pairwise classifiers were sampled with a range of smoothing parameters. That is, for the problem",{"@attributes":{"id":"p-0059","num":"0064"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":[{"munder":{"mi":["max","\u03c5"]},"mo":"\u2062","mrow":{"msup":{"mover":{"mi":"u","mo":"\u2192"},"mi":"T"},"mo":"\u2062","mi":"\u03c5"}},{"mi":["\u03c9","B"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msup":{"mover":{"mi":"\u03c5","mo":"\u2192"},"mi":"T"},"mover":{"mi":"\u03c5","mo":"\u2192"}}],"mo":"+"},"mo":","}}},"br":{},"sup":"c"},"Additionally, as the architecture-category of each nucleus pair was predicted using a greedy algorithm designed to model nuclei as a trunk with branches and a Canny edge detector to estimate cell boundaries, which may be imperfect, the ground-truth architecture-category of each nucleus pair was computed in terms of the nucleus trunk, the cell boundaries, or both. For these \u201cideal\u201d cases, each of the above classification functions was also sampled over a range of smoothing parameters, to determine how well the algorithm would perform if these intermediate values were perfect.","The accuracy of each method was computed on the testing data set for a large range of smoothing parameters \u03c9. For each method that improved the accuracy by at least 1% over the accuracy with only the unary probabilities for some \u03c9, a subset of \u03c9's close to that method's optimal \u03c9 were selected, and the performance of the method was evaluated on a random validation set (83 images randomly selected from the combined training\/testing sets). The method with the highest accuracy on the validation set as was selected as the classifier, together with the optimal \u03c9 for that method on the validation set. (Note that the results shown on the testing set need not be greater than 1% over the unary classifier, as \u03c9 was first optimized on the validation set, and so a different \u03c9 may be used for the overall results than was initially used to select methods to test on the validation set.) This classification method was then used to predict epithelial nuclei on the experimental set, as discussed in the results below.","Data","As part of the spatial-domain low-coherence quantitative phase microscopy (SL-QPM, as described in pending U.S. patent application Ser. No. 13\/695,230, the entirety of which is incorporated herein by reference) protocol used to obtain results, each tissue sample was imaged at 1004 wavelengths, yielding 1004 separate images for a single sample. The average of these 1004 images was used for nuclei segmentation and classification, and the average image was referred to simply as the image. The learning data consisted of 414 stained histology images at 20\u00d7 magnification (0.5 \u03bcm per pixel) from healthy (BE-normal) tissue taken from 89 patients, with each patient yielding four to five images. The average image size in the database was (531\u00d7363) pixels, or 266 \u03bcm\u00d7182 \u03bcm, with image size ranging between 32770 and 359840 pixels. From this data set, 47 patients (215 images) were diagnosed with Barrett's Esophagus, no dysplasia [BE-normal], 28 patients (131 images) were diagnosed with Barrett's Esophagus and High Grade Dysplasia [BE-HGD], and 14 patients (68 images) were diagnosed with Barrett's Esophagus and Esophageal Adenocarcinoma [BE-EAC].","In addition to the 414 images from the learning set used to train and test the nuclei segmentation and epithelial classification algorithms, a set of 424 stained histology images of the same magnification and in the same size range was obtained, for which phase information was calculated using SL-QPM to evaluate the usage of SL-QPM for early cancer detection in Barrett's Esophagus. Images in this \u201cexperimental set\u201d came from the same set of patients as the training set, with diagnoses of BE-normal, BE-HGD, and BE-EAC, and again contained only healthy (BE-normal) tissue.","Importantly, while the images come from patients of three diagnostic classes, the tissue selected for imaging was in all cases healthy BE, with no dysplasia. It should not be apparent, even to an expert, that any of the tissue samples actually come from patients with an increased risk of cancer over BE-normal patients, as the experiment studied cell changes in the field adjacent to carcinoma.","Ground Truth:","Nuclei Segmentation: Ground truth labeling of nuclei boundaries was performed using a matlab GUI designed for the task to label nuclei boundaries, and verified\/edited by a pathologist on a random sample of 10 BE-normal images, 10 BE-HGD images, and 10 BE-EAC images. Epithelial Classification: On a subset of 38 images from the same set of 89 patients, but unique from the learning and experimental sets, image regions were marked by the pathologist as belonging to epithelial cells, stromal cells, inflammatory cells, goblet cells, lymphocytes, other non-epithelial cells, or lumen. The 414 image set was then labeled accordingly and verified\/edited by the pathologist. For the 424 image set, nuclei boundaries were automatically predicted using phase one of our algorithm, and then putative nuclei were labeled as epithelial or non-epithelial.","Evaluation:","The nuclei segmentation and epithelial classification methods of the subject innovation were evaluated according to true positive rate (TPR), false positive rate (FPR), and accuracy. For epithelial classification, the TPR was defined as the percent of nuclei with ground-truth label epithelial that were also predicted to be epithelial. The FPR was the percent of nuclei with ground-truth label non-epithelial that were predicted to be epithelial. The accuracy was defined as the total number of correctly classified putative nuclei, divided by the total number of putative nuclei. For nuclei segmentation, a true positive was defined as any predicted nucleus that overlapped with a ground-truth nucleus, a false positive as any predicted nucleus that did not overlap with any ground-truth nuclei, and a false negative as any ground-truth nucleus that did not overlap with any predicted nuclei. The total number of true nuclei was the number of ground-truth nuclei, and the total number of false nuclei was the number of false positives. As the definition of true positive was very weak, in that one pixel overlap was required for a putative nucleus to be considered correct, two additional measures were required to establish the quality of the predictions while tuning the nuclei segmentation algorithm. The %-covered measures the number of pixels shared by the putative nucleus and its corresponding ground-truth nucleus, divided by the total number of pixels in the ground-truth nucleus. The %-wasted measures the number of pixels from the putative nucleus that are not also in its corresponding ground-truth nucleus, divided by the total number of pixels in the putative nucleus.","Results","Automated Selection of Epithelial Nuclei","Nuclei Segmentation:","Nuclei segmentation methodology and parameters were optimized on an independent data set of 38 images, taken from a subset of the same 89 patients, but not included in the 414 image set. To establish the accuracy of the method, nuclei were hand-segmented on a validation set of 30 images from the 414 image data set, ten from each of the three diagnostic classes. Initial hand-segmentation was later corrected\/verified by the pathologist.","The overall segmentation accuracy on the 30 images is shown in Table 2, below, along with the performance on each diagnostic class. There was little variation in segmentation accuracy between classes, which was to be expected since the images all showed healthy tissue. A representative ground-truth hand-segmentation and computational nucleus segmentation is shown in ; the example of  has a 94% TPR and a 33% FPR.",{"@attributes":{"id":"p-0075","num":"0080"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"63pt","align":"center"}}],"thead":{"row":[{"entry":[{},"TABLE 2"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]},{"entry":[{},"Number images","TPR (%)","FPR (%)"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"63pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Overall","30","94","33"]},{"entry":[{},"BE-normal","10","96","34"]},{"entry":[{},"BE-HGD","10","94","29"]},{"entry":[{},"BE-EAC","10","94","36"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]}]}}]}}},"Epithelial Classification:","The images were randomly split into a training set of 331 images (80%) and a testing set of 83 images (20%). To validate the epithelial classification, all putative nuclei were initially labeled as epithelial or non-epithelial on all 414 images, and then corrected\/verified by the pathologist. Parameters for both unary and pairwise classifiers were learned on the training set. Table 3, below, shows the epithelial classification results using (a) only the unary classifier, and (b) both unary and pairwise classifiers, on both the training and testing sets Improvement in both increased true positive rate [TPR] and decreased false positive rate [FPR] were observed with the addition of the pairwise classifier.  shows an example of improvement in accuracy of epithelial classification through addition of context information encoded in the MRF. For the initial image (top) with ground-truth nuclei labeling shown on the bottom (red=epithelial cell nuclei, white=other nuclei), putative nuclei were predicted in Phase I, as shown in row 2. The unary probability of these nuclei being epithelial is shown in the third row, and all nuclei with unary probability greater than 0.5 could be classified as epithelial, as in row 4 (in various embodiments, other threshold values could be used). By using contextual information encoded in a MRF, the classification improves, as seen in row 5.",{"@attributes":{"id":"p-0078","num":"0083"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"84pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"98pt","align":"center"}}],"thead":{"row":{"entry":[{},"TABLE 3"]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]},{"entry":[{},{},"Training","Testing"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"8"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"42pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},"FPR","TPR","accuracy","FPR","TPR","accuracy"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"7","align":"center","rowsep":"1"}}]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"8"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"42pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Unary","10.9","87.3","88.3","13.3","84.0","85.7"]},{"entry":[{},"cMRF","8.4","89.5","92.0","10.9","85.6","87.0"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"7","align":"center","rowsep":"1"}}]}]}}]}}},"On a 2012 MacBook Pro (2.9 GHz Intel Core i7, 8 GB memory), as used in experimental analysis discussed herein, initial nuclei segmentation took approximately 120 seconds for an average sized pixel image. The epithelial classification took around 60 seconds, thus the algorithm spent an average of approximately 180 seconds per image. By running the algorithm overnight, significant time was saved over the several minutes required for a researcher to manually outline each epithelial nucleus in an image.","Automated Versus Manual Selection:","On the experimental data set of 424 images, an independent researcher, generated putative nuclei using the above nuclei segmentation algorithm and manually selected around 10 epithelial nuclei from the set of putative nuclei from each image for phase analysis. Overall, 4095 nuclei were manually selected, while 7045 nuclei were automatically selected. For 3224 of the manually selected nuclei (78.7%), an automatically selected nucleus shared at least half of its pixels. For 80.1% of the manually selected nuclei, an automatically selected nucleus overlapped by at least one pixel. Note that many more nuclei were expected to be selected automatically than manually, as the algorithm seeks every epithelial nuclei, while the independent researcher only sought around 10 representative nuclei per image. The automated algorithm tended to miss epithelial nuclei that were isolated, as the pairwise nature of the algorithm encourages epithelial nuclei to appear in chains.","Optical Biomarker for Cancer Risk in BE","The below discussion demonstrates that (a) automatically selecting nuclei can produce an equivalent or larger set of epithelial nuclei as manually selecting nuclei, and (b) distributions of features computed on the phase of epithelial cells can be used as an optical biomarker for cancer risk in BE. When computing phase on the predicted epithelial nuclei, nuclei on image boundaries were ignored, as the pixel intensities near the boundaries were generally much darker than in the image interiors do to intensity fall-off.","An average nucleus has approximately 800 pixels, and phase was computed on every pixel in every epithelial nucleus. To summarize the distribution of phases on a nucleus, the entropy was computed as H=\u2212\u03a3plog(p), where b indicated a binning index. In the experimental results discussed herein, 51 bins of length",{"@attributes":{"id":"p-0085","num":"0090"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mfrac":{"mi":"\u03c0","mn":"25"}}},"br":{},"figref":["FIG. 6","FIG. 6A","FIG. 6B"]},"Results provided in Table 4 show that the p-values describing the probability that the entropy distributions from any pair of diagnostic classes were generated from the same distribution in Table 3 for both automatically and manually selected nuclei. Given a cutoff for significance of p-value<0:05, both the manual and automatic nuclei have significantly different distributions for each diagnostic class in at least one, and almost all, phase depths. The HGD and EAC classes were hardest to separate, while the BE-EAC classes were easiest to separate.",{"@attributes":{"id":"p-0087","num":"0092"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"77pt","align":"center"}}],"thead":{"row":[{"entry":[{},"TABLE 4"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]},{"entry":[{},"depth","BE-HGD","HGD-EAC","BE-HGD"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"182pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"Manual"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"77pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"1","0.0000","0.4719","0.0000"]},{"entry":[{},"2","0.0000","0.8209","0.0000"]},{"entry":[{},"3","0.0006","0.0009","0.0000"]},{"entry":[{},"4","0.0219","0.0001","0.0000"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"182pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"Automatic"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"77pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"1","0.0017","0.0009","0.0000"]},{"entry":[{},"2","0.0000","0.0954","0.0000"]},{"entry":[{},"3","0.0000","0.0003","0.0000"]},{"entry":[{},"4","0.0001","0.0005","0.0000"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]}]}}]}}},"Table 5, below, shows the results on validation, training, and testing set for the best performing classifiers (any binary classifier that improved accuracy on testing set by more than 1% over unary classifier. Columns 1-5 describe the parameters used for each binary classifier.). The final two rows show results with only the unary classifier.",{"@attributes":{"id":"p-0089","num":"0094"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"119pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"105pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"49pt","align":"center"}}],"thead":{"row":{"entry":"TABLE 5"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":["method","Validation set results","Overall Results"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"10"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"7pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"8","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"9","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"10","colwidth":"49pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["tree","correction?","\u03a8","\u03bb","edges","FPR","TPR","acc.","Training acc.","Testing acc."]},{"entry":{"@attributes":{"namest":"1","nameend":"10","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"10"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"7pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"7","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"8","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"9","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"10","colwidth":"49pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["angle\/dist.","y","f","{right arrow over (\u03bb)}","spatial","5.2","87.6","91.9","92.0","87.0"]},{"entry":["angle\/dist.","y","f","\u03bb","spatial","6.5","88.0","91.3","91.7","86.8"]},{"entry":["angle\/dist.",{},"f","\u03bb","spatial","6.6","88.1","91.2","91.6","86.7"]},{"entry":["dist.","y","f","{right arrow over (\u03bb)}","spatial","8.1","88.8","90.7","90.8","87.7"]},{"entry":["dist.",{},"f","{right arrow over (\u03bb)}","spatial","8.4","89.2","90.6","90.6","87.4"]},{"entry":["dist.",{},"f","\u03bb","spatial","8.2","88.7","90.5","90.6","86.9"]},{"entry":["dist.","y","f","\u03bb","spatial","8.1","88.4","88.2",{},"86.6"]},{"entry":["dist.","y","f","{right arrow over (\u03bb)}","tree","8.8","89.5","88.4",{},"87.1"]},{"entry":["dist.","y","f","\u03bb","tree","9.0","89.6","88.4",{},"87.1"]},{"entry":["dist.",{},"f","\u03bb","tree","9.2","89.7","88.3",{},"86.8"]},{"entry":["dist.",{},"f","{right arrow over (\u03bb)}","tree","10.6","89.3","89.4",{},"86.8"]},{"entry":["dist.","y","l","\u03bb","tree","9.3","87.3","86.9",{},"86.8"]},{"entry":["dist.",{},"l","\u03bb","tree","9.4","87.3","86.8",{},"86.8"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"6"},"colspec":[{"@attributes":{"colname":"1","colwidth":"119pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"49pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["unary classifier, angle\/dist.","10.9","88.4","88.8","88.3","85.7"]},{"entry":["unary classifier, dist.","11.2","88.6","88.7","88.3","85.8"]},{"entry":{"@attributes":{"namest":"1","nameend":"6","align":"center","rowsep":"1"}}}]}}]}}},"Table 6, below, shows true and false positive rates on training and testing sets for the 25 most predictive unary features. The bottom row shows the FPR and TPR for the combined classifier generated with AdaBoost. Each training set consisted of around 15387 nuclei from 332 images, and each testing set consisted of around 3935 nuclei from 882 images. In total, there were 11459 ground-truth non-epithelial putative nuclei and 7863 ground-truth epithelial nuclei. For canny edge features, a indicates the size of the Gaussian filter used for smoothing. Note that these results are for the combined nuclei set taken from all images\u2014a single image can yield both training and testing nuclei. As epithelial classification depends on neighboring nuclei, entire images were labeled as either testing or training for validating epithelial segmentation. Thus, the overall training and testing accuracy in Table 6 will be slightly different than the accuracy shown for the unary classifier above.",{"@attributes":{"id":"p-0091","num":"0096"},"tables":{"@attributes":{"id":"TABLE-US-00006","num":"00006"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"133pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"center"}}],"thead":{"row":{"entry":[{},"TABLE 6"]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]},{"entry":[{},"Training","Testing"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"133pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"21pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Feature","FPR","TPR","FPR","TPR"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"133pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"5","colwidth":"21pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["median pixel intensity","28.4","90.1","32.1","88.7"]},{"entry":["Area","26.8","77.1","26.9","75.3"]},{"entry":["mean-nuc-back","31.9","76.3","34.3","76.7"]},{"entry":["med-nuc-back","40.3","91","43.5","91"]},{"entry":["\u03c7-distance between nucleus and","36.3","71.1","34.5","69.1"]},{"entry":"surrounding intensities"},{"entry":["average length of closest canny edge, \u03c3 = 20","33.6","59.9","36.8","67.8"]},{"entry":["distance to closest cell boundary","36.9","66.1","37.8","65.1"]},{"entry":["shared-edges-double-20","34.3","60.5","39.3","66.8"]},{"entry":["average length of closest canny edge, \u03c3 = 25","42.8","69.4","44.7","73.5"]},{"entry":["shared-edges-single-20","36.8","60.9","38.3","64.6"]},{"entry":["length of second closest canny edge, \u03c3 = 20","33.2","54.2","34.7","60.7"]},{"entry":["distance to second closest canny edge,","38","61.3","38.7","64.4"]},{"entry":"\u03c3 = 30"},{"entry":["length of first closest canny edge, \u03c3 = 25","38.5","58.9","38.7","63.2"]},{"entry":["shared-edges-single-25","47.1","70.8","46.1","72.8"]},{"entry":["average length of closest canny edge, \u03c3 = 15","36.4","59.9","38.4","62.5"]},{"entry":["-avg-dist-to-two-edges-sig-30","43.4","67.7","44.6","69.8"]},{"entry":["average distance to closest two cell","27.8","51.7","30.9","55.8"]},{"entry":"boundaries"},{"entry":["mean-neigh-orien-diff","42.9","65.1","39.6","63.3"]},{"entry":["average distance to two closest canny edges,","36.8","59.3","38.1","61.2"]},{"entry":"\u03c3 = 25"},{"entry":["distance to second closest canny edge,","38.6","59.7","38.8","61.5"]},{"entry":"\u03c3 = 45"},{"entry":["length of second closest canny edge, \u03c3 = 15","38.4","58.2","39.3","61.8"]},{"entry":["avg-length-edge-sig-30","33.5","57.8","35","57.8"]},{"entry":["average distance to two closest canny edges,","41.8","62.5","40.4","62.7"]},{"entry":"\u03c3 = 45"},{"entry":["angle to closest cell boundary","39.3","58.7","40.8","63.1"]},{"entry":["distance to second closest canny edge,","34.7","57","36.5","58.6"]},{"entry":"\u03c3 = 25"},{"entry":["combined features (AdaBoost)","12.9","88.6","13.8","88.2"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}}]}}},"Still another embodiment can involve a computer-readable medium comprising processor-executable instructions configured to implement one or more embodiments of the techniques presented herein. An embodiment of a computer-readable medium or a computer-readable device that is devised in these ways is illustrated in , wherein an implementation  comprises a computer-readable medium , such as a CD-R, DVD-R, flash drive, a platter of a hard disk drive, etc., on which is encoded computer-readable data . This computer-readable data , such as binary data comprising a plurality of zero's and one's as shown in , in turn comprises a set of computer instructions  configured to operate according to one or more of the principles set forth herein. In one such embodiment , the processor-executable computer instructions  is configured to perform a method , such as at least a portion of one or more of the methods described in connection with embodiments disclosed herein. In another embodiment, the processor-executable instructions  are configured to implement a system, such as at least a portion of one or more of the systems described in connection with embodiments disclosed herein. Many such computer-readable media can be devised by those of ordinary skill in the art that are configured to operate in accordance with the techniques presented herein.",{"@attributes":{"id":"p-0093","num":"0098"},"figref":["FIG. 8","FIG. 8"]},"Generally, embodiments are described in the general context of \u201ccomputer readable instructions\u201d being executed by one or more computing devices. Computer readable instructions are distributed via computer readable media as will be discussed below. Computer readable instructions can be implemented as program modules, such as functions, objects, Application Programming Interfaces (APIs), data structures, and the like, that perform particular tasks or implement particular abstract data types. Typically, the functionality of the computer readable instructions can be combined or distributed as desired in various environments.",{"@attributes":{"id":"p-0095","num":"0100"},"figref":["FIG. 8","FIG. 8"],"b":["800","802","802","806","808","808","804"]},"In these or other embodiments, device  can include additional features or functionality. For example, device  can also include additional storage such as removable storage or non-removable storage, including, but not limited to, magnetic storage, optical storage, and the like. Such additional storage is illustrated in  by storage . In some embodiments, computer readable instructions to implement one or more embodiments provided herein are in storage . Storage  can also store other computer readable instructions to implement an operating system, an application program, and the like. Computer readable instructions can be loaded in memory  for execution by processing unit , for example.","The term \u201ccomputer readable media\u201d as used herein includes computer storage media. Computer storage media includes volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information such as computer readable instructions or other data. Memory  and storage  are examples of computer storage media. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, Digital Versatile Disks (DVDs) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by device . Any such computer storage media can be part of device .","The term \u201ccomputer readable media\u201d includes communication media. Communication media typically embodies computer readable instructions or other data in a \u201cmodulated data signal\u201d such as a carrier wave or other transport mechanism and includes any information delivery media. The term \u201cmodulated data signal\u201d includes a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal.","Device  can include one or more input devices  such as keyboard, mouse, pen, voice input device, touch input device, infrared cameras, video input devices, or any other input device. One or more output devices  such as one or more displays, speakers, printers, or any other output device can also be included in device . The one or more input devices  and\/or one or more output devices  can be connected to device  via a wired connection, wireless connection, or any combination thereof. In some embodiments, one or more input devices or output devices from another computing device can be used as input device(s)  or output device(s)  for computing device . Device  can also include one or more communication connections  that can facilitate communications with one or more other devices  by means of a communications network , which can be wired, wireless, or any combination thereof, and can include ad hoc networks, intranets, the Internet, or substantially any other communications network that can allow device  to communicate with at least one other computing device .","What has been described above includes examples of the innovation. It is, of course, not possible to describe every conceivable combination of components or methodologies for purposes of describing the subject innovation, but one of ordinary skill in the art may recognize that many further combinations and permutations of the innovation are possible. Accordingly, the innovation is intended to embrace all such alterations, modifications and variations that fall within the spirit and scope of the appended claims. Furthermore, to the extent that the term \u201cincludes\u201d is used in either the detailed description or the claims, such term is intended to be inclusive in a manner similar to the term \u201ccomprising\u201d as \u201ccomprising\u201d is interpreted when employed as a transitional word in a claim."],"GOVINT":[{},{}],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 8"}]},"DETDESC":[{},{}]}
