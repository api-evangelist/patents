---
title: Spectral feature generation using high-pass filtering for scene anomaly detection
abstract: A method of detecting an image anomaly (target) within a scene represented by image data comprises obtaining test data from a test window within the scene, combining the test data with reference data to generate combined data, then comparing the combined data with either the test data or the reference data. An improved image analyzer includes a computational unit configured to execute this method. In one example, image data is generated by a hyperspectral imager.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07593587&OS=07593587&RS=07593587
owner: The United States of America as represented by the Secretary of the Army
number: 07593587
owner_city: Washington
owner_country: US
publication_date: 20050701
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["REFERENCE TO RELATED APPLICATION","GOVERNMENT INTEREST","FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DESCRIPTION OF THE PREFERRED EMBODIMENTS"],"p":["This application claims priority to U.S. Provisional Patent Application Ser. No. 60\/670,263, filed on 12 Apr. 2005, the entire content of which is incorporated herein by reference.","The invention described herein may be manufactured, used, and licensed by or for the United States Government.","The present invention relates to data analysis, in particular to the detection of image anomalies using hyperspectral sensor imagery.","In the context of primitive computer vision, it would be desired to have a relatively simple automatic approach capable of focusing its attention in the same way a human analyst would observing the same set of images. For reasons well known and highlighted in standard computer-vision books (see, for instance, Lagarias, J. C., J. A. Reeds, M. H. Wright, and P. E. Wright, \u201cConvergence Properties of the Nelder-Mead Simplex Method in Low Dimensions,\u201d SIAM Journal of Optimization, Vol. 9 Number 1, pp. 112-147, 1998), that expectation, however, is rarely met with experimental results, despite of the fact that sometimes the scenes in reference are characterized by image analysts as easy to focus their attentions to certain types of objects in the scenes.","Humans, of course, use a combination of knowledge-based, local and global information to aid in the analysis of a scene, a capability that may be reproduced by applying, for instance, layers of unsupervised learning methods complementing each other to perform this single task. For example, a suite of algorithms that includes an edge detector, an edge elongation, a clustering method, and a morphological size test might reproduce the humans' performance in certain conditions, albeit with a huge cost: computational time. Needless to say, the topic of automatic focus of attention (FOA) is an open and quite active area of research.","An advantage of choosing hyperspectral data over broadband is that a particular type of material, for instance, may be identified by testing a few pixels of the tested object, independently of the object's orientation, elevation angle, and distance from the sensor. Hyperspectral sensors are passive sensors that simultaneously record images for many contiguous and narrowly spaced regions of the electromagnetic spectrum. A data cube is created from these images, in which each image corresponds to the same ground scene and contains both spatial and spectral information about objects and backgrounds in the scene. These sensors employ several bands and have been used in various fields including urban planning, mapping, and military surveillance.","A further advantage of choosing anomaly detection over a particular type of material detection is that oftentimes the exact material of interest is not known a priori, or the number of spectra in a material of interest library is simply too exhaustive to search for all possible materials. The goal of an anomaly detector is to identify statistical outliers, i.e., data points that are atypical compared to the rest of the data. An anomaly detector that properly detects all, or a significant portion, of the pixels representing meaningful objects (targets) while at the same time having hundreds of meaningless detections (false alarms) has little practical value.","Most conventional anomaly detectors use multivariate models to define the spectral variability of the data, and the majority of the data pixels are assumed to be spectrally homogeneous and are modeled using a multivariate probability density function with a single set of parameters. Until now, no significant work had been done to find non-normal statistical models for the development of anomaly detection techniques using hyperspectral data. Conventional anomaly detectors may detect the presence of targets using hyperspectral data, but in the process they yield a large number of false alarms. This type of performance has little practical value.","A method of detecting an image anomaly within a scene represented by image data, the method comprises defining a test window within the scene, obtaining test data relating to the test window, obtaining reference data, generating combined data by combining the test data and the reference data, and detecting the image anomaly by comparing the combined data with the test data or with the reference data. The reference data can be obtained from a reference window, which may surround the test window, and may be spaced apart from the test window by a guard region from which no data is obtained. The reference data can also be obtained from one or more fixed locations within the scene, and\/or include representative data obtained from another scene, such as data from natural features from within the scene.","Test data can be derived from a test spectrum obtained from the test window, which may be an average spectrum obtained from a number of image pixels. Reference data can be derived from a reference spectra obtained either from a reference window, other locations within the scene, or reference spectra obtained from other scenes. Variability spectra can also be obtained, for example from a variability window surrounding the test window, and used in generating the test data and the reference data. Variability spectra can also be obtained from different portions of the reference window, with the reference spectrum being an average spectrum from the reference window.","In examples of the present invention, the test data comprise a test feature set derived from the test spectrum and the variability spectra, the reference data comprise a reference feature set derived from the reference spectrum and the variability spectra, and the combined data is a combined feature set, the combined feature set being a combination of the test feature set and the reference feature set. The contents of the test feature set can be generated by applying a high-pass filter to the test spectrum to obtain a filtered test spectrum, applying a high-pass filter to the reference spectrum to obtain a filtered reference spectrum, and applying a high pass filter to each of the variability spectra to obtain filtered variability spectra, and generating the test feature set, each component of the test feature set being generated from the filtered test spectrum and one of the filtered variability spectra, the test feature set having a number of components equal to the number of variability spectra, the reference feature set being generated in a similar manner.","Novel algorithms were developed, giving a numerical output corresponding to a strength of anomaly, a strong anomaly being present if the test data represents object characteristics that do not contribute to the reference data, a weak anomaly being present if the test data represents object characteristics that in part contribute to the reference data, and no anomaly being present if the test data represents object characteristics that substantially contribute to the reference data. In representative examples, the algorithm can be a semiparametric algorithm responsive to the estimator of a parameter that determines the likelihood of two distributions being the same, and also responsive to the variance of the combined feature set, or responsive to any combination of lower or higher moments (including the means), and\/or to any combination of lower or higher central moments (including the variances), computed from the test feature set or reference feature set, as a mathematical expression responsive also to these corresponding moments and central moments computed from the combined feature set in order to compare the test and reference feature sets.","An improved image anomaly detector for detecting an anomaly within a scene comprises a source of image data, such as hyperspectral image data, relating to the scene, and a computation unit, the computation unit detecting an anomaly within the scene using an indirect comparison method. The source of image data can be a hyperspectral image database, a hyperspectral imaging device, or other imaging device. The computation unit may output a visual representation of the comparison of the combined data with the test data or the reference data, for example as a visual representation of numerical data. The apparatus may comprise a computational unit operational to define a test window as a portion of the scene, obtain test data from the test window, obtain reference data, and detect the image anomaly by performing an indirect comparison of the test data and the reference data by comparing a combination of the test data and the reference data with either the test data or the reference data. The data source may be a hyperspectral image sensor, for example mounted on a mobile unit.","In an indirect comparison approach, samples are not compared as individual entities, but as individual entities and the union among these entities. Let X and Y denote two random samples, and let Z=X U Y, where U denotes the union (or combination). Moments and central moments of the probability distribution of X can be indirectly compared to moments and central moments of the probability distribution of Y by comparing instead moments and central moments of the distributions of Z and Y.","Conventional techniques produce high numbers of meaningless detections in digitized scenes because these techniques do not address explicitly all of the most common spatial\/spectral variability occurrences that may be observed locally in the imagery. To better understand the behavior of conventional anomaly detectors, five known techniques were applied to actual HS imagery, including the industry standard, and decomposed the most probable spatial\/spectral variability occurrences into three study cases (Case , Case , and Case ), where Case  represents a comparison between two samples from distinct distributions (e.g., land vehicle and grass), Case  represents a comparison between a two-material sample and a sample of one of the two materials (e.g., a spatial transition between tree shadows and surrounding terrain), and Case  represents a comparison between two samples from the same distribution (e.g., grass and grass). Simulations and inspection of these conventional detectors' performances on actual HS data showed that the application of conventional techniques to local anomaly detection problems is flawed. The conventional techniques are developed to account explicitly for Case  and Case , but not for one of the most abundant case-Case . Case  occurs quite often on digital scenes, and represents major transitions of regions, and also strong edges due to the presence of manmade objects in a natural clutter background. Using an indirect comparison technique, described below, Case  is handled in a greatly improved manner.","In an indirect comparison, samples are not compared as individual entities, but as individual entities and the union among these entities. Let X and Y denote two random samples, denote X a reference sample and let Z=X U Y, where U denotes the union. Features of the distribution of X can be indirectly compared to features of the distribution of Y by comparing instead features of the distribution of Z to features of the distribution X. Anomaly detection algorithms based on this principle enjoy the desirable outcome of preserving what is often characterized by image analysts as meaningful detections (e.g., a manmade object in an open terrain), while significantly reducing the number of meaningless detections (e.g., transition of different regions).","Statistical approaches were developed that implement the principle of indirect comparison through a semiparametric model, which is discussed later. This model assumes that the distributions of X and Y (using the denotation above) are related by an exponential distortion. A statistical hypothesis test is then applied to decide whether the exponential distortion is significant. If the distortion is significant, then X and Y are declared anomalous in respect to each other. This model requires that all the components of X and Y are independent, identically distributed (iid) by their corresponding distributions. An alternative detector based on the fundamental behaviors of different mathematical terms in the semiparametric test statistic was also developed; the alternative detector (AsemiP) is based on an iid nonparametric model, which also uses the principle of indirect comparison, and on fundamental theorems of large sample theory. In comparative terms, the AsemiP detector can approximate the performance of the semiparametric detector on actual imagery, and it is significantly easier to implement.","To approximate the iid assumptions by the improved methods using actual HS imagery, an example apparatus was developed that introduces a variability window to generate features from a test patch in the image and from its associated reference window. Features are extracted from the image and labeled as reference or test features by applying a high pass filter (HPF) in the spectral domain and using a metric based on the vector angle between data from the variability window and data from the reference window (Reference-Feature), and on the vector angle between data from the variability window and data from the test window (Test-Feature). The invention constitutes, in this example, the comparison between the information contained in Reference-Feature with the information in Test-Feature using the principle of indirect comparison via the detectors SemiP or AsemiP. This test is performed across the image.","In another example, Reference-Feature is generated by obtaining a priori spectral data from the most abundant object classes (e.g., general terrain, tree leaves, natural rocks), expected in a given scene, and (after the application of a HPF in the spectral domain) computing the vector angles between individual spectra (variability spectra) and the sample mean of these spectra. Notice that contrary to the previous example, the information in Reference-Feature is fixed. The information in Test-Feature is generated by applying a HPF in the imagery's spectral domain and by computing the vector angles between the variability spectra and the sample mean of test spectra from a given location in the digitized scene. Test-Feature is adaptive, since the test is performed across the image. Comparing Reference-Feature and Test Feature, as described in this example, using the principle of indirect comparison via the detectors SemiP or AsemiP constitutes the invention applied to this example.","In these examples, the improved apparatus and methods significantly outperformed conventional detectors performing the same task. The indirect comparison approach reduces the number of nuisance anomaly detections near discontinuities, such as shade boundaries or tree-lines within an image, while accentuating object classes most meaningful for an image analyst (e.g., land vehicle in a natural clutter scene).","Description of improved apparatus and methods are discussed in four parts: A principle of indirect comparison; improved apparatus and method 1, the semiparametric (SemiP) detector; improved apparatus and method 2, the approximation of SemiP (AsemiP) detector; and application to two types of anomaly detection scenarios using actual hyperspectral imagery, one from the top view perspective and another from a ground level view perspective, which includes comparison to alternative conventional techniques.","The advantage of comparing samples using an indirect comparison, in contrast to a conventional method, is illustrated in .","Random samples were simulated in order to show the benefits of indirect comparison in the context of anomaly detection, as shown in . A random sample, by definition, is a sequence of random variables, e.g., X=(X, X, . . . , X), where Xis independent of X(i\u2260j). Two study cases, labeled Case  and Case , are shown in , where Case  depicts the realization of two random samples from different distributions, and Case  depicts the realization of a composite sample and a pure one.",{"@attributes":{"id":"p-0036","num":"0035"},"figref":["FIG. 1A","FIG. 1A"],"b":["1","2"],"sup":["2","2","2","2"],"sub":["i","2","i","i ","i "]},"Let X and S be reference samples and Y be a test sample. A comparison is made of X to Y using the conventional method (i.e., comparing samples as individual entities) and using an improved method according to an example of the present invention, in this example comparing individual entities to the union of entities, and repeating that comparison between S and Y. Comparison between random samples often implies a comparison among the moments and\/or central moments of their distributions, thus, empirical distributions (normalized histograms) of these random samples were computed, and comparisons are possible by visual inspection of the results.","In , under CONVENTIONAL, Case , the empirical distributions of the test sample Y and the reference sample X are shown. Both empirical distributions resemble a relatively tight Gaussian distribution, having the same variance but centered at different means, as expected. By visual inspection, one would expect that statistical methods using the conventional way would be able to distinguish the distribution of Y from the distribution of X\u2014in the bases of their means being so apart, hence, declaring Y as an anomaly in comparison to X. In Case , under CONVENTIONAL, by visual inspection alone, one would also expect Y to be declared as an anomaly in respect to S for the obvious fact that the bimodal distribution of S is quite different from the unimodal distribution of Y. Correct as this declaration may be, it is also unfortunate, because these two study cases are often found together in real image processing problems.","For instance, in a real scenario, Case  could represent a comparison between a random sample X from a motor vehicle and a sample Y from the surrounding natural terrain. Similarly, Case  could represent a comparison between a composite sample (S) from a transition of regions (terrain and tree shadow) and the pure sample (Y) from terrain. A conventional anomaly detector may not be able to distinguish between Case I and Case ; furthermore, in many circumstances, it may even declare Case  a stronger anomaly than Case , yielding instead results that are more comparable to those of edge detectors.","Under UNION in , the empirical distribution of the sample union {circumflex over (Z)} which is bimodal in Case , is quite different from the corresponding unimodal distribution of X. This fact shall preserve the desirable declaration that X and Y are samples from different distributions. What shall not be preserved under UNION, however, is the unfortunate outcome under CONVENTIONAL, Case , since the empirical distributions of the sample union {tilde over (Z)}, in Case , and the composite sample S have the same general characteristics: they are bimodal. Therefore, under UNION, one would expect the differences between X and Y in Case  to be accentuated and the differences between S and Y in Case  to be suppressed, as it would be desired.","The outcomes under both CONVENTIONAL and UNION are expected to be trivial and comparable when both reference and test samples belong to the same distribution, so this case is not discussed further.","The SemiP and AsemiP anomaly detectors are based on this simple principle and produce the desirable outcome of preserving what is often characterized by image analysts as meaningful detections (e.g., a manmade object in an open terrain), while significantly reducing the number of meaningless detections (e.g., transition of different regions, strong edges).","Translating this discussion to an actual scenario, consider an image containing three types of image data, sunlit ground, shaded ground, and objects of interest. A shade boundary runs across part of the image, separating shaded ground and sunlit ground. Supposing the test window includes sunlit ground close to the shade boundary, and the reference window, surrounding the test window, happens to include both shaded and sunlit ground. In a conventional algorithm, the test window (sunlit ground) is compared directly with the reference window (a mixture of shaded and sunlit ground), and an anomaly would be detected as the test window data and reference window data are substantially different. However, in an improved method, the test window data and reference window data are combined, to provide combination data, and the combination is then compared with one of the windows (the test window or reference window). Novel algorithms are described, which provide a softer (less significant) anomaly detection if the combination data and individual window data include common data features. In this example, the test window includes sunlit ground, and the reference window includes some sunlit ground, and so a softer anomaly is detected. However, if the test window includes the object of interest, and the reference window includes sunlit and shaded ground, but not the object of interest, a stronger anomaly is detected.",{"@attributes":{"id":"p-0044","num":"0043"},"figref":["FIG. 2","FIG. 5A"],"b":["10","12","14","16","18","19"]},"In this example, the sampling mechanism uses a test window , guard region , reference window , and variability window . The test window (alternatively referred to as a test cell) provides a spectral sample average ( ) from a (w\u00d7w) window; the reference window (alternatively referred to as a reference cell) provides a spectral sample average ( ) from M vectors surrounding the guard region. The guard region is a blind area between test and reference windows to account for larger than (w\u00d7w) targets. The variability window (also referred to as a variability cell) provides J individual spectral vectors (v) each consisting of k=1, . . . , K spectral responses (\u03bb) for K distinct wavelengths in the visible to SWIR (shortwave infrared) region of the electromagnetic spectrum, for example the region from 0.4 \u03bcm to 2.4 \u03bcm.","The figure shows the test spectrum  obtained from the test window , the reference spectrum  obtained from the reference window , and the plurality of variability spectra  obtained from portions of the variability window. The figure also shows a high-pass filtered test spectrum , discussed further below.","Hyperspectral data have highly correlated\u2014hence, dependent\u2014spatial and spectral clutter, so to promote statistical independence, given that this assumption is made in the models, a high-pass (HP) filter is applied in the spectral domain, thus transforming vinto \u0394, and then \u0394is used to compute a feature, which promotes spatial independence. The rationale for these transformations is discussed further below. The feature is known as spectral angle mapper (SAM), which in essence computes the angle between two vectors, or",{"@attributes":{"id":"p-0048","num":"0047"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msub":{"mi":["x","ij"]},"mo":"=","mrow":{"mfrac":{"mn":"180","mi":"\u03c0"},"mo":"\u2062","mrow":{"mi":"arccos","mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mrow":[{"msubsup":{"mi":["\u0394","j","t"]},"mo":"\u2062","msub":{"mover":{"mi":["\u0394","_"]},"mi":"i"}},{"mrow":[{"mo":["\uf603","\uf604"],"mrow":{"mo":["\uf603","\uf604"],"msub":{"mi":["\u0394","j"]}}},{"mo":["\uf603","\uf604"],"mrow":{"mo":["\uf603","\uf604"],"msub":{"mover":{"mi":["\u0394","_"]},"mi":"i"}}}],"mo":"\u2062"}]}}}}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"1"}}]}}}}},"where, \u0394=(\u03bb\u2212\u03bb, . . . , \u03bb\u2212\u03bb)is the high pass filtered version of v; is the high pass filtered version of ; i=0 (reference cell), 1 (test cell); j=1, . . . , J (J is the number of pixels in the variability cell); x=(x)=(x, . . . , x) is a random sequence of angle differences ranging from 0 to 90 degrees (0 representing minimum class difference between reference and test samples and 90 representing the maximum class difference between these samples); the operator \u2225z\u2225 denotes the squared root of zz; and [\u00b7]denotes the vector transpose operator.  depicts the transformed version of a highly spatially\/spectrally correlated set of hyperspectral samples (as shown in ) from a grassy area in a Californian valley. Transformed data, as shown in , are a good approximation to a set of statistically independent feature values, hence, they can be used as input to the detectors according to examples of the present invention.",{"@attributes":{"id":"p-0050","num":"0049"},"figref":"FIG. 3A","sup":["\u22122","\u22121","\u22121"]},{"@attributes":{"id":"p-0051","num":"0050"},"figref":["FIG. 3B","FIG. 3A"]},"Let xdenote a reference feature sample (alternatively referred to as the reference feature set), xa test feature sample (alternatively referred to as the test feature set), and let both samples be distributed (\u02dc) by unknown joint distributions fand f, respectively, or\n\n=()\u02dc()\n\n=()\u02dc()\u2003\u2003(2)\n","where, n=n=J in this particular implementation.","The window cells are expected to draw samples and to move systematically across the entire imagery, and at each location a detector attempts to answer the following question: Do xand xbelong to the same population, or class?","As it was discussed for the simulated example in , a conventional two-sample hypothesis test would work very well if samples xand xdo belong to distinct classes Cand C, or to one of these classes. Problems occur, however, when one of the samples (e.g., x) belongs to a composite class consisting of both classes Cand C, denote x(CC), and then it is compared to x(C). In those cases, standard statistical tests may reject the hypothesis that x(CC) and x(C) belong to the same class. This rejection\u2014correctly as it may seem\u2014is arguably the most dominant driving force affecting the number of false alarms (FA) produced by most\u2014if not all\u2014local anomaly detectors using sensor imagery. The reason, as discussed, is that region discontinuities (e.g., boundaries between tree clusters and their shadows) are abundant in sensor imagery, and they are not taken into account in conventional statistical models.","The principle of indirect comparison allows circumvention of this problem. The mathematics of novel algorithms (SemiP and AsemiP detectors) using this principle are discussed in details.","Semiparametric (Logistic) Model","Let the random sequences xhave their components independently, identically distributed (iid). Let xbe independent of x. And consider the following:\n\n=()()\n\n=()(),\u2003\u2003(3)\n",{"@attributes":{"id":"p-0059","num":"0058"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mfrac":{"mrow":[{"msub":{"mi":"g","mn":"1"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}},{"msub":{"mi":"g","mn":"0"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}]},"mo":"=","mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"\u03b1","mo":"+","mrow":{"mi":"\u03b2","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"h","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}}}}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}}}}},"where gis regarded as an exponential distortion of gand h(x) is an arbitrary but known function of x. Note in (3)-(4) that no parametric assumption is given to go and that gdepends only on the unknown parameters \u03b1 and \u03b2, hence, justifying the model's name: semiparametric.","The rationale for proposing to use (4) as the baseline, is that many common distribution families can be expressed as a canonical exponential function. These families fall under a category of probability density functions called exponential families, which are known to have many attractive mathematical and statistical properties. Some of these properties are discussed, for instance, in G. Casella and R. L. Berger, Statistical Inference, Belmont, CA: Duxbury Press, 1990, pp. 112-120, 184, 222.) One of these mathematical properties, for example, is that an exponential-family distribution can be expressed as a shift of another exponential-family distribution, as shown in (4). Further details can be found in R. Kay and S. Little, \u201cTransformations of the explanatory variables in the logistic regression model for binary data,\u201d Biometrika, vol. 74, 495-501, 1987.","Using the independence assumptions in model (3)-(4), with h(x)=x, the MLE (maximum likelihood estimate) of \u03b1 and \u03b2 can be attained via the likelihood function,",{"@attributes":{"id":"p-0063","num":"0062"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"mi":"\u03b6","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["\u03b1","\u03b2"],"mo":[",",","],"msub":{"mi":"g","mn":"0"}}}},{"munderover":{"mo":"\u220f","mrow":{"mi":"i","mo":"=","mn":"1"},"msub":{"mi":"n","mn":"0"}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":[{"msub":{"mi":"g","mn":"0"},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"x","mrow":{"mn":"0","mo":"\u2062","mi":"i"}}}},{"munderover":{"mo":"\u220f","mrow":{"mi":"j","mo":"=","mn":"1"},"msub":{"mi":"n","mn":"1"}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":[{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"\u03b1","mo":"+","mrow":{"mi":"\u03b2","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":"x","mrow":{"mn":"1","mo":"\u2062","mi":"j"}}}}}},{"msub":{"mi":"g","mn":"0"},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"x","mrow":{"mn":"1","mo":"\u2062","mi":"j"}}}}],"mo":"\u2062"}}],"mo":"\u2062"}}],"mo":"="}}},{"mtd":{"mrow":{"mo":"=","mrow":{"munderover":{"mo":"\u220f","mrow":[{"mi":"i","mo":"=","mn":"1"},{"mi":"n","mo":"=","mrow":{"msub":[{"mi":"n","mn":"1"},{"mi":"n","mn":"0"}],"mo":"+"}}]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":[{"msub":{"mi":"g","mn":"0"},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["t","i"]}}},{"munderover":{"mo":"\u220f","mrow":{"mi":"j","mo":"=","mn":"1"},"msub":{"mi":"n","mn":"1"}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"\u03b1","mo":"+","mrow":{"mi":"\u03b2","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":"x","mrow":{"mn":"1","mo":"\u2062","mi":"j"}}}}}},"mo":"."}}],"mo":"\u2062"}}}}}]}},{"mrow":{"mo":["(",")"],"mn":"6"}}]}}}}},"where, n=n+nand the sequence of combined features (alternatively referred to as the combined feature set) t is\n\n=()=().\u2003\u2003(7)\n","Notice in (6) that the part involving g(t) (reference distribution) reflects the combined-sample property of a sought model, and the part involving the exponential distortion reflects only the property of the sample that is not the reference. Both properties fit well into a proposed framework of merging samples and then, in some form, comparing this combined structure with one of its original samples.","Notice also that gin (6) is unknown, thus, deriving MLE of \u03b1 and \u03b2 via standard procedures cannot be attained; however, using profiling one can express gin terms of \u03b1 and \u03b2 and then replace gwith its new representation back in (6). Using profiling, the method of Lagrange multiplier has been proposed (see J. A. Anderson, \u201cSeparate sample logistic discrimination,\u201d 59, 19-35, 1972) to attain the maximization of \u03b6 by fixing (\u03b1,\u03b2) and then maximizing \u03b6 with respect to g(t) for i=1, . . . , n, subject to constraints\n\n\u03a3()=1()>0,\n\n\u03a3[exp(\u03b1+\u03b2)\u22121()=0,\u2003\u2003(8)\n","where the last constraint reflects the fact that exp(\u03b1+\u03b2x)g(x) is a distribution function. Following this approach, it can be shown (see J. Qin and B. Zhang, \u201cA goodness of fit test for logistic regression models based on case-control data,\u201d 84, 609-618, 1997) that the maximum value of \u03b6 is attained at",{"@attributes":{"id":"p-0068","num":"0067"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":[{"msub":{"mi":"g","mn":"0"},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["t","i"]}}},{"mfrac":[{"mn":"1","msub":{"mi":"n","mn":"0"}},{"mn":"1","mrow":{"mn":"1","mo":"+","mrow":{"mi":"\u03c1exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"\u03b1","mo":"+","mrow":{"mi":"\u03b2","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["t","i"]}}}}}}}],"mo":"\u2062"}],"mo":"="},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"9"}}]}}}}},"where \u03c1=n\/n, and that ignoring a constant, the log-likelihood function is",{"@attributes":{"id":"p-0070","num":"0069"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"l","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["\u03b1","\u03b2"],"mo":","}}},{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"msub":{"mi":"n","mn":"1"}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mo":["(",")"],"mrow":{"mi":"\u03b1","mo":"+","mrow":{"mi":"\u03b2","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":"x","mrow":{"mn":"1","mo":"\u2062","mi":"i"}}}}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":{"mi":"log","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mn":"1","mo":"+","mrow":{"mi":"\u03c1exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"\u03b1","mo":"+","mrow":{"mi":"\u03b2","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["t","i"]}}}}}}}},"mo":"."}}],"mo":"-"}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"10"}}]}}}}},"A system of score equations that maximizes (10) over (\u03b1,\u03b2) is shown below,",{"@attributes":{"id":"p-0072","num":"0071"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mfrac":{"mrow":[{"mo":"\u2202","mrow":{"mi":"l","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["\u03b1","\u03b2"],"mo":","}}}},{"mo":"\u2202","mi":"\u03b1"}]},"mo":"=","mrow":{"mrow":{"mrow":{"mo":"-","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mfrac":{"mrow":[{"mi":"\u03c1exp","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mi":"\u03b1","mo":"+","mrow":{"mi":"\u03b2","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["t","i"]}}}}},{"mn":"1","mo":"+","mrow":{"mi":"\u03c1exp","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mi":"\u03b1","mo":"+","mrow":{"mi":"\u03b2","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["t","i"]}}}}}}]}}},"mo":"+","msub":{"mi":"n","mn":"1"}},"mo":"=","mn":"0"}}},{"mrow":{"mo":["(",")"],"mn":"11"}}]},{"mtd":[{"mrow":{"mfrac":{"mrow":[{"mo":"\u2202","mrow":{"mi":"l","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["\u03b1","\u03b2"],"mo":","}}}},{"mo":"\u2202","mi":"\u03b2"}]},"mo":"=","mrow":{"mrow":{"mrow":[{"munderover":{"mrow":[{"mo":["-","\u2211"]},{"mi":"i","mo":"=","mn":"1"}],"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mfrac":{"mrow":[{"msub":{"mi":["t","i"]},"mo":"\u2062","mrow":{"mi":"\u03c1exp","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mi":"\u03b1","mo":"+","mrow":{"mi":"\u03b2","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["t","i"]}}}}}},{"mn":"1","mo":"+","mrow":{"mi":"\u03c1exp","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mi":"\u03b1","mo":"+","mrow":{"mi":"\u03b2","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["t","i"]}}}}}}]}},{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"msub":{"mi":"n","mn":"1"}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["x","j"]}}],"mo":"+"},"mo":"=","mn":"0."}}},{"mrow":{"mo":["(",")"],"mn":"12"}}]}]}}}},"Let (\u03b1*,\u03b2*) satisfy (11)-(12), then using (9) it can be shown that the MLE of g(x) is",{"@attributes":{"id":"p-0074","num":"0073"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mover":{"mi":"g","mo":"^"},"mn":"0"},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["t","i"]}}},{"mfrac":{"mn":"1","msub":{"mi":"n","mn":"0"}},"mo":"\u2062","mrow":{"mfrac":{"mn":"1","mrow":{"mn":"1","mo":"+","mrow":{"mi":"\u03c1exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msup":{"mi":"\u03b1","mo":"*"},"mo":"+","mrow":{"msup":{"mi":"\u03b2","mo":"*"},"mo":"\u2062","msub":{"mi":["t","i"]}}}}}}},"mo":"."}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"13"}}]}}}}},"The SemiP Algorithm\u2014Theory Adapted to Object Detection:","The theory described above was adapted to the framework of anomaly detection. The mathematics for this adaptation is nontrivial, as the asymptotic behavior of (\u03b1*,\u03b2*) is employed under a model that does not assume Normality. The important parts of the mathematical development are described below.","First, notice that for g(x)=exp(\u03b1+,\u03b2x)g(x) to be a density, a hypothesis of \u03b2=0 in (4) must imply \u03b1=0, as the term exp(\u03b1) functions as a normalizing factor so that gintegrates over x to a total mass unity. Second, notice that the hypothesis H: \u03b2=0 (given that \u03b1 must also be equal to zero) in (4) implies that a test population and a reference (control) population are equally distributed, namely, g=g. With this logic, one can design an anomaly detector from the following composite hypothesis test:\n\n:\u03b2=0() anomaly absent\n\n:\u03b2\u22600() anomaly present.\u2003\u2003(14)\n","Under this test, local regions in the entire imagery would be individually tested to reject the null hypothesis (H) yielding in the process a binary surface of values of 1, depicting a rejection of H, and values of 0, depicting a non-rejection of H. An isolated object in a scene would be expected to produce a cluster of 1 values (anomalies) in the resulting binary surface. But to design the hypothesis test in (14), one must know the asymptotic behavior of the extremum estimator \u03b2*. Lemma 1 is relevant to estimators based on function maximization with respect to unknown parameters.","Lemma 1. Assumptions:","(i) Let \u0398 be an open subset of the Euclidean K-space. (Thus the true value \u03b8is an interior point of \u0398.)","(ii) Q(y, \u03b8) is a measurable function of vector y for all \u03b8\u03b5\u0398 and \u2202Q\/\u2202\u03b8 exists and is continuous in an open neighborhood N(\u03b8) of \u03b8. (Note that this implies Q(y, \u03b8) is continuous for \u03b8\u03b5N, where T is the sample size.)","(iii) There exists an open neighborhood N(\u03b8) of \u03b8such that TQ(\u03b8) converges to a nonstochastic function Q(\u03b8) in probability uniformly in \u03b8 in N(\u03b8), and Q(\u03b8) attains a strict local maximum at \u03b8.","Let \u0398be set of roots of the equation",{"@attributes":{"id":"p-0084","num":"0083"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mfrac":{"mrow":[{"mo":"\u2202","msub":{"mi":["Q","T"]}},{"mo":"\u2202","mi":"\u03b8"}]},"mo":"=","mn":"0"}},{"mrow":{"mo":["(",")"],"mn":"15"}}]}}}}},"corresponding to the local maxima. If that set is empty, set \u0398equals to {0}.","Then, for any \u03b5>0,",{"@attributes":{"id":"p-0086","num":"0085"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"munder":{"mi":"lim","mrow":{"mi":["T","\u221e"],"mo":"\u2192"}},"mo":"\u2062","mrow":{"mi":"P","mo":["[","]"],"mrow":{"mrow":{"munder":{"mi":"inf","mrow":{"mi":"\u03b8","mo":"\u2208","msub":{"mi":["\u0398","T"]}}},"mo":["\u2062","\u2062"],"mrow":[{"mo":["(",")"],"mrow":{"mi":"\u03b8","mo":"-","msub":{"mi":"\u03b8","mn":"0"}}},{"mo":["(",")"],"mrow":{"mi":"\u03b8","mo":"-","msub":{"mi":"\u03b8","mn":"0"}}}]},"mo":">","mi":"\u025b"}}},"mo":"=","mn":"0."}},{"mrow":{"mo":["(",")"],"mn":"16"}}]}}}}},"In essence, Lemma 1 affirms that there is a consistent root of (15). (Regarding this proof, and also the proof of Theorem 1 following, see T. Amemiya, , Cambridge, Mass.: Harvard U. Press, 1985, pp. 110-112). Under certain conditions, a consistent root of (15) is asymptotically Normal. The affirmation is shown in Theorem 1, where asymptotic convergence is denoted by A\u2192B.","Theorem 1. Assumptions:","(i) All the assumptions of Lemma 1.","(ii)",{"@attributes":{"id":"p-0091","num":"0090"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mfrac":{"mrow":[{"msup":{"mo":"\u2202","mn":"2"},"mo":"\u2062","msub":{"mi":["Q","T"]}},{"mrow":[{"mo":"\u2202","mi":"\u03b8"},{"mo":"\u2202","msup":{"mi":["\u03b8","\u2032"]}}],"mo":"\u2062"}]}}},"br":{},"sub":"o"},"(iii)",{"@attributes":{"id":"p-0093","num":"0092"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mfrac":{"mrow":[{"msup":{"mo":"\u2202","mn":"2"},"mo":"\u2062","msub":{"mi":["Q","T"]}},{"mrow":[{"mo":"\u2202","mi":"\u03b8"},{"mo":"\u2202","msup":{"mi":["\u03b8","\u2032"]}}],"mo":"\u2062"}]}}},"br":{}},{"@attributes":{"id":"p-0094","num":"0093"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"S","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"\u03b8","mn":"0"}}},{"mi":"lim","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"E","mo":"\u2061","mrow":{"mo":["[","]"],"msub":{"mrow":{"msup":{"mi":"T","mrow":{"mo":"-","mn":"1"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mrow":[{"msup":{"mo":"\u2202","mn":"2"},"mo":"\u2062","msub":{"mi":["Q","T"]}},{"mrow":[{"mo":"\u2202","mi":"\u03b8"},{"mo":"\u2202","msup":{"mi":["\u03b8","\u2032"]}}],"mo":"\u2062"}]}}},"msub":{"mi":"\u03b8","mn":"0"}}}}}],"mo":"="}}},"br":{},"sub":["T","T","0"]},{"@attributes":{"id":"p-0095","num":"0094"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"mrow":[{"mrow":{"mo":["(",")"],"mi":"iv"},"mo":["\u2062","\u2062"],"msqrt":{"mi":"T"},"msub":{"mrow":{"mo":["(",")"],"mfrac":{"mrow":[{"mo":"\u2202","msub":{"mi":["Q","T"]}},{"mo":"\u2202","mi":"\u03b8"}]}},"msub":{"mi":"\u03b8","mn":"0"}}},{"mi":"N","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mn":"0","mo":",","mrow":{"mi":"V","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"\u03b8","mn":"0"}}}}}}],"mo":"\u2192"},"mo":",","mi":"where"}},{"mrow":{"mo":["(",")"],"mn":"17"}}]},{"mtd":[{"mrow":{"mrow":[{"mi":"V","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"\u03b8","mn":"0"}}},{"mi":"lim","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":{"mi":"E","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"msub":[{"mrow":{"msup":{"mi":"T","mrow":{"mo":"-","mn":"1"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mrow":[{"mo":"\u2202","msub":{"mi":["Q","T"]}},{"mo":"\u2202","mi":"\u03b8"}]}}},"msub":{"mi":"\u03b8","mn":"0"}},{"mrow":{"mo":["(",")"],"mfrac":{"mrow":[{"mo":"\u2202","msub":{"mi":["Q","T"]}},{"mo":"\u2202","msup":{"mi":["\u03b8","\u2032"]}}]}},"msub":{"mi":"\u03b8","mn":"0"}}],"mo":"\u00d7"}}},"mo":"."}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"18"}}]}]}}},"br":[{},{},{}],"sub":["T","T ","T","0","T","0","0","0","0"],"in-line-formulae":[{},{},{},{}],"i":["T","N","S","V","S"],"sup":["\u22121","\u22121"]},"The semiparametric model's MLE solution satisfies the assumptions of Lemma 1, including of course (15) via (11) and (12). Therefore, by Lemma 1, (\u03b1*,\u03b2*) is consistent and, as seen by Theorem 1, it converges asymptotically to a Normal distribution.","Under H: \u03b2=0 (g=g), the following notation for the moments of t (the combined sample sequence) are used with respect to the reference distribution g:\n\n()\u2261\u222b()\n\n()\u2261()\u2212()\u2003\u2003(21)\n","Let (\u03b1,\u03b2) be the true value of (\u03b1,\u03b2) under model (3)-(4) and assume \u03c1=n\/nremains constant as both nand ngo to infinity. Define",{"@attributes":{"id":"p-0099","num":"0098"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mo":"\u2207","mrow":{"mo":"\u2261","mrow":{"mo":["(",")"],"mrow":{"mfrac":[{"mo":"\u2202","mrow":{"mo":"\u2202","mi":"\u03b1"}},{"mo":"\u2202","mrow":{"mo":"\u2202","mi":"\u03b2"}}],"mo":","}}}}}},"br":{},"sub":["0","0","0","1","0"]},{"@attributes":{"id":"p-0100","num":"0099"},"maths":{"@attributes":{"id":"MATH-US-00015","num":"00015"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":[{"mrow":[{"mrow":{"mo":"-","mfrac":{"mn":"1","mi":"n"}},"mo":"\u2062","mfrac":{"mrow":[{"msup":{"mo":"\u2202","mn":"2"},"mo":"\u2062","mrow":{"mi":"l","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"\u03b1","mn":"0"},{"mi":"\u03b2","mn":"0"}],"mo":","}}}},{"mrow":[{"mo":"\u2202","mi":"\u03b1"},{"mo":"\u2202","mi":"\u03b2"}],"mo":"\u2062"}]}},{"msub":{"mi":"K","mn":"1"},"mo":"\u2062","mrow":{"mo":"\u222b","mrow":{"mfrac":{"mrow":[{"mi":"t","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"\u03b1","mn":"0"},"mo":"+","mrow":{"msub":{"mi":"\u03b2","mn":"0"},"mo":"\u2062","mi":"t"}}}}},{"mn":"1","mo":"+","mrow":{"mi":"\u03c1","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"\u03b1","mn":"0"},"mo":"+","mrow":{"msub":{"mi":"\u03b2","mn":"0"},"mo":"\u2062","mi":"t"}}}}}}]},"mo":["\u2062","\u2062"],"mrow":[{"msub":{"mi":"g","mn":"0"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"mo":"\u2146","mi":"t"}]}}}],"mo":"\u2192"},{"mrow":[{"msub":{"mi":"K","mn":"2"},"mo":"\u2062","mrow":{"mo":"\u222b","mrow":{"mrow":[{"mi":"t","mo":"\u00b7","mrow":{"mrow":[{"msub":{"mi":"g","mn":"0"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"mo":["[","]"],"mrow":{"mrow":[{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"\u03b1","mn":"0"},"mo":"+","mrow":{"msub":{"mi":"\u03b2","mn":"0"},"mo":"\u2062","mi":"t"}}}},{"msub":{"mi":"g","mn":"0"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}],"mo":"\u2062"}}],"mo":"\u2061"}},{"mo":"\u2146","mi":"t"}],"mo":"\u2062"}}},{"mfrac":{"mi":"\u03c1","mrow":{"mn":"1","mo":"+","mi":"\u03c1"}},"mo":"\u2062","mrow":{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}}],"mo":"="}],"mo":"="},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"22"}}]}}}}},"where Kand Kare constants involving (n, n) and \u03c1\/(1+\u03c1)=n\/n (where n=n+n). Using similar argument to arrive at (22) and applying the weak law of large numbers (WLLN) (see, for instance, E. L. Lehmann, Theory of Point Estimation, Pacific Grove, CA: Wadsworth & Brooks, 1991, pp. 333-336, and Chapter 5), one can use assumption (iii) in Theorem 1 to recognize that",{"@attributes":{"id":"p-0102","num":"0101"},"maths":{"@attributes":{"id":"MATH-US-00016","num":"00016"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":{"mrow":[{"mo":"-","mfrac":{"mn":"1","mi":"n"}},{"mo":"\u2207","mrow":{"mo":"\u2207","mrow":{"mrow":[{"msup":{"mo":"\u2009","mi":"\u2032"},"mo":"\u2062","mi":"l"},{"mo":["(",")"],"mrow":{"msub":[{"mi":"\u03b1","mn":"0"},{"mi":"\u03b2","mn":"0"}],"mo":","}}],"mo":"\u2061"}}}],"mo":"\u2062"},"mo":"\u2192","mi":"S"},{"mfrac":{"mi":"\u03c1","mrow":{"mn":"1","mo":"+","mi":"\u03c1"}},"mo":"\u2062","mrow":{"mo":["(",")"],"mtable":{"mtr":[{"mtd":[{"mn":"1"},{"mrow":{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}}]},{"mtd":[{"mrow":{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}},{"mrow":{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":"t","mn":"2"}}}}]}]}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"23"}}]}}}}},"in probability as n\u2192\u221e. It follows that S is nonsingular and its inverse is",{"@attributes":{"id":"p-0104","num":"0103"},"maths":{"@attributes":{"id":"MATH-US-00017","num":"00017"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"S","mrow":{"mo":"-","mn":"1"}},"mo":"=","mrow":{"mfrac":{"mn":"1","mrow":{"mrow":[{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":"t","mn":"2"}}},{"msup":{"mi":"E","mn":"2"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}],"mo":"-"}},"mo":["\u2062","\u2062"],"mrow":[{"mo":["(",")"],"mtable":{"mtr":[{"mtd":[{"mrow":{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":"t","mn":"2"}}}},{"mrow":{"mo":"-","mrow":{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}}}]},{"mtd":[{"mrow":{"mo":"-","mrow":{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}}},{"mn":"1"}]}]}},{"mfrac":{"mrow":{"mn":"1","mo":"+","mi":"\u03c1"},"mi":"\u03c1"},"mo":"."}]}}},{"mrow":{"mo":["(",")"],"mn":"24"}}]}}}}},"Our interest is only in the parameter \u03b2, so, let S denote the lower-right component of the expanded version of Sand use (21) to obtain",{"@attributes":{"id":"p-0106","num":"0105"},"maths":{"@attributes":{"id":"MATH-US-00018","num":"00018"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["S","\u03b2"]},"mo":"=","mrow":{"mrow":[{"mfrac":[{"mn":"1","mrow":{"mrow":[{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":"t","mn":"2"}}},{"msup":{"mi":"E","mn":"2"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}],"mo":"-"}},{"mrow":{"mn":"1","mo":"+","mi":"\u03c1"},"mi":"\u03c1"}],"mo":"\u2062"},{"mfrac":{"mn":"1","mrow":{"mi":"Var","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}},"mo":"\u2062","mrow":{"mfrac":{"mrow":{"mn":"1","mo":"+","mi":"\u03c1"},"mi":"\u03c1"},"mo":"."}}],"mo":"="}}},{"mrow":{"mo":["(",")"],"mn":"25"}}]}}}}},"Using also the application of the central limiting theorem (CLT) in Theorem 1 (iv) and the fact that\n\n(\u03b1,\u03b2)]=0,\u2003\u2003(26)\n","from (11) and (12), one can write\n\n\u221a{square root over ()}[\u2207(\u03b1,\u03b2)]\u2192[0(\u03b1,\u03b2)],\u2003\u2003(27)\n","where",{"@attributes":{"id":"p-0110","num":"0109"},"maths":{"@attributes":{"id":"MATH-US-00019","num":"00019"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"V","mo":"\u2062","mrow":{"mo":"\u2009","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"\u03b1","mn":"0"},{"mi":"\u03b2","mn":"0"}],"mo":","}}}},{"mrow":[{"mfrac":{"mi":"\u03c1","mrow":{"mn":"1","mo":"+","mi":"\u03c1"}},"mo":"\u2061","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mn":"1"},{"mrow":{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}}]},{"mtd":[{"mrow":{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}},{"mrow":{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":"t","mn":"2"}}}}]}]}}},{"mrow":[{"mi":"\u03c1","mo":"\u2061","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mn":"1"}},{"mtd":{"mrow":{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}}}]}}},{"mo":["[","]"],"mtable":{"mtr":{"mtd":[{"mn":"1"},{"mrow":{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}}]}}}],"mo":"\u2061"}],"mo":"-"}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"28"}}]}}}}},"V(\u03b1,\u03b2) is a direct result from (18), see, for instance, J. Qin and B. Zhang, \u201cA goodness of fit test for logistic regression models based on case-control data,\u201d 84, 609-618, 1997. Using the conclusion of Theorem 1, or (19)-(20), in terms of S in (25) and the lower-right component of the expanded version of V(\u03b1,\u03b2) in (28), one can verify that",{"@attributes":{"id":"p-0112","num":"0111"},"maths":{"@attributes":{"id":"MATH-US-00020","num":"00020"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msqrt":{"mi":"n"},"mo":"\u2062","msup":{"mi":"\u03b2","mo":"*"}},{"mi":"N","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"0","mo":",","mfrac":{"msup":{"mrow":{"msup":{"mi":"\u03c1","mrow":{"mo":"-","mn":"1"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"+","mi":"\u03c1"}}},"mn":"2"},"mrow":{"mi":"Var","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}}}}}],"mo":"\u2192"}},{"mrow":{"mo":["(",")"],"mn":"29"}}]}}}}},"and having the left side of (29) normalized by the asymptotic variance and then squared, one can conclude (see convergence results, for instance, in G. Casella and R. L. Berger, Statistical Inference, Belmont, CA: Duxbury Press, 1990, pp. 112-120, 184, 222) that the resulting random variable\n\n\u03c7=\u03c1(1+\u03c1)\u03b2**()\u2192\u03c7\u2003\u2003(30)\n","converges to a chi square distribution with 1 degree of freedom, where V*(t) estimates Var(t). A multivariate solution is presented in K. Fokianos, et al., \u201cA semiparametric approach to the one-way layout,\u201d , pp. 56-65, 2001.","The expression in (30) constitutes the SemiP anomaly detector.","Statistical Independence Assumptions:","Model (3)-(4) is clearly based on idealized assumptions, which in the context of relatively high resolution imagery, the best one could hope for is that in the presence of certain types of terrain those assumptions would not be grossly violated. The assumptions dictate that not only xmust be statistically independent of x, their corresponding components xand x(j=1, . . . , J) must be iid. For those assumptions not to be violated in HSI, the information in the spatial domain must be independent, as well as the information in the spectral domain.","Above, two transformations were described, aimed at promoting statistical independence in both domains: apply a HP (high pass) filter in the spectral domain, followed by a spatial SAM. These transformations use the same basic idea: Let the random variables y, yand ybe statistically dependent and let z=y\u2212yand z=y\u2212y. It can be shown (see, for instance, B. Kedem, Time Series Analysis by Higher Order Crossings, New York: IEEE Press, 1994, pp. 260-266) that z, is statistically independent of z. This transformation is widely used by professional statisticians so that dependent random variables can be addressed using techniques based on statistical independence. The spectral information was high-pass filtered (\u0394=\u03bb\u2212\u03bb) prior to the application of SAM, so that SAM interpreted as the angular difference between two vectors. Hence, it can be shown that the spatial angular differences between two highly correlated random vectors then yield independent random variables.","Implementation of SemiP Detector","Referring again to , this shows a nonhomogeneous, multicomponent scene from a hyperspectral digital imagery collection experiment (HYDICE). A pixel in a HYDICE imagery is represented by a vector.) Typically, local anomaly detectors produce an intolerable high number of false alarms (non-anomalies) in similar scenes; local region discontinuities degrade detectors' performances.","Implementation of the SemiP algorithm is described below:","1) Sampling Mechanism: Use the mechanism described above to sample a pair of random feature vectors x(i=0 [reference], 1 [test]; j=1, . . . , J) from HSI. A 9-pixel (3\u00d73) test window, a 56-pixel reference window, and a 60-pixel variability window were used, as shown in . Note that the size of the variability window determines the size of the feature vectors, that is, xand x; have the same size, J=60. In other examples, the variability window and reference window can be partially or entirely co-extensive.","2) Statistical Independence: An attempt should be made to promote statistical independence in HSI, discussed further elsewhere.","3) Function Maximization: Perform an unconstrained maximization of l(\u03b1,\u03b2) in (10), or minimization of [\u2212l(\u03b1,\u03b2)], to obtain the extremum estimates (\u03b1*,\u03b2*). A standard unconstrained minimization routines available in MATLAB\u2122 software (i.e., fminsearch) can be used, and the initial values of (\u03b1,\u03b2) are set to (0,0).","4) Variance Under the Null Hypothesis: V*(t) in (30) should be computed using (13) and a discrete version estimate of (21):\n\n()=\u03a3()*()=()\u2212()\u2003\u2003(31)\n","5) Decision Threshold: Using (30), high values of \u03c7 reject hypothesis H, hence, detecting anomalies. Set a decision threshold based on the Type I error, i.e., based on the probability of rejecting Hgiven that His true. Using a standard integral table for the chi square distribution, with 1 degree of freedom, find a threshold that yields an acceptable probability of error (e.g., 0.001), or alternatively find and use a suitable threshold that yields a value at the knee of the SemiP's corresponding ROC curve.","The AsemiP Algorithm","In reference to (30), there are two major factors working in harmony and in complementary fashion to promote maximum separation between signal (anomalies) and noise (non-anomalies), they are: the squared value of \u03b2* and the estimated combined variance, V*(t), which is also quadratic.","These factors work in the following way: When two samples from the same class are compared (i.e., is H: \u03b2=0 [g=g] true?), the term (\u03b2*)tends to approach zero very fast, especially for \u03b2* values less than unity. On the other hand, if two samples from distinct classes are compared, the term V*(t) tends to a relatively high number, also very fast, asserting the fact that a combined sample vector t consists of components belonging to distinct populations.","Motivated by these properties, an approximation algorithm is stated and proved, based on large sample theory that replaces complicated SemiP equations with simpler ones describing the same phenomenon.","Proposition 1 (AsemiP Algorithm). Let\n\n=() be ()=\u03bc()=\u03c3<\u221e;\n\n=() be ()=\u03bc(x)=\u03c3<\u221e;\u2003\u2003(32)\n","assume that xand xare independent and that, for some xand x, the combined sequence t\n\n=()=() is ()=\u03c3<\u221e;\u2003\u2003(33)\n","and define",{"@attributes":{"id":"p-0134","num":"0133"},"maths":{"@attributes":{"id":"MATH-US-00021","num":"00021"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":[{"mover":{"mi":"\u03b2","mo":"~"},"mo":"\u2261","mrow":{"msub":[{"mi":"\u03bc","mn":"1"},{"mi":"\u03bc","mn":"0"}],"mo":"-"}},{"msub":{"mi":"\u03b6","mn":"1"},"mo":"\u2261","mfrac":{"msup":{"mi":"\u03c3","mn":"2"},"msubsup":{"mi":"\u03c3","mn":["0","2"]}}}],"mo":";"},{"msub":{"mi":"\u03b6","mn":"2"},"mo":"\u2261","mrow":{"mfrac":{"msup":{"mi":"\u03c3","mn":"2"},"msubsup":{"mi":"\u03c3","mn":["1","2"]}},"mo":"."}}],"mo":","}},{"mrow":{"mo":["(",")"],"mn":"34"}}]}}}}},"Under some regularity conditions, if hypothesis H: ({tilde over (\u03b2)}=0; \u03b6=\u03b6=1) is true and",{"@attributes":{"id":"p-0136","num":"0135"},"maths":{"@attributes":{"id":"MATH-US-00022","num":"00022"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mo":"\u2062","mrow":{"mrow":[{"mover":{"mover":{"mi":"\u03b2","mo":"~"},"mo":"^"},"mo":"=","mrow":{"msub":[{"mover":{"mi":["x","_"]},"mn":"1"},{"mover":{"mi":["x","_"]},"mn":"0"}],"mo":"-"}},{"msub":{"mover":{"mi":["x","_"]},"mi":"i"},"mo":"=","mrow":{"msubsup":{"mi":["n","i"],"mrow":{"mo":"-","mn":"1"}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"1"},"msub":{"mi":["n","i"]}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["x","ik"]}}}},{"mi":"i","mo":"=","mn":"0"},{"mn":"1","mo":";"}],"mo":[",",",",","]}}},{"mrow":{"mo":["(",")"],"mn":"35"}}]},{"mtd":[{"mrow":{"mrow":[{"mrow":[{"mover":{"mi":"V","mo":"~"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msup":{"mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["t","i"]},"mo":"-","mover":{"mi":["t","_"]}}},"mn":"2"},"mo":"\u00b7","mrow":{"mi":"g","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"x","mn":"1"},{"mi":"x","mn":"0"}],"mo":","}}}}}],"mo":"="},{"mi":"n","mo":"=","mrow":{"msub":[{"mi":"n","mn":"1"},{"mi":"n","mn":"0"}],"mo":"+"}},{"mrow":[{"mi":"where","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mover":{"mi":["t","_"]}},{"msup":{"mi":"n","mrow":{"mo":"-","mn":"1"}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["t","i"]}}}],"mo":"="}],"mo":[",",",",","]}},{"mrow":{"mo":["(",")"],"mn":"36"}}]},{"mtd":[{"mrow":{"mrow":[{"mrow":{"mi":"g","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"x","mn":"1"},{"mi":"x","mn":"0"}],"mo":","}}},"mo":"=","mfrac":{"msup":[{"mrow":{"mo":["(",")"],"mrow":{"mi":"n","mo":"-","mn":"2"}},"mn":"2"},{"mrow":{"mo":["[","]"],"mrow":{"mrow":[{"munderover":{"mover":{"mo":"\u2211","msub":{"mi":"n","mn":"1"}},"mrow":{"mi":"i","mo":"=","mn":"1"},"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"x","mrow":{"mn":"1","mo":"\u2062","mi":"i"}},{"mover":{"mi":["x","_"]},"mn":"1"}],"mo":"-"}},"mn":"2"}},{"munderover":{"mover":{"mo":"\u2211","msub":{"mi":"n","mn":"0"}},"mrow":{"mi":"i","mo":"=","mn":"1"},"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"x","mrow":{"mn":"0","mo":"\u2062","mi":"i"}},{"mover":{"mi":["x","_"]},"mn":"0"}],"mo":"-"}},"mn":"2"}}],"mo":"+"}},"mn":"2"}]}},{"mrow":{"mi":"and","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mover":{"mi":"\u03c1","mo":"~"}},"mo":"=","msup":{"mrow":[{"mo":["(",")"],"mrow":{"mfrac":[{"mn":"1","msub":{"mi":"n","mn":"1"}},{"mn":"1","msub":{"mi":"n","mn":"0"}}],"mo":"+"}},{"mo":"-","mn":"1"}]}}],"mo":[";",";"]}},{"mrow":{"mo":["(",")"],"mn":"37"}}]}]}}}},"where, {tilde over ({circumflex over (\u03b2)}, is an estimate of {tilde over (\u03b2)}, then the random variable\n\n{tilde over (\u03c7)}={tilde over (\u03c1)}(1){tilde over ({circumflex over (\u03b2)}()\u2192\u03c7\u2003\u2003(38)\n\nconverges in distribution to a chi-squared distribution with 1 degree of freedom.\n","By inspection of (38), one should readily recognize the behavior of our chosen function {tilde over (\u03b2)}, in Proposition 1, as it approximates the behavior of \u03b2. If two samples from the same population are compared using (38), the estimate of {tilde over (\u03b2)}, {tilde over ({circumflex over (\u03b2)} in (35), would also tend to approach zero\u2014as the sample size increases, and tend otherwise for samples belonging to distinct populations.","A relatively simple estimate of Var(t), as defined in (21), is derived to replace V*(t), as shown in (30). Var(t) is a sum of squared errors individually weighted by their probability of occurrence. In Proposition 1, g(x, x) is proposed to provide that probability feature, but as an average probability of occurrence, instead. In this sense, comparing two samples from distinct populations would produce very high cumulative square errors using the combined vector t, but appropriately weighted by an average proportion.","In principle, the overall behavior of (38) seems to track that of (30), and both random variables are asymptotically identically distributed under \u03c7. Note that the AsemiP's performance will not asymptotically approach that of the SemiP's performance, as the number of samples increases; the former approximates the general behavior of the latter, i.e., it promotes a high separation between meaningful signals (isolated objects) from noise (homogeneous and non-homogenous local regions).","Proof: If hypothesis H: ({tilde over (\u03b2)}=\u03b6=\u03b6=1) is true in Proposition 1, then \u03c3=\u03c3=\u03c3and, using the independent assumptions of xand x, and CLT, it follows that",{"@attributes":{"id":"p-0142","num":"0141"},"maths":{"@attributes":{"id":"MATH-US-00023","num":"00023"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mfrac":{"mover":{"mover":{"mi":"\u03b2","mo":"~"},"mo":"^"},"mrow":{"msqrt":{"mrow":{"mfrac":[{"mn":"1","msub":{"mi":"n","mn":"0"}},{"mn":"1","msub":{"mi":"n","mn":"1"}}],"mo":"+"}},"mo":"\u2062","msub":{"mi":"\u03c3","mn":"0"}}},"mo":"=","mrow":{"mfrac":{"mover":{"mover":{"mi":"\u03b2","mo":"~"},"mo":"^"},"mrow":{"msqrt":{"mrow":{"mfrac":[{"mn":"1","msub":{"mi":"n","mn":"0"}},{"mn":"1","msub":{"mi":"n","mn":"1"}}],"mo":"+"}},"mo":"\u2062","msub":{"mi":"\u03c3","mn":"1"}}},"mo":["\u2062","\u2062"],"munder":{"mo":"\u2192","munder":{"mrow":[{"msub":{"mi":"n","mn":"0"},"mo":"\u2192","mi":"\u221e"},{"msub":{"mi":"n","mn":"1"},"mo":"\u2192","mi":"\u221e"}]}},"mrow":{"mi":"N","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":["0","1"],"mo":","}}}}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"39"}}]}}}},"br":{},"sub":["1","0"],"sup":["2 ","2 "]},{"@attributes":{"id":"p-0143","num":"0142"},"maths":{"@attributes":{"id":"MATH-US-00024","num":"00024"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msubsup":{"mi":"S","mn":["1","2"]},"mo":"=","mfrac":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"msub":{"mi":"n","mn":"1"}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"x","mrow":{"mn":"1","mo":"\u2062","mi":"i"}},{"mover":{"mi":["x","_"]},"mn":"1"}],"mo":"-"}},"mn":"2"}},{"msub":{"mi":"n","mn":"1"},"mo":"-","mn":"1"}]}},{"mrow":[{"mi":"and","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"msubsup":{"mi":"S","mn":["0","2"]}},{"mfrac":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"msub":{"mi":"n","mn":"0"}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"x","mrow":{"mn":"0","mo":"\u2062","mi":"i"}},{"mover":{"mi":["x","_"]},"mn":"0"}],"mo":"-"}},"mn":"2"}},{"msub":{"mi":"n","mn":"0"},"mo":"-","mn":"1"}]},"mo":"."}],"mo":"="}],"mo":","}},{"mrow":{"mo":["(",")"],"mn":"40"}}]}}}}},"Using both samples xand x, let the following be another estimator of \u03c3(or \u03c3), given that under H\u03c3=\u03c3,",{"@attributes":{"id":"p-0145","num":"0144"},"maths":{"@attributes":{"id":"MATH-US-00025","num":"00025"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"S","mn":"2"},"mo":"=","mrow":{"mfrac":{"mrow":[{"mrow":[{"mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"n","mn":"1"},"mo":"-","mn":"1"}},"mo":"\u2062","msubsup":{"mi":"S","mn":["1","2"]}},{"mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"n","mn":"0"},"mo":"-","mn":"1"}},"mo":"\u2062","msubsup":{"mi":"S","mn":["0","2"]}}],"mo":"+"},{"mrow":[{"mo":["(",")"],"mrow":{"msub":{"mi":"n","mn":"1"},"mo":"-","mn":"1"}},{"mo":["(",")"],"mrow":{"msub":{"mi":"n","mn":"0"},"mo":"-","mn":"1"}}],"mo":"+"}]},"mo":"."}}},{"mrow":{"mo":["(",")"],"mn":"41"}}]}}}}},"The estimator Sis unbiased under H, as its expected value E[S] is equal to \u03c3and \u03c3:",{"@attributes":{"id":"p-0147","num":"0146"},"maths":{"@attributes":{"id":"MATH-US-00026","num":"00026"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":"S","mn":"2"}}},"mo":["=","\u2062"],"mi":{},"mfrac":{"mrow":[{"mrow":[{"mrow":[{"mo":["(",")"],"mrow":{"msub":{"mi":"n","mn":"1"},"mo":"-","mn":"1"}},{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":"S","mn":["1","2"]}}}],"mo":"\u2062"},{"mrow":[{"mo":["(",")"],"mrow":{"msub":{"mi":"n","mn":"0"},"mo":"-","mn":"1"}},{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":"S","mn":["0","2"]}}}],"mo":"\u2062"}],"mo":"+"},{"mrow":[{"mo":["(",")"],"mrow":{"msub":{"mi":"n","mn":"1"},"mo":"-","mn":"1"}},{"mo":["(",")"],"mrow":{"msub":{"mi":"n","mn":"0"},"mo":"-","mn":"1"}}],"mo":"+"}]}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mfrac":{"mrow":[{"mrow":[{"mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"n","mn":"1"},"mo":"-","mn":"1"}},"mo":"\u2062","msubsup":{"mi":"\u03c3","mn":["1","2"]}},{"mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"n","mn":"0"},"mo":"-","mn":"1"}},"mo":"\u2062","msubsup":{"mi":"\u03c3","mn":["0","2"]}}],"mo":"+"},{"mrow":[{"mo":["(",")"],"mrow":{"msub":{"mi":"n","mn":"1"},"mo":"-","mn":"1"}},{"mo":["(",")"],"mrow":{"msub":{"mi":"n","mn":"0"},"mo":"-","mn":"1"}}],"mo":"+"}]}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"msubsup":{"mi":"\u03c3","mn":["0","2"]},"mo":"."}}}}]}},{"mrow":{"mo":["(",")"],"mn":"42"}}]}}}}},"This is true because Sand Sare consistent estimators and, under H, \u03c3=\u03c3. A WLLN is now proved for Sto verify that Sis also a consistent estimator. Using Chebychev's inequality, under H:",{"@attributes":{"id":"p-0149","num":"0148"},"maths":{"@attributes":{"id":"MATH-US-00027","num":"00027"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"msup":{"mi":"S","mn":"2"},"mo":"-","msubsup":{"mi":"\u03c3","mn":["0","2"]}}},"mo":"\u2265","mi":"\u025b"}}},"mo":"\u2264","mfrac":{"msup":[{"mrow":{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msup":{"mi":"S","mn":"2"},"mo":"-","msubsup":{"mi":"\u03c3","mn":["0","2"]}}}},"mn":"2"},{"mi":"\u025b","mn":"2"}]}},"mo":"=","mfrac":{"mrow":{"mi":"Var","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":"S","mn":"2"}}},"msup":{"mi":"\u025b","mn":"2"}}}},{"mrow":{"mo":["(",")"],"mn":"43"}}]}}}}},"and, thus, a sufficient condition that Sconverges in probability to \u03c3, or \u03c3,is that",{"@attributes":{"id":"p-0151","num":"0150"},"maths":{"@attributes":{"id":"MATH-US-00028","num":"00028"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mi":"Var","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":"S","mn":"2"}}},"mo":["\u2062","\u2062"],"munder":{"mo":"\u2192","mrow":{"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"n","mn":"0"},{"mi":"n","mn":"1"}],"mo":","}},"mo":"\u2192","mi":"\u221e"}},"mn":"0."}}}},"Note that Var(S) can be expressed as",{"@attributes":{"id":"p-0153","num":"0152"},"maths":{"@attributes":{"id":"MATH-US-00029","num":"00029"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"Var","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":"S","mn":"2"}}},{"mrow":[{"msup":{"mrow":{"mo":["[","]"],"mfrac":{"mrow":[{"mo":["(",")"],"mrow":{"msub":{"mi":"n","mn":"1"},"mo":"-","mn":"1"}},{"mrow":[{"mo":["(",")"],"mrow":{"msub":{"mi":"n","mn":"1"},"mo":"-","mn":"1"}},{"mo":["(",")"],"mrow":{"msub":{"mi":"n","mn":"0"},"mo":"-","mn":"1"}}],"mo":"+"}]}},"mn":"2"},"mo":"\u2062","mrow":{"mi":"Var","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":"S","mn":["1","2"]}}}},{"msup":{"mrow":{"mo":["[","]"],"mfrac":{"mrow":[{"mo":["(",")"],"mrow":{"msub":{"mi":"n","mn":"0"},"mo":"-","mn":"1"}},{"mrow":[{"mo":["(",")"],"mrow":{"msub":{"mi":"n","mn":"1"},"mo":"-","mn":"1"}},{"mo":["(",")"],"mrow":{"msub":{"mi":"n","mn":"0"},"mo":"-","mn":"1"}}],"mo":"+"}]}},"mn":"2"},"mo":"\u2062","mrow":{"mi":"Var","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":"S","mn":["0","2"]}}}}],"mo":"+"}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"44"}}]}}}}},"and, as Sand Sare both consistent estimators, their variances must converge to zero,",{"@attributes":{"id":"p-0155","num":"0154"},"maths":{"@attributes":{"id":"MATH-US-00030","num":"00030"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"mrow":[{"mrow":{"mi":"Var","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":"S","mn":["1","2"]}}},"mo":["\u2062","\u2062"],"munder":{"mo":"\u2192","mrow":{"msub":{"mi":"n","mn":"1"},"mo":"\u2192","mi":"\u221e"}},"mn":"0"},{"mrow":{"mi":"Var","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":"S","mn":["0","2"]}}},"mo":["\u2062","\u2062"],"munder":{"mo":"\u2192","mrow":{"msub":{"mi":"n","mn":"0"},"mo":"\u2192","mi":"\u221e"}},"mn":"0"}],"mo":[";","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},"mo":[",","\u2062"],"mstyle":{"mtext":{}},"mi":"also"}},{"mrow":{"mo":["(",")"],"mn":"45"}}]},{"mtd":[{"mrow":{"mrow":{"mrow":[{"msup":{"mrow":{"mo":["[","]"],"mfrac":{"mrow":[{"mo":["(",")"],"mrow":{"msub":{"mi":["n","k"]},"mo":"-","mn":"1"}},{"mrow":[{"mo":["(",")"],"mrow":{"msub":{"mi":"n","mn":"1"},"mo":"-","mn":"1"}},{"mo":["(",")"],"mrow":{"msub":{"mi":"n","mn":"0"},"mo":"-","mn":"1"}}],"mo":"+"}]}},"mn":"2"},"mo":["\u2062","\u2062"],"munder":{"mo":"\u2192","mrow":{"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"n","mn":"1"},{"mi":"n","mn":"0"}],"mo":","}},"mo":"\u2192","mi":"\u221e"}},"mn":"0"},{"mi":"k","mo":"=","mn":"0"}],"mo":";"},"mo":",","mn":"1."}},{"mrow":{"mo":["(",")"],"mn":"46"}}]}]}}}},"Then",{"@attributes":{"id":"p-0157","num":"0156"},"maths":{"@attributes":{"id":"MATH-US-00031","num":"00031"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mi":"Var","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":"S","mn":"2"}}},"mo":["\u2062","\u2062"],"munder":{"mo":"\u2192","mrow":{"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"n","mn":"1"},{"mi":"n","mn":"0"}],"mo":","}},"mo":"\u2192","mi":"\u221e"}},"mn":"0"}}},"br":{},"sub":"0"},{"@attributes":{"id":"p-0158","num":"0157"},"maths":{"@attributes":{"id":"MATH-US-00032","num":"00032"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mfrac":{"msubsup":{"mi":"\u03c3","mn":["0","2"]},"msup":{"mi":"S","mn":"2"}},"mo":"=","mrow":{"mfrac":{"msubsup":{"mi":"\u03c3","mn":["1","2"]},"msup":{"mi":"S","mn":"2"}},"mo":"\u2192","mn":"1"}},{"mrow":[{"mi":"as","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"n","mn":"0"},{"mi":"n","mn":"1"}],"mo":","}}},{"mi":"\u221e","mo":"."}],"mo":"\u2192"}],"mo":","}},{"mrow":{"mo":["(",")"],"mn":"47"}}]}}}}},"Using the same argument to arrive at (47), it can also be shown that under H:",{"@attributes":{"id":"p-0160","num":"0159"},"maths":{"@attributes":{"id":"MATH-US-00033","num":"00033"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mfrac":{"msubsup":[{"mi":["S","t"],"mn":"2"},{"mi":"\u03c3","mn":["0","2"]}]},"mo":"=","mrow":{"mrow":[{"mfrac":{"msubsup":[{"mi":["S","t"],"mn":"2"},{"mi":"\u03c3","mn":["1","2"]}]},"mo":"\u2192","msub":{"mi":"\u03b6","mn":"1"}},{"msub":{"mi":"\u03b6","mn":"2"},"mo":"=","mn":"1"}],"mo":"="}},{"mrow":[{"mi":["as","n"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},{"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"n","mn":"1"},{"mi":"n","mn":"0"}],"mo":"+"}},"mo":"\u2192","mi":"\u221e"}],"mo":"="}],"mo":[",",","]}},{"mrow":{"mo":["(",")"],"mn":"48"}}]}}}},"br":{},"sub":["t","0"],"sup":["2 ","2 "]},{"@attributes":{"id":"p-0161","num":"0160"},"maths":{"@attributes":{"id":"MATH-US-00034","num":"00034"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msubsup":{"mi":["S","t"],"mn":"2"},"mo":"=","mrow":{"msup":{"mrow":[{"mo":["(",")"],"mrow":{"mi":"n","mo":"-","mn":"1"}},{"mo":"-","mn":"1"}]},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["t","i"]},"mo":"-","mover":{"mi":["t","_"]}}},"mn":"2"}}}},{"mover":{"mi":["t","_"]},"mo":"=","mrow":{"msup":{"mi":"n","mrow":{"mo":"-","mn":"1"}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":["t","i"]},"mo":"."}}}}],"mo":","}},{"mrow":{"mo":["(",")"],"mn":"49"}}]}}}}},"Furthermore, since Sis the sample variance of t (the combined vector) under H, one can readily verify that",{"@attributes":{"id":"p-0163","num":"0162"},"maths":{"@attributes":{"id":"MATH-US-00035","num":"00035"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mfrac":{"msub":[{"mi":["S","t"]},{"mi":"\u03c3","mn":"0"}]},"mo":"=","mrow":{"mfrac":{"msub":[{"mi":["S","t"]},{"mi":"\u03c3","mn":"1"}]},"mo":"\u2192","mn":"1"}},{"mrow":[{"mi":["as","n"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},{"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"n","mn":"1"},{"mi":"n","mn":"0"}],"mo":"+"}},"mo":"\u2192","mi":"\u221e"}],"mo":"="}],"mo":","}},{"mrow":{"mo":["(",")"],"mn":"50"}}]}}}}},"To finalize the proof, consider Theorem 2 below.","Theorem 2 (Slutsky). Let Xtend to X in distribution and Ytend to c in probability, where c is a finite constant. Then","(i) X+Ytend to X+c in distribution;","(ii) XYtend to cX in distribution;","(iii) X\/Ytend to X\/c in distribution, if c is not zero.","(See, for example, proof in R. J. Serfling, Approximation Theorems of Mathematical Statistics, New York: Wiley, 1980, pp. 19).","Using (39), (47), (50) and the Slutsky Theorem, it is concluded that",{"@attributes":{"id":"p-0171","num":"0170"},"maths":{"@attributes":{"id":"MATH-US-00036","num":"00036"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":[{"mo":["[","]"],"mrow":{"mrow":[{"mo":["(",")"],"mfrac":{"mover":{"mover":{"mi":"\u03b2","mo":"^"},"mo":"~"},"mrow":{"msqrt":{"mrow":{"mfrac":[{"mn":"1","msub":{"mi":"n","mn":"0"}},{"mn":"1","msub":{"mi":"n","mn":"1"}}],"mo":"+"}},"mo":"\u2062","msub":{"mi":"\u03c3","mn":"0"}}}},{"mo":["(",")"],"mfrac":{"msubsup":{"mi":"\u03c3","mn":["0","2"]},"msup":{"mi":"S","mn":"2"}}}],"mo":"\u2062"}},{"mo":["(",")"],"mfrac":{"msub":[{"mi":["S","t"]},{"mi":"\u03c3","mn":"0"}]}}],"mo":"\u2062"},{"mi":"N","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":["0","1"],"mo":","}}}],"mo":["\u2062","\u2062"],"munder":{"mo":"\u2192","munder":{"mrow":[{"msub":{"mi":"n","mn":"0"},"mo":"\u2192","mi":"\u221e"},{"msub":{"mi":"n","mn":"1"},"mo":"\u2192","mi":"\u221e"}]}}}},{"mrow":{"mo":["(",")"],"mn":"51"}}]}}}}},"and that by squaring (51) and using convergence results from G. Casella and R. L. Berger, , it can also be concluded that",{"@attributes":{"id":"p-0173","num":"0172"},"maths":{"@attributes":{"id":"MATH-US-00037","num":"00037"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mover":{"mi":"\u03c7","mo":"~"},"mo":"=","mrow":{"mrow":{"mfrac":[{"msup":{"mover":{"mover":{"mi":"\u03b2","mo":"^"},"mo":"~"},"mn":"2"},"mrow":{"mo":["(",")"],"mrow":{"mfrac":[{"mn":"1","msub":{"mi":"n","mn":"0"}},{"mn":"1","msub":{"mi":"n","mn":"1"}}],"mo":"+"}}},{"msubsup":{"mi":["S","t"],"mn":"2"},"msup":{"mi":"S","mn":"4"}}],"mo":"\u2062"},"mo":["\u2062","\u2062"],"munder":{"mo":"\u2192","munder":{"mrow":[{"msub":{"mi":"n","mn":"0"},"mo":"\u2192","mi":"\u221e"},{"msub":{"mi":"n","mn":"1"},"mo":"\u2192","mi":"\u221e"}]}},"msubsup":{"mi":"\u03c7","mn":["1","2"]}}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"52"}}]}}}}},"which can be readily reformatted into (38) using the definitions given in Proposition 1 and in this proof. Equation (52) concludes the proof.","The expression in (52), or alternatively in (38), is used in the AsemiP detector.","Power of the Tests","The power of the statistical test in Proposition 1 is examined. If His true in Proposition 1, the Type-I error probability (i.e., the probability of rejecting H, given that it is true) is",{"@attributes":{"id":"p-0178","num":"0177"},"maths":{"@attributes":{"id":"MATH-US-00038","num":"00038"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":{"mrow":[{"msub":{"mi":"P","mrow":{"mover":{"mi":["\u03b2","_"]},"mo":"=","mn":"0"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mover":{"mi":"\u03c7","mo":"~"},"mo":">","mi":"\u03b3"}}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["\u03be","\u03b3"],"mo":">"}}}],"mo":["\u2062","\u2062"],"munder":{"mo":"\u2192","mrow":{"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"n","mn":"0"},{"mi":"n","mn":"1"}],"mo":","}},"mo":"\u2192","mi":"\u221e"}}},"mo":"=","mi":"\u03b1"},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"53"}}]}}}}},"where \u03be is a chi-square distributed random variable with 1 degree of freedom, {tilde over (\u03c7)} as defined in (38) and expressed in a different form in (52), and \u03b3 an arbitrary scalor. P({tilde over (\u03c7)}>\u03b3) is indeed an asymptotically size a test, which is controlled by the user.","Now consider an alternative parameter value {tilde over (\u03b2)}\u22600. In this case, \u03c3\u2260\u03c3and from (52), it can be written:",{"@attributes":{"id":"p-0181","num":"0180"},"maths":{"@attributes":{"id":"MATH-US-00039","num":"00039"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mover":{"mi":"\u03c7","mo":"~"},"mo":"=","mrow":{"mrow":{"mo":["{","}"],"mrow":{"msup":{"mrow":{"mo":["[","]"],"mrow":{"munder":[{"mrow":{"mo":["(",")"],"mfrac":{"mrow":{"mover":[{"mover":{"mi":"\u03b2","mo":"^"},"mo":"~"},{"mi":"\u03b2","mo":"~"}],"mo":"-"},"msqrt":{"mrow":{"mrow":[{"mfrac":{"mn":"1","msub":{"mi":"n","mn":"0"}},"mo":"\u2062","msubsup":{"mi":"\u03c3","mn":["0","2"]}},{"mfrac":{"mn":"1","msub":{"mi":"n","mn":"1"}},"mo":"\u2062","msubsup":{"mi":"\u03c3","mn":["1","2"]}}],"mo":"+"}}}},"munder":{"mi":["\ufe38","A"]}},{"mrow":{"mo":["(",")"],"mfrac":{"mover":{"mi":"\u03b2","mo":"~"},"msqrt":{"mrow":{"mrow":[{"mfrac":{"mn":"1","msub":{"mi":"n","mn":"0"}},"mo":"\u2062","msubsup":{"mi":"\u03c3","mn":["0","2"]}},{"mfrac":{"mn":"1","msub":{"mi":"n","mn":"1"}},"mo":"\u2062","msubsup":{"mi":"\u03c3","mn":["1","2"]}}],"mo":"+"}}}},"munder":{"mi":["\ufe38","B"]}}],"mo":"+"}},"mn":"2"},"mo":"\u2062","munder":{"mrow":{"mo":["(",")"],"mfrac":{"msubsup":{"mi":["S","t"],"mn":"2"},"msup":{"mi":"S","mn":"4"}}},"munder":{"mi":["\ufe38","C"]}}}},"mo":"."}}},{"mrow":{"mo":["(",")"],"mn":"54"}}]}}}}},"Note that the term A in (54) converges in distribution to the standard Normal, N(0,1), as no and ngo to +\u221e,no matter what the values of {tilde over (\u03b2)}, \u03c3, or \u03c3are. Note also that the term B converges to +\u221e or \u2212\u221e in probability, as nand ngo to +\u221e, depending on whether {tilde over (\u03b2)} is positive or negative. Sconverges in probability to zero, as does S, but the term C converges to +\u221e because (S)=Sis in the denominator. Thus, {tilde over (\u03c7)} converges to +\u221e in probability and",{"@attributes":{"id":"p-0183","num":"0182"},"maths":{"@attributes":{"id":"MATH-US-00040","num":"00040"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":"P","mrow":{"mover":{"mi":"\u03b2","mo":"~"},"mo":"\u2260","mn":"0"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"reject","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"msub":{"mi":"H","mn":"0"}}}},{"mrow":{"msub":{"mi":"P","mrow":{"mover":{"mi":"\u03b2","mo":"~"},"mo":"\u2260","mn":"0"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mover":{"mi":"\u03c7","mo":"~"},"mo":">","mi":"\u03b3"}}},"mo":["\u2062","\u2062"],"munder":{"mo":"\u2192","mrow":{"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"n","mn":"0"},{"mi":"n","mn":"1"}],"mo":","}},"mo":"\u2192","mi":"\u221e"}},"mn":"1."}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"55"}}]}}}}},"In this way, the test in Proposition 1 has the properties of asymptotic size \u03b1 and asymptotic power 1, as it is desired.","Implementation of AsemiP Detector","In contrast to the SemiP algorithm, the AsemiP algorithm is significantly simpler to implement, as the latter does not require specialized subroutines (unconstrained minimization) to perform its function. Using a sampling mechanism such as described above, the variables in Proposition 1 are straightforward to implement. Preferably, statistical independence is promoted, and a sufficiently large number of samples (e.g. larger than 30) is taken to justify the use of approximation theorems of mathematical statistics. The sampling mechanism discussed above were used to obtain a pair of random feature vectors x(i=0 [reference], 1 [test]; j=1, . . . , J). A 9-pixel (3\u00d73) test window, a 56-pixel reference window, and a 60-pixel variability window, as shown in , were used, where J=60. For a statistical decision, high values obtained by using (38), or equivalently (52), reject the hypothesis Hin Proposition 1, thus, detecting anomalies. Set a decision threshold based on a choice of type I error using, as the base distribution, the chi-square distribution with 1 degree of freedom. Alternatively, a suitable threshold is found that yields a value at the knee of the AsemiP's corresponding ROC curve.","Application and Comparison with Other Techniques","Five other anomaly detection techniques were used for comparison purposes, and their mathematical representations are presented without proofs. The five techniques are known as: RX (Reed-Xiaoli), DPC (Dominant Principle Component), EST (eigen separation transform), FLD (Fisher's linear discriminant), and KRX (kernel RX).","The RX technique is based on the generalized likelihood ratio test (GLRT) and on the assumption that the population distribution family of both test and reference samples are multivariate normal. The FLD technique is also based on the same assumption, but differs in its subtleties in answering the question whether the test and reference samples are drawn from the same normal distribution. The FLD technique promotes separation between classes and variance reduction within each class. The PCA and EST techniques are both based on the same general principle, i.e., data are projected from their original high dimensional space onto a significantly lower dimensional space using a criterion that promotes highest sample variability within each domain in this lower dimensional space. Differences between DPC and EST are better appreciated through their mathematical representations. The KRX algorithm extends the original RX space to a nonlinear feature space by kernelizing the corresponding nonlinear GLRT expression of the conventional RX approach. The GLRT (generalized likelihood ratio test) expression of the KRX is similar to the conventional RX, but every term in the expression is in kernel form, which can be readily calculated in terms of the input data in its original data space (H. Kwon, N. Nasrabadi, \u201cKernel RX-Algorithm: A Nonlinear Anomaly Detector for Hyperspectral,\u201d , vol. 43, no. 2, February, 2005).","These five techniques were implemented with the conventional inside-outside window approach, choosing optimum window sizes to account for the target-size range of interest (see both scenes in ), i.e., 5\u00d75 inside window embraced by a 9\u00d79 outside window, with a guard area between the two windows. These techniques are represented by the following set of equations:\n\n=()()\u2003\u2003(56)\n\n()|\u2003\u2003(57)\n\n()|\u2003\u2003(58)\n\n()|\u2003\u2003(59)\n","where is a sample mean vector from a set of inside-window vectors x, each having 150 spectral bands; is similar but sampled from the outside window x; Cis the inverse sample covariance using all vectors sampled from the outside window; Eis the highest energy eigenvector of the eigenvector decomposition of the inside-window covariance; Eis the highest positive energy eigenvector of the eigenvector decomposition of the covariance difference (inside-widow minus outside-widow); |\u25cf| denotes the absolute value; and Eis the eigenvector decomposition of the scatter matrices ratio SS, where",{"@attributes":{"id":"p-0192","num":"0191"},"maths":{"@attributes":{"id":"MATH-US-00041","num":"00041"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"msub":{"mi":["S","W"]},"mo":"=","mrow":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["x","in"],"mrow":{"mo":["(",")"],"mi":"i"}},"mo":"-","msub":{"mover":{"mi":["x","_"]},"mi":"in"}}},"mo":"\u2062","msup":{"mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["x","in"],"mrow":{"mo":["(",")"],"mi":"i"}},"mo":"-","msub":{"mover":{"mi":["x","_"]},"mi":"in"}}},"mi":"t"}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"M"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["x","out"],"mrow":{"mo":["(",")"],"mi":"i"}},"mo":"-","msub":{"mover":{"mi":["x","_"]},"mi":"out"}}},"mo":"\u2062","msup":{"mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["x","out"],"mrow":{"mo":["(",")"],"mi":"i"}},"mo":"-","msub":{"mover":{"mi":["x","_"]},"mi":"out"}}},"mi":"t"}}}],"mo":"+"}},"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}},"mi":"and"}},{"mrow":{"mo":["(",")"],"mn":"60"}}]},{"mtd":[{"mrow":{"mrow":{"msub":{"mi":["S","B"]},"mo":"=","mrow":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["x","in"],"mrow":{"mo":["(",")"],"mi":"i"}},"mo":"-","msub":{"mover":{"mi":["x","_"]},"mi":"total"}}},"mo":"\u2062","msup":{"mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["x","in"],"mrow":{"mo":["(",")"],"mi":"i"}},"mo":"-","msub":{"mover":{"mi":["x","_"]},"mi":"total"}}},"mi":"t"}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"M"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["x","out"],"mrow":{"mo":["(",")"],"mi":"i"}},"mo":"-","msub":{"mover":{"mi":["x","_"]},"mi":"total"}}},"mo":"\u2062","msup":{"mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["x","out"],"mrow":{"mo":["(",")"],"mi":"i"}},"mo":"-","msub":{"mover":{"mi":["x","_"]},"mi":"total"}}},"mi":"t"}}}],"mo":"+"}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"61"}}]}]}}}},"where is the sample average vector using all of the samples from the inside and outside windows, and N and M are the sample size of the inside and outside windows, respectively.","For additional details on the implementation and performance of these techniques, see, for example, H. Kwon, et al., \u201cAdaptive anomaly detection using subspace separation for hyperspectral imagery,\u201d 42(11), 3342-3351, 2003.","The KRX anomaly detector can be compactly represented by the following:\n\n=()(),\u2003\u2003(62)\n","where K=K( , X) is a kernel-function based vector that uses as input and X, representing the dot product between these two inputs nonlinearly mapped onto a higher dimensional space, is the sample mean vector using the columns of X, is the sample mean vector using the columns of X, K=K( , X) is the same kernel function using instead the dot product between and X, and Kis the inverse of K=K(x, X) using the dot product between Xand itself.","Finally, the kernel function used to implement the KRX detector for this research effort was the well known Gaussian (radial basis function) RBF kernel, or",{"@attributes":{"id":"p-0198","num":"0197"},"maths":{"@attributes":{"id":"MATH-US-00042","num":"00042"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":[{"mi":"k","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"x","mn":"0"},{"mi":"x","mn":"1"}],"mo":","}}},{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mrow":[{"mo":"-","msup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msub":[{"mover":{"mi":["x","_"]},"mn":"0"},{"mover":{"mi":["x","_"]},"mn":"1"}],"mo":"-"}},"mn":"2"}},{"mn":"2","mo":"\u2062","msup":{"mi":"\u03c3","mn":"2"}}]}}}],"mo":"="},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"63"}}]}}}}},"where \u2225\u25cf\u2225 denotes the magnitude of a vector.","Top-View Imagery from the HYDICE Sensor","Experiment was carried out on data set from the hyperspectral digital imagery collection experiment (HYDICE) sensor. The HYDICE sensor records  spectral bands in the visible to near infrared (VNIR) and short-wave infrared (SWIR) and, for the purpose the examples described herein, it is sufficient to know that each pixel in a scene is actually a vector, consisting of 210 components. An extended description of this dataset can be found elsewhere.","The results shown for one data sub-cube are representative for other sub-cubes in the HYDICE (forest radiance) dataset. An illustrative sub-cube (shown as an average of 150 bands; 640\u00d7100 pixels) is shown in  (left). (We only used 150 bands by discarding water absorption and low signal to noise ratio bands; the bands used are the 23-101, 109-136, and 152-194.) The scene consists of 14 stationary motor vehicles (targets near a tree-line) in the presence of natural background clutter (e.g., trees, dirt roads, grasses). Each target consists of about 7\u00d74 pixels, and each pixel corresponds to an area of about 1.3\u00d71.3 square meters at the given altitude. The main goals of local anomaly detection algorithms on these types of scenes are to detect objects that seem clearly anomalous to its immediate surroundings, in some predetermined feature space, and to yield in the process a tolerable number of nuisance detections. Targets are often found to be anomalous to its immediate surroundings.","Results Using HYDICE Data","ROC curves are used as a means to quantitatively compare some of these approaches.","The target set was a set of ground vehicles near a tree-line, but as anomaly detectors are not designed to detect a particular target set, the meaning of false alarms is not absolutely clear in this context. For instance, a genuine local anomaly not belonging to the target set would be incorrectly labeled as a false alarm. Nevertheless, it does add some value to our analysis to compare detections of targets versus non-targets among the different algorithms.",{"@attributes":{"id":"p-0206","num":"0205"},"figref":["FIG. 4A","FIG. 5A","FIG. 2"]},{"@attributes":{"id":"p-0207","num":"0206"},"figref":["FIG. 4B","FIG. 4A"],"sup":"\u22122"},"Detection performance was measured using the ground truth information for the HYDICE imagery. The coordinates of all the rectangular target regions and their shadows were used to represent the ground truth target set, call it TargetTruth. If the region outside the TargetTruth is denoted as ClutterTruth, then the intersection between TargetTruth and ClutterTruth is zero and the entire scene is the union of TargetTruth and ClutterTruth. For a given decision threshold, the proportion of target detection (PD) is measured as the proportion between the number of detected pixels belonging to TargetTruth over all pixels belonging to TargetTruth. On the other hand, the proportion of false alarms (PFA) is measured as the proportion between the number of detected pixels belonging to ClutterTruth over all pixels belonging to ClutterTruth.","In general, the quality of a detector can be readily assessed by noticing a key feature in the shape of its ROC curve: the closer the knee of a ROC curve is to the PD axis, the less sensitive the approach is to different decision thresholds. In other words, PFA does not change significantly as PD increases. (An ideal ROC curve resembles a step function that starts at point [PFA=0,PD=1].) As it can be readily assessed from , the SemiP and AsemiP detectors clearly outperform the other four techniques on the tested scene. That difference in performance can be better appreciated in , where PFA is further restricted to a maximum value of 0.02 compared to 0.4 in . Beyond the value of PFA=0.4, these ROC curves reach PD near 1.0.",{"@attributes":{"id":"p-0210","num":"0209"},"figref":["FIG. 5A","FIGS. 5B-5H","FIG. 5","FIG. 5"]},"Although the 2-dim (2-dimensional) version of the output surfaces shown in  displays useful differences among the responses of the six detectors, they do not make full justice to the quality of the SemiP and AsemiP detectors.",{"@attributes":{"id":"p-0212","num":"0211"},"figref":"FIG. 6"},"Both surfaces in  were clipped at the value of 2,000 but some of their values do continue to significantly higher numbers. The dominant (clipped) peaks are the results produced by the fourteen targets near the tree-line.","Areas containing the presence of clutter mixtures (e.g., edge of terrain, edge of tree clusters), where other methods usually yield a high number of false alarms (false anomalies), are suppressed by the new approaches. The reason for this suppression is that, as part of the overall comparison strategy, the reference and test feature vectors are not compared as two individual samples; instead, a new sample is constructed, combined data being the union of data from both the test window and the reference window, and compared to the test sample. This indirect comparison approach, which is inherent in both detectors, can ensure that a component of a mixture (e.g., shadow) shall not be detected as a local anomaly when it is tested against the mixture itself (e.g., trees and shadows). Performances of such cases are represented in  in the form of softer anomalies (significantly less-dominant peaks).","Both detectors perform remarkably well accentuating the presence of dominant local anomalies (e.g., targets and their shadows) from softer anomalies (e.g., region discontinuity). This ability explains the SemiP's and AsemiP's superior ROC-curve performances shown in .",{"@attributes":{"id":"p-0216","num":"0215"},"figref":["FIGS. 7A-7B","FIG. 5A"]},"Ground View Imagery from the SOC-700 Sensor","The ground-view imagery used for this work was collected with a visible to near-IR spectral imager (SOC-700) from Surface Optics Corporation, San Diego, Calif. The system is a relatively small, portable hyperspectral imager, which collects a hyperspectral cube consisting of 640\u00d7640 pixels\u00d7120 spectral bands and has a spectral range covering 0.38 to 0.92 microns. The sensor is commercially available.",{"@attributes":{"id":"p-0219","num":"0218"},"figref":"FIG. 8","b":"1"},"Scene 1 contains three motor vehicles and a standing person in the center of that scene (i.e., two pick-up trucks to the left in proximity to each other, a man slightly forward from the vehicles in the center, and a sport utility to the right). The background in Scene 1 is dominated by the following two most abundant object classes: Californian valley-type trees and terrain. Scene 2 consists of the same sport utility vehicle and the same person standing in proximity to it, they are located in the same valley, but at a different area from the one in Scene 1. Scene 3 (row 3, column 1) depicts a significantly more complicated scenario, where the same man and the same sport utility vehicle can be found in the shadows of a large cluster of trees. Portions of the shadowed vehicle are observable near the center in Scene 3, and the shadowed man is reportedly halfway between the vehicle and the left side of the image. The three scenes were independently displayed using the same colormap (false color transformation), which means that the pixel intensities shown in each individual surface is only relative to the corresponding surface. (Pixel values representing the same object class may be displayed with different intensities in another surface, depending on other objects in the scene.) This fact explains the brightness difference between the terrains displayed in Scene 2 and Scene 3, given that the meteorological conditions and terrains were about the same for all three scenes. The strong reflections from certain parts of the vehicles captured by the sensor in Scene 1 and 2 are not as dominant in Scene 3 because the vehicle there is in the shadow; hence, the terrain in Scene 3 appears to be a strong reflector.","Two approaches can be used: (1) seeking global anomalies in a natural clutter background, given that only a few spectral samples from the most abundant object classes in the background (in this case, trees and terrain) are drawn from the same HS data and presented, as references, to the detectors, and (2) seeking global anomalies, given that the reference samples are not drawn from the same HS data.","The RX and the AsemiP detectors were applied to the scenes in the first column of , and output surfaces are presented in the second and third columns, respectively. In this example, the outside window is represented instead by a fixed reference set, such that samples from the test window [a 3\u00d73 window (nine 120-band spectral samples)] are used in conjunction with the two reference sample sets (one representing responses from tree leaves and another from a patch of terrain), each consisting of 100 spectral samples, for a total of 200 reference samples, a mere 0.05% of the image area. The two small boxes shown in Scene 1 represent the general locations\u2014chosen arbitrarily\u2014where the two reference sets were drawn from. The same data preprocessing (i.e., high pass filtering and angle difference) was applied to the spectral samples. The AsemiP detector suppressed the meaningless detections (edges) and accentuated meaningful detections.","The AsemiP detector was expected to compare systematically across the imagery the preprocessed test samples with the fixed preprocessed samples from both reference sets. If a local set of preprocessed test samples is significantly different from both fixed reference sets, the AsemiP detector should produce an accentuated value at that location indicating this fact; otherwise, it should produce a suppressed value. To achieve this expectation using two reference classes, one could simply collect the individual comparison result from each reference class and then keep only the minimum result between the two, as a final decision for that given 3\u00d73 test location.","The RX detector was adapted to the ground-view problem using the recommended data preprocessing discussed in X. Yu et al., \u201cAutomatic target detection and recognition in multiband imagery: A unified ML detection and estimation approach,\u201d , vol. 6, pp. 143-156, January 1997, i.e., a spatial high pass filter was applied to the untransformed hyperspectral samples belonging to the same reference sets used for the AsemiP detector, also to the samples from the test samples across the imagery. This procedure removes the spatially nonstationary mean, which is not useful for the RX detector, and promotes spatial independence, allowing this detector to exploit an expected correlation in the spectral domain among samples belonging to the same class. Under the assumptions given in the RX model, this detector is expected to produce an accentuated value when the simplified Mahalanobis distance between a high-pass filtered reference set and a test set is significantly high; otherwise, it is expected to produce a suppressed value. Since there are, in this implementation, two fixed reference sets, the minimum between the two distances was also used as a means to produce a final result per location in the imagery. Recall that by using this decision logic, an anomalous test sample to both reference sets would still produce a high value, since both results would likely yield high values.","The output surfaces of the RX and AsemiP detectors are shown in columns 2 and 3, respectively, for the corresponding scenes in column 1. A shade map emphasize anomalies with respect to the reference samples by their intensity levels, i.e., white is equivalent to the strongest anomalies, light gray to strong anomalies, gray to intermediate anomalies, dark gray to weak anomalies, and black to weakest anomalies. The false shades change gradually and are relative only to those results within the same surface, for instance, a shade in one surface does not mean necessarily that its value is equivalent to a similar shade in another surface.","The local results shown in the first RX surface show that a detector based on conventional methods performs well suppressing objects in the scene having low variability and belonging to the same class of a reference set (Case )\u2014the trees were suppressed. Likewise, it performs well accentuating objects that are significantly different from the reference set (Case )\u2014some parts of the target, vehicle at the right, were highly accentuated (zooming close enough, one can observe a few white pixels within the boundaries of the vehicles shown in both RX surfaces in column 2, rows 2 and 3.). Unfortunately, as it was observed in the top-view problem, local areas characterized by class mixtures (transition of regions) might be accentuated by these detectors, (see RX performance in  (row 1, column 2), obscuring the presence of meaningful objects in that scene. In fact, for the scenes presented in , the RX detector seems to perform more as an edge detector than an anomalous object detector.","The AsemiP detector, on the other hand, was able to suppress virtually all the background of Scene 1, and to accentuate large portions of the vehicles and of the standing man. For the bottom image, the pants of a standing man, in shadow, were accentuated as an anomaly with respect to the reference sample sets of tree and terrain. In the upper left-hand side image, the same man is standing next to the vehicle. In a qualitative sense, test samples consisting of, say, a mixture of shadow and terrain were likely suppressed due to the indirect comparison between the mixture itself and the union between that mixture and a component of that mixture, in this case, terrain.","The suppression of shadows may be explained by the following: Regions characterized by tree shadows, for instance, may be interpreted as partially obscured terrain because tree leaves do partially obscure the incident solar light; however, since significant spectral radiances are still reflected from the partially shadowed terrain, such a region will be suppressed when compared to the union of itself and the reference set of open terrain. The RX surface shown in rows 2, column 2, suggests that the RX detector may be susceptible to subtle spectral differences of the same terrain when observed by the same HS sensor in a different area. Recall that Scenes 2 and 3 were tested using the same reference sets drawn from Scene 1. The surface shown in row 2, column 3, suggests that the AsemiP detector is significantly more robust to spectral differences of the same terrain.","The interpretation of a shadowed object as a partially obscured object is especially relevant in Scene 3. The output surface shown in , row 3, column 2, emphasizes the fact that the RX anomaly detector performs as expected: it detects anomalies in the scene. Unfortunately, these anomalies are not necessarily always meaningful to an image analyst. For Scene 3, some of the tracks made by the shadowed vehicle, and the transition between the shadowed and the non-shadowed terrain were the most anomalous regions in the scene, as seen by the RX detector. Fortunately, with the indirect comparison approach that is inherent in the AsemiP detector, these same regions were virtually suppressed, while the more meaningful anomalous structures (vehicle and human pants) were accentuated, see surface in row 3, column 3.","In this example, the test feature set is generated from the test data (which for the Ground-View Imagery is a single spectral sample from a lxl test window or an average sample from an N\u00d7N test window) and the different spectra of a reference set. The reference feature set is generated from an average spectrum of the reference data and the individual reference spectra. All the spectral samples, whether individual or averages, are high-pass filtered (HPF) first in the spectral domain before the test and reference feature sets are generated. HPF will remove statistical dependence (which includes correlation) in the spectral domain.","Furthermore, more than one reference set can be used, and the results using each reference set can be combined to improve anomaly detection. For a given test location in the imagery, each reference set can be processed individually yielding a detection result. For example, for two reference classes R and R, the detector produces two outputs O and O (both are scalors, not vectors). For a final decision at a test location (x,y), the final result is found from O and O as follows: Final_Result(x,y)=Minimum (O, O).","If the test sample at (x,y), or T(x,y), is significantly different from both R and R, both O and O are expected to be very high numbers, hence, the minimum between O and O [Final_Result(x,y)] is also expected to be a very high number, ensuring the fact that T(x,y) is anomalous to R and R. If T(x,y) is not significantly different from R or R, one of the outputs O or O is expected to be very low, hence, Final_Result(x,y) will be that same very low value, ensuring the fact that T(x,y) is very similar to R, R, or to both.","This decision logic is valid and easily generalized for cases involving more than two classes (R, R, R, . . . RN). A collection of Final_Result(x,y) across the imagery yields the output surfaces shown in  for both the RX and AsemiP anomaly detectors, running independently of course.","The computational time is reduced by obtaining reference samples from the most abundant objects in a target region, e.g., terrain (such as rock, sand, soil, snow, ice, water, and the like), flora (such as grass, trees, other foliage), man-made materials not of any particular interest (such as buildings, cement, asphalt, and the like), or other feature expected in the scene but not of particular interest) are collected, preprocessed, and stored a priori in an anomaly detection device, or database in communication with it. The device can then wait for online hyperspectral imagery for testing. Notice that in this example, in contrast with the sampling mechanism shown in , the variability spectra are the actual samples representing the most abundant objects, which may be obtained within the scene, or some or all of the plurality of variability spectra being stored in advance in a database. The spectral average obtained using reference window  in  is represented in this example by the spectral average of the variability spectra.","Examples of the present invention can be used for target detection in other ranges of the electromagnetic spectrum, such as radar imaging. For example, target detection using SAR (synthetic aperture radar) images can be achieved. In a SAR pixel, data is represented by a single value, and one can also use the concept of 3 window cells: Variability, Reference, and Test. Let (y, y, . . . , y) be the pixel values inside the Variability window, ybe the average of all pixel values inside the Reference window, and ybe the average of all pixels inside the Test window, or the single pixel in the Test window (in the case it has only one pixel). Two feature sets can be generated by computing the differences, for example: Reference Feature, xR=(y\u2212y, y\u2212y, . . . , y\u2212y), and Test Feature, xT=(y\u2212y, y\u2212y, . . . , y\u2212y). Further, xR and xT can be combined (formation of the union of the two samples), using C=(xR, xT), and a comparison made of C with xR, or C with xT, for example, using one of the novel algorithms described herein. This approach may also be adapted to other image analysis applications, and other applications where anomalous data are sought. Feature sets can be generated from first and second data examples using a plurality of data examples, and a combination of the feature sets compared with either one of the feature sets.","The invention is not restricted to the illustrative examples described above. Examples are not intended as limitations on the scope of the invention. Methods, apparatus, compositions, and the like described herein are exemplary and not intended as limitations on the scope of the invention. Changes therein and other uses will occur to those skilled in the art. The scope of the invention is defined by the scope of the claims."],"GOVINT":[{},{}],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIGS. 1A-1C"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 3A","sup":["\u22122","\u22121","\u22121"]},{"@attributes":{"id":"p-0019","num":"0018"},"figref":["FIG. 3B","FIG. 3A"]},{"@attributes":{"id":"p-0020","num":"0019"},"figref":["FIGS. 4A and 4B","FIG. 4B"],"sup":"\u22122"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":["FIG. 5A","FIG. 2"]},{"@attributes":{"id":"p-0022","num":"0021"},"figref":["FIGS. 5B-5H","FIG. 5A"]},{"@attributes":{"id":"p-0023","num":"0022"},"figref":["FIG. 6","FIG. 5A"]},{"@attributes":{"id":"p-0024","num":"0023"},"figref":["FIG. 7A","FIG. 5A"]},{"@attributes":{"id":"p-0025","num":"0024"},"figref":["FIG. 7B","FIG. 5A"]},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 8"}]},"DETDESC":[{},{}]}
