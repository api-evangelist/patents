---
title: Information processing device and method, program, and recording medium
abstract: An information processing apparatus, capable of identifying vital sign information. A learning unit identifies an emotion/state of the user by comparing the generated feature vectors with feature vectors accumulated in a storage unit. Information indicating the resultant identified emotion/state of the user is output to an output unit via an output controller.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07548891&OS=07548891&RS=07548891
owner: Sony Corporation
number: 07548891
owner_city: Tokyo
owner_country: JP
publication_date: 20031006
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["TECHNICAL FIELD","BACKGROUND ART","DISCLOSURE OF INVENTION","BEST MODE FOR CARRYING OUT THE INVENTION","EXAMPLE 1 ","EXAMPLE 2 ","EXAMPLE 3 ","EXAMPLE 4 "],"p":["The present invention relates to an information processing apparatus, an information processing method, a program, and a storage medium, and more particularly, to an information processing apparatus, an information processing method, a program, and a storage medium, capable of extracting and learning features from various kinds of information in which the emotion or the state of a user is reflected, and identifying the emotion or the state of the user and the strength level of the emotion or the state on the basis of the learning result.","In conventional communication techniques using information processing apparatuses, in face-to-face communication using no information processing apparatuses, not only meanings are conveyed using linguistic words, but also other information such as information indicating emotion\/state is conveyed via a face expression, a motion, a mood, and\/or other means. In contrast, in communication using a telephone, a mail, or a chat, although meanings using linguistic words can be conveyed, emotion, states, or moods that cannot be expressed using words cannot be conveyed. It is known to use a computer graphic image called an avatar in virtual communication via the Internet. However, avatars cannot well express emotion of users.","Thus, at present, in communication using information apparatuses, transmission of information is very limited.","To break through the above limitation, a communication apparatus has been proposed which transmits voice information, image information, and vital sign information, guesses the psychological or physiological state of a user from received information, and outputs a guessed value (a specific example of this technique may be found, for example, in Japanese Unexamined Patent Application Publication No. 2002-34936 (hereinafter referred to as Patent Document 1)).","In the technique disclosed in Patent Document 1, deviations from mean values are calculated for various items of vital sign information associated with a user, and the emotion or the state of the user is guessed on the basis of the calculated deviations. However, such a simple algorithm cannot necessarily correctly guess the emotion or state such as \u201cjoy\u201d, \u201canger\u201d, \u201csadness\u201d, or \u201chappiness\u201d of a user. Presentation of incorrect information about emotion of a user can results in a misunderstanding in communication between users.","Various manners of displaying information have been proposed. One known technique is to display information using a table, a graph, or a graphic image. Another technique is to change a face expression or a color of a face. It is also known to modulate a voice or a sound depending on information being received. However, any of those known techniques cannot well express user's emotion or state guessed from vital sign information. It is also difficult to represent motion or mood of users.","An object of the present invention is to provide a technique of identifying emotion\/state of a user and a strength level of the emotion\/state. Another object of the present invention is to provide a technique that allows communication using non-linguistic information.","The present invention provides an information processing apparatus comprising input means for inputting non-linguistic information and linguistic information, learning means for leaning emotion of a user by relating input non-linguistic information to linguistic information, storage means for storing information obtained as a result of learning performed by the learning means, and output means for converting input non-linguistic information or stored non-linguistic information into a signal in a predetermined form and outputting the resultant converted signal.","The learning means may learn the emotion of the user together with the strength level of the emotion.","When new non-linguistic information is input, the learning means may identify the emotion of the user on the basis of the non-linguistic information stored in the storage means, and the output means may output an identification result made by the learning means.","The storage means may store new non-linguistic information corresponding to emotion identified by the learning means such that the new non-linguistic information is added to existing non-linguistic information accumulated in the storage means.","The information processing apparatus may further comprise parameter generation means for generating a parameter indicating a feature of the non-linguistic information, and the learning means may perform statistical processing on values of the parameter and may store the resultant statistical value of the parameter in the storage means.","The learning means may identify the emotion of the user by comparing a parameter generated, by the parameter generation mean, from input non-linguistic information with the parameter stored in the storage means.","The input means may input pulse information or body motion information associated with the user as the non-linguistic information.","The input means may input, as the non-linguistic information, vibration information indicating a vibration generated when the user presses a keyboard.","The input means may input, as the non-linguistic information, grasp pressure information indicating a grasp pressure at which a mouse is grasped by the user or ambient temperature information.","The information processing apparatus may be a game machine used to play an on-line game.","The input means may input, as the non-linguistic information, acceleration information or grasp pressure information associated with a controller of the game machine.","The output means may extract particular non-linguistic information specified by the user from the stored non-linguistic information, convert the extracted non-linguistic information into a signal in the predetermined form, and output the resultant signal to another game machine specified by the user.","The present invention also provides an information processing method comprising the steps of inputting linguistic information and non-linguistic information, leaning emotion of a user by relating input non-linguistic information to linguistic information, storing information obtained as a result of learning performed in the learning step, and converting input non-linguistic information or stored non-linguistic information into a signal in a predetermined form and outputting the resultant converted signal.","The present invention also provides a program for causing a computer to execute a process comprising the control steps of inputting linguistic information and non-linguistic information, leaning emotion of a user by relating input non-linguistic information to linguistic information, storing information obtained as a result of learning performed in the learning step, and converting input non-linguistic information or stored non-linguistic information into a signal in a predetermined form and outputting the resultant converted signal.","A storage medium including a program stored therein for causing a computer to execute a process comprising the control steps of: inputting linguistic information and non-linguistic information; leaning emotion of a user by relating input non-linguistic information to linguistic information, storing information obtained as a result of learning performed in the learning step, and converting input non-linguistic information or stored non-linguistic information into a signal in a predetermined form and outputting the resultant converted signal.","In the information processing apparatus, the information processing method, the program, and the storage medium according to the present invention, non-linguistic information and linguistic information are input, and learning on the input non-linguistic information is performed by relating it to the linguistic information. The input non-linguistic information or the stored non-linguistic information is converted into a signal in the predetermined form and output.","In the present description, the term \u201cemotion\u201d is used to describe not only an emotional state such as a state of joy or of dislike but also a general state such as a vital state or a stable state of a user.","The present invention is described in further detail below with reference to specific embodiments in conjunction with the accompanying drawings.  shows a structure of a communication system using information processing apparatuses according to the present invention.","In , a terminal , a terminal , and a terminal  are information processing apparatuses according to the present invention, and they are connected to a network  so that they communicate with each other.","Each of the terminal  (terminal A), the terminal  (terminal B), and the terminal  (terminal C) includes an input unit A, B, or C used to input information, a processing unit A, B, or C for processing the information input via the input unit, a learning unit A, B, or C for learning\/identifying the emotion\/state and the strength level of a user from the input information, a storage unit A, B, or C for storing information learned\/identified by the learning unit, an output unit A, B, or C for outputting information identified by the learning unit or for outputting information stored in the storage unit, a communication unit A, B, or C for transmitting\/receiving information, and a system controller A, B, or C for controlling various parts.","The terminal  (terminal A), the terminal  (terminal B), and the terminal  (terminal C) are similar in structure to each other, and parts thereof, that is, the parts such as the input unit A, . . . , the system controller A of the terminal , the parts such as the input unit B, . . . , the system controller B of the terminal , the parts such as the input unit C, . . . , the system controller C of the terminal  are similar in functions. In the following description, when it is not needed to discriminate those parts among terminals, the parts are represented in a generic manner, such as the input unit , . . . , the system controller .","As shown in , information input to the terminal , the terminal , or the terminal  via their input unit can be classified into defined information and undefined information. Defined information refers to information that can be uniquely interpreted without needing any other additional information. Examples of defined information are linguistic words, symbols assigned unique meanings, self-declared information transmitted in response to a particular operation performed by a user.","Non-defined information refers to information that can be interpreted in many ways or information that is difficult to interpret. Examples of undefined information are physiological information such as a pulse, a blood flow, a blood pressure, breathing, body temperature, skin surface temperature, electric resistance of skin, sweat, brain waves, and brain magnetism, body motion, head motion, or motion of another part of a user, a vibration that occurs when a keyboard is pressed, and a pressure at which a mouse is grasped.","The processing unit  extracts the emotion\/state of a user and the strength level of the emotion\/state (emotion information E) from input defined information. The processing unit  also extracts feature values C and a sequence of signal levels from undefined information. The extracted information, feature values, and signal levels are supplied to the learning unit .","The learning unit  combines the received emotion information E, the feature values C, and the sequence of signal levels into a single data and stores it in the storage unit  separately for each user. The stored data includes PID, CD, and CSD.","PID is information identifying a particular user. PID may include a name, a nickname, an e-mail address, an identification number assigned to the user, sexuality, a birthday, a residence address, and\/or a blood type.","CD includes emotion information E associated with a user identified by identification information PID, a distribution of fc of a feature value extracted from input undefined information, the center value \u03bc of the distribution fc, the standard deviation \u03c3 of the distribution fc, environmental information (such as temperature, humidity, environmental sound, the number of nearby persons, weather information, and\/or position information) Kp indicating an environment in which the user is located, as of the time when information is input by the user, a date\/time T when the information is stored.","The emotion\/state is classified into twelve kinds, as shown in Table 1.",{"@attributes":{"id":"p-0165","num":"0164"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":"TABLE 1"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"EMOTION\/STATE"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"77pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"140pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"JOY"]},{"entry":[{},"SURPRISE"]},{"entry":[{},"SADNESS"]},{"entry":[{},"ANGER"]},{"entry":[{},"DISLIKE"]},{"entry":[{},"INTEREST"]},{"entry":[{},"SHAME"]},{"entry":[{},"LIKING"]},{"entry":[{},"INSTABILITY"]},{"entry":[{},"STABILITY"]},{"entry":[{},"VITALITY"]},{"entry":[{},"NON-VITALITY"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}}},"Each kind of emotion\/state is divided into three strength levels: high level, middle level, and low level. Thus, there are a total of 36 (=12\u00d73) different kinds of emotion information E. Therefore, there are 36 CDs for each feature. If the number of feature values is m, there are 36\u00d7m CDs for each user.","Stored information CDS includes emotion information E associated with a user identified by identification information PID, a distribution fvc of a combination of feature values C extracted from undefined information, the center vector V\u03bc of the distribution, the standard deviation vector V\u03c3 of the distribution, environmental information (such as temperature, humidity, environmental sound, the number of nearby persons, weather information, and\/or position information) Kp indicating an environment in which the user is located, as of the time when information is input by the user, and a date\/time T when the information is stored.","The emotion\/state is classified into twelve kinds, as shown in Table 1, and each kind of emotion\/state is divided into three strength levels: high level, middle level, and low level. Therefore, there are 36 CSDs for each user.","The information stored in the storage unit  is referred to when emotion or a state of a user is learned or identified. New information is stored in the storage unit  such that it is added to the existing information stored in the storage unit .","Once a sufficient amount of information is stored in the storage unit , the information processing apparatus can identify the emotion\/state and the level of a particular user only from undefined information. To identify the emotion, the state and the level of a particular user from undefined information, feature values of the undefined information are extracted and compared with the information associated with the particular user stored in the storage unit .","For example, the processing unit  extracts feature values C, C, . . . , Cm from the undefined information associated with a particular user a, and the learning unit  generates a feature value vector Vcx(C, C, . . . , Cm) from the extracted feature values.","Furthermore, the learning unit  reads plural pieces of stored information CSD associated with the user a from the storage unit  and the learning unit  extracts center vectors V\u03bc, V\u03bc, . . . V\u03bc corresponding to all respective kinds of emotion\/state and the strength levels.","The learning unit  then calculates the inner product (Vcx, V\u03bcn) of the feature value vector Vcx and each center vector V\u03bcn (n=1, 2, . . . , 36). Each vector component is then examined in descending order of the inner product (Vcx, V\u03bcn) to determine whether (V\u03bcnm\u2212\u03c3nm)\u2266Vcxm\u2266(V\u03bcnm+\u03c3nm) is satisfied, where V\u03bcnm denotes an mth component of V\u03bcn, \u03c3nm denotes an mth component of a standard deviation vector \u03c3n of a distribution fvcn, and Vcxm denotes an mth component of Vcx.",{"@attributes":{"id":"p-0174","num":"0173"},"figref":["FIG. 3","FIG. 3"],"b":["1","2","2","2","2","2"]},"All components of the vector are examined in the above-described manner to determine whether the condition is satisfied. Of distributions satisfying the above condition, a distribution corresponding to a maximum inner product (Vcx, V\u03bcn) is detected, and the feature vector is determined to belong to the detected distribution.",{"@attributes":{"id":"p-0176","num":"0175"},"figref":"FIG. 4","b":["95","1","2","1","1","1","1","2","2","2","2"]},"Of inner products (Vcx, V\u03bc), (Vcx, V\u03bc), and (Vcx, V\u03bcn) of Vcx and the respective center vectors V\u03bc, V\u03bc and V\u03bcn, the inner product (Vcx, V\u03bc) is greatest in magnitude, and conditions (V\u03bcm\u2212\u03c3m)\u2266Vcxm\u2266V\u03bcm+\u03c3m) are satisfied for Vcx and all m components of V\u03bc, and thus Vcx can be regarded as belonging to emotion information E.","It is possible to identify the emotion, the state, the level of a particular user from input undefined information in the above-described manner.",{"@attributes":{"id":"p-0179","num":"0178"},"figref":"FIG. 5","b":["91","93","91","94","93","95","94","92","93","96","97"]},"The processing unit  includes an information discriminator  for classifying information input via the input unit  into voice information, pulse information, or body motion information, a feature extractor  for extracting property information from pulse information and body motion information, a meaning extractor  for extracting emotion information from voice information, and an output controller  for outputting information to the output unit . The communication unit  includes an information transmitting unit  for transmitting information and an information receiving unit  for receiving information.",{"@attributes":{"id":"p-0181","num":"0180"},"figref":["FIG. 6","FIG. 6"],"b":["91","91","131","132","133","135","134","131","136","132","137","133"]},{"@attributes":{"id":"p-0182","num":"0181"},"figref":["FIGS. 7 and 8","FIG. 7"],"b":["92","92","152","151","152","152","173","174","171","172"]},"In the example shown in , the output unit  includes a light emitting unit  for emitting light in accordance with a particular signal, and a power supply\/amplifier  for driving the light emitting unit .","Herein, an information processing apparatus of a user a is denoted by a terminal A, and an information processing apparatus of a user b is denoted by a terminal B. The user a and the user b can communicate with each other using their terminals A and B in accordance with a procedure shown in . First, the terminal A transmits a connection request to the terminal B. In response, the terminal B returns a connection acknowledgment signal to the terminal A. Thereafter, textual information such as a mail or a chat, voice information such as a conversation, or image information acquired using a CCD camera or the like is transmitted between the terminal A and the terminal B.","The terminal A performs initial setting associated with transmission\/reception of vital sign information (pulse information and body motion information of users, in this specific example), and transmits a transmission\/reception request to the terminal B together with transmission setting information and reception setting information such as those shown in .","The transmission setting information includes an information transmission mode selection ms, an information processing mode selection ts, and an information output mode selection authority ds. The reception setting information includes an information reception mode selection mr, an information processing mode selection tr, and an information output mode selection authority dr.","The parameter of information transmission mode selection ms indicates whether or not to transmit vital sign information. The parameter of information processing mode selection ts indicates whether or not to process vital sign information when the vital sign information is transmitted. The parameter of information output mode selection authority ds indicates whether setting of the form in which to transmit vital sign information is performed at a transmitting end or a receiving end.","The parameter of information reception mode selection mr indicates whether to receive vital sign information. The parameter of an information processing mode selection tr indicates whether to process vital sign information when it is received. The parameter of an information output mode selection authority dr indicates whether selection of the output form of vital sign information is performed at a receiving end or a transmitting end.","If the terminal B receives the vital sign information transmission request from the terminal A, the terminal B changes the setting associated with transmission\/reception of vital sign information and returns an acknowledgment signal to the terminal A.","The terminal A then transmits vital sign information of the user a to the terminal B, and the terminal B transmits vital sign information of the user b to the terminal A. The terminal A and the terminal B store the received vital sign information.","If the terminal A transmits a connection end request to the terminal B, the terminal B transmits a connection end acknowledgment signal, and the communication is ended.","Communication is performed between the terminal A and the terminal B in the above-described manner, and vital sign information is stored in the terminal A and the terminal B. After a sufficient amount of vital sign information is stored, transmission\/reception of information is performed as shown in . If vital sign information is transmitted between the terminal A and the terminal B, the terminal A and the terminal B detect the emotion or the state of users.","Referring to , a process performed by the terminal B to store vital sign information is described below. First, in step S, at the start of communication with the terminal A, the system controller  of the terminal B acquires information (for example, a name or a nickname) PID identifying the user a of the terminal A with which to communicate. In step S, the system controller  of the terminal B commands the information discriminator  to determine whether voice information of the user a is received from terminal A. If voice information is not yet received, the process waits until voice information is received.","If it is determined in step S that voice information is input, the process proceeds to step S. In step S, the system controller  of the terminal B commands the feature extractor  to start sampling pulse information and body motion information associated with the user a received from the terminal A. In the next step S, the system controller  of the terminal B commands the meaning extractor  to recognize the voice. In step S, the system controller  of the terminal B determines whether the recognized voice includes a word indicating an emotion\/state and a strength level thereof. If it is determined that the recognized voice does not include a word indicating an emotion\/state and the strength level of the emotion\/state, the process returns to step S, and the above-described process is repeated.  shows example of words indicating emotions or states, and  shows examples of words indicating strength levels.","In the case in which it is determined in step S that the recognized voice includes a word indicating an emotion\/state or a strength level of an emotion\/state, then, in step S, the system controller  of the terminal B determines whether the subject of the detected word is the user a, that is, whether the detected word indicates the emotion\/state or the strength level of the emotion\/state of the user a. If the subject is not the user a, then the process returns to step S, and the above-described process is repeated.","In the case in which it is determined in step S that the user is the subject, the process proceeds to step S. In step S, the system controller  of the terminal B determines whether one phrase of voice of the user a has been input. If one phrase of voice has not yet been input, the system controller  of the terminal B waits until one phrase of voice is input.","If it is determined in step S that one phrase has been input, then, in step S, the system controller  of the terminal B commands the feature extractor  to end the sampling of pulse information and body motion information.","The sampling of pulse information and body motion information may be performed by the feature extractor  of the terminal A, and the sampled information may be transmitted to the terminal B.","In step S, the system controller  of the terminal B commands the meaning extractor  to extract emotion information E.","For example, if the user a utters \u201cThat sounds a little interesting.\u201d, the emotional\/state corresponding to a word \u201cinteresting\u201d is \u201cinterest\u201d, and the strength level corresponding to a word \u201ca little\u201d is \u201clow level\u201d. Thus, \u201cweak interest\u201d is extracted as emotion information E.","In step S, the system controller  of the terminal B commands the feature extractor  to generate a feature vector Vc on the basis of pulse information of the user a. In step S, the system controller  of the terminal B reads, from the storage unit , pulse information corresponding to information PID identifying the user and the emotion information E. In this specific case, a feature vector of pulse information corresponding to weak interest of the user a is read. Then in step S, the system controller  of the terminal B commands the learning unit  to add the newly generated feature vector Vc to the read feature vector and recalculate the vector distribution fvc, the distribution center vector V\u03bc, and the standard deviation vector V\u03c3. In step S, the recalculated vectors are stored in the storage unit .","In step S, the system controller  of the terminal B commands the feature extractor  to generate a feature vector Vc on the basis of body motion information of the user a. In step S, the system controller  of the terminal B reads, from the storage unit , body motion information corresponding to information PID identifying the user and the emotion information E. In this specific case, a feature vector of body motion information corresponding to weak interest of the user a is read. Then in step S, the system controller  of the terminal B commands the learning unit  to add the generated feature vector Vc to the read feature vector and recalculate the vector distribution fvc, the distribution center vector V\u03bc, and the standard deviation vector V\u03c3. In step S, the recalculated vectors are stored in the storage unit .","Now, referring to , the process of generating the feature vector Vc of pulse information is described below. This process is performed by the feature extractor  of the terminal B. In step S, the feature extractor  of the terminal B reads sampled pulse information. In step S, the feature extractor  performs a polarity detection process that will be described later. In step S, the feature extractor  performs a Tv generation process that will be described later. In step S, the feature extractor  performs a Vmin\/Vmax detection process that will be described later.","Referring to , the polarity detection process, the Tv generation process, and the Vmin\/Vmax detection process are described.","First, referring to , the process of detecting the polarity of a sampled waveform is described. In step S, the feature extractor  of the terminal B sets a parameter n to 1. In step S, the feature extractor  of the terminal B reads Vs(n). If a waveform  shown in  is sampled at intervals Tclk, sampled values Vs, Vs, Vs, . . . , Vsn are obtained. In this polarity detection process, Vs(n) is read depending on the current value of n such that Vs is read when n=1, Vs is read when n=2, and so on.","In step S, the feature extractor  of the terminal B determines whether Vs(n) exists. If it is determined that Vs(n) does not exit, the process is ended. If it is determined in step S that Vs(n) exists, then the process proceeds to step S. In step S, the feature extractor  of the terminal B determines whether Vs(n) is smaller than a threshold value \u2212Vth. If it is determined that Vs(n) is not smaller than \u2212Vth, then the process proceeds to step S. In step S, it is further determined whether Vs(n) is greater than a threshold value Vth. If it is determined that Vs(n) is not greater than Vth, then the process proceeds to step S. In step S, Sig(n) is set to 1.","In the case in which it is determined in step S that Vs(n) is smaller than the threshold value \u2212Vth, the feature extractor  of the terminal B sets Sig(n) to \u22121. In the case in which it is determined in step S that Vs(n) is greater than the threshold value Vth, the feature extractor  of the terminal B sets Sig(n) to 1.","In step S, the feature extractor  of the terminal B increments the value of n by 1. Then the process returns to step S, and the above-described process is repeated.","Thus, the parameter Sig(n) indicating the polarity of the waveform is produced. That is, when the waveform is positive in polarity. Sig(n) is set to 1, and Sig(n) is set to \u22121 when the waveform is negative in polarity. When the waveform is neither positive nor negative in polarity, that is, when the waveform is zero, Sig(n) is set to 0.","Now, referring to , a Tv generation process is described below. In step S, the feature extractor  of the terminal B sets a parameter n to 2 and a parameter P to 1. In step S, the feature extractor  of the terminal B reads Sig(n) generated as a result of the polarity detection process. In step S, it is determined whether Sig(n) exists. If it is determined that Sig(n) does not exit, the process is ended.","In the case in which it is determined in step S that Sig(n) exists, the process proceeds to step S. In step S, the feature extractor  of the terminal B reads Sig(n\u22121). In step S, the feature extractor  determines whether Sig(n) and Sig(n\u22121) are equal in value to each other. If it is determined that Sig(n) and Sig(n\u22121) are equal in value to each other, then, in step S, it is further determined whether Sig(n) is equal to \u22121. If it is determined that Sig(n) is not equal to \u22121, then in step S it is determined whether Sig(n) is equal to 1. If it is determined that Sig(n) is not equal to 1, the process proceeds to step S. In step S, a parameter c is incremented by 1. Thereafter, the process proceeds to step S.","In the case in which it is determined in step S that Sig(n) is equal to \u22121, the process proceeds to step S. In step S, the feature extractor  of the terminal B increments the parameter a by 1. Thereafter, the process proceeds to step S. In the case in which it is determined in step S that Sig(n) is equal to 1, the process proceeds to step S. In step S, the feature extractor  of the terminal B increments the parameter b by 1. Thereafter, the process proceeds to step S.","In step S, the feature extractor  of the terminal B increments the value of n by 1. The process then returns to step S, and the above-described process is repeated.","If it is determined in step S that Sig(n) and Sig(n\u22121) are not equal in value to each other, the process proceeds to step S. In step S, the feature extractor  of the terminal B determines whether Sig(n\u22121) is equal to \u22121. If it is determined that Sig(n\u22121) is not equal to \u22121, the process proceeds to step S. In step S, the feature extractor  of the terminal B determines whether Sig(n\u22121) is equal to 1. If it is determined that Sig(n\u22121) is not equal to 1, the process proceeds to step S. In step S, the value of Tv(P) is set to c*Tclk. Then in step S, the feature extractor  of the terminal B outputs Tv(P) and resets the value of the parameter c to 0. Thereafter, the process proceeds to step S.","In the case in which it is determined in step S that Sig(n\u22121) is equal to \u22121, the process proceeds to step S. In step S, the feature extractor  of the terminal B sets the value of Tv\u2212(P) to a*Tclk. Then, in step S, Tv\u2212(P) is output. Furthermore, in step S, the value of the parameter a is reset to 0. The process then proceeds to step S. In step S, the value of the parameter P is incremented by 1, and the process proceeds to step S.","In the case in which it is determined in step S that Sig(n\u22121) is equal to \u22121, the process proceeds to step S. In step S, the feature extractor  of the terminal B sets the value of Tv+(P) to b*Tclk. Then in step S, the feature extractor  of the terminal B outputs Tv+(P) and, in step S, resets the value of the parameter b to 0. Thereafter, the process proceeds to step S.","Thus, in the above iteration loop, the value of a is incremented when the waveform is detected to be negative in polarity, while the value of b is incremented when the waveform is detected to be positive in polarity. When the waveform is detected to be 0, the value of c is incremented. When a transition occurs in polarity of the waveform, a, b, or c is multiplied by the sampling interval Tclk. Thus, as shown in , periods during which the waveform is positive are given as Tv+(), Tv+(), and so on, and periods during which the output level of the waveform is within the range from \u2212Vth to +Vth are given as Tv(), Tv(), Tv(), and so on. Periods during which the waveform is negative are given as Tv\u2212(), Tv\u2212(), and so on.","Now, referring to , the Vmin\/Vmax detection process is described below. First, in step S, the feature extractor  of the terminal B sets a parameter n to 2 and a parameter P to 1. Then in step S, the feature extractor  reads Vs(n). In step S, the feature extractor  determines whether Vs(n) exists. If it is determined that Vs(n) does not exit, the process is ended. In the case in which it is determined in step S that Vs(n) exists, the process proceeds to step S. In step S, the feature extractor  of the terminal B the feature extractor  reads Vs(n\u22121), and, in step S, sets the value of Vs(n)\u2212Vs(n\u22121) into \u0394Vs (n\u22121). In step S, the value of \u0394Vs (n\u22121) is stored.","In step S, the feature extractor  of the terminal B determines whether \u0394Vs(n\u22121) is smaller than 0. If it is determined that \u0394Vs(n\u22121) is smaller than 0, then, in step S, the feature extractor  sets \u0394VSig(n) to \u22121. If it is determined in step S that \u0394Vs(n\u22121) is not smaller than 0, then the process proceeds to step S. In step S. \u0394VSig(n) to 1 The process then proceeds to step S. In step S, it is determined whether \u0394VSig(n\u22121) exists. If it is determined that \u0394VSig(n\u22121) does not exit, the process proceeds to step S. In step S, the value of n is incremented by 1. Thereafter, the process returns to step S.","In the case in which it is determined in step S that \u0394VSig(n\u22121) exists, the process proceeds to step S. In step S, the feature extractor  of the terminal B reads \u0394VSig(n\u22121). In step S, the feature extractor  determines whether \u0394VSig(n) and \u0394VSig(n\u22121) are equal in value to each other. If it is determined that \u0394VSig(n) and \u0394VSig(n\u22121) are not equal in value to each other, the process proceeds to step S. In step S, the feature extractor  determines whether Vs(n) is smaller than Vth. If it is determined that Vs(n) is not smaller than Vth, the process proceeds to step S. In step S, Vmax(P) is set to be equal to Vs(n\u22121) and Vmax(P) is output. The value of P is then incremented by 1 and the process proceeds to step S.","If it is determined in step S that \u0394VSig(n) and \u0394VSig(n\u22121) are equal in value to each other, the feature extractor  of the terminal B advances the process to step S.","In the case in which it is determined in step S that Vs(n) is smaller than Vth, then the process proceeds to step S. In step S, the feature extractor  of the terminal B sets the value of Vmin(P) to be equal to the value of Vs(n\u22121) and outputs Vmin(P). Thereafter, the process proceeds to step S.","Thus, as shown in , peak values Vmax() and Vmax() in respective periods Tv+() and Tv+() of the waveform  are obtained, and similarly, peak values Vmin() and Vmin() in respective periods Tv\u2212() and Tv\u2212() of the waveform  are obtained.","Referring again to , in step S, the feature extractor  of the terminal B sets the value of the parameter P to 1. In step S, the feature extractor  of the terminal B determines whether Tv+(P) or Tv\u2212(P) exists. If it is determined that Tv+(P) or Tv\u2212(P) exists, the process proceeds to step S. In step S, the value of Tv+(P) is set into Tppg+(P), and, in step S, the value of Tv\u2212(P) is set into Tpp\u2212(P). Furthermore, in step S, the feature extractor  of the terminal B sets the sum of Tppg+(P) and Tppg\u2212(P) into Tppgi(P). Then in step S, the value of the parameter P is incremented by 1. Then the process returns to step S, and the above-described process is repeated.","For example, when P is set to 3 after Tv+(), Tv+(), Tv\u2212(), and Tv\u2212() have been obtained as a result of outputting Tv in step S, it is determined in step S that Tv+(P) or Tv\u2212(P) does not exist. In this case, in step S, the feature extractor  of the terminal B calculates the mean value of each of Tppg+, Tppg\u2212, and Tppgi as Tppg+m, Tppg\u2212m, and Tppgim as follows: Tppg+m=(Tppg+()+Tppg+())\/2, Tppg\u2212m=(Tppg\u2212()+Tppg\u2212())\/2, and Tppgim=(Tppgi()+Tppgi())\/2.","In the next step S, the feature extractor  of the terminal B sets the value of the parameter P to 1. Then, in step S, the feature extractor  of the terminal B determines whether Vmax(P) or Vmin(P) exists. If it is determined that Vmax(P) or Vmin(P) exists, then, in step S, the value of Vmax(P) is set into Appg+(P), and furthermore in step S the value of Vmin(P) is set into Appg\u2212(P). Then, in step S, the value of the parameter P is incremented by 1. Then the process returns to step S, and the above-described process is repeated.","After Vmax(), Vmax(), Vmin(), and Vmin() have been obtained as a result of the Vmin\/Vmax detection process in step S, when the value of the parameter P is set to 3, it is determined in step S that Vmax(P) or Vmin(P) does not exist. In this case, in step S, the feature extractor  of the terminal B calculates the mean values of Appg+ and Appg\u2212 as Appg+m and Appg\u2212m as follows:\n\n=(+(1)++(2))\/2 and\n\n=(\u2212(1)+\u2212(2))\/2.\n","Then in step S, the feature extractor  of the terminal B generates a feature value vector Vc(Tppg+m, Tppg\u2212m, Tppgim, Appg+m, Appg\u2212m).",{"@attributes":{"id":"p-0229","num":"0228"},"figref":["FIG. 25","FIG. 25"],"b":["241","241","241"]},"In the above-described manner, the feature vector Vc of pulse information corresponding to a low-level interest of the user a is generated.","Referring to , the process of generating a feature vector Vc of body motion information of the user a is described below.","In step S, the feature extractor  of the terminal B reads sampled body motion information. Then in step S, the feature extractor  of the terminal B performs the polarity detection process described above with reference to , then in step S the Tv generation process described above with reference to , and furthermore in step S, the Vmin\/Vmax detection process. Then in step S, the feature extractor  of the terminal B sets the value of the parameter P to 1. In step S, the feature extractor  of the terminal B reads the values of \u0394V stored in the Vmax\/Vmin detection process (step S shown in ).",{"@attributes":{"id":"p-0233","num":"0232"},"figref":"FIG. 26","b":["261","1","2","147","112","148","112","149","150","112","146"]},"Because the \u0394V(P) represents the distance the user a moves in the period of time Tclk, the Vel(P) indicates the speed at which the user a moves and \u03a3\u0394V indicates the total distance the user a moves.","If it is determined in step S that \u0394V(P) does not exist, then, in step S, the feature extractor  of the terminal B sets the value of \u03a3\u0394V into D and calculates the mean value Velm of the speed Vel. Then, in step S, the feature extractor  of the terminal B generates a feature vector Vc(Velm, D).","In the above-described manner, a feature vector Vc of body motion information corresponding to a low-level interest of the user a is generated.","Thus, the terminal B generates feature value vectors Vc and Vc from pulse information and body motion information of the user a transmitted from the terminal A and the terminal B stores the generated feature value vectors Vc and Vc. Similarly, the terminal B generates feature value vectors from pulse information and body motion information of the user b and stores the generated feature value vectors. As a result, as shown in , a learning result  associated with pulse information and a learning result  associated with body motion information are stored in the storage unit  of the terminal B, for the user a and the user b, separately.","Similarly, the terminal A stores learning results associated with pulse information and body motion information of the user a and the user b, separately.","Once a sufficient amount of vital sign information has been stored as a result of a repetition of above-described vital sign information storage process, it becomes possible to identify the emotion\/state and the level of each user from vital sign information.","Referring to , a process performed by the terminal B to identify an emotional state is described below.","First, in step S, at the start of communication with the terminal A, the system controller  of the terminal B acquires information PID identifying the user a of the terminal A with which to communicate. In step S, the system controller  of the terminal B commands, the information discriminator  to determine whether voice information of the user a is received from terminal A. If voice information is not yet received, the process waits until voice information is received. If it is determined that voice information is input, then, in step S, the system controller  of the terminal B commands the feature extractor  to start sampling of pulse information and body motion information of a user a. Then in step S, the system controller  of the terminal B commands the meaning extractor  to recognize the voice.","In step S, the system controller  of the terminal B determines whether one phrase of voice of the user a has been input. If one phrase of voice has not yet been input, the system controller  waits until one phrase of voice is input.","If it is determined in step S that one phrase has been input, then, in step S, the system controller  of the terminal B commands the feature extractor  to end the sampling of pulse information and body motion information.","The sampling of pulse information and body motion information may be performed by the feature extractor  of the terminal A and the sampled information may be transmitted to the terminal B.","In step S, the system controller  of the terminal B commands the feature extractor  to generate a feature vector Vc on the basis of pulse information of the user a. In step , the system controller  of the terminal B commands the feature extractor  to generate a feature vector Vc on the basis of body motion information of the user a. In step S, the system controller  of the terminal B commands the learning unit  to identify the emotional state of Vc and the level thereof. In step S, the system controller  of the terminal B commands the learning unit  to identify the emotional state of Vc and the level thereof. In step S, the system controller  of the terminal B commands the learning unit  to output the identification result and perform a recalculation.","Now, referring to , the process of generating the feature vector Vc of pulse information is described below. This process is performed by the feature extractor  of the terminal B. In step S, the feature extractor  of the terminal B reads sampled pulse information. In step S, the feature extractor  of the terminal B performs the polarity detection process described above with reference to , and, in step S, the Tv generation process described above with reference to , and furthermore in step S the Vmin\/Vmax detection process described above with reference to .","In step S shown in , the feature extractor  of the terminal B sets the value of the parameter P to 1. In step S, the feature extractor  of the terminal B determines whether Tv+(P) or Tv\u2212(P) exists. If it is determined that Tv+(P) or Tv\u2212(P) exists, the process then proceeds to step S. In step S, the value of Tv+(P) is set into Tppg+(P). In step S, the value of Tv\u2212(P) is set into Tppg\u2212(P). In step S, the feature extractor  of the terminal B sets the sum of Tppg+(P) and Tppg\u2212(P) into Tppgi(P). Then in step S, the value of the parameter P is incremented by 1. Thereafter, the flow returns to step S, and the above-described process is repeated.","If it is determined in step S that Tv+(P) or Tv\u2212(P) does not exist, the process jumps to step S. In step S, the feature extractor  of the terminal B calculates the mean value of each of Tppg+, Tppg\u2212, and Tppgi as Tppg+m, Tppg\u2212m, and Tppgim, respectively.","In the next step S, the feature extractor  of the terminal B sets the value of the parameter P to 1. In step S, the feature extractor  of the terminal B determines whether Vmax(P) or Vmin(P) exists. If it is determined that Vmax(P) or Vmin(P) exists, the process proceeds to step S. In step S, the value of Vmax(P) is set into Appg+(P), and, in step S, the value of Vmin(P) is set into Appg\u2212(P). Then in step S, the value of the parameter P is incremented by 1. Thereafter, the flow returns to step S, and the above-described process is repeated.","If it is determined in step S that Vmax(P) or Vmin(P) does not exist, the process proceeds to step S. In step S, the feature extractor  of the terminal B calculates the mean value of each of Appg+ and Appg\u2212 as Appg+m and Appg\u2212m, respectively.","Then in step S, the feature extractor  of the terminal B generates a feature value vector Vc(Tppg+m, Tppg\u2212m, Tppgim, Appg+m, Appg\u2212m).","In the above-described manner, the feature vector Vc of pulse information of the user a is generated.","Now, referring to , the process of generating a feature vector Vc of body motion information of the user a is described below.","In step S, the feature extractor  of the terminal B reads sampled body motion information. In step S, the feature extractor  of the terminal B performs the polarity detection process described above with reference to , and, in step S, the Tv generation process described above with reference to , and furthermore in step S, the Vmin\/Vmax detection process described above with reference to . Then in step S, the feature extractor  of the terminal B and a parameter P to 1. In step S, the feature extractor  of the terminal B reads the values of \u0394V stored in the Vmax\/Vmin detection process (step S shown in ).","In step S, the feature extractor  of the terminal B determines whether \u0394V(P) exists. If it is determined that \u0394V(P) exists, then, in step S, the feature extractor  of the terminal B sets the value of |\u0394V(P)|\/Tclk into Vel(P), and furthermore, in step S, the value of \u03a3\u0394V+|\u0394V(P)| into \u03a3\u0394V. Thereafter, in step S, the feature extractor  of the terminal B increments the value of P by 1 and returns the flow to step S to repeat the above-described process.","If it is determined in step S that \u0394V(P) does not exist, the process proceeds to step S. In step S, the feature extractor  of the terminal B sets the value of \u03a3\u0394V into D and calculates the mean value Velm of Vel. Then in step S, the feature extractor  of the terminal B generates a feature vector Vc(Velm, D).","In the above-described manner, the feature vector Vc of body motion information of the user a is generated.","Now, referring to , a process of identifying the emotional state and the level of Vc is described below. This process is performed by the learning unit  of the terminal B.","In step S, the learning unit  of the terminal B reads, from the storage unit , the feature value vector distribution fvc, the distribution center vector V\u03bc, and the distribution standard deviation vector V\u03c3 associated with pulse information of the user a.","As described above, the emotion\/state is classified into twelve kinds, as shown in Table 1, and each kind of emotion\/state is divided into three strength levels: high level, middle level, and low level. Therefore, there are a total of 36 kinds of emotion information. Thus, there are 36 feature value vector distributions fvc, 36 distribution center vectors V\u03bc, and 36 distribution standard deviation vectors V\u03c3 corresponding to the respective 36 kinds of emotion information.","In step S, the learning unit  of the terminal B sets a parameter Q to 1. Then in step S, the learning unit  of the terminal B calculates the inner product of the vector V\u03bc(Q) and the feature value vector Vc calculated in step S (in ), and stores V\u03bc(Q) in relation to the inner product. Then in step S, the learning unit  of the terminal B increments the value of Q by 1. In step , the learning unit  of the terminal B determines whether the calculation is completed for all kinds of the emotion information, that is, whether the calculation of the inner product is completed for all 36 center vectors V\u03bc. If it is determined that the inner product is not calculated for all kinds of the emotion information, the flow returns to step S, and the above-described process is repeated.","When the calculation of the inner product is completed for all 36 center vectors V\u03bc corresponding to the respective kinds of emotion information, data is stored as shown in  in the storage unit (not shown) in the learning unit  of the terminal B. In step S, the learning unit  of the terminal B sorts the data in descending order of inner product.","In step S, the learning unit  of the terminal B sets a parameter R to 1. In step S, the learning unit  of the terminal B determines whether data associated with all kinds of the emotion information are read. If it is determined that data associated with all kinds of the emotion information are not read, the process proceeds to step S. In step S, the sorted data is read. In the example shown in Table 2, a center vector V\u03bc- of a feature value vector corresponding to a \u201cmiddle-level stable state\u201d has a greatest inner product, and thus data corresponding to the center vector V\u03bc- is first read.","In step S, the learning unit  of the terminal B sets a parameter m to 1. Then in step S, the learning unit  of the terminal B determines whether the condition V\u03bc(R)m\u2212V\u03c3(R)m\u2266Vcm\u2266V\u03bc(R)m+V\u03c3(R)m is satisfied, wherein V\u03bc(R)m denotes an mth component of the vector V\u03bc(R), and V\u03c3(R)m and Vcm respectively denote mth components of vectors V\u03c3 and Vc. As described above, the feature value vector of pulse information includes following five components: Tppg+m, Tppg\u2212m, Tppgim, Appg+m, and Appg\u2212m. When m=1, the component Tppg+m is extracted from each of vectors V\u03bc(R), V\u03c3(R), Vc and subjected to the process.","If it is determined in step S that the condition V\u03bc(R)m\u2212V\u03c3(R)m\u2266Vcm\u2266V\u03bc(R)m+V\u03c3(R)m is not satisfied, then, in step S, the learning unit  of the terminal B increments the value of R by 1 and returns the flow to step S to repeat the above-described process. If it is determined in step S that the condition V\u03bc(R)m\u2212V\u03c3(R)m\u2266Vcm\u2266V\u03bc(R)m+V\u03c3(R)m is satisfied, then, in step S, the learning unit  of the terminal B increments the value of m by 1 and advances the process to step S. In step S, the learning unit  of the terminal B determines whether m is equal to 6. If it is determined that m is not equal to 6, the flow returns to step S to repeat the above-described process.","If it is determined in step S that m is equal to 6, that is, if it is determined that the condition V\u03bc(R)m\u2212V\u03c3(R)m\u2266Vchm\u2266V\u03bc(R)m+V\u03c3(R)m is satisfied for all five components of respective vectors V\u03bc(R), V\u03c3(R), and Vcc, the process proceeds to step S. In step S, the learning unit  of the terminal B determines that Vc belongs to emotion information E corresponding to R. When R=1, the emotion information E corresponding to R is a \u201cmiddle-level stable state\u201d.","If it is determined in step S that data associated with all kinds of the emotion information are read, the learning unit  of the terminal B determines that the emotion\/state and the level of Vc cannot be identified, and ends the process.",{"@attributes":{"id":"p-0268","num":"0267"},"figref":["FIG. 35","FIG. 28"],"b":["381","1","177","382","1","29","383","1","381","1"]},"Now, referring to , a process of identifying the emotional, the state, and the level of Vc is described below. This process is performed by the learning unit  of the terminal B.","In step S, the learning unit  of the terminal B reads, from the storage unit , the feature value vector distribution fvc, the distribution center vector V\u03bc, and the distribution standard deviation vector V\u03c3 associated with body motion information of the user a.","As described above, the emotion\/state is classified into twelve kinds, as shown in Table 1, and each kind of emotion\/state is divided into three strength levels: high level, middle level, and low level. Therefore, there are a total of 36 kinds of emotion information. Thus, there are 36 feature value vector distributions fvc, 36 distribution center vectors V\u03bc, and 36 distribution standard deviation vectors V\u03c3 corresponding to the respective 36 kinds of emotion information.","In step S, the learning unit  of the terminal B sets a parameter Q to 1. In step S, the learning unit  of the terminal B calculates the inner product of the vector Vp(Q) and the feature value vector Vc calculated in step S (in ), and stores V\u03bc(Q) in relation to the inner product. Then in step S, the learning unit  of the terminal B increments the value of Q by 1. In step , the learning unit  of the terminal B determines whether the calculation is completed for all kinds of the emotion information, that is, whether the calculation of the inner product is completed for all 36 center vectors V\u03bc. If it is determined that the inner product is not calculated for all kinds of the emotion information, the flow returns to step S, and the above-described process is repeated.","When the calculation of the inner product is completed for all 36 center vectors V\u03bc corresponding to the respective kinds the emotion information, data is stored as shown in  in the storage unit (not shown) in the learning unit  of the terminal B. In step S, the learning unit  of the terminal B sorts the data in descending order of inner product.","In step S, the learning unit  of the terminal B sets the parameter R to 1. In step S, the learning unit  of the terminal B determines whether data associated with all kinds of the emotion information are read. If it is determined that data associated with all kinds of the emotion information are not read, the process proceeds to step S. In step S, the sorted data is read. In the example shown in , a center vector V\u03bc- of a feature value vector corresponding to a \u201cmiddle-level stable state\u201d has a greatest inner product, and thus data corresponding to the center vector V\u03bc- is first read.","In step S, the learning unit  of the terminal B sets the parameter m to 1. Then in step S, the learning unit  of the terminal B determines whether the condition V\u03bc(R)m\u2212V\u03c3(R)m\u2266Vcm\u2266V\u03bc(R)m+V\u03c3(R)m is satisfied, wherein V\u03bc(R)m denotes an mth component of the vector V\u03bc(R), and V\u03c3(R)m and Vcm respectively denote mth components of vectors V\u03c3 and Vc. As described above, the feature value vector of body motion information includes two components Velm and D. When m=1, the Velm component of each of vectors V\u03bc(R), V\u03c3(R), and Vc is extracted and subjected to the process.","If it is determined in step S that the condition V\u03bc(R)m\u2212V\u03c3(R)m\u2266Vcm\u2266V\u03bc(R)m+V\u03c3(R)m is not satisfied, then, in step S, the learning unit  of the terminal B increments the value of R by 1 and returns the flow to step S to repeat the above-described process. On the other hand, if it is determined in step S that the condition V\u03bc(R)m\u2212V\u03c3(R)m\u2266Vcm\u2266V\u03bc(R)m+V\u03c3(R)m is satisfied, then, in step S, the learning unit  of the terminal B increments the value of m by 1 and advances the process to step S. In step S, the learning unit  of the terminal B determines whether m is equal to 3. If it is determined that m is not equal to 3, then the flow returns to step S to repeat the above-described process.","If it is determined in step S that m is equal to 3, that is, if it is determined that the condition V\u03bc(R)m\u2212V\u03c3(R)m\u2266Vcm\u2266V\u03bc(R)m+V\u03c3(R)m is satisfied for two components of respective vectors V\u03bc(R), V\u03c3(R), and Vc, the process proceeds to step S. In step S, the learning unit  of the terminal B determines that Vc belongs to emotion information E corresponding to R. When R=1, the emotion information E corresponding to R is a \u201cmiddle-level stable state\u201d.","If it is determined in step S that data associated with all kinds of emotion information are read, the learning unit  of the terminal B determines that the emotion\/state and the level of Vc cannot be identified, and ends the process.",{"@attributes":{"id":"p-0279","num":"0278"},"figref":["FIG. 38","FIG. 38","FIG. 28"],"b":["401","2","178","402","2","29","403","2","401","2"]},"Now, referring to , the process of outputting the identification result and performing a recalculation is described below. This process is performed by the learning unit  of the terminal B. In step S, the learning unit  of the terminal B determines whether the emotion information E identified by Vc and the emotion information E identified by Vc are identical to each other. If it is determined that E and E are not identical to each other, then, in step S, the learning unit  of the terminal B corrects the identification result such that the feature value vector Vc of body motion information generated in step S () belongs to E. As described above, emotion information E is identified by the feature value vector Vc generated from pulse information, and emotion information E is identified on the basis of body motion information. E and E are not necessarily identical to each other. If E and E are different, emotion information identified by a vector with a greater number components, that is, with a greater number of dimensions is selected. In this specific case, emotion information E identified by a 5-dimensional vector Vc is selected, and emotion information E identified by a 2-dimensional vector Vc is corrected such that the feature value vector Vc must correspond to emotion information E.","In step S, the learning unit  of the terminal B outputs the identification result to the output controller . In step S, the learning unit  of the terminal B recalculates the distribution fvc of the feature value vector of pulse information corresponding to emotion information E of the user a and the center vector V\u03bc and the standard deviation vector V\u03c3 of the distribution fvc, and the distribution fvc of the feature value vector of body motion information corresponding to emotion information E of the user a and the center vector V\u03bc and the standard deviation vector V\u03c3 of the distribution fvc, and the learning unit  of the terminal B stores the recalculated result. More specifically, the feature value vectors Vc and Vc generated in steps S and S () are respectively added to the feature value vector of pulse information indicating the \u201cmiddle-level stable state\u201d of the user a and the feature value vector of body motion information stored in the storage unit, and fvc, V\u03bc, and V\u03c3 and also fvc, V\u03bc, and V\u03c3 are recalculated and stored in the storage unit .","In this way, the emotion\/state and the level are identified from pulse information and body motion information, and information associated with the feature vectors of the identified emotion, state, and level is stored.","Thus, the terminal B generates feature value vectors Vc and Vc from pulse information and body motion information of the user a transmitted from the terminal A, and the terminal B identifies the emotion\/state and the strength level from the generated feature value vectors Vc and Vc. Similarly, the terminal B also can generate feature value vectors from pulse information and body motion information of the user b and can identify the emotion\/state and the level thereof.","Similarly, the terminal A identifies the emotion\/state and the strength level of the user a and the user b.","Now, referring to , the process of outputting information is described below. In step S, the system controller  determines whether information to be output is selected. If information to be output is not selected, the process waits until information to be output is selected. The selection of information to be output may be performed, for example, by clicking a menu item displayed on the display of the terminal. The selection menu used by a user to select information to be output includes the following four items: \u201cidentification result\u201d, \u201cinformation based on feature values\u201d, \u201cwaveform information\u201d, and \u201cstored information\u201d.","If it is determined in step S that information to be output is selected, then in step S the system controller  determines whether the information selected to be output is the identification result. If it is determined that the information selected to be output is the identification result, then in step S the system controller  transmits, to the output controller , 2-bit selection information 00 indicating that the identification result should be output.","In step S, the output controller  outputs an identification result associated with the emotion\/state and the level thereof. For example, a text message \u201cA middle-level stable state detected\u201d is displayed on the display of the terminal.","If it is determined in step S that the information selected to be output is not the identification result, then, in step S, the system controller  determines whether the information selected to be output is the information based on feature values. If it is determined that the information selected to be output is the information based on feature values, then, in step S, the system controller  transmits, to the output controller , 2-bit selection information 01 indicating that the information based on feature values should be output.","In step S, the output controller  acquires a feature value (Tppg+, Tppg\u2212, Tppgi, Appg+, Appg\u2212) from the feature extractor . In step S, the output controller  generates a light emission unit driving signal. More specifically, as shown in , a sinusoidal wave signal with an amplitude Appg+, an on-period Tppg+, and an off-period Tppg\u2212 is generated. In step S, the output controller  drives the light emission unit .","In step S, the output controller  acquires a feature value (Vel, \u0394V) from the feature extractor . Then in step S, the output controller  generates a driving signal for causing a moving object to be moved a distance \u0394V at a moving speed of Vel. In step S, the output controller  drives the moving object .","If it is determined in step S that the information selected to be output is not the information based on feature values, then, in step S, the system controller  determines whether the information selected to be output is the waveform information. If it is determined that the information selected to be output is the waveform information, the system controller  transmits, to the output controller , 2-bit selection information 10 indicating that the waveform information should be output. In step S, the output controller  acquires sampled pulse information from the feature extractor  and generates a waveform signal. In step S, the output controller  drives the light emission unit .","In step S, the output controller  acquires sampled pulse information from the feature extractor  and generates a waveform signal. In step S, the output controller  drives the moving object .","If it is determined in step S that the information selected to be output is not the waveform information, then in step S the system controller  determines whether the information selected to be output is the stored information. If it is determined that the information selected to be output is the stored information, then, in step S, the system controller  transmits, to the output controller , 2-bit selection information 01 indicating that the stored information should be output.","In the case in which the information selected to be output is the stored information, a sub-menu is further displayed on the display of the terminal so that a user can specify an item of stored information to be output. More specifically, the user can make selections in terms of items of information inf or inf such as pulse information or body motion information, information PID identifying a user, and emotion information E. For example, the user can select pulse information as inf, body motion information as inf, information indicating the user a as PID, and \u201cstrong interest\u201d as emotion information E.","In step S, the system controller  determines whether the items of information inf and inf, information PID identifying a user, and emotion information E are selected. If selection is not performed, the process waits until selection is made.","In step S, the system controller  generates information (inf, inf, PID, E) in accordance with the selection made by the user and transmits the generated information to the output controller .","In step S, on the basis of (inf, inf, PID, E), the output controller  acquires the center vector of pulse information and the center vector of body motion information from the storage unit . In this specific case, the center vector V\u03bc(Tppg+, Tppg\u2212, Tppgi, Appg+, Appg\u2212) of feature value vectors of pulse information corresponding to \u201chigh-level interest\u201d of the user a and the center vector V\u03bc(Velm, D) of feature value vectors of body motion information corresponding to \u201chigh-level interest\u201d of the user a are acquired from the storage unit.","In step S, the output controller  generates a light emission unit driving signal in accordance with the center vector V\u03bc. More specifically, as shown in , a sinusoidal wave signal  with an amplitude Appg+, an on-period Tppg+, and an off-period Tppg\u2212 is generated. Then, in step S, the output controller  drives the light emission unit  three times in accordance with the generated driving signal.","In step S, on the basis of the center vector V\u03bc, the output controller  generates a driving signal for causing the moving object to be moved a distance D at a moving speed of Vel. Then, in step S, the output controller  drives the moving object .","If it is determined in step S that the information selected to be output is not the stored information, then in step S the system controller  performs error handling. After completion of the error handling, the process is ended.","Thus, the emotion\/state and the strength level thereof identified from the vital sign information are displayed so that the user can recognize them. Furthermore, light may be blinked and\/or the object may be moved so that the user can recognize vital sign information via bodily sensations.",{"@attributes":{"id":"p-0302","num":"0301"},"figref":"FIG. 44","b":["301","303","301","304","303","305","304","302","303","306","307"]},"The processing unit  includes an information discriminator  for classifying information input via the input unit  into linguistic information or keyboard vibration information, a feature extractor  for extracting property information from keyboard vibration information, a meaning extractor  for extracting emotion information from linguistic information, an output controller  for outputting information to the output unit . The communication unit  includes an information transmitting unit  for transmitting information and an information receiving unit  for receiving information.",{"@attributes":{"id":"p-0304","num":"0303"},"figref":["FIG. 45","FIG. 45"],"b":["301","301","341","342","341","343","342"]},{"@attributes":{"id":"p-0305","num":"0304"},"figref":["FIG. 46","FIG. 46"],"b":["302","302","362","362","361"]},"Herein, an information processing apparatus of a user a is denoted by a terminal A, and an information processing apparatus of a user b is denoted by a terminal B. The user a and the user b can communicate with each other using their terminals A and B in accordance with a procedure shown in . First, the terminal A transmits a connection request to the terminal B. In response, the terminal B returns a connection acknowledgment signal to the terminal A. Thereafter, textual information such as a mail or a chat is transmitted between the terminal A and the terminal B.","The terminal A performs initial setting associated with transmission\/reception of vibration information (vibration generated when a user presses a keyboard, in this specific example), and transmits a transmission\/reception request to the terminal B together with transmission setting information and reception setting information such as those shown in , as in the EXAMPLE 1.","If the terminal B receives the vibration information transmission request from the terminal A, the terminal B changes the setting associated with transmission\/reception of vibration information and returns an acknowledgment signal to the terminal A.","The terminal A then transmits vibration information of the user a to the terminal B, and the terminal B transmits vibration information of the user b to the terminal A. The terminal A and the terminal B store the received vibration information.","If the terminals A transmits a connection end request to the terminal B, the terminal B transmits a connection end acknowledgment signal, and the communication is ended.","Communication is performed between the terminal A and the terminal B in the above-described manner, and vibration information is stored in the terminal A and the terminal B. After a sufficient amount of vibration information is stored, transmission\/reception of information is performed as shown in . If vibration information is transmitted between the terminal A and the terminal B, the terminal A and the terminal B detect the emotion or the state of users.","Referring to , a process performed by the terminal B to store vibration information is described below. First, in step S, at the start of communication with the terminal A, the system controller  of the terminal B acquires information (for example, a name or a nickname) PID identifying the user a of the terminal A with which to communicate. In step S, the system controller  of the terminal B commands the information discriminator  to determine whether the keyboard of the terminal A is pressed to input information. If information input via the keyboard is not yet received, the process waits until information input via the keyboard is received.","If it is determined in step S that information is input by pressing the keyboard, then, in step S, the system controller  of the terminal B commands the feature extractor  to start sampling of vibration information that is generated when the user a presses the keyboard and that is transmitted from the terminal A. Then in step S, the system controller  of the terminal B analyzes input text information, using the meaning extractor . In step S, the system controller  of the terminal B determines whether the recognized text information includes a word indicating an emotion, a state, and a strength level of an emotion\/state of the user a. If it is determined that the recognized text information does not include a word indicating an emotion, a state, or a strength level of an emotion\/state, then the process returns to step S, and the above-described process is repeated. Words indicating emotions or states may be such as those shown in , and words indicating strength levels may be such as those shown in , as in EXAMPLE 1.","In the case in which it is determined in step S that the text information includes a word indicating an emotion, a state, or the strength level of an emotion\/state, then, in step S, the system controller  of the terminal B determines whether the subject of the detected word is the user a, that is, whether the detected word indicates the emotion, state, or the level of the emotion of the user a. If the subject is not the user a, then the process returns to step S, and the above-described process is repeated.","In the case in which it is determined in step S that the user is the subject, the process proceeds to step S. In step S, the system controller  of the terminal B determines whether one phrase of text information has been input. If one phrase of voice has not yet been input, the system controller  of the terminal B waits until one phrase of text information is input.","If it is determined in step S that one phrase has been input, then, in step S, the system controller  of the terminal B commands the feature extractor  to end the sampling of vibration information indicating a vibration caused by pressing of the keyboard.","The sampling of vibration information indicating a vibration caused by pressing the keyboard may be performed by the feature extractor  of the terminal A and the sampled information may be transmitted to the terminal B. Then in step S, the system controller  of the terminal B extracts emotion information E via the meaning extractor .","For example, when the user a inputs \u201cI am very glad.\u201d, an emotional state corresponding to a word \u201cglad\u201d is joy, and a strength level corresponding to a word \u201cvery\u201d is a high level. Thus, a high-level joyful state is detected as the emotion information E.","In step S, the system controller  of the terminal B commands the feature extractor  to generate feature vector Vc on the basis of pulse information of the user a. In step S, vibration information corresponding to information PID identifying the user and the emotion information E is read from the storage unit . In this specific case, a feature vector of vibration information corresponding to weak interest of the user a is read. Then in step S, using the learning unit , the system controller  of the terminal B adds the generated feature vector Vc to the read feature vector and recalculates the vector distribution fvc, the distribution center vector V\u03bc, and the standard deviation vector V\u03c3. In step S, the recalculated vectors are stored in the storage unit .","Now, referring to , the process of generating the feature vector Vc of vibration information is described below. This process is performed by the feature extractor  of the terminal B. In step S, the feature extractor  of the terminal B reads sampled vibration information. Thereafter, in step S, as in EXAMPLE 1, the feature extractor  of the terminal B performs the polarity detection process described above with reference to , and, in step S, the Tv generation process described above with reference to , and furthermore in step S the Vmin\/Vmax detection process described above with reference to .",{"@attributes":{"id":"p-0321","num":"0320"},"figref":"FIG. 54","b":["491","491","492","494","493","495","492","494","0","0","493","495","0","0","0"]},"In step S shown in , the feature extractor  of the terminal B sets both parameters n and P to 1. Then, in step S, the feature extractor  of the terminal B determines whether Tv(P) is greater than a threshold value Th. If it is determined that Tv(P) is greater than the threshold value Th, then, in step S, the feature extractor  of the terminal B sets Tstr(n)+Tv+(P)+Tv\u2212(P) into Tstr(n). Note that Tstr(n) is initially set to 0.","In step S, the feature extractor  of the terminal B determines whether Vmax(P) is greater than Imax(n). Note that the initial value of Imax(n) is 0. If it is determined in step S that Vmax(P) is greater than Imax(n), then, in step S, the feature extractor  of the terminal B sets the value of Vmax(P) into Imax(n). Thereafter, in step S, the feature extractor  of the terminal B increments the value of P by 1 and returns the flow to step S to repeat the above-described process. On the other hand, if it is determined in step S that Vmax(P) is not greater than Imax(n), step S is skipped.","In the case in which it is determined in step S that Tv(P) is greater than the threshold value Th, then in step S, the feature extractor  of the terminal B sets the value of Tv(P) into Tint(n), increments the value of P by 1, and generates a feature value vector Vc(Tstr(n), Tint(n), Imax(n)). In step S, the feature extractor  of the terminal B determines whether Tv(P) exists. If it is determined that Tv(P) exists, then in step S, the feature extractor  of the terminal B increments the value of n by 1. Thereafter, the flow returns to step S, and the above-described process is repeated.","On the other hand, if it is determined in step S that Tv(P) does not exist, the feature extractor  of the terminal B ends the process.","Thus, as shown in , Tstr() , Tint() , Tstr() , Tint() , Tstr() , Tint() , Tstr() , Tint() , Tstr() , and Tint()  are obtained from the waveform  shown in . Tstr() to Tstr() indicate periods during which some keys are pressed down by the user, and Tint() to Tint() indicate intervals between two adjacent periods during which some keys are pressed down. In this specific example, a key (keys) is pressed down five times.","Furthermore, as shown in , Imax() , Imax() , Imax() , Imax() , and Imax()  are obtained from the waveform  shown in , wherein Imax() to Imax() indicate maximum vibration strengths of first to fifth vibration periods.","In the above-described manner, a feature vector Vc of vibration information corresponding to the high-level joy of the user a is generated.","As described above, in accordance with keyboard vibration information received from the terminal A of the user a, the terminal B generates the feature value vector Vc and stores it. Similarly, the terminal B generates a feature value vector from vibration information associated with a keyboard of the user b and stores the generated feature value vector. Thus, as shown in , a learning result  associated with keyboard vibration information is stored in the storage unit  of the terminal B, for the user a and the user b, separately.","Similarly, the terminal A stores a learning result associated with keyboard vibration information, for the user a and the user b, separately.","Once a sufficient amount of vibration information has been stored as a result of a repetition of above-described vibration information storage process, it becomes possible to identify the emotion\/state and the strength level of each user from vibration information.","Referring to , a process performed by the terminal B to identify an emotional state is described below.","First, in step S, at the start of communication with the terminal A, the system controller  of the terminal B acquires information PID identifying the user a of the terminal A with which to communicate. In step S, the system controller  of the terminal B commands the information discriminator  to determine whether the keyboard of the terminal A of the user a is pressed to input information. If information input via the keyboard is not yet received, the process waits until information input via the keyboard is received.","If it is determined in step S that the keyboard is pressed to input information, then, in step S, the system controller  of the terminal B commands the feature extractor  to start sampling of vibration information that is generated when the user a presses the keyboard and that is transmitted from the terminal A. Then in step S, the system controller  of the terminal B commands the meaning extractor  to determine whether one phrase of text information has been input. If one phrase of textual information has not yet been input, the process waits until one phrase of textual information is input. If it is determined in step S that one phrase has been input, the system controller  of the terminal B commands the feature extractor  to end sampling of vibration information that is generated when the user a presses the keyboard and that is transmitted from the terminal A.","The sampling of vibration information indicating a vibration caused by pressing the keyboard may be performed by the feature extractor  of the terminal A and the sampled information may be transmitted to the terminal B.","In step S, the system controller  of the terminal B commands the feature extractor  to generate a feature vector Vch. In step S, the system controller  of the terminal B commands the learning unit  to identify the emotion\/state of Vch and the strength level thereof. In step S, the system controller  of the terminal B commands the learning unit  to output the identification result and perform a recalculation.","Now, referring to , a Vch generation process is described below. This process is performed by the feature extractor  of the terminal B. In step S, the feature extractor  of the terminal B reads sampled vibration information. Thereafter, in step S, as in EXAMPLE 1, the feature extractor  of the terminal B performs the polarity detection process described above with reference to , and, in step S, the Tv generation process described above with reference to , and furthermore in step S, the Vmin\/Vmax detection process described above with reference to .","In step S shown in , the feature extractor  of the terminal B sets both parameters n and P to 1. Then, in step S, the feature extractor  of the terminal B determines whether Tv(P) is greater than a threshold value Th. If it is determined that Tv(P) is not greater than the threshold value Th, then in step S, the feature extractor  of the terminal B sets Tstr(n)+Tv+(P)+Tv\u2212(P) into Tstr(n). Note that Tstr(n) is initially set to 0.","In step S, the feature extractor  of the terminal B determines whether Vmax(P) is greater than Imax(n). Note that the initial value of Imax(n) is 0. If it is determined in step S that Vmax(P) is greater than Imax(n), then, in step S, the feature extractor  of the terminal B sets the value of Vmax(P) into Imax(n). Thereafter, in step S, the feature extractor  of the terminal B increments the value of P by 1 and returns the flow to step S to repeat the above-described process. On the other hand, if it is determined in step S that Vmax(P) is not greater than Imax(n), step S is skipped.","In the case in which it is determined in step S that Tv(P) is greater than the threshold value Th, then in step S, the feature extractor  of the terminal B sets the value of Tv(P) into Tint(n), and increments the value of P by 1. Furthermore, in step S, the feature extractor  of the terminal B generates a feature value vector Vc(Tstr(n), Tint(n), Imax(n)). In step S, the feature extractor  of the terminal B determines whether Tv(P) exists. If it is determined that Tv(P) exists, then, in step S, the feature extractor  of the terminal B increments the value of n by 1. Thereafter, the flow returns to step S, and the above-described process is repeated.","On the other hand, if it is determined in step S that Tv(P) does not exist, then in step S, the feature extractor  of the terminal B generates a mean vector Vch of Vc.","For example, if the user a of the terminal A inputs alphabetic characters \u201cHow about that?\u201d, vibration information is transmitted to the terminal B. In response to receiving the vibration information, the terminal B generates fifteen feature value vectors Vc corresponding to input fifteen characters including spaces, and the mean vector Vch of those fifteen feature vectors Vc is generated.","Referring to , a process of identifying the emotional, the state, and the strength level of Vch is described below. This process is performed by the learning unit  of the terminal B.","In step S, the learning unit  of the terminal B reads, from the storage unit , the feature value vector distribution fvc, the distribution center vector V\u03bc, and the distribution standard deviation vector V\u03c3 associated with vibration information of the user a.","As described above, there are 36 kinds of emotion information, and there are 36 feature value vector distributions fvc, 36 distribution center vectors V\u03bc, and 36 distribution standard deviation vectors V\u03c3 corresponding to the respective 36 kinds of emotion information.","In step S, the learning unit  of the terminal B sets the parameter Q to 1. Then, in step S, the learning unit  of the terminal B calculates the inner product of the vector V\u03bc(Q) and the feature value vector Vch calculated in step S (in ), and stores V\u03bc(Q) in relation to the inner product. Then in step S, the learning unit  of the terminal B increments the value of Q by 1. In step , the learning unit  of the terminal B determines whether the calculation is completed for all kinds of the emotion information, that is, whether the calculation of the inner product is completed for all 36 center vectors V\u03bc. If it is determined that the inner product is not calculated for all kinds of the emotion information, the flow returns to step S, and the above-described process is repeated.","When the calculation of the inner product is completed for all 36 center vectors V\u03bc corresponding to the respective kinds of emotion information, data associated with the emotion information, the inner products, data indicating the kinds of the center vector V\u03bc, and data indicating the order of the inner products are stored in the storage unit (not shown) of the learning unit  of the terminal B, as in EXAMPLE 1. In step S, the learning unit  of the terminal B sorts the data in descending order of inner product.","In step S, the learning unit  of the terminal B sets the parameter R to 1. In step S, the learning unit  of the terminal B determines whether data associated with all kinds of the emotion information are read. If it is determined that all data associated with kinds of the emotion information are not read, then in step S, the learning unit  of the terminal B reads the sorted data. In step S, the learning unit  of the terminal B sets the parameter m to 1. In step S, the learning unit  of the terminal B determines whether the condition V\u03bc(R)m\u2212V\u03c3(R)m\u2266Vchm\u2266V\u03bc(R)m+V\u03c3(R)m is satisfied, wherein V\u03bc(R)m denotes an mth component of the vector V\u03bc(R), and V\u03c3(R)m and Vchm respectively denote mth components of vectors V\u03c3 and Vch. As described above, each feature value vector of vibration information includes three components: Tstr, Tint, and Imax. When m=1, the component Tstr is extracted from each vector of V\u03bc(R), V\u03c3(R), and Vc and subjected to the process.","If it is determined in step S that the condition V\u03bc(R)m\u2212V\u03c3(R)m\u2266Vchm\u2266V\u03bc(R)m+V\u03c3(R)m is not satisfied, then, in step S, the learning unit  of the terminal B increments the value of R by 1. Then the process returns to step S, and the above-described process is repeated. On the other hand, if it is determined in step S that the condition V\u03bc(R)m\u2212V\u03c3(R)m\u2266Vchm\u2266V\u03bc(R)m+V\u03c3(R)m is satisfied, then, in step S, the learning unit  of the terminal B increments the value of m by 1. In step S, the learning unit  of the terminal B determines whether m is equal to 4. If it is determined that m is not equal to 4, then the flow returns to step S, and the above-described process is repeated.","If it is determined in step S that m is equal to 4, that is, if it is determined that the condition V\u03bc(R)m\u2212V\u03c3(R)m\u2266Vchm\u2266V\u03bc(R)m+V\u03c3(R)m is satisfied for all three components of each vector V\u03bc(R), V\u03c3(R), and Vch, then in step S, the learning unit  of the terminal B determines that Vch belongs to emotion information E corresponding to R.","If it is determined in step S that data associated with all kinds of emotion information are read, the learning unit  of the terminal B determines that the emotion\/state and the strength level of Vch cannot be identified, and ends the process.",{"@attributes":{"id":"p-0352","num":"0351"},"figref":["FIG. 60","FIG. 60","FIG. 56"],"b":["555","436","556","15","557","555"]},"In this way, the emotion\/state and the strength level thereof are identified from the feature value vector Vch.","Now, referring to , the process of outputting the identification result and performing a recalculation is described below. This process is performed by the learning unit  of the terminal B. In step S, the learning unit  of the terminal B outputs the identification result to the output controller . In step S, the learning unit  of the terminal B recalculates the distribution fvc of the feature value vector of vibration information corresponding to emotion information E of the user a and the center vector V\u03bc and the standard deviation vector V\u03c3 of the distribution fvc, and the learning unit  of the terminal B stores the recalculated result. That is, the feature value vector Vch generated in step S () is added to the feature value vector of vibration information indicating the \u201cweak dislike state\u201d of the user a stored in the storage unit, and fvc, V\u03bc, and V\u03c3 are recalculated and stored in the storage unit .","In this way, the emotion\/state and the strength level thereof are identified from vibration information, and information associated with the feature vectors of the identified emotion\/state and the strength level thereof is stored.","As described above, in accordance with keyboard vibration information received from the terminal A of the user a, the terminal B generates the feature value vector Vch and identifies the emotion\/state and the strength level of Vch. Similarly, in accordance with vibration information associated with the keyboard of the user b, the terminal B generates the feature value vector Vch and identifies the emotion\/state and the strength level of Vch.","Similarly, the terminal A identifies the emotion\/state and the strength level of the user a and the user b.","Now, referring to , an information outputting process is described below. First, in step S, the system controller  determines whether information to be output is selected. If information to be output is not selected, the process waits until information to be output is selected. The selection of information to be output may be performed, for example, as in EXAMPLE 1, by clicking a menu item displayed on the display of the terminal. The selection menu used by a user to select information to be output includes the following four items: \u201cidentification result\u201d, \u201cinformation based on feature values\u201d, \u201cwaveform information\u201d, and \u201cstored information\u201d.","If it is determined in step S that information to be output is selected, then in step S the system controller  determines whether the information selected to be output is the identification result. If it is determined that the information selected to be output is the identification result, then in step S, the system controller  transmits, to the output controller , 2-bit selection information 00 indicating that the identification result should be output.","In step S, the output controller  outputs an identification result associated with the emotion\/state and the strength level thereof. For example, a text \u201cUser A is in a weak dislike state\u201d is displayed on the display of the terminal.","If it is determined in step S that the information selected to be output is not the identification result, then, in step S, the system controller  determines whether the information selected to be output is the information based on feature values. If it is determined that the information selected to be output is the information based on feature values, then, in step S, the system controller  transmits, to the output controller , 2-bit selection information 01 indicating that the information based on feature values should be output.","In step S, the output controller  acquires a feature value (Tstr, Tint, Imax) from the feature extractor . Then, in step S, the output controller  generates a vibration presentation unit driving signal. More specifically, as shown in , a rectangular wave signal with an output level Imax, an on-period Tstr, and an off-period Tint is generated. Then in step S, the output controller  drives the vibration presentation unit .","If it is determined in step S that the information selected to be output is not the information based on feature values, then, in step S, the system controller  determines whether the information selected to be output is the waveform information. If it is determined that the information selected to be output is the waveform information, the system controller  transmits, to the output controller , 2-bit selection information 10 indicating that the waveform information should be output. In step S, the output controller  acquires sampled vibration information from the feature extractor  and generates a waveform signal. In step S, the output controller  drives the vibration presentation unit .","If it is determined in step S that the information selected to be output is not the waveform information, then, in step S, the system controller  determines whether the information selected to be output is the stored information. If it is determined that the information selected to be output is the stored information, then, in step S, the system controller  transmits, to the output controller , 2-bit selection information 01 indicating that the stored information should be output.","In the case in which the information selected to be output is the stored information, a sub-menu is further displayed on the display of the terminal so that a user can specify an item of stored information to be output. More specifically, the user can make selections in terms of items of information inf such as keyboard vibration information, information PID identifying a user, and emotion information E. For example, the user can select keyboard vibration information as inf, information indicating the user a as PID, and \u201cstrong vital state\u201d as emotion information E.","In step S, the system controller  determines whether inf indicating a particular kind of information, PID identifying a user, and emotion information E are selected. If selection is not performed, the process waits until selection is made.","In step S, the system controller  generates information (inf, PID, E) in accordance with the selection made by the user and transmits the generated information to the output controller .","In step S, in accordance with (inf, PID, E), the output controller  acquires the center vector of vibration information associated with the keyboard from the storage unit . In this specific case, the center vector V\u03bc(Tstr, Tint, Imax) of feature value vectors of pulse information corresponding to the \u201chigh-level vitality\u201d of the user a is acquired from the storage unit.","In step S, the output controller  generates a vibration presentation unit driving signal in accordance with the center vector V\u03bc. More specifically, as shown in , a rectangular wave signal  with an output level Imax, an on-period Tstr, and an off-period Tint is generated. Then in step S, the output controller  drives the vibration presentation unit  three times in accordance with the generated driving signal.","If it is determined in step S that the information selected to be output is not the stored information, then, in step S, the system controller  performs error handling. After completion of the error handling, the process is ended.","Thus, the emotion\/state and the strength level thereof identified from the vibration information are displayed so that the user can recognize them. Furthermore, the vibration presentation unit is driven so that the user can recognize vibration information via bodily sensations.",{"@attributes":{"id":"p-0372","num":"0371"},"figref":"FIG. 66","b":["611","613","611","614","613","615","614","612","613","616","617"]},"The processing unit  includes an information discriminator  for classifying information input via the input unit  into emotion\/state declaration information, ambient temperature information, or mouse click information\/grasp pressure information, a feature extractor  for extracting property information from mouse click information and mouse grasp pressure information, and an output controller  for outputting information to the output unit . The communication unit  includes an information transmitting unit  for transmitting information and an information receiving unit  for receiving information.",{"@attributes":{"id":"p-0374","num":"0373"},"figref":["FIG. 67","FIG. 67"],"b":["611","611","671","652","671","653","654","653"]},{"@attributes":{"id":"p-0375","num":"0374"},"figref":"FIG. 68","b":["612","612","692","691","692","692","693","693"]},"Herein, an information processing apparatus of a user a is denoted by a terminal A, and an information processing apparatus of a user b is denoted by a terminal B. The user a and the user b can communicate with each other using their terminals A and B in accordance with a procedure shown in . First, the terminal A transmits a connection request to the terminal B. In response, the terminal B returns a connection acknowledgment signal to the terminal A. Thereafter, textual information such as a mail or a chat is transmitted between the terminal A and the terminal B.","The terminal A performs initial setting associated with transmission\/reception of click information and grasp pressure information (grasp pressure with which a user grasps a mouse, in this specific example), and transmits a transmission\/reception request to the terminal B together with transmission setting information and reception setting information such as those shown in , as in the EXAMPLE 1.","If the terminal B receives the vibration information transmission request from the terminal A, the terminal B changes the setting associated with transmission\/reception of click information and grasp pressure information and returns an acknowledgment signal to the terminal A.","The terminal A then transmits click information and grasp pressure information of the user a to the terminal B, and the terminal B transmits vibration information of the user b to the terminal A. The terminal A and the terminal B store the received <click information and grasp pressure information.","If the terminals A transmits a connection end request to the terminal B, the terminal B transmits a connection end acknowledgment signal, and the communication is ended.","Communication is performed between the terminal A and the terminal B in the above-described manner, and click information and grasp pressure information are stored in the terminal A and the terminal B. After sufficient amounts of click information and grasp pressure information are stored, transmission\/reception of information is performed as shown in . If click information and grasp pressure information are transmitted between the terminal A and the terminal B, the terminal A and the terminal B detect the emotion or the state of users.","Now, referring to , a process performed by the terminal B to store click information and grasp pressure information is described below. First, in step S, at the start of communication with the terminal A, the system controller  of the terminal B acquires information (for example, a name or a nickname) PID identifying the user a of the terminal A with which to communicate. In step S, the system controller  of the terminal B acquires ambient temperature information K transmitted from the terminal A. In step S, the system controller  of the terminal B commands the information discriminator  to determine whether emotion\/state declaration information is input from the terminal A. If emotion\/state declaration information is not yet input, the process waits until emotion\/state declaration information is input.","An emotion\/state chart such as that shown in  is displayed on the display of the terminal A. The user a declares his\/her emotion\/state by clicking a corresponding area of the emotion\/state chart a particular number of times using a mouse. If the emotion\/state chart is clicked with the mouse, emotion\/state information such as that shown in  is transmitted from the terminal A to the terminal B. In the declaration, the strength level of the emotion\/state is indicated by the number of times the emotion\/state chart is clicked. More specifically, a low level is indicated by clicking twice, a middle level by clicking three times, and a high level by clicking four times.","The emotion\/state declaration information associated with a user may be acquired by using an agent (such as an avatar) assigned to the user in a virtual space.","In the case in which it is determined in step S that emotion\/state declaration information is input, then, in step S, the system controller  of the terminal B commands the feature extractor  to start sampling of click information and grasp pressure information associated with the user a transmitted from the terminal A. Then in step S, the system controller  of the terminal B extracts emotion information E from the emotion\/state declaration information. In step S, the system controller  of the terminal B commands the feature extractor  to end the sampling of the click information and grasp pressure information.","The sampling of mouse click information and grasp pressure information may be performed by the feature extractor  of the terminal A and the resultant sampled information may be transmitted to the terminal B.","For example, when an area assigned to surprise in the emotion\/state declaration chart is clicked four times with the mouse by the user a, a \u201cstrong surprise\u201d is extracted as the emotion information E.","In step S, the system controller  of the terminal B commands the feature extractor  to generate a feature vector Vc on the basis of click information and grasp pressure information of the user a. In step S, it is determined whether the ambient temperature K is equal to or higher than a reference value K. If it is determined that the ambient temperature K is equal to or higher than a reference value K, then in step S, click information and grasp pressure information corresponding to information PID and the emotion information E with K\u2267K is read from the storage unit . In this specific case, feature value vectors of click information and grasp pressure information corresponding to the strong surprise of the user a at an ambient temperature equal to or higher than K are read.","In the case in which it is determined in step S that the ambient temperature K is lower than the reference value K, then, in step S, the system controller  of the terminal B reads, from the storage unit , click information and grasp pressure information corresponding to information PID and the emotion information E with K\u2267K. In this specific case, feature value vectors of click information and grasp pressure information corresponding to the strong surprise of the user a at an ambient temperature lower than K are read.","In step S, using the learning unit , the system controller  of the terminal B adds the newly generated feature vector Vc to the read feature vectors and recalculates the vector distribution fvc, the distribution center vector V\u03bc, and the standard deviation vector V\u03c3. In step S, the recalculated vectors are stored in the storage unit .","Now, referring to , the process of generating the feature vectors Vc of click information and grasp pressure information is described below. This process is performed by the feature extractor  of the terminal B. In step S, the feature extractor  of the terminal B reads sampled click information and grasp pressure information. Thereafter, in step S, as in EXAMPLE 1, the feature extractor  of the terminal B performs the polarity detection process described above with reference to , and, in step S, the Tv generation process described above with reference to , and furthermore in step S, the Vmin\/Vmax detection process described above with reference to . In step S, the feature extractor  of the terminal B performs a click feature value calculation to obtain Tclm and Tcim. The details of the calculation process will be described later.","Then in step S, the feature extractor  of the terminal B reads sampled grasp pressure information. In step S, the feature extractor  of the terminal B performs the polarity detection process, and, in step S, the Tv generation process, and furthermore in step S the Vmin\/Vmax detection process. In step S, the feature extractor  of the terminal B performs a grasp pressure feature value calculation to obtain Tgrpm and Igrpm. The details of the calculation process will be described later.","In step S, the feature extractor  of the terminal B generates a feature value vector Vc(Tclm, Tcim, Tgrpm, Igrpm).","Now, referring to , the process of calculating the click feature value is described below.  shows an example of a waveform of click information generated when a user clicks a mouse. That is, when a click button is clicked down by the user, a pulse , a pulse , a pulse , and a pulse  are output when a click button of the mouse is pressed down by the user. When the click button is not pressed, the output becomes 0 as in periods denoted by , , and . If this waveform is sampled, and the sampled data is subjected to the polarity detection process and the Tv generation process, Tv+, Tv, Tv+, Tv, Tv+ . . . are obtained as the result.","In step S in , the feature extractor  of the terminal B sets both parameters n and P to 1. Then, in step S, the feature extractor  of the terminal B sets the value of Tv+(P) into Tc(n) and the value of Tv(P) into Tci(n). Then in step S, the feature extractor  of the terminal B increments the values of P and n by 1. In step S, the feature extractor  of the terminal B determines whether Tv+(P) or Tv(P) exists. If it is determined that Tv+(P) or Tv(P) exists, then the flow returns to step S, and the above-described process is repeated.","If it is determined in step S that Tv+(P) or Tv\u2212(P) does not exist, then in step S, the feature extractor  of the terminal B calculates the mean value Tclm of Tcl and the mean value Tcim of Tci. In the specific example shown in , Tclm and Tcim are respectively calculated in accordance with the following equations.\n\n=((1)+(2)+(3)+())\/4\n\n=((1)+(2)+(3))\/3\n","Now, referring to , the process of calculating the grasp pressure feature value is described below.  shows an example of a waveform  of a grasp pressure at which a mouse is grasped by a user. If this waveform is sampled, and the sampled data is subjected to the polarity detection process and the Tv generation process, Tv+, Tv, Tv+ . . . are obtained as the result. That is, Tv+ is obtained when the mouse is grasped by the user at a grasp pressure higher than a threshold value Vth, while Tv is obtained when the grasp pressure is lower than the threshold value Vth.","By performing the Vmax\/Vmin detection process, Vmax indicating a maximum grasp pressure in a period Tv+ during which the mouse is grasped by the user at grasp pressures higher than the threshold value Vth is obtained.","In step S, the feature extractor  of the terminal B sets both parameters n and P to 1. Then, in step S, the value of Tv+(P) is set into Tgrp(n) and the value of Vmax(P) is set into Igrp(n).","In step S, the feature extractor  of the terminal B increments the value of P by 1. In step S, the feature extractor  of the terminal B determines whether Tv+(P) or Tv\u2212(P) exists. If it is determined that Tv+(P) or Tv\u2212(P) exists, then, in step S, the feature extractor  of the terminal B calculates the mean value Tgrpm of Tgrp and the mean value Igrpm of Igrp. In the specific example shown in , Tgrpm and Igrpm are respectively calculated in accordance with the following equations.\n\n=((1)+(2))\/2\n\n=((1)+(2))\/2\n","In the above-described manner, a feature vector Vc of click information and grasp pressure information corresponding to the strong surprise of the user a is generated.","Thus, the terminal B generates the feature value vector Vc from the click information and grasp pressure information associated with the mouse of the user a received from the terminal A, and the terminal B stores the generated feature value vector Vc. Similarly, the terminal B generates a feature value vector from click information and grasp pressure information associated with the mouse of the user b and stores the generated feature value vector. As a result, as shown in , a learning result  associated with the mouse click information and grasp pressure information are stored in the storage unit  of the terminal B, separately for the case  in which the ambient temperature K\u2267K and the case  in which the ambient temperature K<K. In each of cases  and  of the ambient temperature K\u2267K and K<K, the mouse click\/grasp pressure information is stored separately for that associated with the user a and that associated with the user b.","Similarly, the terminal A stores a learning result associated with mouse click\/grasp pressure information, for the user a and the user b, separately.","Once a sufficient amount of mouse click\/grasp pressure information has been stored as a result of a repetition of the above-described click\/grasp pressure information storage process, it becomes possible to identify the emotion\/state and the strength level of each user from the click\/grasp pressure information.","Referring to , a process performed by the terminal B to identify an emotional state is described below.","First, in step S, at the start of communication with the terminal A, the system controller  of the terminal B acquires information PID identifying the user a of the terminal A with which to communicate. In step S, the system controller  of the terminal B commands the information discriminator  to determine whether a mouse is clicked by the user a to input information. If the mouse is not clicked, the process waits until the mouse is clicked to input information.","If it is determined in step S that information is input by clicking the mouse, then in step S, the system controller  of the terminal B commands the feature extractor  to start sampling of click information and grasp pressure information associated with the mouse of the user a transmitted from the terminal A. Then in step S, the system controller  of the terminal B commands the information discriminator  to determine whether inputting of information using the mouse is completed. If inputting using the mouse is not completed, the process waits until inputting is completed.","If it is determined in step S that inputting using the mouse is completed, the system controller  of the terminal B commands the feature extractor  to end the sampling of click information and grasp pressure information associated with the mouse of the user a transmitted from the terminal A.","The sampling of the click information and the grasp pressure information associated with the mouse may be performed by the feature extractor  of the terminal A and the resultant sampled information may be transmitted to the terminal B.","In step S, the system controller  of the terminal B commands the feature extractor  to generate a feature vector Vc. In step S, the system controller  of the terminal B commands the learning unit  to identify the emotion\/state of Vc and the strength level thereof. Furthermore, in step S, the system controller  of the terminal B commands the learning unit  to output the identification result and perform a recalculation.","Referring to , a Vc generation process is described below. This process is performed by the feature extractor  of the terminal B. In step S, the feature extractor  of the terminal B reads sampled click information and grasp pressure information. In step S, the feature extractor  of the terminal B performs the polarity detection process, and in step S the Tv generation process, and furthermore, in step S, the Vmin\/Vmax detection process. In step S, the feature extractor  of the terminal B performs a click feature value calculation to obtain Tclm and Tcim. The details of the calculation process will be described later.","In step S, the feature extractor  of the terminal B reads sampled grasp pressure information. In step S, the feature extractor  of the terminal B performs the polarity detection process, and in step S the Tv generation process, and furthermore, in step S, the Vmin\/Vmax detection process. In step S, the feature extractor  of the terminal B performs a grasp pressure feature value calculation to obtain Tgrpm and Igrpm. The details of the calculation process will be described later.","In step S, the feature extractor  of the terminal B generates a feature value vector Vc(Tclm, Tcim, Tgrpm, Igrpm).","Referring to , the process of calculating the click feature value is described below.","In step S, the feature extractor  of the terminal B sets both parameters n and P to 1. In step S, the feature extractor  of the terminal B sets the value of Tv+(P) into Tcl(n) and the value of Tv(P) into Tci(n). Then in step S, the feature extractor  of the terminal B increments the values of P and n by 1. In step S, the feature extractor  of the terminal B determines whether Tv+(P) or Tv\u2212(P) exists. If it is determined that Tv+(P) or Tv\u2212(P) exists, the flow returns to step S to repeat the above-described process.","If it is determined in step S that Tv+(P) or Tv\u2212(P) does not exist, then in step S, the feature extractor  of the terminal B calculates the mean value Tclm of Tcl and the mean value Tcim of Tci.","Referring to , the process of calculating the grasp pressure feature value is described below. In step S, the feature extractor  of the terminal B sets both parameters n and P to 1. Then, in step S, the feature extractor  of the terminal B sets the value of Tv+(P) into Tgrp(n) and the value of Vmax(P) into Igrp(n).","In step S, the feature extractor  of the terminal B increments the value of P by 1. Then, in step S, the feature extractor  of the terminal B determines whether Tv+(P) or Tv\u2212(P) exists. If it is determined that Tv+(P) or Tv\u2212(P) exists, then, in step S, the feature extractor  of the terminal B calculates the mean value Tgrpm of Tgrp and the mean value Igrpm of Igrp.","Thus, the feature value vector Vc is generated from the mouse click\/grasp pressure information associated with the user a.","Referring to , a process of identifying the emotion\/state and the strength level of Vc is described below. This process is performed by the learning unit  of the terminal B.","In step S, the learning unit  of the terminal B determines whether the ambient temperature K received from the terminal A is equal to or higher than the reference value K. If it is determined that the ambient temperature K is equal to or higher than the reference value K, the learning unit  of the terminal B reads, from the storage unit , the distribution vector fvc, the distribution center vector V\u03bc and the distribution standard deviation vector V\u03c3 of feature value vectors of the click information and the grasp pressure information associated with the mouse of the user a with K\u2267K. In the case in which it is determined in step S that K is lower than the reference value K, the learning unit  of the terminal B reads, from the storage unit , the distribution vector fvc, the distribution center vector V\u03bc and the distribution standard deviation vector V\u03c3 of feature value vectors of the click information and the grasp pressure information associated with the mouse of the user a with K<K.","As described above, there are 36 kinds of emotion information, and there are 36 feature value vector distributions fvc, 36 distribution center vectors V\u03bc, and 36 distribution standard deviation vectors V\u03c3 corresponding to the respective 36 kinds of emotion information.","In step S, the learning unit  of the terminal B sets the parameter Q to 1. In step S, the learning unit  of the terminal B calculates the inner product of the vector V\u03bc(Q) and the feature value vector Vc calculated in step S (), and stores V\u03bc(Q) in relation to the inner product. Then in step S, the learning unit  of the terminal B increments the value of Q by 1. In step , the learning unit  of the terminal B determines whether the calculation is completed for all kinds of the emotion information, that is, whether the calculation of the inner product is completed for all 36 center vectors V\u03c3. If it is determined that the inner product is not calculated for all kinds of the emotion information, the flow returns to step S, and the above-described process is repeated.","When the calculation of the inner product is completed for all 36 center vectors V\u03bc corresponding to the respective kinds of emotion information, data associated with the emotion information, the inner products, data indicating the kinds of the center vector V\u03bc, and data indicating the order of the inner products are stored in the storage unit (not shown) of the learning unit  of the terminal B, as in EXAMPLE 1. In step S, the learning unit  of the terminal B sorts the data in descending order of inner product.","In step S, the learning unit  of the terminal B sets the parameter R to 1. In step S, the learning unit  of the terminal B determines whether data associated with all kinds of the emotion information are read. If it is determined that all data associated with kinds of the emotion information are not read, the learning unit  of the terminal B reads the sorted data. In step S, the learning unit  of the terminal B sets the parameter m to 1. Then in step S, the learning unit  of the terminal B determines whether the condition V\u03bc(R)m\u2212V\u03c3(R)m\u2266Vcm\u2266V\u03bc(R)m+V\u03c3(R)m is satisfied, wherein V\u03bc(R)m denotes an mth component of the vector V\u03bc(R), and V\u03c3(R)m and Vcm respectively denote mth components of vectors V\u03c3 and Vch. As described above, each feature value vector of mouse click\/grasp pressure information includes four components: Tclm, Tcim, Tgrpm, and Igrpm. When m=1, the Tclm component is extracted from each vector of V\u03bc(R), V\u03c3(R), and Vc and subjected to the process.","If it is determined in step S that the condition V\u03bc(R)m\u2212V\u03c3(R)m\u2266Vchm\u2266V\u03bc(R)m+V\u03c3(R)m is not satisfied, then in step S, the learning unit  of the terminal B increments the value of R by 1. Thereafter, the flow returns to step S, and the above-described process is repeated. On the other hand, if it is determined in step S that the condition V\u03bc(R)m\u2212V\u03c3(R)m\u2266Vcm\u2266V\u03bc(R)m+V\u03c3(R)m is satisfied, then in step S, the learning unit  of the terminal B increments the value of m by 1. Then, in step S, the learning unit  of the terminal B determines whether m is equal to 5. If it is determined that m is not equal to 5, the flow returns to step S, and the above-described process is repeated.","If it is determined in step S that m is equal to 5, that is, if it is determined that the condition V\u03bc(R)m\u2212V\u03c3(R)m\u2266Vcm\u2266V\u03bc(R)m+V\u03c3(R)m is satisfied for all four components of each vector V\u03bc(R), V\u03c3(R), and Vch, the process proceeds to step S. In step S, the learning unit  of the terminal B determines that Vc belongs to emotion information E corresponding to R.","If it is determined in step S that data associated with all kinds of the emotion information are read, the learning unit  of the terminal B determines that the emotion\/state and the strength level of Vc cannot be identified, and the learning unit  of the terminal B ends the process.",{"@attributes":{"id":"p-0429","num":"0428"},"figref":["FIG. 86","FIG. 86","FIG. 81"],"b":["811","666","812","1","813","811"]},"In this way, the emotion\/state and the strength level of the user a are identified from the feature value vector Vc.","Now, referring to , the process of outputting the identification result and performing a recalculation is described below. This process is performed by the learning unit  of the terminal B. In step S, the learning unit  of the terminal B outputs the identification result to the output controller . In step S, the learning unit  of the terminal B recalculates the distribution fvc of the feature value vector of mouse click information and grasp pressure information corresponding to emotion information E of the user a and the center vector V\u03bc and the standard deviation vector V\u03c3 of the distribution fvc, and the learning unit  of the terminal B stores the recalculated result. That is, the feature value vector Vc generated in step S () is added to the feature value vector of mouse click\/grasp pressure information indicating the \u201cstrong joy\u201d of the user a stored in the storage unit, and fvc, V\u03bc, and V\u03c3 are recalculated and stored in the storage unit .","In this way, the emotion\/state and the level are identified from the mouse click\/grasp pressure information, and information associated with the feature vectors of the identified emotion\/state and the strength level thereof is stored.","Thus, the terminal B generates the feature value vector Vc from the click\/grasp pressure information associated with the mouse of the user a received from the terminal A, and the terminal B identifies the emotion\/state and the strength level thereof. Similarly, the terminal B generates the feature value vector Vc from the click\/grasp pressure information associated with the mouse of the user b and identifies the emotion\/state and the strength level thereof.","Similarly, the terminal A identifies the emotion\/state and the strength level of the user a and the user b.","Now, referring to , an information outputting process is described below. In step S, the system controller  determines whether information to be output is selected. If information to be output is not selected, the process waits until information to be output is selected. The selection of information to be output may be performed, for example, as in EXAMPLE 1, by clicking a menu item displayed on the display of the terminal. The selection menu used by a user to select information to be output includes the following four items: \u201cidentification result\u201d, \u201cinformation based on feature values\u201d, \u201cwaveform information\u201d, and \u201cstored information\u201d.","If it is determined in step S that information to be output is selected, then in step S the system controller  determines whether the information selected to be output is the identification result. If it is determined that the information selected to be output is the identification result, then in step S, the system controller  transmits, to the output controller , 2-bit selection information 00 indicating that the identification result should be output.","In step S, the output controller  outputs an identification result associated with the emotion\/state and the strength level thereof. For example, a text \u201cUser A is in a strong joyful state\u201d is displayed on the display of the terminal.","If it is determined in step S that the information selected to be output is not the identification result, then in step S, the system controller  determines whether the information selected to be output is the information based on feature values. If it is determined that the information selected to be output is the information based on feature values, then, in step S, the system controller  transmits, to the output controller , 2-bit selection information 01 indicating that the information based on feature values should be output.","In step S, the output controller  acquires a feature value (Tgrp, Igrp) from the feature extractor . Then in step S, the output controller  generates a temperature presentation unit driving signal. More specifically, as shown in , a rectangular wave signal with an output level Igrp and an on-period Tgrp is generated. Then in step S, the output controller  drives the temperature presentation unit .","If it is determined in step S that the information selected to be output is not the information based on feature values, then, in step S, the system controller  determines whether the information selected to be output is the waveform information. If it is determined that the information selected to be output is the waveform information, then in step S, the system controller  transmits, to the output controller , 2-bit selection information 10 indicating that the waveform information should be output. In step S, the output controller  acquires sampled mouse click\/grasp pressure information from the feature extractor , and generates a waveform signal from the acquired information. In step S, the output controller  drives the temperature presentation unit .","If it is determined in step S that the information selected to be output is not the waveform information, then in step S the system controller  determines whether the information selected to be output is the stored information. If it is determined that the information selected to be output is the stored information, then, in step S, the system controller  transmits, to the output controller , 2-bit selection information 01 indicating that the stored information should be output.","In the case in which the information selected to be output is the stored information, a sub-menu is further displayed on the display of the terminal so that a user can specify an item of stored information to be output. More specifically, the user can make selections in terms of items of information inf such as mouse click information or grasp pressure information, information PID identifying a user, and emotion information E. For example, the user can select mouse click information and grasp pressure information as inf, information indicating the user a as PID, and \u201cslight surprise\u201d as emotion information E.","In step S, the system controller  determines whether inf indicating a particular kind of information, PID identifying a user, and emotion information E are selected. If information to be output is not selected, the process waits until information to be output is selected.","In step S, the system controller  generates information (inf, PID, E) in accordance with the selection made by the user and transmits the generated information to the output controller .","In step S, in accordance with (inf, PID, E), the output controller  acquires the center vector of vibration information associated with the keyboard from the storage unit . In this specific case, the center vector V\u03bc(Tcl, Tci, Tgrp, Igrp) of feature value vectors of pulse information corresponding to the \u201cweak surprise\u201d of the user a is acquired from the storage unit.","In step S, the output controller  generates a temperature presentation unit driving signal in accordance with the center vector V\u03bc. More specifically, as shown in , a rectangular wave signal  with an output level Igrp, an on-period Tgrp, and an off-period Tgrp is generated. In step S, the output controller  drives the temperature presentation unit  three times in accordance with the generated driving signal.","If it is determined in step S that the information selected to be output is not the stored information, then in step S the system controller  performs error handling. After completion of the error handling, the process is ended.","Thus, the emotion\/state and the strength level thereof identified from the mouse click\/grasp pressure information are displayed so that the user can recognize them. Furthermore, the temperature presentation unit is driven so that the user can recognize grasp information via a bodily sensation.",{"@attributes":{"id":"p-0449","num":"0448"},"figref":["FIG. 92","FIG. 1"],"b":["1","2","3","871","872","873","871","873","881","880"]},"Each of the game machine  (game machine A), the game machine  (game machine B), and the game machine  (game machine C) includes, as in the communication system shown in , an input unit A, B, or C used to input information, a processing unit A, B, or C for processing the information input via the input unit, a learning unit A, B, or C for learning\/identifying the emotion\/state and the strength level of a user from the input information, a storage unit A, B, or C for storing information learned\/identified by the learning unit, an output unit A, B, or C for outputting information identified by the learning unit or outputting information stored in the storage unit, a communication unit A, B, or C for transmitting\/receiving information, and a system controller A, B, or C for controlling various parts.","The game machine  (the game machine A), the game machine  (the game machine B), and the game machine  (the game machine C) are similar in structure to each other, and parts thereof, that is, the parts such as the input unit A, . . . , the system controller A of the game machine  the parts such as the input unit B, . . . , the system controller B of the game machine  and the parts such as the input unit C, . . . , the system controller C of the game machine  are similar in functions. In the following description, when it is not needed to discriminate those parts among game machines, the parts are represented in a generic manner, such as the input unit , . . . , the system controller .",{"@attributes":{"id":"p-0452","num":"0451"},"figref":["FIG. 93","FIG. 93","FIG. 94"],"b":["871","873","891","941","893","891","894","893","895","894","892","893","896","897"]},"The processing unit  includes an information discriminator  for classifying information input via the input unit  into acceleration information, grasp pressure information, or voice information, a feature extractor  for extracting property information from the acceleration information and the grasp pressure information associated with the controller , a meaning extractor  for extracting emotion information from voice information, and an output controller  for outputting information to the output unit . The communication unit  includes an information transmitting unit  for transmitting information and an information receiving unit  for receiving information.","The input unit  includes a microphone, an acceleration sensor for detecting an acceleration in directions along three axes (X, Y, and Z axes normal to each other), and grasp pressure sensors for detecting a pressure at which the controller  is grasped by a user. The input unit  is disposed on the controller  of the game machine. The output unit  includes a light emission unit for emitting light in accordance with a driving signal, and as vibration presentation unit including a vibration motor that vibrates in accordance with a driving signal. The output unit  is also disposed on the controller .",{"@attributes":{"id":"p-0455","num":"0454"},"figref":["FIG. 94","FIG. 94","FIG. 94","FIG. 94"],"b":["941","941","961","941","941","961","962","941","962","963","963","941","941","963","963"]},"Light emission parts L and R are respectively disposed at locations close to the left and right of the 3-axis acceleration sensor . Vibration presentation units L and R are disposed at locations substantially the same, but below, as the grasp pressure sensors L and R. As described above, the vibration presentation unit L and R are formed of vibration motors disposed in the controller .","Let us assume that the game machine A is used by a user a, and the game machine B is used as a user b. Using the game machines A and B, the user a and the user b can participate in a multi-player on-line game. In the on-line game, each player can make voice communicate with other players. For example, a voice of the user a is input to the microphone  of the controller of the game machine A and transmitted to the game machine B. On receiving the voice signal, the game machine B outputs the voice of the user a from a speaker of the television set connected to the game machine B or a speaker disposed on the game machine B so that the user b can listen to the voice of the user a. Similarly, a voice generated by the user b is transmitted to the game machine A so that the user a can listen to the voice of the user b. Thus, the users a and the user b can make a voice communication with each other.","Transmission\/reception of information is performed in accordance with a procedure shown . First, in step S, the game machine A transmits a connection request to a server . In step S, the server  receives the connection request. In step S, the server  performs authentication associated with the connection request. The connection request transmitted from the game machine A includes information identifying the user a such as a name or a nickname of the user a and a password needed to participate in the game. The server  determines (authenticates), on the basis of such information, whether the user a is authorized to participate in the game. If it is determined that the user a is authorized to participate in the game, then in step S the server  transmits a connection request acceptance message to the game machine A. In step S, the game machine A receives the connection request acceptance message. In step S, the game machine A starts the game.","After information is transmitted\/received between the game machine B and the server  in a similar manner (step S (corresponding to step S), (step S (corresponding to step S), (step S (corresponding to step S), (step S (corresponding to step S), and (step S (corresponding to step S)), the game machine B starts the game in step S.","In step S, the game machine A transmits, to the server , a request for connection to the game machine B. In step S, the server  receives the request for connection to the game machine B. In step S, the server  transmits a connection request acceptance message to the game machine A. In step S, the game machine A receives the connection request acceptance message. In step S, the server  calls the game machine B. The call is received by the game machine B in step S. In step S, the game machine B returns an acknowledgment message to the server . In step S, the server  receives the acknowledgment message. In step S, a process of connecting the game machine A and the game machine B is started. In step S, the server  transmits, to the game machine A, a notification of connection link establishment. In step S, the game machine A receives the notification.","Thereafter, the game machine A transmits voice information to the game machine B via the server , and the game machine B also transmits voice information to the game machine A via the server  (step S, S, S, S, S, S, S, and S).","In step S, the game machine A performs initial setting associated with transmission\/reception of acceleration information and grasp pressure information. In step S, the game machine A transmits an acceleration\/grasp pressure information transmission\/reception request to the server . In step S, the server  receives the request. In step S, the server  transmits the acceleration\/grasp pressure information transmission\/reception request to the game machine B together with transmission setting information and reception setting information such as that shown in  transmitted from the game machine A, as in EXAMPLE 1.","In step S, the game machine B receives the acceleration\/grasp pressure information transmission\/reception request transferred from the server . In response, in step S, the game machine B changes the setting associated with transmission\/reception of acceleration information and grasp pressure information. In step S, the game machine B transmits an acknowledgment message to the server . In step S, the server  receives the acknowledgment message transmitted from the game machine B. In response, in step S, the server  starts transmission\/reception of acceleration\/grasp pressure information between the game machines A and B. In step S, the server  transmits a communication start notification message to the game machine A. In step S, the game machine A receives the notification message. Thus, the game machine A recognizes that acceleration information and grasp pressure information associated with the user b are going to be transmitted from the game machine B, and the game machine B recognizes that acceleration information and grasp pressure information associated with the user a are going to be transmitted from the game machine A.","From the game machine A to the game machine B via the server , acceleration\/grasp pressure information is transmitted (step S, S, S, and S). In step S, the game machine B stores feature value information. Similarly, from the game machine B to the game machine A via the server , acceleration\/grasp pressure information is transmitted (step S, S, S, and S). In step S, the game machine A stores feature value information. The details of the feature value information storage process will be described later with reference to .","If the game machine A transmits, in step S, a connection end request to the game machine B via the server , the connection end request is received by the server  in step S. In step S, the server  transfers the connection end request to the game machine B. In step S, the game machine B receives the connection end request. In response, in step S, the game machine transmits an acknowledgment message. In step S, the server  receives the acknowledgment message. As a result, the communication is ended. In step S, the server  transmits a connection end request acceptance message to the game machine A. In step S, the game machine A receives the acceptance message.","In this way, communication between the game machine A and the game machine B is performed via the server , and acceleration information and grasp pressure information are stored. After sufficient amounts of acceleration information and grasp pressure information have been stored, an emotion information identification process is performed in the game machine A and the game machine B. The details of the emotion information identification process will be described later with reference to . In this process, information transmitted\/received as shown in .","In the process shown in , steps S to S performed by the game machine A are similar to step S to S in the process shown in  except that in step S the game machine A performs emotion information identification on the basis of acceleration information and grasp pressure information received from the game machine B. In the process shown in , steps S to S performed by the game machine B are similar to step S to S in the process shown in  except that in step S the game machine B performs emotion information identification on the basis of acceleration information and grasp pressure information received from the game machine A.","In the process shown in , steps  to  performed by the server  are similar to step S to  in the process shown in , and thus a duplicated description of those steps is not given herein.","Referring to , the feature information storage process (steps S and S in ) performed by the game machine A and the game machine B is described below. The mage machine A can store feature information on the basis of acceleration information and grasp pressure information associated with the user a output from the controller  coupled with the game machine A and also can store feature information on the basis of acceleration information and grasp pressure information associated with the user b transmitted from the game machine B.","First, the process is described below for a case in which the game machine A stores feature information on the basis of acceleration information and grasp pressure information associated with the user a. In step S, the system controller  of the game machine A acquires information (for example, a name or a nickname) PID identifying the user a of the game machine A In step S, the system controller  commands the information discriminator  to determine whether voice information of the user a is input. If voice information is not yet input, the process waits until voice information is input.","If it is determined in step S that voice information is input, the process proceeds to step S. In step S, the system controller  commands the feature extractor  to start sampling of acceleration information and grasp pressure information associated with the user a. In step S, using the meaning extractor , the system controller  recognizes the input voice information, using the meaning extractor .","In step S, the system controller  determines whether the recognized voice includes a word indicating an emotion, a state, or a strength level of an emotion\/state of the user a. If it is determined that the recognized voice does not include a word indicating emotion, a state, or the strength or level of the emotion, then the process returns to step S, and the above-described process is repeated. Words indicating emotions or states may be such as those shown in , and words indicating strength levels may be such as those shown in , as in EXAMPLE 1.","In the case in which it is determined in step S that the recognized voice includes a word indicating an emotion, a state, or a strength level of an emotion\/state, then, in step S, the system controller  determines whether the subject of the detected word is the user a, that is, whether the detected word indicates the emotion, state, or the level of the emotion of the user a. If the subject is not the user a, then the process returns to step S, and the above-described process is repeated.","In the case in which it is determined in step S that the user is the subject, the process proceeds to step S. In step S, the system controller  determines whether one phrase of voice of the user a has been input. If one phrase of voice has not yet been input, the system controller  waits until one phrase of voice is input. If it is determined in step S that one phrase has been input, then, in step S, the system controller  commands the feature extractor  to end the sampling of acceleration information and grasp pressure information.","In step S, the system controller  commands the meaning extractor  to extract emotion information E.","For example, if the user a utters \u201cI am surprised!\u201d, an emotion\/state corresponding to a word \u201csurprised\u201d is surprise. In this case, no adverbs (such as \u201cvery\u201d or \u201ca little\u201d) are included in the voice information uttered by the user a, and thus the strength level is regarded as a middle level. As a result, \u201cmiddle-level surprise\u201d is detected as the emotion information E.","In step S, the system controller  commands the feature extractor  to generate a feature vector Vc on the basis of acceleration information and grasp pressure information of the user a. In step S, acceleration information and grasp pressure information corresponding to information PID and the emotion information E is read from the storage unit . In this specific case, a feature vector of acceleration\/grasp pressure information corresponding to surprise of the user a is read.","In step S, using the learning unit , the system controller  adds the newly generated feature vector Vc to the read feature vectors and recalculates the vector distribution fvc, the distribution center vector V\u03bc, and the standard deviation vector V\u03c3. In step S, the recalculated vectors are stored in the storage unit .","Thus, the feature value vector corresponding to the surprise of the user a is generated from the acceleration information and the grasp pressure information, and the vector distribution, the distribution center vector, and the standard deviation vector are recalculated and stored.","Referring to , the process of generating the feature vectors Vc of acceleration information and grasp pressure information is described below. This process is performed by the feature extractor . In step S, the feature extractor  calculates the feature value of acceleration information. The details of this calculation process will be described later with reference to . As a result, of components of the feature value vector Vc, first to twelfth components (Acc+(), Acc+(), Acc+(), Tac+(), Tac+(), Tac+(), Acc\u2212(), Acc\u2212(), Acc\u2212(), Tac\u2212(), Tac\u2212(), Tac\u2212()) are obtained. In step S, the feature extractor  calculates the feature value of grasp pressure information. The details of this calculation process will be described later with reference to . As a result, of components of the feature value vector Vc, thirteenth to twentieth components (lgr_R(), lgr_R(), Tgr_R(), Tgr_R(), Igr_L(), Igr_L(), Tgr_L(), Tgr_L()) are obtained.","In step S, the feature extractor  generates a feature vector Vc (Acc+(), Acc+(), Acc+(), Tac+(), Tac+(), Tac+(), Acc\u2212(), Acc\u2212(), Acc\u2212(), Tac\u2212(), Tac\u2212(), Tac\u2212(), Igr_R(), Igr_R(), Tgr_R(), Tgr_R(), Igr_L(), Igr_L(), Tgr_L(), Tgr_L()).","Thus, the feature value vector Vc corresponding to the surprise of the user a is generated from the acceleration information and the grasp pressure information.","Now, referring to , the process of calculating the feature value associated with acceleration information in step S in  is described below. In step S, the feature extractor  reads sampled acceleration information. Thereafter, in step S, as in EXAMPLE 1, the feature extractor  performs the polarity detection process described above with reference to , and then in step S the Tv generation process described above with reference to , and furthermore in step S the Vmin\/Vmax detection process described above with reference to .",{"@attributes":{"id":"p-0484","num":"0483"},"figref":["FIG. 102","FIG. 102"],"b":["961","941","961","1672","1674","0","0"]},"Tv+ indicates a period of time during which a possible acceleration is applied to the controller. Tv\u2212 indicates a period of time during which a negative acceleration is applied to the controller. Tv indicates a period of time during which substantially no acceleration is applied to the controller.","Furthermore, via the Vmin\/Vmax detection process in step S, an output value Vmax in the period Tv+ and an output value Vmin in the period Tv\u2212 are obtained. As a result, waveform information is obtained as shown in .","In step S shown in , the feature extractor  sorts the output values Vmax(P) currently arranged in ascending order of time into a descending order of value, and the feature extractor  extracts output values Vmax(), Vmax(), and Vmax(). In step S, the feature extractor  extracts periods of time Tv+(), Tv+(), and Tv+() corresponding to the respective output values Vmax(), Vmax(), and Vmax().","More specifically, of the three rectangular wave signals with negative output values shown in , a rectangular wave signal with a greatest amplitude (that is, a third rectangular wave signal as counted from the leftmost end in ) is first selected, and the values of the output value and the period thereof are respectively set as Vmax() and Tv+(). Similarly, the output value and the period of time with a second greatest amplitude are respectively set as Vmax() and Tv+(), and the output value and the period of time with a third greatest amplitude are respectively set as Vmax() and Tv+(). In the example shown in , the waveform includes three positive rectangular wave signals. When the waveform includes only two positive rectangular wave signals, Vmax() and Tv+() are respectively set to 0.","Referring again to , in step S, the feature extractor  sets the values of Vmax(), Vmax(), Vmax(), Tv+(), Tv+(), and Tv+() into first to sixth components Acc+(), Acc+(), Acc+(), Tac+(), Tac+(), and Tac+(), respectively, of the feature vector Vc.","In step S, the feature extractor  sorts the output values Vmin(P) currently arranged in ascending order of time into a ascending order of value, and the feature extractor  extracts output values Vmin(), Vmin(), and Vmin(). In step S, the feature extractor  extracts periods of time Tv\u2212(), Tv\u2212(), and Tv\u2212() corresponding to the respective output values Vmin(), Vmin(), and Vmin().","More specifically, of the three rectangular wave signals with negative output values shown in , a rectangular wave signal with a greatest absolute amplitude (that is, a rectangular wave signal at the leftmost end) is first selected, and the values of the output value and the period thereof are respectively set as Vmin() and Tv\u2212(). Similarly, the output value and the period of time with a second greatest absolute amplitude are respectively set as Vmin() and Tv\u2212(), and the output value and the period of time with a third greatest absolute amplitude are respectively set as Vmin() and Tv\u2212(). In the example shown in , the waveform includes three negative rectangular wave signals. When the waveform includes only two negative rectangular wave signals, Vmin() and Tv\u2212() are respectively set to 0.","Referring again to , in step S, the feature extractor  sets the values of Vmin(), Vmin(), Vmin(), Tv\u2212(), Tv\u2212(), and Tv\u2212() into seventh to twelfth components Acc\u2212(), Acc\u2212(), Acc\u2212(), Tac\u2212(), Tac\u2212(), and Tac\u2212(), respectively, of the feature vector Vc.","In this way, feature values of acceleration information are calculated.","Now, referring to , the process of calculating feature values associated with grasp pressure information in step S in  is described below. Grasp pressure information is output from the right-hand grasp pressure sensor R and the left-hand grasp pressure sensor L of the controller . The feature values of grasp pressure information are calculated for each of grasp pressure information output from the grasp pressure sensor R and grasp pressure information output from the grasp pressure sensor L.","In step S, the feature extractor  reads sampled grasp pressure information. Thereafter, in step S, as in EXAMPLE 1, the feature extractor  performs the polarity detection process described above with reference to , then in step S, the Tv generation process described above with reference to , and furthermore in step S, the Vmin\/Vmax detection process described above with reference to .  shows an example of a waveform of grasp pressure information output from the grasp pressure sensor R, and  shows an example of a waveform of grasp pressure information output from the grasp pressure sensor L. In , the vertical axis of the graph indicates the grasp pressure, and the horizontal axis indicates the time.","If this waveform is sampled, and the sampled data is subjected to the polarity detection process in step S and the Tv generation process in step S, Tv+, Tv, Tv+, Tv, . . . are obtained as the result.","Tv+ indicates a period of time during which the controller is grasped by the user a at a grasp pressure higher than a predetermined value. Tv indicates a period of time during which the controller is grasped by the user a at a grasp pressure lower than the predetermined value.","Furthermore, via the Vmin\/Vmax detection process in step S, an output value Vmax in the period Tv+ and an output value Vmin in the period Tv\u2212 are obtained. As a result, waveform information such as that shown in  is obtained from the grasp information shown in , and waveform information such as that shown in  is obtained from the grasp information shown in .","In step S shown in , the feature extractor  sorts the values Vmax(P) output from the right-hand grasp pressure sensor R into a descending order of value from the current ascending order of time and the feature extractor  extracts output values Vmax() and Vmax(). In step S, the feature extractor  extracts periods of time Tv+() and Tv+() corresponding to the respective output values Vmax() and Vmax().","More specifically, of the two positive rectangular wave signals shown in , a rectangular wave signal with a greatest amplitude (that is, a rectangular wave signal at the leftmost end in ) is first selected, and the values of the output value and the period thereof are respectively set as Vmax() and Tv+(). Similarly, the output value and the period of time with a second greatest amplitude are respectively set as Vmax() and Tv+(). In the example shown in , the waveform includes two positive rectangular wave signals. When the waveform includes only one positive rectangular wave signal, Vmax() and Tv+() are respectively set to 0.","Referring again to , in step S, the feature extractor  sets the values of Vmax(), Vmax(), Tv+(), and Tv+() into thirteenth to sixteenth components Igr_R(), Igr_R(), Tgr_R(), and Tgr_R(), respectively, of the feature vector Vc.","In step S, the feature extractor  sorts the values Vmax(P) output from the left-hand grasp pressure sensor L into a descending order of value from the current ascending order of time and the feature extractor  extracts output values Vmax() and Vmax(). In step S, the feature extractor  extracts periods of time Tv+() and Tv+() corresponding to the respective output values Vmax() and Vmax().","In step S, the feature extractor  sets the values of Vmax(), Vmax(), Tv+(), and Tv+() into seventeenth to twentieth components Igr_L(), Igr_L(), Tgr_L(), and Tgr_L(), respectively, of the feature vector Vc.","In this way, feature values of grasp pressure information are calculated.","The process has been described above for the case in which the game machine A stores feature information on the basis of acceleration information and grasp pressure information associated with the user a. The game machine can also store feature information on the basis of acceleration information and grasp pressure information associated with the user b transmitted from the game machine B. In this case, acceleration information and grasp pressure information associated with the user b are sampled by the feature extractor  of the game machine B and transmitted together with a header to the game machine A via the output controller  and the information transmitting unit . Voice information uttered by the user b is transmitted to the game machine A, and the meaning extractor  of the game machine A extracts emotion information from the voice information of the user b.","If the game machine receives the acceleration information and grasp pressure information associated with the user b from the game machine B, the game machine A performs the feature value storage process described above with reference to . For example, if the user b utters \u201cI am embarrassed.\u201d, then, in the feature information storage process in step S in , the game machine A extracts \u201cmiddle-level instability\u201d as emotion information. On the other hand, in the game machine B, when the user b utters \u201cI am embarrassed\u201d, the signal with the waveform shown in  is detected by the 3-axis acceleration sensor  of the controller  of the game machine B, the signal with the waveform shown in  is detected by the grasp pressure sensor R of the controller  of the game machine B, and the signal with the waveform shown in  is detected by the grasp pressure sensor L of the controller  of the game machine B.","The signals with waveforms shown in  are sampled at intervals Tclk by the feature extractor  of the game machine B. From the signal shown in , sampled values Vs, Vs, Vs, . . . , Vsn are obtained as acceleration information and transmitted to the game machine A together with a header indicating that the information is acceleration information associated with the user b. From the signal shown in , sampled values VsR, VsR, VsR, . . . , VsRn are obtained as grasp pressure information and transmitted to the game machine A together with a header indicating that the information is right-hand grasp pressure information associated with the user b. Similarly, from the signal shown in , sampled values VsL, VsL, VsL, . . . , VsLn are obtained as grasp pressure information and transmitted to the game machine A together with a header indicating that the information is left-hand grasp pressure information associated with the user b. In the process of generating a feature value vector Vc in step S in , the game machine A generates a feature value vector corresponding to a middle-level instability of the user b on the basis of the acceleration information and grasp pressure information transmitted from the game machine B, and the game machine stores the resultant feature value vector Vc.","The terminal B generates a feature value vector from acceleration information and grasp pressure information associated with the user b and stores the generated feature value vector, and the terminal B also generates a feature value vector from acceleration information and grasp pressure information associated with the user a transmitted from the game machine A and stores the generated feature value vector. As a result, as shown in , a learning result  associated with acceleration information and grasp pressure information is stored in the storage unit  of each of the game machine A and the game machine B, wherein the learning result  includes information  associated with the user a and information  associated with the user b.","Once sufficient amounts of acceleration information and grasp pressure information have been stored via a repetition of above-described feature information storage process, it becomes possible to identify the emotion\/state and the strength level of each user from the acceleration information and grasp pressure information.","Now, referring to , the process performed by the terminal B to identify an emotional state of the user a is described below. In step S, the system controller  of the game machine B acquires information PID identifying the user a of the game machine A with which to communicate. In step S, the system controller  of the game machine B commands the information discriminator  to determine whether at least one of output values of the acceleration information or grasp pressure information associated with the user a exceeds a threshold value. If either one is lower than the threshold value, the process waits until at least one of output values of the acceleration information or grasp pressure information associated with the user a exceeds the threshold value. The threshold value may be set in a program or the like in advance, or the threshold value may be specified by the user each time a communication is performed.","If it is determined in step S that at least one of output values of the acceleration information or grasp pressure information associated with the user a exceeds the threshold value, then in step S, the system controller  stores the acceleration information or grasp pressure information associated with the user a into a storage unit (not shown) in the feature extractor . Then in step S, the system controller  commands the information discriminator  to determine whether the output value of the acceleration information or grasp pressure information associated with the user a becomes and remains smaller than the threshold value for a period longer than a predetermined value (for example, 10 sec). If it is determined that the acceleration information or grasp pressure information associated with the user a does not become and remain smaller than the threshold value for a period longer than the predetermined value, the process returns to step S.","If it is determined in step S that the acceleration information or grasp pressure information associated with the user a becomes and remains smaller than the threshold value for a period longer than the predetermined value, then in step S, the system controller  commands the feature extractor  to generate a feature vector Vcag that will be described later with reference to . Thus, a feature vector Vcag is generated on the basis of the acceleration information and grasp pressure information associated with the user a. In step S, the system controller  commands the learning unit  to perform an emotion information identification process associated with Vcag. The details of the emotion information identification process will be described later with reference to . Thus, emotion information corresponding to the feature vector Vcag is identified. In step S, the system controller  commands the learning unit  to output the identification result and perform a recalculation. The details of this process will be described later with reference to .","Now, referring to , the process of generating Vcag is described below. This process is performed by the feature extractor  of the game machine B. In step S, the feature extractor  performs the process of calculating the feature value associated with acceleration information (this process is performed in a similar manner as described above with reference to , and thus a duplicated description of the details of the process is not given herein). In step S, the feature extractor  performs the process of calculating the feature value associated with grasp pressure information (this process is performed in a similar manner as described above with reference to , and thus a duplicated description of the details of the process is not given herein). Then in step S, the feature extractor  generates a feature vector Vcag(Acc+(), Acc+(), Acc+(), Tac+(), Tac+(), Tac+(), Acc\u2212(), Acc\u2212(), Acc\u2212(), Tac\u2212(), Tac\u2212(), Tac\u2212(), Igr_R(), Igr_R(), Tgr_R(), Tgr_R(), Igr_L(), Igr_L(), Tgr_L(), Tgr_L()).","Thus, the feature vector Vcag is generated from the acceleration information and grasp pressure information associated with the user a.","Now, referring to , the emotion information identification process associated with Vcag is described below. This process is performed by the learning unit  of the game machine B.","In step S, the learning unit  reads, from the storage unit , the feature value vector distribution fvc, the distribution center vector V\u03bc, and the distribution standard deviation vector V\u03c3 associated with acceleration information and grasp pressure information of the user a.","As described above, there are 36 kinds of emotion information, and there are 36 feature value vector distributions fvc, 36 distribution center vectors V\u03bc, and 36 distribution standard deviation vectors V\u03c3 corresponding to the respective 36 kinds of emotion information.","Then in step S, the learning unit  sets the parameter Q to 1. Note that the parameter Q can take an integral value in the range of from 1 to 36 depending on which one of the 36 kinds of emotion information is dealt with. In step S, the learning unit  of the terminal B calculates the inner product of the vector V\u03bc(Q) and the feature value vector Vcag generated in step S (), and stores V\u03bc(Q) in relation to the inner product. Then in step S, the learning unit  increments the value of Q by 1. In step S, the learning unit  of the terminal B determines whether the calculation is completed for all kinds of emotion information, that is, whether the calculation of the inner product is completed for all 36 center vectors V\u03bc. If it is determined that the inner product is not calculated for all kinds of emotion information, the flow returns to step S, and the above-described process is repeated.","When the calculation of the inner product is completed for all 36 center vectors V\u03bc corresponding to the respective kinds of emotion information, data associated with the emotion information, the inner products, data indicating the kinds of the center vector V\u03bc, and data indicating the order of the inner products are stored in the storage unit (not shown) of the learning unit , as in EXAMPLE 1. In step S, the learning unit  sorts the data in descending order of inner product. The order of the magnitude of the inner product is represented by R that can take an integral value in the range from 1 to 36.","In step S, the learning unit  sets the parameter R to 1. In step S, the learning unit  determines whether data associated with all kinds of the emotion information are read. If it is determined that data associated with all kinds of the emotion information are not read, then in step S, the learning unit  reads, one by one, data sorted in step S. The inner products of the feature vector Vcag and center vectors corresponding to respective 36 kinds of emotion information are calculated in step S as described above, and a greatest one of all inner products is selected. If the center vector corresponding to the detected greatest inner product of the center vector and the feature vector Vcag is, for example, a center vector V\u03bc of feature vectors corresponding to emotion information \u201cstrong vitality\u201d of the user a, then, when R=1, the center vector V\u03bc and the standard deviation vector V\u03c3 corresponding to emotion information \u201cstrong vitality\u201d of the user a are read.","In step S, the learning unit  sets the parameter m to 1. Then in step S, the learning unit  determines whether the condition V\u03bc(R)m\u2212V\u03c3(R)m\u2266Vcagm\u2266V\u03bc(R)m+V\u03c3(R)m is satisfied, wherein V\u03bc(R)m denotes an mth component of the vector V\u03bc(R), and V\u03c3(R)m and Vcagm respectively denote mth components of vectors V\u03c3 and Vcag. As described above, the feature vector associated with acceleration information and grasp pressure information includes the following 20 components: Acc+(), Acc+(), Acc+(), Tac+(), Tac+(), Tac+(), Acc\u2212(), Acc\u2212(), Acc\u2212(), Tac\u2212(), Tac\u2212(), Tac\u2212(), Igr_R(), Igr_R(), Tgr_R(), Tgr_R(), Igr_L(), Igr_L(), Tgr_L(), and Tgr_L(). When m=1, the component Acc+() is extracted from each vector of V\u03bc(R), V\u03c3(R), and Vcag and subjected to the process.","If it is determined in step S that the condition V\u03bc(R)m\u2212V\u03c3(R)m\u2266Vcagm\u2266V\u03bc(R)m+V\u03c3(R)m is not satisfied, the process proceeds to step S. In step S, the learning unit  increments the value of R by 1 and returns the flow to step S to repeat the above-described process. In the case in which it is determined in step S that the condition V\u03bc(R)m\u2212V\u03c3(R)m\u2266Vchm\u2266V\u03bc(R)m+V\u03c3(R)m is satisfied, the process proceeds to step S. In step S, the learning unit  increments the value of m by 1 and advances the flow to step S. In step S, the learning unit  determines whether m is equal to 21, that is, whether the judgment about the above condition has been performed for all 20 components of respective vectors V\u03bc(R), V\u03c3(R), and Vcag. If it is determined that m is not equal to 21, the process returns to step S, and the above-described process is repeated.","On the other hand, if it is determined in step S that m is equal to 21, that is, if it is determined that the condition V\u03bc(R)m\u2212V\u03c3(R)m\u2266Vcagm\u2266V\u03bc(R)m+V\u03c3(R)m is satisfied for all 20 components of respective vectors V\u03bc(R), V\u03c3(R), and Vcag, then, in step S, the learning unit  determines that Vcag belongs to emotion information E corresponding to R. For example, when R=1, if the condition V\u03bcm\u2212V\u03c3m\u2266Vcagm\u2266V\u03bcm+V\u03c3m is satisfied for all 20 components of respective vectors V\u03bc and V\u03c3, the feature vector Vcag is regarded as belonging to emotion information \u201cstrong vitality\u201d.","If it is determined in step S that data associated with all kinds of emotion information are read, the process proceeds to step S. In step S, the learning unit  determines that the emotion information corresponding to Vcag cannot be identified, and ends the process. That is, when the condition V\u03bc(R)m\u2212V\u03c3(R)m\u2266Vcagm\u2266V\u03bc(R)m+V\u03c3(R)m is examined for all 36 center vectors V\u03bc(R) and for all 36 standard deviation vectors V\u03c3(R) corresponding to respective 36 kinds of emotion information, if the condition V\u03bc(R)m\u2212V\u03c3(R)m\u2266Vcagm\u2266V\u03bc(R)m+V\u03c3(R)m for all 20 components of respective vectors V\u03bc(R), V\u03c3(R), and Vcag is not satisfied for any R, it is determined that the emotion information corresponding to Vcag cannot be identified.",{"@attributes":{"id":"p-0525","num":"0524"},"figref":["FIG. 117","FIG. 117"],"b":["1","2","3","1","2","3","1","2","3","1","2","3","1","2","1","2","1","2","1","2","9","9","15","15","17","17"]},"Among center vectors corresponding to respective kinds of emotion information, the feature value vector Vcag generated in step S in  is close to a center vector V\u03bc of a distribution of a feature vectors V corresponding to emotion information \u201cstrong vitality\u201d, and thus, as a result of the process of identifying the emotion information identification process associated with Vcag, the Vcag is determined as a feature vector corresponding to \u201cstrong vitality\u201d.","In this way, the emotion\/state and the strength level of the user a are identified from the feature value vector Vcag.","Now, referring to , the process of outputting the identification result and performing a recalculation is described below. This process is performed by the learning unit  of the game machine B. In step S, commands the learning unit  outputs the identification result to the output controller . In step S, the learning unit  of the terminal B recalculates the distribution fvc of the feature value vector of acceleration information and grasp pressure information corresponding to emotion information E of the user a and the center vector V\u03bc and the standard deviation vector V\u03c3 of the distribution fvc, and the learning unit  of the terminal B stores the recalculated result. That is, the feature value vector Vcag generated in step S () is added to the feature value vector of acceleration\/grasp pressure information indicating the \u201cstrong activity\u201d of the user a stored in the storage unit , and fvc, V\u03bc, and V\u03c3 are recalculated and stored in the storage unit .","In this way, the emotion\/state and the level are identified from the acceleration information and grasp pressure information, and information associated with the feature vectors of the identified emotion\/state and the strength level thereof is stored.","Thus, the game machine B generates the feature value vector Vcag from the acceleration information and grasp pressure information associated with the user a received from the game machine A, and the game machine B identifies the emotion\/state and the strength level thereof. The game machine B can also generate feature value vectors from acceleration information and grasp pressure information associated with the user b and can identify the emotion\/state and the strength level thereof.","Similarly, the game machine A identifies the emotion\/state and the strength level of the user a and the user b.","Now, referring to , an information outputting process is described below. In step S, the system controller  determines whether information to be output is selected. If information to be output is not selected, the process waits until information to be output is selected. The selection of information to be output may be performed, for example, by clicking a menu item displayed on a display or a television set connected to the game machine of the terminal. The selection menu used by a user to select information to be output includes the following four items: \u201cidentification result\u201d, \u201cinformation based on feature values\u201d, \u201cwaveform information\u201d, and \u201cstored information\u201d.","If it is determined in step S that information to be output is selected, then, in step S, the system controller  determines whether the information selected to be output is the identification result. If it is determined that the information selected to be output is the identification result, then in step S, the system controller  transmits, to the output controller , 2-bit selection information 00 indicating that the identification result should be output.","In step S, the output controller  controls displaying of a character on accordance with the identification result of emotion information. The character selected by the user is displayed on a television set or a display connected to the game machine, and the expression and the motion of the character are controlled by the output controller  in accordance with the identified emotion information. For example, when \u201cstrong vitality\u201d is identified as the emotion information, a vivid face expression of the character is displayed, and arms and feet of the character are moved up and down.","If it is determined in step S that the information selected to be output is not the identification result, then, in step S, the system controller  determines whether the information selected to be output is the information based on feature values. If it is determined that the information selected to be output is the information based on feature values, then in step S, the system controller  transmits, to the output controller , 2-bit selection information 01 indicating that the information based on feature values should be output.","In step S, the output controller  acquires feature values (Acc+(), Acc+(), Acc+(), Tac+(), Tac+(), Tac+(), Acc\u2212(), Acc\u2212(), Acc\u2212(), Tac\u2212(), Tac\u2212(), Tac\u2212()) from the feature extractor . Then in step S, the output controller  generates a vibration presentation unit driving signal. That is, as shown in , a driving signal composed of rectangular wave signals with output levels Acc+(), Acc+(), and Acc+() and on-periods Tac+(), Tac+(), and Tac+() and rectangular wave signals with output levels Acc\u2212(), Acc\u2212(), and Acc\u2212() and on-periods Tac\u2212(), Tac\u2212(), and Tac\u2212() is generated. Then in step S, the output controller  simultaneously drives the vibration presentation units L and R in accordance with the generated driving signal.","In step S, the output controller  acquires feature values (Igr_R(), Igr_R(), Tgr_R(), Tgr_R(), Igr_L(), Igr_L(), Tgr_L(), Tgr_L()) from the feature extractor . Then in step S, the output controller  generates a light emission unit driving signal. More specifically, as shown in , the output controller  generates a driving signal composed of a sinusoidal wave signal with an amplitude Igr_R() and an on-period Tgr_R() and a sinusoidal wave signal with an amplitude Igr_R() and an on-period Tgr_R(), and also generates, as shown in , a driving signal composed of a sinusoidal wave signal with an amplitude Igr_L() and an on-period Tgr_L() and a sinusoidal wave signal with an amplitude Igr_L() and an on-period Tgr_L().","In step S, the output controller  drives the light emission unit R in accordance with the driving signal shown in  and drives the light emission unit L in accordance with the driving signal shown in .","If it is determined in step S that the information selected to be output is not the information based on feature values, then, in step S, the system controller  determines whether the information selected to be output is the waveform information. If it is determined that the information selected to be output is the waveform information, then in step S, the system controller  transmits, to the output controller , 2-bit selection information 10 indicating that the waveform information should be output.","In step S, the output controller  acquires sampled acceleration information from the feature extractor  and generates driving a signal for driving the vibration presentation units L and R on the basis of the acquired acceleration information. In step S, the output controller  simultaneously drives both the vibration presentation units L and R in accordance with the generated driving signal. In step S, the output controller  acquires sampled grasp pressure information from the feature extractor  and generates a driving signal for driving the light emission unit R on the basis of the acquired right-hand grasp pressure information (grasp pressure information detected by the grasp pressure sensor R) and also generates a driving signal for driving the light emission unit L on the basis of the acquired left-hand grasp pressure information (grasp pressure information detected by the grasp pressure sensor L). In step S, the output controller  drives the light emission units R and L in accordance with the respective generated driving signals.","If it is determined in step S that the information selected to be output is not the waveform information, then, in step S, the system controller  determines whether the information selected to be output is the stored information. If it is determined that the information selected to be output is not the stored information, then the process jumps to step S. In step S, the system controller  performs error handling. If it is determined in step S that the information selected to be output is the stored information, then, in step S, the system controller  transmits, to the output controller , 2-bit selection information 01 indicating that the stored information should be output.","In the case in which the information selected to be output is the stored information, a sub-menu is displayed on the television set or the display connected to the game machine so that a user can specify an item of stored information to be output and also can specify a game machine to which to output the stored information. More specifically, the user can make selections in terms of items of information inf such as acceleration information or grasp pressure information, information PID identifying a user, emotion information E, and information To indicating a destination game machine. For example, the user can select acceleration information or grasp pressure information as inf, information indicating the user a as PID, \u201cstrong vitality\u201d as emotion information E, and a user b as a destination.","In step S, the system controller  determines whether inf indicating a particular kind of information, PID identifying a user, emotion information E, and a destination To are selected. If selection is not performed, the process waits until selection is made.","In step S, the system controller  generates information (inf, PID, E, To) in accordance with the selection made by the user and transmits the generated information to the output controller .","In step S, the output controller  outputs stored information. The details of this process will be described later with reference to . In this process, a signal is generated in accordance with inf indicating the selected kind of information, PID identifying the user, and emotion information E, and the generated signal is transmitted to the selected destination To.","Referring to , the process of outputting stored information in step S in  is described below.","In step S, in accordance with the selected information (PID, E), the output controller  acquires a center vector of acceleration information and grasp pressure information from the storage unit . In this specific example, a center vector of feature vectors of acceleration information and grasp pressure information corresponding to \u201cstrong vitality\u201d of the user a is acquired from the storage unit , that is, a center vector V\u03bc(Acc+30(), Acc+30(), Acc+30(), Tac+30(), Tac+30(), Tac+30(), Acc\u221230(), Acc\u221230(), Acc\u221230(), Tac\u221230(), Tac\u221230(), Tac\u221230(), Igr_R(), Igr_R(), Tgr_R(), Tgr_R(), Igr_L(), Igr_L(), Tgr_L(), Tgr_L()) is acquired.","In step S, the output controller  determines whether acceleration information is selected as the kind of information inf. If it is determined that acceleration information is selected, the process proceeds to step S. In step S, the output controller  extracts first to twelfth components of V\u03bc.","In step S, the output controller  generates a vibration presentation unit driving signal. That is, as shown in , the output controller  generates a driving signal composed of rectangular wave signals with output levels Acc+30(), Acc+30(), and Acc+30() and on-periods Tac+30(), Tac+30(), and Tac+30() and rectangular wave signals with output levels Acc\u221230(), Acc\u221230(), and Acc\u221230() and on-periods Tac\u221230(), Tac\u221230(), and Tac\u221230().","In step S, the output controller  determines whether the destination To of information is the terminal (game machine) of the present user. If it is determined that the destination is not the terminal of the current user, the process proceeds to step S. In step S, the output controller  transmits the driving signals to the terminal specified as the destination. For example, in the case in which the user b is specified as the destination To, the driving signals are transmitted to the terminal (game machine B) of the user b. In step S, the game machine B receives the driving signals via the information receiving unit . The received driving signals are supplied to the output controller  via the information discriminator . The output controller  of the game machine B simultaneously drives both the vibration presentation units R and L in accordance with the driving signals.","On the other hand, if it is determined in step S that the destination To is the terminal (game machine) of the present user, the process proceeds to step S. In step S, the output controller  simultaneously drives both the vibration presentation units R and L of the present terminal (game machine) in accordance with the driving signals.","In the case in which it is determined in step S that acceleration information is not selected as the kind of information inf (that is, grasp pressure information is selected), the process proceeds to step S. In step S, the output controller  extracts thirteenth to twentieth components of V\u03bc.","Then in step S, the output controller  generates a light emission unit driving signal. More specifically, as shown in , the output controller  generates a driving signal composed of a sinusoidal wave signal with an amplitude Igr_R() and an on-period Tgr_R() and a sinusoidal wave signal with an amplitude Igr_R() and an on-period Tgr_R(), and also generates, as shown in , a driving signal composed of a sinusoidal wave signal with an amplitude Igr_L() and an on-period Tgr_L() and a sinusoidal wave signal with an amplitude Igr_L() and an on-period Tgr_L().","In step S, the output controller  determines whether the destination To of information is the terminal (game machine) of the present user. If it is determined that the destination is not the terminal of the current user, the process proceeds to step S. In step S, the output controller  transmits the driving signals to the terminal specified as the destination. For example, in the case in which the user b is specified as the destination To, the driving signals are transmitted to the terminal (game machine B) of the user b. In step S, the game machine B receives the driving signals via the information receiving unit . The received driving signals are supplied to the output controller  via the information discriminator . The output controller  of the game machine B drives the light emission unit R in accordance with the driving signal shown in  and drives the light emission unit L in accordance with the driving signal shown in .","On the other hand, if it is determined in step S that the destination To is the terminal (game machine) of the present user, the process proceeds to step S. In step S, the output controller  drives the light emission unit R of the present terminal (game machine) in accordance with the driving signal shown in  drives the light emission unit L of the present terminal (game machine) in accordance with the driving signal shown in .","Thus, the emotion\/state and the strength level thereof identified from the acceleration information and grasp pressure information are displayed so that the user can recognize them. The controller of the game machine is vibrated so that the user can recognize the acceleration information via a bodily sensation. Furthermore, light is emitted from the controller of the game machine so that the user can recognize grasp information via a bodily sensation. It is also possible for a user to specify acceleration information or grasp pressure information corresponding to a particular kind of emotion information of a particular user and specify a particular user to whom the acceleration information or grasp pressure information is presented so that the user can recognize the acceleration information or grasp pressure information via bodily sensations.","In on-line games, a user can recognize emotion\/state and strength level of an opposing player via bodily sensations. This makes it possible for players to enjoy realistic feelings in on-line games.","The present invention has been described above with reference to specific examples 1 to 4. Not that a terminal may be formed according to a combination of techniques disclosed in examples 1 to 4 described above.","A sequence of processing steps described above in each example may or may not be performed time-sequentially in the same order as the order in which steps are described above. For example, steps may be performed in a parallel fashion or a separate fashion.","INDUSTRIAL APPLICABILITY","As described above, the present invention make it possible to identify the emotion\/state the strength level of a user from vital sign information, keyboard vibration information, mouse click information, mouse grasp pressure information, and\/or information about acceleration\/grasp pressure applied to a controller of a game machine. Not only text information, voice information, and image information that can be transmitted in conventional techniques, but also vital sign information, keyboard vibration information, mouse click information, mouse grasp pressure information, information about acceleration\/grasp pressure applied to a controller of a game machine and other similar information can be transmitted and output in various fashions, thereby making it possible to realize close communication that allows emotion to be directly conveyed to each other."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":["FIG. 6","FIG. 5"]},{"@attributes":{"id":"p-0032","num":"0031"},"figref":["FIG. 7","FIG. 5"]},{"@attributes":{"id":"p-0033","num":"0032"},"figref":["FIG. 8","FIG. 5"]},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 14","b":"1"},{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 15","b":"1"},{"@attributes":{"id":"p-0041","num":"0040"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0042","num":"0041"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0043","num":"0042"},"figref":"FIG. 18"},{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 19"},{"@attributes":{"id":"p-0045","num":"0044"},"figref":"FIG. 20"},{"@attributes":{"id":"p-0046","num":"0045"},"figref":"FIG. 21","b":"2"},{"@attributes":{"id":"p-0047","num":"0046"},"figref":"FIG. 22","b":"2"},{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIG. 23"},{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIG. 24"},{"@attributes":{"id":"p-0050","num":"0049"},"figref":"FIG. 25"},{"@attributes":{"id":"p-0051","num":"0050"},"figref":"FIG. 26"},{"@attributes":{"id":"p-0052","num":"0051"},"figref":"FIG. 27"},{"@attributes":{"id":"p-0053","num":"0052"},"figref":"FIG. 28"},{"@attributes":{"id":"p-0054","num":"0053"},"figref":"FIG. 29","b":"1"},{"@attributes":{"id":"p-0055","num":"0054"},"figref":"FIG. 30","b":"1"},{"@attributes":{"id":"p-0056","num":"0055"},"figref":"FIG. 31","b":"2"},{"@attributes":{"id":"p-0057","num":"0056"},"figref":"FIG. 32","b":"2"},{"@attributes":{"id":"p-0058","num":"0057"},"figref":"FIG. 33","b":"1"},{"@attributes":{"id":"p-0059","num":"0058"},"figref":"FIG. 34","b":"1"},{"@attributes":{"id":"p-0060","num":"0059"},"figref":"FIG. 35","b":"1"},{"@attributes":{"id":"p-0061","num":"0060"},"figref":"FIG. 36","b":"2"},{"@attributes":{"id":"p-0062","num":"0061"},"figref":"FIG. 37","b":"2"},{"@attributes":{"id":"p-0063","num":"0062"},"figref":"FIG. 38","b":"2"},{"@attributes":{"id":"p-0064","num":"0063"},"figref":"FIG. 39"},{"@attributes":{"id":"p-0065","num":"0064"},"figref":"FIG. 40"},{"@attributes":{"id":"p-0066","num":"0065"},"figref":"FIG. 41"},{"@attributes":{"id":"p-0067","num":"0066"},"figref":"FIG. 42"},{"@attributes":{"id":"p-0068","num":"0067"},"figref":"FIG. 43"},{"@attributes":{"id":"p-0069","num":"0068"},"figref":"FIG. 44"},{"@attributes":{"id":"p-0070","num":"0069"},"figref":["FIG. 45","FIG. 44"]},{"@attributes":{"id":"p-0071","num":"0070"},"figref":["FIG. 46","FIG. 44"]},{"@attributes":{"id":"p-0072","num":"0071"},"figref":"FIG. 47"},{"@attributes":{"id":"p-0073","num":"0072"},"figref":"FIG. 48"},{"@attributes":{"id":"p-0074","num":"0073"},"figref":"FIG. 49"},{"@attributes":{"id":"p-0075","num":"0074"},"figref":"FIG. 50"},{"@attributes":{"id":"p-0076","num":"0075"},"figref":"FIG. 51"},{"@attributes":{"id":"p-0077","num":"0076"},"figref":"FIG. 52"},{"@attributes":{"id":"p-0078","num":"0077"},"figref":"FIG. 53"},{"@attributes":{"id":"p-0079","num":"0078"},"figref":"FIG. 54"},{"@attributes":{"id":"p-0080","num":"0079"},"figref":"FIG. 55A"},{"@attributes":{"id":"p-0081","num":"0080"},"figref":"FIG. 55B"},{"@attributes":{"id":"p-0082","num":"0081"},"figref":"FIG. 56"},{"@attributes":{"id":"p-0083","num":"0082"},"figref":"FIG. 57"},{"@attributes":{"id":"p-0084","num":"0083"},"figref":"FIG. 58"},{"@attributes":{"id":"p-0085","num":"0084"},"figref":"FIG. 59"},{"@attributes":{"id":"p-0086","num":"0085"},"figref":"FIG. 60"},{"@attributes":{"id":"p-0087","num":"0086"},"figref":"FIG. 61"},{"@attributes":{"id":"p-0088","num":"0087"},"figref":"FIG. 62"},{"@attributes":{"id":"p-0089","num":"0088"},"figref":"FIG. 63"},{"@attributes":{"id":"p-0090","num":"0089"},"figref":"FIG. 64"},{"@attributes":{"id":"p-0091","num":"0090"},"figref":"FIG. 65"},{"@attributes":{"id":"p-0092","num":"0091"},"figref":"FIG. 66"},{"@attributes":{"id":"p-0093","num":"0092"},"figref":["FIG. 67","FIG. 66"]},{"@attributes":{"id":"p-0094","num":"0093"},"figref":["FIG. 68","FIG. 66"]},{"@attributes":{"id":"p-0095","num":"0094"},"figref":"FIG. 69"},{"@attributes":{"id":"p-0096","num":"0095"},"figref":"FIG. 70"},{"@attributes":{"id":"p-0097","num":"0096"},"figref":"FIG. 71"},{"@attributes":{"id":"p-0098","num":"0097"},"figref":"FIG. 72"},{"@attributes":{"id":"p-0099","num":"0098"},"figref":"FIG. 73"},{"@attributes":{"id":"p-0100","num":"0099"},"figref":"FIG. 74"},{"@attributes":{"id":"p-0101","num":"0100"},"figref":"FIG. 75"},{"@attributes":{"id":"p-0102","num":"0101"},"figref":"FIG. 76"},{"@attributes":{"id":"p-0103","num":"0102"},"figref":"FIG. 77"},{"@attributes":{"id":"p-0104","num":"0103"},"figref":"FIG. 78"},{"@attributes":{"id":"p-0105","num":"0104"},"figref":"FIG. 79"},{"@attributes":{"id":"p-0106","num":"0105"},"figref":"FIG. 80"},{"@attributes":{"id":"p-0107","num":"0106"},"figref":"FIG. 81"},{"@attributes":{"id":"p-0108","num":"0107"},"figref":"FIG. 82"},{"@attributes":{"id":"p-0109","num":"0108"},"figref":"FIG. 83"},{"@attributes":{"id":"p-0110","num":"0109"},"figref":"FIG. 84"},{"@attributes":{"id":"p-0111","num":"0110"},"figref":"FIG. 85"},{"@attributes":{"id":"p-0112","num":"0111"},"figref":"FIG. 86"},{"@attributes":{"id":"p-0113","num":"0112"},"figref":"FIG. 87"},{"@attributes":{"id":"p-0114","num":"0113"},"figref":"FIG. 88"},{"@attributes":{"id":"p-0115","num":"0114"},"figref":"FIG. 89"},{"@attributes":{"id":"p-0116","num":"0115"},"figref":"FIG. 90"},{"@attributes":{"id":"p-0117","num":"0116"},"figref":"FIG. 91"},{"@attributes":{"id":"p-0118","num":"0117"},"figref":"FIG. 92"},{"@attributes":{"id":"p-0119","num":"0118"},"figref":"FIG. 93"},{"@attributes":{"id":"p-0120","num":"0119"},"figref":"FIG. 94"},{"@attributes":{"id":"p-0121","num":"0120"},"figref":"FIG. 95"},{"@attributes":{"id":"p-0122","num":"0121"},"figref":"FIG. 96"},{"@attributes":{"id":"p-0123","num":"0122"},"figref":"FIG. 97"},{"@attributes":{"id":"p-0124","num":"0123"},"figref":"FIG. 98"},{"@attributes":{"id":"p-0125","num":"0124"},"figref":"FIG. 99"},{"@attributes":{"id":"p-0126","num":"0125"},"figref":"FIG. 100"},{"@attributes":{"id":"p-0127","num":"0126"},"figref":"FIG. 101"},{"@attributes":{"id":"p-0128","num":"0127"},"figref":"FIG. 102"},{"@attributes":{"id":"p-0129","num":"0128"},"figref":"FIG. 103"},{"@attributes":{"id":"p-0130","num":"0129"},"figref":"FIG. 104"},{"@attributes":{"id":"p-0131","num":"0130"},"figref":"FIG. 105"},{"@attributes":{"id":"p-0132","num":"0131"},"figref":"FIG. 106"},{"@attributes":{"id":"p-0133","num":"0132"},"figref":"FIG. 107"},{"@attributes":{"id":"p-0134","num":"0133"},"figref":"FIG. 108"},{"@attributes":{"id":"p-0135","num":"0134"},"figref":"FIG. 109"},{"@attributes":{"id":"p-0136","num":"0135"},"figref":"FIG. 110"},{"@attributes":{"id":"p-0137","num":"0136"},"figref":"FIG. 111"},{"@attributes":{"id":"p-0138","num":"0137"},"figref":"FIG. 112"},{"@attributes":{"id":"p-0139","num":"0138"},"figref":"FIG. 113"},{"@attributes":{"id":"p-0140","num":"0139"},"figref":"FIG. 114"},{"@attributes":{"id":"p-0141","num":"0140"},"figref":"FIG. 115"},{"@attributes":{"id":"p-0142","num":"0141"},"figref":"FIG. 116"},{"@attributes":{"id":"p-0143","num":"0142"},"figref":"FIG. 117"},{"@attributes":{"id":"p-0144","num":"0143"},"figref":"FIG. 118"},{"@attributes":{"id":"p-0145","num":"0144"},"figref":"FIG. 119"},{"@attributes":{"id":"p-0146","num":"0145"},"figref":"FIG. 120"},{"@attributes":{"id":"p-0147","num":"0146"},"figref":"FIG. 121"},{"@attributes":{"id":"p-0148","num":"0147"},"figref":"FIG. 122"},{"@attributes":{"id":"p-0149","num":"0148"},"figref":"FIG. 123"},{"@attributes":{"id":"p-0150","num":"0149"},"figref":"FIG. 124"},{"@attributes":{"id":"p-0151","num":"0150"},"figref":"FIG. 125"},{"@attributes":{"id":"p-0152","num":"0151"},"figref":"FIG. 126"},{"@attributes":{"id":"p-0153","num":"0152"},"figref":"FIG. 127"}]},"DETDESC":[{},{}]}
