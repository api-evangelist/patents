---
title: Hierarchical managed switch architecture
abstract: Some embodiments provide a method for managing several managed switching elements in a network. The method determines configurations for the several managed switching elements to implement a first logical data path set. Based on the determined configurations, the method configures a first set of the several managed switching elements to implement the first logical data path set. The method configures a second set of the several managed switching elements to implement a second logical data path set.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08750164&OS=08750164&RS=08750164
owner: Nicira, Inc.
number: 08750164
owner_city: Palo Alto
owner_country: US
publication_date: 20110706
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CLAIM OF BENEFIT TO PRIOR APPLICATION","BACKGROUND","BRIEF SUMMARY","DETAILED DESCRIPTION"],"p":["This application claims benefit to U.S. Provisional Patent Application 61\/361,912, filed on Jul. 6, 2010; U.S. Provisional Patent Application 61\/361,913, filed on Jul. 6, 2010; U.S. Provisional Patent Application 61\/429,753, filed on Jan. 4, 2011; U.S. Provisional Patent Application 61\/429,754, filed on Jan. 4, 2011; U.S. Provisional Patent Application 61\/466,453, filed on Mar. 22, 2011; U.S. Provisional Patent Application 61\/482,205, filed on May 3, 2011; U.S. Provisional Patent Application 61\/482,615, filed on May 4, 2011; U.S. Provisional Patent Application 61\/482,616, filed on May 4, 2011; U.S. Provisional Patent Application 61\/501,743, filed on Jun. 27, 2011; and U.S. Provisional Patent Application 61\/501,785, filed on Jun. 28, 2011. These provisional applications are incorporated herein by reference.","Many current enterprises have large and sophisticated networks comprising switches, hubs, routers, servers, workstations and other networked devices, which support a variety of connections, applications and systems. The increased sophistication of computer networking, including virtual machine migration, dynamic workloads, multi-tenancy, and customer specific quality of service and security configurations require a better paradigm for network control. Networks have traditionally been managed through low-level configuration of individual components. Network configurations often depend on the underlying network: for example, blocking a user's access with an access control list (\u201cACL\u201d) entry requires knowing the user's current IP address. More complicated tasks require more extensive network knowledge: forcing guest users' port  traffic to traverse an HTTP proxy requires knowing the current network topology and the location of each guest. This process is of increased difficulty where the network switching elements are shared across multiple users.","In response, there is a growing movement, driven by both industry and academia, towards a new network control paradigm called Software-Defined Networking (SDN). In the SDN paradigm, a network controller, running on one or more servers in a network, controls, maintains, and implements control logic that governs the forwarding behavior of shared network switching elements on a per user basis. Making network management decisions often requires knowledge of the network state. To facilitate management decision-making, the network controller creates and maintains a view of the network state and provides an application programming interface upon which management applications may access a view of the network state.","Three of the many challenges of large networks (including datacenters and the enterprise) are scalability, mobility, and multi-tenancy and often the approaches taken to address one hamper the other. For instance, one can easily provide network mobility for virtual machines (VMs) within an L2 domain, but L2 domains cannot scale to large sizes. Also, retaining tenant isolation greatly complicates mobility. Despite the high-level interest in SDN, no existing products have been able to satisfy all of these requirements.","The network architecture of some embodiments includes network controllers, managed switching elements, and machines. In some embodiments, the managed switching elements route network data (e.g., packets) between network elements in the network that are coupled to the managed switching elements.","The managed switching elements of some embodiments can be configured to route network data according to defined rules. In some embodiments, the managed switching elements routes network data based on routing criteria defined in the rules. Examples of routing criteria include source media access control (MAC) address, destination MAC, packet type, source Internet Protocol (IP) address, destination IP address, source port, destination port, and\/or virtual local area network (VLAN) identifier, among other routing criteria.","In some embodiments, the managed switching elements can include standalone physical switching elements, software switching elements that operate within a computer, or any other type of switching element. For example, each of the managed switching elements may be implemented as a hardware switching element, a software switching element, a virtual switching element, a network interface controller (NIC), or any other type of network element that can route network data. Moreover, the software or virtual switching elements may operate on a dedicated computer, or on a computer that performs non-switching operations.","The machines send and receive network data between each other over the network. In some embodiments, the machines are referred to as network hosts that are each assigned a network layer host addresses (e.g., IP address). Some embodiments refer to the machines as end systems because the machines are located at the edge of the network. In some embodiments, each of the machines can be a desktop computer, a laptop computer, a smartphone, a virtual machine (VM) running on a computing device, a terminal, or any other type of network host.","In some embodiments, each of the network controllers controls one or more managed switching elements that are located at the edge of a network (e.g., edge switching elements or edge devices). In some embodiments, an edge switching element is the last switching element before end machines in a network. In this application, a switching element that is controlled by a network controller of some embodiments may be referred to as a managed switching element.","In addition to controlling edge switching elements, the network controllers of some embodiments also utilize and control non-edge switching elements (e.g., pool nodes, root nodes, and extenders, which are described in further detail below) that are inserted in the network to simplify and\/or facilitate the operation of the managed edge switching elements. For instance, in some embodiments, the network controllers require the switching elements that the network controllers control to be interconnected in a hierarchical switching architecture that has several edge switching elements as the leaf nodes in the hierarchical switching architecture and one or more non-edge switching elements as the non-leaf nodes in this architecture. In some such embodiments, each edge switching element connects to one or more of the non-leaf switching elements, and uses such non-leaf switching elements to facilitate the communication of the edge switching element with other edge switching elements. Examples of such communications with an edge switching elements in some embodiments include (1) routing of a packet with an unknown destination address (e.g., unknown MAC address) to the non-leaf switching element so that the non-leaf switching element can route the packet to the appropriate edge switching element, (2) routing a multicast or broadcast packet to the non-leaf switching element so that the non-leaf switching element can distribute the multicast or broadcast packet to the desired destinations.","Some embodiments employ one level of non-leaf (non-edge) switching elements that connect to edge switching elements and in some cases to other non-leaf switching elements. Other embodiments, on the other hand, employ multiple levels of non-leaf switching elements, with each level of non-leaf switching elements after the first level serving as a mechanism to facilitate communication between lower level non-leaf switching elements and leaf switching elements. In some embodiments, the non-leaf switching elements are software switching elements that are implemented by storing the switching tables in the memory of a standalone computer instead of an off the shelf switch. In some embodiments, the standalone computer may also be executing in some cases a hypervisor and one or more virtual machines on top of that hypervisor. Irrespective of the manner by which the leaf and non-leaf switching elements are implemented, the network controllers of some embodiments store switching state information regarding the leaf and non-leaf switching elements.","As mentioned above, the switching elements of some embodiments route network data between network elements in the network. In some embodiments, the network controllers configure the managed switching elements' routing of network data between the network elements in the network. In this manner, the network controllers can control the flow (i.e., specify the data path) of network data between network elements.","For example, in some embodiments, a network controller might instruct a set of the managed switching elements to route network data from a first machine to a second machine (and vice versa) and to not route (e.g., drop) network data from other machines to the first and second machines. In some such embodiments, the network controller controls the flow of network data through the set of the managed switching elements such that network data transmitted to and from the first machine is only routed to the second machine. Thus, the first and second machines cannot send and receive network data to and from the other machines.","In the following detailed description of the invention, numerous details, examples, and embodiments of the invention are set forth and described. However, it will be clear and apparent to one skilled in the art that the invention is not limited to the embodiments set forth and that the invention may be practiced without some of the specific details and examples discussed.","I. Environment","The following section will describe the environment in which some embodiments of the inventions are implements. In the present application, switching elements and machines may be referred to as network elements. In addition, a network that is managed by one or more network controllers may be referred to as a managed network in the present application. In some embodiments, the managed network includes only managed switching elements (e.g., switching elements that are controlled by one or more network controllers) while, in other embodiments, the managed network includes managed switching elements as well as unmanaged switching elements (e.g., switching elements that are not controlled by a network controller).",{"@attributes":{"id":"p-0066","num":"0065"},"figref":"FIG. 1","b":["100","100","110","120","130","150","155","185"]},"In some embodiments, the managed switching elements - route network data (e.g., packets) between network elements in the network that are coupled to the managed switching elements -. For instance, the managed switching element  routes network data between the machines - and the managed switching element . Similarly, the managed switching element  routes network data between the machine  and the managed switching elements  and , and the managed switching element  routes network data between the machines - and the managed switching element .","The managed switching elements - of some embodiments can be configured to route network data according to defined rules. In some embodiments, the managed switching elements - routes network data based on routing criteria defined in the rules. Examples of routing criteria include source media access control (MAC) address, destination MAC, packet type, source Internet Protocol (IP) address, destination IP address, source port, destination port, and\/or virtual local area network (VLAN) identifier, among other routing criteria.","In some embodiments, the managed switching elements - can include standalone physical switching elements, software switching elements that operate within a computer, or any other type of switching element. For example, each of the managed switching elements - may be implemented as a hardware switching element, a software switching element, a virtual switching element, a network interface controller (NIC), or any other type of network element that can route network data. Moreover, the software or virtual switching elements may operate on a dedicated computer, or on a computer that performs non-switching operations.","The machines - send and receive network data between each other over the network. In some embodiments, the machines - are referred to as network hosts that are each assigned a network layer host addresses (e.g., IP address). Some embodiments refer to the machines - as end systems because the machines - are located at the edge of the network. In some embodiments, each of the machines - can be a desktop computer, a laptop computer, a smartphone, a virtual machine (VM) running on a computing device, a terminal, or any other type of network host.","In some embodiments, each of the network controllers  and  controls one or more managed switching elements - that are located at the edge of a network (e.g., edge switching elements or edge devices). In this example, the managed switching elements - are edge switching elements. That is, the managed switching elements - are switching elements that are located at or near the edge of the network. In some embodiments, an edge switching element is the last switching element before end machines (the machines - in this example) in a network. As indicated by dashed arrows in , the network controller  controls (i.e., manages) switching elements  and  and the network controller  controls switching element . In this application, a switching element that is controlled by a network controller of some embodiments may be referred to as a managed switching element.","In addition to controlling edge switching elements, the network controllers  and  of some embodiments also utilize and control non-edge switching elements (e.g., pool nodes, root nodes, and extenders, which are described in further detail below) that are inserted in the network to simplify and\/or facilitate the operation of the managed edge switching elements. For instance, in some embodiments, the network controller  and  require the switching elements that the network controller  and  control to be interconnected in a hierarchical switching architecture that has several edge switching elements as the leaf nodes in the hierarchical switching architecture and one or more non-edge switching elements as the non-leaf nodes in this architecture. In some such embodiments, each edge switching element connects to one or more of the non-leaf switching elements, and uses such non-leaf switching elements to facilitate the communication of the edge switching element with other edge switching elements. Examples of such communications with an edge switching elements in some embodiments include (1) routing of a packet with an unknown destination address (e.g., unknown MAC address) to the non-leaf switching element so that the non-leaf switching element can route the packet to the appropriate edge switching element, (2) routing a multicast or broadcast packet to the non-leaf switching element so that the non-leaf switching element can distribute the multicast or broadcast packet to the desired destinations.","Some embodiments employ one level of non-leaf (non-edge) switching elements that connect to edge switching elements and in some cases to other non-leaf switching elements. Other embodiments, on the other hand, employ multiple levels of non-leaf switching elements, with each level of non-leaf switching elements after the first level serving as a mechanism to facilitate communication between lower level non-leaf switching elements and leaf switching elements. In some embodiments, the non-leaf switching elements are software switching elements that are implemented by storing the switching tables in the memory of a standalone computer instead of an off the shelf switch. In some embodiments, the standalone computer may also be executing in some cases a hypervisor and one or more virtual machines on top of that hypervisor. Irrespective of the manner by which the leaf and non-leaf switching elements are implemented, the network controllers  and  of some embodiments store switching state information regarding the leaf and non-leaf switching elements.","As mentioned above, the switching elements - of some embodiments route network data between network elements in the network. In some embodiments, the network controllers  and  configure the managed switching elements -s' routing of network data between the network elements in the network. In this manner, the network controllers  and  can control the flow (i.e., specify the data path) of network data between network elements.","For example, the network controller  might instruct the managed switching elements  and  to route network data from the machine  to the machine  (and vice versa) and to not route (e.g., drop) network data from other machines to the machines  and . In such case, the network controller  controls the flow of network data through the managed switching elements  and  such that network data transmitted to and from the machine  is only routed to the machine . Thus, the machines  and  cannot send and receive network data to and from the machines , , and -.","In some embodiments, the network controllers  and  store physical network information and logical network information. The physical network information specifies the physical components in the managed network and how the physical components are physically connected one another in the managed network. For example, the physical network information may include the number of machines, managed switching elements, pool nodes, root nodes, and extenders (the latter three are described in further detail in the following sections), and how the components are physically connected to one another in the managed network. The logical network information may specify the logical connections between a set of physical components in the managed network (e.g., machines) and a mapping of the logical connections across the physical components of the managed network.","Some embodiments of the network controllers  and  implement a logical switching element across the managed switching elements - based on the physical network information and the logical switching element information described above. A logical switching element can be defined to function any number of different ways that a switching element might function. The network controllers  and  implement the defined logical switching element through control of the managed switching elements -. In some embodiments, the network controllers  and  implement multiple logical switching elements across the managed switching elements -. This allows multiple different logical switching elements to be implemented across the managed switching elements - without regard to the network topology of the network.","In some embodiments, a logical data path set defines a logical switching element. A logical data path set, in some embodiments, is a set of network data paths through the managed switching elements - that implement the logical switching element and the logical switch's defined functionalities. In these embodiments, the network controllers  and  translate (e.g., maps) the defined logical data path set into network configuration information for implementing the logical switching element. The network controllers  and  translate the defined logical data path set into a corresponding set of data flows (i.e., data paths) between network elements in the network, in some embodiments. In these instances, the network controllers  and  instruct the managed switching elements - to route network data according to the data flows and, thus, implement the functionalities of the defined logical switching element.","Different embodiments of the network controllers  and  are implemented differently. For example, some embodiments implement the network controllers  and  in software as instances of a software application. In these cases, the network controllers  and  may be executed on different types of computing devices, such as a desktop computer, a laptop computer, a smartphone, etc. In addition, the software application may be executed on a virtual machine that runs on a computing device in some embodiments. In some embodiments, the network controllers  and  are implemented in hardware (e.g., circuits).","As mentioned above by reference to , the managed switching elements controlled by network controllers of some embodiments may be physical switching elements.  illustrates an example of a network control system that includes physical switching elements. This figure conceptually illustrates a network control system  of some embodiments for managing physical switching elements. Specifically, the network control system  manages network data in a data center that includes top of the rack (TOR) switching elements - and racks of hosts -. Network controllers  and  manage the network by controlling the TOR switching elements -.","A TOR switching element, in some embodiments, routes network data between hosts in the TOR switch's rack and network elements coupled to the TOR switching element. In the example illustrated in , the TOR switching element  routes network data between the rack of hosts  and TOR switching elements  and , the TOR switching element  routes network data between the rack of hosts  and TOR switching elements  and , and the TOR switching element  routes network data between the rack of hosts  and TOR switching elements  and .","As shown, each rack of hosts - includes multiple hosts. The hosts of some embodiments in the racks of hosts - are physical computing devices. In some embodiments, each host is a computing device that is assigned a network layer host address (e.g., IP address). The hosts of some embodiments send and receive network data to and from each other over the network.","As mentioned above, the network controller of some embodiments can be implemented in software as an instance of an application. As illustrated in , the network controllers  and  are instances of a software application. As shown, each of the network controllers  and  includes several software layers: a control application layer, a virtualization application layer, and a networking operating system layer.","In some embodiments, the control application layer receives user input that specifies a network switching element. The control application layer may receive the user input in any number of different interfaces, such as a graphical user interface (GUI), a command line interfaces, a web-based interface, a touchscreen interface, etc. In some embodiments, the user input specifies characteristics and behaviors of the network switching element, such as the number of switching element ports, access control lists (ACLs), network data forwarding, port security, or any other network switching element configuration options.","The control application layer of some embodiments defines a logical data path set based on user input that specifies a network switching element. As noted above, a logical data path set is a set of network data paths through managed switching elements that are used to implement the user-specified network switching element. In other words, the logical data path set is a logical representation of the network switching element and the network switch's specified characteristics and behaviors.","Some embodiments of the virtualization application layer translate the defined logical data path set into network configuration information for implementing the logical network switching element across the managed switching elements in the network. For example, the virtualization application layer of some embodiments translates the defined logical data path set into a corresponding set of data flows. In some of these cases, the virtualization application layer may take into account various factors (e.g., logical switching elements that are currently implemented across the managed switching elements, the current network topology of the network, etc.), in determining the corresponding set of data flows.","The network operating system layer of some embodiments configures the managed switching elements' routing of network data. In some embodiments, the network operating system instructs the managed switching elements to route network data according to the set of data flows determined by the virtualization application layer.","In some embodiments, the network operating system layer maintains several views of the network based on the current network topology. One view that the network operating system layer maintains is a logical view. The logical view of the network includes the different logical switching elements that are implemented across the managed switching elements, in some embodiments. Some embodiments of the network operating system layer maintain a managed view of the network. Such managed views include the different managed switching elements in the network (i.e., the switching elements in the network that the network controllers control). In some embodiments, the network operating system layer also maintains relationship data that relate the logical switching elements implemented across the managed switching elements to the managed switching elements.","While  (and other figures in this application) may show a set of managed switching elements managed by a network controller, some embodiments provide several network controllers (also referred to as a cluster of network controllers or a control cluster) for managing the set of managed switching elements. In other embodiments, different control clusters may manage different sets of managed switching elements. Employing a cluster of network controllers in such embodiments to manage a set of managed switches increases the scalability of the managed network and increases the redundancy and reliability of the managed network. In some embodiments, the network controllers in a control cluster share (e.g., through the network operating system layer of the network controllers) data related to the state of the managed network in order to synchronize the network controllers.",{"@attributes":{"id":"p-0090","num":"0089"},"figref":"FIG. 3","b":["300","300","310","320","330","350","360","380"]},"The TOR switching elements - are similar to the TOR switching elements -. The TOR switching elements - route network data between network elements in the network that are coupled to the TOR switching elements -. In this example, the TOR switching element  routes network data between the rack of hosts  and TOR switching elements  and , the TOR switching element  routes network data between the rack of hosts  and TOR switching elements  and , and the TOR switching element  routes network data between the rack of hosts  and TOR switching elements  and . Since the TOR switching elements - are not managed switching elements, the network controllers  and  do not control these switching elements. Thus, the TOR switching elements - rely on the switching elements' preconfigured functionalities to route network data.","As illustrated in , each host in the racks of hosts - includes a software switching element (an open virtual switch (OVS) in this example) and several VMs. The VMs are virtual machines that are each assigned a set of network layer host addresses (e.g., a MAC address for network layer 2, an IP address for network layer 3, etc.) and can send and receive network data to and from other network elements over the network.","The OVSs of some embodiments route network traffic between network elements coupled to the OVSs. For example, in this example, each OVS routes network data between VMs that are running on the host on which the OVS is running, OVSs running on other hosts in the rack of hosts, and the TOR switching element of the rack.","By running a software switching element and several VMs on a host, the number of end machines or network hosts in the network may increase. Moreover, when a software switching element and several VMs are run on hosts in the racks of hosts -, the network topology of the network is changed. In particular, the TOR switching elements - are no longer edge switching elements. Instead, the edge switching elements in this example are the software switching elements running on the hosts since these software switching elements are the last switching elements before end machines (i.e., VMs in this example) in the network.","The network controllers  and  perform similar functions as the network controllers  and , which described above by reference to , and also are for managing edge switching elements. As such, the network controllers  and  manage the OVSs that are running on the hosts in the rack of hosts -.","The above  illustrate a network control systems for managing physical switching elements and a network control system for managing software switching elements, respectively. However, the network control system of some embodiments can manage both physical switching elements and software switching elements.  illustrates an example of such a network control system. In particular, this figure conceptually illustrates a network control system  of some embodiments for managing TOR switching element  and OVSs running on hosts in the racks of hosts  and .","The network controllers  and  perform similar functions as the network controllers  and , which described above by reference to , and also are for managing edge switching elements. In this example, the managed switching element  and the OVSs running on the hosts in the racks of hosts  and  are edge switching elements because they are the last switching elements before end machines in the network. In particular, the network controller  manages the TOR switching element  and the OVSs that are running on the hosts in the rack of hosts , and the network controller  manage the OVSs that are running on the hosts in the rack of hosts .","The above figures illustrate examples of network controllers that control edge switching elements in a network. However, in some embodiments, the network controllers can control non-edge switching elements as well.  illustrates a network control system that includes such network controllers. In particular,  conceptually illustrates a network control system  of some embodiments for managing TOR switching elements - and OVS running on hosts in the racks of hosts  and .","As shown in , the network controllers  and  manage edge switching elements and non-edge switching elements. Specifically, the network controller  manages the TOR switching elements  and , and the OVSs running on the hosts in the rack of hosts . The network controller  manages TOR switching element  and the OVSs running on the hosts in the rack of hosts . In this example, the TOR switching element  and the OVSs running on the hosts in the racks of hosts  and  are edge switching elements, and the TOR switching elements  and  are non-edge switching elements. The network controllers  and  perform similar functions as the network controllers  and , which are described above by reference to .","II. Network Constructs","The following section describes several network constructs. Different embodiments described in this application may utilize one or more of these network constructs to facilitate some or all of the functionalities of the different embodiments.",{"@attributes":{"id":"p-0101","num":"0100"},"figref":["FIG. 6","FIG. 6"],"b":["600","610","620","630","640","650","680","650","680","155","185"]},"The machines - of some embodiments are network hosts that are each assigned a set of network layer host addresses (e.g., a MAC address for network layer 2, an IP address for network layer 3, etc.). The machines - may also be referred to as end machines. Similar to the machines - described above, each of the machines - can be a desktop computer, a laptop computer, a smartphone, a virtual machine (VM) running on a computing device, a terminal, or any other type of network host. In addition, the machines - may belong to different tenants (e.g., in a data center environment). As illustrated in , each of the machines - belongs to either tenant A or tenant B.","The switching elements  and  are network switching elements that route (e.g., forwards) network data at the data link layer (also referred to as layer 2 or L2 layer) based on protocols such as the Ethernet protocol. The switching elements  and  may also be referred to as network bridges in some embodiments. As shown, the switching element  routes network data at the data link layer between the machines  and  and the router , and the switching element  routes network data at the data link layer between the machines  and  and the router .","To route network data at the data link layer, some embodiments of the switching elements  and  use a media access control (MAC) address of a network host's network interface card (NIC) to determine where to route network data (e.g., packets, frames, etc.). The switching elements  and  are implemented differently in different embodiments. For instance, each of the switching elements  and  can be implemented as a hardware switching element, a software switching element, a virtual switching element, some types of network interface card (NIC), or any other type of network element that can route network data at the data link layer.","Furthermore, the switching elements  and  support any number of different types of tunneling protocols in different embodiments. As shown, examples of tunneling protocols include control and provisioning of wireless access points (CAPWAP), generic route encapsulation (GRE), GRE Internet Protocol Security (IPsec), among other types of tunneling protocols.","The routers  and  are network routers that route network data at the network layer (also referred to as the layer 3 or L3 layer) based on protocols such as the Internet Protocol (IP). As illustrated in , the router  routes network data at the network layer between the router  and the switching element , and the router  routes network data at the network layer between the router  and the switching element .","In order to route network data at the network layer, the routers  and  of some embodiments use an IP address assigned to a network host to determine where to route network data (e.g., packets). Moreover, the routers  and  of some embodiments may provide other functions as well, such as security functions, quality of service (QoS) functions, checksum functions, flow accounting functions, or any other type of router functions.","Different embodiments of the routers  and  can be implemented differently. For example, each of the routers  and  can be implemented as a hardware router, a software router, a virtual router, or any other type of network element that can route network data at the network layer.","As mentioned above, the switching elements  and  of some embodiments can support tunneling protocols. In some embodiments, a tunneling protocol allows network data to be sent along a path between two points in a network where the tunneling protocol used by the network elements along the path in the network is different than the payload protocol used by the destination network element","In some embodiments, a tunneling protocol is a network protocol (e.g., a delivery protocol) that encapsulates another protocol (e.g., a payload protocol). A tunneling protocol can be used, for example, to transmit network data over an incompatible delivery-network. For instance, in this example, a tunneling protocol may provide a tunnel over a layer 3 network through which layer 2 network data is transmitted. As such, from the perspective of the machines -, the machines - are communicating over an L2 network. In other words, a tunneling protocol facilitates the communication of layer 2 network data between network hosts separated by a layer 3 network.",{"@attributes":{"id":"p-0111","num":"0110"},"figref":"FIG. 6","b":["690","630","640","690","695","690","650","680","690","695"]},"As shown in , a single tunnel  is established between the switching elements  and . However, in some embodiments multiple tunnels using the same or different tunneling protocols may be established between the switching elements  and . For example, the tunnel  shown in  is a bidirectional tunnel, as indicated by an arrow at each end of the tunnel . However, some embodiments may provide unidirectional tunnels. In such cases, a tunnel is established for each direction of communication between two points in the network. Referring to  as an example, when one of the machines  and  wishes to communicate with one of the machines  and , a tunnel is established that allows network data to be transmitted only from the switching element  to the switching element . Conversely, when one of the machines  and  wishes to communicate with one of the machines  and , a tunnel is established that allows network data to be transmitted from only the switching element  to the switching element .","Although  illustrates routers and switching elements as separate components, the functions described above for the router and switching elements may be performed by a single component in some embodiments. For instance, some embodiments combine the functions of the router  and the switching element  into one component and\/or combine the functions of the router  and the switching element  into another component.",{"@attributes":{"id":"p-0114","num":"0113"},"figref":["FIG. 7","FIG. 7"],"b":["770","700","710","720","730","760","730","760","155","185"]},"As illustrated in , the tunnel  is established between the switching element  and the switching element . For this example, the tunnel  is a unidirectional tunnel, as indicated by an arrow, that allows network data to be transmitted from the switching element  to the switching element . As described above, different tunneling protocols (e.g., CAPWAP, GRE, etc.) can be used to establish the tunnel  in different embodiments.","When transmitting network data through the tunnel , some embodiments include an identifier (ID) tag with the network data when the network data is transmitted through the tunnel . In some embodiments, an ID tag is a unique identifier for identifying a tenant to which the network data is associated. In this manner, switching elements can identify the tenant to which the network data belongs. This enables network data for different tenants to be transmitted through a single tunnel. In some embodiments, an ID tag allows machines of different tenants to have overlapping network identifiers (e.g., logical MAC addresses or logical IP addresses). For example, in a layer 2 network where some machines of different tenants each has the same MAC address, an ID tag can be used to differentiate between the machines of the different tenants and the network data directed at the different tenants. Similarly, an ID tag may be used to differentiate between machines of different tenants where some of the machines of the different tenants each has the same IP address.","The following will describe an example of transmitting network data belonging to different tenants that have overlapping network identifiers through a single tunnel by reference to . In this example, an ID tag \u201cID \u201d is associated with tenant A and an ID tag \u201cID \u201d is associated with tenant B. As such, the switching elements  and  are configured with this ID tag information (e.g., stored in a lookup table). In addition, tenant A's machines and tenant B's machines have overlapping network identifiers (e.g., they have the same MAC addresses or are use the same private IP address space).","When the machine  sends packet A to machine , the packet A is transmitted to the switching element . When the switching element  receives the packet A, the switching element  determines that the packet A originated from a machine that belongs to tenant A (e.g., based on the packet A's source MAC address and\/or the port through which the packet A is received). Then, the switching element  identifies the ID tag (e.g., by performing a lookup on a lookup table) that is associated with tenant A (ID  in this example) and includes the ID tag in the packet A before the packet is transmitted to the switching element  through the tunnel . Since tenant A's machine (machine ) and tenant B's machine (machine ) have overlapping network identifiers (e.g., the machine  and  each has the same MAC address or use the same private IP address space), the switching element  would not be able to differentiate between tenant A's machines and tenant B's machines based only on the machines' network identifiers. However, the ID tag allows the switching element  to differentiate between tenant A's machines and tenant B's machines. Therefore, when the switching element  receives the packet A from the switching element  through the tunnel , the switching element  examines the ID tag included in the packet A and determines the tenant to which the packet A belongs (e.g., by performing a lookup on a lookup table). After determining the tenant to which the packet A belongs, the switching element  removes the ID tag from the packet A and transmits to the packet A to the machine , the intended recipient of the packet A in this example.","When the machine  sends packet B to machine , the switching elements  and  perform similar functions as those performed for the packet A described above. That is, the switching element  determines the tenant to which the packet B belongs, identifies the ID tag associated with the tenant, and includes the ID tag in the packet B. Then, the switching element  transmits the packet B to the switching element  through the tunnel . When the switching element  receives the packet B from the switching element  through the tunnel , the switching element  determines the tenant to which the packet B belongs by examining the ID tag included in the packet, removes the ID tag from the packet B, and transmits the packet B to the machine . As explained, the ID tag allows network data for tenants A's machines and tenant B's machines, which have overlapping network identifiers, to be transmitted through a single tunnel .","As mentioned above, the managed switching elements of some embodiments can be configured to route network data based on different routing criteria. In this manner, the flow of network data through switching elements in a network can be controlled in order to implement multiple logical switching elements across the switching elements.",{"@attributes":{"id":"p-0121","num":"0120"},"figref":["FIG. 8","FIG. 8","FIG. 8"],"b":["870","880","810","830","800","810","830","840","865","840","865","155","185","840","850","860","845","855","865"]},"The switching elements - of some embodiments route network data (e.g., packets, frames, etc.) between network elements in the network that are coupled to the switching elements -. As shown, the switching element  routes network data between the machines  and  and the switching element . Similarly, the switching element  routes network data between the machine  and the switching elements  and , and the switching element  routes network data between the machines - and the switching element .","Moreover, each of the switching elements - routes network data based on the switch's forwarding tables. In some embodiments, a forwarding table determines where to route network data (e.g., a port on the switch) according to routing criteria. For instance, a forwarding table of a layer 2 switching element may determine where to route network data based on MAC addresses (e.g., source MAC address and\/or destination MAC address). As another example, a forwarding table of a layer 3 switching element may determine where to route network data based on IP addresses (e.g., source IP address and\/or destination IP address). Many other types of routing criteria are possible.","As shown in , the forwarding table in each of the switching elements - includes several records. In some embodiments, each of the records specifies operations for routing network data based on routing criteria. The records may be referred to as flow entries in some embodiments as the records control the \u201cflow\u201d of data through the switching elements -.",{"@attributes":{"id":"p-0125","num":"0124"},"figref":"FIG. 8","b":["880","885","840","850","860","890","895","845","855","865"]},"The following will describe the conceptual flow entries for implementing the flow of network data originating from the machine  and destined for the machine  and originating from the machine  and destined for the machine . First, the flow entries for routing network data originating from the machine  and destined for the machine  will be described followed by the flow entries for routing network data originating from the machine  and destined for the machine .","The flow entry \u201cA to A\u201d in the switching element 's forwarding table instructs the switching element  to route network data that originates from machine  and is destined for the machine  to the switching element . The flow entry \u201cA to A\u201d in the forwarding table of the switching element  instructs the switching element  to route network data that originates from machine  and is destined for the machine  to the machine . Therefore, when the machine  sends network data that is destined for the machine , the switching elements  and  route the network data along data path  based on the corresponding records in the switching elements' forwarding tables.","Furthermore, the flow entry \u201cA to A\u201d in the switching element 's forwarding table instructs the switching element  to route network data that originates from machine  and is destined for the machine  to the switching element . The flow entry \u201cA to A\u201d in the forwarding table of the switching element  instructs the switching element  to route network data that originates from machine  and is destined for the machine  to the switching element . The flow entry \u201cA to A\u201d in the forwarding table of the switching element  instructs the switching element  to route network data that originates from machine  and is destined for the machine  to the machine . Thus, when the machine  sends network data that is destined for the machine , the switching elements - route the network data along data path  based on the corresponding records in the switching elements' forwarding tables.","While conceptual flow entries for routing network data originating from the machine  and destined for the machine  and originating from the machine  and destined for the machine  are described above, similar flow entries would be included in the forwarding tables of the switching elements - for routing network data between other machines in tenant A's logical network . Moreover, similar flow entries would be included in the forwarding tables of the switching elements - for routing network data between the machines in tenant B's logical network .","In some embodiments, tunnels provided by tunneling protocols described above may be used to facilitate the implementation of the logical switching elements  and  across the switching elements -. The tunnels may be viewed as the \u201clogical wires\u201d that connect machines in the network in order to implement the logical switching elements  and . In some embodiments, unidirectional tunnels are used. For instance, a unidirectional tunnel between the switching element  and the switching element  may be established and through which network data originating from the machine  and destined for the machine  is transmitted. Similarly, a unidirectional tunnel between the switching element  and the switching element  may be established and through which network data originating from the machine  and destined for the machine  is transmitted. In some embodiments, a unidirectional tunnel is established for each direction of network data flow between two machines in the network.","Alternatively, or in conjunction with unidirectional tunnels, bidirectional tunnels can be used in some embodiments. For instance, in some of these embodiments, only one bidirectional tunnel is established between two switching elements. Referring to  as an example, a tunnel would be established between the switching elements  and , a tunnel would be established between the switching elements  and , and a tunnel would be established between the switching elements  and . In some embodiments, ID tags are utilized to distinguish between the network data of different tenants (e.g., tenants A and B in ), as described above by reference to .","Configuring the switching elements in the various ways described above to implement multiple logical switching elements across a set of switching elements allows multiple tenants, from the perspective of each tenant, to each have a separate network and\/or switching element while the tenants are in fact sharing some or all of the same set of switching elements and\/or connections between the set of switching elements (e.g., tunnels, physical wires).",{"@attributes":{"id":"p-0133","num":"0132"},"figref":"FIG. 9","b":["900","900","900","910","920","930","940"]},"The ingress ports  conceptually represent a set of ports through which the switching element  receives network data. The ingress ports  may include different amounts of ingress ports in different embodiments. As shown, the ingress ports  can receive network data that is external to the switching element , which is indicated as incoming packets in this example. The ingress ports  can also receive network data (e.g., packets) within the switching element  from the dispatch port . When the ingress ports  receives network data, the ingress ports  forwards the network data to the forwarding tables .","The forwarding tables  conceptually represent a set of forwarding tables for routing and modifying network data received from the ingress ports . In some embodiments, the forwarding tables  include a set of records (or rules) that instruct the switching element  to route and\/or modify network data and send the network data to the egress ports  and\/or the dispatch port  based on defined routing criteria. As noted above, examples of routing criteria include source media access control (MAC) address, destination MAC, packet type, source Internet Protocol (IP) address, destination IP address, source port, destination port, and\/or virtual local area network (VLAN) identifier, among other routing criteria. In some embodiments, the switching element  routes network data to a particular egress port according to the routing criteria.","The egress ports  conceptually represent a set of ports through which the switching element  sends network data out of the switching element . The egress ports  may include different amounts of egress ports in different embodiments. In some embodiments, some or all of the egress ports  may overlap with some or all of the ingress ports . For instance, in some such embodiments, the set of ports of the egress ports  is the same set of ports as the set of ports of ingress ports . As illustrated in , the egress ports  receives network data after the switching element  processes the network data based on the forwarding tables . When the egress ports  receive network data (e.g., packets), the switching element  sends the network data out of the egress ports , which is indicated as outgoing packets in this example, based on the routing criteria in the forwarding tables .","In some embodiments, the dispatch port  allows packets to be reprocessed by the forwarding tables . In some cases, the forwarding tables  are implemented as a single table (e.g., due to the switching element s hardware and\/or software limitations). However, some embodiments of the forwarding tables  may logically need more than one table. Therefore, in order to implement multiple forwarding tables in a single table, the dispatch port  may be used. For example, when the forwarding tables  processes a packet, the packet may be tagged (e.g., modifying a context tag of the packet or a header field of the packet) and sent to the dispatch port  for the forwarding tables  to process again. Based on the tag, the forwarding tables  processes the packet using a different set of records. So logically, a different forwarding table is processing the packet.","The dispatch port  receives after the switching element  processes the network data according to the forwarding tables . As noted above, the switching element  might route the network data to the dispatch port  according to routing criteria defined the forwarding tables . When the dispatch port  receives network data, the dispatch port  sends the network data to the ingress ports  to be further processed by the forwarding tables . For example, the switching element  might modify the network data based on the forwarding tables  and send the modified network data to the dispatch port  for further processing by the forwarding tables .",{"@attributes":{"id":"p-0139","num":"0138"},"figref":"FIG. 10","b":["1000","1000","1010","1020","1030","1040","1050","1060","1070","1080","1090"]},"The ingress ports  are similar to the ingress ports  illustrated in  except the ingress ports  sends network data to the packet processor  instead of forwarding tables. The egress ports  are similar to the ingress ports  illustrated in  except the egress ports  receive network data from the packet processor  instead of forwarding tables. Similarly, the dispatch port  is similar to the dispatch port  of  except the dispatch port  receives network data from the packet processor  instead of forwarding tables.","The management processor  controls the operations and functions of the switching element . As shown in , the management processor  of some embodiments receives commands for controlling the switching element  through a switching control protocol. One example of a switching control protocol is the Openflow protocol. The Openflow protocol, in some embodiments, is a communication protocol for controlling the forwarding plane (e.g., forwarding tables) of a switching element. For instance, the Openflow protocol provides commands for adding flow entries to, removing flow entries from, and modifying flow entries in the switching element .","The management processor  also receives configuration information through a configuration protocol. When the management processor  receives configuration information, the management processor  sends the configuration information to the configuration database  for the configuration database  to store. In some embodiments, configuration information includes information for configuring the switching element , such as information for configuring ingress ports, egress ports, QoS configurations for ports, etc.","When the management processor  of some embodiments receives switching control commands and the configuration commands, the management processor  translates such commands into equivalent commands for configuring the switching element  to implement the functionalities of the commands. For instance, when the management processor  receives a command to add a flow entry, the management processor  translates the flow entry into equivalent commands that configure the switching element  to perform functions equivalent to the flow entry. In some embodiments, the management processor  might request configuration information from the configuration database  in order to perform translation operations.","Some embodiments of the management processor  are implemented as electronic circuitry while other embodiments of the management processor  are implemented as an embedded central processing unit (CPU) that executes switching element management software (e.g., OVS) that performs some or all of the functions described above.","The configuration database  of some embodiments stores configuration information that the configuration database  receives from the management processor . In addition, when the management processor  sends requests for configuration information to the configuration database , the configuration database  retrieves the appropriate configuration information and sends the requested configuration information to the management processor .","In some embodiments, the control plane  stores a set of flow tables that each includes a set of flow entries (also referred to collectively as configured flow entries). The control plane  of some embodiments receives flow entries from the management processor  to add to the set of flow tables, and receives requests from the management processor  to remove and modify flow entries in the set of flow tables. In addition, some embodiments of the control plane  might receive requests from the management processor  for flow tables and\/or flow entries. In such instances, the control plane  retrieves the requested flow tables and\/or flow entries and sends the flow tables and\/or flow entries to the management processor .","In addition, the control plane  of some embodiments stores different flow tables and\/or flow entries that serve different purposes. For instance, as mentioned above, a switching element may be one of several switching elements in a network across which multiple logical switching elements are implemented. In some such embodiments, the control plane  stores flow tables and\/or flow entries for operating in the physical domain (i.e., physical context) and stores flow tables and\/or flow entries for operating in the logical domain (i.e., logical context). In other words, the control plane  of these embodiments stores flow tables and\/or flow entries for processing network data (e.g., packets) through logical switching elements and flow tables and\/or flow entries for processing network the data through physical switching elements in order to implement the logical switching elements. In this manner, the control plane  allows the switching element  to facilitate implementing logical switching elements across the switching element  (and other switching elements in the managed network).","In some embodiments, the flow tables and\/or flow entries for operating in the physical domain process packets based on a set of fields in the packets' header (e.g., source MAC address, destination MAC address, source IP address, destination IP address, source port number, destination port number) and the flow tables and\/or flow entries for operating in the logical domain process packets based on the packets logical context ID (e.g., as described above by reference to ) or a logical context tag (e.g., as described below by reference to , , , , and ).","Some embodiments of the communication interface  facilitate communication between management processor  and packet processor . For instance, when the communication interface  receives messages (e.g., commands) from the management processor , the communication interface  forwards the messages to the packet processor  and when the communication interface  receives messages from the packet processor , the communication interface  forwards the messages to the management processor . In some embodiments, the communication interface  translates the messages such that the recipient of the message can understand the message before sending the message to the recipient. The communication interface  can be implemented as a peripheral component interconnect (PCI) or PCI express bus in some embodiments. However, the communication interface  may be implemented as other types of busses in other embodiments.","In some embodiments, the forwarding tables  store active flow tables and\/or flow entries that are used to determine operations for routing or modifying network data (e.g., packets). In some embodiments, active tables and\/or flow entries are a subset of the flow tables and\/or entries stored in the control plane  that the forwarding tables  is currently using or was recently using to process and route network data.","In this example, each flow entry is includes a qualifier and an action. The qualifier defines a set of fields to match against the network data. Examples of fields for matching network data include ingress port, source MAC address, destination MAC address, Ethernet type, VLAN ID, VLAN priority, multiprotocol label switching (MPLS) label, MPLS traffic class, source IP address, destination IP address, transport control protocol (TCP)\/user datagram protocol (UDP)\/stream control transmission protocol (SCTP) source port, and\/or TCP\/UDP\/SCTP destination port. Other types of packet header fields are possible as well in other embodiments. The action of a flow entry defines operations for processing the network data when the network data matches the qualifier of the flow entry. Examples of actions include modify the network data and route the network data to a particular port or ports. Other embodiments provide additional and\/or other actions to apply to the network data.","In some embodiments, the packet processor  processes network data (e.g., packets) that the packet processor  receives from the ingress ports . Specifically, the packet processor  processes (e.g., route, modify, etc.) the network data based on flow entries in the forwarding tables . In order to process the network data, the packet processor  accesses the flow entries in the forwarding tables . As mentioned above, the forwarding tables  includes a subset of flow tables and\/or flow entries stored in the control plane . When the packet processor  needs a flow table and\/or flow entries that is not in the forwarding tables , the packet processor  requests the desired flow table and\/or flow entries, which are stored in the control plane , from the management processor  through the communication interface .","Based on the flow entries in the forwarding tables , the packet processor  sends the network data to one or more ports of the egress ports  or the dispatch port . In some embodiments, the network data may match multiple flow entries in the forwarding tables . In such cases, the packet processor  might process the network data based on the first flow entry that has a qualifier that matches the network data.","In some embodiments, the packet processor  is an application-specific integrated circuit (ASIC) that performs some or all of the functions described above. In other embodiments, the packet processor  is an embedded CPU that executes packet processing software that performs some or all of the functions described above.","Different embodiments of the switching element  may implement the packet processor  and forwarding tables  differently. For instance, in some embodiments, the packet processor  and forwarding tables  are implemented as a multi-stage processing pipeline. In these embodiments, each flow entry in the forwarding tables  are implemented as one or more operations along one or more stages of the multi-stage packet processing pipeline. As explained above, the management processor  of some embodiments translates flow entries into equivalent commands that configure the switching element  to perform functions equivalent to the flow entry. Accordingly, the management processor  would configure the multi-stage packet processing pipeline to perform the functions equivalent to the flow entries in the forwarding tables.",{"@attributes":{"id":"p-0156","num":"0155"},"figref":["FIG. 11","FIG. 11","FIGS. 3-5","FIG. 11"],"b":["1100","1110","1100","1110","1120","1135","1100","1100","1100","1100","1140","1145","1150","1110","1155","1120","1135"]},"In some embodiments, the physical ports  of the physical host  are a set of network interface controllers (NICs) that are for receiving network data and sending network data outside the physical host . In some embodiments, the physical ports  are a set of wireless NICs. The physical ports  of other embodiments are a combination of NICs and wireless NICs.","The hypervisor  (also referred to as a virtual machine monitor (VMM)) of some embodiments is a virtualization application that manages multiple operating systems (e.g., VMs) on the physical host . That is, the hypervisor  provides a virtualization layer in which other operating systems can run with the appearance of full access to the underlying system hardware (not shown) of the physical host  except such access is actually under the control of the hypervisor . In this example, the hypervisor  manages the VMs - running on the physical host .","In some embodiments, the hypervisor  manages system resources, such as memory, processors (or processing units), persistent storage, or any other type of system resource, for each of the operating systems that the hypervisor  manages. For this example, the hypervisor  manages the physical ports , the network resources of the physical host . In particular, the hypervisor  manages and controls network data flowing through the physical ports  and the patch ports  by, for example, mapping each port of the patch ports  to a corresponding port of the physical ports .","Different embodiments use different hypervisors. In some embodiments, the hypervisor  is a Xen hypervisor is used while, in other embodiments, the hypervisor  is a VMware hypervisor. Other hypervisors can be used in other embodiments.","The patch ports  are a set of virtual ports (e.g., virtual network interfaces (VIFs)). To the software switching element  and the hypervisor , the patch ports  appear and behave similar to physical ports on a hardware switching element. For instance, the software switching element  and the hypervisor  may send and receive network data through the patch ports . In some embodiments, the patch ports  are provided by the hypervisor  to the software switching element  while, in other embodiments, the patch ports  are provided by the software switching element  to the hypervisor .","The patch ports  are a set of virtual ports that are similar to the patch ports . That is, to the software switching element  and the VMs -, the patch ports  appear and behave similar to physical ports on a hardware switching element. As such, the software switching element  and the VMs - may send and receive network data through the patch ports . In some embodiments, the patch ports  are provided by the software switching element  to the VMs - while, in other embodiments, the patch ports  are provided by the VMs - to the software switching element .","As shown, the software switching element  includes a control plane , a configuration database , a forwarding plane , and forwarding tables . The control plane  of some embodiments is similar to the control plane  of  in that the control plane  also stores configured flow entries (i.e., a set of flow tables that each includes a set of flow entries). Also, the configuration database  is similar to the configuration database  of . That is, the configuration database  stores configuration information for configuring the software switching element . (e.g., information for configuring ingress ports, egress ports, QoS configurations for ports, etc.)","In some embodiments, the forwarding plane  and the forwarding tables  performs functions similar to ones performed by packet processor  and the forwarding tables  described above by reference to . The forwarding plane  of some embodiments processes network data (e.g., packets) that the forwarding plane  receives from the patch ports  and the patch ports . In some embodiments, the forwarding plane  processes the network data by accessing the flow entries in the forwarding tables . When the forwarding plane  needs a flow table and\/or flow entries that is not in the forwarding tables , the forwarding plane  of some embodiments requests the desired flow table and\/or flow entries from the control plane .","Based on the flow entries in the forwarding tables , the forwarding plane  sends the network data to one or more ports of the patch ports  and\/or one or more ports of the patch ports . In some embodiments, the network data may match multiple flow entries in the forwarding tables . In these instances, the forwarding plane  might process the network data based on the first flow entry that has a qualifier that matches the network data.",{"@attributes":{"id":"p-0166","num":"0165"},"figref":["FIG. 12","FIG. 12","FIG. 12","FIGS. 3-5"],"b":["1200","1245","1285","1265","1267","1285","1245","1265","1267","1285","1245","1265","1267","1200"]},"As illustrated in , the host  includes hardware , hypervisor , and VMs -. The hardware  may include typical computer hardware, such as processing units, volatile memory (e.g., random access memory (RAM)), non-volatile memory (e.g., hard disc drives, optical discs, etc.), network adapters, video adapters, or any other type of computer hardware. As shown, the hardware  includes NICs  and , which are typical network interface controllers for connecting a computing device to a network.","The hypervisor  is a software abstraction layer that runs on top of the hardware  and runs below any operation system. The hypervisor  handles various management tasks, such as memory management, processor scheduling, or any other operations for controlling the execution of the VMs -. Moreover, the hypervisor  communicates with the VM  to achieve various operations (e.g., setting priorities). In some embodiments, the hypervisor  is a Xen hypervisor while, in other embodiments, the hypervisor  may be any other type of hypervisor for providing hardware virtualization of the hardware  on the host .","As shown, the hypervisor  includes device drivers  and  for the NICs  and , respectively. The device drivers  and  allow an operating system to interact with the hardware of the host . In this example, the device driver  allows the VM  to interact with the NIC . And the device driver  allows the VM  to interact with the NIC . The hypervisor  may include other device drivers (not shown) for allowing the VM  to interact with other hardware (not shown) in the host .","VMs - are virtual machines running on the hypervisor . As such, the VMs - run any number of different operating systems. Examples of such operations systems include Solaris, FreeBSD, or any other type of Unix-based operating system. Other examples include Windows-based operating systems as well.","In some embodiments, the VM  is a unique virtual machine, which includes a modified Linux kernel, running on the hypervisor . In such cases, the VM  may be referred to as domain  or dom in some embodiments. The VM  of such embodiments is responsible for managing and controlling other VMs running on the hypervisor  (e.g., VMs  and ). For instance, the VM  may have special rights to access the hardware  of the host . In such embodiments, other VMs running on the hypervisor  interact with the VM  in order to access the hardware . In addition, the VM  may be responsible for starting and stopping VMs on the hypervisor . The VM  may perform other functions for managing and controlling the VMs running on the hypervisor .","Some embodiments of the VM  may include several daemons (e.g., Linux daemons) for supporting the management and control of other VMs running on the hypervisor . Since the VM  of some embodiments is manages and controls other VMs running on the hypervisor , the VM  may be required to run on the hypervisor  before any other VM is run on the hypervisor .","As shown in , the VM  includes a kernel and a user space. In some embodiments, the kernel is the most basic component of an operating system that runs on a separate memory space and is responsible for managing system resources (e.g., communication between hardware and software resources). In contrast, the user space is a memory space where all user mode applications may run.","As shown, the user space of the VM  includes the OVS daemon  and the OVS DB daemon . Other applications (not shown) may be included in the user space of the VM  as well. The OVS daemon  is an application that runs in the background of the user space of the VM . Some embodiments of the OVS daemon  communicate with a network controller  in order to process and route packets that the VM  receives. For example, the OVS daemon  receives commands from the network controller  regarding operations for processing and routing packets that the VM  receives. The OVS daemon  communicates with the network controller  through the Openflow protocol. In some embodiments, another type of communication protocol is used. Additionally, some embodiments of the OVS daemon  receives configuration information from the OVS DB daemon  to facilitate the processing and routing of packets.","In some embodiments, the OVS DB daemon  is also an application that runs in the background of the user space of the VM . The OVS DB daemon  of some embodiments communicates with the network controller  in order to configure the OVS switching element (e.g., the OVS daemon  and\/or the OVS kernel module ). For instance, the OVS DB daemon  receives configuration information from the network controller  for configuring ingress ports, egress ports, QoS configurations for ports, etc., and stores the configuration information in a set of databases. In some embodiments, the OVS DB daemon  communicates with the network controller  through a database communication protocol (e.g., a JavaScript Object Notation (JSON) remote procedure call (RPC)-based protocol). In some embodiments, another type of communication protocol is utilized. In some cases, the OVS DB daemon  may receive requests for configuration information from the OVS daemon . The OVS DB daemon , in these cases, retrieves the requested configuration information (e.g., from a set of databases) and sends the configuration information to the OVS daemon .","The network controller  is similar to the various network controllers described in this application, such as the ones described by reference to . That is, the network controller  manages and controls the software switching element running on the VM  of the host .",{"@attributes":{"id":"p-0177","num":"0176"},"figref":"FIG. 12","b":["1265","1270","1275","1270","1280","1270","1280","1270","1280","1270","1275","1270","1265"]},"The flow processor  manages the rules for processing and routing packets. For instance, the flow processor  stores rules (e.g., in a storage medium, such as a disc drive) that the flow processor  receives from the Openflow protocol module  (which, in some cases, the Openflow protocol module  receives from the network controller ). In some embodiments, the rules are stored as a set of flow tables that each includes a set of flow entries (also referred to collectively as configured flow entries). As noted above, flow entries specify operations for processing and\/or routing network data (e.g., packets) based on routing criteria. In addition, when the flow processor  receives commands from the Openflow protocol module  to remove rules, the flow processor  removes the rules.","In some embodiments, the flow processor  supports different types of rules. For example, the flow processor  of such embodiments supports wildcard rules and exact match rules. In some embodiments, an exact match rule is defined to match against every possible field of a particular set of protocol stacks. A wildcard rule is defined to match against a subset of the possible fields of the particular set of protocol stacks. As such, different exact match rules and wildcard rules may be defined for different set of protocol stacks.","The flow processor  handles packets for which integration bridge  does not have a matching rule. For example, the flow processor  receives packets from the integration bridge  that does not match any of the rules stored in the integration bridge . In such cases, the flow processor  matches the packets against the rules stored in the flow processor , which include wildcard rules as well as exact match rules. When a packet matches an exact match rule or a wildcard rule, the flow processor  sends the exact match rule or the wildcard rule and the packet to the integration bridge  for the integration bridge  to process.","In some embodiment, when a packet matches a wildcard rule, the flow processor  generates an exact match rule based on the wildcard rule to which the packet matches. As mentioned above, a rule, in some embodiments, specifies an action to perform based on a qualifier. As such, in some embodiments, the generated exact match rule includes the corresponding action specified in the wildcard rule from which the exact match rule is generated.","In other embodiment, when a packet matches a wildcard rule, the flow processor  generates a wildcard rule that is more specific than the wildcard rule to which the packet matches. Thus, in some embodiments, the generated (and more specific) wildcard rule includes the corresponding action specified in the wildcard rule from which the exact match rule is generated.","In some embodiments, the flow processor  may not have a rule to which the packet matches. In such cases, some embodiments of the flow process  send the packet to the network controller  (through the Openflow protocol module ). However, in other cases, the flow processor  may have received from the network controller  a catchall rule that drops the packet when a rule to which the packet matches does not exist in the flow processor .","After the flow processor  generates the exact match rule based on the wildcard rule to which the packet originally matched, the flow processor  sends the generated exact match rule and the packet to the integration bridge  for the integration bridge  to process. This way, when the integration bridge  receives a similar packet that matches generated the exact match rule, the packet will be matched against the generated exact match rule in the integration bridge  so the flow processor  does not have to process the packet.","Some embodiments of the flow processor  support rule priorities for specifying the priority for a rule with respect to other rules. For example, when the flow processor  matches a packet against the rules stored in the flow processor , the packet may match more than one rule. In these cases, rule priorities may be used to specify which rule among the rules to which the packet matches that is to be used to match the packet.","The flow processor  of some embodiments is also responsible for managing rules in the integration bridge . As explained in further detail below, the integration bridge  of some embodiments stores only active rules. In these embodiments, the flow processor  monitors the rules stored in the integration bridge  and removes the active rules that have not been access for a defined amount of time (e.g., 1 second, 3 seconds, 5, seconds, 10 seconds, etc.). In this manner, the flow processor  manages the integration bridge  so that the integration bridge  stores rules that are being used or have recently been used.","Although  illustrates one integration bridge, the OVS kernel module  may include multiple integration bridges. For instance, in some embodiments, the OVS kernel module  includes an integration bridge for each logical switching element that is implemented across a managed network to which the software switching element belongs. That is, the OVS kernel module  has a corresponding integration bridge for each logical switching element that is implemented across the managed network.","As illustrated in , the kernel includes a hypervisor network stack  and an OVS kernel module . The hypervisor network stack  is an Internet Protocol (IP) network stack that runs on the VM . The hypervisor network stack  processes and routes IP packets that are received from the OVS kernel module  and the PIF bridges  and . When processing a packet that is destined for a network host external to the host , the hypervisor network stack  determines to which of physical interface (PIF) bridges  and  the packet is to be sent. The hypervisor network stack  may make such determination by examining the destination IP address of the packet and a set of routing tables (not shown). In some embodiments, the hypervisor network stack  is provided by the hypervisor .","The OVS kernel module  processes and routes network data (e.g., packets) between VMs running on the host  and network hosts external to the host  (i.e., network data received through the NICs  and ). For example, the OVS kernel module  of some embodiments routes packets between VMs running on the host  and network hosts external to the host  (e.g., when packets are not routed through a tunnel) through a set of patch ports (not shown) that couple the OVS kernel module  to the PIF bridges  and . In several of the figures in this application (e.g., ), forwarding tables are illustrated as part of a forwarding plane of a software switching element. However, the forwarding tables may be conceptual representations and may be implemented by the OVS kernel module , in some embodiments.","To facilitate the processing and routing of network data, the OVS kernel module  communicates with OVS daemon . For example, the OVS kernel module  receives processing and routing information (e.g., flow entries) from the OVS daemon  that specifies how the OVS kernel module  is to process and route packets when the OVS kernel module  receives packets. Some embodiments of the OVS kernel module  include a bridge interface (not shown) that allows the hypervisor network stack  to send packets to and receiving packets from the OVS kernel module . In other embodiments, the hypervisor  sends packets to and receives packets from the bridges included in OVS kernel module  (e.g., integration bridge  and\/or PIF bridges  and ).",{"@attributes":{"id":"p-0191","num":"0190"},"figref":"FIG. 12","b":["1245","1250","1255","1260","1250","1240","1290","1295","1255","1260","1250","1255","1260","1250","1240","1250"]},"In some embodiments, the set of rules that the integration bridge  stores are only exact match rules. The integration bridge  of some such embodiments stores only active exact match rules, which are a subset of the rules stored in the flow processor  (and\/or rules derived from rules stored in the flow processor ) that the integration bridge  is currently using or was recently using to process and route packets. The integration bridge  of some embodiments stores a set of rules (e.g., flow entries) for performing mapping lookups and logical forwarding lookups, such as the ones described below in further detail by reference to , , , , and . Some embodiments of the integration bridge  may also perform standard layer 2 packet learning and routing.","In some embodiments, the OVS kernel module  includes a PIF bridge for each NIC in the hardware . For instance, if the hardware  includes four NICs, the OVS kernel module  would include four PIF bridges for each of the four NICs in the hardware . In other embodiments, a PIF bridge in the OVS kernel module  may interact with more than one NIC in the hardware .","The PIF bridges  and  route network data between the hypervisor network stack  and network hosts external to the host  (i.e., network data received through the NICs  and ). As shown, the PIF bridge  routes network data between the hypervisor network stack  and the NIC  and the PIF bridge  routes network data between the hypervisor network stack  and the NIC . The PIF bridges  and  of some embodiments perform standard layer 2 packet learning and routing. In some embodiments, the PIF bridges  and  performs physical lookups\/mapping, such as the ones described below in further detail by reference to , , , and .","In some embodiments, the VM  provides and controls the PIF bridges  and . However, the network controller  may, in some embodiments, control the PIF bridges  and  (via the OVS daemon ) in order to implement various functionalities (e.g., quality of service (QoS)) of the software switching element.","In several of the figures in this application (e.g., ), forwarding tables are illustrated as part of a forwarding plane of a software switching element. However, these forwarding tables may be, in some embodiments, conceptual representations that can be implemented by the OVS kernel module . Also, some of the figures in this application (e.g., , , and ) illustrate a control plane in a switching element. These control planes may similarly be conceptual representations, which can be implemented by the OVS daemon , in some embodiments.","The architectural diagram of the software switching element and the host illustrated in  is one exemplary configuration. One of ordinary skill in the art will recognize that other configurations are possible. For instance, some embodiments may include several integration bridges in the OVS kernel module, additional NICs and corresponding PIF bridges, and additional VMs.","The following will describe an exemplary operation of the OVS switching element illustrated in  according to some embodiments of the invention. Specifically, a packet processing operation performed by the OVS switching element will be described. As described above, the OVS kernel module  processes packets and routes packets. The OVS kernel module  can receive packets in different ways. For instance, the OVS kernel module  can receive a packet from the VM  or the VM  through the VM's VIF. In particular, the OVS kernel module  receives the packet from the VM  or the VM  at the integration bridge .","Furthermore, the OVS kernel module  can receive a packet from a network host external to the host  through one of the NICs  and , the NIC's corresponding PIF bridge (i.e., PIF bridge  or PIF bridge ), and the hypervisor network stack . The hypervisor network stack  then sends the packets to the integration bridge  of the OVS kernel bridge . In some cases, the packet is received from a network host external to the host  through a tunnel. In some embodiments, the tunnel terminates at the hypervisor network stack . Thus, when the hypervisor network stack  receives the packet through the tunnel, the hypervisor network stack  unwraps (i.e., decapsulates) the tunnel header and determines, based on the tunnel information (e.g., tunnel ID), which integration bridge of the OVS kernel module  to which to send the unwrapped packet. As mentioned above, the OVS kernel module  of some embodiments may include an integration bridge for each logical switching element that is implemented across the managed network to which the OVS switching element belongs. Accordingly, the hypervisor network stack  determines the logical switching element to which the tunnel belongs, identifies the integration bridge that corresponds to the determined logical switching element, and sends the packet to the identified integration bridge.","In addition, the OVS kernel module  can receive a packet from a network host external to the host  through one of the NICs  and , the NIC's corresponding PIF bridge (i.e., PIF bridge  or PIF bridge ), and a set of patch ports (not shown) that couple the PIF bridge to the OVS kernel module . As noted above, the OVS kernel module  of some embodiments may include an integration bridge for each logical switching element that is implemented across the managed network to which the OVS switching element belongs. Accordingly, the NIC's corresponding PIF bridge determines the logical switching element to which the tunnel belongs, identifies the integration bridge that corresponds to the determined logical switching element, and sends the packet to the identified integration bridge.","When the integration bridge  receives a packet in any of the manners described above, the integration bridge  processes the packet and routes the packet. As noted above, some embodiments of the integration bridge  stores only active exact match rules, which are a subset of the rules stored in the flow processor  (and\/or rules derived from rules stored in the flow processor ) that the integration bridge  is currently using or was recently using to process and route packets. The integration bridge  performs a lookup based on a set of fields in the packet's header (e.g., by applying a hash function to the set of fields). In some embodiments, the set of fields may include a field for storing metadata that describes the packet. If the lookup returns a rule to which the packet matches, the integration bridge  performs the action (e.g., forward the packet, drop the packet, reprocess the packet, etc.) specified in the rule. However, if the lookup does not return a rule, the integration bridge  sends the packet to the flow processor  to process.","As explained above, the flow processor  handles packets for which the integration bridge  does not have a matching rule. When the flow processor  receives the packet from the integration bridge , the flow processor  matches the packet against the rules stored in the flow processor , which include wildcard rules as well as exact match rules. When a packet matches an exact match rule, the flow processor  sends the exact match rule and the packet to the integration bridge  for the integration bridge  to process. When a packet matches a wildcard rule, the flow processor  generates an exact match rule based on the wildcard rule to which the packet matches, and sends the generated exact match rule and the packet to the integration bridge  for the integration bridge  to process.","Although  illustrates the VM  as a virtual machine, different embodiments may implement the VM  differently. For example, some embodiments may implement the VM  as part of the hypervisor . In such embodiments, the VM  performs the same or similar functions as those described above with respect to the VM .",{"@attributes":{"id":"p-0204","num":"0203"},"figref":["FIG. 13","FIG. 13"],"b":["1300","1320","1310","1320","1300","1320","1300"]},"The network controller  is similar to the network controllers described above by reference to  except the network controller  communicates with the switching element  through a database connection and an Openflow connection. In some embodiments, a JavaScript Object Notation (JSON) remote procedure call (RPC)-based protocol is used to establish the database connection and to communicate (e.g., updating databases) through the database connection. In other embodiments, any of the many known database connection and communication methods (e.g., Java DataBase Connectivity (JDBC) or Open Database Connectivity (ODBC)) may be used. The Openflow connection uses the Openflow protocol to establish a connection and facilitate communication.","In some embodiments, the switching element  is a software switching element (e.g., the OVS switching element illustrated in ) while, in other embodiments, the switching element  is a hardware switching elements (e.g., the switching element illustrated in ). Therefore, even for a hardware switching element, OVS is executed on the hardware switching element. For example, referring to , which illustrates a hardware switching element, some embodiments of the management processor  are implemented as an embedded central processing unit (CPU) that executes switching element management software. In this example, the switching element management software is OVS.","As shown, the switching element  includes a user space daemon  and a forwarding plane . The user space daemon  includes an OVS connection manager , a configuration database controller , a configuration database , a control plane controller , and a control plane . The OVS connection manager  manages the connection between the network controller  and the configuration database controller , and the connection between the network controller  and the control plane controller  so that communications received over a particular connection is routed to the appropriate controller.","In some embodiments, the OVS connection manager  translates the commands and\/or messages into a format that the recipient can understand. For example, when the network controller  sends a command to the switching element  through the database connection, the OVS connection manager  may translate the command so that the configuration database controller  can understand the command. Similarly, when the network controller  sends a command to the switching element  through the Openflow connection, the OVS connection manager  may translate the command so that the control plane controller  can understand the command.","The configuration database controller  of some embodiments manages the configuration database  and receives commands from the OVS connection manager  related to the configuration database . Examples of commands include create a table, delete a table, create a record in a table, modify (i.e., update) a record in a table, delete a record in a table, among other types of database commands. When the configuration database controller  receives a command from the OVS connection manager , the configuration database controller  performs the corresponding action to the configuration database .","The configuration database  is similar to the configuration database , which is described above by reference to . That is, the configuration database  stores configuration information for configuring the switching element . (e.g., information for configuring ingress ports, egress ports, QoS configurations for ports, etc.).","Some embodiments of the control plane controller  manage the Openflow rules stored in the control plane  and receives commands from the OVS connection manager  related to the control plane . Examples of commands include add a rule, modify (i.e., update) a rule, delete a rule, or other types of Openflow commands. When the configuration database controller  receives a command from the OVS connection manager , the configuration database controller  performs the command's corresponding action to the configuration database .","The control plane  is similar to the control plane , which is described above by reference to . Thus, the control plane  stores configured flow entries that are, in some embodiments, a set of flow tables that each includes a set of flow entries. In some of these embodiments, the control plane  also stores flow tables and\/or flow entries for operating in the physical domain (i.e., physical context) and stores flow tables and\/or flow entries for operating in the logical domain (i.e., logical context) in order to implement logical switching elements. In addition, the control plane  receives flow entries from the network controller  (through the OVS connection manager  and the control plane controller ) to add to the configured flow entries, and receives requests from the network controller  (through the OVS connection manager  and the control plane controller ) to remove and modify the configured flow entries. The control plane  may manage the flow entries stored in the forwarding plane  in a similar manner that the flow processor  manages rules in the integration bridge . For example, the control plane  monitors the flow entries stored in the forwarding plane  and removes the flow entries that have not been access for a defined amount of time (e.g., 1 second, 3 seconds, 5, seconds, 10 seconds, etc.) so that the control plane  stores flow entries that are being used or have recently been used.","The forwarding plane  is similar to the forwarding plane described above by reference to . That is, the forwarding plane  processes and routes network data (e.g., packets). In some embodiments, the forwarding plane  stores only active rules (e.g., flow entries) that specify operations for processing and routing packets. In some embodiments, the forwarding plane  sends packets to the control plane  that the forwarding plane  cannot process (e.g., the forwarding plane  does not have a flow entry that matches the packets). As mentioned above, the switching element  of some embodiments is a software switching element. In these embodiments, the forwarding plane  is implemented as a software forwarding plane, such as the software forwarding planes described above by reference to . Similarly, in some embodiments where the switching element  is a hardware switching elements, the forwarding plane  is implemented, for example, as the hardware forwarding plane described above by reference to .",{"@attributes":{"id":"p-0214","num":"0213"},"figref":"FIG. 14","b":["1400","1400","1410","1440","1400"]},"In some embodiments, a packet includes a header and a payload. The header includes, in some embodiments, a set of fields that contains information used for routing the packet through a network. Switching elements may determine switching decisions based on the contained in the header and may, in some cases, modify some or all of the header fields. As explained above, some embodiments determine switching decisions based on flow entries in the switching elements' forwarding tables.","In some embodiments, the processing pipeline  may be implemented by flow entries in the managed switching elements in the network. For instance, some or all of the flow entries are defined such that the packet is processed against the flow entries based on the logical context tag in the packet's header. Therefore, in some of these embodiments, the managed switching elements are configured (e.g., by a network controller illustrated in ) with such flow entries.","In the first stage  of the processing pipeline , a logical context lookup is performed on a packet to determine the logical context of the packet. In some embodiments, the first stage  is performed when the logical switching element receives the packet (e.g., the packet is initially received by a managed switching element in the network that implements the logical switching element).","In some embodiments, a logical context represents the state of the packet with respect to the logical switching element. For example, some embodiments of the logical context may specify the logical switching element to which the packet belongs, the logical port of the logical switching element through which the packet was received, the logical port of the logical switching element through which the packet is to be transmitted, the stage of the logical forwarding plane of the logical switching element the packet is at, etc. Referring to  as an example, the logical context of some embodiments for packets sent from tenant A's machines specify that the packets are to be processed according to the logical switching element , which is defined for tenant A (rather than the logical switching element , which is defined for tenant B).","Some embodiments determine the logical context of a packet based on the source MAC address of the packet (i.e., the machine from which the packet was sent). Some embodiments perform the logical context lookup based on the source MAC address of the packet and the inport (i.e., ingress port) of the packet (i.e., the port of the managed switching element through which the packet was received). Other embodiments may use other fields in the packet's header (e.g., MPLS header, VLAN id, etc.) for determining the logical context of the packet.","After the logical context of the packet is determined, some embodiments store the information that represents the determined logical context in one or more fields of the packet's header. These fields may also be referred to as a logical context tag or a logical context ID. Furthermore, the logical context tag may coincide with one or more known header fields (e.g., the VLAN id field) in some embodiments. As such, these embodiments do not utilize the known header field or its accompanying features in the manner that the header field is defined to be used.","In the second stage  of the processing pipeline , logical forwarding lookups are performed on the packets to determine where to route the packet based on the logical switching element (e.g., the logical port of the logical switching element of which to send the packet out) through which the packet is being processed. In some embodiment, the logical forwarding lookups include a logical ingress ACL lookup for determining access control when the logical switching element receives the packet, a logical L2 lookup for determining where to route the packet through a layer 2 network, and a logical egress ACL lookup for determining access control before the logical switching element routes the packet out of the logical switching element. Alternatively, or in conjunction with the logical L2 lookup, some embodiments of the logical forwarding lookups include a logical L3 lookup for determining where to route the packet through a layer three network. These logical lookups are performed based on the logical context tag of the packet in some of these embodiments.","In some embodiments, the result of the logical forwarding lookups may include dropping the packet, forwarding the packet to one or more logical egress ports of the logical switching element, or forwarding the packet to a dispatch port of the logical switching element. When the logical forwarding lookups determines that the packet is to be routed to the dispatch port of the logical switching element, some embodiments repeat the logical forwarding lookups until the packet is determined to be either dropped or forwarded to one or more logical egress ports.","Next, the third stage  of the processing pipeline  performs a mapping lookup on the packet. In some embodiments, the mapping lookup is a logical to physical mapping lookup that determines the logical egress port of the logical switching element. That is, the mapping lookup determines one or more ports of one or more managed switching elements that correspond to the logical egress port of the logical switching element through which the packet is to be sent out. For instance, if the packet is a broadcast packet or a multicast packet, the third stage  of some embodiments determines the ports of the managed switching elements that correspond to the logical egress ports of the logical switching element through which the packet is to be broadcasted or multicasted out (i.e., the logical ports to which the intended recipients of the packet is coupled). If the packet is a unicast packet, the third stage  determines a port of a managed switching element that corresponds to the logical egress port of the logical switching element through which the packet is to be sent out (i.e., the logical port to which the intended recipient of the packet is coupled). In some embodiments of the third stage , the mapping lookups are performed based on the logical context tag of the packet.","At the fourth stage  of the processing pipeline , a physical lookup is performed. The physical lookup of some embodiments determines operations for routing the packet to the physical port(s) that corresponds to the logical egress port(s) that was determined in the third stage . For example, the physical lookup of some embodiments determines one or more ports of the managed switching element on which the processing pipeline  is being performed through which to send the packet out in order for the packet to reach the physical port(s) determined in the third stage . This way, the managed switching elements can route the packet along the correct path in the network for the packet to reach the determined physical port(s) that corresponds to the logical egress port(s).","Some embodiments remove the logical context tag after the fourth stage  is completed in order to return the packet to its original state before the packet was processed by the processing pipeline .","As mentioned above, in some embodiments, the processing pipeline  is performed by each managed switching element in the managed network that is used to implement the logical switching element. In some embodiments, some of the managed switching elements perform only a portion of the processing pipeline . For example, in some embodiments, the managed switching element that initially receives the packet may perform the first-fourth stages - and the remaining managed switching elements that subsequently receive the packet only perform the first, third, and fourth stages , , and .",{"@attributes":{"id":"p-0227","num":"0226"},"figref":"FIG. 15","b":["1500","1400","1500"]},"The process  begins by determining (at ) whether the packet has a logical context tag. When the process  determines that the packet does not have a logical context tag, the process  determines (at ) whether the packet matches a flow entry that specifies a logical context. In some embodiments, the process  determines the packet's logical context in a similar fashion as that described above by reference to the first stage  of . That is, the process  determines the logical context of the packet based on a defined set of fields in the packet's header (e.g., the source MAC address, inport, etc.).","When the process  determines that the packet does not match a flow entry that specifies a logical context, the process  drops (at ) the packet and the process  then ends. When the process  determines that the packet matches a flow entry that specifies a logical context, the process  adds (at ) a logical context tag to the header of the packet. After the process  adds the logical context tag to the header of the packet, the process  proceeds to . When the process  determines that the packet does have a logical context tag, the process  proceeds to .","At , the process  determines whether the packet matches a flow entry that specifies the packet's logical context tag to be modified. In some embodiments, the flow entries that the process  matches the packet against are flow entries that implement the logical ingress ACL lookup described above by reference to the second stage  of . When the process  determines that the packet matches a flow entry that specifies the packet's logical context tag to be modified, the process  modifies (at ) the packet according to the flow entry against which the packet matches. Then, the process  proceeds to . When the process  determines that the packet does not match a flow entry that specifies the packet's logical context tag to be modified, the process  proceeds to .","Next, the process  determines (at ) whether the packet matches a flow entry that specifies the packet to be dropped. In some embodiments, the flow entries that the process  matches the packet against are flow entries that implement the logical L2 lookup described above by reference to the second stage  of . When the process  determines that the packet matches a flow entry that specifies the packet to be dropped, the process  drops (at ) the packet and the process  ends.","When the process  determines that the packet does not match a flow entry that specifies the packet to be dropped, the process  determines (at ) whether the packet matches a flow entry that specifies the destination of the packet is local. In some embodiments, the destination of the packet is local when the recipient of the packet is coupled to the managed switching element on which the process  is being performed. When the process  determines that the packet matches a flow entry that specifies the destination of the packet is local, the process  removes (at ) the logical context tag from the packet's header. Next, the process  forwards (at ) the packet to the local destination. In some embodiments, the process  determines the local destination by matching the packet against flow entries that implement the logical L2 lookup described above by reference to the second stage  of . After forwarding the packet to the local destination, the process  ends.","When the process  determines that the packet does not match a flow entry that specifies the destination of the packet is local, the process  forwards (at ) the packet to the next managed switching element for further processing. Then, the process  ends.","III. Hierarchical Switching Architecture",{"@attributes":{"id":"p-0234","num":"0233"},"figref":["FIG. 16","FIG. 1","FIG. 16"],"b":["1600","1605","1600","100","1600","1605","130","140","110","120","155","160","170","175","165","180","185"]},"In some embodiments, the pool node  is a switching element (e.g., a hardware switching element or an OVS) that is coupled to and positioned above the managed switching elements - in the hierarchy of the network architecture  to assist in the implementation of logical switching elements across the managed switching elements -. The following will describe some of the functions that some embodiments of the pool node  provide.","The pool node  of some embodiments is responsible for processing packets that the managed switching elements - cannot process. In instances where one of the managed switching elements - cannot process a packet, the managed switching element sends the packet to the pool node  to process. For instance, the pool nodes  processes packets with destination MAC addresses that are not known to one of the managed switching elements - (e.g., the managed switching element does not have a flow entry that matches the destination MAC address). In some cases, one of the managed switching elements - cannot process a packet due to the limited storage capacity of the managed switching element and does not include flow entries for processing the packet. Another example where the managed switching elements - cannot process a packet is because the packet is destined for a remote network that may not be managed by the network controllers  and .","In some embodiments, the pool node  serves as a communication bridge between managed switching elements. Referring to  as an example, absent the pool node , the managed switching element  cannot communicate with the managed switching elements  and . Therefore, when the managed switching element  wants to send packets, for example, to the managed switching element  or the managed switching element , the managed switching element  sends the packets to the pool node  to forward to the managed switching element  or the managed switching element . Similarly, when the managed switching element  or the managed switching element  wants to send packets to the managed switching element , the managed switching element  or the managed switching element  sends the packets to the pool node  to forward to the managed switching element .","Some embodiments of the pool node  process packets are that are intended for multiple recipients (e.g., broadcast packets and multicast packets) in the same logical network. For instance, when one of the managed switching elements - receives a broadcast or multicast packet from one of the machines, the managed switching element sends the broadcast or multicast packet to the pool node  for processing. Referring to  as an example, when the managed switching element  receives a broadcast from the machine , the managed switching element  sends the broadcast packet to the pool node . The pool node  determines that the broadcast is destined for the machines on tenant A's logical network. Accordingly, the pool node  determines that the machines , , , and  belong to tenant A and sends the packet to each of those machines. The pool node  processes multicast packets in a similar manner except, for the multicast packet, the pool node  identifies the intended recipients of the multicast packet.","As explained above, the pool node  of some embodiments processes packets that are intended for multiple recipients in the same logical network.  conceptually illustrates an example multi-recipient packet flow through the network architecture  illustrated in  according to some embodiments of the invention. Specifically,  conceptually illustrates a managed switching element performing the replication of packets for the multi-recipient packet.","In this example, tenant B's machine  sends a multi-recipient packet (e.g., a broadcast packet or a multicast packet) to the managed switching element . In some embodiments, the multi-recipient packet specifies a destination MAC address that is defined (e.g., by a network controller managing) to indicate the packet is a multi-recipient packet. Some embodiments might indicate that the packet is a multi-recipient packet through data stored in a set of fields (e.g., a context tag) in the packet's header. The managed switching element  identifies the packet as a multi-recipient packet based on the defined destination MAC address and\/or the set of header fields. Since the pool node  is responsible for processing multi-recipient packets, the managed switching element  forwards the packet to the pool node  for processing.","When the pool node  receives the packet from the managed switching element , the pool node  determines that the packet is a multi-recipient packet by examining the destination MAC address of the packet and\/or the set of header fields. In some embodiments, the packet also specifies the logical network to which the packet belongs (e.g., via a context tag). In this example, the packet specifies that the packet belongs to the logical network that includes tenant B's machines (machines , , and  in this example). After the pool node  determines that logical network to which the packet belongs, the pool node  determines the managed switching elements to which to route the multi-recipient packet. Since the managed switching element  is not coupled to any of tenant B's machines, the pool node  only forwards the multi-recipient packet to the managed switching element .","When the managed switching element  receives the packet, the managed switching element  determines that the packet is a multi-recipient packet by examining the destination MAC address of the packet. The managed switching element  then determines the logical network to which the packet belongs and identifies the machines coupled to the managed switching element  that belong to the logical network to which the packet belongs. For this example, the packet belongs to tenant B's logical network. Therefore, the managed switching element  identifies the machines  and  as the machines coupled to the managed switching element  that belong to tenant B's logical network. Then, the managed switching element  replicates the multi-recipient packet for each identified machine, modifies each replicated packet to specify the MAC address of the corresponding machine as the packet's destination MAC address, and sends the replicated packets to the machines.","As shown,  illustrates a packet flow of a multi-recipient packet through a network architecture of some embodiments where a managed switching element performs the replication of packets for the multi-recipient packet. However, in some embodiments, the pool node of some embodiments may perform the replication of packets for a multi-recipient packet.  conceptually illustrates such an example multi-recipient packet flow through the network architecture  illustrated in  according to some embodiments of the invention.","For this example, tenant A's machine  sends a multi-recipient packet (e.g., a broadcast packet or a multicast packet) to the managed switching element  that specifies tenant A's machine  and  as recipients of the packet. In some embodiments, the multi-recipient packet specifies a destination MAC address that is defined (e.g., by a network controller managing) to indicate the packet is a multi-recipient packet and the recipients of the multi-recipient packet. Some embodiments might indicate that the packet is a multi-recipient packet through data stored in a set of fields (e.g., a context tag) in the packet's header. The managed switching element  identifies the packet as a multi-recipient packet based on the defined destination MAC address and\/or the set of header fields. As the pool node  is responsible for processing multi-recipient packets, the managed switching element  forwards the packet to the pool node  for processing.","When the pool node  receives the packet from the managed switching element , the pool node  determines that the packet is a multi-recipient packet by examining the destination MAC address of the packet and\/or the set of header fields. In some embodiments, the packet also specifies the logical network to which the packet belongs (e.g., via a context tag). In this example, the packet specifies that the packet belongs to the logical network that includes tenant A's machines (machines , , , and  in this example). After the pool node  determines the logical network to which the packet belongs, the pool node  identifies the set of managed switching elements (the managed switching element  in this example) to which the intended recipients of the multi-recipient packet (the machines  and  in this example) are coupled. The pool node  then replicates the multi-recipient packet and sends a copy of the multi-recipient packet to each of the identified set of managed switching elements.","The above description by reference to  describes packets that are sent from a managed switching element to a pool node and from a pool node to a managed switching element. In some embodiments, the packets are sent through tunnels in a similar manner that is described above by reference to .",{"@attributes":{"id":"p-0247","num":"0246"},"figref":["FIG. 19","FIGS. 1-5","FIGS. 1-5"],"b":["1605","130","150","130","150","1605","130","150"]},"As shown, the managed switching element  includes a forwarding table  and the managed switching element  includes a forwarding table . As noted above, the managed switching elements of some embodiments may have limited storage capacity and cannot store all the necessary flow entries to process the different packets in the network. In this example, the managed switching element  can only store  flow entries (i.e., 9 flow entries for each of the machines -) and the managed switching element  can only store  flow entries (i.e., 7 flow entries for each of the machines -). The flow entries in each of the forwarding tables  and  conceptually represent the packets that the managed switching elements  and  can process.","As described above, the pool node  processes packets that the managed switching elements  and  cannot process (e.g., unknown destination MAC address, broadcast and multicast packets, etc.). As shown, the pool node  includes a forwarding table  with m+n flow entries. The flow entries in the forwarding table  conceptually represent flow entries for processing packets that the managed switching elements  and  cannot process.","In some embodiments, a pool node includes all the flow entries that are used to manage the network. For instance, referring to  as an example, the pool node  of such embodiments would include the flow entries in the forwarding tables  and  in addition to the flow entries shown in the forwarding table . Moreover, a pool node of some embodiments includes information (e.g., MAC addresses) related to every machine in the managed network. In some such embodiments, the pool node would include flow entries for forwarding network data from every machine in the managed network to each other. In cases where a managed network includes multiple pool nodes, some embodiments configure each pool node similarly while other embodiments may configure one or more pool nodes differently.","Although  shows forwarding tables with the same number of flow entries for each machine stored in a forwarding table of the managed switching elements and pool node, this figure illustrates an exemplary configuration of the managed switching elements and the pool node. One of ordinary skill will recognize that the managed switching elements and the pool node may include multiple forwarding tables with a different number of flow entries for each of the different machines.",{"@attributes":{"id":"p-0252","num":"0251"},"figref":["FIG. 20","FIG. 14"],"b":["2000","2000","2000","1420","1400"]},"The process  starts by determining (at ) whether the packet has an unknown destination MAC address. In some embodiments, the destination MAC address of the packet is unknown when the managed switching element that is performing the process  does not have a flow entry that matches the packet's destination MAC address. When the process  determines that the packet does not have an unknown destination MAC address, the process  proceeds to . Otherwise, the process  forwards (at ) the packet to a pool node and then the process  ends.","Next, the process  determines (at ) whether the packet can be processed. In some embodiments, the packet can be processed when the managed switching element on which the process  is being performed has a flow entry that matches the packet. When the process  determines that the packet cannot be processed, the process  forwards (at ) the packet to a pool node and then the process  ends.","When the process  determines that the packet can be processed, the process  processes (at ) the packet. The process  of some embodiments processes the packet by performing the action specified in the flow entry that matches the packet. After processing the packet, the process  proceeds to .","At , the process  determines whether the packet is a multicast or broadcast packet. Some embodiments define a multicast or broadcast packet as a packet with defined values in a set of header fields (e.g., destination MAC address, inport, etc.). When the process  determines that the packet is not a multicast or broadcast packet, the process  ends. Otherwise, the process  determines (at ) whether the packet needs further processing. A packet may need further processing when the packet is a multicast or broadcast packet and one or more of the recipients of the multicast or broadcast packet are unknown (e.g., the recipients are not coupled to the managed switching element that is performing the process ).","When the process  determines that the packet needs further processing, the process  forwards (at ) the packet to a pool node and then the process  ends. When the process  determines that the packet does not need further processing, the process  ends.","In some embodiments, some or all of the operations in the process  is implemented by flow entries in the managed switching element on which the process  is performed. For instance, the managed switching element may include a set of flow entries that define a broadcast or multicast packet in some such embodiments. In such cases, the managed switching element performs a lookup on the set of flow entries to determine whether a packet is a broadcast or multicast packet (i.e., whether the packet matches against the set of flow entries).",{"@attributes":{"id":"p-0259","num":"0258"},"figref":["FIG. 21","FIG. 21"],"b":["2100","2105","2110","2100","2105","2110","2115","2130","2135","2170"]},"In some embodiments, a root node is similar to a pool node in that the root node is a switching element (e.g., a hardware switching element or an OVS) that is for assisting in the implementation of logical switching elements across managed switching elements. However, the root node provides different functions than a pool node and is positioned at a different level in the network hierarchy. The following will describe some functions that the root node of some embodiments provides.","Some embodiments of the root nodes  and  provide a communication bridge between zones in the network. In some embodiments, a zone is a defined group of machines in a network. A zone may be defined any number of different ways in different embodiments. For instance, a zone may be defined as a group of machines in an office, a group of machines in a section of a data center, a group of machines in a building. As shown, zone  of the network architecture includes the pool nodes  and  and the managed switching elements - and the zone  of the network architecture includes the pool nodes  and  and the managed switching elements -.","As shown in , the network elements in zone  of the network cannot communicate with the network elements in zone  of the network. When a network element in one of the zones wants to communicate with a network element in the other zone, such communications are forwarded to the corresponding root node in the zone. For instance, if the managed switching element  wants to send a packet to the managed switching element , the managed switching element  sends the packets to the pool node , which sends the packet to the root node . The root node  of zone  then forwards the packet to the root node  of zone  to forward to the managed switching element  through the pool node .","In some embodiments, the root nodes  and  perform logical context learning. Logical context learning, in some embodiments, is a process of identifying the network element(s) to which packets are forwarded so that the packets can reach the packets' intended destination. Referring to  as an example, if the root node  receives from the pool node  a packet from a new machine (e.g., the packet includes an unknown source MAC address or IP address) that has recently been connected to the managed switching element , the root node  \u201clearns\u201d that the root node  should forward packets destined for the new machine to the root node  (as opposed to forwarding the packets to the pool node  or the root node ). By performing logical context learning, the root nodes  and  of some embodiments is indirectly aware of the location of all the network elements in the network and can thus forward packets to the correct network element in order for packets to reach their intended destinations. Thus, when the pool nodes - do not know or cannot determine the logical context of a packet, the packet is sent to the corresponding root node in the pool node's zone for processing (e.g., to forward to the packet's intended destination).","As described above,  shows root nodes as separate components at the top of a network architecture hierarchy. However, in some embodiments, a similar network architecture may be implemented with pool nodes, which include some or all of the functions described above by reference to the root nodes in , in place of root nodes at the top of the network architecture hierarchy. In other embodiments, the some or all of the root node functions are implemented by each of the pool nodes. In addition, while  illustrates one level of pool nodes in the hierarchy of a network architecture, different embodiments of different network architectures may include different numbers of levels of pool nodes in the hierarchy of the network architecture as well as any number pool nodes at each level in the hierarchy of the network architecture.",{"@attributes":{"id":"p-0265","num":"0264"},"figref":["FIG. 22","FIG. 22"],"b":["2210","2230","2210","2210"]},"As shown, the pool node  includes pool node network stack , the root bridge , patch bridge , and a set of NICs . In some embodiments, each NIC in the set of NICs  is typical network interface controllers for connecting a computing device to one or more networks and sending and receiving network data (e.g., packets) over such networks. In addition, the set of NICs  sends and receives network data from the pool node network stack .","The pool node network stack  is similar to the hypervisor network stack described above by reference to . The pool node network stack  is an IP network stack that runs on the pool node . Also, the pool node network stack  processes and routes IP packets that are received from the patch bridge  and the set of NICs , by utilizing a set of routing tables (not shown) to route the packets.","In some embodiments, the patch bridge  stores a set of rules (e.g., flow entries) that specify operations for processing and routing packets. The patch bridge  communicates with a network controller  in order to process and route packets that the patch bridge  receives. For instance, the patch bridge  receives commands from the network controller  related to processing and routing of packets that the pool node  receives. In some embodiments, the patch bridge  communicates with the network controller  through the Openflow protocol while, in other embodiments, another type of communication protocol may be used. The network controller  is similar to the various network controllers described in this application, such as the ones described by reference to . The network controller  manages and controls the switching element (OVS in this example) that is running on the pool node .","As explained above, a pool node of some embodiments is responsible for processing packets that managed switching elements in a managed network cannot process. In this example, the patch bridge  processes and routes such packets. The patch bridge  receives packets from managed switching elements through the set of NICs  and the pool node network stack . When the patch bridge  receives a packet, the patch bridge  processes and routes the packet according to the set of rules stored in the patch bridge . In some cases, the patch bridge  cannot process a packet (e.g., the patch bridge  does not have a rule to which the packet matches). In these cases, the patch bridge  sends the packet to the root bridge  for processing.","Some embodiments of the root bridge  are responsible for a learning function. The root bridge  of some embodiments stores a set of tables of learned MAC addresses (unlike the pool nodes and managed switches of some embodiments, which are controlled by a network controller). The root bridge  learns MAC addresses in the typical manner that layer 2 switches learn MAC addresses. For instance, when the root bridge  does not know a MAC address (i.e., a destination MAC address of a packet is not included in the set of tables of learned MAC addresses), the root bridge  floods all of the ports of the root bridge  and records the MAC address of the packet that responds to the flood in the set of tables. As another example, when the root bride  receives a packet that includes a destination MAC address that the root bridge  does not know (i.e., the destination MAC address of the packet is not included in the set of tables of learned MAC addresses), the root bridge  records the source MAC address of the packet in the set of tables of learned MAC addresses. When the root bridge  knows the MAC address of a packet (i.e., the MAC address is included in the set of tables of learned MAC addresses), the root bridge  sends the packet to the patch bridge  to forward to the appropriate NIC in the set of NICs  in order for the packet to reach the packet's destination. In some embodiments, the root bridge  and the patch bridge  communicate through a set of patch ports, which are for connecting two bridges directly together. In some embodiments, the root bridge  may be directly connected to one or more extenders. In some of these embodiments, a tunnel is established between the root bridge  and each of the extenders in order for the root bridge  and the extenders to communicate.","Although  illustrates a pool node that includes a root bridge, some embodiments may not include a root bridge. In some of these embodiments, the functions described above are implemented in the patch bridge of the pool node.",{"@attributes":{"id":"p-0272","num":"0271"},"figref":"FIG. 23","b":["2300","2305","2310","2300","2305","2376","2320","2335","2340","2352","2362","2376","2320","2335","2340","2352","2362","2310","2378","2325","2330","2345","2350","2364","2374","2305","2310","2378","2325","2330","2345","2350","2364","2374"]},"In some embodiments, an extender is a switching element (e.g., a hardware switching element or an OVS) for communicatively bridging remote managed networks that are separated by one or more other networks. As shown in , the San Diego zone and the Chicago zone are separated by external network . To allow communication between the two zones, the extender , which is physically located in the Chicago datacenter, and the extender  provide a communication bridge between the San Diego zone and the Chicago zone. In this example, the communication bridge between the two zones is partially provided by a tunnel, which is established using any of the tunneling protocols described above by reference to , between the extender  and the root node . In addition, the tunnel in  is a secure tunnel that is secured using Internet Protocol Security (IPsec) since communications are sent between the two zones through the external network , which may be unsecure.","The above  describes extenders that are used to bridge managed networks that are separately by an external network. However, the extenders of some embodiments can be used to bridge a managed network with an unmanaged network. An unmanaged network is a network that is not managed by a network controller, in some embodiments. The following  conceptually illustrates an example of extenders used for such a purpose.",{"@attributes":{"id":"p-0275","num":"0274"},"figref":["FIG. 24","FIG. 21","FIG. 16","FIG. 12"],"b":["2400","2415","2420","2425","2430","2455","2415","2420","2425","2430","2455"]},"The unmanaged network zone includes an extender , switching elements -, and multiple end hosts. One of ordinary skill in the art will realize that the unmanaged network zone may include any number of different networks and end hosts, as indicated by dashed lines in . In some embodiments, the extender  in the unmanaged network zone is configured before deploying the extender in the unmanaged network zone. For example, some embodiments require an IP address of a network controller (or a network controller of a control cluster) that is will be controlling the extender  to be specified (e.g., through a command line interface provided by the extender ).","Since the network elements (e.g., switching elements -) in the unmanaged network zone are not used to implement logical switching elements (i.e., not controlled by a network controller), the network elements in the unmanaged network zone will not recognize logical context tags defined for the managed network. Accordingly, some embodiments of the extenders  and  remove the logical context tag from packets before sending the packets to the network elements of the unmanaged network zone. In some embodiments, the extender  removes the logical context tag from packets to be forwarded to the extender  while, in other embodiments, the extender  removes the logical context tag from packets that the extender  receives from the extender  and that are to be forwarded to network elements in the unmanaged network zone.","Conversely, some embodiments of the extenders  and  add logical context tags to packets that are received from network elements in the unmanaged network zone and destined for the managed network zone. For instance, the extender  of some embodiments may add a logical context tag to a packet that the extender  receives from one of the network elements (e.g., switching elements -). The logical context tag may, in some embodiments, indicate that the packet belongs to a generic logical context representing packets that originate from an unmanaged network that are destined for the managed network zone. In some embodiments, the extender  adds the logical context tag to the packet when the extender  receives the packets from network elements in the unmanaged network zone while, in other embodiments, the extender  adds the logical context tag to the packet when the extender  receives the packets from the extender .",{"@attributes":{"id":"p-0279","num":"0278"},"figref":["FIG. 25","FIG. 25"],"b":"2500"},"As illustrated in , the managed network zone includes a root node , a pool node , managed switching elements  and , and machines -. These network elements may be implemented by different embodiments of corresponding network elements that are described in this application. For instance, the root node  may be implemented by the root nodes described above by reference to , the pool node  may be implemented by the pool nodes described above by reference to , the managed switching elements  and  may be implemented by the switching element described above by reference to , and the machines may be implemented by the machines describe above by reference to .","The unmanaged network zone includes an extender , switching elements -, and multiple machines. One of ordinary skill in the will realize that the unmanaged network zone may include any number of different networks and end hosts, as indicated by dashed lines. In addition,  illustrates that the managed network zone and the unmanaged network are coupled to each other through network . Specifically, the root node  of the managed network zone and the extender  of the unmanaged network zone are coupled to each other through the network . The network  may be a layer 2 network (e.g., a local area network (LAN)) in some embodiments while the network  may be a layer 3 network.","In some embodiments, the extender  in the unmanaged network zone is configured before deploying the extender in the unmanaged network zone. For example, some embodiments require an IP address of a network controller (or a network controller of a control cluster) that is will be controlling the extender  to be specified (e.g., through a command line interface provided by the extender ).","Because the network elements (e.g., switching elements -) in the unmanaged network zone are not used to implement logical switching elements (i.e., not controlled by a network controller), the network elements in the unmanaged network zone will not recognize logical context tags defined for the managed network. Therefore, some embodiments of the extender  removes the logical context tag from packets before sending the packets to the network elements of the unmanaged network zone through the network . In addition, the extender  of some embodiments adds logical context tags to packets that are received from network elements in the unmanaged network zone and destined for the managed network. For instance, the extender  of some embodiments may add a logical context tag to a packet that the extender  receives from one of the network elements (e.g., switching elements -). The logical context tag may, in some embodiments, indicate that the packet belongs to a generic logical context representing packets that originate from an unmanaged network. In some embodiments, the extender  adds the logical context tag to the packet when the extender  receives the packets from network elements in the unmanaged network zone that are destined for the managed network zone.","Although  shows a managed network zone coupled to an unmanaged network through a root node in the managed network zone and an extender in the unmanaged network zone, some embodiments may utilize an extender in the managed network zone to couple the managed network zone to the unmanaged network, similar to the managed network zone illustrated in . Furthermore,  illustrates the use of an extender to facilitate the implementation of a logical switching element that logically connects one tenant's machines that are spread across a managed network zone and an unmanaged network zone. However, the extender may utilized to facilitate the implementation of different logical switching elements that logically connects different tenant's machines that are spread across a managed network zone and an unmanaged network zone.",{"@attributes":{"id":"p-0285","num":"0284"},"figref":["FIG. 26","FIG. 26"],"b":"2630"},"In some embodiments, the extender  receives packet from the managed network zone that includes a logical context tag. Referring to  as an example, packet A includes a logical context tag, as indicated by an \u201cID\u201d in the packet's header. When the extender  receives the packet A, the extender  removes the logical context tag from the packet A. As shown, when the extender  sends the packet A to the unmanaged network zone, the packet A no longer has the \u201cID\u201d logical context tag.","The extender  of some embodiments maps packets from the unmanaged network zone to the managed network zone. In some of these embodiments, the extender  identifies a logical context for the packets and adds a logical context tag that represents the identified logical context. Referring to  as an example, when packet B is sent to the extender , the packet B does not have a logical context tag. When the extender  receives the packet B, the extender  identifies a logical context for the packet B (e.g., by matching the packet B against flow entries) and adds a logical context tag that represents the identified logical context of the packet B. As noted above, the logical context tag may, in some embodiments, indicate that the packet B belongs to a generic logical context representing packets that originate from an unmanaged network. Then, the extender  sends the packet B to the managed network zone.","While  illustrates mapping of logical context tags between managed networks and unmanaged networks by an extender, some embodiments implement such functionality in a different network element. For instance, a root node to which the extender is connected may perform logical context tag mapping between managed networks and unmanaged networks, in some embodiments.",{"@attributes":{"id":"p-0289","num":"0288"},"figref":["FIG. 27","FIG. 12"],"b":["2785","2785","1285","2785","2785"]},"The extender  essentially functions similar to the VM , as explained above. Thus, NICs  and  function similar to the NICs  and , extender network stack  functions similar to the hypervisor network stack , PIF bridges  and  function similar to the PIF bridges  and , integration bridge  functions similar to the integration bridge , flow processor  functions similar to the flow processor , and Openflow protocol module  functions similar to the Openflow protocol module . However, the extender  of some embodiments serves different purposes in a managed network, as noted above, and, thus, may be configured differently by a network controller of the managed network.",{"@attributes":{"id":"p-0291","num":"0290"},"figref":"FIG. 28","b":["2800","2805","2810","2800","2805","2810","2815","2825","2830","2860","2815","2825","2830","2860","2830","2840","2815","2845","2820","2850","2860","2825"]},"As described above, a software switching element may be an OVS that runs on a physical host in some embodiments. In this example, the software switching elements - are OVSs that each runs a physical host. On the right side of , a block diagram of the software switching element  and the physical host on which the software switching element  runs is shown. The physical host includes physical ports , hypervisor , patch ports , OVS , patch ports , and the VMs -. The physical ports , hypervisor , patch ports , OVS , patch ports , and the VMs - are similar to the corresponding components illustrated in .","To distribute packet processing between the pool nodes  and , each of the pool nodes  and  needs to be able to process a given packet. As such, the pool nodes  and  each include the same set of flow entries, in some embodiments. This way, either the pool node  or the pool node  can process a given packet.","Moreover, each of the software switching elements - needs to be able to access both of the pool nodes  and  in some embodiments. As such, some embodiments couple the software switching elements - to the pool nodes  and  using tunnels that are provided by tunneling protocols that are described above by reference to . As shown in , each of the software switching elements - is coupled to each of the pool nodes  and  through a tunnel. In addition, each of the software switching elements - is also coupled to each of the other software switching elements - through a tunnel (e.g., a layer 3 tunnel), and, thus, can each communicate with one another. These tunnels are indicated by dashed arrows. This way, each of the software switching elements - is aware of the interface (e.g., VIF) through which each VM is coupled, and, thus, has access to the MAC address associated with each of the interfaces through which the VMs are coupled. The tunnel configuration between the pool nodes  and  and the software switching elements - illustrated in  is referred to as a full tunnel mesh in some embodiments.","In some embodiments, software switching elements - send packets to the pool nodes  and  through designated ports. The designated ports are referred to as uplink ports in some embodiments. As shown in , the patch ports  include uplink ports  and . The uplink port  corresponds to the pool node  and the uplink port  corresponds to the pool node . Therefore, when the software switching element  wants to send packet to the pool node , the software switching element  sends the packet to the uplink port  and when the software switching element  wants to send packet to the pool node , the software switching element  sends the packet to the uplink port . The hypervisor  of some embodiments manages the uplink ports  and  such that the uplink ports  and  correspond to the correct physical ports  for the packets to reach the pool nodes  and .","As mentioned above,  illustrates a full tunnel mesh configuration between software switching elements and pool nodes in a managed network. However, different embodiments may use different tunnel configurations between the software switching elements and the pool nodes. For example, some embodiments might implement a partial tunnel mesh configuration. In some such embodiments, the pool nodes are divided into subsets of pool nodes and each subset of pool nodes handles a portion of the packet processing load.","As the number of pool nodes, root nodes, and\/or managed switching elements increases in a manage network utilizing a full tunnel mesh configuration, the complexity of the configuration can increase and the resources for establishing tunnels can decrease.  conceptually illustrates a tunnel configuration for reducing the number of tunnels between the pool nodes, root nodes, and\/or managed switching elements in the managed network while providing all the managed switching elements access to the pool node and root nodes.","As illustrated in , a managed network  includes pool and root nodes - and cliques  and . For this example, a pool and root node is a physical host (e.g., a server computer) on which an OVS runs as a pool node and an OVS runs as a root node. In some embodiments, a clique includes two or more managed switching elements that are coupled to each other in a full tunnel mesh configuration.","Referring to , the managed switching elements in the clique  are each coupled to each other through tunnels. Similarly, the managed switching elements in the clique  also are each coupled to each other through tunnels. However, none of the managed switching elements in the clique  are coupled to any of the managed switching elements in the clique . Thus, a lower number of tunnels are utilized than the number of tunnels that would be required if the managed switching elements in the cliques  and  were all configure in a full tunnel mesh configuration. Furthermore, each managed switching element in the cliques  and  are coupled to each of the pool and root nodes - through a tunnel. Although only a single arrow is shown between the cliques  and  and each of the pool and root nodes -, these arrows actually represent the tunnels (three tunnels in this example) from each of the managed switching elements in the cliques  and  and the pool and root nodes -.",{"@attributes":{"id":"p-0300","num":"0299"},"figref":["FIG. 30","FIG. 28","FIG. 14"],"b":["3000","3000","3000","1420","1400"]},"The process  is similar in many respects to the process  described above by reference to . However, the process  includes an addition operation for determining a hash value to determine a pool node to which to send the packet.","The operations - of the process  are the same as the operations - of the process . That is, the process  determines (at ) whether the packet has an unknown destination MAC address. If the packet has an unknown destination MAC address, the process  continues to . Otherwise, the process  determines (at ) whether the packet can be processed. If the packet cannot be processed, the process  proceeds to . If the process  determines that the packet can be processed, the process  processes (at ) the packet and then the process  determines (at ) whether the packet is a multicast or broadcast packet.","If the process  determines that the packet is not a multicast or broadcast packet, the process  ends. Otherwise, the process  determines (at ) whether the packet needs further processing. If the packet does not need further processing, the process  ends. Otherwise, the process  proceeds to .","At , the process  applies a hash function on a set of fields of the packet. Different embodiments of the process  apply a hash function on different sets of fields of the packet. For instance, some embodiments apply a hash function on the source MAC address of the packet while other embodiments apply a hash function on the source IP address of the packet. In some embodiments, a hash function is applied on the destination MAC address of the packet. Some embodiments may apply a hash function on both the source MAC address and the source IP address. Other ways of applying a hash function on the packet are possible in other embodiments.","Finally, the process  forwards (at ) the packet to a pool node based on the hash of the packet. In some embodiments, the hash function used to hash the packet may be defined based on the number of pool nodes from which to choose in the managed network. For instance, referring to  as an example, some embodiments may define a hash function that hashes to three different values that each correspond to each of the pool and root nodes -. This way, a hash of a packet selects one of the pool nodes based on the value of the hash of the packet. After the process  forwards the packet to the pool node, the process  ends.",{"@attributes":{"id":"p-0306","num":"0305"},"figref":"FIG. 31","b":["3100","3100","3110","3120","3130","3140","3150","3160","3170","3175","3180","3190"]},"The ingress ports , the egress ports , the dispatch port , and the forwarding tables  are similar to the ingress ports , the egress ports , the dispatch port , and the forwarding tables , which are described above by reference to . However, the forwarding tables  include a set of flow entries for processing packets to determine a pool node to which to send the packet. Specifically, the forwarding tables  includes a flow entry that specifies a hash function to be performed on packet when the packet is identified as a multicast packet, and flow entries that specify one of the pool node - to which to sent the packet based on a hash value.","In some embodiments, the packet processor  is similar to the packet processor , which is described above by reference to . That is, the packet processor  processes network data (e.g., packets) that the packet processor  receives from the ingress ports  based on flow entries in the forwarding tables . When the packet processor  wants to apply a hash function to a packet, the packet processor  sends a copy of the packet to the hash function module  and, in return, receives a hash value. In some cases, the packet processor  sends the hash value to the range list module , and, in return, receives a value that corresponds to a pool node in the managed network.","In some embodiments, the hash function module  performs a hash function on the packet and returns a hash value. As mentioned above, different embodiments define different types of hash functions that can be applied on different sets of fields of the packet (e.g., the source MAC address, the source IP address, etc.). The hash function module  of some embodiments receives hash functions from the virtualization application .","The range list module  of some embodiments restricts the hash values of the hash functions to a defined range of values. The range of values corresponds to the number of pool nodes in the managed network from which a pool node can be selected. Some embodiments of the range list module  restrict the hash values of the hash function to the defined range of values by mapping hash values to a corresponding value in the defined range of values.","In some embodiments, the virtualization application  is similar to the virtualization applications described above by reference to . In addition, the virtualization application  of some embodiments defines a range of values for the range list module . When a pool node is added or removed from the managed network, the virtualization application  of some embodiments dynamically redefines the range of values to reflect the number of pool nodes currently in managed network from which to select and provides the redefined range of values to the range list module .","Further, the virtualization application  sends defined hash functions to the hash function module , in some embodiments. When a pool node is added or removed (e.g., the pool node fails) from the managed network, some embodiments of the virtualization application  alternatively, or in conjunction with redefining a range of values for the range list module , redefine a hash function and provide the redefined hash function to the hash function module .","The following will describe an example packet processing operation to determine a pool node to which to send a packet. When the switching element  receives a packet through a port of the ingress ports , the packet is forwarded to the packet processor  to process. The packet processor  matches the packet against the flow entries in the forwarding tables  to process the packet. In this example, the packet is a multicast packet and needs to be processed by a pool node in the managed network. As such, the packet processor  determines that the packet matches the first flow entry illustrated in the forwarding tables . The first flow entry specifies to apply a hash function on the packet in order to select a pool node from the pool nodes - to which to sent the packet for processing.","The packet processor  sends a copy of the packet to the hash function module . The hash function module  applies the defined hash function on the copy of the packet and returns a hash value to the packet processor . Then, the packet processor  sends the hash value to the range list module  to receive a value that corresponds to one of the pool nodes -. When the range list module  receives the hash value from the packet processor , the range list module  identifies a value in a defined set of values to which the hash value maps and returns the identified value to the packet processor . For this example, the identified value is 2.","Next, the packet processor  stores the value that the packet processor  receives from the range list module  in the packet (e.g., in a logical context tag or another field in the packet header). The packet processor  then sends the packet to the dispatch port  for further processing. When the dispatch port  receives the packet, the packet is sent back to a port of the ingress ports . The packet is then forwarded back to the packet processor  for processing.","Alternatively, some embodiments of the packet processor  store the value that the packet processor  receives from the range list module  as metadata that is associated with (instead of stored in the packet itself) and passed along with the packet. In some of these embodiments, the packet processor  sends the packet and the associated metadata to the dispatch port  for further processing. When the dispatch port  receives the packet and the associated metadata, the packet and the associated metadata is sent back to a port of the ingress ports . The packet and the associated metadata is then forwarded back to the packet processor  for processing.","The packet processor  again matches the packet against the flow entries in the forwarding tables  to process the packet. This time, the packet processor  determines that the packet matches the third flow entry illustrated in the forwarding tables . The third flow entry specifies that the packet be sent to uplink port , which corresponds to the pool node  in this example. Accordingly, the packet processor  sends the packet to the port of the egress ports  that corresponds to the uplink port . In some embodiments, the packet processor  removes the value (\u201c2\u201d in this example) resulting from the hash operation from the packet's header before sending the packet to the egress ports .","IV. Defining Switching Infrastructures","The following section will describe several examples of operations that are performed when a managed network is operating. Some of the operations relate to pool node creation, root node creation, hash function updating, and network controller creation, among other operations.",{"@attributes":{"id":"p-0319","num":"0318"},"figref":["FIG. 32","FIGS. 2-5"],"b":["3200","3200","3200","3200"]},"The process  begins by determining (at ) whether the managed network needs switching elements. In some embodiments, switching elements include pool nodes, root nodes, and extenders. The process  of some embodiments can determine whether the managed network needs switching elements based on several factors. Examples of such factors include the number of machines, VMs, hosts, and any other type of network host in the managed network, the number of managed switching elements in the managed network, the attributes of the managed switching elements (e.g., hardware switching element or software switching element, amount of memory, amount of processing power, etc.) in the managed networks, the number of tenants in the managed network, etc. When the process  determines that the managed network does not need switching elements, the process  proceeds to .","When the process  determines that the managed network needs switching elements, the process  creates (at ) a set of switching elements for the managed network. Some embodiments of the process  determine the number of switching elements to create based on the same or similar factors listed above for the operation .","Next, the process  creates (at ) tunnels in the managed network. As described in various sections above, different embodiments create tunnels for different purposes and in different situations. For instance, some embodiments use tunnels to connect pool nodes and managed switching elements in a full tunnel mesh configuration in order to distribute packet processing between the pool nodes. Some embodiments use tunnels to form cliques of managed switching elements.","Finally, the process  populates (at ) flow entries in the managed switching elements and switching elements in the managed network. Flow entries specify operations for processing packets as the packets flow through the various managed switching elements and switching elements in the managed network. As such, the process  of some embodiments determines and defines flow entries for each managed switching element and switching element in the managed network. In some embodiments, flow entries are determined and defined based on the same factors used in the operation  described above. Some embodiments also take into account the switching elements, if any, that were created at the operation  and the tunnels that were created at the operation  in determining and defining the flow entries. After the process  determines and defines all the flow entries, the process  populates the flow entries into the respective managed switching elements and switching elements (e.g., through a switching control protocol, such as the Openflow protocol). The process  then ends.","At any given time while a managed network is operating, changes to the managed network (e.g., machines added, machines removed, switching elements added, switching elements removed, etc.) may occur. In some embodiments, the managed network may be reconfigured (e.g., by a network controller managing the managed network) in response to a change. For instance, additions of machines to the managed network might require additional switching elements (e.g., managed switching elements, pool nodes, root nodes, etc.). Conversely, when machines are removed from the managed network, switching elements might be removed from the managed network as well. Different embodiments consider any number of different factors in determine when and in what manner to respond to a change in the managed switching element. Several of the following figures illustrate examples of how a managed network may respond to changes that occur to the managed network.",{"@attributes":{"id":"p-0325","num":"0324"},"figref":["FIG. 33","FIG. 33"],"b":["3300","3300","3310","3320","3300","3300"]},"The first stage  illustrates that the managed network  includes a pool node , managed switching elements -, and machines belonging to a tenant A that are coupled to each of the managed switching elements -. In addition, the first stage  illustrates that tunnel is established between the each of the managed switching elements - and the pool node , and between the managed switching element  and the managed switching element .","In the second stage  of the managed network , additional machines have been added to the managed network . Specifically, machines that belong to a tenant B are now coupled to each of the managed switching elements -. In this example, the pool node  cannot handle processing load with the addition of tenant B's machines. Therefore, a set of network controllers (not shown) that are managing the managed network  determined that the managed network  requires another pool node  to lessen the load on the pool node .","In this example, only one pool node can support each of the managed switching elements -. Therefore, the set of network controllers also determined that the pool node  will support the managed switching element . In response, the tunnel between the managed switching element  and the pool node  is torn down and a tunnel between the managed switching element  and the pool node  is established. As a result, the pool node  and the managed switching element  will not be able to communicate with the pool node  and the managed switching elements  and . In addition, since there are multiple tenants in the managed network , logical context learning needs to be performed. Thus, the set of network controllers determined to create a root node  to provide a communication bridge between the pool nodes  and  and to perform logical context learning. As shown, tunnels between the pool nodes  and  and the root node  are established.",{"@attributes":{"id":"p-0329","num":"0328"},"figref":["FIG. 34","FIG. 34"],"b":["3400","3400","3405","3410","3400","3400"]},"As shown in the first stage , the managed network  includes a pool node , cliques  and , and groups of machines  and , which to a tenant A. Each of the cliques  and  includes three managed switching elements that are coupled to each other with tunnels in a full tunnel mesh configuration. In addition, for each of the cliques  and , the managed switching elements each include the same set of flow entries (not shown). As shown, the machines  are coupled to the clique  and the machines  are coupled to the clique .","In this example, the pool node  processes packets that the managed switching elements in the cliques  and  cannot process. As such, the cliques  and  are each coupled to the pool node  through tunnels. That is, a tunnel is established between each of the managed switching elements in the cliques  and  and the pool node .","The second stage  illustrates that additional groups of machines  and  have been added to the managed network . As shown, the machines  are coupled to the managed switching elements in the clique  and the machines  are coupled to the managed switching elements in the clique . In some embodiments, the addition of the machines  and  increases the load on the three managed switching elements in the cliques  and  that are illustrated in the first stage . As a result, a set of network controllers (not shown) that are managing the managed network  determined that the managed network  requires additional managed switching elements. As illustrated in the second stage  of , the cliques  and  now each include six managed switching elements in order to handle the additional load of processing packets from the machines , , , and . The six managed switching elements in the cliques  and  are coupled to each other in a full tunnel mesh configuration (not shown) in some embodiments.","In some embodiments, the addition of the machines  and  and the managed switching elements to the cliques  and  also increases the load on the pool node . The pool nodes  may not have sufficient resources (e.g., memory or data storage) to handle all the packets that the managed switching elements in the cliques  and  cannot handle. Thus, the set of network controllers has also determined that the managed network  needs another pool node . As shown in the second stage , the pool node  has been created and added to the managed network . In this example, the packet processing distribution technique described above by reference to  is utilized. Accordingly, as shown in , the cliques  and  are coupled to each of the pool nodes  and  (i.e., each of the managed switching elements cliques  and  are coupled to each of the pool nodes  and ). That way, the packet processing load is distributed between the pool nodes  and .",{"@attributes":{"id":"p-0334","num":"0333"},"figref":"FIGS. 33 and 34"},"As explained above, some embodiments use a hashing technique to distribute packet processing that managed switching elements cannot handle across several pool nodes in a managed network.  conceptually illustrates an example of updating a hash function when a pool node is added to a managed network. In particular,  conceptually illustrates a switching element  at three different stages - of a hash function update operation. In some embodiments, the switching element  is a software switching element (e.g., an OVS switch) while, in other embodiments, the switching element  is a hardware switching element. In other embodiments, the switching element  may be any other type of network element that can route network data.","The first stage  illustrates that the managed network includes the switching element  and pool nodes  and . As shown, the switching element  includes a forwarding plane . The forwarding plane  of some embodiments is similar to the forwarding plane  described above by reference to . That is, in these embodiments, the forwarding plane  processes network data that the switching element  receives and determines where to route the network data. Since the packet processing is distributed between the pool nodes  and , the pool nodes  and  include the same set of flow entries.","In addition, the forwarding plane  includes a hash function X. The hash function X represents is a hash function that the forwarding plane  uses to select one of the pool nodes  and  when the forwarding plane  wants to send a packet to a pool node for processing. In this example, packet processing is distributed based on logical data paths. Therefore, different logical data paths in a logical data path set may be distributed to different pool nodes. The hash function X may be applied to data in the packet (e.g., a header field, such as a logical context tag) that represents the logical data path to which the packet belongs, in some embodiments. The first stage  shows that the hash function X is defined to map packets that belong to the logical data path of flow A to the pool node , map packets that belong to the logical data path of flow B to the pool node , and map packets that belong to the logical data path of flow C to the pool node .","In the second stage , another pool node  is added to the managed network, as indicated by a box with dashed lines. The pool node  includes the same set of flow entries as the pool nodes  and . At this stage , the hash function for selecting a pool node is still hash function X. As shown, packets that belong to the logical data path of flow A are still mapped to the pool node , packets that belong to the logical data path of flow B are still mapped to the pool node , and packets that belong to the logical data path of flow C are still mapped to the pool node .","The third stage  illustrates the switching element  after the hash function X has been updated to a hash function Y in response to the addition of the pool node . In some embodiments, the hash function Y is provided to the switching element  by a network controller that manages the switching element . The hash function Y is defined to evenly distribute packets that belong to the logical data paths A, B, and C. For this example, the hash function Y maps packets that belong to the logical data path of flow A to the pool node , maps packets that belong to the logical data path of flow B to the pool node , and maps packets that belong to the logical data path of flow C to the pool node .","While  illustrates the update of a hash function for selecting a pool node from a group of pool nodes, this method may be similarly used in other embodiment as well. For instance, the hash function in the hash function module  may also be updated (e.g., by the virtualization application ) in a similar manner as described above.",{"@attributes":{"id":"p-0341","num":"0340"},"figref":["FIG. 36","FIG. 28"],"b":["3600","3600"]},"The process  begins by determining (at ) whether a change in the status of pool nodes in the managed network has occurred. In some embodiments, a change in the status of the pool nodes includes a pool node is added to the managed network, a pool node is removed from the managed network, or a pool node in the managed network is not functioning. A change in the status of pool nodes in the managed network may include additional and\/or other types of events in other embodiments.","When the process  determines that a change in the status of the pool nodes has occurred, the process  updates (at ) the status of uplink ports on the managed switching elements in the managed network. For instance, when a pool node is added to the managed network, the process  of some embodiments updates the status of the uplink ports on the managed switching elements to include another uplink port for the newly added pool node. Conversely, when a pool node is removed from the managed network, some embodiments of the process  updates the status of the uplink ports on the managed switching elements to remove an uplink port. Next, the process  sends (at ) an updated hash flow entry to the managed switching elements. In some embodiments, the hash flow entry specifies the hash function for the managed switching elements to select a pool node in the managed network to which to send packets that the managed switching elements cannot process. The process  then ends.","When the process  determines that a change in the status of the pool nodes has not occurred, the process  continues to , the process  determines (at ) whether a hash error has occurred on one of the managed switching elements in the managed network. Examples of hash errors include hash value collisions, hash values that are outside a defined range, etc. When the process  determines that a hash error has occurred on one of the managed switching elements in the managed network, the process  sends (at ) an updated hash flow entry to the managed switching elements. As noted above, some embodiments sends a hash flow entry that specifies a hash function for the managed switching elements to select a pool node in the managed network to which to send packets that the managed switching elements cannot process. Specifically, the process  sends a hash flow entry that corrects the hash error. Then, the process  ends.","In some embodiments, the process  is constantly repeated while the network controller is managing the managed switching elements in the managed network in order to continue checking for changes in the status of pool nodes in the managed network and updating the hash flow entries in the managed switching elements accordingly. In other embodiments, the process  is repeated at defined intervals (e.g., 1 minute, 5 minutes, 30 minutes, 1 hour, etc.).","The above description of  relate to updating hash functions when a pool node is added or removed to a managed network. In some instances, a pool node is removed from a managed network because the pool node has failed.  conceptually illustrates an example of pool node failure handling according to some embodiments of the invention. As shown, a network architecture  includes managed switching elements  and , and pool nodes A-C. In this example, each of the arrows in  represents a tunnel.","Some embodiments utilize tunnel \u201cbundling\u201d as a pool node fault tolerance technique. In some such embodiments, each pool node in the network is designated a failover pool node so that packets destined for the failed pool node may quickly continue to be processed by the network architecture. In some embodiments, the failover pool node is referred to as a secondary pool node and the pool node for which the failover pool node is designated is referred to as a primary pool node.","Different embodiments designate secondary pool nodes for the primary pool nodes in the network differently. For instance, some embodiments specify, for a particular primary pool node, another primary pool node in the network as a secondary pool node.  conceptually illustrates such an example. Specifically,  illustrates a hierarchy traversal table  of the managed switching element . As shown, the primary pool node for the pool node  is the pool node A, the primary pool node for the pool node  is the pool node B, and the primary pool node for the pool node  is the pool node C. Additionally, the hierarchy traversal table  specifies the secondary pool nodes for each of the primary pool nodes -. In particular, the secondary pool node for the pool node  is the pool node B, the primary pool node for the pool node  is the pool node C, and the primary pool node for the pool node  is the pool node A. In this example, the managed switching elements  and  monitor the pool nodes - in order to detect when one of the pool nodes - fails.",{"@attributes":{"id":"p-0349","num":"0348"},"figref":["FIG. 37B","FIG. 37B"],"b":["3700","3705","3705","2","3715","3705","3705","3715","2","2","2","3705","2","3705"]},"In addition, since the pool node B was designated as the secondary pool node for the pool node , the managed switching element  has modified the hierarchy traversal table  to no longer specify a secondary pool node for the pool node . However, in some embodiments, the managed switching element  automatically designates new secondary pool nodes when a pool node fails. The managed switching element , for example, may designate the pool node C as the secondary pool node for the pool node  and designate the pool node A as the secondary pool node for the pool node .",{"@attributes":{"id":"p-0351","num":"0350"},"figref":["FIG. 37C","FIG. 37C"],"b":["3700","3700","2","3715","3705","1","2","3715","1","2"]},"Instead of specifying one of the primary pool nodes in the network as a secondary pool node of a particular primary pool node, some embodiments may provide backup pool nodes as secondary pool nodes. The backup pool nodes of some embodiments are configured to stand by and replace a primary pool node when the primary pool node fails.  conceptually illustrates an example of the network architecture  that employs backup pool nodes. As shown,  illustrates the hierarchy traversal table . For this example, the hierarchy traversal table  specifies the primary pool node for the pool node  as the pool node A, the primary pool node for the pool node  as the pool node B, and the primary pool node for the pool node  as the pool node C. In additional, the hierarchy traversal table  specifies the secondary pool node for pool node  as the pool node B, the primary pool node for pool node  as the pool node C, and the primary pool node for pool node  as the pool node A.",{"@attributes":{"id":"p-0353","num":"0352"},"figref":["FIG. 37E","FIG. 37E"],"b":["3700","3705","3705","2","3715","3705","3705","3715","2","2","2","3705","2","3705"]},{"@attributes":{"id":"p-0354","num":"0353"},"figref":"FIG. 37F","b":["3700","3700","3700","2","3715","3705","2","2"]},"Moreover, by utilizing a tunnel bundling technique, the tunnels to the pool nodes and the pool nodes may be viewed as a single entity (a \u201cbundle\u201d of tunnels) from the perspective of the network controllers in the network. Specifically, the network controllers view the managed switching element as coupled to a single pool node through a single tunnel. In some such embodiments, the network controllers may send flow entries that only specify that packets be sent to a pool node instead of having to determine the number of pool nodes in the network and to specify pool node to which the packet be sent. In other words, the managed switching elements are responsible for selecting a pool node when a packet to be sent to a pool node for processing.","By having the managed switching elements  and  handle pool node failures, the network controller or control cluster managing the managed network does not need to specify new flow entries to the managed switching elements  and  each time a pool node fails. In addition, the response time to a pool node failure is faster by implementing this functionality in the managed switching elements  and  instead of the network controller or control cluster.",{"@attributes":{"id":"p-0357","num":"0356"},"figref":["FIG. 38","FIG. 38A"],"b":["3800","3800","3810","3820","3800","3800"]},"The first stage  of  illustrates the managed network . The managed network  is similar to the managed network  illustrated in  except managed network  also includes a network controller . The network controller  is similar to the network controllers described above by reference to . At this stage , the network controller  manages the pool node  and the managed switching elements -.","The second stage  of  is similar to the second stage  that is described above by reference to , but the second stage  of the managed network  shows additional machines added to the managed network  that belong to a tenant C. As shown machines that belong to tenant C are now coupled to each of the managed switching elements  and .","Similar to the second stage , the pool node , at the second stage , cannot handle processing load with the addition of tenant B's and tenant C's machines. Therefore, the network controller  determined that the managed network  requires another pool node  to lessen the load on the pool node . As a result, the tunnel between the managed switching element  and the pool node  is torn down, a tunnel between the managed switching element  and the pool node  is established, and a root node  is created to provide a communication bridge between the pool nodes  and  and to perform logical context learning.","In addition, the second stage  illustrates that another network controller  has been added to the control cluster. In some embodiments, the computation demands of a network controller  increases as the number of tenants increases in the managed network  since the network controller would have to implement a logical switching element for each additional tenant across the managed switching elements in the managed network. Similarly, an increase in the number of machines and\/or switching elements in the managed network  would increase the computational demands of the network controller .","In this example, the network controller cannot handle the load of managing managed network  due to the addition of tenant B's and tenant C's machines to the managed network . For instance, the network controller  would have to define logical data path sets for each of the tenants B and C in order to implement corresponding logical switching elements for the tenants across the managed switching elements - in the managed network . Therefore, the network controller  determined to add the network controller  to assist in the management of the managed network .","As shown,  illustrates a simple case of creating additional network controllers to a control cluster for managing a managed network. However, the addition of one network controller to the control cluster in this example may be problematic from a reliability point of view. For example, some embodiments employ a majority\/minority technique for maintaining reliability of a control cluster. In some such embodiments, the network controllers communicate with each other and the control cluster continues to operate as long as a majority (i.e., greater than half) of the network controllers in the control cluster can communicate with each other. Therefore, the control cluster can withstand a minority (i.e., less than half) of the network controllers in the control cluster failing before the control cluster fails.","Referring to the example illustrated in , the addition of one network controller to the control cluster is thus problematic under the majority\/minority technique. Specifically, while the addition of the one network controller to the control cluster increases the compute capacity of the control cluster, the reliability of the control cluster is reduced because the number of points of failure in the control cluster is increased to two (i.e., a failure of any one of the two network controllers causes the control cluster to fail) without an increase in the number of failures that the control cluster can withstand (one in this example).","Thus, in order to maximize reliability of the control cluster, additions of network controllers to the control clusters are constrained to numbers that maximizes the size of the minority of network controllers in the control cluster.  conceptually illustrates such an example of creating additional network controllers in the control cluster for the managed network  at two stages  and  of the operation of the managed network  in response to an increase in the number of machines in the managed network .","The first stage  of  is similar to the first stage  illustrated in . At this stage , the network controller  manages the pool node  and the managed switching elements -.","The second stage  of  is similar to the second stage  of  except the second stage  of the managed network  shows two network controllers  and  added to the control cluster due to the increased computation demands of the network controller . In this example, utilizing majority\/minority technique, the addition of the two network controllers  and  increases the compute capacity of the control cluster and increases the minority (from zero to one in this example) of the network controllers , , and  in the control cluster failing before the control cluster fails.",{"@attributes":{"id":"p-0368","num":"0367"},"figref":"FIG. 38B"},"While some factors for determining whether to add a network controller to a managed network have been described above, other embodiments may consider additional and\/or other factors as well in such a determination.",{"@attributes":{"id":"p-0370","num":"0369"},"figref":"FIG. 38"},"Some embodiments may provide a network controller fault tolerance method for handling the failure of a network controller. In some embodiments, a logical switching element is managed by only one network controller (but a network controller may manage more than one logical switching elements). Thus, some of these embodiments specify, for a particular network controller, another network controller as a failover network controller in the event the particular network controller fails. In some embodiments, the failover network controller is referred to as a secondary network controller and the network controller for which the failover network controller is specified is referred to as a primary network controller.",{"@attributes":{"id":"p-0372","num":"0371"},"figref":["FIG. 47","FIG. 47"],"b":["4700","1","2","4705","4710","4710","4710"]},"In , the logical switching element master table  specifies that the primary network controller for the logical switching element  is the network controller A, the primary network controller for the logical switching element  is the network controller B, and the primary network controller for the logical switching element  is the network controller C. In additional, the logical switching element master table  specifies that the secondary network controller for the logical switching element  is the network controller B, the secondary network controller for the logical switching element  is the network controller C, and the secondary network controller for the logical switching element  is the network controller A. For this example, the network controllers A-C communicate with each other in order to detect when one of the network controllers A-C fails.",{"@attributes":{"id":"p-0374","num":"0373"},"figref":["FIG. 47B","FIG. 47B"],"b":["4700","4710","4710","1","1","1"]},"Additionally, since the network controller A was designated as the secondary network controller for the logical switching element , the network controllers B and C have modified the logical switching element master table  to no longer specify a secondary network controller for the logical switching element . However, in some embodiments, the network controllers B and C may automatically designate new secondary network controllers when a network controller fails. For instance, the network controllers B and C may specify the network controller C as the secondary network controller for the logical switching element  and specify the network controller B as the secondary network controller for the logical switching element .",{"@attributes":{"id":"p-0376","num":"0375"},"figref":["FIG. 47C","FIG. 47C"],"b":["4700","4700","1","4710","1","3","4710","1","3"]},"Although  illustrate failure handling of a network controller that manages a logical switching element, some embodiments also provide failure handling of a network controller of a managed switching element. In some cases, a managed switching element of some embodiments is managed by only one network controller (but a network controller may manage more than one managed switching elements). As such, some embodiments specify, for a particular network controller, another network controller as a secondary network controller in the event the particular network controller fails.",{"@attributes":{"id":"p-0378","num":"0377"},"figref":["FIG. 48","FIG. 48"],"b":["4800","4805","1","3","4810","4810","4810"]},"In , the managed switching element master table  specifies that the primary network controller for the managed switching element  is the network controller A, the primary network controller for the managed switching element  is the network controller B, and the primary network controller for the managed switching element  is the network controller C. Additionally, the managed switching element master table  specifies that the secondary network controller for the managed switching element  is the network controller B, the secondary network controller for the managed switching element  is the network controller C, and the secondary network controller for the managed switching element  is the network controller A. In this example, the network controllers A-C communicate with each other in order to detect when one of the network controllers A-C fails.",{"@attributes":{"id":"p-0380","num":"0379"},"figref":["FIG. 48B","FIG. 48B"],"b":["4800","4810","4810","2","2","2"]},"Furthermore, since the network controller B was designated as the secondary network controller for the managed switching element , the network controllers A and C have modified the managed switching element master table  to no longer specify a secondary network controller for the managed switching element . However, the network controllers A and C of some embodiments may automatically specify new secondary network controllers when a network controller fails. For instance, the network controllers A and C may specify the network controller C as the secondary network controller for the managed switching element  and specify the network controller A as the secondary network controller for the logical switching element .",{"@attributes":{"id":"p-0382","num":"0381"},"figref":["FIG. 48C","FIG. 48C"],"b":["4800","4800","2","4810","1","2","4810","1","2"]},"V. Logical Processing",{"@attributes":{"id":"p-0383","num":"0382"},"figref":"FIG. 39","b":["3900","3900"]},"The process  starts by mapping (at ) the packet to a logical context. As noted above, a logical context of some embodiments represents the state of the packet with respect to a logical switching element. The process  maps the packet to the packet's logical context in order to identify the stage in the logical switching element the packet is at.","Next, the process  performs (at ) logical processing on the packet. Different embodiments perform logical processing on the packet differently. For example, the logical switching element may be implemented as a layer 2 switching element. In these cases, the logical processing includes performing logical layer 2 operations on the packet, such as performing a logical layer 2 lookup on the packet to determine the logical egress port of the logical switching element through which to send the packet.","In some cases, the process  performs only a portion of the logical processing on the packet. For example, the process  may start performing the logical processing on the packet, but the process  does not complete the logical processing. Rather than waste the logical processing that has already been performed on the packet, the process  modifies the logical context of the packet to indicate the stage in the logical processing that the packet is at so that logical processing on the packet can resume where the logical processing left off the next time the logical processing is performed on the packet (e.g., by the managed switching element that receives the packet next).","Other instances where the process  performs only a portion of the logical processing on the packet is when a portion of the logical processing has already been performed on the packet (e.g., by a previous managed switching element). In these instances, the logical context of the packet, which was identified by the mapping of the packet to a logical context in the operation , indicates the stage in the logical processing that the packet is at. Accordingly, the process  resumes performing the logical processing on the packet at this point in the logical processing.","After the process  performs the logical processing (or a portion of the logical processing) on the packet, the process  maps (at ) the result of the logical processing of the packet a corresponding physical result. For example, when the result of the logical processing of the packet determines a logical port of the logical switching element through which to send the packet, the process  maps the logical port(s) to a corresponding physical port(s) (e.g., a port of a managed switching element that is used to implement the logical switching element) through which to send the packet. In some embodiments, the physical port may be a physical port of a managed switching element that is different from the managed switching element that is performing the process .","Finally, the process  performs (at ) physical processing on the packet to determine the physical port of the managed switching element that is performing the process  through which to send the packet so the packet reaches the physical port(s) determined at the operation .",{"@attributes":{"id":"p-0390","num":"0389"},"figref":"FIG. 40","b":["4000","4000","4020","4070","4000"]},"In some embodiments, a packet includes a header and a payload. The header includes, in some embodiments, a set of fields that contains information used for routing the packet through a network. Switching elements may determine switching decisions based on the contained in the header and may, in some cases, modify some or all of the header fields. As explained above, some embodiments determine switching decisions based on flow entries in the switching elements' forwarding tables.","In some embodiments, the processing pipeline  may be implemented by flow entries in the managed switching elements in the network. For instance, some or all of the flow entries are defined such that the packet is processed against the flow entries based on the logical context tag in the packet's header. Therefore, in some of these embodiments, the managed switching elements are configured (e.g., by a network controller illustrated in ) with such flow entries.","As shown,  illustrates a set of ingress ports , a set of queues , and a set of egress ports . The set of ingress ports  conceptually represent a set of ports (e.g., a tunnel port, NICs, VIFs, PIFs) of the managed switching element that is performing the processing pipeline . The ingress ports  are ports through which the managed switching element receives packets. The set of queues  conceptually represents a set of queues of the managed switching element that is performing the processing pipeline . In some embodiments, the set of queues  are for implementing resource control mechanisms, such as quality of service (QoS). The set of egress ports  conceptually represent a set of ports (e.g., a tunnel port, NICs, VIFs, PIFs) of the managed switching element that is performing the processing pipeline . The egress ports  are ports through which the managed switching element sends packets. In some embodiments, at least one port in the set of ingress ports  is also a port in the set of egress ports . In some embodiments, the set of ingress ports  and the set of egress ports  are the same set of ports. That is, the managed switching element includes a set of ports that are used both to receive packets and to send packets.","The first stage  is similar to the first stage  of the processing pipeline , which is described above by reference to . At the stage , ingress context mapping is performed on a packet to determine the logical context of the packet. In some embodiments, the first stage  is performed when the logical switching element receives the packet (e.g., the packet is initially received by a managed switching element in the network that implements the logical switching elements). As noted above, a logical context, in some embodiments, represents the state of the packet with respect to the logical switching element. The logical context may, for example, specify the logical switching element to which the packet belongs, the logical port of the logical switching element through which the packet was received, the logical port of the logical switching element through which the packet is to be transmitted, the stage of the logical forwarding plane of the logical switching element the packet is at, etc.","Some embodiments determine the logical context of a packet based on the source MAC address of the packet (i.e., the machine from which the packet was sent). Some embodiments perform the logical context lookup based on the source MAC address of the packet and the inport (i.e., ingress port) of the packet (i.e., the port of the managed switching element through which the packet was received). Other embodiments may use other fields in the packet's header (e.g., MPLS header, VLAN id, etc.) for determining the logical context of the packet.","After the first stage  is performed, some embodiments store the information that represents the logical context in one or more fields of the packet's header. These fields may also be referred to as a logical context tag or a logical context ID. Furthermore, the logical context tag may coincide with one or more known header fields (e.g., the VLAN id field) in some embodiments. As such, these embodiments do not utilize the known header field or its accompanying features in the manner that the header field is defined to be used. Alternatively, some embodiments store the information that represents the logical context as metadata that is associated with (instead of stored in the packet itself) and passed along with the packet.","In some embodiments, the second stage  is defined for the logical switching element. In some such embodiments, the second stage  operates on the packet's logical context to determine ingress access control of the packet with respect to the logical switching element. For example, an ingress ACL is applied to the packet to control the packet's access to the logical switching element when the logical switching element receives the packet. The ingress ACL may be defined to implement other ACL functionalities, such as counters, port security (e.g., allow packets received through a port that originated only from a particular machine(s)), and machine isolation (e.g., allow broadcast\/multicast packets received from a particular machine to be sent to only machines that belong to the same tenant or logical switching element), among other ACL functionalities. Based on the ingress ACL defined for the logical switching element, the packet may be further processed (e.g., by the third stage ) or the packet may be dropped, for example.","In the third stage  of the processing pipeline , logical processing is performed on the packet in the context of the logical switching element. In some embodiments, the third stage  operates on the packet's logical context to process and route the packet with respect to the logical switching element. Different embodiments define logical processing for the logical switching element differently. For instance, some embodiments define a logical layer 2 table for processing the packet at layer 2 of the logical network. Alternatively, or in conjunction with the logical layer 2 table, some embodiments define a logical layer 3 table for processing the packet at layer 3 of the logical network. Other embodiments may define other logical process for the packet at the stage .","The fourth stage  of some embodiments is defined for the logical switching element. The fourth stage  of some such embodiments operates on the packet's logical context to determine egress access control of the packet with respect to the logical switching element. For instance, an egress ACL may be applied to the packet to control the packet's access out of the logical switching element after logical processing has been performed on the packet. Based on the egress ACL defined for the logical switching element, the packet may be further processed (e.g., sent out of a logical port of the logical switching element or sent to a dispatch port for further processing) or the packet may be dropped, for example.","In the fifth stage  of the processing pipeline  is similar to the third stage  of the processing pipeline , which is described above by reference to . At the fifth stage , egress context mapping is performed to identify a physical result that corresponds to the result of the logical processing of the packet. For example, the logical processing of the packet may specify that the packet is to be sent out of one or more logical ports (e.g., a logical egress port) of the logical switching element. As such, the egress context mapping operation identifies a physical port(s) of one or more of the managed switching elements that corresponds to the particular logical port of the logical switching element.","The sixth stage  of the processing pipeline  performs a physical mapping based on the egress context mapping performed at the fifth stage . In some embodiments, the physical mapping determines operations for routing the packet to the physical port that was determined in the fifth stage . For example, the physical mapping of some embodiments determines one or more queues in the set of queues  associated with one or more ports of the set of ports  of the managed switching elements that is performing the processing pipeline  through which to send the packet in order for the packet to reach the physical port(s) determined in the fifth stage . This way, the managed switching elements can route the packet along the correct path in the network for the packet to reach the determined physical port(s). Also, some embodiments remove the logical context tag after the sixth stage  is completed in order to return the packet to its original state before the packet was processed by the processing pipeline .","As mentioned above, in some embodiments, the processing pipeline  is performed by each managed switching element in the managed network that is used to implement the logical switching element. The processing pipeline  of some embodiments may be distributed across the managed switching elements in the managed network. For example, in some embodiments, the second-fourth stages - are distributed across the managed switching elements in the managed network. In some of these embodiments, the managed switching element that initially receives the packet may perform the first-sixth stages - and the remaining managed switching elements that subsequently receive the packet only perform the first, fifth, and sixth stages , , and .",{"@attributes":{"id":"p-0403","num":"0402"},"figref":"FIG. 41","b":["4100","4100","4120","4150","4100"]},"As explained above, a packet, in some embodiments, includes a header and a payload. In some embodiments, the header includes a set of fields that contains information used for routing the packet through a network. Switching elements may determine switching decisions based on the fields contained in the header and may, in some cases, modify some or all of the header fields. As explained above, some embodiments determine switching decisions based on flow entries in the switching elements' forwarding tables.","In this example, the 64-bit context tag is a field that is included in the header of a packet. As shown, the 64-bit context tag includes a 32-bit virtual routing function (VRF) field, a 16-bit logical inport field, and a 16-bit logical outport field. The 32-bit VRF field represents the logical switching element to which the packet belongs and the stage of the logical forwarding plane of the logical switching element the packet is at, the 16-bit logical inport field represents the logical port of the logical switching element through which the packet was received, and the 16-bit logical outport field represents the logical port of the logical switching element through which the packet is to be transmitted.","In some embodiments, the processing pipeline  may be implemented by flow entries in the managed switching elements in the network. For instance, some or all of the flow entries are defined such that the packet is processed against the flow entries based on the 64-bit logical context tag in the packet's header. Therefore, in some of these embodiments, the managed switching elements are configured (e.g., by a network controller illustrated in ) with such flow entries.","As shown,  illustrates a set of ingress ports , a set of queues , and a set of egress ports . The set of ingress ports , the set of queues , and the set of egress ports  are similar to the set of ingress ports , the set of queues , and the set of egress ports , respectively. The set of ingress ports  conceptually represent a set of ports (e.g., a tunnel port, NICs, VIFs, PIFs) of the managed switching element that is performing the processing pipeline . The ingress ports  are ports through which the managed switching element receives packets. The set of queues  conceptually represents a set of queues of the managed switching element that is performing the processing pipeline . In some embodiments, the set of queues  are for implementing resource control mechanisms, such as quality of service (QoS). The set of egress ports  conceptually represent a set of ports (e.g., a tunnel port, NICs, VIFs, PIFs) of the managed switching element that is performing the processing pipeline . The egress ports  are ports through which the managed switching element sends packets. In some embodiments, at least one port in the set of ingress ports  is also a port in the set of egress ports . In some embodiments, the set of ingress ports  and the set of egress ports  are the same set of ports. That is, the managed switching element includes a set of ports that are used both to receive packets and to send packets.","At the first stage  of the processing pipeline , a physical to logical mapping is performed on a packet to determine the logical context of the packet. In this example, the physical to logical mapping of the first stage  determines the logical switching element to which the packet belongs, the stage of the logical forwarding plane of the logical switching element the packet is at, and the logical port of the logical switching element through which the packet was received. In some embodiments, the first stage  is performed when the logical switching element receives the packet (e.g., the packet is initially received by a managed switching element in the network that implements the logical switching elements).","Different embodiments determine the logical context of a packet based on different fields of the packet's header. For instance, as shown in , some embodiments determine the logical context of a packet based on the source MAC address of the packet (i.e., the machine from which the packet was sent), an inport (i.e., an ingress port in the set of ingress ports ) of the packet (i.e., the physical port of the managed switching element through which the packet was received), a VLAN id, the 64-bit context tag, or any combination of the four fields.","After the first stage  is performed, some embodiments store the information that represents the logical context in packet's the 64-bit logical context tag, as illustrated by arrows from the stage  to the corresponding fields below. For example, the logical switching element to which the packet belongs and the stage of the logical forwarding plane of the logical switching element the packet is at is stored in the 32-bit VRF field, and the logical port of the logical switching element through which the packet was received is stored in the 16-bit logical inport field.","In some embodiments, the second stage  is defined for the logical switching element. In this example, the second stage  operates on the packet's 64-bit logical context tag to determine access control of the packet with respect to the logical switching element. As shown by arrows pointing from the fields below to the stage , an ACL operates on the 16-bit logical inport field and the 32-bit VRF field of the packet's 64-bit logical context tag, which results in allowing the packet to be further processed (e.g., by the third stage ), denying the packet (i.e., dropping the packet), or enqueuing the packet. In some embodiments, enqueuing the packet involves sending the packet to a queue in the set of queues  that is associated with a port in the set of egress ports  for QoS purposes. In addition, the ACL may be defined to implement other ACL functionalities (not shown), such as counters, port security (e.g., allow packets received through a port that originated only from a particular machine(s)), and machine isolation (e.g., allow broadcast\/multicast packets received from a particular machine to be sent to only machines that belong to the same tenant or logical switching element), among ACL functionalities.","In the third stage  of the processing pipeline , the packet is processed against a logical L2 (layer 2) table to determine a logical outport, which corresponds to a logical port of the logical switching element through which the packet is to be sent. As shown by arrows pointing from the fields below to the stage , the L2 table operates on the 16-bit logical inport field and the 32-bit VRF field of the packet's 64-bit logical context tag in addition to the destination MAC address of the packet. After the third stage  is performed, some embodiments store the information that represents the determined logical outport in the 16-bit logical outport field of the packet's 64-bit logical context tag, as illustrated by an arrow from the stage  to the outport field below.","At the fourth stage  of the processing pipeline , a logical to physical mapping is performed to identify one or more physical ports of one or more managed switching elements in the managed network that corresponds to the logical outport, which was determined in the third stage , of the logical switching element. For this example, the fourth stage  operates on the packet's 64-bit logical context tag to identify one or more physical ports in the set of egress ports  through which to send the packet out in order for the packet to reach the determined logical outport. As shown by arrows pointing from the fields below to the stage , the fourth stage  operates on the 16-bit logical outport field and the 32-bit VRF field of the packet's 64-bit logical context tag, which results in setting the 64-bit logical context tag (e.g., saving the stage of the logical switching element that the packet is at, removing the 64-bit logical context tag), setting the one or more queues in the set of queues  associated with the physical ports, and setting the one or more physical ports in the set of egress ports  through which to send the packet out.","As mentioned above, in some embodiments, the processing pipeline  is performed by each managed switching element in the managed network that is used to implement the logical switching element. The processing pipeline  of some embodiments may be distributed across the managed switching elements in the managed network. For example, in some embodiments, the second and third stages  and  are distributed across the managed switching elements in the managed network. In some of these embodiments, the managed switching element that initially receives the packet may perform the first-fourth stages - and the remaining managed switching elements that subsequently receive the packet only perform the first and fourth stages  and .","In the above description of , , and , reference to \u201cphysical\u201d components (e.g., physical switching element, physical ports, etc.) refers to the managed switching elements in the managed network. As explained above, a managed switching element may be a hardware switching element, a software switching element, or a virtual switching element. Thus, one of ordinary skill in the art will realize that the reference to a physical component is not meant to refer to an actual physical component, but rather the reference is meant to distinguish from logical components (e.g., a logical switching element, a logical port, etc.).","As mentioned above, some embodiments may distribute the processing of a processing pipeline across managed switching elements in a managed network.  conceptually illustrates distribution of logical processing across managed switching elements in a managed network according to some embodiments of the invention. In particular,  conceptually illustrates a processing pipeline  distributed across two managed switching elements  and . The processing pipeline  is similar to the processing pipeline  described above by reference to . Stage  corresponds to the stage , stage  corresponds to the stage , stage  corresponds to the stage , stage  corresponds to the stage , stage  corresponds to the stage , and stage  corresponds to the stage . In addition,  conceptually illustrates forwarding tables in the managed switching elements  and  that are each implemented as a single table and implementing multiple forwarding tables (e.g., using a dispatch port, which is not shown) with the single table.","As illustrated in , VM  is coupled to the managed switching element , the managed switching element  is coupled to the managed switching element , and the managed switching element  is coupled to VM . In this example, the VM  sends a packet  to VM  through a logical switching element that is implemented by the managed switching elements  and .","As shown in the top half of , the managed switching element  includes a forwarding table that includes rules (e.g., flow entries) for processing and routing the packet . When the managed switching element  receives the packet  from the VM  through a VIF (not shown) of the managed switching element , the managed switching element  begins processing the packet  based on the forwarding tables of the managed switching element . The managed switching element  identifies a record indicated by an encircled  (referred to as \u201crecord \u201d) in the forwarding tables that implements the context mapping of the stage . The record  identifies the packet 's logical context based on the inport, which is the VIF through which the packet  is received from the VM . In addition, the record  specifies that the managed switching element  store the logical context of the packet  in a set of fields (e.g., a VLAN id field) of the packet 's header. The record  also specifies the packet  be further processed by the forwarding tables (e.g., by sending the packet  to a dispatch port).","Based on the logical context and\/or other fields stored in the packet 's header, the managed switching element  identifies a record indicated by an encircled  (referred to as \u201crecord \u201d) in the forwarding tables that implements the ingress ACL of the stage . In this example, the record  allows the packet  to be further processed and, thus, specifies the packet  be further processed by the forwarding tables (e.g., by sending the packet  to a dispatch port). In addition, the record  specifies that the managed switching element  store the logical context (i.e., the packet  has been processed by the second stage  of the processing pipeline ) of the packet  in the set of fields of the packet 's header.","Next, the managed switching element  identifies, based on the logical context and\/or other fields stored in the packet 's header, a record indicated by an encircled  (referred to as \u201crecord \u201d) in the forwarding tables that implements the logical L2 forwarding of the stage . The record  identifies the logical port of the logical switching element, which is implemented by the managed switching elements  and , to which the packet  is to be forwarded. The record  also specifies that the packet  be further processed by the forwarding tables (e.g., by sending the packet  to a dispatch port). Also, the record  specifies that the managed switching element  store the logical context (i.e., the packet  has been processed by the third stage  of the processing pipeline ) in the set of fields of the packet 's header.","Based on the logical context and\/or other fields stored in the packet 's header, the managed switching element  identifies a record indicated by an encircled  (referred to as \u201crecord \u201d) in the forwarding tables that implements the egress ACL of the stage . In this example, the record  allows the packet  to be further processed and, thus, specifies the packet  be further processed by the forwarding tables (e.g., by sending the packet  to a dispatch port). In addition, the record  specifies that the managed switching element  store the logical context (i.e., the packet  has been processed by the fourth stage  of the processing pipeline ) of the packet  in the set of fields of the packet 's header.","In the fifth stage  of the processing pipeline , the managed switching element  identifies, based on the logical context and\/or other fields stored in the packet 's header, a record indicated by an encircled  (referred to as \u201crecord \u201d) in the forwarding tables that implements the context mapping of the stage . In this example, the record  identifies the VIF (not shown) of the managed switching element  to which the VM  is coupled as the port that corresponds to the logical port of the logical switching element to which the packet  is to be forwarded. The record  additionally specifies that the packet  be further processed by the forwarding tables (e.g., by sending the packet  to a dispatch port).","Based on the logical context and\/or other fields stored in the packet 's header, the managed switching element  then identifies a record indicated by an encircled  (referred to as \u201crecord \u201d) in the forwarding tables that implements the physical mapping of the stage . The record  specifies the port of the managed switching element  through which the packet  is to be sent in order for the packet  to reach the VM . In this case, the managed switching element  is to send the packet  out of the port (not shown) of managed switching element  that is coupled to the managed switching element .","As shown in the bottom half of , the managed switching element  includes a forwarding table that includes rules (e.g., flow entries) for processing and routing the packet . When the managed switching element  receives the packet  from the managed switching element , the managed switching element  begins processing the packet  based on the forwarding tables of the managed switching element . The managed switching element  identifies a record indicated by an encircled  (referred to as \u201crecord \u201d) in the forwarding tables that implements the context mapping of the stage . The record  identifies the packet 's logical context based on the logical context that is stored in the packet 's header. The logical context specifies that the packet  has been processed by the second-fourth stages - of the processing pipeline , which was performed by the managed switching element . As such, the record  specifies that the packet  be further processed by the forwarding tables (e.g., by sending the packet  to a dispatch port).","Next, the managed switching element  identifies, based on the logical context and\/or other fields stored in the packet 's header, a record indicated by an encircled  (referred to as \u201crecord \u201d) in the forwarding tables that implements the context mapping of the stage . In this example, the record  identifies the VIF (not shown) of the managed switching element  to which the VM  is coupled as the port that corresponds to the logical port of the logical switching element (which was determined by the managed switching element ) to which the packet  is to be forwarded. The record  additionally specifies that the packet  be further processed by the forwarding tables (e.g., by sending the packet  to a dispatch port).","Based on the logical context and\/or other fields stored in the packet 's header, the managed switching element  identifies a record indicated by an encircled  (referred to as \u201crecord \u201d) in the forwarding tables that implements the physical mapping of the stage . The record  specifies the port of the managed switching element  through which the packet  is to be sent in order for the packet  to reach the VM . In this case, the managed switching element  is to send the packet  out of the VIF (not shown) of managed switching element  that is coupled to the VM .","The above description of  illustrates a managed switching element in a managed network that performs an entire logical processing of a processing pipeline of some embodiments. However, some embodiments may distribute the logical processing of a processing pipeline across several managed switching element in a managed network. The following figure conceptually illustrates an example of such an embodiment.  conceptually illustrates the distribution of logical processing across managed switching elements in a managed network according to some embodiments of the invention. Specifically,  conceptually illustrates the processing pipeline  distributed across the two managed switching elements  and .",{"@attributes":{"id":"p-0428","num":"0427"},"figref":["FIG. 43","FIG. 42","FIG. 43","FIG. 43"],"b":["4210","4200","4220","4200","4210","4240","4250","4260","4280","4290","4210","4270","4200","4220","4230","4220","4290","4230","4230","4260","4200"]},"As illustrated in the bottom half of , when the managed switching element  receives the packet  from the managed switching element , the managed switching element  begins processing the packet  based on the forwarding tables of the managed switching element . The managed switching element  identifies a record indicated by an encircled  (referred to as \u201crecord \u201d) in the forwarding tables that implements the context mapping of the stage . The record  identifies the packet 's logical context based on the logical context that is stored in the packet 's header. The logical context specifies that the packet  has been processed by the second and third stages  and  of the processing pipeline , which was performed by the managed switching element . As such, the record  specifies that the packet  be further processed by the forwarding tables (e.g., by sending the packet  to a dispatch port).","Based on the logical context and\/or other fields stored in the packet 's header, the managed switching element  identifies a record indicated by an encircled  (referred to as \u201crecord \u201d) in the forwarding tables that implements the egress ACL of the stage . In this example, the record  allows the packet  to be further processed and, thus, specifies the packet  be further processed by the forwarding tables (e.g., by sending the packet  to a dispatch port). In addition, the record  specifies that the managed switching element  store the logical context (i.e., the packet  has been processed by the fourth stage  of the processing pipeline ) of the packet  in the set of fields of the packet 's header.","Finally, the managed switching element  performs the context mapping of the stage  and the physical mapping of the stage  is a similar manner was that described above by reference to .","While  show examples of distributing logical processing across managed switching elements in a managed network, in some instance, some or all of the logical processing may need to be processed again. For instance, in some embodiments, a root node does not preserve the logical context of a packet. Thus, when a pool node receives a packet from the root node of such embodiments (e.g., when a patch bridge of a pool node receives a packet from a root bridge, which are illustrated in ), the pool node may have to perform the logical processing of the processing pipeline due to the lack of a logical context in the packet.",{"@attributes":{"id":"p-0433","num":"0432"},"figref":["FIG. 44","FIG. 45","FIG. 45"],"b":["4000","4500","1","1","1","1","2","2","2","2","1","2","3","1","1","2","1","2","1","2","2","1","1"]},"As shown, flow entry  is for performing physical to logical mapping (i.e., ingress context mapping). The flow entry  specifies that when a packet is received on port , the packet's VLAN id is to be modified to  and the packet is to be submitted to port , which is the dispatch port. The VLAN id of  represents the context of the packet and indicates that the packet has been received on port  of the managed switching element .","Flow entry  is for modifying the packet's context to indicate that the packet is at the start of logical processing (e.g., stages - of the processing pipeline ) of the processing pipeline. As shown, the flow entry  specifies that when a packet is received on port  and the packet's VLAN id is , the packet's VLAN id is to be modified to  and the packet is to be submitted to port , which is the dispatch port. The VLAN id of  represents the context of the packet and indicates that the packet is at the start of the logical processing of the processing pipeline.","Next, flow entry  is for performing an ingress ACL lookup. As shown, the flow entry  specifies that when a packet is received on port  and the packet's VLAN id is 2054, the packet's VLAN id is to be modified to  and the packet is to be submitted to port , which is the dispatch port. The VLAN id of  represents the context of the packet and indicates that the packet has been processed by the ingress ACL and allowed through the ingress ACL.","Flow entries - are for performing logical lookups. The flow entry  specifies that when a packet is received on port , the packet's VLAN id is 2055, and the packet's destination MAC address is 00:23:20:01:01:01, the packet's VLAN id is to be modified to  and the packet is to be submitted to port , which is the dispatch port. The VLAN id of  represents the context of the packet and indicates that the packet is to be sent to the VM .","The flow entry  specifies that when a packet is received on port , the packet's VLAN id is , and the packet's destination MAC address is 00:23:20:03:01:01, the packet's VLAN id is to be modified to  and the packet is to be submitted to port , which is the dispatch port. The VLAN id of  represents the context of the packet and indicates that the packet is to be sent to the VM .","The flow entry  specifies that when a packet is received on port , the packet's VLAN id is 2055, and the packet's destination MAC address is ff:ff:ff:ff:ff:ff, the packet's VLAN id is to be modified to  and the packet is to be submitted to port , which is the dispatch port. The VLAN id of  represents the context of the packet and indicates that the packet is a broadcast packet.","As shown, flow entry  is for performing logical to physical mapping (i.e., egress context mapping). The flow entry  specifies that when a packet is received on port , and the packet's VLAN id is , the packet's VLAN id is to be stripped (i.e., removed) and the packet is to be submitted to port  which is the port to which VM  is coupled. Thus, the flow entry  is for sending the packet to VM .","Flow entry  is for performing logical to physical mapping (i.e., egress context mapping). As illustrated in , the flow entry  specifies that when a packet is received on port  and the packet's VLAN id is , the packet's VLAN id is to be modified to  and the packet is to be submitted to port , which is the port to the tunnel (i.e., a tunnel port) that couples the managed switching element  to the managed switching element . As such, the flow entry  is for sending the packet to the host .","Next, flow entry  is for processing a broadcast packet. Specifically, the flow entry  specifies that when a packet is received on port  and the packet's VLAN id is 2050, the packet's VLAN id is to be modified to  and the packet is to be submitted to port , which is the dispatch port. In addition, the flow entry  specifies that when a packet is received on port  and the packet's VLAN id is , the packet's VLAN id is to be modified to  and a copy of the packet is to be submitted to port . Therefore, the flow entry  is for sending a broadcast packet to the VM  and to other VMs in the same logical network as the VM , which include the VM  in this example.","Flow entry  is for sending a broadcast packet to the pool node. As shown in , the flow entry  specifies that when a packet is received on port  and the packet's VLAN id is , the packet's VLAN id is to be modified to  and the packet is to be submitted to port , which is the port to the tunnel (i.e., a tunnel port) that couples the managed switching element  to the pool node. As mentioned above, the VLAN id of  represents the context of the packet and indicates that the packet is a broadcast packet.","As shown, flow entry  is for performing logical to physical mapping (i.e., egress context mapping). The flow entry  specifies that when a packet is received on port , which is the tunnel (i.e., a tunnel port) that couples the managed switching element  to the managed switching element , and the packet's VLAN id is , the packet's VLAN id is to be modified to  and the packet is to be submitted to port , which is the dispatch port. Therefore, the flow entry  is for sending the packet, which is received from the managed switching element , to the VM .","Next, flow entry  is for performing logical to physical mapping (i.e., egress context mapping). As illustrated, the flow entry  specifies that when a packet is received on port , which is the tunnel (i.e., a tunnel port) that couples the managed switching element  to the pool node, and the packet's VLAN id is , the packet's VLAN id is to be modified to  and the packet is to be submitted to port , which is the dispatch port. As such, the flow entry  is for sending the packet, which is received from the pool node, to the VM .","Flow entry  is for performing a logical lookup. Specifically, the flow entry  is for sending all packets with unknown destination MAC addresses to an pool node via an uplink. As shown in , the flow entry  specifies that when a packet is received on port  and the packet's VLAN id is , the packet's VLAN id is to be modified to  and the packet is to be submitted to port , which is the dispatch port. The VLAN id of  represents the context of the packet and indicates that the packet is a packet with an known MAC address. In addition, the flow entry  includes a priority value that is lower that the flow entries -, which are also for performing logical lookups. Since the priority value of the flow entry  is lower than all the other flow entries, the flow entry  is evaluated after all the other flow entries have been evaluated against the packet. Thus, the flow entry  is for sending a packet with an unknown MAC address to the pool node.","Finally, flow entry  is for sending a packet with an unknown MAC address to the pool node. As illustrated in , the flow entry  specifies that when a packet is received on port  and the packet's VLAN id is , the packet's VLAN id is to be modified to  and the packet is to be submitted to port , which is the port to the tunnel (i.e., a tunnel port) that couples the managed switching element  to the pool node. As mentioned above, the VLAN id of  represents the context of the packet and indicates that the packet is a packet with unknown MAC address.",{"@attributes":{"id":"p-0448","num":"0447"},"figref":"FIG. 44"},"VI. Computer System","Many of the above-described features and applications are implemented as software processes that are specified as a set of instructions recorded on a computer readable storage medium (also referred to as computer readable medium). When these instructions are executed by one or more processing unit(s) (e.g., one or more processors, cores of processors, or other processing units), they cause the processing unit(s) to perform the actions indicated in the instructions. Examples of computer readable media include, but are not limited to, CD-ROMs, flash drives, RAM chips, hard drives, EPROMs, etc. The computer readable media does not include carrier waves and electronic signals passing wirelessly or over wired connections.","In this specification, the term \u201csoftware\u201d is meant to include firmware residing in read-only memory or applications stored in magnetic storage which can be read into memory for processing by a processor. Also, in some embodiments, multiple software inventions can be implemented as sub-parts of a larger program while remaining distinct software inventions. In some embodiments, multiple software inventions can also be implemented as separate programs. Finally, any combination of separate programs that together implement a software invention described here is within the scope of the invention. In some embodiments, the software programs, when installed to operate on one or more electronic systems, define one or more specific machine implementations that execute and perform the operations of the software programs.",{"@attributes":{"id":"p-0451","num":"0450"},"figref":"FIG. 46","b":["4600","4600","4600","4605","4610","4620","4625","4630","4635","4640","4645"]},"The bus  collectively represents all system, peripheral, and chipset buses that communicatively connect the numerous internal devices of the electronic system . For instance, the bus  communicatively connects the processing unit(s)  with the read-only memory , the GPU , the system memory , and the permanent storage device .","From these various memory units, the processing unit(s)  retrieve instructions to execute and data to process in order to execute the processes of the invention. The processing unit(s) may be a single processor or a multi-core processor in different embodiments. Some instructions are passed to and executed by the GPU . The GPU  can offload various computations or complement the image processing provided by the processing unit(s) . In some embodiments, such functionality can be provided using CoreImage's kernel shading language.","The read-only-memory (ROM)  stores static data and instructions that are needed by the processing unit(s)  and other modules of the electronic system. The permanent storage device , on the other hand, is a read-and-write memory device. This device is a non-volatile memory unit that stores instructions and data even when the electronic system  is off. Some embodiments of the invention use a mass-storage device (such as a magnetic or optical disk and its corresponding disk drive) as the permanent storage device .","Other embodiments use a removable storage device (such as a floppy disk, flash drive, or ZIP\u00ae disk, and its corresponding disk drive) as the permanent storage device. Like the permanent storage device , the system memory  is a read-and-write memory device. However, unlike storage device , the system memory is a volatile read-and-write memory, such a random access memory. The system memory stores some of the instructions and data that the processor needs at runtime. In some embodiments, the invention's processes are stored in the system memory , the permanent storage device , and\/or the read-only memory . For example, the various memory units include instructions for processing multimedia clips in accordance with some embodiments. From these various memory units, the processing unit(s)  retrieve instructions to execute and data to process in order to execute the processes of some embodiments.","The bus  also connects to the input and output devices  and . The input devices enable the user to communicate information and select commands to the electronic system. The input devices  include alphanumeric keyboards and pointing devices (also called \u201ccursor control devices\u201d). The output devices  display images generated by the electronic system. The output devices include printers and display devices, such as cathode ray tubes (CRT) or liquid crystal displays (LCD). Some embodiments include devices such as a touchscreen that function as both input and output devices.","Finally, as shown in , bus  also couples electronic system  to a network  through a network adapter (not shown). In this manner, the computer can be a part of a network of computers (such as a local area network (\u201cLAN\u201d), a wide area network (\u201cWAN\u201d), or an Intranet, or a network of networks, such as the Internet. Any or all components of electronic system  may be used in conjunction with the invention.","Some embodiments include electronic components, such as microprocessors, storage and memory that store computer program instructions in a machine-readable or computer-readable medium (alternatively referred to as computer-readable storage media, machine-readable media, or machine-readable storage media). Some examples of such computer-readable media include RAM, ROM, read-only compact discs (CD-ROM), recordable compact discs (CD-R), rewritable compact discs (CD-RW), read-only digital versatile discs (e.g., DVD-ROM, dual-layer DVD-ROM), a variety of recordable\/rewritable DVDs (e.g., DVD-RAM, DVD-RW, DVD+RW, etc.), flash memory (e.g., SD cards, mini-SD cards, micro-SD cards, etc.), magnetic and\/or solid state hard drives, read-only and recordable Blu-Ray\u00ae discs, ultra density optical discs, any other optical or magnetic media, and floppy disks. The computer-readable media may store a computer program that is executable by at least one processing unit and includes sets of instructions for performing various operations. Examples of computer programs or computer code include machine code, such as is produced by a compiler, and files including higher-level code that are executed by a computer, an electronic component, or a microprocessor using an interpreter.","While the above discussion primarily refers to microprocessor or multi-core processors that execute software, some embodiments are performed by one or more integrated circuits, such as application specific integrated circuits (ASICs) or field programmable gate arrays (FPGAs). In some embodiments, such integrated circuits execute instructions that are stored on the circuit itself.","As used in this specification and any claims of this application, the terms \u201ccomputer\u201d, \u201cserver\u201d, \u201cprocessor\u201d, and \u201cmemory\u201d all refer to electronic or other technological devices. These terms exclude people or groups of people. For the purposes of the specification, the terms display or displaying means displaying on an electronic device. As used in this specification and any claims of this application, the terms \u201ccomputer readable medium\u201d and \u201ccomputer readable media\u201d are entirely restricted to tangible, physical objects that store information in a form that is readable by a computer. These terms exclude any wireless signals, wired download signals, and any other ephemeral signals.","While the invention has been described with reference to numerous specific details, one of ordinary skill in the art will recognize that the invention can be embodied in other specific forms without departing from the spirit of the invention. In addition, a number of the figures (including , , , , , and ) conceptually illustrate processes. The specific operations of these processes may not be performed in the exact order shown and described. The specific operations may not be performed in one continuous series of operations, and different specific operations may be performed in different embodiments. Furthermore, the process could be implemented using several sub-processes, or as part of a larger macro process."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The novel features of the invention are set forth in the appended claims. However, for purposes of explanation, several embodiments of the invention are set forth in the following figures.",{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0032","num":"0031"},"figref":["FIG. 17","FIG. 16"]},{"@attributes":{"id":"p-0033","num":"0032"},"figref":["FIG. 18","FIG. 16"]},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 19"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 20"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 21"},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 22"},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 23"},{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 24"},{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 25"},{"@attributes":{"id":"p-0041","num":"0040"},"figref":"FIG. 26"},{"@attributes":{"id":"p-0042","num":"0041"},"figref":"FIG. 27"},{"@attributes":{"id":"p-0043","num":"0042"},"figref":"FIG. 28"},{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 29"},{"@attributes":{"id":"p-0045","num":"0044"},"figref":"FIG. 30"},{"@attributes":{"id":"p-0046","num":"0045"},"figref":"FIG. 31"},{"@attributes":{"id":"p-0047","num":"0046"},"figref":"FIG. 32"},{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIG. 33"},{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIG. 34"},{"@attributes":{"id":"p-0050","num":"0049"},"figref":"FIG. 35"},{"@attributes":{"id":"p-0051","num":"0050"},"figref":"FIG. 36"},{"@attributes":{"id":"p-0052","num":"0051"},"figref":"FIGS. 37A-F"},{"@attributes":{"id":"p-0053","num":"0052"},"figref":"FIG. 38"},{"@attributes":{"id":"p-0054","num":"0053"},"figref":"FIGS. 47A-C"},{"@attributes":{"id":"p-0055","num":"0054"},"figref":"FIGS. 48A-C"},{"@attributes":{"id":"p-0056","num":"0055"},"figref":"FIG. 39"},{"@attributes":{"id":"p-0057","num":"0056"},"figref":"FIG. 40"},{"@attributes":{"id":"p-0058","num":"0057"},"figref":"FIG. 41"},{"@attributes":{"id":"p-0059","num":"0058"},"figref":"FIG. 42"},{"@attributes":{"id":"p-0060","num":"0059"},"figref":"FIG. 43"},{"@attributes":{"id":"p-0061","num":"0060"},"figref":"FIG. 44"},{"@attributes":{"id":"p-0062","num":"0061"},"figref":"FIG. 45"},{"@attributes":{"id":"p-0063","num":"0062"},"figref":"FIG. 46"}]},"DETDESC":[{},{}]}
