---
title: Determining a class associated with an image
abstract: The technology is directed to determining a class associated with an image. In some examples, a method determines the class associated with an image. The method can include determining a segmentation score for an image segment based on a comparison of the image segment and a region of an image. The region of the image can be associated with the image segment. The method further includes determining a confidence score for the image segment based on the segmentation score and a classification score. The classification score can be indicative of a similarity between the image segment and at least one class. The method further includes determining a class associated with the image based on the confidence score. The method further includes outputting the class associated with the image.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08620078&OS=08620078&RS=08620078
owner: Matrox Electronic Systems, Ltd.
number: 08620078
owner_city: 
owner_country: CA
publication_date: 20100714
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS REFERENCES TO RELATED APPLICATIONS","FIELD OF THE INVENTION","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["This application claims priority to the provisional patent application entitled \u201cDetermining a Classification of an Image Segment,\u201d U.S. Provisional Patent Application No. 61\/225,544, filed on Jul. 14, 2009, the disclosure of which is hereby incorporated herein by reference.","The present invention relates generally to determining a class associated with an image.","The process of recognizing one or more objects or patterns in an image is object or pattern recognition (e.g., optical character recognition, face detection, vehicle detection, etc.). Object recognition has applications in a variety of fields, such as automated manufacturing, biomedical engineering, security, and document analysis.","Optical character recognition (OCR) consists of recognizing a string of characters in an image and returning a corresponding string of characters (e.g., in text form). OCR has a wide range of applications including the recognition of vehicle license plate numbers (e.g., for use in automated traffic law enforcement, surveillance, access control, tolls, etc.), the recognition of serial numbers on parts in an automated manufacturing environment, the recognition of labels on packages for routing purposes, and various document analysis applications.","The utilization of object recognition for a machine vision system is challenging due to image issues such as: changes in character angle with respect to a string, aspect ratio, scale, skew, lighting (e.g., non-uniform, reflection), and overlapping images.","Thus, a need exists to improve the determination of a class associated with an image as described herein.","One approach to determining a class associated with an image is a method. The method includes determining a segmentation score for an image segment based on a comparison of the image segment and a region of an image. The region of the image is associated with the image segment. The method further includes determining a confidence score for the image segment based on the segmentation score and a classification score. The classification score is indicative of a similarity between the image segment and at least one class. The method further includes determining a class associated with the image based on the confidence score. The method further includes outputting the class associated with the image.","Another approach to determining a class associated with an image is a computer program product. The computer program product is tangibly embodied in an information carrier. The computer program product includes instructions being operable to cause a data processing apparatus to determine a segmentation score for an image segment based on a comparison of the image segment and a region of an image. The region of the image is associated with the image segment. The computer program product further includes instructions being operable to cause a data processing apparatus to determine a confidence score for the image segment based on the segmentation score and a classification score. The classification score is indicative of a similarity between the image segment and at least one class. The computer program product further includes instructions being operable to cause a data processing apparatus to determine a class associated with the image based on the confidence score. The computer program product further includes instructions being operable to cause a data processing apparatus to output the class associated with the image.","Another approach to determining a class associated with an image is a system. The system includes a segmentation score module, a confidence score module, a class determination module, and a class output module. The segmentation score module is configured to determine a segmentation score for an image segment based on a comparison of the image segment and a region of an image. The region of the image is associated with the image segment. The confidence score module is configured to determine a confidence score for the image segment based on the segmentation score and a classification score. The classification score is indicative of a similarity between the image segment and at least one class. The class determination module is configured to determine a class associated with the image based on the confidence score. The class output module is configured to output the class associated with the image.","In other examples, any of the approaches above can include one or more of the following features.","In some examples, the method further includes determining a second segmentation score for a second image segment based on a comparison of the second image segment and a second region of the image. The second region of the image is associated with the second image segment.","In other examples, the method further includes determining a second confidence score for the second image segment based on the second segmentation score and a second classification score. The second classification score is indicative of a similarity between the second image segment and at least one class.","In some examples, the method further including determining the class associated with the image based on the confidence score and the second confidence score.","In other examples, the method further includes generating the image segment based on a first segmentation of the image and\/or generating the second image segment based on a second segmentation of the image. The second image segment can be associated with the region of the image associated with the image segment.","In some examples, the method further includes generating the image segment and the second image segment based on segmentation of substantially distinct regions of the image.","In other examples, the method further includes comparing pixels of the image segment with pixels of the region of the image to form the comparison of the image segment and the region of an image.","In some examples, the method further includes determining the classification score indicative of the similarity between the image segment and the at least one class.","In other examples, the method further includes determining the at least one class based on a comparison of the image segment with a plurality of classes. Each of the plurality of classes is associated with a character.","In some examples, the method further includes receiving the image from an image acquisition device, binarizing the image to form a binarized image and segmenting the binarized image based on objects within the binarized image to form the image segment.","In other examples, wherein the determining the class associated with the image further includes determining the class associated with the image based on the confidence score by comparing the confidence score to a threshold value.","In some examples, the system further includes the segmentation score module further configured to determine a second segmentation score for a second image segment based on a comparison of the second image segment and a second region of the image. The second region of the image can be associated with the second image segment.","In other examples, the system further includes the confidence score module further configured to determine a second confidence score for the second image segment based on the second segmentation score and a second classification score. The second classification score can be indicative of a similarity between the second image segment and at least one class.","In some examples, the system further includes the class determination module further configured to determine the class associated with the image based on the confidence score and the second confidence score.","In other examples, the system further includes a segmentation module configured to generate the image segment based on a first segmentation of the image; and generate the second image segment based on a second segmentation of the image. The second image segment can be associated with the region of the image associated with the image segment.","In some examples, the system further includes a segmentation module configured to generate the image segment and the second image segment based on segmentation of substantially distinct regions of the image.","In other examples, the system further includes the segmentation score module further configured to compare pixels of the image segment with pixels of the region of the image to form the comparison of the image segment and the region of an image.","In some examples, the system further includes a classification score module configured to determine the classification score indicative of the similarity between the image segment and the at least one class.","In other examples, the system further includes a classification score module configured to determine the at least one class based on a comparison of the image segment with a plurality of classes. Each of the plurality of classes can be associated with a character.","In some examples, the system further includes an image acquisition module configured to receive the image. The system can further include a segmentation module configured to binarize the image to form a binarized image and segment the binarized image based on objects within the binarized image to form the image segment.","In other examples, the system further includes the class determination module further configured to determine the class associated with the image based on the confidence score by comparing the confidence score to a threshold value.","The class determination techniques described herein can provide one or more of the following advantages. An advantage of the technology is the ability to identity characters in a string with a known or unknown number of evenly or proportionally spaced characters, thereby enabling increased efficiency in optical character recognition (OCR) applications by not requiring a known number of evenly spaced characters. An additional advantage of the technology is that the integration of the segmentation score and the classification score in the determination of the confidence score enables the identification of classes (e.g., characters, numbers, symbols, etc.) that do not use a model or template (e.g., known character font, known symbol location and orientation, etc.), thereby increasing the efficiency of the technology across a wide range of applications (e.g., machine component identification, multi-nation vehicle tag identification, etc.). Another advantage of the technology is that use of classes that can be trained to identify the image segment enables the identification of strings that are uneven, skewed, or have other deformities, thereby increasing the industrial applications of the technology to any type of application via the use of a training set of sample objects.","Other aspects and advantages of the present invention will become apparent from the following detailed description, taken in conjunction with the accompanying drawings, illustrating the principles of the invention by way of example only.","As a general overview of the technology, the technology is directed to classifying an image based on one or more classes. For example, a class can be associated with a physical entity (e.g., manufactured part, vehicle, person, animal, organ, etc.), a character (e.g., alphanumeric character), and\/or a character string (e.g., word, serial number, etc.). The class can be utilized in various machine vision applications (e.g., optical character recognition (OCR), parts identification, security screening, face detection, license plate number recognition, etc.).","For example, in an image containing the text \u201cLZ8342\u201d, six image segments can be isolated and classified as classes \u201cLetter L\u201d, \u201cLetter Z\u201d, \u201cDigit 8\u201d, \u201cDigit 3\u201d, \u201cDigit 4\u201d, and \u201cDigit 2\u201d. These classes can then be output to a requesting application, such as an automated toll collection or garage entry system based on license plate numbers, mail sorting system, medical diagnostic application, and\/or any other type of application that utilizes image classes.","As another example, an image depicting a vehicle entering a parking garage is analyzed to determine the vehicle make, model, color, and year (e.g., for a statistical analysis of the vehicles parked in a parking garage, for an automated analysis for building security, etc.). The technology analyzes the image to isolate the vehicle in the image and determine the vehicle make, model, color, and year based on classifications of vehicles (e.g., a classification for every vehicle manufactured in the world, etc.). The technology can advantageously identify objects in an image with known or unknown parameters (e.g., lighting, angle, color, modifications, accessories, etc.), thereby enabling increased efficiency in object recognition applications by not requiring set parameters for the image (e.g., no modifications to vehicle, no accessories on the vehicle, no color variations, etc.).","As a further general overview of the technology, the technology can include, for example, one or more of the following steps. It should be understood that the following steps can occur in any order and\/or can be executed on any type of computing system or device that can execute computer-executable instructions (e.g., a personal computer, a network server, an intelligent camera, a stand-alone image processor, a mobile device, etc.).","In a first step, an image (e.g., grayscale image, color image, binary image, color filter array or bayer image, etc.) including one or more objects to be recognized is acquired. This step can be referred to as \u201cimage acquisition,\u201d and the image can be referred to as \u201cimage\u201d or \u201cacquired image\u201d or \u201coriginal image.\u201d It should be understood that various image processing operations (e.g., format conversion, compression\/decompression, demosaicing, cropping, noise reduction, contrast enhancement, filtering, morphological transformation, etc.) can be applied to the image before the image is processed as described herein. Thus, the image processed by the technology described herein can be the acquired image or a processed version of the acquired image. Also, it should be understood that any means of image acquisition (e.g., camera, scanner, image database, etc.) known in the art can be used. For example, the image can be acquired from a digital or analog camera connected, directly or indirectly, to the computing system (or device) or acquired from a database of images stored locally on the computing system or remotely (e.g., on a network server).","In a second step, the image is analyzed to isolate individual objects from each other and from the background. This step can be referred to as \u201csegmentation.\u201d The individual objects isolated in the image can be referred to as \u201cimage segments.\u201d The segmentation step can include one or more pre-processing operations and\/or processing operations. This segmentation step can be repeated using different segmentation techniques, segmentation parameters, and\/or segmentation pre-processing operations to form a plurality of image segments for each region in the image (each region containing, for example, a single object to be recognized). For example, as illustrated in , multiple image segments , and can be generated, using different segmentation parameters, from an image  depicting a single character \u201cA\u201d.","For example, the segmentation of an image can be performed in two steps. First, the image is binarized by applying a global or local threshold value to each pixel. This step can be referred to as \u201cbinarization\u201d and the output can be referred to as \u201cbinarized image.\u201d Second, connected pixels of the same binary value are grouped into objects. This step can be referred to as \u201cconnected component\u201d or \u201cblob\u201d analysis and the output can be referred to as \u201cbinarized image segments.\u201d If the image is binarized and the individual objects are isolated in the binarized image, the individual objects isolated in the binarized image can be referred to as \u201cbinarized image segments.\u201d","In some examples, the segmentation of an image can further include a step of combining or merging the connected components. For example, an image segment representing a complete letter \u201ci\u201d can be obtained by combining two connected components corresponding to the dot and the body, or an image segment representing a complete letter \u201cL\u201d can be obtained by merging two connected components corresponding to the vertical and horizontal bars (which may have been split during the image acquisition or binarization process, for example).","In other examples, the image segments can be obtained utilizing other segmentation techniques known in the art, such as, segmentation techniques based directly on the grayscale or color image, based on detected edges or contours, based on horizontal and\/or vertical intensity projections, based on detected corners, watershed transformations, etc., and the output can be referred to as \u201cimage segments.\u201d The image segments can refer to the binarized image segments and\/or the image segments obtained by other techniques. It should be understood that any suitable technique of segmentation, binarization, and\/or connected component analysis known in the art can be utilized.","In a third step, a \u201csegmentation score\u201d is determined based on a comparison of an image segment and a corresponding region in the non-segmented image (e.g., in the acquired grayscale or color image or in a processed version thereof). The segmentation score can provide a measure of confidence of the segmentation of the image segment indicating to what degree the image segment \u201cresembles\u201d the corresponding region in the non-segmented image. In the case of multiple image segments, such as resulting from the segmentation of an image or region containing multiple objects and\/or from multiple segmentations of an image or region containing a single object, a segmentation score can be determined for each of the image segments.","In a fourth step, a classification score is determined for an image segment. The classification score can indicate the degree to which the image segment exhibits the characteristics associated with a particular class. For example, the classification score can be a score returned by a \u201cclassifier\u201d, such as a \u201cstatistical\u201d or \u201cartificial intelligence\u201d-based classifier.","In the case of a single class, a classifier can classify the image segment as belonging or not to a particular class or category of objects. For example, a classifier for the alphanumeric character class \u201cA\u201d can classify an image segment (e.g., representing an isolated character) as belonging or not to the alphanumeric character class \u201cA\u201d. As another example, the classifier for a part class \u201cwidget\u201d can classify an image segment (e.g., representing a single part in a machine) as belonging or not to the part class \u201cwidget\u201d.","The classifier can return a classification score representing the confidence with which the classifier classifies the image segment as belonging to the particular class. For example, the classification score can indicate the degree to which the image segment exhibits the characteristics associated with the particular class (e.g., characteristics of a letter, part, human, vehicle, etc.). For example, characteristics can include geometric features or feature vectors, geometric moments, edge or contour characteristics, skeleton characteristics, wavelet transformation coefficients, number of holes, etc., as well as any combination of these and\/or other characteristics.","In the case of multiple classes (e.g., character classes A-Z and 0-9, multiple part classes, etc.), the classifier can, for example, determine a classification score for the image segment and each of the classes, in turn, and then return one or more classes that yielded the highest classification scores.","In the case of multiple image segments, such as resulting from multiple objects and\/or multiple segmentations, a classification score can be determined for each of the image segments.","In a fifth step, a \u201cconfidence score\u201d is determined for an image segment based on the segmentation score and the classification score (e.g., by multiplying the segmentation score and the classification score, by adding the segmentation score and the classification score, by averaging the segmentation score and the classification score, by weighting the segmentation score and the classification score, etc.). The confidence score can be indicative of the degree to which the non-segmented image (or a region of the non-segmented image corresponding to the image segment) exhibits the characteristics associated with the particular class or the confidence with which a classifier would classify the non-segmented image (or image region) as belonging to the particular class. As mentioned, the non-segmented image can be the acquired grayscale or color image or a processed version thereof. In the case of multiple image segments, such as resulting from multiple objects and\/or multiple segmentations, a confidence score can be determined for each of the image segments.","In a sixth step, a class associated with the image is determined based on the confidence score. In the case of a single image segment, the image segment and its corresponding classification can be validated or rejected based on the confidence score and one or more criteria, such as, by comparing the confidence score to a threshold value (e.g., defined by the user, dynamically generated based on the image, etc.). In the case of multiple image segments, one or more of the image segments and their corresponding classifications can be selected based on the confidence scores and one or more criteria. For example, the criteria can include: Selecting a predefined number of the image segments and their corresponding classifications that yielded the highest confidence scores; Selecting all or a predefined number of the image segments and their corresponding classifications whose confidence scores are above a threshold value; etc. Note that the threshold value can be predefined (e.g., defined by the user) or dynamically determined (e.g., determined based on the average, mean, median, deviation from the mean or mode of the confidence scores of other image segments associated with the image, etc.).","In a final step, the determined class or classes associated with the image are output. The corresponding confidence score(s) and\/or image segment(s) can also be output, as required by the application. The output class can be, for example, the ASCII character corresponding to the character class (e.g., A, B, etc.), a class index (e.g., class A5, class C523, etc.), a \u201cNULL\u201d identifier (e.g., if the classification failed) and\/or any other type of class identifier. The class(es), confidence score(s) and\/or image segment(s) can be output to a display device (e.g., computer monitor, etc.), a computer application program (e.g., for quality assurance, access control, etc.), an output device (e.g., printer, text to speech converter, etc.), and\/or any other type of output mechanism (e.g., sent via a transmission control protocol packet to another computing system, sent via an application program interface (API) return call to a requesting computer executable program, sent via a simple object access protocol (SOAP) message to a factory machine, etc.).","An advantage of the technology is that the integration of the segmentation score and the classification score in the determination of the confidence score enables the identification of classes (e.g., characters, numbers, symbols, parts, vehicles, animals, humans, etc.) that do not use a model or template (e.g., known character font, known symbol location and orientation, known background, known lighting, known size, etc.), thereby increasing the efficiency of the technology across a wide range of applications (e.g., machine component identification, multi-nation vehicle tag identification, security identification, etc.). Another advantage of the technology is that use of classes that can be trained to identify the image segment enables the identification of strings that are uneven, skewed, or have other deformities, thereby increasing the industrial applications of the technology to any type of application via the use of a training set of sample objects.",{"@attributes":{"id":"p-0071","num":"0070"},"figref":["FIG. 1","FIG. 1"],"b":["102","102","104","116","108","116","108"]},"The image acquisition device  (which may, for example, be a digital camera) is arranged to acquire an image of a desired field of view within a world space of the system . This world space may, for example, be defined within an inspection station (not shown) of a production line, in order to enable recognition and localization of objects passing through the inspection station (e.g., part number, lot number, expiration date, shipping date, shipping identification, etc.). It should be understood that other types of image acquisition devices (e.g., electromagnetic imaging devices such as radar and nuclear magnetic resonance imaging systems; or ultrasonic imaging systems etc.) can be employed, as can be appropriate for the desired view. In any event, the world space definition includes a world surface  providing a visual reference frame.","The system  operates to recognize one or more objects  laying on the world surface . The system  can detect objects  in the image  and can determine a classification for one or more of the detected objects  (such as, the classifications\u2014\u201czero\u201d \u201ctwo\u201d \u201ctwo\u201d \u201czero\u201d \u201czero\u201d \u201cnine\u201d). The system  outputs the classifications for the objects and\/or any other associated information (e.g., the confidence scores, the image with the detected objects in annotation, etc.) to a display device and\/or to another computer-executable program (e.g., quality assurance computer application, software operating a robotic arm, etc.) of the system  or of a remote system (not shown).","The segmentation step as described herein can be essential to the overall object recognition process. Poor image segmentation can result in incorrect classification of the objects appearing in the non-segmented image (e.g., the acquired grayscale or color image or a processed version thereof). The importance of the segmentation step is illustrated by the OCR examples of .",{"@attributes":{"id":"p-0075","num":"0074"},"figref":"FIG. 2","b":["210","211","212","213","211","212","213"]},{"@attributes":{"id":"p-0076","num":"0075"},"figref":"FIG. 3","b":["320","210","321","322","320","213","210","322","322","213","211","212","210","321","321"]},{"@attributes":{"id":"p-0077","num":"0076"},"figref":"FIG. 4","b":["430","210","431","432","433","430","211","212","210","431","432","213","210","213","210","433"]},"As illustrated, it is not always possible to find a threshold value that properly segments all objects appearing in the original grayscale image. The problem is that classification is based on the segmented object in the binary image, not on the object as it appears in the original grayscale or color image. In , the accuracy of the segmentation process is not taken into account during classification and thus, image segments in  are improperly classified as a character. An advantage of confidence score is that the determination of the confidence score utilizes both the segmentation score, i.e., the accuracy of the segmentation process, and the classification score, i.e., the accuracy of the classification process, to determine the classification of an image segment, thereby decreasing mis-classifications and increasing the accuracy of the technology. Another advantage of the technology is the recursive determinations of the segmentation score (e.g., determining the segmentation score for multiple image segments associated with the same region of the image) and the classification score (e.g., comparing each image segment to every classification) to determine the image segment with the highest confidence score, thereby decreasing mis-classifications by identifying the image segment that closely resembles the region in the image.",{"@attributes":{"id":"p-0079","num":"0078"},"figref":["FIG. 5","FIG. 5"],"b":["500","500","510","510","511","512","513","514","515","516","517","518","521","522","523","525","526","525","510"]},"The transceiver  transmits and\/or receives images, classifications, confidence scores and\/or other data (e.g., user preferences, analysis results, configuration parameters, system information, etc.) to\/from the computing system .","The image acquisition module  receives an image from an image acquisition device (not shown), from storage device , from transceiver , from input device , etc.","The segmentation module  generates the image segment(s) based on a segmentation of the image. The segmentation module  can generate the image segment(s) utilizing any of the techniques described herein.","The segmentation score module  determines a segmentation score for an image segment based on a comparison of the image segment and a region of an image. The region of the image is associated with the image segment. The segmentation score module  can determine the segmentation score utilizing any of the techniques described herein.","The classification score module  determines the classification score indicative of the similarity between the image segment and the at least one class. The classification score module  can determine the classification score utilizing any of the techniques described herein.","The confidence score module  determines a confidence score for the image segment based on the segmentation score and a classification score. The confidence score module  can determine the confidence score utilizing any of the techniques described herein.","The class determination module  determines a class associated with the image based on the confidence score. The class determination module  can determine the class utilizing any of the techniques described herein.","The class output module  outputs the class associated with image. The class output module  can output the class utilizing any of the techniques described herein.","The output device  outputs images, image segments, classifications, scores and\/or any other data associated with the computing system  to a printer, speaker, etc. The input device  receives information associated with the computing system  (e.g., user-defined criteria, user specifications, etc.) from a user (not shown) and\/or another computing system (not shown). The input device  can include, for example, a keyboard, mouse, touchscreen, scanner, etc.","The display device  displays the images, image segments, classifications, scores and\/or any other data associated with the computing system . The processor  executes the operating system and\/or any other computer-executable instructions for the computing system  (e.g., any of modules  to , a graphical user interface, other application programs, etc.).","The storage device  stores images, image segments, classifications, scores, and\/or any other data associated with the computing system . The storage device  can include, for example, long-term storage (e.g., a hard drive, a tape storage device, flash memory, etc.), short-term storage (e.g., a random access memory, a graphics memory, etc.), and\/or any other type of computer readable storage.",{"@attributes":{"id":"p-0091","num":"0090"},"figref":"FIG. 6","b":["600","600","620","630","620","625","626","630","635","630","635","620","620","620","510"]},"For example, the camera  acquires the image  illustrated in . The camera  transmits the image  to the computing system  via the transceiver . The computing system  analyzes the image  utilizing the technology as described herein and outputs the class associated with the image based on the confidence scores determined for each image segment utilizing the different thresholds. In other words, the computing system  determines that the image  includes the classes of \u201cN\u201d \u201cM\u201d and NULL utilizing both of the binary images  and .",{"@attributes":{"id":"p-0093","num":"0092"},"figref":"FIG. 7","b":["700","700","730","740","730","735","730","735","735","740","735","510"]},{"@attributes":{"id":"p-0094","num":"0093"},"figref":"FIG. 8","b":["800","800","850","830","830","830","830","840","850","855","855","830","830","830","835","835","835","835","830","835","850","840","855","855","855","855","510"],"i":["a","b","c ","a ","b","a","b","c ","a","b","c ","a ","b ","a ","b "]},{"@attributes":{"id":"p-0095","num":"0094"},"figref":"FIG. 9","b":["900","900","910","910","951","952","953","954","955","956","957","958","959","959","510"]},"The antenna  is utilized to receive and\/or transmit data signals and may be constructed from any known antenna materials. The housing  is a casing utilized to hold the components of the computing system  (e.g., components illustrated in the computing device  of ). The casing may be constructed from any known casing materials (e.g., plastic, metal, etc.). The speaker  is utilized to reproduce audio. The communication device  may include, for example, a speaker output (not shown) that is utilized by an external speaker (e.g., head set, third-party speakers, wireless connected speakers, etc.) to reproduce audio.","The display  displays information associated with the image and\/or the class associated with the image. The keypad  is utilized for input of selections and\/or other input information (e.g., name, phone number, etc.). The microphone  is utilized for input of audio data (e.g., voice call, instructions, audio recording, etc.). The communication device  may include, for example, a microphone output (not shown) that is utilized by an external microphone (e.g., head set, third-party microphone, wireless connected microphone, etc.) to input audio. The storage  is utilized to store data (e.g., store multimedia data parts, retrieve multimedia data parts, store phone numbers, etc.). The storage  may be any type of memory storage including a removable memory storage and\/or a permanent memory storage.","The camera  acquires an image and the computing system  processes the image to determine a class utilizing the technology as described herein. The computing system  can communication the determined class and\/or the image to other devices (not shown) utilizing the network (not shown).",{"@attributes":{"id":"p-0099","num":"0098"},"figref":["FIG. 10","FIG. 5"],"b":["1000","1015","510","512","1010","1015","1015","526","513","1020","1015","1025","1025","1025","515","1030","1025","1025","1025"],"i":["a","b","c","a","b","c. "]},"For example, for each binarized image segment A , B , and C , the classification score module  can determine a classification score for each class in a set of classes. As in this example, the classification score module  can then determine the highest classification score (and the corresponding class) for each image segment A , B , and C ","The segmentation score module  determines () a segmentation score for each binarized image segment A , B , and C by comparing each binarized image segment and a region of the image  associated with the respective binarized image segment (e.g., the region in the image  outlined by the binarized image segment A , B , or C ).","The confidence score module  determines () a confidence score , , and associated with each binarized image segment A , B , and C , respectively, based on the segmentation score and the classification score (in this example, by multiplying the segmentation score and the classification score).","The class determination module  determines () a class associated with the image  based on the confidence scores , , and associated with the binarized image segments A , B , and C , respectively. In this example, the class associated with the image  is determined to be class E, because binarized image segment B and corresponding class E yielded the highest confidence score of confidence scores , and . The class output module  outputs () the class  to a requesting device or module executing on the computing system  or in a remote system (not shown).",{"@attributes":{"id":"p-0104","num":"0103"},"figref":["FIG. 11","FIG. 5"],"b":["1110","1120","1120","1120","1115","1115","1115","1110","510","512","1110","1110"],"i":["a","b","c ","a","b","c"]},"The segmentation module  segments the image  to form the binarized image segments A , B , and C . In this example, the segmentation module  binarizes the image  using 3 different threshold levels, threshold levels 220, 90, and 20, to form the binarized image segments A , B , and C , respectively.","The classification score module  determines a classification score, classification scores 1.0, 1.0, and 1.0, for each binarized image segment A , B , and C , respectively. As illustrated, the classification score (and the corresponding class) is the same for each of the binarized image segments. In other words, the class of the image  is class A.","The segmentation score module  determines a segmentation score, segmentation scores 0.59, 0.88, and 0.58, for each binarized image segment A , B , and C , respectively, by comparing each binarized image segment with a region of the image  associated with the respective binarized image segment. In this example, the segmentation score varies because the binarized image segments A , B , and C exhibit varying degrees of erosion and dilation compared to the image . The binarized image segment B more closely represents the image  than the binarized image segment A that exhibits dilation or the binarized image segment C that exhibits erosion.","The confidence score module  determines a confidence score, confidence scores 0.59, 0.88, and 0.58, associated with each binarized image segment A , B , and C , respectively, based on the segmentation score and the classification score (in this example, the confidence score module  determines the confidence score by multiplying the segmentation score by the classification score).","The class determination module  determines a class associated with the image  based on the confidence scores , , and associated with the binarized image segments A , B , and C , respectively. In this example, the class associated with the image  is determined to be class A, because the binarized image segment B and corresponding class A yielded the highest confidence score. The class output module  outputs the determined class\u2014class A\u2014and\/or the binarized image segment B to a requesting device or module executing on the computing system  or in a remote system (not shown).","In some examples, the precise location, dimensions, and\/or boundary of the image segment in the image is determined. This determination can be in addition to recognizing or determining the class associated with an image.","In the example of , the image  (or a larger image containing the image  such as the image  in  depicting multiple characters) showing in annotation the image segment, the determined class, and the associated confidence score is displayed for a user. In this example, although the three image segments correctly classify the image  as class A, the image  can be annotated with a bounding box, boundary, or other region associated with the binarized image segment B , which yielded the highest confidence score.","Although  illustrates an image segment that is recognized or classified as a character, various other objects can be recognized or classified by the technology described herein, e.g., a manufactured part, tissue or bone in a medical image, etc. In the example of a manufactured part, a determination of the precise location and dimensions of the part in the image can be utilized for other applications. The image coordinates x, yof the part in the image  of  can be, for example, converted into real world coordinates xand y, as illustrated in . These real world coordinates xand ycan be, for example, used by a pick and place machine to pick up and place the part, e.g., based on the result of the classification.",{"@attributes":{"id":"p-0113","num":"0112"},"figref":"FIG. 12","b":["1210","1220","1220","1220","1220","1220","1212","1212","1212","1212","1210","1212","1212","1212","1212","1220","1210","1220","1220","1220","1220","1220"],"i":["a","b","c","d","a","b","c","d ","a","b","c","d ","a","b","c","d"]},"The class determination module  can determine, for example, if the determined classifications meet one or more criteria (e.g., a pre-defined number of classifications associated with an image, a high confidence score for any one classification, a set length, a set size, etc.). In this example, initially four classifications , , , and are determined, while the criteria associated with the image  indicates that there should only be three classifications. The class determination module  processes the classifications based on this criteria and outputs only the three classifications , , and and disregards the classification that yielded the lowest confidence score\u20140.8\u2014of the four classifications , , , and as illustrated in Table 1.",{"@attributes":{"id":"p-0115","num":"0114"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 1"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Exemplary Action based on Classification Criteria"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"70pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"42pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Image Region","Classification","Confidence Score","Action"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]},{"entry":[{},"A 1212a","A","1.0","Output"]},{"entry":[{},"B 1212b","B","1.0","Output"]},{"entry":[{},"C 1212c","C","1.0","Output"]},{"entry":[{},"D 1212d","E","0.8","Disregard"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]}]}}]}}},{"@attributes":{"id":"p-0116","num":"0115"},"figref":"FIG. 13","b":["1310","1320","1320","1310","1325","1325","1310","1320","1320","1310","1320","1310","1320","1320","1320","1320","1310"],"i":["a ","b ","a ","b","a ","b ","a","b ","a ","b","b "]},{"@attributes":{"id":"p-0117","num":"0116"},"figref":"FIG. 17","b":["1710","1720","1710","1725","515","517"]},"As illustrated in the previous examples, the class determination module  can validate or reject an image segment and its corresponding classification based on the confidence score of the image segment and one or more criteria, such as, by comparing the confidence score to a threshold value, a range, or to the confidence score of one or more other image segments. The threshold value or range can be predefined (e.g., defined by the user) or dynamically determined (e.g., determined based on the average or mode of the confidence scores of other image segments associated with the image, etc.). For example, the class determination module  can reject an image segment and classification by returning a \u201cNULL\u201d identifier for the classification of the image segment or by not returning a result.",{"@attributes":{"id":"p-0119","num":"0118"},"figref":["FIG. 14","FIG. 5"],"b":["1400","510","514","1410","515","1420","516","1430","517","1440","518","1450"]},{"@attributes":{"id":"p-0120","num":"0119"},"figref":["FIG. 15","FIG. 5"],"b":["1560","1550","513","1560","1550","1570","1550","513","1510","1550","1570","1530","1550","1570","1570","1510","1530","1550","1570"]},"As further described herein, the segmentation score module  of  can determine a segmentation score based on a comparison of an image segment and a corresponding region in the non-segmented image, such as, based on a comparison of pixels in the image segment and pixels in the corresponding region in the non-segmented image. For example, the segmentation score module  can determine a segmentation score between the portion  of the binary image  and a corresponding portion  of the grayscale image , e.g., where the portions  and  have the same location and dimensions. For example, the segmentation score can be the normalized grayscale correlation (e.g., as defined below) between the binary image region  (image x in the equation below) and the grayscale image region  (image y in the equation below).","In another example, the segmentation score module  can determine a segmentation score (e.g., the normalized grayscale correlation) between the portion  of the binary image  and a corresponding portion  of the grayscale image .",{"@attributes":{"id":"p-0123","num":"0122"},"figref":["FIG. 16","FIG. 15"],"b":["1560","1550","514","1570","1550","1580","1560","514"]},"In other examples, the region in the binary image and the corresponding region in the image are selected based on one or more parameters. For example, the regions are selected based on a pre-determined size (e.g., one hundred pixels by one hundred pixels, etc.), a pre-determined shape (e.g., circular, rectangle, square, etc.), and\/or any other type of selection criteria (e.g., five pixels of white space surrounding the object segment, only include foreground pixels, include 90% foreground pixels and 10% background pixels, shape fitted to object, etc.).","In some examples, the segmentation score module  determines the segmentation score for an image segment by comparing a set of pixels in the image segment (e.g., all pixels, a sub-sampled set of pixels, a randomly-selected set of pixels, edge pixels, etc.) with a corresponding set of pixels in the non-segmented image. In other examples, the segmentation score module  determines the segmentation score for an image segment by comparing each pixel in the image segment with a corresponding pixel in the non-segmented image. The comparison of the pixels can be, for example, a binary comparison (e.g., the comparison result is either zero or one) and\/or a relative comparison (e.g., the comparison result is between zero and one). The comparison of the pixels can be combined (for example, added, averaged, weighted, multiplied, etc.) to determine the segmentation score.","In some examples, the segmentation score module  determines a segmentation score based on a correlation operation between two images or image regions. The segmentation score can be, for example, based on a normalized correlation. For example, the segmentation score can be based on a normalized grayscale correlation. The normalized grayscale correlation between an image x and an image y can be, for example, as depicted in Equation 1.",{"@attributes":{"id":"p-0127","num":"0126"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"mi":["Normalized","Grayscale","Correction","Equation"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]}}},{"mtd":{"mrow":{"msub":{"mi":["r","xy"]},"mo":"=","mrow":{"mfrac":{"mrow":[{"mrow":[{"mo":"\u2211","mrow":{"msub":[{"mi":["x","i"]},{"mi":["y","i"]}],"mo":"\u2062"}},{"mi":"n","mo":["\u2062","\u2062","\u2062"],"mover":[{"mi":["x","_"]},{"mi":["y","_"]}],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}],"mo":"-"},{"mrow":{"mo":["(",")"],"mrow":{"mi":"n","mo":"-","mn":"1"}},"mo":["\u2062","\u2062"],"msub":[{"mi":["s","x"]},{"mi":["s","y"]}]}]},"mo":"=","mrow":{"mfrac":{"mrow":[{"mrow":[{"mi":"n","mo":"\u2062","mrow":{"mo":"\u2211","mrow":{"msub":[{"mi":["x","i"]},{"mi":["y","i"]}],"mo":"\u2062"}}},{"mo":"\u2211","mrow":{"msub":{"mi":["x","i"]},"mo":"\u2062","mrow":{"mo":"\u2211","msub":{"mi":["y","i"]}}}}],"mo":"-"},{"msqrt":[{"mrow":{"mrow":{"mi":"n","mo":"\u2062","mrow":{"mo":"\u2211","msubsup":{"mi":["x","i"],"mn":"2"}}},"mo":"-","msup":{"mrow":{"mo":["(",")"],"mrow":{"mo":"\u2211","msub":{"mi":["x","i"]}}},"mn":"2"}}},{"mrow":{"mrow":{"mi":"n","mo":"\u2062","mrow":{"mo":"\u2211","msubsup":{"mi":["y","i"],"mn":"2"}}},"mo":"-","msup":{"mrow":{"mo":["(",")"],"mrow":{"mo":"\u2211","msub":{"mi":["y","i"]}}},"mn":"2"}}}],"mo":"\u2062"}]},"mo":"."}}}}}]}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"1"}}]}}}},"br":{}},"n is the number of pixels in images x and y;","xrepresents the intensity of the ipixel in image x; and","yrepresents the intensity of the ipixel in image y.","Table 2 illustrates an exemplary comparison of corresponding pixels in the image segment and the non-segmented image utilizing a binary comparison of the pixels (i.e., \u201c1\u201d if the pixels match and \u201c0\u201d otherwise). In this example, the pixels are represented using red green blue (RGB) intensity values. As illustrated in Table 2, the segmentation score is 0.67 based on the average of the comparison values.",{"@attributes":{"id":"p-0132","num":"0131"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 2"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Binary Comparison of Pixels"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"4","colwidth":"42pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Pixel Location","Image Segment","Image","Comparison"]},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}},{"entry":["0x0","(0.4, 0.3, 0.4)","(0.0, 0.0, 0.0)","0"]},{"entry":["0x1","(0.23, 0.54, 0.87)","(0.23, 0.54, 0.87)","1"]},{"entry":["0x2","(0.23, 0.54, 0.87)","(0.23, 0.54, 0.87)","1"]},{"entry":["1x0","(0.23, 0.24, 0.87)","(0.23, 0.23, 0.87)","0"]},{"entry":["1x1","(0.23, 0.54, 0.86)","(0.23, 0.54, 0.86)","1"]},{"entry":["1x2","(0.23, 0.54, 0.85)","(0.23, 0.54, 0.85)","1"]},{"entry":["2x0","(0.23, 0.44, 0.87)","(0.23, 0.34 0.87)","0"]},{"entry":["2x1","(0.23, 0.54, 0.84)","(0.23, 0.54, 0.84)","1"]},{"entry":["2x2","(0.23, 0.54, 0.83)","(0.23, 0.54, 0.83)","1"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"175pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Sum","6"]},{"entry":["Number of Pixels","9"]},{"entry":["Segmentation Score (Sum\/Number of Pixels)","6\/9 = 0.67"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}]}}},"Table 3 illustrates another exemplary comparison of corresponding pixels in the image segment and the non-segmented image utilizing a relative comparison of the pixels (i.e., the comparison value is between 0 and 1 based on the difference between the pixel values). In this example, the pixels are represented using red green blue (RGB) intensity values. As illustrated in Table 3, the segmentation score is 0.86 based on the average of the comparison values.",{"@attributes":{"id":"p-0134","num":"0133"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 3"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Relative Comparison of Pixels"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"4","colwidth":"42pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Pixel Location","Image Segment","Image","Comparison"]},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"4","colwidth":"42pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["0x0","(0.4, 0.3, 0.4)","(0.0, 0.0, 0.0)","0.0"]},{"entry":["0x1","(0.23, 0.54, 0.87)","(0.23, 0.54, 0.87)","1.0"]},{"entry":["0x2","(0.23, 0.54, 0.87)","(0.23, 0.54, 0.87)","1.0"]},{"entry":["1x0","(0.23, 0.24, 0.87)","(0.23, 0.23, 0.87)","0.87"]},{"entry":["1x1","(0.23, 0.54, 0.86)","(0.23, 0.54, 0.86)","1.0"]},{"entry":["1x2","(0.23, 0.54, 0.85)","(0.23, 0.54, 0.85)","1.0"]},{"entry":["2x0","(0.23, 0.44, 0.87)","(0.23, 0.34 0.87)","0.83"]},{"entry":["2x1","(0.23, 0.54, 0.84)","(0.23, 0.54, 0.84)","1.0"]},{"entry":["2x2","(0.23, 0.54, 0.83)","(0.23, 0.54, 0.83)","1.0"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"175pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Sum","7.7"]},{"entry":["Number of Pixels","9"]},{"entry":["Segmentation Score (Sum\/Number of Pixels)","7.7\/9 = 0.86"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}]}}},"In some examples, the segmentation score module  determines the segmentation score for an image segment based on a comparison of pixels that is not \u201cone-to-one\u201d. For example, the segmentation score can be determined by comparing a \u201cstatistical indicator\u201d computed on a set of pixels in the image segment with a statistical indicator computed on a corresponding set of pixels in the non-segmented image. For example, the statistical indicator can be the mean value or the median value of the intensities of the set of pixels, the standard deviation of the intensities from the mean intensity value, a histogram of the intensities, a horizontal or vertical projection of the intensities, or any other suitable statistical indicator. The intensities can be, for example, grayscale values, RGB values, binary values, etc. In another example, the segmentation score can be determined by computing a statistical indicator on a set of pixels in the non-segmented image corresponding to a set of pixels in the image segment and comparing it with an expected value or range associated with the image segment. For example, referring to , the segmentation score can be determined by computing a standard deviation of the grayscale intensities of pixels in region  of grayscale image  corresponding to the connected component  and comparing it with an expected standard deviation.","In some examples, the segmentation score module  determines the segmentation score for an image segment based on a comparison of image moments. For example, the segmentation score is determined by comparing moments computed on a set of pixels in the image segment with moments computed on a corresponding set of pixels in the non-segmented image. The moments can be ordinary moments or central moments of any order. The moments can be any type of image moment (e.g. standard moments, geometric moments, Zernike moments, Hu moments, etc.). In some examples, multiple moments (e.g., of different orders, of different types) can be combined to form a \u201cmoment vector\u201d, and the segmentation score can be determined by comparing a moment vector computed from the image segment with a moment vector computed from the non-segmented image. The comparison can be a distance (e.g., a Euclidean distance) between the vectors, a correlation between the vectors, or any other type of comparison.","In some examples, the segmentation score module  can determine multiple intermediate segmentation scores using different techniques (e.g., normalized correlation, statistical analysis, moments analysis, etc.) and determine a final segmentation score based on the intermediate segmentation scores (e.g., by averaging the intermediate segmentation scores, adding the intermediate segmentation scores, determining a median or mode of the intermediate segmentation scores, etc.). As illustrated in Tables 2 and 3 above, the binary comparison of the pixels and the relative comparison of the pixels provide different segmentation scores. In this example, the segmentation score module  can average the two segmentation scores to determine a final segmentation score of (0.67+0.86)\/2=0.765.","In some examples, the classification score module  determines the classification score for an image segment and at least one class using an artificial intelligence (AI) based classifier. For example, during a training or learning phase, the AI based classifier can be presented a training set, from which the classifier will learn to recognize characteristics associated with a particular class.","For example, the training set can include, for each class, image segments (or images) that belong to the class and\/or image segments (or images) that do not belong to the class. For example, a training set for the character class \u201cA\u201d can include images (or image segments) of the letter A in various fonts, sizes and exhibiting various deformations. The training set for the character class \u201cA\u201d can also include images (or image segments) of characters or objects other than the letter A. The training set can include, for example, labels identifying the class of each image segment (e.g., image segment associated with class \u201cA\u201d, image segment associated with class \u201c$\u201d, etc.). In other examples, a user inputs the class for each image segment (e.g., the user inputs the class via a keyboard, etc.).","The AI based classifier can, for example, utilize any type of AI technique (e.g., a boosting algorithm, Adaboost, GentleBoost, a neural network, a support vector machine, a k-nearest neighbor method, an expert system, a Bayesian belief network, a fuzzy logic system, a decision tree, etc.). For example, the training phase can include extracting features from the image segments of the training set and the classification score can be determined using similar features extracted from the image segment to be classified.","In some examples, the classification score module  determines a classification score for each class in a set of classes. For example, for the image segment B of , the classification score module  can determine the classification score for each class in the set of letter classes as illustrated in Table 4. The classification score module  can then determine the class with the highest classification score (in this example, class E with a classification score of 1.0) and return the determined class and the corresponding classification score to the confidence score module .",{"@attributes":{"id":"p-0142","num":"0141"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 4"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Classification Scores for Classes"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"147pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Class","Classification Score"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]},{"entry":[{},"A","0.00"]},{"entry":[{},"B","0.68"]},{"entry":[{},"C","0.54"]},{"entry":[{},"D","0.10"]},{"entry":[{},"E","1.00"]},{"entry":[{},"F","0.85"]},{"entry":[{},"G","0.00"]},{"entry":[{},"H","0.31"]},{"entry":[{},"I","0.00"]},{"entry":[{},"J","0.00"]},{"entry":[{},"K","0.00"]},{"entry":[{},"L","0.13"]},{"entry":[{},"M","0.00"]},{"entry":[{},"N","0.00"]},{"entry":[{},"O","0.00"]},{"entry":[{},"P","0.32"]},{"entry":[{},"Q","0.00"]},{"entry":[{},"R","0.00"]},{"entry":[{},"S","0.00"]},{"entry":[{},"T","0.00"]},{"entry":[{},"U","0.12"]},{"entry":[{},"V","0.00"]},{"entry":[{},"W","0.00"]},{"entry":[{},"X","0.00"]},{"entry":[{},"Y","0.00"]},{"entry":[{},"Z","0.00"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}]}}},"In other examples, the set of classes can be, for example, a pre-defined set of classes (e.g., letter characters, digit characters, alphanumeric characters, part classes, vehicle classes, organ classes, etc.) and\/or a dynamically defined set of classes (e.g., language analysis to determine set of classes, analysis to determine if string includes letters, etc.).","In some examples, the computing system  executes the following steps for an image in the case of multiple segments and multiple classes. For each segmentation i of an image, resulting in a segmented object i and corresponding region i in image:","(1) Determine SG(segmented object i, region i);","(2) For each class j belonging to pre-defined set of classes: Determine CL(segmented object i, class j);","(3) Determine classthat yielded highest CLfor segmented object i, CL;","(4) Determine CON(class)=SG*CL;","(5) Once all segmentations have been processed: Return classthat yielded highest CON. Step 1 can be, for example, performed in parallel with and\/or after steps 2-3.","In other examples, the confidence score module  determines the confidence score by weighting the segmentation score and\/or the classification score. In some examples, the confidence score module  determines the confidence score by selecting a highest score, a lowest score, an average score, a mean score, and\/or any other selection process. Exemplary equations and\/or weighting factors for determining the confidence score follow below. It should be understood that any variety and\/or combination of the equations and\/or weighting factors described herein can be utilized to determine the confidence score based on the segmentation score and the classification score.","(i) Confidence Score=Segmentation Score+Classification Score","(ii) Confidence Score=Segmentation Score\u00d7Classification Score","(iii) Confidence Score=(Segmentation Score+Classification Score)\/2","(iv) Confidence Score=(Segmentation Score\u00d72)+Classification Score","(v) Confidence Score=Segmentation Score+(Classification Score\u00d71.5)","(vi) Confidence Score=(Segmentation Score\u00d72.8)+(Classification Score\u00d72.9)","In some examples, the class output module  outputs the class or classes as ASCII text (e.g., \u201c02 2009\u201d) and\/or in any other computer readable format (e.g., unicode, binary, etc.). The output of the classes can be, for example, via an application programming interface (API) call, a return call from a request for the class(es) associated with an image, and\/or any other type of communication between software, hardware, and\/or other computing devices.","The above-described systems and methods can be implemented in digital electronic circuitry, in computer hardware, firmware, and\/or software. The implementation can be as a computer program product. The implementation can, for example, be in a machine-readable storage device, for execution by, or to control the operation of, data processing apparatus. The implementation can, for example, be a programmable processor, a computer, and\/or multiple computers.","A computer program can be written in any form of programming language, including compiled and\/or interpreted languages, and the computer program can be deployed in any form, including as a stand-alone program or as a subroutine, element, and\/or other unit suitable for use in a computing environment. A computer program can be deployed to be executed on one computer or on multiple computers at one site and\/or can be any form of computer executable instructions.","Method steps can be performed by one or more programmable processors executing a computer program to perform functions of the invention by operating on input data and generating output. Method steps can also be performed by and an apparatus can be implemented as special purpose logic circuitry. The circuitry can, for example, be a graphics processor, a video card, a FPGA (field programmable gate array), and\/or an ASIC (application-specific integrated circuit). Modules, subroutines, and software agents can refer to portions of the computer program, the processor, the special circuitry, software, and\/or hardware that implements that functionality.","Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor receives instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memory devices for storing instructions and data. Generally, a computer can be operatively coupled to receive data from and\/or transfer data to one or more mass storage devices for storing data (e.g., magnetic, magneto-optical disks, or optical disks).","Data transmission and instructions can also occur over a communications network. Information carriers suitable for embodying computer program instructions and data include all forms of non-volatile memory, including by way of example semiconductor memory devices. The information carriers can, for example, be EPROM, EEPROM, flash memory devices, magnetic disks, internal hard disks, removable disks, magneto-optical disks, CD-ROM, and\/or DVD-ROM disks. The processor and the memory can be supplemented by, and\/or incorporated in special purpose logic circuitry.","To provide for interaction with a user, the above described techniques can be implemented on a computer having a display device. The display device can, for example, be a cathode ray tube (CRT) and\/or a liquid crystal display (LCD) monitor. The interaction with a user can, for example, be a display of information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer (e.g., interact with a user interface element). Other kinds of devices can be used to provide for interaction with a user. Other devices can, for example, be feedback provided to the user in any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback). Input from the user can, for example, be received in any form, including acoustic, speech, and\/or tactile input.","The above described techniques can be implemented in a distributed computing system that includes a back-end component. The back-end component can, for example, be a data server, a middleware component, and\/or an application server. The above described techniques can be implemented in a distributing computing system that includes a front-end component. The front-end component can, for example, be a client computer having a graphical user interface, a Web browser through which a user can interact with an example implementation, and\/or other graphical user interfaces for a transmitting device. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network (LAN), a wide area network (WAN), the Internet, wired networks, and\/or wireless networks.","The system can include clients and servers. A client and a server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.","Packet-based networks can include, for example, the Internet, a carrier internet protocol (IP) network (e.g., local area network (LAN), wide area network (WAN), campus area network (CAN), metropolitan area network (MAN), home area network (HAN)), a private IP network, an IP private branch exchange (IPBX), a wireless network (e.g., radio access network (RAN), 802.11 network, 802.16 network, general packet radio service (GPRS) network, HiperLAN), and\/or other packet-based networks. Circuit-based networks can include, for example, the public switched telephone network (PSTN), a private branch exchange (PBX), a wireless network (e.g., RAN, bluetooth, code-division multiple access (CDMA) network, time division multiple access (TDMA) network, global system for mobile communications (GSM) network), and\/or other circuit-based networks.","The computer system can include, for example, a computer, a computer with a browser device, a telephone, an IP phone, a mobile device (e.g., cellular phone, personal digital assistant (PDA) device, laptop computer, electronic mail device), and\/or other communication devices. The browser device includes, for example, a computer (e.g., desktop computer, laptop computer) with a world wide web browser (e.g., Microsoft\u00ae Internet Explorer\u00ae available from Microsoft Corporation, Mozilla\u00ae Firefox available from Mozilla Corporation). The mobile computing device includes, for example, a personal digital assistant (PDA).","Comprise, include, and\/or plural forms of each are open ended and include the listed parts and can include additional parts that are not listed. And\/or is open ended and includes one or more of the listed parts and combinations of the listed parts.","One skilled in the art will realize the invention may be embodied in other specific forms without departing from the spirit or essential characteristics thereof. The foregoing embodiments are therefore to be considered in all respects illustrative rather than limiting of the invention described herein. Scope of the invention is thus indicated by the appended claims, rather than by the foregoing description, and all changes that come within the meaning and range of equivalency of the claims are therefore intended to be embraced therein."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The foregoing and other objects, features, and advantages of the present invention, as well as the invention itself, will be more fully understood from the following description of various embodiments, when read together with the accompanying drawings.",{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0041","num":"0040"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0042","num":"0041"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0043","num":"0042"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0045","num":"0044"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0046","num":"0045"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0047","num":"0046"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0050","num":"0049"},"figref":["FIG. 16","FIG. 15"]},{"@attributes":{"id":"p-0051","num":"0050"},"figref":"FIG. 17"}]},"DETDESC":[{},{}]}
