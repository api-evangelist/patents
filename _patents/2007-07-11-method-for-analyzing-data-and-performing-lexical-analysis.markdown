---
title: Method for analyzing data and performing lexical analysis
abstract: A system and method provide the ability to construct lexical analyzers on the fly in an efficient and pervasive manner. The system and method split the table describing the automata into two distinct tables and splits the lexical analyzer into two phases, one for each table. The two phases consist of a single transition algorithm and a range transition algorithm, both of which are table driven and permit the dynamic modification of those tables during operation. A third ‘entry point’ table may also be used to speed up the process of finding the first table element from state 0 for any given input character.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08099722&OS=08099722&RS=08099722
owner: 
number: 08099722
owner_city: 
owner_country: 
publication_date: 20070711
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCES TO RELATED APPLICATIONS","BACKGROUND OF THE INVENTION","SUMMARY OF INVENTION","DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT"],"p":["This application is a divisional application of application Ser. No. 10\/357,326 filed on Feb. 3, 2003, titled \u201cSYSTEM AND METHOD FOR ANALYZING DATA,\u201d which claims the benefit of U.S. Provisional Application Ser. No. 60\/353,487 filed on Feb. 1, 2002, titled \u201cINTEGRATED MULTIMEDIA INTELLIGENCE ARCHITECTURE,\u201d both of which are incorporated herein by reference in their entirety for all that is taught and disclosed therein.","Lexical analyzers are generally used to scan sequentially through a sequence or \u201cstream\u201d of characters that is received as input and returns a series of language tokens to the parser. A token is simply one of a small number of values that tells the parser what kind of language element was encountered next in the input stream. Some tokens have associated semantic values, such as the name of an identifier or the value of an integer. For example if the input stream was:",{"@attributes":{"id":"p-0004","num":"0003"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"154pt","align":"left"}}],"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"dst = src + dst\u2212>moveFrom"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},"After passing through the lexical analyzer, the stream of tokens presented to the parser might be:",{"@attributes":{"id":"p-0006","num":"0005"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"189pt","align":"left"}}],"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"(tok=1,string=\u201cdst\u201d) -- i.e., 1 is the token for identifier"]},{"entry":[{},"(tok=100, string=\u201c=\u201d)"]},{"entry":[{},"(tok=1,string=\u201csrc\u201d)"]},{"entry":[{},"(tok=101, string=\u201c+\u201d)"]},{"entry":[{},"(tok=1,string=\u201cdst\u201d)"]},{"entry":[{},"(tok=102, string=\u201c\u2212>\u201d)"]},{"entry":[{},"(tok=1,string=\u201cmoveFrom\u201d)"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},"To implement a lexical analyzer, one must first construct a Deterministic Finite Automaton (DFA) from the set of tokens to be recognized in the language. The DFA is a kind of state machine that tells the lexical analyzer given its current state and the current input character in the stream, what new state to move to. A finite state automaton is deterministic if it has no transitions on input C (epsilon) and for each state, S, and symbol, A, there is at most one edge labeled A leaving S. In the present art, a DFA is constructed by first constructing a Non-deterministic Finite Automaton (NFA). Following construction of the NFA, the NFA is converted into a corresponding DFA. This process is covered in more detail in most books on compiler theory.","In , a state machine that has been programmed to scan all incoming text for any occurrence of the keywords \u201cdog\u201d, \u201ccat\u201d, and \u201ccamel\u201d while passing all other words through unchanged is shown. The NFA begins at the initial state (0). If the next character in the stream is \u2018d\u2019, the state moves to 7, which is a non-accepting state. A non-accepting state is one in which only part of the token has been recognized while an accepting state represents the situation in which a complete token has been recognized. In , accepting states are denoted by the double border. From state 7, if the next character is \u2018o\u2019, the state moves to 8. This process will then repeat for the next character in the stream. If the lexical analyzer is in an accepting state when either the next character in the stream does not match or in the event that the input stream terminates, then the token for that accepting state is returned. Note that since \u201ccat\u201d and \u201ccamel\u201d both start with \u201cca\u201d, the analyzer state is \u201cshared\u201d for both possible \u201cLexemes\u201d. By sharing the state in this manner, the lexical analyzer does not need to examine each complete string for a match against all possible tokens, thereby reducing the search space by roughly a factor of 26 (the number of letters in the alphabet) as each character of the input is processed. If at any point the next input token does not match any of the possible transitions from a given state, the analyzer should revert to state 10 which will accept any other word (represented by the dotted lines above). For example if the input word were \u201cdoctor\u201d, the state would get to 8 and then there would be no valid transition for the \u2018c\u2019 character resulting in taking the dotted line path (i.e., any other character) to state 10. As will be noted from the definition above, this state machine is an NFA not a DFA. This is because from state 0, for the characters \u2018c\u2019 and \u2018d\u2019, there are two possible paths, one directly to state 10, and the others to the beginnings of \u201cdog\u201d and \u201ccat\u201d, thus we violate the requirement that there be one and only one transition for each state-character pair in a DFA.","Implementation of the state diagram set forth in  in software would be very inefficient. This is in part because, for any non-trivial language, the analyzer table will need to be very large in order to accommodate all the \u201cdotted line transitions\u201d. A standard algorithm, often called \u2018subset construction\u2019, is used to convert an NFA to a corresponding DFA. One of the problems with this algorithm is that, in the worst-case scenario, the number of states in the resulting DFA can be exponential to the number of NFA states. For these reasons, the ability to construct languages and parsers for complex languages on the fly is needed. Additionally, because lexical analysis is occurring so pervasively and often on many systems, lexical analyzer generation and operation needs to be more efficient.","The following system and method provides the ability to construct lexical analyzers on the fly in an efficient and pervasive manner. Rather than using a single DFA table and a single method for lexical analysis, the present invention splits the table describing the automata into two distinct tables and splits the lexical analyzer into two phases, one for each table. The two phases consist of a single transition algorithm and a range transition algorithm, both of which are table driven and, by eliminating the need for NFA to DFA conversion, permit the dynamic modification of those tables during operation. A third \u2018entry point\u2019 table may also be used to speed up the process of finding the first table element from state 0 for any given input character (i.e., states 1 and 7 in ). This third table is merely an optimization and is not essential to the algorithm. The two tables are referred to as the \u2018onecat\u2019 table and the \u2018catrange\u2019 table. The onecat table includes records, of type \u201cET_onecat\u201d, that include a flag field, a catalyst field, and an offset field. The catalyst field of an ET_onecat record specifies the input stream character to which this record relates. The offset field contains the positive (possibly scaled) offset to the next record to be processed as part of recognizing the stream. Thus the \u2018state\u2019 of the lexical analyzer in this implementation is actually represented by the current \u2018onecat\u2019 table index. The \u2018catrange\u2019 table consists of an ordered series of records of type ET_CatRange, with each record having the fields \u2018lstat\u2019 (representing the lower bound of starting states), \u2018hstat\u2019 (representing the upper bound of starting states), \u2018kat\u2019 (representing the lower bound of catalyst character), \u2018heat\u2019 (representing the upper bound of catalyst character) and \u2018estat\u2019 (representing the ending state if the transition is made).","The method of the present invention begins when the analyzer first loops through the \u2018onecat\u2019 table until it reaches a record with a catalyst character of 0, at which time the \u2018offset\u2019 field holds the token number recognized. If this is not the final state after the loop, the lexical analyzer has failed to recognize a token using the \u2018onecat\u2019 table and must now re-process the input stream using the \u2018catrange\u2019 table. The lexical analyzer loops re-scanning the \u2018catrange\u2019 table from the beginning for each input character looking for a transition where the initial analyzer state lies between the \u2018lstat\u2019 and \u2018hstat\u2019 bounds, and the input character lies between the \u2018lcat\u2019 and \u2018hcat\u2019 bounds. If such a state is found, the analyzer moves to the new state specified by \u2018estat\u2019. If the table runs out (denoted by a record with \u2018lstat\u2019 set to ) or the input string runs out, the loop exits.","The invention also provides a built-in lexical analyzer generator to create the catrange and onecat tables. By using a two-table approach, the generation phase is extremely fast but more importantly, it can be incremental, meaning that new symbols can be added to the analyzer while it is running. This is a key difference over conventional approaches because it opens up the use of the lexical analyzer for a variety of other purposes that would not normally be possible. The two-phase approach of the present invention also provides significant advantages over standard techniques in terms of performance and flexibility when implemented in software, however, more interesting applications exist when one considers the possibility of a hardware implementation. As further described below, this invention may be implemented in hardware, software, or both.","The following description of the invention references various C programming code examples that are intended to clarify the operation of the method and system. This is not intended to limit the invention as any number of programming languages or implementations may be used.","The present invention provides an improved method and system for performing lexical analysis on a given stream of input. The present invention comprises two distinct tables that describe the automata and splits the lexical analyzer into two phases, one for each table. The two phases consist of a single transition algorithm and a range transition algorithm. A third \u2018entry point\u2019 table may also be used to speed up the process of finding the first table element from state 0 for any given input character (i.e., states 1 and 7 in ). This third table is merely an optimization and is not essential to the algorithm. The two tables are referred to as the \u2018onecat\u2019 table and the \u2018catrange\u2019 table.","Referring now to , programming code illustrating a sample ET_onecat record  is provided. The onecat table includes records, of type \u201cET_onecat\u201d, that include a flag field, a catalyst field, and an offset field. The catalyst field of an ET_onecat record specifies the input stream character to which this record relates. The offset field contains the positive (possibly scaled) offset to the next record to be processed as part of recognizing the stream. Thus the \u2018state\u2019 of the lexical analyzer in this implementation is actually represented by the current \u2018onecat\u2019 table index. The \u2018onecat\u2019 table is a true DFA and describes single character transitions via a series of records of type ET_onecat . A variety of specialized flag definitions exist for the flags field  but for the purposes of clarity, only \u2018kLexJump\u2019 and \u2018kNeedDelim\u2019 will be considered. The catalyst field  of an ET_onecat record  specifies the input stream character to which this record relates. The offset field  contains the positive (possibly scaled) offset to the next record to be processed as part of recognizing the stream. Thus the \u2018state\u2019 of the lexical analyzer in this implementation is actually represented by the current \u2018onecat\u2019 table index. For efficiency, the various \u2018onecat\u2019 records may be organized so that for any given starting state, all possible transition states are ordered alphabetically by catalyst character.","The basic algorithm for the first phase of the lexical analyzer, also called the onecat algorithm, is provided. The algorithm begins by looping through the \u2018onecat\u2019 table (not shown) until it reaches a record with a catalyst character of 0, at which time the \u2018offset\u2019 field  holds the token number recognized. If this is not the final state after the loop, the algorithm has failed to recognize a token using the \u2018onecat\u2019 table and the lexical analyzer must now re-process the input stream from the initial point using the \u2018catrange\u2019 table.",{"@attributes":{"id":"p-0027","num":"0026"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"133pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"84pt","align":"left"}}],"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":["ch = *ptr;","\/\/ \u2018ptr\u2019"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"119pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"98pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":["tbl = &onecat[entryPoint[ch]];","\/\/ initialize using 3table"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"for ( done = NO ;; )"},{"entry":"{"},{"entry":"tch = tbl\u2212>catalyst;"},{"entry":"state = tbl\u2212>flags;"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"98pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"105pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"if ( !*ptr ) done = YES;","\/\/ oops! the source string ran out!"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"98pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"119pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":["if ( tch == ch )","\/\/ if \u2018ch\u2019 matches catalyst char"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"112pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"105pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":["{","\/\/\u2003match found, increment to next"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"84pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"133pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":["if ( done ) break;","\/\/ exit if past the terminating NULL"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"98pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"119pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["tbl++;","\/\/ increment pointer if char accepted"]},{"entry":["ptr++;","\/\/ in the input stream."]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"ch = *ptr;"},{"entry":"}"},{"entry":"else if ( tbl\u2212>flags & kLexJump )"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"84pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"133pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":["tbl += tbl\u2212>offset;","\/\/ there is a jump alternative available"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"112pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"105pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":["else break;","\/\/ no more records, terminate loop"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"}"},{"entry":"match = !tch && (*ptr is a delimiter \u2225 !(state & "},{"entry":"(kNeedDelim+kLexJump)));"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"if (match) return tbl\u2192offset; \/\/ on success, offset field holds token#","Referring now to , sample programming code for creating an ET_Catrange record  is shown. The \u2018catrange\u2019 table (not shown) consists of an ordered series of records of type ET_CatRange . In this implementation, records of type ET_CatRange  include the fields \u2018lstat\u2019  (representing the lower bound of starting states), \u2018hstat\u2019  (representing the upper bound of starting states), \u2018lcat\u2019  (representing the lower bound of catalyst character), \u2018hcat\u2019  (representing the upper bound of catalyst character) and \u2018estat\u2019  (representing the ending state if the transition is made). These are the minimum fields required but, as described above, any number of additional fields or flags may be incorporated.","A sample code implementation of the second phase of the lexical analyzer algorithm, also called the catrange algorithm, is set forth below.",{"@attributes":{"id":"p-0031","num":"0030"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"tab = tab1 = &catRange[0];"},{"entry":"state = 0;"},{"entry":"ch = *ptr;"},{"entry":"for (;;)"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"105pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"112pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":["{","\/\/ LSTAT byte = 255 ends table"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"if ( tab\u2212>lstat == 255 ) break;"},{"entry":"else if ( ( tab\u2212>lstat <= state && state <= tab\u2212>hstat ) &&"},{"entry":"( tab\u2212>lcat <= ch && ch <= tab\u2212>hcat ) )"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"147pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":["{","\/\/ state in range & input char a valid catalyst"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"state = tab\u2212>estat; \/\/ move to final state specified"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"91pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"126pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":["ptr++;","\/\/ accept character"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"ch = *ptr;"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"154pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["if ( !ch ) break;","\/\/ whoops! the input string ran out"]},{"entry":["tab = tab1;","\/\/ start again at beginning of table"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"}"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"112pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"105pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":["else tab++;","\/\/ move to next record if not end"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"}"},{"entry":"if ( state > maxAccState \u2225 *ptr not a delimiter && *(ptr\u22121) not a"},{"entry":"delimiter )"},{"entry":"return bad token error"},{"entry":"return state"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"As the code above illustrates, the process begins by looping and re-scanning the \u2018catRange\u2019 table from the beginning for each input character looking for a transition where the initial analyzer state lies between the \u2018lstat\u2019  and \u2018hstat\u2019  bounds, and the input character lies between the \u2018lcat\u2019  and \u2018hcat\u2019  bounds. If such a state is found, the analyzer moves to the new state specified by \u2018estat\u2019 . If the table runs out (denoted by a record with \u2018lstat\u2019 set to ) or the input string runs out, the loop exits. In the preferred embodiment, a small number of tokens will be handled by the \u2018catRange\u2019 table (such an numbers, identifiers, strings etc.) since the reserved words of the language to be tokenized will be tokenized by the \u2018onecat\u2019 phase. Thus, the lower state values (i.e. <64) could be reserved as accepting while states above that would be considered non-accepting. This boundary line is specified for a given analyzer by the value of \u2018maxAccState\u2019 (not shown).","To illustrate the approach, the table specification below is sufficient to recognize all required \u2018catRange\u2019 symbols for the C programming language:",{"@attributes":{"id":"p-0034","num":"0033"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"161pt","align":"left"}}],"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["0 1 1 a z","<eol> 1 = Identifier"]},{"entry":["0 1 1 _ _","<eol> more identifier"]},{"entry":["1 1 1 0 9","<eol> more identifier"]},{"entry":["0 0 100 \u2018\u2019","<eol> \u2018 begins character constant"]},{"entry":["100 100 101 \\ \\","<eol> a \\ begins character escape sequence"]},{"entry":["101 102 102 0 7","<eol> numeric character escape sequence"]},{"entry":["101 101 103 x x","<eol> hexadecimal numeric character escape sequence"]},{"entry":["103 103 103 a f","<eol> more hexadecimal escape sequence"]},{"entry":["103 103 103 0 9","<eol> more hexadecimal escape sequence"]},{"entry":["100 100 2 \u2018\u2019","<eol> \u2019 terminates the character sequence"]},{"entry":["102 103 2 \u2018\u2019","<eol> you can have multiple char constants"]},{"entry":["100 103 100","<eol> 2 = character constant"]},{"entry":["0 0 10 0 0","<eol> 10 = octal constant"]},{"entry":["10 10 10 0 7","<eol> more octal constant"]},{"entry":["0 0 3 1 9","<eol> 3 = decimal number"]},{"entry":["3 3 3 0 9","<eol> more decimal number"]},{"entry":["0 0 110 . .","<eol> start of fp number"]},{"entry":["3 3 4 . .","<eol> 4 = floating point number"]},{"entry":["10 10 4 . .","<eol> change octal constant to fp #"]},{"entry":["4 4 4 0 9","<eol> more fp number"]},{"entry":["110 110 4 . .","<eol> more fp number"]},{"entry":["3 4 111 e e","<eol> 5 = fp number with exponent"]},{"entry":["10 10 111 e e","<eol> change octal constant to fp #"]},{"entry":["111 111 5 0 9","<eol> more exponent"]},{"entry":["111 111 112 + +","<eol> more exponent"]},{"entry":["0 0 0 \\ \\","<eol> continuation that does not belong to anything"]},{"entry":["111 111 112 \u2212 \u2212","<eol> more exponent"]},{"entry":["112 112 5 0 9","<eol> more exponent"]},{"entry":["5 5 5 0 9","<eol> more exponent"]},{"entry":["4 5 6 f f","<eol> 6 = fp number with optional float marker"]},{"entry":["4 5 6 l l","<eol> more float marker"]},{"entry":["10 10 120 x x","<eol> beginning hex number"]},{"entry":["120 120 7 0 9","<eol> 7 = hexadecimal number"]},{"entry":["120 120 7 a f","<eol> more hexadecimal"]},{"entry":["7 7 7 0 9","<eol> more hexadecimal"]},{"entry":["7 7 7 a f","<eol> more hexadecimal"]},{"entry":["7 7 8 l l","<eol> 8 = hex number with L or U specifier"]},{"entry":["7 7 8 u u","<eol>"]},{"entry":["3 3 9 l l","<eol> 9 = decimal number with L or U specifier"]},{"entry":["3 3 9 u u","<eol>"]},{"entry":["10 10 11 l l","<eol> 11 = octal constant with L or U specifier"]},{"entry":["10 10 11 u u","<eol>"]},{"entry":["0 0 130 \u201c \u201d","<eol> begin string constant..."]},{"entry":["130 130 12 \u201c \u201d","<eol> 12 = string constant"]},{"entry":["130 130 13 \\ \\","<eol> 13 = string const with line continuation \u2018\\\u2019"]},{"entry":["13 13 131 0 7","<eol> numeric character escape sequence"]},{"entry":["131 131 131 0 7","<eol> numeric character escape sequence"]},{"entry":["13 13 132 x x","<eol> hexadecimal numeric character escape sequence"]},{"entry":["131 132 12 \u201c \u201d","<eol> end of string"]},{"entry":["13 13 130","<eol> anything else must be char or escape char"]},{"entry":["132 132 132 a f","<eol> more hexadecimal escape sequence"]},{"entry":["132 132 132 0 9","<eol> more hexadecimal escape sequence"]},{"entry":["130 132 130","<eol> anything else is part of the string"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},"In this example, the \u2018catRange\u2019 algorithm would return token numbers 1 through 13 to signify recognition of various C language tokens. In the listing above (which is actually valid input to the associated lexical analyzer generator), the 3 fields correspond to the \u2018lstat\u2019 , \u2018hstat\u2019 , \u2018estat\u2019 , \u2018lcat\u2019  and \u2018hcat\u2019  fields of the ET_CatRange record . This is a very compact and efficient representation of what would otherwise be a huge number of transitions in a conventional DFA table. The use of ranges in both state and input character allow us to represent large numbers of transitions by a single table entry. The fact that the table is re-scanned from the beginning each time is important for ensuring that correct recognition occurs by arranging the table elements appropriately. By using this two pass approach, we have trivially implemented all the dotted-line transitions shown in the initial state machine diagram as well as eliminating the need to perform the NFA to DFA transformation. Additionally since the \u2018oneCat\u2019 table can ignore the possibility of multiple transitions, it can be optimized for speed to a level not attainable with the conventional NFA\u2192DFA approach.","The present invention also provides a built-in lexical analyzer generator to create the tables described. \u2018CatRange\u2019 tables are specified in the format provided in , while \u2018oneCat\u2019 tables may be specified via application programming interface or \u201cAPI\u201d calls or simply by specifying a series of lines of the form provided below.",{"@attributes":{"id":"p-0037","num":"0036"},"tables":{"@attributes":{"id":"TABLE-US-00006","num":"00006"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"154pt","align":"left"}}],"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"[ token# ] tokenString [ . ]"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},"As shown above, in the preferred embodiment, a first field is used to specify the token number to be returned if the symbol is recognized. This field is optional, however, and other default rules may be used. For example, if this field is omitted, the last token number+1 may be used instead. The next field is the token string itself, which may be any sequence of characters including whitespace. Finally, if the trailing period is present, this indicates that the \u2018kNeedDelim\u2019 flag (the flags word bit for needs delimiter, as illustrated in ) is false, otherwise it is true.","Because of the two-table approach, this generation phase is extremely fast. More importantly, however, the two table approach can be incremental. That is, new symbols can be added to the analyzer while it is running. This is a key difference over conventional approaches because it opens up the use of the lexical analyzer for a variety of other purposes that would not normally be possible. For example, in many situations there is a need for a symbolic registration database wherein other programming code can register items identified by a unique \u2018name\u2019. In the preferred embodiment, such registries are implemented by dynamically adding the symbol to a \u2018oneCat\u2019 table, and then using the token number to refer back to whatever was registered along with the symbol, normally via a pointer. The advantage of this approach is the speed with which both the insertion and the lookup can occur. Search time in the registry is also dramatically improved over standard searching techniques (e.g., binary search). Specifically, search time efficiency (the \u201cBig O\u201d efficiency) to lookup a given word is proportional to the log (base N) of the number of characters in the token, where \u2018N\u2019 is the number of different ASCII codes that exist in significant proportions in the input stream. This is considerably better than standard search techniques. Additionally, the trivial nature of the code needed to implement a lookup registry and the fact that no structure or code needs to be designed for insertion, removal and lookup, make this approach very convenient.","In addition to its use in connection with flat registries, this invention may also be used to represent, lookup, and navigate through hierarchical data. For example, it may be desirable to \u2018flatten\u2019 a complete directory tree listing with all files within it for transmission to another machine. This could be easily accomplished by iterating through all files and directories in the tree and adding the full file path to the lexical analyzer database of the present invention. The output of such a process would be a table in which all entries in the table were unique and all entries would be automatically ordered and accessible as a hierarchy.","Referring now to , a state diagram representing a directory tree is shown. The directory tree consists of a directory A containing sub-directories B and C and files F1 and F2 and sub-directory C contains F1 and F3. A function, LX_List( ), is provided to allow alphabetized listing of all entries in the recognizer database. When called successively for the state diagram provided in , it will produce the sequence:","\u201cA:\u201d, \u201cA:B:\u201d, \u201cA:C:\u201d, \u201cA:C:F1\u201d, \u201cA:C:F3\u201d, \u201cA:F1\u201d, \u201cA:F2\u201d","Furthermore, additional routines may be used to support arbitrary navigation of the tree. For example, routines could be provided that will prune the list (LX_PruneList( ), to save the list (LX_SaveListContext( )) and restore the list (LX_RestoreListContext( )). The routine LX_PruneList( ) is used to \u201cprune\u201d the list when a recognizer database is being navigated or treated as a hierarchical data structure. In one embodiment, the routine LX_PruneList( ) consists of nothing more than decrementing the internal token size used during successive calls to LX_List( ). The effect of a call to LX_PruneList( ) is to remove all descendant tokens of the currently listed token from the list sequence. To illustrate the point, assume that the contents of the recognizer DB represent the file\/folder tree on a disk and that any token ending in \u2018:\u2019 is a folder while those ending otherwise are files. A program could easily be developed to enumerate all files within the folder \u201cDisk:MyFiles:\u201d but not any files contained within lower level folders. For example, the following code demonstrates how the LX_PruneList( ) routine is used to \u201cprune\u201d any lower level folders as desired:",{"@attributes":{"id":"p-0044","num":"0043"},"tables":{"@attributes":{"id":"TABLE-US-00007","num":"00007"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"175pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"91pt","align":"left"}}],"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["tokSize = 256;","\/\/ set max file path length"]},{"entry":["prefix = \u201cDisk:MyFiles:\u201d;",{}]},{"entry":["toknum = LX_List(theDB,0,&tokSize,0,prefix);","\/\/ initialize to start folder path"]},{"entry":["while ( toknum != \u22121 )","\/\/ repeat for all files"]},{"entry":["{",{}]},{"entry":["\u2003toknum = LX_List(theDB,fName,&tokSize,0,prefix);","\/\/ list next file name"]},{"entry":["\u2003if (toknum != \u22121 )","\/\/ is it a file or a folder ?"]},{"entry":["\u2003\u2003if ( fName[tokSize\u22121] == \u2018:\u2019 )","\/\/ it is a folder"]},{"entry":["\u2003\u2003\u2003LX_PruneList(theDB)","\/\/ prune it and all it's children"]},{"entry":["\u2003\u2003else","\/\/ it is a file..."]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"266pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"\u2003\u2003\u2003-- process the file somehow"},{"entry":"}"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"In a similar manner, the routines LX_SaveListContext( ) and LX_RestoreListContext( ) may be used to save and restore the internal state of the listing process as manipulated by successive calls to LX_List( ) in order to permit nested\/recursive calls to LX_List( ) as part of processing a hierarchy. These functions are also applicable to other non-recursive situations where a return to a previous position in the listing\/navigation process is desired. Taking the recognizer DB of the prior example (which represents the file\/folder tree on a disk), the folder tree processing files within each folder at every level could be recursively walked non-recursively by simply handling tokens containing partial folder paths. If a more direct approach is desired, the recursiveness could be simplified. The following code illustrates one direct and simple process for recursing a tree:",{"@attributes":{"id":"p-0046","num":"0045"},"tables":{"@attributes":{"id":"TABLE-US-00008","num":"00008"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"280pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"void myFunc ( charPtr folderPath )"},{"entry":"{"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"175pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"91pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"tokSize = 256;","\/\/ set max file path length"]},{"entry":[{},"toknum = LX_List(theDB,0,&tokSize,0,folderPath);","\/\/ initialize to start folder"]},{"entry":[{},"while ( toknum != \u22121 )","\/\/ repeat for all files"]},{"entry":[{},"{",{}]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"161pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"91pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"toknum = LX_List(theDB,fName,&tokSize,0,prefix);","\/\/ list next file name"]},{"entry":[{},"if (toknum != \u22121 )","\/\/ is it a file or a folder ?"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"147pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"91pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"if ( fName[tokSize\u22121] == \u2018:\u2019 )","\/\/ it is a folder"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"133pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"91pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"sprintf(nuPath,\u201c%s%s\u201d,folderPath,fName);","\/\/ create new folder path"]},{"entry":[{},"tmp = LX_SaveListContext(theDB);","\/\/ prepare for recursive listing"]},{"entry":[{},"myFunc(nuPath);","\/\/ recurse!"]},{"entry":[{},"LX_RestoreListContext(theDB,tmp);","\/\/ restore listing context"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"147pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"91pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"else","\/\/ it is a file..."]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"224pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"-- process the file somehow"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"266pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"}"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"280pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"}"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"These routines are only a few of the routines that could be used in conjunction with the present invention. Those in the prior art will appreciate that any number of additional routines could be provided to permit manipulation of the DB and lexical analyzer. For example, the following non-exclusive list of additional routines are basic to lexical analyzer use but will not be described in detail since their implementation may be easily deduced from the basic data structures described above:","LX_Add( )\u2014Adds a new symbol to a recognizer table. The implementation of this routine is similar to LX_Lex( ) except when the algorithm reaches a point where the input token does not match, it then enters a second loop to append additional blocks to the recognizer table that will cause recognition of the new token.","LX_Sub( )\u2014Subtracts a symbol from a recognizer table. This consists of removing or altering table elements in order to prevent recognition of a previously entered symbol.","LX_Set( )\u2014Alters the token value for a given symbol. Basically equivalent to a call to LX_Lex( ) followed by assignment to the table token value at the point where the symbol was recognized.","LX_Init( )\u2014Creates a new empty recognizer DB.","LX_KillDB( )\u2014Disposes of a recognizer DB.","LX_FindToken( )\u2014Converts a token number to the corresponding token string using LX_List( ).","In addition to the above routines, additional routines and structures within a recognizer DB may be used to handle certain aspects of punctuation and white space that may vary between languages to be recognized. This is particularly true if a non-Roman script system is involved, such as is the case for many non-European languages. In order to distinguish between delimiter characters (i.e., punctuation etc.) and non-delimiters (i.e., alphanumeric characters), the invention may also include the routines LX_AddDelimiter( ) and LX_SubDelimiter( ). When a recognizer DB is first created by LX_Init( ), the default delimiters are set to match those used by the English language. This set can then be selectively modified by adding or subtracting the ASCII codes of interest. Whether an ASCII character is a delimiter or not is determined by whether the corresponding bit is set in a bit-array \u2018Dels\u2019 associated with the recognizer DB and it is this array that is altered by calls to add or subtract an ASCII code. In a similar manner, determining whether a character is white-space is crucial to determining if a given token should be recognized, particularly where a longer token with the same prefix exists (e.g., Smith and Smithsonian). For this reason, a second array \u2018whitespace\u2019 is associated with the recognizer DB and is used to add new whitespace characters. For example an Arabic space character has the ASCII value of the English space plus . This array is accessed via LX_AddDelimiter( ) and LX_SubDelimiter( ) functions.","A sample structure for a recognizer DB  is set forth in . The elements of the structure  are as follows: onecatmax  (storing the number of elements in \u2018onecat\u2019), catrangemax  (storing number of elements in \u2018catrange\u2019), lexFlags  (storing behavior configuration options), maxToken  (representing the highest token number in table), nSymbols  (storing number of symbols in table), name  (name of lexical recognizer DB ), Dels  (holds delimiter characters for DB), MaxAccState  (highest accepting state for catrange), whitespace  (for storing additional whitespace characters), entry  (storing entry points for each character), onecat  (a table for storing single state transitions using record type ET_onecat ) and catrange  (a table storing range transitions and is record type ET_CatRange ).","As the above description makes clear, the two-phase approach to lexical analysis provides significant advantages over standard techniques in terms of performance and flexibility when implemented in software. Additional applications are enhanced when the invention is implemented in hardware.","Referring now to , a sample implementation of a hardware device based on the \u2018OneCat\u2019 algorithm (henceforth referred to as a Single Transition Module  or STM ) is shown. The STM module  is preferably implemented as a single chip containing a large amount of recognizer memory  combined with a simple bit-slice execution unit , such as a 2610 sequencer standard module and a control input . In operation the STM  would behave as follows:\n\n","Referring now to , another illustration of the operation of the STM  is shown. As the figure illustrates, once the \u2018Reset\u2019 line  is released, the STM  fetches successive input bytes by clocking based on the \u2018Next\u2019 line , which causes external circuitry to present the new byte to input port . The execution unit  (as shown in ) then performs the \u2018OneCat\u2019 lexical analyzer algorithm described above. Other hardware implementations, via a sequencer or otherwise, are possible and would be obvious to those skilled in the art. In the simple case, where a single word is to be recognized, the algorithm drives the \u2018Break\u2019 line  high at which time the state of the \u2018Match\u2019 line  determines how the external processor\/circuitry  should interpret the contents of the table address presented by the port . The \u2018Break\u2019 signal  going high signifies that the recognizer (not shown) has completed an attempt to recognize a token within the text . In the case of a match, the contents presented by the port  may be used to determine the token number. The \u2018Break\u2019 line  is fed back internally within the Lexical Analyzer Module or \u2018LAM\u2019 (see ) to cause the recognition algorithm to re-start at state zero when the next character after the one that completed the cycle is presented.","Referring now to , a logical representation of an internal STM implementation is shown. The fields\/memory described by the ET_onecat  structure is now represented by three registers , , , two of 8 bits ,  and one of at least 32 bits  which are connected logically as shown. The \u2018Break\u2019 signal  going high signifies that the STM  has completed an attempt to recognize a token within the text stream. At this point external circuitry or software can examine the state of the \u2018Match\u2019 line  in order to decide between the following actions:\n\n","The \u201c=?\u201d block , \u201c0?\u201d blocks , , and \u201cAdd\u201d block  in  could be implemented using standard hardware gates and circuits. Implementation of the \u201cdelim?\u201d block  would require the external CPU to load up a 256*1 memory block with 1 bits for all delimiter characters and 0 bits for all others. Once loaded, the \u201cdelim?\u201d block  would simply address this memory with the 8-bit text character  and the memory output (0 or 1) would indicate whether the corresponding character was or was not a delimiter. The same approach can be used to identify white-space characters and in practice a 256*8 memory would be used thus allowing up to 8 such determinations to be made simultaneously for any given character. Handling case insensitive operation is possible via lookup in a separate 256*8 memory block.","In the preferred implementation, the circuitry associated with the \u2018OneCat\u2019 recognition algorithm is segregated from the circuitry\/software associated with the \u2018CatRange\u2019 recognition algorithm. The reason for this segregation is to preserve the full power and flexibility of the distinct software algorithms while allowing the \u2018OneCat\u2019 algorithm to be executed in hardware at far greater speeds and with no load on the main system processor. This is exactly the balance needed to speed up the kind of CAM and text processing applications that are described in further detail below. This separation and implementation in hardware has the added advantage of permitting arrangements whereby a large number of STM modules () can be operated in parallel permitting the scanning of huge volumes of text while allowing the system processor to simply coordinate the results of each STM module . This supports the development of a massive and scaleable scanning bandwidth.","Referring now to , a sample hardware implementation for the range transition algorithm is shown. The preferred embodiment is a second analyzer module similar to the STM , which shall be referred to as the Range Transition Module or RTM . The RTM module  is preferably implemented as a single chip containing a small amount of range table memory  combined with a simple bit-slice execution unit , such as a  sequencer standard module. In operation the RTM would behave as follows:\n\n","In a complete hardware implementation of the two-phase lexical analyzer algorithm, the STM and RTM are combined into a single circuit component known as the Lexical Analyzer Module or LAM . Referring now to , a sample LAM  is shown. The LAM  presents a similar external interface to either the STM  or RTM  but contains both modules internally together with additional circuitry and logic  to allow both modules ,  to be run in parallel on the incoming text stream and their results to be combined. The combination logic  provides the following basic functions in cases where both modules are involved in a particular application (either may be inhibited):\n\n","The final stage in implementing very high performance hardware systems based on this technology is to implement the LAM as a standard module within a large programmable gate array which can thus contain a number of LAM modules all of which can operate on the incoming text stream in parallel. On a large circuit card, multiple gate arrays of this type can be combined. In this configuration, the table memory for all LAMs can be loaded by external software and then each individual LAM is dynamically \u2018tied\u2019 to a particular block of this memory, much in the same manner that the ET_LexHdl structure (described above) achieves in software. Once again, combination logic similar to the combination logic  utilized between STM  and RTM  within a given LAM  can be configured to allow a set of LAM modules  to operate on a single text stream in parallel. This allows external software to configure the circuitry so that multiple different recognizers, each of which may relate to a particular recognition domain, can be run in parallel. This implementation permits the development and execution of applications that require separate but simultaneous scanning of text streams for a number of distinct purposes. The external software architecture necessary to support this is not difficult to imagine, as are the kinds of sophisticated applications, especially for intelligence purposes, for which this capability might find application.","Once implemented in hardware and preferably as a LAM module , loaded and configured from software, the following applications (not exhaustive) can be created:\n\n","Other applications. A variety of other applications based on a hardware implementation of the lexical analysis algorithm described are possible including (but not limited to); routing hierarchical text based address strings, sorting applications, searching for repetitive patterns, and similar applications.","The foregoing description of the preferred embodiment of the invention has been represented for the purposes of illustration and description. Any number of other basic features, functions, or extensions of the foregoing method and systems would be obvious to those skilled in the art in light of the above teaching. For example, other basic features that would be provided by the lexical analyzer, but that are not described in detail herein, include case insensitivity, delimiter customization, white space customization, line-end and line-start sensitive tokens, symbol flags and tagging, analyzer backup, and other features of lexical analyzers that are well-known in the prior art. For these reasons, this description is not intended to be exhaustive or to limit the invention to the precise forms disclosed. It is intended that the scope of the invention be limited not by this detailed description but rather by the claims appended hereto."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE FIGURES","p":[{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 10"}]},"DETDESC":[{},{}]}
