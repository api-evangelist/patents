---
title: Method of speech recognition using hidden trajectory Hidden Markov Models
abstract: A method of speech recognition is provided that determines a production-related value, vocal-tract resonance frequencies in particular, for a state at a particular frame based on the production-related values associated with two preceding frames using a recursion. The production-related value is used to determine a probability distribution of the observed feature vector for the state. A probability for an observed value received for the frame is then determined from the probability distribution. Under one embodiment, the production-related value is determined using a noise-free recursive definition for the value. Use of the recursion substantially improves the decoding speed. When the decoding algorithm is applied to training data with known phonetic transcripts, forced alignment is created which improves the phone segmentation obtained from the prior art.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07617104&OS=07617104&RS=07617104
owner: Microsoft Corporation
number: 07617104
owner_city: Redmond
owner_country: US
publication_date: 20030121
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF ILLUSTRATIVE EMBODIMENTS"],"p":["The present invention relates to pattern recognition. In particular, the present invention relates to speech recognition.","A pattern recognition system, such as a speech recognition system, takes an input signal and attempts to decode the signal to find a pattern represented by the signal. For example, in a speech recognition system, a speech signal (often referred to as a test signal) is received by the recognition system and is decoded to identify a string of words represented by the speech signal.","Many speech recognition systems utilize Hidden Markov Models in which phonetic units, which are also referred to as acoustic units or speech units, are represented by a single tier of connected states. Using a training signal, probability distributions for occupying the states and for transitioning between states are determined for each of the phonetic units. To decode a speech signal, the signal is divided into frames and each frame is transformed into a feature vector. The feature vectors are then compared to the distributions for the states to identify a most likely sequence of HMM states that can be represented by the frames. The phonetic unit that corresponds to that sequence is then selected.","Although HMM-based recognition systems perform well in many relatively simple speech recognition tasks, they do not model some important dynamic aspects of speech directly (and are known to perform poorly for difficult tasks such as conversational speech). As a result, they are not able to accommodate dynamic articulation differences between the speech signals used for training and the speech signal being decoded. For example, in casual speaking settings, speakers tend to hypo-articulate, or under articulate their speech. This means that the trajectory of the user's speech articulation may not reach its intended target before it is redirected to a next target. Because the training signals are typically formed using a \u201creading\u201d style of speech in which the speaker provides more fully articulated speech material than in hypo-articulated speech, the hypo-articulated speech does not match the trained HMM states. As a result, the recognizer provides less than ideal recognition results for casual speech.","A similar problem occurs with hyper-articulated speech. In hyper-articulated speech, the speaker exerts an extra effort to make the different sounds of their speech distinguishable. This extra effort can include changing the sounds of certain phonetic units so that they are more distinguishable from similar sounding phonetic units, holding the sounds of certain phonetic units longer, or transitioning between sounds more abruptly so that each sound is perceived as being distinct from its neighbors. Each of these mechanisms makes it more difficult to recognize the speech using an HMM system because each technique results in a set of feature vectors for the speech signal that often do not match well to the feature vectors present in the training data. Even if the feature vectors corresponding to the hyper- or hypo-articulated speech match those in the training data (which may be very expensive to obtain), the conventional HMM technique will still perform poorly because of the increased phonetic confusability for the HMM system that does not take into account the underlying causes of the changes in the feature vector trajectories induced by hyper- or hypo-articulation.","HMM systems also have trouble dealing with changes in the rate at which people speak. Thus, if someone speaks slower or faster than the training signal, the HMM system will tend to make more errors decoding the speech signal.","Alternatives to HMM systems have been proposed. In particular, it has been proposed that the trajectory or behavior of a production-related parameter of the speech signal should be modeled directly. However, these models have not provided efficient means for decoding based on the trajectory.","In light of this, a speech recognition framework is needed that allows for faster decoding while taking into account changes of speech feature trajectories caused by speech production mechanisms due to a variety of speaking styles.","A method of speech recognition is provided that determines a production-related value, vocal tract resonance frequencies in particular, for a state at a particular frame based on a production-related value determined for a preceding frame. The production-related value is used to determine a probability distribution for the state. A probability for an observed value received for the frame is then determined from the probability distribution. Under one embodiment, the production-related value is determined using a noise-free recursive definition for the value. Use of the recursion substantially improves the decoding speed over the prior art.","In some aspects of the invention, a combined Hidden Trajectory and Hidden Markov Model is used to decode training acoustic data, which have known phonetic transcriptions or HMM state sequences, and thereby produce an alignment between the training data and a set of states. This forced alignment improves the phone segmentation, which is then used to train the combined model in an iterative manner.","Under further aspects of the invention, a production-related value is calculated based on one of a plurality of targets where all of the targets in the plurality are trained simultaneously.",{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 1","b":["100","100","100","100"]},"The invention is operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of well-known computing systems, environments, and\/or configurations that may be suitable for use with the invention include, but are not limited to, personal computers, server computers, hand-held or laptop devices, multiprocessor systems, microprocessor-based systems, set top boxes, programmable consumer electronics, network PCs, minicomputers, mainframe computers, telephony systems, distributed computing environments that include any of the above systems or devices, and the like.","The invention may be described in the general context of computer-executable instructions, such as program modules, being executed by a computer. Generally, program modules include routines, programs, objects, components, data structures, etc. that perform particular tasks or implement particular abstract data types. The invention may also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment, program modules may be located in both local and remote computer storage media including memory storage devices.","With reference to , an exemplary system for implementing the invention includes a general-purpose computing device in the form of a computer . Components of computer  may include, but are not limited to, a processing unit , a system memory , and a system bus  that couples various system components including the system memory to the processing unit . The system bus  may be any of several types of bus structures including a memory bus or memory controller, a peripheral bus, and a local bus using any of a variety of bus architectures. By way of example, and not limitation, such architectures include Industry Standard Architecture (ISA) bus, Micro Channel Architecture (MCA) bus, Enhanced ISA (EISA) bus, Video Electronics Standards Association (VESA) local bus, and Peripheral Component Interconnect (PCI) bus also known as Mezzanine bus.","Computer  typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by computer  and includes both volatile and nonvolatile media, removable and non-removable media. By way of example, and not limitation, computer readable media may comprise computer storage media and communication media. Computer storage media includes both volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information such as computer readable instructions, data structures, program modules or other data. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by computer . Communication media typically embodies computer readable instructions, data structures, program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term \u201cmodulated data signal\u201d means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media includes wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, RF, infrared and other wireless media. Combinations of any of the above should also be included within the scope of computer readable media.","The system memory  includes computer storage media in the form of volatile and\/or nonvolatile memory such as read only memory (ROM)  and random access memory (RAM) . A basic input\/output system  (BIOS), containing the basic routines that help to transfer information between elements within computer , such as during startup, is typically stored in ROM . RAM  typically contains data and\/or program modules that are immediately accessible to and\/or presently being operated on by processing unit . By way of example, and not limitation,  illustrates operating system , application programs , other program modules , and program data .","The computer  may also include other removable\/non-removable volatile\/nonvolatile computer storage media. By way of example only,  illustrates a hard disk drive  that reads from or writes to non-removable, nonvolatile magnetic media, a magnetic disk drive  that reads from or writes to a removable, nonvolatile magnetic disk , and an optical disk drive  that reads from or writes to a removable, nonvolatile optical disk  such as a CD ROM or other optical media. Other removable\/non-removable, volatile\/nonvolatile computer storage media that can be used in the exemplary operating environment include, but are not limited to, magnetic tape cassettes, flash memory cards, digital versatile disks, digital video tape, solid state RAM, solid state ROM, and the like. The hard disk drive  is typically connected to the system bus  through a non-removable memory interface such as interface , and magnetic disk drive  and optical disk drive  are typically connected to the system bus  by a removable memory interface, such as interface .","The drives and their associated computer storage media discussed above and illustrated in , provide storage of computer readable instructions, data structures, program modules and other data for the computer . In , for example, hard disk drive  is illustrated as storing operating system , application programs , other program modules , and program data . Note that these components can either be the same as or different from operating system , application programs , other program modules , and program data . Operating system , application programs , other program modules , and program data  are given different numbers here to illustrate that, at a minimum, they are different copies.","A user may enter commands and information into the computer  through input devices such as a keyboard , a microphone , and a pointing device , such as a mouse, trackball or touch pad. Other input devices (not shown) may include a joystick, game pad, satellite dish, scanner, or the like. These and other input devices are often connected to the processing unit  through a user input interface  that is coupled to the system bus, but may be connected by other interface and bus structures, such as a parallel port, game port or a universal serial bus (USB). A monitor  or other type of display device is also connected to the system bus  via an interface, such as a video interface . In addition to the monitor, computers may also include other peripheral output devices such as speakers  and printer , which may be connected through an output peripheral interface .","The computer  may operate in a networked environment using logical connections to one or more remote computers, such as a remote computer . The remote computer  may be a personal computer, a hand-held device, a server, a router, a network PC, a peer device or other common network node, and typically includes many or all of the elements described above relative to the computer . The logical connections depicted in  include a local area network (LAN)  and a wide area network (WAN) , but may also include other networks. Such networking environments are commonplace in offices, enterprise-wide computer networks, intranets and the Internet.","When used in a LAN networking environment, the computer  is connected to the LAN  through a network interface or adapter . When used in a WAN networking environment, the computer  typically includes a modem  or other means for establishing communications over the WAN , such as the Internet. The modem , which may be internal or external, may be connected to the system bus  via the user input interface , or other appropriate mechanism. In a networked environment, program modules depicted relative to the computer , or portions thereof, may be stored in the remote memory storage device. By way of example, and not limitation,  illustrates remote application programs  as residing on remote computer . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.",{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 2","b":["200","200","202","204","206","208","210"]},"Memory  is implemented as non-volatile electronic memory such as random access memory (RAM) with a battery back-up module (not shown) such that information stored in memory  is not lost when the general power to mobile device  is shut down. A portion of memory  is preferably allocated as addressable memory for program execution, while another portion of memory  is preferably used for storage, such as to simulate storage on a disk drive.","Memory  includes an operating system , application programs  as well as an object store . During operation, operating system  is preferably executed by processor  from memory . Operating system , in one preferred embodiment, is a WINDOWS\u00ae CE brand operating system commercially available from Microsoft Corporation. Operating system  is preferably designed for mobile devices, and implements database features that can be utilized by applications  through a set of exposed application programming interfaces and methods. The objects in object store  are maintained by applications  and operating system , at least partially in response to calls to the exposed application programming interfaces and methods.","Communication interface  represents numerous devices and technologies that allow mobile device  to send and receive information. The devices include wired and wireless modems, satellite receivers and broadcast tuners to name a few. Mobile device  can also be directly connected to a computer to exchange data therewith. In such cases, communication interface  can be an infrared transceiver or a serial or parallel communication connection, all of which are capable of transmitting streaming information.","Input\/output components  include a variety of input devices such as a touch-sensitive screen, buttons, rollers, and a microphone as well as a variety of output devices including an audio generator, a vibrating device, and a display. The devices listed above are by way of example and need not all be present on mobile device . In addition, other input\/output devices may be attached to or found with mobile device  within the scope of the present invention.","The present invention provides a generative model of speech. Under this model, speech is represented as the output of an attempt by the speaker to phonetically implement a linguistic definition of a sequence of phonological units. During this attempt, the speaker produces a production-related value that follows a trajectory toward a target associated with a current phonological unit. Under embodiments of the present invention, this trajectory is modeled as a noise-free, second-order, discrete-time critically-damped, low-pass filter with unity gain.","The model of the present invention is a special form of a Hidden Trajectory Model. This hidden trajectory model includes two layers, a dynamic or trajectory model component that describes hidden production-related parameters (such as vocal tract resonance frequencies), and a mapping model component that translates the production-related parameters into observable acoustic features such as Mel-Frequency Cepstral Coefficients. The trajectory model predicts a sequence of trajectory values (z(1), . . . , z(t), . . . , z(T)) for a production-related parameter. The mapping model predicts a sequence of acoustic observation vectors o(t) given the sequence of trajectory values.","The trajectory and mapping models can be succinctly represented by the two equations:\n\n()=()+()\u2003\u2003Eq.1\n\n()=(())+()\u2003\u2003Eq.2\n\nwith g(t) being the state-dependent expected trajectory (hereinafter referred to as g(t) for simplicity), z(t) being the hidden true trajectory, u(t) being a discrete speech state or speech unit at frame t, u(1 . . . t) is a sequence of discrete speech states or speech units from time  to time t, and hbeing a state-dependent mapping function to map the production-related parameters to the feature space. In practice, each speech unit u is broken down into a few HMM-like smaller units (denoted by s as in some of the following equations) sequentially connected from left to right.\n","The summands w(t) and v(t) denote i.i.d. Gaussian noise with zero mean and covariance matrices Q=Cand R=C, respectively, that model the deviation of the actual observation from the expected values. I.e.:\n\n(()|())=(();(),)\u2003\u2003Eq.3\n\n(()|(),())=(();(()),)\u2003\u2003Eq.4\n","Under the invention, the trajectory at any time t is calculated using a recursive, noise-free function defined as:\n\n()=2\u03b3(1)\u2212\u03b3(2)+(1\u2212\u03b3)\u2003\u2003Eq.5\n\nwhere Tis the target for the trajectory of speech unit u at time t, \u03b3is a time constant associated with speech unit u, g(t\u22121) is the value of the trajectory at the preceding frame and g(t\u22122) is the value of the trajectory at the second preceding frame. Note that g(t\u22121) and g(t\u22122) could be computed using different values of \u03b3and Tbecause they may have been calculated for a different speech unit u.\n","Note that the recursive calculation of Equation 5 does not include a noise term. By eliminating the noise term, the present invention simplifies training and decoding.","Equation 5 can be rewritten in a canonical form as:\n\n()=\u03a6(1)+\u2003\u2003Eq.6\n\nwhere:\n",{"@attributes":{"id":"p-0041","num":"0040"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"msub":{"mi":"\u03a6","mrow":{"mi":"u","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}},"mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mrow":{"mn":"2","mo":"\u2062","msub":{"mi":"\u03b3","mrow":{"mi":"u","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}}}},{"mrow":{"mo":"-","msubsup":{"mi":"\u03b3","mrow":{"mi":"u","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},"mn":"2"}}}]},{"mtd":[{"mn":"1"},{"mn":"0"}]}]}}}},{"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"7"}}]},{"mtd":[{"mrow":{"mrow":[{"mi":"G","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mrow":{"mi":"g","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}}},{"mtd":{"mrow":{"mi":"g","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"t","mo":"-","mn":"1"}}}}}]}}],"mo":"="}},{"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"8"}}]},{"mtd":[{"mrow":{"mrow":[{"mi":"G","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"t","mo":"-","mn":"1"}}},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mrow":{"mi":"g","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"t","mo":"-","mn":"1"}}}}},{"mtd":{"mrow":{"mi":"g","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"t","mo":"-","mn":"2"}}}}}]}}],"mo":"="}},{"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"9"}}]},{"mtd":[{"mrow":{"msub":{"mi":"V","mrow":{"mi":"u","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}},"mo":"=","mrow":{"msup":{"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","msub":{"mi":"\u03b3","mrow":{"mi":"u","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}}}},"mn":"2"},"mo":"\u2061","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msub":{"mi":"T","mrow":{"mi":"u","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}}}},{"mtd":{"mn":"0"}}]}}}}},{"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"10"}}]}]}}}},"Under one embodiment of the present invention, the state-dependent mapping function hused to map the production-related trajectory onto acoustic features is a Taylor series of the form:\n\n(())= \u00b7(()\u2212)+residual(()\u2212)\u2003\u2003EQ. 11\n\nwhere m is a mixture index, and is the expected trajectory value. Under one embodiment, is initially set equal to the mean for mixture m of a context-independent Hidden Markov Model (HMM) state and s is the HMM state. Thus, this embodiment provides a combined Hidden Trajectory and Hidden Markov Model.\n","Under one aspect of the present invention, the predicted vectors for silence and noise phones are formed by assuming that H=0. As a result, the predicted feature vectors for silence and noise are not dependent on the trajectory of the production-related value. This is consistent with the generative model where silence and noise represent an interruption in speech generation.","Using this mapping function and combining the residual and noise terms, Equations 2 and 4 become:\n\n()= \u00b7(()\u2212)+\u2032()\u2003\u2003Eq.12\n\n(()|(),)=((); (()\u2212),)\u2003\u2003Eq.13\n","The model parameters T, \u03b3, H, , , Q, R, are trained using an Expectation-Maximization training algorithm where is arbitrarily chosen to coincide with the HMM mean estimate. The algorithm includes an E-step in which a set of training observation vectors are used with an initial estimate of the model parameters to develop sufficient statistics to predict the value of certain hidden variables including mixture weights, the trajectory, and the square of the trajectory.","To perform the first iteration of the E-step, initial estimates of the model parameters must be provided. Under one embodiment, initial estimates for Tand \u03b3are selected using combined knowledge of the Klatt speech synthesizer and some spectrogram analysis results. In addition, under one embodiment of the invention, the set of training data that will be used to train the model is first applied to an existing Hidden Markov Model to identify an alignment between the states of the Hidden Markov Model and the frames of the training data.","Using the targets T, the time constants \u03b3, and alignment boundaries set by the HMM model, a set of trajectories g(t) is estimated using Equation 5 above. The means of the HMM states in the previously trained HMM are used as the initial estimates of .","Assuming that the noise w(t) in the determination of each trajectory is zero and randomly assigning observation vectors to mixtures, Hand , are estimated for each mixture of each state to minimize the sum of the squared error between the observed feature vectors o(t) and the calculated feature vectors, where the error is computed as:\n\n()=()\u2212( \u00b7(()\u2212 ))\u2003\u2003Eq.14\n","Once Hand have been determined for each mixture and each state, the covariance matrix Rcan be estimated for each mixture in each state as:",{"@attributes":{"id":"p-0050","num":"0049"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"msub":{"mi":"R","mrow":{"mi":["m","s"],"mo":","}},"mo":["=","\u2062"],"mi":{},"mrow":{"mi":"E","mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mi":"v","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mi":["v","trans"]}}}}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"mfrac":{"mn":"1","mi":"T"},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mi":"t"},"mo":"\u2062","mrow":{"mrow":{"mi":"v","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},"mo":"\u00b7","msup":{"mrow":{"mi":"v","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},"mi":"trans"}}}}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"mfrac":{"mn":"1","mi":"T"},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mi":"t"},"mo":"\u2062","mrow":{"mo":"(","mrow":{"mrow":[{"mi":"o","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"mrow":{"mo":["(",")"],"mrow":{"msub":{"mover":{"mi":["\u03bc","_"]},"mrow":{"mi":["m","s"],"mo":","}},"mo":"+","mrow":{"msub":{"mi":"H","mrow":{"mi":["m","s"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"g","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},"mo":"-","msub":{"mover":{"mi":["z","_"]},"mrow":{"mi":["m","s"],"mo":","}}}}}}},"mo":"\u00b7"}],"mo":"-"}}}}}}},{"mtd":{"mrow":{"mi":{},"mo":"\u2062","mrow":{"mo":"(","mrow":{"mrow":{"mi":"o","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},"mo":"-","msup":{"mrow":{"mo":["(",")"],"mrow":{"msub":{"mover":{"mi":["\u03bc","_"]},"mrow":{"mi":["m","s"],"mo":","}},"mo":"+","mrow":{"msub":{"mi":"H","mrow":{"mi":["m","s"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"g","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},"mo":"-","msub":{"mover":{"mi":["z","_"]},"mrow":{"mi":["m","s"],"mo":","}}}}}}},"mi":"trans"}}}}}}]}},{"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"15"}}]}}}}},"The estimate of Q is determined by first estimating the noise w(t) in the determination of the trajectory g(t) such that the observation noise v(t) is minimized. This results in:\n\n()=()\u00b7(()\u2212( (()\u2212))\u2003\u2003Eq.16\n","Q is then initialized as:",{"@attributes":{"id":"p-0053","num":"0052"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"mi":["Q",{}],"mo":["=","\u2062"],"mrow":{"mi":"E","mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mi":"w","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mi":["w","trans"]}}}}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"mfrac":{"mn":"1","mi":"T"},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mi":"t"},"mo":"\u2062","mrow":{"mrow":{"mi":"w","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},"mo":"\u00b7","msup":{"mrow":{"mi":"w","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},"mi":"trans"}}}}}}}]}},{"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"17"}}]}}}}},"Under one embodiment, Rand Q are assumed to be diagonal matrices so only the diagonals of the matrices are calculated.","After initialization, the E step involves the following calculations:",{"@attributes":{"id":"p-0056","num":"0055"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":"\u03c9","mrow":{"mi":["m","s"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"m","mo":"\u2758","mrow":{"mi":"o","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}}}},"mo":"=","mfrac":{"mrow":[{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"o","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},"mo":"\u2758","mi":"m"}}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"m"}}],"mo":"\u2062"},{"munderover":{"mo":"\u2211","mrow":{"msup":{"mi":["m","\u2032"]},"mo":"=","mn":"1"},"mi":"M"},"mo":"\u2062","mrow":{"mrow":[{"mi":"p","mo":["(",")"],"mrow":{"mrow":{"mi":"o","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},"mo":"\u2758","msup":{"mi":["m","\u2032"]}}},{"mi":"P","mo":["(",")"],"msup":{"mi":["m","\u2032"]}}],"mo":"\u2062"}}]}}],"mo":"="}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"18"}}]}}}},"br":[{},{},{},{},{}],"in-line-formulae":[{},{},{},{},{},{},{},{}],"i":["p","o","t","m","N","o","t","+H","g","t",{"o":"z"},"S","E[z","]=[H","R","H","+Q","[H","R","o","t","+H",{"o":"z"},"Q","g","t","E[z","z","]=[H","R","H","+Q","+E[z","]E[z","S","=H","QH","+R"],"o":["\u03bc","\u03bc"],"sub":["m,s","m,s","m,s","m,s","t","m,s","m,s","m,s","m,s","m,s","m,s","m,s","m,s","t","t","m,s","m,s","m,s","t","t","m,s","m,s","m,s","m,s"],"sup":["TRANS","\u22121","\u22121","\u22121","TRANS","\u22121","\u22121","trans","TRANS","\u22121","\u22121","\u22121","Trans","trans"]},"\u03c9is a mixture weight for state s and mixture m,","o(t) is the observed training vector at time t,","g(t) is the value of the expected trajectory at time t,","zis the value of the actual trajectory at time t,","m is a mixture component,","M is the number of mixture components associated with state s,","the probability of each mixture component P(m) is uniform and equal to 1\/M,","\u201ctrans\u201d represents the transpose of a matrix, and","E\u2514x\u2518 represents the expected value of x.","The calculations above complete the E-step in the EM algorithm.","The initial model parameters and the results from the E-step are then used to re-estimate the model parameters in the M-step. Specifically, the model parameters are calculated as:",{"@attributes":{"id":"p-0068","num":"0067"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"msub":{"mover":{"mi":"H","mo":"^"},"mrow":{"mi":["m","s"],"mo":","}},"mo":["=","\u2062"],"mi":{},"mrow":{"mrow":{"mo":["{","}"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"t","mo":"=","mn":"1"},"mi":"T"},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"msub":{"mi":"\u03c9","mrow":{"mi":["m","s"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"mover":{"mi":"v","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},"mo":"\u00b7","mi":"E"},"mo":"\u2062","msup":{"mrow":{"mo":["{","}"],"msub":{"mi":["z","t"]}},"mi":"Trans"}}}],"mo":"\u00b7"}}}},"mo":"\u00b7"}}}},{"mtd":{"mrow":{"mi":{},"mo":"\u2062","msup":{"mrow":[{"mo":["{","}"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"t","mo":"=","mn":"1"},"mi":"T"},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"mrow":{"msub":{"mi":"\u03c9","mrow":{"mi":["m","s"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},"mo":"\u00b7","mi":"E"},{"mo":["{","}"],"mrow":{"msub":{"mi":["z","t"]},"mo":"\u00b7","msubsup":{"mi":["z","t","Trans"]}}}],"mo":"\u2062"}}}},{"mo":"-","mn":"1"}]}}}}]}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"23"}}]},{"mtd":[{"mrow":{"msub":{"mover":{"mover":{"mi":["\u03bc","_"]},"mo":"^"},"mrow":{"mi":["m","s"],"mo":","}},"mo":"=","mfrac":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"t","mo":"=","mn":"1"},"mi":"T"},"mo":"\u2062","mrow":{"mrow":[{"msub":{"mi":"\u03c9","mrow":{"mi":["m","s"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"mi":"o","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}],"mo":"\u00b7"}},{"munderover":{"mo":"\u2211","mrow":{"mi":"t","mo":"=","mn":"1"},"mi":"T"},"mo":"\u2062","mrow":{"msub":{"mi":"\u03c9","mrow":{"mi":["m","s"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}}]}}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"24"}}]},{"mtd":[{"mrow":{"msub":{"mover":{"mover":{"mi":["z","_"]},"mo":"^"},"mrow":{"mi":["m","s"],"mo":","}},"mo":"=","mfrac":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"t","mo":"=","mn":"1"},"mi":"T"},"mo":"\u2062","mrow":{"mrow":[{"mrow":{"msub":{"mi":"\u03c9","mrow":{"mi":["m","s"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},"mo":"\u00b7","mi":"E"},{"mo":["{","}"],"msub":{"mi":["z","t"]}}],"mo":"\u2062"}},{"munderover":{"mo":"\u2211","mrow":{"mi":"t","mo":"=","mn":"1"},"mi":"T"},"mo":"\u2062","mrow":{"msub":{"mi":"\u03c9","mrow":{"mi":["m","s"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}}]}}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"25"}}]},{"mtd":[{"mrow":{"msub":{"mover":{"mi":"R","mo":"^"},"mrow":{"mi":["m","s"],"mo":","}},"mo":"=","mfrac":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"t","mo":"=","mn":"1"},"mi":"T"},"mo":"\u2062","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"msub":{"mi":"\u03c9","mrow":{"mi":["m","s"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"munder":{"mo":"\u2211","mi":"t"},"mo":"\u2062","mrow":{"mo":"(","mrow":{"mrow":{"mi":"o","mo":"\u2062","mrow":{"mo":["(",")"],"mi":"t"}},"mo":"-"}}}],"mo":["\u2062","\u2062"],"mfrac":{"mn":"1","mi":"T"}}}},{"mtd":{"mrow":{"mrow":{"mo":["(",")"],"mrow":{"msub":{"mover":{"mi":["\u03bc","_"]},"mrow":{"mi":["m","s"],"mo":","}},"mo":"+","mrow":{"msub":{"mi":"H","mrow":{"mi":["m","s"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"g","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},"mo":"-","msub":{"mover":{"mi":["z","_"]},"mrow":{"mi":["m","s"],"mo":","}}}}}}},"mo":"\u00b7"}}},{"mtd":{"mrow":{"mo":"(","mrow":{"mrow":{"mi":"o","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},"mo":"-","msup":{"mrow":{"mo":["(",")"],"mrow":{"msub":{"mover":{"mi":["\u03bc","_"]},"mrow":{"mi":["m","s"],"mo":","}},"mo":"+","mrow":{"msub":{"mi":"H","mrow":{"mi":["m","s"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"g","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},"mo":"-","msub":{"mover":{"mi":["z","_"]},"mrow":{"mi":["m","s"],"mo":","}}}}}}},"mi":"trans"}}}}}]}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"t","mo":"=","mn":"1"},"mi":"T"},"mo":"\u2062","mrow":{"msub":{"mi":"\u03c9","mrow":{"mi":["m","s"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}}]}}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"26"}}]},{"mtd":[{"mrow":{"mover":{"mi":"Q","mo":"^"},"mo":"=","mfrac":{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"t","mo":"=","mn":"1"},"mi":"T"},"mo":"\u2062","mrow":{"mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"E","mo":"\u2062","mrow":{"mo":["{","}"],"msub":{"mi":["z","t"]}}},{"mi":"g","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}],"mo":"-"}},"mo":"\u2062","msup":{"mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"E","mo":"\u2062","mrow":{"mo":["{","}"],"msub":{"mi":["z","t"]}}},{"mi":"g","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}],"mo":"-"}},"mi":"trans"}}},"mi":"T"}}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"27"}}]}]}}},"br":{},"sub":["u(t) ","u(t) ","u(t) "]},{"@attributes":{"id":"p-0069","num":"0068"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msubsup":{"mover":{"mi":"\u03b3","mo":"^"},"mrow":[{"mi":"u","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"mi":"r","mo":"+","mn":"1"}]},"mo":"=","mrow":{"msubsup":{"mover":{"mi":"\u03b3","mo":"^"},"mrow":{"mi":"u","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},"mi":"r"},"mo":"+","mrow":{"mi":"\u025b","mo":"\u2062","mfrac":{"mtable":{"mtr":[{"mtd":{"mrow":{"mo":"\u2202","mrow":{"mo":"\u230a","mrow":{"mi":"E","mo":"\u2062","mrow":{"mo":"{","mrow":{"mrow":[{"mi":"z","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"mn":"2","mo":["\u2062","\u2062","\u2062"],"msub":{"mi":"\u03b3","mrow":{"mi":"u","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}},"mi":"g","mrow":{"mo":["(",")"],"mrow":{"mi":"t","mo":"-","mn":"1"}}}],"mo":["-","-"]}}}}}}},{"mtd":{"mrow":{"mrow":{"mrow":{"mrow":[{"msubsup":{"mi":"\u03b3","mrow":{"mi":"u","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},"mn":"2"},"mo":"\u2062","mrow":{"mi":"g","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"t","mo":"-","mn":"2"}}}},{"msup":{"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","msub":{"mi":"\u03b3","mrow":{"mi":"u","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}}}},"mn":"2"},"mo":"\u2062","msub":{"mi":"T","mrow":{"mi":"u","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}}}],"mo":"+"},"mo":"}"},"mo":"\u230b"}}}]},"mrow":{"mo":"\u2202","msubsup":{"mover":{"mi":"\u03b3","mo":"^"},"mrow":{"mi":"u","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},"mi":"r"}}}}}}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"28"}}]}}}}},"One aspect of the present invention is that the targets Tfor the possible speech units u are trained simultaneously instead of being trained individually. This is done because changing the target for one speech unit changes the value of g(t) for the next speech unit. The training is performed using the following matrix equation:",{"@attributes":{"id":"p-0071","num":"0070"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"mover":{"mi":"T","mo":"^"},"mo":["=","\u2062"],"mi":{},"mrow":{"mrow":{"mo":["[","]"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"t","mo":"=","mn":"1"},"mi":"T"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"m","mo":"=","mn":"1"},"mi":"M"},"mo":"\u2062","mrow":{"mrow":[{"mrow":{"msub":{"mi":"\u03c9","mrow":{"mi":["m","s"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},"mo":"\u00b7","mi":"E"},{"mrow":{"mo":["{","}"],"msub":{"mi":["z","t"]}},"mo":"\u00b7","msub":{"mrow":[{"mo":["(",")"],"mrow":{"mover":{"mi":"b","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}},{"mi":"u","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}]}}],"mo":"\u2062"}}}},"mo":"."}}}},{"mtd":{"mrow":{"mi":{},"mo":"\u2062","msup":{"mrow":[{"mo":["[","]"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"t","mo":"=","mn":"1"},"mi":"T"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"m","mo":"=","mn":"1"},"mi":"M"},"mo":"\u2062","mrow":{"mrow":[{"msub":{"mi":"\u03c9","mrow":{"mi":["m","s"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"mover":{"mi":"b","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}],"mo":["\u00b7","\u00b7"],"msub":{"mrow":[{"mo":["(",")"],"mrow":{"mover":{"mi":"b","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}},{"mi":"u","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}]}}}}},{"mo":"-","mn":"1"}]}}}}]}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"29"}}]}}}},"br":[{},{},{},{},{}],"in-line-formulae":[{},{},{},{},{},{}],"i":["{circumflex over (T)}","{circumflex over (T)}",", {circumflex over (T)}","{circumflex over (b)}","t","{circumflex over (b)}","t\u2212","{circumflex over (b)}","t\u2212","e","e","u"],"sub":["1","2","u(t)","u(t)","u(t)","u(t)","u(t)","u(t) "],"sup":["2","2","Trans "]},"The E-step and the M-step may be iterated a number of times to come to a final set of model parameters. Under one embodiment, after each iteration of the E-step and M-step, the sequence of training vectors O(t) is decoded using the current form of the Hidden Trajectory Hidden Markov Model. This decoding provides a new set of state boundaries that can be used in the next iteration of the EM algorithm. Once the final set of parameters has been determined, they can be used to decode an observed set of acoustic vectors.","The decoding task after training is complete involves finding a word sequence that most likely generated a sequence of acoustic observations. The decoding task during training involves finding the most likely time alignment of a sequence of acoustic observations for a given word sequence. Under one embodiment of the present invention, decoding is performed using a Finite-State Transducer that consists of a network of connected states. Each state in the network represents an HMM state and the connections between the states are based on a dictionary that defines a sequence of states for each word in the dictionary. Thus, every path through the state network represents a word in the dictionary. The network is closed on itself such that the ending states of each word connects to the beginning states of each word in the dictionary.","A simple example state network is shown in . The state network of  represents four words: \u201cslow\u201d, \u201csit\u201d, \u201ccat\u201d, and \u201ccan.\u201d Although not shown, each ending state , , and  connects to each beginning state  and . Note that each state includes a self transition, such as transition .","Decoding involves finding the highest probability path through the network given the input feature vectors. For each input feature vector, a most-likely transition into each available state is selected based on the probability of the complete path up to and including the state. After all of the feature vectors have been processed, the path that ends at the end of a word in the highest probability is selected as the decoded word sequence.","Under one embodiment, the probability for a path is dependent on a number of separate probabilities including a language model probability that provides the probability of transitioning between language model states, h, along the path; HMM transition probabilities that indicate the likelihood of transitioning between states, s, along the path; and state probabilities that are dependent on the current value of the trajectory G at state s in time frame t.",{"@attributes":{"id":"p-0077","num":"0076"},"figref":"FIG. 4","b":"400"},"At step , an input value from the input signal is selected. As discussed further below, this involves dividing the input signal into frames, converting each frame into a feature vector, such as Mel-Frequency Cepstrum Coefficients, and selecting one of the feature vectors for decoding at each time point t.","At step , a set of state probabilities are calculated using the selected feature vector. In particular, a separate state probability is calculated for each combination of HMM state s, language model state h, and class of trajectory c(G) that is active at time t. Under one embodiment of the invention, the continuous trajectory value G is assigned to one of 2possible classes. This is done to make the decoding feasible. If the classes were not used, a separate state score would have to be calculated for all possible values of G. Note that the class of the trajectory is only used to limit the search space and is not used in the probability calculation itself. For the probability calculation, the continuous value of G is used directly.","The state probability for an active [s,h,c(G)] set is calculated as:",{"@attributes":{"id":"p-0081","num":"0080"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":[{"mi":"o","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"mi":"g","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}],"mo":"\u2758"},"mo":",","mi":"s"}}},{"mi":"log","mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"m","mo":"=","mn":"1"},"mi":"M"},"mo":"\u2062","mrow":{"msub":{"mi":"\u03c9","mrow":{"mi":["m","s"],"mo":","}},"mo":"\u2062","mrow":{"mi":"N","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":[{"mi":"o","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"msub":{"mover":{"mi":["\u03bc","_"]},"mrow":{"mi":["m","s"],"mo":","}},"mo":["+","\u2062","\u2062"],"mstyle":[{"mtext":{}},{"mspace":{"@attributes":{"width":"10.3em","height":"10.3ex"}}}],"mrow":{"msub":{"mi":"H","mrow":{"mi":["m","s"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"g","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},"mo":"-","msub":{"mover":{"mi":["z","_"]},"mrow":{"mi":["m","s"],"mo":","}}}}}}],"mo":";"},"mo":",","msub":{"mi":"R","mrow":{"mi":["m","s"],"mo":","}}}}}}}}],"mo":"="}},{"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"33"}}]}}}},"br":{},"o":"\u03bc","sub":"m,s"},"Each state probability is combined with a path probability for the path leading to the state to form a total probability for each set [s,h,c(G)]. The highest total probability among all of the active sets is determined at step .","At step , the method determines if there are more frames of the input signal to process. If there are, the process continues at step , where those sets that are not within a threshold of the highest total probability are pruned.","For each set [s,h,c(G)] that is not pruned at state , a successor set [s\u2032,h\u2032,c\u2032(G)] is identified at step . The dictionary is used to identify each possible successor state, s\u2032, and each possible language model state h\u2032 from the present state s and language model state h. The successor class of the trajectory is determined by first utilizing the successor state s\u2032 and the continuous value of the trajectory G(t) associated with the present set [s,h,c(G(t))] to determine the value of the trajectory G(t+1) in the successor state. In particular, the value of the trajectory G(t+1) is calculated as:\n\n(1)=\u03a6()+\u2003\u2003Eq.34\n\nwhere the parameters in matrices \u03a6and Vare selected based on the speech unit that the successor state S\u2032 is part of and G(t) is the current continuous value of trajectory G for set [s, h, c(G(t))].\n","The class of the continuous value G(t+1) is then determined to determine the successor class of the trajectory, c\u2032(G), and thereby define the successor set [s\u2032,h\u2032,c\u2032(G)].","The log probability of transitioning from the present set to the successor set is then added to the log probability for the best path ending at set [s,h,c(G)] to produce a probability for a path entering the successor set [s\u2032,h\u2032,c\u2032(G)]. For within word transitions, the set transition probability is based only on the HMM transition probability of transitioning from state s to state s\u2032. However, when a transition is between words, the probability of transitioning is the sum of the log probability of transitioning between states s and s\u2032 as provided by the HMM and the log probability of transitioning from state h to state h\u2032 as provided by a language model that describes the likelihood of particular sequences of words.","If this successor state has been previously created, the probability for this current path into the successor set is compared to the probability for the existing path into the set. If the probability of the current path is greater than the probability for the existing path, the existing path is replaced with the current path. However, if the probability of the existing path into the successor set [s\u2032,h\u2032,c\u2032(G)] is greater than the current path, the existing path is maintained and the current path is deleted. Thus, at step , only the highest probability path into each possible successor set is maintained.","After step , a group of successor sets has been identified for the next frame of input and each successor set has only one path entering it. In addition, each successor set includes a back tracing data structure that allows the sequence of words or states represented by the path entering the successor state to be recovered. When decoding after training is complete, this data structure includes the last word identified along the path, a pointer to a previous data structure that was constructed for the word before the last word, and, optionally, the time point at which the last word ended. Thus, there is a chain of data structures, one for each word along the path, that can be traced back using the pointers in the data structures to recover the word sequence of the path and if desired the segmentation of the word sequence relative to the input frames. When decoding during training, this data structure includes the identities of states instead of words.","When there are no more frames to be processed at step , the system selects the highest probability path that ends at the end of a word as representing the input signal at step . The back pointer data structures for this path are then traced back to find the sequence of words or states represented by the path. This sequence of words or states is then used as the decoded output.",{"@attributes":{"id":"p-0090","num":"0089"},"figref":["FIG. 5","FIG. 5"],"b":["500","504","504","502","504","506"]},"A-to-D converter  converts the analog signal from microphone  into a series of digital values. In several embodiments, A-to-D converter  samples the analog signal at 16 kHz and 16 bits per sample, thereby creating 32 kilobytes of speech data per second. These digital values are provided to a frame constructor , which, in one embodiment, groups the values into 25 millisecond frames that start 10 milliseconds apart.","The frames of data created by frame constructor  are provided to feature extractor , which extracts a feature from each frame. Examples of feature extraction modules include modules for performing Linear Predictive Coding (LPC), LPC derived cepstrum, Perceptive Linear Prediction (PLP), Auditory model feature extraction, and Mel-Frequency Cepstrum Coefficients (MFCC) feature extraction. Note that the invention is not limited to these feature extraction modules and that other modules may be used within the context of the present invention.","If the input signal is a training signal, this series of feature vectors is provided to a trainer , which uses the feature vectors and a training text  to train the generative model  of the present invention. For example, the EM training algorithm described above may be used to train the generative model.","As mentioned above, the EM training algorithm may be iterative. In such cases, after each iteration of the EM algorithm, the training feature vectors are applied to a decoder  to identify alignment boundaries between the frames of the training signal and the states in the generative model. These alignment boundaries are then provided to trainer  for the next iteration of training. During such training decoding, the state network is limited to the sequence of states defined in the training text . As such, lexicon  is not used during the training decoding.","Decoder  identifies a most likely sequence of words based on the stream of feature vectors, a lexicon  or training text , a language model , and the generative model . Under one embodiment, lexicon  defines the finite state network that is traversed by decoder  to identify a word from a sequence of feature vectors.","The most probable sequence of hypothesis words is provided to a confidence measure module . Confidence measure module  identifies which words are most likely to have been improperly identified by the speech recognizer, based in part on a secondary acoustic model (not shown). Confidence measure module  then provides the sequence of hypothesis words to an output module  along with identifiers indicating which words may have been improperly identified. Those skilled in the art will recognize that confidence measure module  is not necessary for the practice of the present invention.","Although the present invention has been described with reference to particular embodiments, workers skilled in the art will recognize that changes may be made in form and detail without departing from the spirit and scope of the invention."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 5"}]},"DETDESC":[{},{}]}
