---
title: Question answering framework
abstract: Described herein is a technology to facilitate automated question answering. In one implementation, an input question is first received. Different search strategies may be used to search multiple types of data from multiple types of knowledge databases to generate one or more candidate answers to the input question. The one or more candidate answers are evaluated to generate a final answer to the input question.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09213771&OS=09213771&RS=09213771
owner: SAP SE
number: 09213771
owner_city: Walldorf
owner_country: DE
publication_date: 20130606
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["TECHNICAL FIELD","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["The present disclosure relates generally to information retrieval, and more specifically, to a question answering framework.","The rapid development of science and technology has led to a rapidly increasing amount of published information. Question answering (QA) systems have been designed to access and search through such information to automatically answer questions posed by humans in a natural language.","One of the major challenges in such QA systems is to provide relevant answers amid multifarious search results. Search engines often return a large set of search results that are irrelevant to the question, causing the user to be confused and lost in the myriad of results. Even the top ranked search result may not be related to the question itself. This is especially prevalent in cases where the question is short and includes common words with spellings that are very similar to names or content of other different topics, such as the name of a film or lyrics of a popular song.","There are two common causes of retrieving irrelevant answers. Firstly, irrelevancy may be caused by low accuracy in question parsing and text analysis. In other words, the QA system may not correctly interpret the meaning of the input question. Irrelevancy in answers may also be caused by low accuracy in the answer finding capability of the QA system.","In addition, answers tend to be limited in domain-specific QA systems. Most QA systems support only a certain kind of domain, and do not support access to a wide collection of knowledge bases. Frequently, no answer is provided for questions with answers from an unsupported domain. Even if answers can be found, they may not be comprehensible by the user. In most cases, the answers may be very long with many definitions, principle introductions, references, related topics, etc. that make it difficult for the user, particularly a school-age child, to quickly understand.","Even further, the performance of conventional QA systems is typically unsatisfactory. In order to correctly interpret human language and extract answers from large knowledge bases, QA systems often employ a wide data search and deep data mining that are computationally expensive and often result in slow retrieval time and low accuracy.","Therefore, there is a need for an improved framework that addresses the above-mentioned challenges.","A computer-implemented technology for facilitating question-answering is described herein. In accordance with one aspect of the technology, an input question is first received. Different search strategies are used to search multiple types of data from multiple types of knowledge databases to generate one or more candidate answers to the input question. The one or more candidate answers are evaluated to generate a final answer to the input question.","In accordance with another aspect, an input question is first received. The technology then determines one or more types of knowledge databases available for searching. If a question-answer paired knowledge database is available, question-answer paired data from the question-answer paired knowledge database is searched to determine a first candidate answer to the input question. If a plain text knowledge database is available, plain text data from the plain text knowledge database is searched to determine a second candidate answer to the input question. If a resource description framework (RDF) knowledge database is available, RDF data from the RDF knowledge database is searched to determine a third candidate answer to the input question. The first, second or third candidate answer may then be evaluated to generate a final answer to the input question.","With these and other advantages and features that will become hereinafter apparent, further information may be obtained by reference to the following detailed description and appended claims, and to the figures attached hereto.","In the following description, for purposes of explanation, specific numbers, materials and configurations are set forth in order to provide a thorough understanding of the present frameworks and methods and in order to meet statutory written description, enablement, and best-mode requirements. However, it will be apparent to one skilled in the art that the present frameworks and methods may be practiced without the specific exemplary details. In other instances, well-known features are omitted or simplified to clarify the description of the exemplary implementations of the present framework and methods, and to thereby better explain the present framework and methods. Furthermore, for ease of understanding, certain method steps are delineated as separate steps; however, these separately delineated steps should not be construed as necessarily order dependent in their performance.","A technology for facilitating question-answering is described herein. One aspect of the technology provides a question-answering (QA) framework that supports multiple types of knowledge databases. A knowledge database generally refers to an information repository where information may be collected, organized, shared, searched and\/or used. Different types of knowledge databases include, for example, formulated question-answer paired knowledge databases, plain text knowledge databases and resource description framework (RDF) knowledge databases, which will be described in more detail in the following description.","One aspect of the present technology implements a different search strategy for each type of knowledge database. Different search strategies may be combined to support a query in multiple types of knowledge databases and provide the most relevant answer in response to the query. This allows the user to combine knowledge databases without having to consolidate data from the different types of knowledge databases. With such expanded knowledge database support, the user can build a comprehensive knowledge system that increases the rate of answer finding, particularly for specific domain areas.","Another aspect of the present technology provides an improved methodology for question pairing and answer assembling with enhanced accuracy in question identification and answer extractions. With in-memory technology, parallel computing and in-built text analysis functionality, more data may be processed with high accuracy and speed that are acceptable to the user. It is only with such powerful computing ability that multiple data sources and unlimited domains are feasible. These and other advantages and aspects will be described in more detail in the following description.","The framework described herein may be implemented as a method, a computer-controlled apparatus, a computer process, a computing system, or as an article of manufacture such as a computer-usable medium. These and various other features will be apparent from the following description.",{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 1","b":["100","100","106","102","104","106","116","132","130","132","106","154","132","106","150","150","106"]},"Computer system  includes a central processing unit (CPU) , an input\/output (I\/O) unit , and a memory module . Other support circuits, such as a cache, a power supply, clock circuits and a communications bus, may also be included in computer system . In addition, any of the foregoing may be supplemented by, or incorporated in, application-specific integrated circuits. Examples of computer system  include a handheld device, a mobile device, a personal digital assistance (PDA), a workstation, a server, a portable laptop computer, another portable device, a mini-computer, a mainframe computer, a storage system, a dedicated digital appliance, a device, a component, other equipment, or some combination of these capable of responding to and executing instructions in a defined manner.","Memory module  may be any form of non-transitory computer-readable media, including, but not limited to, dynamic random access memory (DRAM), static random access memory (SRAM), Erasable Programmable Read-Only Memory (EPROM), Electrically Erasable Programmable Read-Only Memory (EEPROM), flash memory devices, magnetic disks, internal hard disks, removable disks, magneto-optical disks, Compact Disc Read-Only Memory (CD-ROM), any other volatile or non-volatile memory, or a combination thereof.","Memory module  serves to store machine-executable instructions, data, and various software components for implementing the techniques described herein, all of which may be processed by CPU . As such, the computer system  is a general-purpose computer system that becomes a specific-purpose computer system when executing the machine-executable instructions. Alternatively, the various techniques described herein may be implemented as part of a software product, which is executed via an application server  and\/or a data server . Each computer program may be implemented in a high-level procedural or object-oriented programming language (e.g., C, C++, Java, Advanced Business Application Programming (ABAP\u2122) from SAP\u00ae AG, Structured Query Language (SQL), etc.), or in assembly or machine language if desired. The language may be a compiled or interpreted language. The machine-executable instructions are not intended to be limited to any particular programming language and implementation thereof. It will be appreciated that a variety of programming languages and coding thereof may be used to implement the teachings of the disclosure contained herein.","In one implementation, the memory module  of the computer system  includes an application server (or stack)  and a data server (or stack) . Application server (or stack)  may store a QA framework  that may be coded using a high-level programming language, such as Java, C++, ABAP\u2122, etc. Other types of programming languages are also useful. QA framework  may include a set of function modules or programs of a QA framework designed to perform various data collection and\/or QA processing functions, such as question parsing, answer retrieval and ranking, answer assembly and so forth. More details of these and other exemplary functions will be provided in the following description.","Data server (or stack)  may include a database management system (DBMS)  and a database . DBMS  may be coded using a database query language, such as SQL or extensions thereof. Other types of programming languages are also useful. DBMS  may include a set of programs, functions or procedures (e.g., HANA custom procedures) for defining, administering and processing the database . More particularly, DBMS  may include an index server  and a preprocessor server . Index server  provides database functionality for storage and retrieval of query data (e.g. indexes) that is associated with information stored in the database , while preprocessor server  may serve to analyze documents to be indexed by the index server . More details of these and other exemplary functions will be provided in the following description.","A user at the client computer  may interact with a user interface  to communicate with the database  via the application server  and the DBMS . In one implementation, database  is an in-memory database that relies primarily on the system's main memory for efficient computer data storage. More particularly, the data in the in-memory database may reside in volatile memory and not persistently stored on a hard drive, thereby allowing the data to be instantly accessed and scanned at a speed of several megabytes per millisecond. The in-memory database  allows seamless access to and propagation of high volumes of data in real-time. Parallel processing may further be achieved by using a multicore processor  in conjunction with the in-memory database . In-memory database technology includes systems such as SAP's HANA (high performance analytic appliance) in-memory computing engine.","Column-based data storage may further be implemented in the in-memory database , where data tables are stored as columns of data, in sequence and in compressed memory blocks. This may facilitate faster aggregation of data when calculations are performed on single columns. Alternatively, row-based data storage is also possible. In some implementations, instead of updating entire rows, only fields that have changed will be updated. This avoids having to lock entire data tables during updates to prevent conflicting modifications to a set of data. High levels of parallelization may be achieved, which is critical to real-time processing of live data streams and performing constant and substantially simultaneous updates.","It should be appreciated that the different components and sub-components of the computer system  may be located on different machines or systems. For example, application server  and data server  may be implemented on different physical machines or computer systems. It should further be appreciated that the different components of the client computer  may also be located on the computer system , or vice versa.",{"@attributes":{"id":"p-0037","num":"0036"},"figref":["FIG. 2","FIG. 1","FIG. 2"],"b":["200","100","122","126"]},"As shown, client devices (to ) communicate with the QA framework  to provide one or more input questions and to receive responses to the input questions from the QA framework . The input question may be expressed in natural language, and provided in the form of a statement or an answer seeking an appropriate question. The client devices (to ) and the QA framework  may communicate with each other over a network using a web service. The web service may implement various web technologies, including a representational state transfer (REST) based interface, remote procedure call (RPC) based technologies, simple object access protocol (SOAP) based technologies, service-oriented architecture (SOA) based technologies, and so forth.","In response to the question provided by one or more client devices -, the QA framework  may automatically generate one or more answers. In some implementations, the QA framework  includes a data collector  and a QA processor . The data collector  may serve to retrieve information from one or more different knowledge databases (to ) for generating the answers. In one implementation, the data collector  retrieves data by automatically downloading a collection of data from the knowledge databases (to ), parsing and converting the data into a native format suitable for storage in the database . The downloading may be performed on-demand or periodically at predetermined time intervals. Additionally, or alternatively, the data collector  may include a crawler, spider, robot, or other similar application that is configured to automatically discover and retrieve relevant information directly from the one or more knowledge databases (to ). Even further, the data collector  may retrieve previously stored information directly from the database .","Knowledge databases (KBs) (to ) may provide information in different formats. The knowledge databases may be structured, semi-structured or unstructured. A first exemplary type of knowledge database provides a structured collection of formulated or prepared question-answer pairs. Such question-answer paired knowledge databases include, but are not limited to, Yahoo! Answers, WikiAnswer and Baidu Zhidao.","A second exemplary type of knowledge database provides a semi-structured or unstructured collection of natural language documents containing plain text paragraphs. Such plain text knowledge databases may include, but are not limited to, public or private databases or knowledge bases, Intranets, the Internet, web-pages (e.g., news website, domain-based website, Wikipedia, etc.), which can be searched and\/or crawled for content. In some implementations, the data collector  retrieves the plain text from the plain text knowledge databases and preprocesses the plain text before storing it in the database . The plain text may be preprocessed by categorizing it using, for example, a training model.","A third exemplary type of knowledge database stores fact-based information in the Resource Description Framework (RDF) format. RDF is a standard data model for data interchange on the Web. It is published by the World Wide Web Consortium (W3C), and is generally used to conceptually describe or model information that is implemented in web resources. A collection of RDF statements intrinsically represent a labeled, directed multi-graph. As such, an RDF-based data model is naturally suited for certain types of knowledge representation. In practice, RDF data often persist in relational databases or native representations (also known as Triple stores).","RDF knowledge databases include, for instance, YAGO and DBPedia. YAGO extracts data from Wikipedia, WordNet and GeoNames. More particularly, YAGO is built on Wikipedia's info boxes and category pages. Info boxes are standardized tables that contain basic information about the entity described in the article. For instance, there are info boxes for countries, which contain the native name of the country, its capital and its size. Info boxes may be much easier to parse and exploit, relative to natural language text. RDF may also express entities, facts, relations between facts and properties of relations. Facts in YAGO are represented in RDF triple store format, and may be queried using SQL and\/or SPARQL language.","QA processor  serves to automatically process the input question received by the QA framework  and generate one or more answers in response to the question. In one implementation, QA processor  further includes a question parser , an answer retrieval and ranking unit  and an answer assembler .","Question parser  analyzes the question to identify one or more properties associated with the question. Such properties may include, but are not limited to, question type, answer type expected by the user, key words, search focus of key words, and so forth. In addition, question parser  may convert the input question into a database query statement (e.g., in SQL) based on the identified properties by using a predetermined template. The query statement may then be sent to the query language processor  in the DBMS  to access data in the database .","Answer retrieval and ranking unit  performs or invokes a search function to retrieve one or more candidate answers to the input question. The search function may be, for example, a full text search function provided by the DBMS . Answer retrieval and ranking unit  may further generate credit points or scores for each candidate answer based on pre-defined evaluation rules. The credit points or scores may then be used to rank and\/or order the candidate answers. Answer assembler  may serve to identify the most suitable answer from the candidate answers, extract the relevant answer paragraph from the whole text content, and\/or construct the answer text. These and other exemplary features will be described in more detail in the following paragraphs.","QA framework  is communicatively coupled to the DBMS , which includes an index server  and preprocessor server . Index server  generally includes the actual data and the engines for processing the data. It may also coordinate and use other servers. Index server  may include a query language processor  and a search module . Search module  may further include a data store optimizer and execution controller , data store operators  and a text search engine .","To search for data stored in the database , a query statement (e.g., SQL) or a full text search may be invoked. If a query statement is received from the question parser , the query language processor  checks the syntax and semantics of the query statement and generates the logical execution plan. Data store optimizer and execution controller  receives the logical execution plan from the query language processor  as input and generates the optimized physical execution plan. Data store optimizer and execution controller  then executes the optimized physical execution plan by invoking data store operators  to access in-memory data  stored in database .","In some implementations, to enhance the performance of the search, in-memory computing may be leveraged by the DBMS . Data collector  may download original documents from the knowledge databases (-) and store them as in-memory data  in the database . The in-memory data  may be column-based, which can be more efficient than traditional row storage databases. Alternatively, row-based in-memory data  is also useful.","Text search engine  provides full-text (or document) indexing and search capabilities to allow full-text queries to be run. Full-text queries may include simple words and phrases or multiple forms of a word or phrase. In one implementation, text search engine  accelerates query execution by building one or more full text indexes  to facilitate location of records. Document analyzer  in the preprocessor server  may segment sentences or paragraphs in the original documents from the knowledge databases (to ) into separate words for the text search engine  to build the full text indexes . In the case of certain languages based on ideographic characters (e.g., Chinese, Japanese, Korean, etc.) there are no spaces between words. For example, in Chinese, the sentence \u201c\u201d (English translation: His name is Yao Ming), there are no spaces between the words \u201c\u201d (His), \u201c\u201d (Name), \u201c\u201d (is), \u201c\u201d (Yao Ming). Document analyzer  may provide a mechanism to recognize such words.","In response to a full-text query, the text search engine  may perform a full-text search against text data  in full-text indexes  to return search results . The search may include an exact search for words and phrases, a fuzzy search that tolerates typing errors, and a linguistic search that finds variations of words based on linguistic rules of a particular language (e.g., English). Search results  may include any document returned by the search that contains at least one match (also known as a hit). A match occurs when a target document contains all the terms specified in the full-text query, and meets other search conditions, such as the minimum match distance (or similarity measure) between the terms.",{"@attributes":{"id":"p-0052","num":"0051"},"figref":["FIG. 3","FIGS. 1 and 2","FIGS. 1 and 2"],"b":["300","300","100","200"]},"At , the QA processor  receives an input question. The type of question may include, but is not limited to, a fact, list, definition, How, Why, or hypothetical question. The question may also be closed domain (i.e. under a specific domain) or open domain (i.e. about nearly anything). In addition, the question may be multidimensional. For example, the question \u201cWhat does SAP stand for?\u201d may be interpreted as a factual question. Alternatively, it may also be interpreted as an abbreviation question.","At , the question parser  processes the input question to identify properties associated with the question before passing the query to the DBMS . In one implementation, the question parser  includes or invokes functions from a natural language processing (NLP) library for parsing the natural-language input question. NLP may also be used to parse document content from the knowledge databases (-) and extract more detailed semantic and linguistic information.","The input question or document content may be parsed into a set of linguistic distinctions, including, but are not limited to, distinctions as parts-of-speech (POS), sentences, named entities, text tokens, document categories, word chunks, sentence structures, word dependencies, and so forth. For instance, part-of-speech (POS) tagging is a major NLP function that determines if a word in a sentence is a preposition, a noun or any other parts of a speech. Tokenization generally refers to the segmentation of a sentence into \u201ctokens\u201d (e.g., words, punctuation marks, numbers, etc.). Text chunking generally refers to dividing text into syntactically correlated groups of words, such as noun groups, verb groups, and so forth. Such \u201cchunks\u201d of text typically do not specify the internal structure or their role in the original sentence. Named entities generally refer to atomic elements with predefined categories, such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.","The question parser  may use the extracted NLP distinctions to derive higher-level properties associated with the input question. Such properties may include, but are not limited to, the question class (e.g., \u201cwhat\u201d, \u201cwhich\u201d, \u201cwhen\u201d, \u201cwhere\u201d, \u201cwho\u201d, \u201cwhy\u201d, etc.), question sub-class, expected answer type (e.g., about location, person, date, money, etc.), focus, keyword, etc. The question properties may be used to identify the input question's keywords for performing a full text search and finding the most related paragraph.","More particularly, the question parser  may derive the question class by using pre-defined templates that describe a certain question class based on the NLP distinctions. For example, a WHAT question may begin with \u201cWHAT\u201d or sometimes contains a \u201cWHAT\u201d in the beginning of the question. Accordingly, the template for \u201cWHAT\u201d class may be pre-defined as: token value is \u201cWHAT\u201d && group type is \u2018NP\u2019 && token type is \u201cWP\u201d, where NP represents \u201cproper noun\u201d and WP represents \u201cwh-pronoun\u201d. To identify the question class, a multiple-tier loop (e.g., two-tier loop) may be implemented to parse the input sentence. For example, the question parser  may first scan the sentence to identify one or more chunk groups. If a question class cannot be identified based on chunk groups, the question parser  may further scan the sentence to identify one or more tokens. If the tokens match a certain pre-defined template, the corresponding class is determined accordingly and saved.","For purposes of illustration, assume that the input question is: \u201cWhat is Microsoft Office?\u201d. A chunk group identifier may mark the question as follows: \u201cWhat|is|Microsoft Office?\u201d. In other words, \u201cWhat\u201d, \u201cis\u201d and \u201cMicrosoft Office\u201d are identified as chunk groups with corresponding types [WP], [VBZ] and [NP] and its two children are both [NNP]. If these chunk group types are associated with a pre-defined question class, then the question class is identified accordingly. For example, the question class may be identified as \u201cWhat\u201d class, which is defined by the question word \u201cWhat\u201d, its token type is [WP], and the question's target group type is [NP]. If the chunk groups do not match any pre-defined template, the question may be tokenized as [WP] [VBZ] [NNP] [NNP], where [VBZ] represents a verb, 3rd person singular present and [NNP] represents a Proper noun, singular.","Once the question class is determined, the name entity list may be checked to see if any name entity matches the first [NN] token type, where NN represents a noun, singular or mass. If a match is found, the matching name entity from the list is directly selected as the focus. If no match is found, a code level check may be performed to determine the sub-class. More particularly, one or more pre-defined keywords may be used to determine the sub-class. For example, the classes CAPITAL, CITY, COUNTRY, etc. belong to a sub-class LOCATION. Therefore, in the question \u201cWhat city is the largest one in China?\u201d, the pre-defined keyword \u201ccity\u201d may be used to determine the sub-class \u201clocation\u201d.","Another property that may be identified based on the input question is the expected answer type. Answer type generally refers to the type of answer expected by the user or application in response to the input question. In some implementations, the input question class and\/or sub-class is mapped to the corresponding answer type in pre-defined tables.",{"@attributes":{"id":"p-0061","num":"0060"},"figref":"FIG. 4","b":["400","204"]},"For instance, according to a previous rule, the question parser  may only know that the questions \u201cWhat company is the largest in the world?\u201d & \u201cWhat entrepreneur is the richest in the world?\u201d are all WHAT-WHO questions. The first noun (e.g., \u201ccompany\u201d and \u201centrepreneur\u201d) after the question word \u201cwhat\u201d may be used to identify the answer type. Since it is not easy to manually create a rule to cover all cases, an existing training data set may be used as a reference for further determination of the question type.","Yet another property that may be identified based on the input question is the \u201cfocus\u201d. \u201cFocus\u201d generally refers to a sequence of words that defines what the input question is looking for. For example, in the question \u201cWhat is the capital of China?\u201d, the question parser  understands that the question is asking about a LOCATION. However, the question class LOCATION may be too broad, and it may be helpful to narrow down the search to focus on CAPITAL. This means that the answer should be the name of a capital (or city name). The question parser  may identify the focus by extracting, from the input question, the first noun after the question class word (e.g., CAPITAL).","Yet another property that may be identified based on the input question is \u201ckeyword\u201d. An input question may include one or more keywords, which provide the context of the input question and may significantly impact the search results. Keywords may include the focus, although it is not necessarily so in all cases. It may be important to identify the keywords of an input question so that a search may be performed to find the paragraph related to the question. To identify the keywords, the following ordered set of heuristics may be used:\n\n","During runtime, the question parser  processes each heuristic to extract one or more keywords in accordance with the heuristic order shown. For example, Heuristic 1 may be processed before Heuristic 2, Heuristic 2 processed before Heuristic 3, and so forth. It should be appreciated that other types of heuristics or any other heuristic order may also be applied. In one implementation, the question parser  considers only keywords extracted based on the first 4 heuristics; keywords extracted based on the last 2 heuristics may be considered only if more keywords are needed to identify the answer. Alternatively, if an answer cannot be found using any of the extracted keywords, keywords may be dropped in a reversed order in which they have been entered in next several iterations of search so as to broaden the search criteria.","Returning to , at , the QA processor  identifies the type of knowledge database (KB) available for providing data to the QA framework . As discussed previously, the KB may be a question-answer paired KB, a plain text KB, an RDF KB and\/or any other type of KB. The data from the different types of KB may be downloaded and stored in the database , or retrieved directly from the KB using, for instance, a crawler application in the data collector . The type of KB available may be stored in a configuration file. Different search strategies may be implemented for different types of data from the different types of knowledge databases. It should be appreciated that more than one type of knowledge database may be available. By supporting different types of knowledge databases, the present technology is able to provide a high answer finding rate using an expanded collection of knowledge databases.","In one implementation, at , the QA processor  determines that a question-answer paired KB is available. At , the answer retrieval and ranking unit  invokes a full text search by the DBMS  to generate search results . shows a table  containing exemplary question-answer paired data from a question-answer paired KB. The table  may be downloaded from the question-answer paired KB and stored in, for example, the database . As shown, each row of the table  stores a pair of question (or TITLE) and answer (or CONTENT). To facilitate the search, two full text indexes  may be generated based on the columns \u201cTITLE\u201d and \u201cCONTENT\u201d, which represent question and answer contents.","The answer retrieval and ranking unit  may invoke the full text search of the question-answer paired data by generating a query statement (e.g., SQL) based on one or more question properties (e.g., focus, keywords, etc.), and sending the query statement to the query language processor  of the DBMS  for processing and execution. The full text search may be performed based on the full text indexes , rather than the original table.","For example, an exemplary SQL statement may be as follows:",{"@attributes":{"id":"p-0070","num":"0075"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"SELECT *, SCORE ( ) as RANK from \u201cQUESTIONS\u201d where"},{"entry":"CONTAINS ((TITLE, CONTENT), \u2018why is the ocean blue\u2019, FUZZY"},{"entry":"(0.8)) order by RANK DESC;"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}}},"br":{},"b":"220"},{"@attributes":{"id":"p-0071","num":"0076"},"figref":"FIG. 5","i":"b ","b":["512","512"]},"As discussed previously, the full text search may provide fault tolerance for user input of the question text. This allows the search to accommodate typographical errors and improve search quality by returning more possible search results. In one implementation, an n-gram matching algorithm is used to provide a fuzzy full-text search.  shows an exemplary n-gram model . For each string , a set of n-grams  may be generated. The fuzzy search may be performed by matching the n-grams of the input string with the n-grams of the data string. For example, assuming that the input string is \u201chans\u201d and the data strings are \u201cgans\u201d and \u201chaas\u201d, the fuzzy match score  between \u201chans\u201d and \u201cgans\u201d is 63% (i.e. 5 out of 7 n-grams match), while the fuzzy match score  between \u201chans\u201d and \u201chaas\u201d is also 63%. If the desired minimum match score is less than 63%, the data strings \u201chans\u201d and \u201chaas\u201d are returned as search results.","Returning to , at , the answer retrieval and ranking unit  ranks the search results  to determine the most relevant candidate answer. The search results  may include a set of candidate question-answer pairs related to the input question. The answer retrieval and ranking unit  may rank the search results  by assigning a score to each candidate question-answer pair. The most related question is identified based on the scores and the corresponding answer is returned as the candidate answer.","In some implementation, a fuzzy score is assigned to each candidate question-answer pair. The fuzzy score may be computed by determining a measure of similarity between the input question and candidate questions. The measure of similarity is determined by comparing the words in the input question and each candidate question. For example, if the input question is: \u201cWho is Bill Gates?\u201d, and the candidate question contains the exact same words, the fuzzy score is 1.0. However, if the candidate question is \u201cWho is Bill Gate's daughter?\u201d, the fuzzy score may be computed as follows:\n\nSCORE=Common_Word_Count\/((Word_Count_of_Input_Question)*(Word_Count_of_Candidate_Question))=4\/((4)*(5))=0.894.\u2003\u2003(1)\n","At , the QA processor  may determine that a plain text KB is available. At , a search is performed on plain text data from the plain text KB. The question parser  may construct a query statement based on a mining set of search terms (e.g., keywords, focus, etc.) associated with the input question. The query statement is then sent to the query language processor  for processing, and to invoke the search to generate search results. Candidate plain text paragraphs may be returned as search results.","The answer retrieval and ranking unit  may iteratively refine the search by broadening or narrowing the search criteria until the number of search results meets a pre-specified threshold. The search may be broadened by expanding the mining set of search terms (i.e. conflation). A synonyms dictionary (e.g., WordNet) may be used to add synonyms of extracted question properties (e.g., focus, keywords, etc.) to the mining set.","For example, the input question may be as follows: \u201cWho is the principal of Thomas Jefferson high school?\u201d. The keywords may be \u201cprincipal\u201d, \u201cThomas\u201d, \u201cJefferson\u201d, \u201chigh school\u201d and the focus may be \u201cprincipal\u201d. The answer retrieval and ranking unit  may use expanded keywords to perform the search. Expanded keywords are obtained by, for example, mapping original keywords (or stems of keywords) to synonyms, as previously described. For example, if the keywords are \u201chigh school\u201d, the synonym \u201csenior\u201d may be added to the mining set, which may include the following:\n\n","In the above-mentioned mining set of search terms, the word \u201chigh\u201d may be considered too common in the English language, and may represent a totally different meaning than the original keywords \u201chigh school\u201d by itself or combined in other phrases. Such common words may be removed from the mining set to yield more accurate search results. To identify common words, the answer retrieval and ranking unit  may perform a look-up on a pre-defined list of common words.","Another method of expanding the mining set of search terms to broaden the query is via stemming. \u201cStemming\u201d generally refers to a process for reducing inflected (or sometimes derived) words to their stem, base or root form. For example, a stemmer for English may identify the string \u201ccats\u201d (and possibly \u201ccatlike\u201d, \u201ccatty\u201d etc.) based on the root \u201ccat\u201d, and \u201cstemmer\u201d, \u201cstemming\u201d, \u201cstemmed\u201d based on \u201cstem\u201d. Once the stem of a keyword is determined, variations of the stem may be added to the mining set of search terms to broaden the query.","At , the answer retrieval and ranking unit  ranks the search results  to determine the most relevant candidate answer. The search results  may include a set of plain text candidate paragraphs. The number of candidate paragraphs may be very large, particularly when the keywords of the input question are common or ordinary words. The answer retrieval and ranking unit  may rank the candidate paragraphs by assigning a score to each candidate paragraph. The candidate paragraph most relevant to the input question may be identified based on the scores and returned as the candidate answer.","In one implementation, the score includes a term frequency-inverse document frequency (TF-IDF) score. A TF-IDF score is computed by counting the number of words (or synonyms of the words) that are common to both the candidate paragraph and the input question. For example, if the candidate paragraph shares a same common word with the input question, 2 points may be assigned. If the candidate paragraph shares a same synonym with the input question, 1 point may be assigned. No points may be assigned if there are no common words or synonyms. The final score for the candidate paragraph may be obtained by the following:\n\nFinal Score=Total Score\/(Number of different words)\u2003\u2003(2)\n","It should be noted that other types of scoring techniques may also be implemented.","In some cases, the text of the original candidate paragraph may be too long and therefore not suitable as a candidate answer. Before computing the score and ranking such candidate paragraph, the answer retrieval and ranking unit  may segment the long text of the candidate paragraph into smaller segments of text. In one implementation, hierarchy information in metadata provided by the knowledge database is used to split the search result text. For example, original web pages from Wikipedia contain hierarchy information of each paragraph in the metadata that may be used to segment the text. Other information, such as font size and indentation format, may also be used to derive the hierarchy and segment the text.","In some implementations, particularly where there is no information available for segmentation, a tokenization method is used to segment the text. The original text may be tokenized by removing stop words and using the remaining words as basic units. \u201cStop words\u201d generally refer to words that are filtered out prior to, or after, processing the natural language text. For example, \u201cthe\u201d, \u201cis\u201d, \u201care\u201d, etc. are very common words that may be considered stop words.",{"@attributes":{"id":"p-0085","num":"0091"},"figref":["FIG. 7","FIG. 7"],"b":["702","702","702","702","702","702","702","702","702","702","702","702","706","702"],"i":["a","d ","a","b","c ","d","a ","b","b ","c","c ","d","c","d","c","d"]},"Referring back to , at , the answer assembler  constructs the candidate answer based on the most relevant candidate paragraph. There are many ways of delivering the same answer. In one implementation, the answer assembler  determines the template of the input question, and maps the question template to an answer template. The candidate answer may then be assembled in accordance with the answer template.",{"@attributes":{"id":"p-0087","num":"0093"},"figref":"FIG. 8","b":["800","800","800","208"]},"In some cases, the level of detail in the most relevant paragraph may be too high, and needs to be further reduced to yield higher precision. For instance, the question may be as follows:\n\n","It should be appreciated that not all paragraphs may be refined or shortened. For example, a factoid question such as \u201cWhy is the sky blue?\u201d may yield the following paragraph: \u201cThe sky is blue because of the way the Earth's atmosphere scatters light from the sun.\u201d It is not necessary to shorten such paragraph. The entire paragraph may be returned as the candidate answer.","To extract the candidate answer from the paragraph, the answer assembler  may employ a machine learning framework. For example, the input question may be as follows:\n\n","In some implementations, the training set is generated by first collecting a set of known question and answer pairs. For example, the question \u201cWhen was Bill Gates born?\u201d may be paired with the known answer \u201cOct. 28, 1955.\u201d The set of known question and answer pairs may then be analyzed to extract the key words (e.g., \u201cBill Gates\u201d, \u201cborn\u201d, etc.). The internet (e.g., Google, Bing, Yahoo, etc.) or other knowledge databases may be searched via, for example, a search engine, to collect documents associated with the key words. Unnecessary sentences in the documents that do not include the keywords may be removed. In addition, the documents may be tokenized for further analysis. The longest matching strings may be retrieved from the original documents to form the training set.","For example, the following training information may be collected: \u201cEntrepreneur Bill Gates was born on Oct. 28, 1955, in Seattle, Wash.\u201d; \u201cBill Gates born on 28 Oct. 1955 Seattle, Wash. is the founder of Microsoft\u201d; and so forth. Based on the collected information, the following answer templates may be identified: (i)<NAME> was born on <Answer>; and (ii)<NAME> born on <Answer>.","After the training set is generated, the candidate answer may be extracted from input paragraphs. For purposes of illustration, assume that the input question is: \u201cWhen was Wolfgang Amadeus Mozart born?\u201d. The original text of a paragraph returned by the answer retrieval and ranking unit  may be as follows: \u201cWolfgang Amadeus Mozart was born on 1756\u201d. Based on the training set, the input question type may correspond to the answer template: <NAME> was born on <Answer>. Since the paragraph matches the answer template, the answer \u201c1756\u201d may be extracted as the candidate answer.","The above-mentioned answer assembly mechanism is particularly suitable for factoid and certain types of questions. However, it should be appreciated that not all types of questions may be processed by the above-mentioned answer assembly mechanism. A degradation mechanism may be required for certain question types. For instance, if answer extraction is difficult, the answer assembler  may search the paragraph only by the keywords and return only the search result as the candidate answer. Although answer accuracy may not be high, at least basic related information is provided to the end user.","Referring back to , at , the QA processor  may determine that an RDF KB is available. At , the answer retrieval and ranking unit  invokes an RDF search by the DBMS . The RDF search may be invoked by submitting a query constructed by the user using SPARQL Protocol and RDF Query Language (SPARQL). SPARQL is standardized by the RDF Data Access Working Group of the W3C and is an official W3C recommendation. SPARQL allows for a query to include triple patterns, conjunctions, disjunctions, patterns, etc. SPARQL also allows for federated queries where the query is distributed to multiple locations and results from the distributed query are aggregated. SPARQL queries are translated to SQL queries before processing by query language processor .",{"@attributes":{"id":"p-0096","num":"0109"},"figref":"FIG. 9","b":"129","ul":{"@attributes":{"id":"ul0009","list-style":"none"},"li":{"@attributes":{"id":"ul0009-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0010","list-style":"none"},"li":"\u201cWhat is the capital of India?\u201d\n\nUpon parsing the input question, the keywords of the input question may be \u201ccapital\u201d and \u201cIndia\u201d. Predefined patterns may be stored in the DBMS  in, for example, column tables that define corresponding question types, keywords, predicates, etc. By using a predefined pattern, the answer retrieval and ranking unit  translates the input question directly into an SQL query to access the RDF knowledge database, as follows:\n"}}}},{"@attributes":{"id":"p-0097","num":"0111"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"Select object from Yago facts where subject = \u2018<India>\u2019 and"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"predicate = \u2018<hasCapital>\u2019"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}}},"In another example, the input question may be as follows:\n\n",{"@attributes":{"id":"p-0099","num":"0114"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"select distinct T3.subject, T3.object from Yago facts T1"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"inner join Yago facts T2 on T1.object = T2.subject"]},{"entry":[{},"inner join Yago facts T3 on T1.object = T3.subject"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"where T1.subject =\u2018<Bill_Clinton>\u2019 and T1.predicate = \u2018<hasChild>\u2019"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"and T2.predicate = \u2018<hasGender>\u2019 and T2.object = \u2018<female>\u2019"]},{"entry":[{},"and T3.predicate = \u2018<wasBornOnDate>\u2019"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}}},"Alternatively, the above-mentioned input question may be translated into a more readable SPARQL query, as follows:",{"@attributes":{"id":"p-0101","num":"0116"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"196pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"Select ?d where {"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"77pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"56pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"?s","wasBornOnDate","?d."]},{"entry":[{},"?s","hasGender","\u2002female."]},{"entry":[{},"Bill_Clinton","hasChild","?s}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]}}]}}},"To facilitate the query formulation process, a SPARQL-to-SQL mapping engine may be implemented. This allows SPARQL queries to be supported by an SQL-based DBMS. To map keywords to SPARQL triple patterns, relevant Internationalized Resource Identifiers (IRIS) related to each keyword may first be retrieved. An IRI within an RDF graph is a Unicode string that conforms to the syntax defined in RFC 3987 [IRI]. IRIs generalize uniform resource identifiers (URIs) or uniform resource locators (URLs). Every absolute URI or URL is an IRI.","Next, a pre-configured SPARQL triple pattern template is applied to the query based on the retrieved IRIs. Some SPARQL triple pattern templates may be configured based on observations of YAGO facts (e.g., predicates like <hasGender>, <hasCapital>, etc.). For keywords that have no pre-configured patterns, the DBMS performs a full text fuzzy search on the subject and object columns to retrieve an approximately matching IRI in the YAGO facts table. The matching IRI is then used in the SPARQL query.","As discussed previously, to enable a full text search on subject and object columns, a full text search index may first be created on the subject and object columns. The answer retrieval and ranking unit  may generate the following SQL statement to perform the fuzzy search:",{"@attributes":{"id":"p-0105","num":"0120"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"select *, score( ) as rank from \u201cyaofacts\u201d where contains((subject,"]},{"entry":[{},"object),"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"\u2018XXX_XXX\u2019,FUZZY(0.8)) order by rank desc;"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"If no answer is retrieved with the selected SPARQL query, the query may be repeated with a reduced fuzzy co-efficient and\/or reduced number of keywords until some answers are retrieved.","After the search results are obtained, the number of search results (i.e. count) is compared to a pre-specified threshold. If the search count is less than the pre-specified threshold, the search criteria may be broadened by, for example, removing search terms and\/or changing logical operators between the search terms from AND to OR. For example, the original search criteria may be as follows: principal&Thomas&Jefferson&(high school). The search criteria may be expanded to the following: (principal|(school principal)|(head teacher))&Thomas&Jefferson&((high school)|(senior high school)).","Conversely, if the search count is higher than the pre-specified threshold, the search criteria may be narrowed by, for example, adding search terms and\/or changing logical operators between the search terms from OR to AND. Accordingly, a suitable number of search results (e.g., text paragraphs) may be returned.","Referring back to , at , the answer retrieval and ranking unit  ranks the RDF search results to determine the most relevant candidate answer. The answer retrieval and ranking unit  may rank the RDF search results by assigning a fuzzy score or any other suitable score to each RDF search result. The most related question may be identified based on the score and the corresponding candidate answer is returned as the candidate answer.","At , the QA processor  determines whether there are any other types of KB available. If so, step  is repeated to determine what type KB is available. It should be appreciated that steps ,  and\/or  may also be performed in parallel to obtain the search results .","At , the QA processor  evaluates the candidate answers to find the final answer to be returned to the user. Each type of knowledge database (e.g., question-answer paired KB, plain text KB, RDF KB, etc.) may yield zero or one candidate answer. Where there are multiple types of knowledge databases, there may be multiple candidate answers.","Since it is not easy to compare the candidate answers obtained from the different types of knowledge databases, the QA processor  may use one or more pre-defined heuristic rules based on the question type to determine the final answer. In some implementations, all the candidate answers are ranked to find the final best one. The ranking may be based on one or more pre-defined heuristic rules. For example, if the candidate answer was retrieved from a question-answer paired KB and its SCORE ( ) is greater than or equal to 80%, the candidate answer will be returned as the final answer. Such candidate answer is deemed the most accurate and thus assigned the highest priority, because question-answer pairs are defined manually. Next, if no candidate answer was retrieved from a question-answer paired KB, the answer assembler  determines if the input question is a pure factoid question. If so, the candidate answer retrieved from an RDF KB is returned as the final answer if possible. RDF data are structure data that are naturally suitable for factual description, which is typically used to answer a factoid question. Factoid questions are questions such as \u201cwhat is\u201d, \u201cwho is\u201d and etc. Next, if there is no suitable candidate answer retrieved from an RDF KB, the candidate answer retrieved form the plain text KB is returned as the final answer.","In some cases, the QA processor  may determine that all the candidate answers are not good enough, or that there is no candidate answer. In such case, the answer assembler  may send a text message to the end user, such as \u201cSorry, I don't know the answer.\u201d","The evaluation process may be refined by recording the feedback from the end user in historical question and answering tasks. In such case, the user may rate or input a ranking score for each answer. The QA processor  may also allow the end user to correct the answer and manually answer the question. The QA processor  may then improve its question-answering capability based on the user feedback.","Although the one or more above-described implementations have been described in language specific to structural features and\/or methodological steps, it is to be understood that other implementations may be practiced without the specific features or steps described. Rather, the specific features and steps are disclosed as preferred forms of one or more implementations."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["Some embodiments are illustrated in the accompanying figures, in which like reference numerals designate like parts, and wherein:",{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 5","i":"a "},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 5","i":"b "},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 9"}]},"DETDESC":[{},{}]}
