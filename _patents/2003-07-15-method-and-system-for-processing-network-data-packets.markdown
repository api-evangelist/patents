---
title: Method and system for processing network data packets
abstract: A system for transmitting and receiving TCP/IP data packets using a hardware engine is provided. The system includes an inbound MAC Receive state machine for processing MAC frames received from a network; an inbound IP verifier state machine for verifying IP packet headers; an inbound IP fragment processing state machine for processing and reassembling IP fragments; and an inbound TCP state machine for processing TCP segments received from an IP layer. The system also includes an outbound MAC Transmit state machine that sends MAC frames to a network; an outbound IP state machine that processes IP data to be passed to a MAC layer for transmission; and an outbound TCP state machine that processes TCP data to be passed to the IP layer for transmission.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07403542&OS=07403542&RS=07403542
owner: QLOGIC, Corporation
number: 07403542
owner_city: Aliso Viejo
owner_country: US
publication_date: 20030715
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS REFERENCE TO RELATED APPLICATIONS","BACKGROUND","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS"],"p":["This application claims priority to U.S. provisional patent application Ser. No. 60\/397,419, filed on Jul. 19, 2002, the disclosure of which is incorporated herein by reference in its entirety.","This application is also related to the following patent applications, filed on even date herewith:","Application Ser. No. 10\/620,076, entitled \u201cMETHOD AND SYSTEM FOR PROCESSING NETWORK DATA PACKETS\u201d; and","Application Ser. No. 10\/619,719, entitled \u201cMETHOD AND SYSTEM FOR PROCESSING NETWORK DATA PACKETS\u201d. the disclosure of which are incorporated herein by reference in their entirety.","1. Field of the Invention","The present invention relates to computer networks, and more particularly, to processing network data packets using hardware components.","2. Background of the Invention","Computer networking is commonplace in today's world. Network computing allows users to share information regardless of where they are located. Network computing has also increased the use of mass storage devices that can store data. Such storage devices often have to interface with networks to exchange commands and\/or read and write data. Storage controllers are used to facilitate interaction between storage systems and computing systems.","Traditionally, storage controllers (e.g., disk array controllers, tape library controllers) have supported the SCSI-3 protocol and have been attached to computers by a Small Computer System Interface (SCSI) parallel bus or Fibre Channel.","Internet SCSI (iSCSI) standard as defined by the Internet Engineering Task Force (IETF) maps the standard SCSI protocol on top of the TCP\/IP protocol.","Networks are generally defined as having layers of protocol. The iSCSI and TCP\/IP protocol suite consist of 4 protocol layers; the application layer (of which iSCSI is one application), the transport layer (TCP), the network layer (IP) and the link layer (i.e. Ethernet). A complete description of the TCP\/IP protocol suite is provided in \u201cTCP\/IP\u201d Illustrated, Vol. 1 by W. Richard Stevens and Volume 2 by Gary R. Wright and W. Richard Stevens published by Addison Wesley Professional Computing Series.","TCP Overview","TCP is a network protocol that provides connection-oriented, reliable, byte stream service. This means that two nodes must establish a logical connection before sending data and that TCP maintain state information regarding the data transfer. Reliable means that data is guaranteed to be delivered in the same order that it was sent. A byte stream service means that TCP views data to be sent as a continuous data stream that is sent in any way it sees fit and delivers it to the remote node as a byte stream. There is no concept of a data frame boundary in a TCP data stream. Applications, such as iSCSI, must provide their own mechanisms for framing data, if it is needed.","Sequence Numbering in TCP Data Transfer","Each byte of data sent using a TCP connection is tagged with a sequence number. Each TCP segment header contains the sequence number of the first byte of data in the segment. This sequence number is incremented for each byte of data sent so that when the next segment is to be sent, the sequence number is again for the first byte of data for that segment. The sequence numbering is used to determine when data is lost during delivery and needs to be retransmitted.","A data packet receiver keeps track of the sequence numbers and knows the next sequence number when a new segment arrives. If the sequence number in the segment is not the expected one, the receiver knows that the segment has arrived out of order. This could be because the network reordered the segments or a segment was lost. Typically, TCP handles both of these cases.","TCP initially assumes that data is arriving out of order for a short number of segments or time. If the out of order segment does not arrive after three segments, the segment is considered lost and is retransmitted.","TCP Data Segments","All TCP data segments are protected by a checksum. The checksum algorithm includes 16 bit ones complement addition of the entire TCP segment. On transmission, the \u201cones\u201d complement of the calculation is stored in the segment. On reception, the checksum calculation includes the transmitted complemented checksum so that the result of the receiver's checksum is all 1's.",{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 1A"},"Delayed ACK Packets","Typically, when a TCP segment is received on a node, an acknowledgement (\u201cACK\u201d) packet is returned to acknowledge reception of the packet. To help reduce the number of segments on a network, TCP may delay the delivery of an ACK packet. The ACK packet is held for a set time period to see if another ACK packet is to be sent or if the ACK can be coupled to a data segment that is being sent back. The delay in sending ACK packets occurs when data is being received in order, and skipped, if a segment is out of order.","Internet Protocol (\u201cIP\u201d) Overview","The IP protocol provides a datagram service whose function is to enable routing of data through various network subnets. Each of these subnets could be a different physical link such as Ethernet, ATM, etc. IP is also responsible for fragmentation of the transmit data to match a local link's MTU. IP can fragment data at the source node or at any intervening router between the source and destination node. The destination IP reassembles fragments into the original datagram sent.","Most conventional solutions for controlling communications between storage controllers and networks are via software often based on Open Systems Interconnection (OSI) model. The iSCSI protocol with the TCP\/IP protocol stack running in software on a computer requires a large amount of computing power, especially at current 1 giga bits per second (1 Gbps) and future 10 Gbps network rates.","Mixed software and hardware solutions have been also been proposed. One such solution is provided in U.S. Pat. No. 6,226,680 (Boucher et al.). In Boucher et. al., a network interface card uses a \u201cfast path microprocessor or the host stack\u201d. This decision is based on a summary of packet headers. A host software stack processes some packets and others are processed by a \u201cfast path microprocessor\u201d.","The system and process illustrated in Boucher et. al. still requires processing by a software stack and hence is not suitable to the present high bandwidth requirements.","Therefore, what is needed is a process and system that can process network packets in storage controllers efficiently and quickly to meet the present and future high bandwidth requirements.","In one aspect of the present invention, a system for transmitting and receiving TCP\/IP data packets using a hardware engine is provided. The system includes an inbound MAC Receive state machine for processing MAC frames received from a network; an inbound IP verifier state machine for verifying IP packet headers; an inbound IP fragment processing state machine for processing and reassembling IP fragments; and an inbound TCP state machine for processing TCP segments received from an IP layer.","The system also includes an outbound MAC Transmit state machine that sends MAC frames to a network; an outbound IP state machine that processes IP data to be passed to a MAC layer for transmission; and an outbound TCP state machine that processes TCP data to be passed to the IP layer for transmission.","The outbound IP state machine builds IP header data and passes the header data to the outbound MAC Transmit state machine and the outbound TCP state machine builds TCP header data and passes the header data to the outbound IP state machine. The inbound IP verifier state machine passes non-IP data packets to a host and also verifies IP packet header information and if the header information is valid, and then temporarily stores the packet in an external memory.","The inbound IP fragment processing state machine provides a timer, to time each datagram reassembly with a programmable timer value.","The inbound TCP state machine maintains a segment re-assembly list for each network connection that is linked to a network control block and is used to re-order out of order TCP data segments.","In yet another aspect, a system for processing network data packets using a hardware engine is provided. The system includes a verification module that verifies incoming data packets; a first in-bound TCP processor for processing TCP segments received from a network; a fragment processor that receives data packet fragments and reassembles them into complete datagrams for delivery; and a second in-bound processor for processing incoming TCP segments destined for iSCSI.","In another aspect of the present invention, a system for processing incoming TCP data packets, is provided. The system includes, an input processing module that determines if a TCP connection is established and checks for TCP flags to determine if a TCP datapacket should be processed; an acknowledgement processor module that handles any acknowledgement information included in the TCP packet; and a Data processor module that handles any data included in the TCP data packet.","In yet another aspect of the present invention, a network control block (NCB) used in a system for processing network data packets using a hardware engine is provided. The NCB includes plural status flags, control flags, destination address, header fields and\/or TCP connection information, wherein NCBs are used to provide plural parameters to plural modules in the system and are maintained in a local memory and\/or host memory.","In yet another aspect, a system for processing network data packets using a hardware engine is provided. The system includes, a TCP Table manager for managing a TCP connection's state information by providing a pool of buffers used for various data structures and providing plural registers and timer functions to various system sub-modules. The TCP Table manager maintains a free list of data structures that are used for storage of TCP connection state and for torage of TCP transfer requests.","The TCP Table Manager includes a command processor that arbitrates between plural command sources and translates a received command to an output action(s) to other TCP Table Manager components.","In yet another aspect of the present invention, a system for processing network data packets using a hardware engine is provided. The system includes an outbound TCP processor that takes requests from a host to transmit TCP data, transmits the TCP data following TCP rules and signals to a host when the transmission is complete and has arrived on the remote node; and transmits TCP acknowledgements in response to TCP data received.","The system also includes a request manager that downloads an input\/output control block (\u201cIOCB\u201d) and determines what action is required with respect to the downloaded IOCB.","In yet another aspect of the present invention a system for processing network data packets using a hardware engine is provided. The system includes an inbound IP fragment processor that receives IP datagram fragments and manages the reassembly of any number of in-process datagrams, wherein re-assembled datagrams are passed to a TCP processor or to a host for non-TCP packets.","The IP fragment processor includes an input processor for parsing data packet header information, assembling datagrams, and interfacing with an output processor and a return processor.","In yet another aspect of the present invention, a method for processing IP datagrams using an outbound processing state machine in an outbound processor, wherein the IP datagrams are generated by a host system is provided. The method includes creating an IOCB with plural host memory addresses that define host data to be sent and a host memory address of a network control block (\u201cNCB\u201d) used to build network protocol headers, wherein the host sends the IOCB to the outbound processor.","The outbound processor reads the NCB from host memory and creates an IP and MAC level protocol header(s) for a data packet(s) used to send the IP data. If a datagram fits into an IP packet, the outbound processor builds headers to send the datagram and then uses the plural host memory addresses defining the host data to read the data from the host, places the data into the packet and sends the packet.","If a datagram is greater than a certain size, the outbound processor generates packets with fragments of the datagram using the NCB information to build headers and then uses the plural host memory addresses defining the host data to read the data from the host, places the fragments of the datagram into each packet and sends the packets.","In yet another aspect, a method for processing TCP data packets generated by a host system using an outbound processing state machine in an outbound processor, is provided. The process includes, creating an IOCB with plural host memory addresses that define the host data to be sent and a memory address of a NCB used to build network protocol headers, wherein the host sends the IOCB to the outbound processor; verifying if a TCP window is open; building TCP\/IP\/MAC headers; and sending the data packet(s).","In yet another aspect, a method for processing a TCP data transmit request after a TCP window is closed and then reopened by the reception of an ACK packet using an outbound processor is provided. The process includes, reading a network control block (NCB) into a local memory; reading a delayed request (IOCB) linked to the NCB; verifying if a TCP window is open; building TCP\/IP\/MAC headers; and sending the data packet(s).","In yet another aspect of the present invention, a method for processing fragmented IP datagrams received from a network is provided. The method includes, receiving the IP fragments into buffers in a local memory; linking the IP fragment to a reassembly list for a particular IP datagram; and when all fragments are present, sending the complete datagram to TCP or a host for additional processing.","This brief summary has been provided so that the nature of the invention may be understood quickly. A more complete understanding of the invention can be obtained by reference to the following detailed description of the preferred embodiments thereof concerning the attached drawings.",{"@attributes":{"id":"p-0093","num":"0092"},"figref":"FIG. 2A","b":["100","102","104","107","108","103","105","106","109","102","104","107","108","101","104","104"]},"In one aspect of the present invention, a single chip system  of  is provided that allows connection of a SCSI based mass storage device system directly to a gigabit Ethernet LAN. The system (chip) according to the present invention can be used for both initiator and target applications (i.e. can be used on a host bus adapter or on a redundant array of inexpensive disks (\u201cRAID\u201d) controller). The chip provides hardware assistance to improve the speed of iSCSI read and write transactions as well as a full hardware implementation of a TCP\/IP protocol stack to assure full gigabit operation. The chip also includes an embedded gigabit Ethernet MAC, to connect a PCI based host to a LAN.","The present invention provides a hardware implementation of a full network protocol stack. Application Programming Interfaces (APIs) to this protocol stack are made available to allow host software to take advantage of the hardware acceleration for straight network applications.","The present invention may be used on a PCI development board with a Field Programmable gate Array (\u201cFPGA\u201d). The chip may also be integrated into an Application Specific Integrated Circuit (\u201cASIC\u201d) with an embedded serialize\/de-serializer (\u201cSERDES\u201d . ) and internal programmable RAM  and SDRAM .",{"@attributes":{"id":"p-0097","num":"0096"},"figref":["FIG. 2B","FIG. 2B"],"b":["200","300","300","206","202","205","300"]},{"@attributes":{"id":"p-0098","num":"0097"},"figref":"FIG. 2C","b":["200","300"]},{"@attributes":{"id":"p-0099","num":"0098"},"figref":"FIG. 3A","b":["300","312","313","307","314"]},"MAC Transmit module , Outbound IP\/MAC Processor module (\u201cOIP\u201d)  and the Outbound TCP processor (\u201cOTP\u201d)  implement the Outbound TCP\/IP Hardware Stack, which processes all outbound networking requests from Host  and the ULPP subsystem (not shown).","MAC Receive module , Inbound FIFO Block (IFB) , the IP Verify\/Input Queuing module (\u201cIPV\u201d) A, IP fragment Processor (\u201cIFP\u201d)  and the Inbound TCP Processor (\u201cITP\u201d)  implement the Inbound TCP\/IP Hardware Stack, which processes all inbound networking packets destined for Host  or the ULPP Subsystem.","Memory Access Manager (\u201cMAM\u201d) , Buflet List Manager  and Local RAM  implement the Local Memory Subsystem, which is used to store received network frames while they are processed, TCP connection state information and various other state information used by the TCP and IP protocol standard.","PCI\/PCI-X Interface  (same as  and A. ) and direct memory access (\u201cDMA\u201d) Arbiter (DA)  implement a DMA subsystem that is used to transfer data between system  and host . Network Request Manager (\u201cNRM\u201d)  and the Network Completion Manager (\u201cNCM\u201d)  implement a subsystem for transferring messages between the TCP\/IP hardware engines and host  via  and . SCSI Request Manager (\u201cSRM\u201d)  and SCSI Completion Manager (\u201cSCM\u201d)  perform the same function for the ULPP subsystem. Outbound DMA Engine (\u201cODE\u201d)  and Inbound DMA Engine (\u201cIDE\u201d)  are used to transfer network data between Host  and System . This data can consist of TCP, IP or MAC level packet data. The remaining modules of system  provide other support functions for the subsystems described below.","The following provides a description of various  components:","PCI Interface  (PCI I\/F):","PCI Interface  performs the following functions:\n\n","DMA Arbiter (DA ):","DA  takes all connected block requests for DMA, prioritizes and executes each request. DA  provides synchronization across clock domains from the variable PCI clock (0-133 MHz) to an internal system clock. DA  provides a generic Host  register access port to Register Block  to hide the actual host bus.","Most functional components that interact with DA  request a fixed length of data. DA  knows the lengths for these components and requests the appropriate size of DMA transfer without the need for the block to provide the length.","DA  converts the \u201clittle endian\u201d format of the PCI bus to the \u201cbig endian\u201d format used by System . This requires DA  to do word swaps for components that perform control structure movements. For components that perform packet data movement, DA  does an 8 byte swap (i.e. from the least significant byte (\u201cLSB\u201d) to most significant byte (\u201cMSB\u201d) and MSB to LSB).","For outbound DMA engine (ODE) , DA  accepts a control bit that indicates control structure access versus data access. On control structure access, it also performs word swaps.","For IDE , ODE  and SDE , DA  implements large memory based FIFOs to provide for long bursts on PCI-X .","For remaining components, DA  has a small (16-64 bytes) FIFO for each client to allow the client to queue up its entire transfer before the PCI DMA is requested to PCI Interface .","Register Block ","Register Block  performs the following functions:","Implements Configuration, Control, Status and Port Serial identification (\u201cID\u201d) registers; provides interfaces for other components that have Host  accessible registers and generates timer tick for TTM . Register Block  also provides interface to external Flash BIOS ROM  via register access and multiplexes signals from PCI I\/F  to access BIOS ROM  via this external interface; and provides interface to external Serial NVRAM  via register access and multiplexes signals from PCI I\/F  to access NVRAM  via this external interface.","Network Request Queue Manager (NRM ):","NRM  maintains a queue that Host  can place requests for data transmission and passes these requests to Network Pipeline A when it is ready. NRM  manages Host  memory resident circular queue with Host  as the producer and System  as the consumer.","NRM  maintains a pair of pointers (the producer and consumer pointers) that track the requests in the circular queue. Host  updates the producer pointer when it places new requests in the queue and System  updates the consumer pointer when it takes the request from the queue.","NRM  also maintains a copy of the consumer pointer in Host  memory location to keep Host  from having to read from System  to find out if a queue entry has been used. This allows Host  to use a fast memory fetch to see the pointer instead of a slow I\/O fetch to read the register.","NRM  also provides a special operating mode for OTP  to allow it to read down a request, except the last word. The last word is read if resources are found to allow the request to be processed. If the resources are not there, OTP  aborts the request and later asks the same request to be passed down when the resource is available.","Network Completion Queue Manager (NCM )","NCM  maintains a message queue between System  and Host . It takes completion messages from any of the attached components, prioritizes them and then passes a completion message to Host  memory queue.","NCM  manages a Host  memory resident circular queue with System  as the producer and Host  as the consumer. NCM  maintains a pair of pointers (producer and consumer pointers), which track messages in the circular queue. System  updates the producer pointer when it places new messages in the queue and Host  updates the consumer pointer when it takes the message from the queue.","NCM  also maintains a copy of the producer pointer in a Host  memory location to keep Host  from having to read System  to find out if a queue entry has been filled. This allows Host  to use a fast memory fetch to see the pointer instead of a slow I\/O fetch to read the register.","NCM  generates a signal to cause DA  and PCI Interface  to generate an interrupt when the completion message is in Host  memory. NCM  implements interrupt avoidance rules to prevent unnecessary interrupts from being generated.","Request Arbiter (RA )","Request Arbiter  takes requests from TTM , ERM  and NRM , arbitrates between them and grants them access to the Network Pipeline A.","RA  also provides three programmable priority schemes; round robin, network highest or OAP  highest and grants access when Network Pipeline A is idle as indicated by various idle signals.","Outbound DMA Engine (ODE ):","ODE  takes DMA requests from OTP  and OIP , multiplexes them into a single DMA request and then passes the individual requests to Memory Access Manager (MAM ), DA , or to RISC Memory Interface (RMI ). ODE  also accepts a signal from components that indicate whether the DMA requested is for data or control structures and passes it to DA  to program the proper type of \u201clittle to big endian format conversion\u201d; and truncates 64 bit address to a 32 bit address for access to MAM  and RMI . ODE  also word-packs 32-bit data from MAM  or RMI  into 64-bit data.","Outbound TCP Processor (OTP ): OTP  provides the following functions:","Handshakes outbound data transfer requests, also known as I\/O Control Blocks (IOCBs) from RA . These requests can originate from Host  or OAP .","Obtains connection state information, Network Control Block (NCB) from Host  memory, RISC memory or Local RAM  via TCP Table Manager (TTM ) block.","Sends as much data as allowed by TCP windowing protocol and congestion avoidance algorithms. This involves fetching address lists from Host  or RMI  and then fetching the actual data from Host  memory, RMI  or Local RAM .","The process of sending data includes signaling OIP  to build the IP and MAC layer headers, building the TCP header and then passing the header and data to OIP  to be sent onto the Ethernet link. It also includes saving new connection state in NCB and write the NCB to Local RAM  for later use when ACKs are returned from remote node via TTM  interface. As ACKs return, OTP  sends more data if needed or else finish processing request and passes a completion message to Host  signaling that the request is done.","OTP  supports all currently defined congestion control techniques including; Slow start, congestion avoidance, fast retransmit and fast recovery. (Per the RFC2581 standard, incorporated herein by reference in its entirety).","Outbound IP Processor (OIP ):","OIP  processes both MAC and IP transfer requests (IOCBs) and transmits the associated data. It also acts as a pass through for TCP data from OTP . OIP  performs the following: For MAC layer transfers, System  passes an entire frame from Host  memory to the link. The hardware assumes that Host  has completely formatted the frame, with the possible exception of inserting the source address.","The term MAC throughout the specification means Media Access Control, as used with respect to MAC address and MAC layer. Media Access Control address, is a hardware address that uniquely identifies each node of a network. In IEEE 802 networks, the Data Link Control (DLC) layer of the OSI Reference Model is divided into two sublayers: the Logical Link Control (LLC) layer and the Media Access Control (MAC) layer. The MAC layer interfaces directly with the network media. The MAC sublayer uses MAC protocols to ensure that signals sent from different stations across the same channel (link) don't collide.","Media Access Control Layer is one of two sublayers that make up the Data Link Layer of the OSI model. The MAC layer is responsible for moving data packets to and from one Network Interface Card (NIC) to another across a shared channel.","OSI is an ISO standard for worldwide communications that defines a networking framework for implementing protocols in seven layers. Control is passed from one layer to the next, starting at the application layer in one station, proceeding to the bottom layer, over the channel to the next station and back up the hierarchy.","For MAC frames less than 64 bytes, OIP  pads them to be 64 bytes. For IP layer transfers, System  builds the IP header from information contained in an NCB, whose address is passed down in the IOCB. OIP  then DMAs the data for the IP packet from Host , RMI  or Local RAM  using ODE . OIP  also fragments IP packets that are larger than programmed maximum transmission unit (\u201cMTU\u201d) size of the Ethernet link. This requires generation of new IP and MAC headers for each fragment of the IP datagram.","OIP  generates MAC and IP headers for pass through data sent by OTP ; generates IP, TCP and\/or UDP checksums and inserts them into the data stream; stops transmitting packets (at the next possible packet boundary) when the MAC signals that a Pause frame has been received. OIP  sends Pause packets after the next packet when Buflet List Manager (BLM ) indicates it is too low on memory to receive new frames and sends the \u201cresume frame\u201d when BLM  indicates it is time to send.","IP requests coming from Host  are in two forms, fully formed datagrams to be passed without modification or IP data to have a header attached to it. For fully formed datagrams, System  adds a MAC header and passes it to Outbound FIFO Block OFB . For IP data requests, OIP  builds complete IP header from entries in the NCB. OIP  may fragment the resulting datagram and add a MAC header. This means that all relevant IP fields in the NCB are filled before the send request is made.","Outbound FIFO Block (OFB ):","The function of OFB  is to store outbound frames and then burst them to the Ethernet Network. OFB  is sized to handle jumbo packets and stores\/forwards frames so that no underruns occur due to a slow back plane. OFB  also implements shadow pointers for TCP and IP checksum insertions and records memory locations for words with the TCP and\/or IP checksum location tags.","Ethernet MAC A:","Ethernet MAC A (or MAC A) supports a full duplex operation and supports a connection to an external Serializer\/Deserializer (SerDes) via a Ten Bit Interface (TBI). Ethernet MAC A handshakes received frame data for inbound FIFO (IFB)  and verifies CRC. It then provides a signal to inbound FIFO (IFB)  to flush a current frame if a received frame is too short, too long, invalid EOP, invalid transmission character, or bad cyclic redundancy check (\u201cCRC\u201d).","Ethernet MAC A can source a status word to Inbound FIFO (IFB)  as the last word of each frame, which specifies frame length, broadcast, multicast, unicast and length of the MAC header and pad 2 bytes before MAC header to the packets intended for inbound FIFO (IFB)  to align the IP header on a 64 bit boundary. Ethernet MAC A also adjusts MAC header length and total length in status word to account for this.","Ethernet MAC A generates CRC for transmit frames, supports reception of Ethernet and IEEE 802.3 frames, and supports VLAN\/Priority for which a receiver removes VLAN tags, if present, to keep subsequent protocol headers aligned. The VLAN tag is passed up as part of the status word. Ethernet MAC A recognizes a pause packet and provides a pause signal to OIP ; and supports - MAC unicast addresses (reception). Ethernet MAC A also provides receive error counters, including CRC error, invalid transmission characters, loss of signal\/loss of sync greater than a certain value, for example 10 ms, frame too short, or frame too long. It also provides counters for: transmitted frame count, transmitted byte count, received frame count, and received byte count.","Ethernet MAC A also generates and checks parity, accepts all packets to multicast addresses, supports auto-negotiation as defined in IEEE802.3 Section 37, and inserts source MAC address in transmitted frame B.","Inbound FIFO Block (IFB ):","IFB  buffers incoming Ethernet frames while MAC A (at MAC Rx , the receive segment of MAC A) validates them. IFB  allows for crossing from the 62.5 MHz receive clock domain of MAC A to the 62.5 MHz system clock domain of System . IFB  also provides storage for a jumbo frame, a shadow pointer to allow a status word to be written at the head of the frame and the frame to be flushed or dumped.","Buflet Free List Manager (BLM ):","BLM  manages a list of empty buffers (also called buflets) used to receive frame data. BLM  delivers empty buflets to IPV A and accepts empty buflets from any of the inbound components that processes incoming data. BLM  initializes Local RAM  to create the original list of free buflets; provides for a programmable buflet length; and sends a signal to OIP  to send a Pause packet if free buflet list gets below programmable threshold and removes the signal when the list grows back above threshold.","BLM  also implements a state machine that operates in the background and runs a linked list, counts the number of buflets and then compares a current count. If a comparison error occurs, BLM  sets a status bit and sends a signal to MAM  to stop memory access; and sends the threshold window of buflets available (2\u2032b11=almost full, 2\u2032b00=almost empty) to ITP . This is used to adjust the window on active connections.","IP Verifier (IPV A):","IPV A moves received frames from IFB  to buflets in Local RAM . IPV A performs header checking for IP packets and a first pass calculation of the TCP\/UDP checksum, if present. IPV A also passes packets to Host  via IDE , to OAP  via input list manager (ILM)  or to IFP , when needed. IPV A also adjusts pointers and lengths in the buflet header to move past MAC and possibly IP headers to assist later modules to find their headers; and calculates TCP\/UDP checksum as data is moved to RAM . This creates the pseudo header from the data, which is a part of the TCP checksum.","If a received MAC frame is not for IP, the address of the first buflet of the frame is passed to IDE  and sent to Host  for disposition. If MAC type field=IP, IPVA adjusts the buffer offset field in the buflet to skip over the MAC header. IPV A also adjusts the length in the status word to conform to the length of the IP data payload.","If a packet is for IP, IPV A verifies the header. Packet verification includes: header length check (>=), header checksum check, IP version supported, and data length versus actual packet length check. IP packets that don't pass verification are discarded and their buflets returned to BLM . If a packet header is verified and IP address is not proper, address of the first buflet of the frame is passed to IDE  and sent to Host  for disposition.","If packet header is verified, and IP address is proper, address of the first buflet of the frame is added to an output list maintained by IPV A for the IP Fragment Processor . Details of IPVA functionality are provided below.","IP Fragment Processor (IFP ):","IFP  receives IP fragments, reassembles them into a complete datagram and then delivers the datagram to Host  or ITP , which ever is appropriate. IPV A also handles overlapping fragments and trims the fragments. Temporary storage of datagram fragments is via a linked list, referenced by a hash table, maintained in Local RAM . Each datagram is identified by a 4-tuple {IPID, IPSRC, IPDST, IPP). This identifier is hashed to a 16 bit value. A programmable number of bits are used to index into a hash table to search for a linked list of fragments.","IFP  also provides a timer to time each datagram reassembly with a default timeout value. The timeout value is programmable. A time ordered list of datagrams is maintained by using a timeout linked list. The oldest entry in the list is at the head of the list. If a timeout occurs, the entire datagram is removed from the reassembly list and its buffers returned to the free list.","If a packet is received that has an error that requires an ICMP message to be returned, a completion message is sent to Host  with enough information to allow it to build the return error message.","If overlapping fragments arrive, a flag is set in the status word to indicate TCP checksum needs to be rerun on completed datagram and data is re-read. Note that a counter is incremented each time this occurs.","Details of IFP  functionality are provided below.","Inbound TCP Processor (ITP ):","ITP  processes incoming TCP segments, re-orders out of order segments and then passes TCP data to Host  or OAP  for delivery to an application. If the TCP data is for an iSCSI connection, the data is passed to IAP  instead.","ITP  also retrieves NCBs, via TTM , using source and destination IP addresses and the source and destination TCP port numbers. ITP  updates connection state information (NCB) based upon what was received in the segment.","ITP  also maintains a segment reassembly list for each connection. This list is linked from the NCB. It supports passing out of order segments to IAP  to allow out of order data placement at the iSCSI level. A configuration bit controls this option.","TCP data passed to Host  has the TCP header stripped. \u201cFIN\u201d segments as well as segments for unknown connections are passed to Host  with their headers. Details of ITP  are discussed below.","Details of ITP  are discussed below.","Inbound DMA Engine (IDE ):","IDE  moves data from Local RAM  buflets to Host  memory. This is done at the request of various inbound processing modules (IPV A, IFP , ITP  and IAP ). If IDE  gets behind in the actual processing of the requests, it maintains an input list of requests to be processed. IDE  takes data from Local RAM  and DMAs it into Host  memory using large data buffers from RBM . It creates a list of these buffers in a small buffer from RBM  and passes a pointer to this list and two status words to NCM  to create a completion entry. If RBM  detects a low condition on either of its queues, IDE  generates a Buffer Alert completion message indicating a low queue condition. When the DMA is completed, IDE  returns the buflet chain to BLM .","Rx Buffer Queue Manager (RBM ): RBM  manages two queues that pass pointers to empty Host  buffers, from Host  to System . These buffers are not associated with any particular protocol layer or application and are used to receive all data that is not associated with an iSCSI exchange. One queue maintains a pool of small (for example, 64-512 bytes) buffers and the other queue maintains a pool of large (for example, 512-64K) buffers.","RBM  manages Host  memory resident circular queues with Host  as the producer and System  as the consumer. It maintains a pair of pointers, producer and consumer pointers, which tracks requests in each circular queue. Host  updates the producer pointer when it places new entries of empty buffers in the queue and System  updates the consumer pointer when it takes the entries from the queue. RBM  also maintains a small FIFO of buffer addresses (large and small) to provide buffers to IDE  in a timely manner and signals IDE  when the last entry on either queue is taken. This is used to send a message to Host  that inbound stream is flow controlled, potentially losing Ethernet packets.","TCP Table Manager (TTM )","TTM  manages TCP connection state tables for ITP , OTP  and IAP . This includes locating, loading from Local RAM  or Host  memory, writing back to Local RAM  and maintaining coherency of the NCBs. TTM  provides working NCB register sets for ITP , OTP  and IAP ; provides Read\/Write access to the working register sets for OTP , OIP , ITP  and IAP . This allows simultaneous access to ITP , IAP  and outbound as well as internal access to the registers.","TTM  also provides Fetch\/Update\/Flush functions for working register sets from Host  memory, RISC memory or to\/from Local RAM ; signals an error to ITP \/IAP  if a requested inbound NCB is not found in Local RAM ; signals an overload condition to OTP  if Local RAM  memory resources are not available; maintains timer functions for all TCP connections; and coordinates inbound and outbound channel's access to the network data structures.","TTM  maintains a free list of 64 byte data structures, Delayed Request Blocks (DRB), which are used to place outbound IOCBs that are waiting to be processed. DRBs are also used to place Outbound Address Lists associated with the IOCB, into Local RAM . When an OAL is placed into Local RAM , it is referred to as a Delayed Address List (DAL).","TTM  also maintains a free list of data structures to contain NCBs for connections that are being processed by the hardware; and maintains an outbound request list. This is a linked list of NCBs processed by OTP . ITP  and the timer list manager add NCBs to the list.","Details of TTM  are also provided below.","TTM DMA Engine (TDE ): TTM DMA Engine  DMAs NCBs from Host  memory or RMI  to TTM .","Memory Access Manager (MAM ):","MAM  provides a generic and simple interface for many of System 's components to Local RAM . MAM  manages various requests for Local RAM  access, and coordinates them to provide the maximum bandwidth access to Local RAM .","MAM  passes parity to IPV A, writes and generates parity on all other module writes; checks parity on all module reads and passes parity to IDE , SDE , PMD  and ODE  reads.","MAM  provides a transaction buffer for each interface to help accumulate data for bursting and can freeze all memory access, via a control register bit to allow Host  to view Local Memory . Access to local RAM  is frozen if a fatal chip error is detected.","MAM  performs read-modify-write operation for write access that are less than 64 bits.","SCSI Request Manager (SRM ): SRM  manages the message queue for passing iSCSI requests (IOCBs) from Host  to OAP . SRM  also implements the SCSI request queues as circular queue in Host  memory with Host  as the producer and System  as the consumer. SRM  accepts a pointer from SCM , which points to an empty buffer in RISC Memory; reads down the IOCB from Host  request queue and passes it to the buffer provided by SCM . SRM  maintains a copy of the consumer index in Host  memory and interrupts OAP  to indicate that a request is pending in RMI . It also provides a register for OAP  to read the address of the buffer where the next request has been placed. SRM  also maintains a list of buffers waiting to be processed by OAP , if OAP  lags in processing.","SCSI Completion Manager (SCM ): SCM  transmits messages from OAP  to Host . These messages report the status of previous I\/O requests or the occurrence of an unexpected event. SCM  implements the SCSI completion queue as circular queue in Host  memory with Host  as the consumer and System  as the producer. It accepts a pointer from OAP , which points to a buffer in RMI ; reads completion messages from RMI  and passes it to a completion queue entry in Host  memory; and maintains a copy of the producer index in Host  memory.","SCM  interrupts Host  to indicate that a completion is pending on the queue, using normal interrupt avoidance techniques; adds RMI  buffer back to the free list when a completion message is sent to Host ; and accepts a linked list of completion buffers from OAP , if SCM  gets behind OAP .","RISC Memory Interface (RMI ):","RMI  acts as an arbiter for various devices that want to access RISC RAM. RMI  includes a sequencer state machine to control access to an external Synchronous SRAM.","RMI  maintains a pipeline of requests for memory to keep SSRAM interface as busy as possible; and provides an instruction prefetch mechanism to try and stay ahead of OAP  instruction fetches.","Outbound ARC Processor (OAP ):","OAP  processes SCSI requests from Host , converts them into the associated iSCSI PDUs and sends them via the hardware TCP stack. OAP  also processes incoming iSCSI PDUs and performs the required operations. When a particular SCSI\/iSCSI operation is complete, OAP  sends a completion message to Host .","PCI to RISC DMA Engine (PRD ):","PRD  assists OAP  in moving data between Host  memory and RMI .","PCI to MAM DMA Engine (PMD ):","PMD  assists OAP  in moving data between Host  memory and Local RAM . Data can be moved in either direction.","a. Embedded Processor Completion Queue Manager (ECM ):","ECM  maintains a message queue between Network pipeline A and OAP . It takes completion requests from any of the attached components, prioritizes them and then passes completion messages to OAP .","ECM  also implements a circular queue with System  as the producer and OAP  as the consumer. The queue is maintained in RMI .","ECM  generates an interrupt to OAP , when completion is DMAed into RMI .","EP Request Manager (ERM ):","ERM  manages a queue of transmit requests from OAP  and passes them to Network Pipeline A to be processed. This functionality is almost identical to that of NRM .","ERM  also manages a RISC memory resident circular queue, with Host  as the producer and System  as the consumer. It maintains a pair of pointers, the producer and consumer pointers, that track the requests in the circular queue. Host  updates the producer pointer when it places new requests in the queue and System  updates the consumer pointer when it takes the request from the queue.","ERM  provides a special operating mode for OTP  to allow it to read down a request, except the last word. The last word is read if resources are found to allow the request to be processed. If the resources are not there, OTP  aborts the request and later asks for the same request when the resource is available.","EP Input List Manager (ILM ):","ILM  takes buflet indexes of network packets that are destined to OAP  and generates completion messages to be passed to ECM  for delivery to OAP . ILM  also maintains a list of packets that are waiting to have completions generated, if ILM  gets backed up.","Inbound ARC Processor (IAP ):","As described below in detail, IAP  processes incoming TCP segments destined for iSCSI or other designated protocols. IAP  has access to Local RAM  to interrogate received packets and has access to TTM  to fetch, update and writeback NCBs associated with the received TCP segments.","IAP  can also access SDE  to allow IAP  to move data from Local RAM  to Host  memory.","IAP  shares access to OAP 's program RAM. With this, OAP  and IAP  can communicate regularly where to put the received data.","IAP  also has an interface to NPF , which allows it to pass packets from Local RAM  to RISC memory and has an interface with ITP , from which it gets the info on the next segment to process.","It is noteworthy that IAP  is not limited to any particular processor.","a. Non-Data PDU FIFO Block (NPF ):","NPF  moves iSCSI protocol data units (\u201cPDUs\u201d) from Local RAM  into RISC RAM (not shown). For each PDU moved, an interrupt may be generated to OAP . IAP  programs NPF  data movements. NPF  offloads OAP  from having to fetch the PDU from Local RAM  and wait for its arrival. It also checks the iSCSI digest for the data portion of the PDU and flags the PDU as good or bad. CRC checking is enabled by IAP .","NPF  also accepts pointers for empty RISC memory buffers and maintains a free list of buffers to place PDU data into. NPF  provides a register interface for OAP  to give free buffers to NPF ; and accepts one or two words of data to be attached to PDU data in RISC memory for each A\/L.","NPF  accepts address and length of PDU to read from Local RAM ; and moves PDU data from Local RAM  to free buffers. PDUs can be larger than the size of an individual buffer, therefore NPF  can link a number of buffers together to fit the entire PDU. When all data for an A\/L is moved to RMI , NPF  signals IAP  that it is done, so that IAP  can free the buffer.","NPF  provides a register interface for OAP  to read the buffer pointers from NPF . NPF  maintains a two way linked list of PDUs ready to be read by OAP , if it lags.","SCSI DMA Engine (SDE ):","SDE  provides IAP  with a DMA channel from Local RAM  to Host  memory. SDE  includes a byte packer function that takes unaligned or less than 8 byte buffers and packs them into 8 byte words before passing them to DA  to be sent to Host . SDE  also provides a data path with byte parity. This channel moves user data.","SDE  packs and aligns data from Local RAM  to be passed to Host  via DA ; signals IAP  after each buflet's worth of data has been transferred; and calculates the iSCSI CRC across all words transferred.","IOCBs","An Input\/Output Control Block (\u201cIOCB\u201d) is a single entry in one of the request queues, discussed above. The first word of an IOCB is the control word. The control word contains a Command operation code (Opcode) and other control bits that describe how a requested operation is to be processed. The second word is a transaction identifier (ID). The transaction ID is given by host  and is passed back in any completion message generated for the IOCB. Host  can use the ID to determine which IOCB has completed and to release any resources used for the operation.","In general, each IOCB has three buffer descriptors, which identify data buffers or point to another list of descriptors. The remainder of the IOCB contains command specific information.","System  reads the IOCB from host memory (not shown) to execute a requested operation. Once the contents of the IOCB is read, the IOCB entry is returned to host  to be reused, even though the requested operation may not be complete. IP and MAC data transmissions are executed immediately, since they do not require any response from the remote node. These operations are handled in order, since the IOCB processing is handled in order.","TCP is handled differently. OTP  executes one IOCB until it finishes sending all the data and waits for acknowledgement (\u201cACK\u201d) packets, or if the credit window closes so that no more data can be sent. In these cases, OTP  writes a copy of the IOCB to local RAM  while it waits for an inbound action that allows the IOCB processing to continue. After the IOCB is saved, OTP  attempts to get another IOCB to work on. As all the data for a certain IOCB is sent and the ACK packets are received, OTP  generates a completion message. Operations for a particular TCP connection completes in the order in which they are received from the host. This is done to guarantee in order delivery of data to a remote port.","iSCSI PDU transmissions use TCP, and are therefore handled in the same way that TCP is handled. iSCSI exchanges use multiple iSCSI PDUs that are sent and received using TCP. Again, each one of these messages is handled in the same way as TCP packets.","Network Control Blocks (NCB)","NCBs are data structures that are used to provide plural network specific parameters to various modules shown in . System  uses NCBs to build MAC, IP and TCP protocol headers. NCBs are maintained in host memory and in local RAM . NCBs include information regarding various status flags, control flags, destination MAC address, source and destination IP address, IP header fields, a pointer to IP options, source and destination TCP ports, host address of the NCB, TCP connection information and various local RAM  linked list fields.","NCBs are created in Host  memory for TCP and IP operations. NCBs created for iSCSI and TCP operations exist as long as the TCP connection is up. NCBs created for IP operations can be deleted as soon as the IP transmission takes place. When an NCB is created for a TCP operation (and an iSCSI operation, which uses TCP) it is read into System  when the TCP connection is established. System  maintains a local copy of the NCB for as long as the connection stays up. This allows System  to quickly process TCP transfers without needing to access Host  memory for each one.","One field in the NCB, the TCP Timer Scale Factor will now be described in more detail. Each TCP timer in System  is referenced to a local timer and is defined as a certain number of local timer ticks. The scale factor (\u201cSF\u201d) is used to adjust the time interval between timer ticks, on a per connection basis. This is done to allow for faster timeouts on connections that are on a very small network versus connections being run across a very large network. The scale factor is defined as a 3 bit field in the NCB and is an exponential multiplier. The timer tick interval is increased by a factor of 2. The scale factor is used to increase or decrease the timer tick from that defined in the current BSD 4.4 release. A scale factor of 2 uses the same timer defined in the BSD implementation. Scale factors 1 and 0 divide the timers by 2 and 4 respectively. Scale factors of 3 or greater increase the timers by a power of two for each increment above 2.","Network Data Descriptor Processing","An example of a network IOCB is provided in FIG. A. Transmission of network data starts with host  creating a network IOCB in the network request queue. NRM  reads down the IOCB into its internal buffer and then asserts a request available signal to RA  for Network Pipeline A. When the pipeline is ready, RA  returns a Request Grant signal to NRM . NRM  then asserts a data available signal to Network Pipeline A and puts the first word of the IOCB on the data bus. Each network processor interrogates the data bus to see if it is the intended destination for the request. The destination processor handshakes the IOCB from NRM . As the destination processor starts to handshake the descriptor, it also deasserts its idle signal to RA . This holds off a new request from being started until the current one is done. When all the processors in Network Pipeline A are done, they assert an idle signal, which in turn enables RA  to accept another request. Note that the protocol processors only look at the lower bits of the opcode. A value of 01b is a MAC command, 10b is an IP command and 00b and 11b are a TCP commands.","Passing Inbound Data to Host","When any of the inbound processors (for example, ITP ) want to send data to Host , they assert a data available signal to IDE . When IDE  is ready, it asserts its \u201cacknowledge\u201d and handshake a status word and the address of a list of buflets that contain the data to be passed to Host . IDE  places the list of buflets on an Output List maintained in Local RAM .","When IDE  is ready to handle the data, it signals to DMA (i.e to provide \u201cdirect memory access\u201d) the frame data to Host  memory into one or more large Rx buffers. When IDE  is done with the buflets, it passes the linked list of buflets back to BLM  to be added to the free list. IDE  then places the addresses of the large buffers in a small buffer and make a request to NCM  to send a completion to Host .","When NCM  is ready, it acknowledges the request and handshakes the completion data to an internal buffer. IDE  handshakes the completion words to NCM , which includes the status word and the address of the small buffer that has the list of addresses of the large buffers that contain the frame data. NCM  then updates its producer pointer and generate an interrupt, if necessary.","Sending Outbound Completion to Host :","Once an outbound processor (for example OTP ) completes sending requested data, it requests NCM  to send a completion message. When NCM  is ready to take the completion, it handshakes the completion data into an internal buffer. The processor sends data, with the last word having an end bit set to indicate that there is no more data. NCM  DMAs the completion data into the next available completion entry in Host  memory, update its producer pointer and generate an interrupt, if necessary. This completes the outbound data transmission.","Local Memory Access:","Local memory  may be accessed by plural functional components of System  using MAM . Each block that can access memory has a read and write bus to MAM  as well as a set of handshake signals. MAM  can also buffer data for each attached functional block to allow for a reasonable sized burst into memory.","Read Access:","A read access to Local RAM  starts with a block writing the start address of the transfer and in cases where the length is not predefined, a length is also written. MAM  then reads data into its internal buffer and asserts a data available signal to the block. The block then reads the data using a two wire handshake until all data has been transferred. The two-wire handshake allows MAM  and the destination block to flow control the data stream, if necessary. This can occur when MAM  is going to fetch more data after the initial burst was read. MAM  continues sourcing data until the given length of data has been transferred.","Write Access:","MAM  handles write accesses to Local RAM  by buffering a certain amount of data and then sending it to RAM . A write access starts with a block writing the start address of the transfer and in cases where the length is not predefined, a length is also written. MAM  then handshakes a buffer full of data and then writes it to memory. MAM  uses a two-wire handshake with all components connected to it to allow for full flow control on any interface if it gets busy with another one. Data continues to be handshaked until the given length is reached.","MAC Frame Transmission","The following subsection discusses transmission of Host  originated MAC frames using OIP :","Normal Data Frame Transmission:","To send a MAC frame, host  sends a descriptor to OIP . Thereafter, OIP  programs ODE  to move Ethernet frames from host memory to outbound FIFO OFB . The entire frame is placed into outbound FIFO OFB  before the frame is sent. Once a frame is sent, OIP  sends a completion message to host  through NCM . OIP  does not process data, but copies it into the outbound FIFO and then sends it. This means that Host  must create a complete Ethernet frame (or IEEE802.3 frame, if desired).","Pause Frame Transmission","BLM  maintains a count of the number of buflets in its free pool (not shown). If that number drops below a certain threshold, BLM  asserts a signal to the MAC transmitter to pause transmission until more buffer space is freed. On the rising edge of the pause signal, OIP  creates and sends a Pause frame with the time to pause set to maximum. On the falling edge of the pause signal, OIP  sends another Pause frame with the time to pause set to zero. Note that if a request comes from Request Arbiter  while processing a Pause transmit, the request is ignored until the Pause frame has been transmitted.","IP Datagram Transmission:","This sub-section discusses transmission of Host  originated IP datagram using OIP . Host  can send two types of IP datagrams, locally generated or one that is being routed through Host  with an IP header included in data buffers.","Locally Generated IP Datagrams:","To send an IP datagram, Host  sends an IOCB to OIP , which includes host memory address of a NCB, with the necessary information to build the network headers. OIP  writes the host address of the NCB to TTM  register, as described below, and then makes a request to TTM  to fetch the NCB from host memory. When the NCB is read, TTM  signals OIP  that NCB is present. Thereafter, OIP  is ready to start sending the data.","Single Packet Datagram:","If the total datagram length fits into one IP packet, OIP  builds both the MAC and IP headers in outbound FIFO OFB . Destination MAC address is copied from the NCB and source MAC address is copied from the Ethernet MAC Address register. For an IP Header, the IP Version field is set to either 4 or 6, depending on the \u201cIPv6\u201d bit in the NCB. IP header length field is calculated by adding 5 to the IP Option Length field from the NCB. IP Type of Service is copied from the NCB. IP packet length field is calculated from data length field in the descriptor plus the size of the IP header, w\/ options. IP Identifier field is taken from a register maintained on System  that is incremented for each datagram. IP fragment flags and offset are all set to zero. IP Time to Live is copied from the NCB. IP Protocol field is copied from the NCB. IP Checksum is initially written as zero and is later rewritten after all the data has been moved and the checksum calculated.","IP Source Address is copied from the port's IP Address register. IP Destination Address is copied from the NCB. If the IP Options bit is set, OIP  programs ODE  to move the fully formed IP options data from Host  memory down to the outbound FIFO. After all the other header fields are filled in, OIP  sends the calculated checksum with a tag that tells the MAC to write it into the IP checksum field. Note that the IP checksum is always at a fixed offset from the beginning of the Ethernet frame. Once all the data is down in the FIFO, OIP  sends a completion message to Host  as described.","Fragmented Datagram:","If the total datagram length is greater than a certain size, e.g., 1500 bytes, then OIP  proceeds to generate IP packets with fragments of the datagram. The process for sending fragment packets is the same as that used for a single packet datagram with the following exceptions:","The IP Packet Length field for all the packets, except the last one, is the same, e.g., 1500 bytes. The last length is the remainder. The IP fragment flags and offset are set to indicate which fragment is being sent. IP Options are handled differently and only some IP options are copied into each fragment.","Forwarded IP Datagrams:","Forwarded IP datagrams have a single restriction imposed by system . This datagram does not require fragmentation. The process to send a forwarded frame is the same as that for locally generated IP traffic with one exception that Host  sets the Header (\u2018H\u2019) bit in the descriptor. This tells system  not to generate the IP header. It also tells the hardware not to do fragment processing.","TCP Data Transmission","This subsection discusses how an IOCB is read from Host , and transmitted using OTP . It assumes that a TCP connection has already been established.","Network IOCB Processing:",{"@attributes":{"id":"p-0272","num":"0279"},"figref":"FIG. 4A","b":["104","404","404","309","337","323","309","310","308","326"]},"After OIP  is done, OTP  builds its header in outbound FIFO OFB . Each field of the TCP header is filled in as follows: Source and Destination TCP ports are copied from the NCB. TCP sequence and acknowledgement numbers are copied from the NCB. TCP header length is calculated. TCP flags are copied from the NCB. Hardware sets the ACK flag regardless of the state of the flag in the NCB. TCP Window Size is copied from the current value in the NCB. TCP checksum is initially set to zero and then adjusted","OTP  processes NCB and adds a timestamp, if the connection is configured. OTP  sends TCP data from host memory to outbound FIFO OFB  via ODE . As the TCP header and data are passed to OIP , OIP  calculates the TCP checksum. If a retransmission timer is not already running on this connection, OTP  links the NCB on the timer queue for the retransmission timer. After the last word of data is passed to OIP , OIP  sends the calculated TCP checksum with a tag that tells OFB  to write it into the TCP checksum field, and the frame is sent. If all the data for the IOCB has been sent, OTP  writes the sequence number of the last byte of data for the IOCB in the DRB. OTP  also sets the Last Sequence number valid flag. Thereafter, OTP  updates all NCB entries and does a \u201cwrite-back\u201d of the NCB to local RAM .","Delayed Request Processing:","The Delayed request process occurs when an IOCB has been placed on the Output Request List because a TCP connection with a closed window received an ACK packet or a timer expired that requires OTP  processing. The following describes the delayed request processing:","TTM  signals RA  that it has an NCB that needs processing. RA  signals back to TTM  that it has won arbitration. TTM  reads the NCB from Local RAM  and asserts a request to OTP . OTP  checks action flags in NCB. OTP  updates the \u201cSnd_Una\u201d NCB fields (except sequence number, which ITP  updates) to account for the amount of data acknowledged. OTP  reads the first delayed request from local RAM .","OTP  checks if the data transfer requested in the delayed request is done. If so, OTP  generates an outbound TCP completion message and removes the DRB from the list. If there are other DRBs on the list, OTP  repeats the check for a complete DRB until all are done or one is encountered that still has data to send.","If a DRB that need to send data is left, the processing continues, or else an Idle message is sent to RA ; and the process ends.","Thereafter, the process checks if TCP window is open to send at least one segment. If not, idle signal is sent to RA ; and the process ends.","OTP  reads the delayed request from local RAM  that is pointed to by the Snd_Max Descriptor Address field in the NCB. OTP  signals OIP  to build MAC and IP headers in outbound FIFO OFB . When OIP  is done, OTP  build's the header in the outbound FIFO OFB .","OTP  processes and adds a timestamp option, if connection is configured. OTP  sends TCP data from host memory to outbound FIFO OFB . As TCP header and data are passed to OIP , OTP  calculates the TCP checksum. If a retransmission timer is not already running, OTP  links the NCB on the timer queue for retransmission timer.","After the last word of data is passed down to OIP , it sends the calculated TCP checksum with a tag that informs MAC  to write the tag into the TCP checksum field and thereafter the frame is sent.","If all the data for a request is sent, OTP  updates the DRB with the Last Sequence number (also referred to as \u201cSeq #\u201d) valid flag set, indicating that all data for this IOCB has been sent and what the last sequence number was. OTP  checks if there is more data to be sent. If there is, the delayed request process starts over, or else an idle signal is sent to RA  and the process ends.","Unassisted TCP Segment Transmission:","Unassisted TCP transmissions are used for sending \u201cSYN\u201d and \u201cFIN\u201d TCP segments. An unassisted transmission means that the hardware does not wait for an ACK packet to return before sending a completion to the host. For the SYN segment, a NCB may not be created until the SYN is sent.","To send an unassisted segment, host  creates the same data structure as defined for data transmission, but also sets additional flags. The first is the \u201ccomplete immediately\u201d flag that informs system  not to wait for the ACK packet but to immediately generate completion when a segment has been sent. The other flag is the \u201chost NCB address flag\u201d, which indicates that the NCB address in the IOCB is a host address and not a local RAM  address.","Hardware processing of the request proceeds as described in the TCP data transmission section with the exceptions that as soon as the segment has been transmitted, OTP  generates a completion and does not store the IOCB or NCB in local RAM .","Retransmissions:","Retransmissions are initiated by setting the retransmission (RET) flag in an NCB. The RET flag is set if:","Three duplicate ACK packets are received in a row or","the retransmit timer expired.","A retransmission packet is processed the same way as the Delayed Request processing discussed above with one exception, that the data to transmit is taken from the place pointed to by the \u201cSnd_Una\u201d pointer instead of the \u201cSnd_Max\u201d pointer.","Normal ACK Tx Processing","This section discusses transmission of ACK packets without accompanying data. This occurs when the Send Ack Now (SAN) flag is set in a NCB and data is not ready for transmission. If data is also ready for transmission, the ACK packet follows the data transfer. This is covered in the previous discussion of delayed request processing.","The SAN flag is set if:\n\n","The following process is used to send an ACK only packet:\n\n","Duplicate ACK Processing","This section discusses transmission of a duplicate ACK packet. This occurs when the \u201cSend Duplicate ACK (SDA)\u201d flag is set in a NCB. If data is also ready to transmit, the ACK packet transmission takes precedence over the data transfer.","The SDA flag is set because an out of order segment was received. The following process is used to send an immediate duplicate ACK packet:","TTM  signals RA  that it has an NCB that needs processing. RA  signals back to TTM  that it has won arbitration. TTM  reads the NCB from memory  and asserts a request to OTP .","OTP  checks for the SDA flag in NCB. OTP  signals OIP  to build MAC and IP headers in outbound FIFO OFB . When OIP  is done, OTP  builds its header in outbound FIFO OFB . This header indicates that only an ACK packet is being sent.","OTP  processes and adds a timestamp option, if connection is configured. As the TCP header is passed to OIP , OIP  calculates the TCP checksum.","After the last word of data is passed down to OIP , OIP  sends the calculated TCP checksum with a tag that tells the MAC to write it into the TCP checksum field. This causes the frame to be sent. OTP  sends an idle signal to RA  and the process ends.","TCP Table Manager Request Processing","Persist Timer Processing","The \u201cPersist Timer\u201d process starts when OTP  sends data for a connection and the window closes before all the data is sent. OTP  makes a request to TTM  that a NCB is added to the timer list with the persist timer running.","The process stops if an ACK packet arrives that opens the window or the timer expires, which results in a window probe being sent. If an ACK packet arrives and opens the window, ITP  checks the NCB to see if it is on a persist timer. If it is, ITP  requests the entry be removed from the timer list. ITP  also sets the Window Update bit in the NCB and requests TTM  to add the NCB to the outbound request list. TTM  then makes a request to OTP  to look at the window and send data.","If the persist timer expires, TTM  sets the send window probe (SWP) bit in the NCB and place the NCB on the outbound request list. OTP  then sends one byte of data as a window probe and then restart the persist timer again. The following is the process used to send a window probe segment:","TTM  signals RA  that it has an NCB that needs processing. RA  signals back to TTM  that if it wins arbitration. TTM  reads the NCB from memory  and asserts a request to OTP . OTP  checks action flags in the NCB. In this case, the \u201cSWP\u201d flag is set. OTP  reads the delayed descriptor from local RAM  that is pointed to by the Snd_Max Descriptor Address field in the NCB.","OTP  signals OIP  to build MAC and IP headers in outbound FIFO OFB . When OIP  is done, OTP  builds its header in outbound FIFO OFB . Source and Destination TCP ports are copied from the NCB.","OTP  processes and adds a timestamp option, if connection is configured. OTP  sends one byte of TCP data from host memory to outbound FIFO OFB . As TCP headers and data are passed to OTP , it calculates TCP checksum as well as counting the IP datagram length. After the last word of data is passed down to OIP , OIP  sends the calculated TCP checksum with a tag that tells MAC  to write it into the TCP checksum field and sends the frame. OTP  then sends an idle signal to RA , and the process ends.","Retransmit Timer Processing:","The retransmit timer process has two steps, start and stop. To start the timer, OTP  sends a segment and then requests TTM  to add the NCB to the timer list. Retransmit timer processing stops when an ACK packet for a sequence number that is being timed returns via ITP . When the ACK packet is received, ITP  looks at the NCB to see which sequence number is being timed and if the ACK packet includes that number, ITP  requests TTM  to remove the NCB from the timer list. The next time OTP  sends a segment, it knows that the timer is not running and restarts it.","Retransmit timer processing also stops if the timer expires. When this occurs, TTM  places the NCB on the outbound request list with the Retransmit (RET) bit set. OTP  retransmits a segment starting at the snd_una location in the data stream. If retransmission occurs, OTP  requests that the sequence number is timed again, but the timer value is doubled. This cycle repeats plural times and if the sequence number is not acknowledged by then, the connection is dropped. Then OTP  generates a completion message that indicates that the connection should be reset due to retransmit timeout.","Delayed ACK Timer Processing:","The delayed ACK timer is started when a data segment is received by ITP  and the delayed ACK packet timer is not running. In this case, ITP  requests TTM  to place the NCB on the timer list. The Delayed ACK timer is stopped for plural reasons. For example, if another segment is received for a connection, ITP  requests that the NCB be removed from the timer list and put on the outbound request list with the SAN bit set. This causes an ACK packet to be sent that acknowledges the last two segments.","Another reason the timer processing stops is if OTP  needs to send a data segment for the connection. The data segment sent includes data and the ACK packet. In this case, OTP  requests that the NCB be removed from the timer list.","The timer processing also stops if the timer expires. When this occurs, TTM  places the NCB on the outbound request list with the SAN bit set and the ACK packet is sent.","t_Idle Timer Processing:","The t_idle timer is used in a TCP implementation to reset a congestion window on a connection that has been idle for a \u2018long\u2019 period, which may be one round trip delay. If no activity occurs on a connection for round trip time (RTT), the congestion window value is reset back to one segment and a \u201cslow start\u201d begins when transmissions are restarted.","The t_idle timer may also be used to test a connection that has been idle for certain period. The timeout period is referred to as the \u201cKeepalive\u201d time. In one aspect, a special Keepalive segment is sent on an idle connection to check if the timer has expired because a physical connection broke or the connection is merely idle. If the connection is just idle, it gets a response. If the connection is broken, no response or error is returned and the node can terminate the TCP connection.","MAC Frame Reception:","As frame packets () arrive from an Ethernet network, they are placed into inbound FIFO , while the MAC receiver (Rx, also referred to as MAC )  verifies the CRC. When the entire frame is in FIFO (IFB)  and if the CRC is valid, MAC  adds a \u201cstatus word\u201d to the beginning of the frame. The last word of data and the status word is written into FIFO (IFB)  with an \u201cEND\u201d bit set. This status includes a frame length field, a header length field and status bits that indicate what type of address were matched to receive the frame. FIFO (IFB)  then signals IPV A that the frame is available. IPV A reads the frame out of FIFO (IFB)  and places it into Local RAM  using buflets acquired from BLM . IPV A links together as many buflets as necessary to contain the entire frame. IPV A notes that the frame type field indicates that the frame is not destined for IP and send the frame to host  via IDE .","Pause Frame Reception:","MAC  supports standard flow control using a Pause frame. A Pause frame is recognized and the timer value associated with the frame is extracted. A timer is started based on the timer value. Also, a signal is sent to the MAC transmitter to stop transmission, at the next frame boundary, until the timer expires, or another Pause frame is received that disables the pause function.","Reception of Frames for Multiple Addresses:","MAC  receives frames addressed to plural addresses that are programmed into MAC address registers and the addresses are enabled via control register bits. MAC receiver  receives the frames addressed to the Broadcast address as well as Multicast frames.","IP Datagram Reception:","When an IP frame arrives from an Ethernet network, it is placed into inbound FIFO (IFB)  while MAC receiver  verifies the CRC. When the entire frame is stored in FIFO (IFB)  and if the CRC is valid, MAC receiver  adds a status word to the frame. The last word of data and the status word is written into FIFO (IFB)  with an END bit set. This status includes a length field, a header length field and status bits that indicate what type of address was matched to receive the frame. FIFO (IFB)  then signals IPV A that the frame is available.","IPV A reads the frame out of FIFO (IFB)  and transfers it into Local RAM , using buflets acquired from Buflet List Manager . IPV A links together as many buflets as necessary to contain the entire frame. IPV A evaluates the frame type field, and if it indicates that the frame is destined for IP, then IPV A amends the first buflet data pointer to skip over the MAC header, based upon the MAC header length given in the status word. The IP header for the packet is placed into local RAM , and IPV A performs various validation checks, including IP header checksum and the comparison of the IP length against the actual received packet length.","If the packet fails validation, it is deleted and the buflets are returned to the free list. If the destination IP address is not for the specified node, IPV A sends the packet to host .","Routed packets are not reassembled on intermediate nodes, and sent directly to host . IPV A also evaluates the \u201cMore Fragments\u201d IP flag and the IP fragment offset field to determine if the entire datagram is present in a packet. If it is and the datagram is not destined for TCP, IPV A passes the packet to host . If the datagram is for TCP, it is passed to IFP  and then passed to ITP .","If a frame is destined for IP, IPV A calculates a TCP checksum for the packet. If the packet is the first or the only packet of a datagram, IPV A calculates a TCP checksum for the packet, including the pseudo header, which is based on various IP header fields.","If the packet is a fragment that is not the first fragment of the datagram, IPV A skips over the IP header and calculates a partial checksum of the datagram's data payload. When IPV A finishes moving the entire packet into memory , it writes the calculated TCP checksum value and status word to the 1buflet.","If the packet has not been otherwise disposed of and if IFP  is idle, IPV A passes the address for the first buflet of the packet and a copy of the IP header to IFP . If IFP  is not idle, the new packet is placed on the IFP  input list, and when IFP  is idle, IPV A re-reads the IP header and sends it. IFP  processes the fragmented datagram as described below and shown as  in .","Fragmented Datagram:","IFP  checks if an entry already exists in the re-assembly list. It does this by hashing the IP n-tuple {IPID, IPSRC, IPDST and IPP} and looking into a hash table A () for a filled entry. If no entry exists in the hash table (as indicated by the valid bit being clear), an entry is made and the address of the packet is written in the entry.","When the 1fragment of a datagram is added to the reassembly list, the Nxt_Dgm_Lnk and the Prv_Dgm_Lnk are set to zero. If an entry already exists, the entry can be pointing to one or more datagrams (B, C and E) that matched the hash. IFP  compares the IPSRC, IPDST, IPID and IPP fields () of each datagram associated with the hash.","If the datagram is not already on the list, it is added to the end of the list associated with the hash. When the 1fragment of a datagram (B) is added to the reassembly list, the Nxt_Dgm_Lnk and the Prv_Dgm_Lnk are set to the proper values.","If the datagram is found on the list, the buffer for this fragment is added to the list of fragments for the datagram.","If the received fragment is not in-order, it is inserted in the ordered fragment list using the \u201cFrg_Lnk\u201d field (D). The fragment offset in the IP header determines the insertion position on the list. If the fragment is placed before the fragment that was the first on the list for this datagram, the \u201cNxt_Dgm_Lnk\u201d and \u201cPrv_Dgm_Lnk\u201d are copied into the buflet (F).","If the fragment is in-order with respect to another fragment then the buffers for the fragments is linked using the \u201cBuf_Lnk\u201d field (H and G). TCP partial checksums are summed together and placed in the first buflet of the resulting list. When this linking takes place, the block also fixes the buflets if fragment overlap occurs. Further, if fragment overlap occurs, IFP  sets a \u2018C\u2019 bit for the datagram to force the TCP block to recalculate the TCP checksum, since the sum of the partial TCP checksums is invalid due to the overlap.","When a new datagram is added to the reassembly list, it is also added to the tail of the timeout list.","When the 0fragment (fragment offset=0) is placed on the fragment list, the 1fragment bit in the status word is set. When the last fragment (the more fragments header bit=0) is placed on the fragment list, the last fragment bit in the status word is set.","IFP  checks if the entire datagram is in memory . This is indicated by the fragment link valid bit being clear and if the first and last fragment bits are set in the status word. If a full datagram is present, IFP  removes the datagram from the reassembly list. This includes revising the timeout list.","If a datagram is not destined for TCP, the datagram is sent to host . If the datagram is destined for TCP, and ITP  is idle, the address of the buflet and the status word are passed to ITP . If ITP  is not idle, IFP  links the datagram onto the datagram wait list until ITP  can process it.","TCP Data Reception","Received TCP data from the network goes through the same processing described above for a MAC frame and IP datagram, except that the datagram is not passed to host . Instead, the datagram is processed by IFP  and then passed to ITP , if ITP  is not busy. If ITP  is busy, the datagram is linked to the Datagram Wait list until ITP  can process it. When the datagram is passed to ITP , the buffer address of the datagram, the status, the IP header and the TCP header are passed to ITP .","ITP  takes the segment and validates the TCP header. If it is valid, ITP  checks if the segment is a SYN or FIN segment. If it is, the segment is passed to the host. If it is not a SYN or FIN, ITP  fetches the proper NCB from local RAM . It does this by loading the necessary hash parameters into TTM  and then sending a command to TTM  to fetch the NCB using the loaded hash parameters. If the NCB is found, TTM  signals ITP  to continue processing. ITP  then checks whether the segment is in order or not.","If the NCB is not found, the segment is passed to the host for disposition.","iSCSI PDU Processing","If a received TCP segment is received on an iSCSI MAC address, the segment is passed directly to IAP . ITP  adds the segment to the NCB's reassembly list and then passes the 1buflet address of the segment to IAP  to be placed on its input list. ITP  does not perform in its normal reassembly, except normal ACK processing required for the received data, either in-order or out-of-order.","In Order Data Reception Processing:","If a received TCP segment is in order, ITP  checks the NCB to see if any other data has been previously received out of order. If not, ITP  passes the segment to IDE  to be sent to host . If data is out of order (\u201cOOO\u201d) on the reassembly list, ITP  checks to see if the data on the reassembly list can also be passed to the host. ITP  appends any data that is in order with the received segment and then passes the resulting list of data to IDE  to be sent to host .","Out-of-Order Data Reception Processing:","If a received TCP segment is out of order, ITP  adds the segment to the reassembly list. The processing of the segment stops, until the missing \u201cin order\u201d segment arrives.","Normal ACK Reception Processing:","Normal ACK packet processing includes the processing of a segment that only includes an ACK packet as well as a segment that has an ACK packet attached to the received data.","Normal ACK processing proceeds just like data reception processing and in the case of attached ACK packet, the system performs the complete data processing as well as ACK packet processing. The difference in ACK packet processing is that ITP  evaluates the ACK sequence number and compares it to the snd_una value in the NCB. If the ACK sequence number is greater than the snd_una value, snd_una is updated to the new value, the window update flag is set in the NCB and ITP  requests TTM  to add the NCB to the Outbound Request List to be processed by OTP .","ITP  also updates the remote receiver credit information in the NCB. Once the NCB has been updated, ITP  discards the standalone ACK packet by returning the buflet to the free list. If the segment also contained data, it is processed as explained above.","Duplicate ACK Reception Processing:","Duplicate ACK packets are sent as an indication that data is arriving at the remote node out of order. The processing for a duplicate ACK packet is different than a normal ACK packet since it does not acknowledge any new data. The basic processing for the duplicate ACK packet is to count the packets. If three consecutive duplicate ACK packets are received, ITP  sets the retransmit bit and request that the NCB be placed on the Outbound Request List to retransmit the oldest segment. Once the ACK packets have been processed, the buflet containing the segment is freed.","The various modules of  will now be described.","Inbound TCP Processor (\u201cITP\u2019)",{"@attributes":{"id":"p-0366","num":"0379"},"figref":"FIG. 3B","b":["306","331","104","307","305","306"]},"Input processor A performs the initial check for a TCP data packet checksum. If the checksum fails or the data packet is for broadcast or multicast, the data packet is dropped by return processor B. If the checksum passes, then input processor A sends a signal to TTM , which is described below, requesting a NCB. Input processor A sends the signal through TCP control interface C.","If an NCB is found, input processor A performs plural tests to determine if a particular packet should be processed further. ACK processor E and data processor F perform the tests. ACK processor E performs various acknowledgement related process steps, as described below. Data processor F processes data including portions in-sequence and out of sequence TCP code.","If incoming data  is to be dropped, the \u201cbuflet index\u201d is sent to return processor B. Data from ACK processor E and data processor F is sent to an output processor D that transfers the data based on the destination.",{"@attributes":{"id":"p-0370","num":"0383"},"figref":"FIG. 3C","b":["306","328","305","330","323","329"]},"After the checksum is validated TTM  is requested to fetch a NCB from local memory . Simultaneously, option block  searches for a time stamp. If a timestamp regarding the data is not found in the data received from IFP , and the data header indicates that there may be a time stamp, then additional data is requested from MAM . Option block  then searches data in MAM  for timestamp and validation block C verifies when a NCB was found by TTM .","Thereafter, validation block C performs a series of checks on a received segment to determine if further processing is required. If further processing is required, all the necessary data is passed to ACK processor E and data processor F. If any of the checks fail, output processor D is started to send completion messages and the received NCB is written back.","TCP options include timestamp option, which can be used by TCP senders to calculate round trip times. The TCP protocol recommends a 32-bit format for the first four bytes of the timestamp option data defined as 0x0101080a. The 0x01 are NOPs. The 0x08 is the \u201ckind\u201d field, which indicates timestamp, and the 0x0a is the length field, which indicates 10 bytes. Although this is the recommended format for the first byte of timestamp option data, there is no guarantee that all implementations will use it. Therefore, System  is designed to detect any format.","ITP  receives a data buflet which contains the TCP header and 12-bytes of option data. The option data, if formatted according to RFC 1323, Appendix A (Industry standard for \u201cTCP Extensions for High Performance\u201d), would contain the previously described word first, followed by the 4-byte timestamp value and the 4-byte timestamp echo reply value. A state machine, described below, parses the option data and looks for the 0x0101080a value in the first word. If the first word detected contains this value, then the next two option words are processed as timestamp and echo reply.","If the first option word is not based on RFC 1323 format, and the TCP length is greater than 32 bytes (20-byte TCP header and 12-byte TCP option data), that indicates that more option data is present. Such data is retrieved from local RAM , and each byte of the option data is parsed to detect the timestamp opcode.","Option data is read from local RAM  one word at a time. Each byte of the word is checked for the 0x08 timestamp opcode. When it is detected, one of the four cases is true. The location of the timestamp opcode is in one of the four possible byte positions within the word. This location is coupled (encoded as a number 1 through 4 to indicate its byte position with a zero value used to indicate no timestamp opcode detected yet), a \u201cts_found\u201d flag is set, and the incoming word count+1 at the cycle the opcode was detected is latched as \u201cts_found_cnt\u201d. The four cases are identified based on the byte location of 0x08 to determine which byte positions of the subsequent words from local RAM  contain the actual timestamp and echo reply values, and these values are extracted from the data stream and saved.","FIG. C shows a state machine diagram of a state machine option block . FIG. C shows how option block  state machine determines if a timestamp is present and more data needs to be acquired from local memory . If a time stamp is included in the data from IFP , ts_present is set to 1 and ts_ecr & ts_val are updated. If not, and the header length is greater than 32 bytes, option data is received from memory .","Also, if TCP option field is found for a time stamp then ts_present is set to 1 and ts_ecr & ts_val values are updated.","FIG. C shows the various state machine states of validation state machine in the validation block module C. The validate state machine is divided into 4 sub state machines, as described below:","Idle State\u2014Determines if the connection is in a valid TCP state to receive data. Also checks if there are any flags set that would require this segment be sent to the host.","Check Trimming\u2014If trimming of this segment occurs, the set flags to indicate how. The actual trimming is handled in data processor F.","Timestamp\u2014If a timestamp was found in the TCP options data, validate and save.","ACK\u2014If the ACK is out of range, or if no ACK was sent at all, the segment is dropped.","FIG. C shows validation block module C state machine states for checking reset, SYN and\/or invalid state. The following are the process steps for FIG. C:\n\n","FIG. C shows plural validation module C state machines for trimming.","If ti_len=0\n\n","The first check \u201cti_seq+ti_len<rcv_nxt\u201d verifies that the packet data is before the window. This applies regardless of whether the fin_flag is set or clear. The second check includes a test for the fin_flag. In that case, ti_len is the data length+1 for the fin_flag. If ti_seq+ti_len is equal to rcv_nxt which implies that there is 1 byte of data and the fin_flag is set (the 1 byte of data), then this is also a duplicate packet so we clear the fin_flag in the packet. The BSD code sends an ACK packet in this case, which may or may not be required. No received data is processed from this packet, however the ACK information is processed normally.","Set SAN bit","Set needoutput=1\n\n","If ((ti_seq+ti_len)>rcv_adv (part of segment after the window)","Set needtrimming=1","Set endtrimming=1","If entire segment after the window (ti_seq>=rcv_adv)","Set len_eq_zero=1","Set SAN bit","Set needoutput=1","If window probe received (((rcv_adv\u2212rcv_nxt)=0) & (ti_seq=rcv_nxt))","Increment received window probe counter","Else","Send the buflet index to the return processor.","Set no_comp_msg=1","END","FIG. C shows a block diagram of validation module C's state machine states for timestamp functionality, as illustrated by the following process steps:\n\n","Set ts_recent=ts_val","Set ts_recent_age=tcp_now","FIG. C shows plural state machines used by ACK processor E. ACK processor E performs various functions, as discussed above. Input processor A contacts ACK processor E when a received packet is to be dropped\/routed to host . ACK processor E handles some of TCP connection state machines including completing a \u201cpassive open\u201d connection and handles state transitions when FIN segments are acknowledged.","ACK processor E handles receipt of duplicate packets, as described above, including fast retransmit and recovery mechanisms of TCP. ACK processor E also performs normal TCP path processing including updating congestion window and RTT times and updating send window at the transmit side. The following shows ACK Processor E states:\n\n","FIG. C shows the state machine process flow for data processor F. Data Processor F starts if ITP  determines that a segment should not be dropped or routed to the host. ITP  provides data processor (DP) F with miscellaneous header fields and in some cases the results of previously calculated values. DP F includes a Data Processor Controller (DC) whose functionality is described below:","Call Out of Order (\u201cOOO\u201d) processor module (located within the data processor module F ( of ITP ) to trim data that doesn't fit in window.","Update buflet offset to a point past the header to the first byte of data in the TCP segment.","Set the fin flag in the buflet header if FIN flag in TCP header is set and trimming didn't trim from the end of the segment.","If segment is in order and nothing is on the Reassembly list","If NCB has delayed ack timer set on","Set NCB.SAN (send ack now).","Clear delay timer","Request Output Processor D to add to Output Request list.","Else","Start the delayed ack timer.","Queue NCB on timer list.","Update rcv_nxt.","Pass segment up to output processor D.","Else if Data is out of order or Reassembly list is not empty, pass segment to OOO Data Placement to place the buflet accordingly.","Call OOO processor to properly place data.","If in order data is received","Update rcv_nxt.","Set SAN (send ack now) bit.","Pass segment to output block.","Else (out of order data)","Set SDA (send duplicate ACK) bit.","Request Output Processor D to add to Output Request List","Signal Output Processor D if new in order data was received.","Update Re-assembly list if needed.","FIG. C shows a process flow diagram where DP F processes in-order packet, as described below:","In Order","Lock the NCB.","Read the delay ACK timer.","If (delay ack timer is set)\n\n","FIG. C shows a process flow diagram where DP F processes out of order packets, as described below:","Out of ORDER","If (reassembly list is NOT empty)","Call place data.","Wait place data to finish.","If (reassembly list is empty)","Write recv'd buflet index into reassembly head","Else if (Out of order segment==NULL)","Invalidate the reassembly head.","Else if (Out of order segment returned from OO is not equal to the head)","Reassembly head=Out of Order segment returned by OO.","Else","Seg_length=new seg length from OO","If (out of order data recv'd)","Next_state=SEND_DUP_ACK.","Else","Next_state=SEND_ACK_NOW.","FIG. C provides a top-level view for ITP output processor D. Output Processor D state machine is split up into four sub-modules; idle, send error completion, send data, and fin processing.","The function of the sub-modules are described below:","Idle\u2014Checks which module is requesting a completion and send the completion message if no NCB was found.","Send Error Completion\u2014Sends the completion messages if Input Processor A detects an error with this segment.","Send Data\u2014There is valid in order data to send to the host\/embedded processor.","Fin Processing\u2014The current valid segment was received with a fin flag. If this was the first fin notify the host\/embedded processor when the segment is thrown.","TCP Table Manager","FIG. D is a block diagram of TTM  showing plural sub-modules that are used, according to one aspect of the present invention. TTM  includes plural registers (register set A) for ITP , IAP  and OAP , and provides read\/write access for the foregoing modules. TTM  provides Fetch\/Update\/Flush functions for working registers at host memory or local RAM . TTM  also sends error signal(s) to ITP  and IAP  if a requested inbound NCB is not present in local RAM . TTM  also sends an overload signal to OTP  if local RAM  resources are not available.","TTM  maintains timer functions for all TCP connections and co-ordinates all inbound and\/or outbound channel access to network data structures. TTM  maintains a free list of data structures (C), delayed request blocks that are used to place IOCBs into a waiting FIFO for processing. DRBs are also used to place OAL associated with an IOCB into local RAM . When an OAL is placed into local RAM  it may be referred to as a delayed address list (DAL).","TTM  also maintains a list of data structures to include NCBs (D) for connections that need to be processed by OTP , and maintain an outbound request list, which is a linked list of NCBs that are processed by OTP . Typically, ITP  and timer list manager E add NCBs to the list.","TTM  includes a command processor (CP) B that interfaces with plural command buses from OTP , OIP , ITP , IAP , TLM (Timer List Manager) E and ORLM (Outbound Request List Manager) F. TTM  arbitrates between various command sources and acknowledges the winner. CP B translates commands that are received from various modules to specific output actions of other TTM  components, as discussed below.","Outbound IOCB and NCM Management:","TTM  processes an outbound IOCB and builds the local RAM  data structures for the outbound channel. For a new IOCB for data transfer, the entire IOCB and OALs are read and placed in local RAM  before data is actually sent. In order to build a data structure for a newly created TCP connection, OTP  requests TTM  to do the following (see also , ):","Read the new NCB from host memory (not shown) and place it in register set (\u201cRS\u201d) A, and then write the new NCB into local RAM  using an entry from NCB free list. Thereafter, accept hash parameters into register set (\u201cRS\u201d) A and generate a hash value. Link the new NCB (B) off the hash table (A.  in ) using the generated hash value. This may involve following links from a hash table (A, ) entry that has other connections that match the same hash value.","For a hardware assisted TCP data transfer, the associated DRB is read from host memory and placed in register set (\u201cRS\u201d) A. Resident DAL (F) is linked to the most recent DRB (D) using a DRB from the free list. It is noteworthy that many DALs (F and G. ) may be coupled to a single DRB. After all the DALs for the transfer have been linked, a resident DRB (H or E) is linked to the NCB (C) in local RAM  using a DRB from the free list.","To build the local RAM data structure for an existing TCP connection, OTP  requests TTM  to perform the following:","Read NCB from local RAM  using the address provided in the DRB and place the NCB in register set (\u201cRS\u201d) A. For a hardware assisted TCP data transfer, read the DRB from host memory and place it in register set (\u201cRS\u201d) A. Thereafter, for each DAL, read the DAL from host memory and place it in register set A. Resident DAL is linked to the most recent DRB using a DRB from a free list. It is noteworthy that various DALs may be linked to a single DRB. After all DALs have been linked, link the resident DRB to the NCB in local RAM  using a DRB from the free list.","Flushing NCBs","OTP  may flush (or delete) an NCB after receiving commands from host  or OAP  through an IOCB to terminate an NCB. All data structures linked to the NCB are freed to their respective list managers. This includes DRBs, DALs and bufflets on the re-assembly lists. When the command to flush a particular NCB(S) is received, the NCB may be on the timer list, outbound request list, or in use by ITP  and\/or IAP .","When CP B receives the command to flush the NCB, and the NCB is on either list, or in use, CP B updates the NCB's tcp-state field to \u201cFLUSHED\u201d but does not free the memory associated with the NCB. List managers remove any NCB with tcp_state flushed.","When a NCB is being used by ITP  and\/or IAP , CP B updates the resident tcp_state to be flushed. ITP  and\/or IAP  is allowed to finish the current segment, however, the NCB is not written back to local RAM  after the processing is complete.","Outbound Request List Management",{"@attributes":{"id":"p-0477","num":"0547"},"figref":"FIG. 3F","b":["402","323","306","323","309"]},"TTM  manages a first in-first out (\u201cFIFO\u201d) process requests to OTP  from ITP  or TTM  itself. The FIFO process results in an \u201coutbound request list\u201d. The FIFO is implemented in RAM . The list may include requests to send linked list through NCBs (C, F and G, ) in local ACKs, notifications of ACKs received and notifications of data packets timeouts.","Typically, ITP  or TTM  place entries at the end of the request list (B). If more than one request exists in the FIFO, TTM  requests arbitration to RA . When RA  grants permission, TTM  fetches the NCB. When the NCB is available, TTM  notifies OTP  and removes the outbound request from the head of the request list (A).","Inbound Re-Assembly Data Structure Management",{"@attributes":{"id":"p-0481","num":"0551"},"figref":"FIG. 3G","b":["400","323","399","306","306","337","323","306","307","306","306","323","399"]},"TTM  compares local port, remote port, MA bits and remote IP address information (C, E and F, ) of the newly loaded NCB with the hash parameters in the hash parameter registers shown in register set A. If a match is found, ITP  is notified that a NCB is available. If a match is not found, TTM  searches for chained NCBs (A, B, D, ), and if found, ITP  is notified.","Timer List Management","TTM  implements the timer function through timer list manager E. This includes a pre-set timer setting, actual timer list and the ability to scan the timer list at certain intervals. Timer list manager E manages events or lack of events for both ITP  and OTP . Timer list  (See ) may be maintained in local RAM  as a linked list within the NCB data structures.","For OTP , TTM  maintains a \u201cpersist\u201d timer C (with timer list lead A and timer list tail B) and a \u201cretransmit timer\u201d D for each connection. For ITP , TTM  maintains an idle timer (F) and a delayed ACK timer (E) for each connection.","If a TCP connection needs to be timed and is not already on the timer list, OTP  or ITP  requests TTM  to add the connection's NCB to the timer list. When an NCB is added to the timer list, it is resident in TTM . When TTM  processes the timer list , it loads timer fields (G) into a TTM  cache (not shown). However, timer link field may be resident in local RAM .","At a pre-determined interval or programmable time, TTM  scans timer list  and checks for timer flags (G). If a flag is set, TTM  compares the timer value to the current tcp_now time. If the values are substantially equal then the timer has elapsed.","If a persist timer (C) elapses, TTM  places an NCB to the outbound request list. If a re-transmit timer has elapsed, TTM  places the NCB on an outbound request list to re-transmit the oldest unacknowledged segment. If the delayed ACK timer has elapsed TTM  places the NCB on outbound request list to have OTP  send an ACK.","If the delayed ACK timer has elapsed, TTM  places the NCB on outbound request list to have OTP  send an ACK.","If a timed event needs to be cleared, OTP  and\/or ITP  clears the timer valid flag. TTM  removes the entry from the list on its next timer list  scan, if no other timer flags are set.","Command Processor (\u201cCP\u201d) B","Takes commands from OTP , OIP , ITP , IAP ,","TLM E and ORLM. Arbitrates between command sources and acknowledge the winner. Completes processing on one command before starting another. Translates the received command to output actions to the other TTM  components.","The following provides a list of various command functions that command processor \u201cCP\u201d B executes:","store_ncb(reg_flag):","Writes an NCB from TTM  logical register set specified by reg_flag to local RAM  through the Local RAM Interface. Uses the Local RAM  Address Register to get the local RAM  address for NCB.","load_ncb(reg_flag, local_ram_addr):","Loads the NCB from specified local RAM address into TTM  register set specified by reg_flag using the Local RAM interface (\u201cLRI\u201d) A.","load_drb(local_ram_addr):","Loads the DRB at local_ram_addr from local RAM  to the DRB registers","store_dal(local_ram_addr):","Writes the resident DAL to local RAM  at the local_ram_addr.","CP write_register (reg_flag, addr, data):","CP B writes the a word (32 bits) of data specified by \u201cdata\u201d to the logical register set specified by reg_flag at the register field specified by addr.","CP B read_register (reg_flag, addr):","CP B reads a word (32 bits) of data from the register set specified by reg_flag at the register specified by addr.","CP B write_ram (local_ram_addr, data):","CP B writes a word (32 bits) of data specified by \u201cdata\u201d to the local RAM address specified by local_ram_addr.","CP B read_ram (local_ram_addr)","CP B reads the 1 word (32 bits) of data from the","local RAM address specified by local_ram_addr.","check_resident (local_ram_addr)","Checks if the given address corresponds to any valid NCBs resident in the RS A, including the NCB, if any, in the TTM  cache.","copy_tlc(reg_set):","CP B sets cp_rs_active_flag=TLM, asserts cp_rs_rql and waits for rs_cp_gtl.","Timer List Manager (\u201cTLM\u201d) E:","TLM E adds a persist expired timer counter and re-transmit timer expired counter. The following describes the various process steps for running the timer list (FIG. D).\n\n","Outbound Request List Manager (\u201cORLM\u201d) F:","ORLM F creates and manages outbound request list by adding NCBs when requested by Command processor B, and removing NCBs when they can be given to OTP .","Outbound TCP Processor (OTP ):","OTP :","Provides an \u201cidle\u201d signal to RA .","Reads outbound TCP IOCBs from NRM  and ERM  via Request Arbiter .","Writes an IOCB to TTM . Once the IOCB is written to TTM , its format changes, and is stored as a DRB.","Sends requests to ODE  for OALs associated with an IOCB and handshakes them to TTM  to be written to Local RAM . OTP  distinguishes between linking a DAL to the last DRB written to TTM  and linking a DAL to the last OAL written to TTM .","Sends requests to TTM  to read an NCB from host memory, if the NCB is resident in the host as indicated by the H and opcode bits in an IOCB.","Sends requests to TTM  to read an NCB from RISC memory, if the NCB is resident in the RISC memory.","If an NCB is downloaded from host\/RISC memory, OTP  fills in the remaining fields in the NCB and saves the NCB to local memory  if instructed to do so.","Sends a request to TTM  to read a resident NCB from Local RAM .","Updates any local NCB fields required (all local fields are initialized to 0) when an NCB is created (H bit in the first word of the IOCB).","Reads appropriate fields within the local NCB to determine if data and\/or ACK packets are transmitted for the NCB. This occurs when the IOCB's are read down from the host\/EP or when the request ready signal from TTM  is asserted.","If data transmission for an NCB is required, OTP  insures if a valid segment is transmitted. This may require reading data from the current DRB followed by reading data from a DRB linked to the NCB to fill the segment.","When all the data for a given DRB has been transmitted, OTP  writes the maximum sequence number used by the DRB before instructing TTM  to add the DRB to the delayed request list.","Instructs OIP  to build a header for data transmissions. OIP  uses the local NCB to build the header.","OTP  builds the TCP header and handshakes it to OIP .","Reads Address\/Length pairs using the TTM  interface to determine where to fetch data for transmission. This includes recognizing and following chains using the fetch OAL command to TTM .","Sends requests to the DMA Manager for data to be transmitted using Address\/Length pairs read from TTM  and handshakes this data to OIP .","When all the data has been transmitted for a DRB or no data can be transmitted due to window size, update appropriate fields and instruct TTM  to writeback the NCB to Local RAM .","When a request ready signal is asserted from TTM , OTP  checks the NCB to determine if an ACK packet was received that completes the DRB at the head of the delayed request list. This is done by comparing the sequence number of the oldest unacknowledged byte (Snd_Una Seq #) to the sequence number in the DRB. If a completion is required, OTP  sources and handshakes an Outbound TCP Completion to NCM . DRB data required for the completion is obtained from TTM . OTP  then writes a command to TTM  to remove the DRB from the head of the delayed request list then checks the next DRB (if it exists) to determine if another completion is required.","Build iSCSI digests if the appropriate bits are set in the NCB.",{"@attributes":{"id":"p-0541","num":"0661"},"figref":"FIG. 3I","b":["309","309","309","309","323","309"]},"Completion Manager E: Sends completion requests to either the network or NCM . Requests are sent from the various components of OTP  indicating the type of completion message to send. Completion manager E gets additional data, if required, from TTM  before setting the cm_cdone signal.","Request Manager G: Downloads an IOCB, saves it in TTM , and determines what action needs to be taken. If this is an update or flush command, it passes the command to TTM  and exits. If data is included in the IOCB it has Outbound DMA Interface (also referred to as ODE Interface Module) A fetch the OAL chain and links them. After the NCB has all the information in it, control is passed to the Main Control block C to continue data processing. After Main Control block C is finished, request manager G saves the NCB.","Window Update Module D: Handles all requests from TTM  for processing. This includes updating window size, updating the retransmission\/persist timers, sending completion messages, and removing DRBs if all the data has been acknowledged. After all information in an NCB has been updated, control is passed to Main Control block C to continue processing. After Main Control block C has finished, the Window update module D saves the NCB.","Main Control block C: Determines if a NCB is in a valid state to send data, how much data is to be sent, and if data should be sent based upon amount to send, window size, if the Nagle algorithm is enabled, and timer status. If there is a segment to send, Window Update Module D starts ODE Interface block A and waits for it to finish. After ODE Interface block A has finished, if there is more data to send, the process is repeated.","ODE Interface Module A: This module scans an OAL chain and then links the chain to an NCB, and then scans the DAL chain. Thereafter, it fetches the requested length of data from ODE  and passes the data to OIP . This block also requests OIP  to generate the TCP header for a new segment.","OIP Interface Module B: When requested by ODE Interface Block A, this block gets all data necessary from TTM  to generate a TCP header. While generating the header, this block locks the NCB and obtains the latest ACK information that it can. It then clears the delay ACK timer before unlocking the NCB. This module also requests OIP  to generate the IP header.","IPV A ():","IPV A has been described above with respect to various other modules. The following describes various sub-modules of IPV A with respect to . IPV A includes registers A and an input processor A that is coupled to MAM  and BLM . Input processor A receives input data from IFB , processes the data and sends it to output processor A, which is coupled, to IFP . Input processor A is also linked with IDE  and ILM .","FIG. J shows process flow diagram for a buflet list structure, as processed by IPV A. The following describes the various process steps:","If IFB  has data to send to local RAM :\n\n","1. Transfer frame to MAM .\n\n","2. Check data in transit from FIFO (IFB)  to MAM :\n\n","3. Update data structures:\n\n","4. Loop control decision\n\n","5. Frame verified and stored in local RAM \n\n","(26 because 24 for buflet control+2 bytes of alignment padding on mac header. (Mac_hdr_len includes this 2 bytes)","signature=signature","buflet_count=buf cnt",{"@attributes":{"id":"p-0558","num":"0000"},"ul":{"@attributes":{"id":"ul0047","list-style":"none"},"li":{"@attributes":{"id":"ul0047-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0048","list-style":"none"},"li":"buflet_data_len=For a frame where this is the ONLY buflet: save_data_len for a frame with multiple buflets: bufsize\n\ntcp_checksum=tcp_checksum\n\nlength=if MAC: frame_length\u22122 (align padding), if IP: IPLEN\u2212(IPHL*4)\n\nI=1\u2032b0 B=B, M=M, MA=MA, opcode=MAC\u2225IP\n"}}}},"6. Forward processing:","If not IP: Pass address of the frame head buflet to IDE  for forwarding to Host .","If IP and not known address: Pass address of frame head buflet to IDE  for forwarding to Host.","If IP and know address: add frame to tail of IFP  input list.",{"@attributes":{"id":"p-0560","num":"0000"},"ul":{"@attributes":{"id":"ul0049","list-style":"none"},"li":{"@attributes":{"id":"ul0049-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0050","list-style":"none"},"li":["if !(empty) {\n        \n        ","}","else {\n        \n        ","}"]}}}},"7. Error Processing:\n\n","OIP :",{"@attributes":{"id":"p-0563","num":"0718"},"figref":"FIG. 3K","b":["308","308","308","308"]},"OIP  provide an \u201cidle\u201d signal to RA  and reads outbound IP or outbound MAC IOCB from RA  and passes to TTM  for temporary storage.","If it is an IP packet, OIP  requests TTM  to fetch the NCB from the host. After processing all the OALs in the IOCB, OIP  uses ODE  to fetch the OAL List associated with the IOCB and passes it to TTM . OIP  also reads the appropriate fields within the local NCB to build the IP and MAC Headers and writes these headers to Outbound FIFO OFB .","For source MAC address field, OIP  sends the index in the first byte to FIFO OFB . The MAC block converts this to a proper address. The index is taken from the first word of the NCB. The location of the source address in FIFO OFB  is maintained by padding it with zeros.","For IP packets, OIP  also calculates the IP header checksum and TCP checksum of the data as it passes through flags when the locations of the IP and TCP checksum fields are being passed to Outbound FIFO OFB ; reads Address\/Length pairs from TTM  and pass them to ODE  to fetch packet data.","OIP  handshakes data from ODE  and passes it to Outbound FIFO OFB  and byte packs data obtained from ODE \/OTP . For an IP packet, OIP  also fragments data if the length from ODE  is greater than max_frame_size number of bytes (default ). This requires generation of a new header for each fragment.","OIP  sends completion error \u201cFrame too long\u201d if the IOCB request to add UDP checksum or a MAC header only transfer would cause data fragmentation because length is greater than max_frame_size number of bytes (default ).","OIP  also sends completion error \u201cFrame too short\u201d if the IOCB request to add UDP checksum and the Datagram Length is less than 8 bytes, MAC data transfer and the Datagram Length is less than 14 bytes, MAC data transfer with CRC disabled and Datagram Length is less than 64 bytes, or a MAC header only transfer has a Datagram Length of less than 20 (or any other number).","OIP  also sends \u201cFrame padded\u201d in the completion packet if the IOCB request MAC data transfer and the Datagram Length is less than, for example, 60 bytes.","When all the data has been sent, OIP  passes the IP packet and TCP checksums to Outbound FIFO OFB  with a flag, which indicates it is the actual checksum data inserted in the packet. The last word of data has an end bit set on it along with the length.","When all the data has been transmitted for an IOCB, OIP  generates a completion packet using data from the NCB\/IOCB in TTM  and handshake this data to Completion Manager E.","Thereafter, OIP  stops transmitting packets (at the next possible packet boundary) while mac_pause_rxd is true.","Outbound FIFO interface A handles all handshaking in the outbound pipeline through a byte packer and calculates IP header checksum and TCP\/UDP checksum. Outbound FIFO interface A uses dav\/dak signals to transfer data. It calculates the IP header checksum and TCP checksum of the data as it passes through. It flags when the locations of the IP and TCP checksum fields are being passed to Outbound FIFO OFB ; handshakes data from ODE , OTP , TTM Interface B and parses it to Outbound FIFO OFB ; and byte packs data obtained from ODE","When all the data has been sent, interface A passes the IP and TCP checksums to Outbound FIFO OFB  with a flag that indicates it is the actual checksum data to be inserted in the packet.","TTM interface B reads outbound IP IOCB from RA  and pass to TTM  for temporary storage and saves the H (mac_hdr_only), U (UDP_En), Opcode_Embedded, and D (Disable_Comp) bits in transit. Interface B requests TTM  to fetch the NCB from the host or EP based upon the Opcode_Embedded bit above. After processing all the OALs in the IOCB, it uses ODE  to fetch bytes of the OAL associated with the IOCB and passes it to TTM  for temporary storage. Interface B reads the appropriate fields within the local NCB to build the IP and MAC Headers and writes these headers to Outbound FIFO OFB .","When creating the source MAC address field, interface B passes the index to the correct MAC address registers instead of filling in the actual address. The MAC block converts this to the proper address. The index is taken from the first word of the NCB. IP length field is precalculated since it is placed in the header before some other IP header info.\n\n","IFP ",{"@attributes":{"id":"p-0580","num":"0736"},"figref":"FIG. 3L","b":["305","305","305","305","3","1","305"]},"IFP  includes input processor D that is responsible for handshaking and parsing IP header data received from IPV A. Input processor D also assembles complete datagrams, including checking for timeout. It also provides buflets for completed datagrams to output processor C and provides timed out buflets to return processor A.","The following describes IFP  functionality including IP packet reassembly with respect to FIG. L (same as ).","Input Processor D handshakes received IP packet headers from IPV A. If the received IP packet from IPV A is a complete datagram, processor D passes the received IP packet header and buflet pointer to output processor C.","An \u201cipv_ifp_dav\u201d signal from IPV A indicates that there is a frame for IFP  to process. Processor D accepts frame buflet address, status and IP header from IPV A. If the packet is a full datagram, the address of the first buflet of the frame is linked on the output queue. This queue of datagrams is sent to ITP . Each datagram may be identified by a 4-tuple {IPID, IPSRC, IPDST, IPP}. This identifier is hashed to a 16 bit value, using a 16 bit XOR function. A programmable number of bits are used to index into a hash table to search for a linked list of fragments.","If the packet is not full datagram, processor D checks if an entry already exists in reassembly list by hashing the IP 4-tuple and reading the corresponding hash table entry from MAM . Processor D checks the Valid bit in the returned entry to see if the entry is filled. If no entry exists in the hash table, an entry is made and the address of the first buflet of the frame is written in the entry with the Valid bit set. When the 1fragment (fragment offset=0) of a datagram is added to the reassembly list, the first fragment flag is set in the status word in the 1buflet.","If an entry already exists, the entry points to one or more datagrams that matched the hash. Processor D reads the IP header of the first frame associated with the hash from MAM . If the 4-tuple matches the 4-tuple of the current frame, the current frame is part of this existing datagram, if the tuple does not match IFP  follows the datagram link field in the buflet header and reads the IP header of the next frame on the datagram list for this hash entry from MAM  until a match is found or the end of list is reached.","If the datagram does not exist already, it is added to the end of the datagram list associated with the hash. When the 1fragment of a datagram (fragment offset=0) is added to the reassembly list, the first fragment flag is set in the status word in the 1buflet.","If the datagram is found on the list, the buflet for this fragment is added to the list of fragments for the datagram. The head of the datagram fragment list is saved. If the fragment is added as the first fragment of the datagram (fragment offset=0), the first fragment flag is set in the status word in the 1buflet. If the fragment is the last fragment of the datagram, as signaled by the \u201cmore fragments\u201d bit being clear in the IP header, the last fragment bit is set in the 1buflet's status word.","When a fragment is added to the reassembly list, IFP  checks to see if the fragment is sequential to either the previous fragment or the next. If it is, Processor D trims the fragment and then buflinks the fragments. By doing this as each fragment arrives, the entire datagram is buflinked when that last fragment arrives and keeps IFP  from having to run the link list to do the required linking.","When fragments are combined, the partial TCP checksum fields are added together.","When a new datagram is added to the reassembly list, the timestamp field is set to current_time plus the programmable IP timeout value, typically 300 (30 seconds) and it is added to the tail of the timeout list.","If this is the first entry in the timer queue, this same value is loaded into the \u201chead_timestamp_value\u201d register.","Processor D then checks if the entire datagram is in memory, using the saved head of the datagram. The hardware checks if both the first and last fragment bits are set and that the fragment link is NULL. If the full datagram is present, the block removes the datagram from the reassembly list.","If the reassembled datagram is not destined for TCP, the address of the first buflet of the frame is passed to IDE  to send to host  for disposition.","If the reassembled datagram is destined for TCP, the address of the first buflet of the frame is added to the output queue. When there is an entry on the output queue, IFP  puts the address of the first buflet, the IP header, and the TCP header to Output FIFO , which handshakes this data to ITP . When finished, IFP  de-queues this item and determines if there are other items on the output queue, if so, the items are sent to ITP .","When IDLE, IFP  checks for timeout. If there is an entry on the timeout list, IFP  de-queues the entry (note that only entries at the head of the list can get de-queued). Because of this, de-queuing an entry means setting the \u201cTO_list_head\u201d to the entries \u201cnx_TO_lnk\u201d. If TO_list_head is NULL, set \u201cTO_list_tail\u201d to NULL. The de-queued entry is given to BLM . It goes through frg_lnk and then to BLM .","FIG. L shows various sub-modules of input processor D, which are described below.","Input Register D:","Input register D handshakes received IP packet headers from IPV A. If the received IP packet from IPV A is a complete datagram, it passes the received buflet pointer to output processor C.","If received IP packet is not a complete datagram, it signals Fragment Processor D to process the packet.","FIG. L shows a state machine diagram for input register D, and the following describes the various states:","IDLE: Waits for data available signal from IPV A. When asserted, handshakes the data from IPV A and save in the input registers.\n\n","IR2OP: Transfer header data in reg0, reg1 and reg5 to output processor C.","ip_op_dav<=1","if (op_ip_dak)","xfer_cnt<=xfer_cnt\u22121","if (xfer_cnt=1)","ip_op_end<=1","WAIT_FP: Wait for the fragment processor D to complete","Fragment Processor D:","FIG. L shows the sub-modules of fragment processor D. Fragment processor D processes an IP packet that is not a complete datagram as described above. Each block in FIG. L represents a control state machine and associated logic to perform the tasks discussed above.","Fragment Processor Main D starts when the input register D determines that the packet currently being processed is a datagram fragment.","Fragment Processor Main D starts Hash logic D, which calculates the hash, and if necessary runs the hash Nxt_Dgm_Lnk (see FIG. L\/B) to try and find a match. If a match is found, the Fragment Processor D starts Place Data module D, which determines where the received fragment is placed by running the Frg_Lnks.","If trimming is required, Placed Data module D starts the Trim Logic D to perform this function.","FIG. LA-LC show the various state machine processes of Fragment main processor D to process IP datagrams. The following describes the various states:","IDLE: Wait for input register block to signal that it has received fragment for processing. Read the first 8 words pointed to by input register buflet pointer into the receive buflet registers, clearing the Nxt_Dgm_Lnk, Prv_Dgm_Lnk, and Frg_Lnk fields and updating the signature field.","If (frag & !ma_done)","ld_rxbr=1","mrd=1","buflet=ir.bp","offset=0","length=32","CHK_HASH_TBL: Check hash table: Read the Hash table entry (1 word) pointed to by the hash value computed by hash logic D. Save the upper 16 bits from this read in treg1. If the V bit is set, signal hash logic D to check for a match of the 4 tuple.","buflet=hl.hash","mht=1","mrd=1","offset=0","length=4","if (ma_done)","treg1=ifp_rd_data[31:16]","treg2=16\u2032h0000","hl.hcalc=ma_done & ifp_rd_data[0]","WR_HASH: Write the buflet pointer of the input registers to the hash table entry pointed to by hash value computed by the hash logic setting the V bit.","index=hl.hash","mht=1","mwr=1","length=4","data={hl.hash, 16\u2032h0001}","WT_HASH: Wait for the hash logic D to complete. If the hash logic has a match, signal the place data logic to place the data and save the buflet pointer in treg1 to head datagram register.","If (!hl.hcalc & hl.match)","set_pl_dat=1","hddg=treg1","UD_PDG: Update the next datagram link of the last entry with the address of the received buflet. Update the receive buffer reg previous datagram link with the last entry.","mrmwl=1","index=treg2","offset=24","length=32","data=ir.bp","rxbr.pdgrm=treg2","WR_RXB: Write the 8 words in the receive buflet registers to memory. Buflet address is the buflet pointer in input registers.","wr_rxbr=1","mwr=1","index=ir.bp","offset=0","length=32","data=rxbr.dat","TMR_ADD: Signal the timer process to add the receive buflet address to the timer list.","WT_PLACE: Wait for the place data D state machine to complete. Check to see if the datagram was completed\u2014Head frag link null, first and last bits set.","WT_CKSUM: Wait for the checksum calculation block to recalculate the tcp checksum.","RMV_DGM1: If P bit is set, write the Nxt_Dgm_Lnk of the receive buflet registers to the Hash table entry pointed to by the hash value computed by hash logic D. If P bit is not set, read modify write memory address=Prv_Dgm_Lnk+7 update the Nxt_Dgm_Lnk at this location to the Nxt_Dgm_Lnk in the receive buflet registers.","If (rxbr.p_bit)","index=hl.hash","mht=1","mwr=1","offset=0","data=rxbr.ndg","else","index=rxbr.pdg","mrmwu=1","offset=28","data=rxbr.ndg","RMV_DGM2: If P bit is set, read modify memory address=Nxt_Dgm_Lnk+12 set P bit. If the P bit is not set and the Nxt_Dgm_Lnk of the receive buflet registers is not null, read modify write memory address=Nxt_Dgm_Lnk+7 update the Prv_Dgm_Lnk at this location to the Prv_Dgm_Lnk in the receive buflet registers.","If (rxbr.p_bit)","index=rxbr.ndg","mrmwu=1","offset=12","pb_ud=1","else","index=rxbr.ndg","mrmwu=1","offset=28","data=rxbr.pdg","RMV_DGM3: Update the PRV_DGM_LNK of the buflet pointed to by receive buflet registers Nxt_Dgm_Lnk to the hash index.","index=rxbr.ndg","mrmwl=1","offset=28","data=rxbr.pdg","TMR_RMV: Signal timer processor D to remove the receive buflet address from the timer list.","PASS2OUT: Signal input register D to handshake the IP packet header to output processor C.","GEN_COMP: Handshake the buflet pointer of the head of the datagram to completion processor D.","Hash Logic D:","Hash logic D calculates the hash value for the received ip packet when signaled by the fragment processor state machine, it runs the Nxt_Dgm_Link chain searching for a fragment link, which matches the 4-tuple of the received fragment. FIG. L shows the various state machine states and processes performed by hash logic D. The following describes the process flow and the various states:","IDLE: Wait for hcalc to be set. When set read first 8 words of the buflet pointed to by treg1 and save in the temp buffer registers.","index=treg1","offset 0","mrd=1","length=32","CK_HDAT1: Read the 3 words of IP header data to include the 4-tuple. Check for a match between ipid, from the input register D and the ipid in the data from memory.","index=treg1","offset=tmpb_ipbofs","mrd=1","length=12","CK_HDAT2: Check for a match between ipp from the input register D and the ipp in the data from memory.","CK_HDAT3: Check for a match between ipsrc from the input register block and the ipsrc in the data from memory.","GET_NXT: Read the Nxt_Dgm_Lnk of the buflet pointed to by treg1 into treg.","index=treg1","offset=28","mrd=1","length=4","if (ma_done)","mn.treg1=ip_rd_data[31:16]","mn.treg2=ip_rd_data[15:0]","FIG. LA-LD shows the process flow diagram for Fragment Processor Place Data D state machines. The various states are described below:","IDLE: Wait for a place data request from the Fragment Processor D. If a request occurs, read the first 8 words of the buflet pointed to by main treg1 and save in temp buflet registers.","if (pl_dat & !ma_done)","ld_tmpbr=1","index=mn.treg1","offset=0","length=32","mrd=1","CALC: \u201cCalculate Position\u201d Determine where the received fragment is placed relative to the tmp fragment.","UD_PDGM: \u201cUpdate Previous Datagram\u201d If the P bit is set in the temp buflet registers, write the hash table entry pointed to by Prv_Dgm_Lnk of the temp buflet registers with the receive buflet pointer. If the P bit is not set and the H bit is set, read, modify and write the Nxt_Dgm_Lnk of the buflet pointed to by Prv_Dgm_Lnk of the temp buflet registers with the receive buflet pointer.","offset=0","length=4","if (tbr.p_bit)",{"@attributes":{"id":"p-0717","num":"0000"},"ul":{"@attributes":{"id":"ul0059","list-style":"none"},"li":{"@attributes":{"id":"ul0059-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0060","list-style":"none"},"li":"data={ir.bp, 16\u2032h0000}"}}}},"index=tmpbr.pdgm","mht=1","mwr=1","else if (tbr.h_bit)","data=ir.bp","index=tmpbr.ndgm\n\n","BRANCH: Go to the correct state dependent on the calculated position of the receive fragment.","GET_NXT_DG: If backup, load the main temp_reg1 register with the last_frag address. If backup is not set, load the main temp_reg1 register with the Frg_Lnk from the temp buflet registers.","mn.treg1=tmpbr.frg_lnk","JOIN_P1: Copy the Nxt_Dgm_Lnk, Prv_Dgm_Lnk, Nxt_TO_Lnk, Prv_TO_Lnk, Frg_Lnk, Timestamp, Pbit and H bit from the temporary (\u201ctemp\u201d) buflet registers to the receive buflet register clearing these fields in the temp buflet registers","rxbr.ndl=tmpbr.ndl","rxbr.pdl=tmpbr.pdl","rxbr.ntl=tmpbr.ntl","rxbr.ptl=tmpbr.ptl","rxbr.fl=tmpbr.fl","rxbr.ts=tmpbr.ts","rxbr.p_bit=tmpbr.p_bit","JOIN_P:","If the Buf_Lnk field of the receive buflet registers is null, update it with the buflet address of the temp buflet (in treg1).","If (rxblnk_null)","rxbr.bl=mn.treg1","JOIN_P: Write the Buf_Lnk field of the buflet pointed to by the Buf_Lnk_Tail of the receive buflet registers with the buflet address of the temp buflet registers (in treg1).","index=rxbr.bltl","offset=0","length=4","data={mn.treg2, 16\u2032h0000}","mwr=1","JOIN_P:","Load treg1 with the last fragment to go to \u201cnext\u201d. mn.treg1=last_frag","Update the length field of the receive buflet registers with its value and the Length in the temp buflet registers.","new_len=rxbr.len+tmpbr.len","rxbr.len=new_len","Update the Buflet Count field of the receive buflet registers with its value plus the Buflet Count field of the temp buflet registers.","new_cnt=rxbr.cnt+tmpbr.cnt","rxbr.cnt=new_cnt","Update the Buf_Lnk_Tail of the receive buflet registers with the Buf_Lnk_Tail of the temp buflet registers.","rxbr.bltl=tmpbr.bltl","Update the checksum field of the receive buflet registers with its value+the checksum filed of the temp buflet registers.","new_cksum=rxbr.cksum+tmpbr.cksum","rxbr.cksum=new_cksum","JOIN_N1:","If the Buf_Lnk field of the temp buflet registers is null, update it with the buflet address of the receive buflet. If (tmpblnk_null)","tmpbr.bl=ir.bp","If the Frg_link field of the temp buflet registers is equal to the buflet pointer of the receive buflet registers, update it with the Frg_link in the receive buflet registers (This is null if the datagram is complete).","if (tmpbr.fl==rxbr.fl)","tmpbr.fl=rxbr.fl\n\n","index=tmpbr.bltl","offset=0","length=4","data={mn.treg2, 16\u2032h0000}","mwr=1","JOIN_N3:","Update the length field of the temp buflet registers with its value and the Length in the receive buflet registers.","new_len=rxbr.len+tmpbr.len","tmpbr.len=new_len","Update the Buflet Count field of the temp buflet registers with its value plus the Buflet Count field of the receive buflet registers.","new_cnt=rxbr.cnt+tmpbr.cnt","tmpbr.cnt=new_cnt","Update the Buf_Lnk_Tail of the temp buflet registers with the Buf_Lnk_Tail of the receive buflet registers.","tmpbr.bltl=rxbr.bltl","Update the checksum field of the temp buflet registers with its value+the checksum field of the receive buflet registers.","new_cksum=rxbr.cksum+tmpbr.cksum","tmpbr.cksum=new.cksum","FRAG_LNK_N: Update the Frg_Lnk of the temp buflet registers with the receive buflet pointer.","tmpbr.fl=ir.bp","FRAG_LNK_P: Update the Frg_Lnk of the receive buflet registers with the temp buflet pointer. If the h_bit is set in the temp buflet registers, copy the Nxt_Dgm_Lnk, Prv_Dgm_Lnk, Nxt_TO_Lnk, Prv_TO_Lnk, Timestamp, and the P bit from the temp buflet registers to the receive buflet register clearing these fields in the temp buflet registers.","rxbr.fl=tmpbr.fl","tmpbr.fl=0","if (tmpbr.h_bit)\n\n","rxbr.ndl=tmpbr.ndl","rxbr.pdl=tmpbr.pdl","rxbr.ntl=tmpbr.ntl","rxbr.ptl=tmpbr.ptl","rxbr.fl=tmpbr.fl","rxbr.ts=tmpbr.ts","rxbr.p_bit=tmpbr.p_bit","FREE_RX: Pass the receive buflet pointer to the return processor A to be freed.","free_req=1","buf2free=ir.bp","FREE_TMP1: Copy the following fields from the temp buflet registers to the receive buflet registers:","Timestamp, Nxt_TO_Lnk, Prv_TO_Lnk, Frg_Lnk, Nxt_Dgm_Lnk, Prv_Dgm_Lnk, and P bit. Pass the temp buflet pointer to the return processor to be freed. If the tmpbr frag link !=0, Load treg1 with the frag link in the temp buflet registers.","free_req=1","buf2free=mn.treg1","if (free_idle)\n\n","FREE_TMP2: If the H bit is not set, read modify write address=last_frag.frag_lnk data=ir.bp (write the frag link field of the previous fragment with the address of the receive buflet).","If (!tmpbr.h_bit)","index=last_frag","offset=?","data=ir.bp","mrmwl=1","WR_RXBR: Write the receive buffer registers to memory.","wr_rxbr=1","index=ir.br","offset=0","length=32","data=rxbr.data","mwr=1","mn.treg1=last_frag","WR_TMPBR: Write the temp buffer registers to memory.","wr_tmpbr=1","index=mn.treg1","offset=0","length=32","data=tmpbr.data","mwr=1","Fragment Processor Trim Logic D:","Fragment Processor Trim Logic D performs different functions depending on the location of the received fragment relative to a temp fragment. If the receive fragment is located \u201cbefore\u201d the temp fragment, Fragment Processor Trim Logic D calculates the amount of data to save in the receive fragment then updates the buflet and or buflets linked through the buf link chain correctly.","If the receive fragment is located \u201cafter\u201d the temp fragment, Fragment Processor Trim Logic D calculates the amount of data to trim from the beginning of the received fragment then updates the buflet and\/or buflets linked through the buf link chain correctly.","FIG. LA-LB shows various states for Fragment Processor Trim Logic D state machines. The following describes the various states:","IDLE: Wait for a trim data request from the Place Data State Machine. Calculate the new fragment length for the received fragment.","If (ptrim)","Pre_calc=rx_begin\u2212tmp_begin","Else","Pre_calc=tmp_end\u2212rx_end","Wk_len=pre_calc","UD_LEN: Load the length of the receive buflet registers with the new length (new_len). Load wk_len with the amount of data to save or the amount of data to trim.","rxbr.len=new_len","If (ptrim)","Pre_calc=rx_begin\u2212tmp_begin","Else","Pre_calc=rx_begin\u2212tmp_end","Wk_len=pre_calc","NTRIM: Determine if the current receive buflet has data equal to or less than the amount of data to be trimmed.","NUPD_BUF: Load the Buflet Data_Len of the receive buflet registers with 0. Set the Buffer_Offset to the buf_size. Save the address in Buf_Lnk of the receive buflet registers (temp_reg2).","New_bdlen=0","rxbr.bdl=new_bdlen","New_ofs=buf_size","rxbr.ofs=new_ofs","mn.treg2=rxbr.bl","Wk_len=wk_len\u2212buf_len","NWR_BUF: Write the receive buflet registers to memory.","wr_rxbr=1","index=ir.rb","offset=0","length=32","mwr=1","NRD_BNXT: Read the first 8 words of the buflet pointed to by Buf_Lnk of the receive buflet registers (address=treg2) into the receive buflet registers.","ld_rxbr=1","index=treg2","offset=0","length=32","mrd=1","NLAST_BUD: Load the Buflet Data_Len of the receive buflet registers with the calculated length (new_bdlen). Load the Buffer Offset of the receive buflet registers with the calculated offset (new_bofs).","new_bdlen=bdlen","rxbr.bdl=new_bdlen","New_ofs=bofs","rxbr.ofs=new_ofs","PTRIM: Determine if the current receive buflet has data equal to or less than the amount of data to be saved (wk_len). Save the address in Buf_Lnk of the receive buflet registers (temp_reg2).","Mn.treg2=rxdr.bl","PUPD_BUF: Load the Buflet Data_Len of the receive buflet registers with the Buflet Data Length\u2212Working Length (new_bdlen). Save the address in Buf_Lnk of the receive buflet registers (temp_reg2).","Mn.treg2=rxbr.bl","New_bdlen=bdlen","rxbr.bdl=new_bdlen","PWR_BUF: Write the receive buflet registers to memory.","wr_rxbr=1","index=ir.rb","offset=0","length=32","mwr=1","PRD_BNXT: Read the first 8 words of the buflet pointed to by Buf_Lnk of the receive buflet registers (address=temp_reg2) into the receive buflet registers.","ld_rxbr=1","index=treg2","offset=0","length=32","mrd=1","PCLR_BUF: Clear the Buflet Data Length of the receive buflet registers.","New_bdlen=0 rxbr.bdl=new_bdlen","Timer Processor D:","Timer processor D maintains a linked timer list for IP datagram fragments and provides an \u201cidle\u201d signal to Fragment Processor. Timer processor D adds items to the end of the list and replaces items on the list when signal asserted by Fragment Processor. Timer processor D maintains timestamp of the item at the head of the list and generate a timeout signal if the item times out.","FIG. L shows various timer processor D states, which are described below:","IDLE: Wait for an add, remove, or swap request from the fragment processor D. If remove request and head equals tail (1 item on the list), clear the head and the tail. If add request, load the rxbr.timeout with the to_value register. If add request and the list is empty, load the head and the tail with the new entry and load the to_value register with the timeout value. If swap request and head equals tail (1 item on the list), load the head and the tail with new index ir.bp.","UPD_TAIL: Read and modify upper the Nxt_TO_Lnk of the tail register with ir.bp. Update rxbr.Prv_TO_Lnk with the tail register.","Index=tail","Offset=20","Length=4","Rmwu=1","Data=ir.bp","READ_NEWTO: Read the timeout value of the new head and load the to_value register with this value.","Index=rxbr.nextto","Offset=16","Length=4","UD_PREV: Read, modify and write the next timeout link of the buflet pointed to by tmpbr.prevto with tmpbr.nextto. If mn.treg1 equals tail, load tail with tmpbr.prevto.","Rmwu=1","Index=tmpbr.prevto","Offset=20","Length=4","Data=tmpbr.nextto","UD_NXT: Read, modify and write the previous timeout link of the buflet pointed to by tmpbr.nextto with tmpbr.prevto.","Rmwl=1","Index=tmpbr.nextto","Offset=20","Length=4","Data=tmpbr.prevto","LD_NEWHD: Load the head pointer with the new buflet pointer (ir.newbp).","SWAP_PREV: Read, modify and write the next timeout link of the buflet pointed to by the tmpbr.prevto with ir.bp. If treg1 equals tail, load tail with ir.bp","Rmwu=1","Index=tmpbr.prevto","Offset=20","Length=4","Data=ir.bp","SWAP_NXT: Read, modify and write the previous timeout link of the buflet pointed to by tmpbr.nextto with ir.bp.","Rmwl=1","Index=tmpbr.nextto","Offset=20","Length=4","Data=ir.bp","Output Processor C:","Output processor C maintains an \u201coutput list\u201d of IP datagrams destined for TCP and maintains a register array to store header data destined for TCP. Processor C accepts a buflet pointer from processor D for received IP datagrams destined for TCP.","If the output list and the register array are empty, processor C handshakes the header data that follows the buflet pointer from processor D to the cut-thru register array and reads the buffer offset field and IP header length from memory to determine beginning of TCP header.","Processor C also reads the TCP header (20 bytes) and TCP options (12 bytes) from memory and writes to the output register array. If the output list or the register array is not empty. Drop the header data from the input processor on the floor and add the buflet pointer to the output list.","FIG. L shows the various states of Output processor C state machines. The various states are described below:","IDLE: State machine is waiting input to do the following in order:","Load output register with the next item on the list.","Load the output register with data from the input processor.","Add the index from the input processor D to the head & tail if the output register is full and the list is empty.","Add the index from the input processor to the tail of the list if the output register full and the list is not empty.","WR_TAIL:","1) Assert op_ip_dak for as long as ip_op_dav is asserted to drain the ip data registers","2) Write address of buflet pointer from the input processor to frag_lnk field of previous tail pointer.","3) When ma_done is asserted, update the tail with the buflet pointer from the input processor.","DRAIN_IP: If data remain in the IP data registers (ip_op_dav is asserted), assert op_ip_dak until the registers are empty (\u02dcip_op_dav).","SNP_BCTL: Store first 3 words of data to pass to ITP from IP into Cut-Through array.","RD_BCTL: Read words - of buflet control fields of head buflet and store buflet index, Checksum, length, and flags in register array.","RD_IPHDR: Read IP source address and store in register array.","RD_TCPHDR: Read TCP header and max size TCP options of head buflet and store in register array.","Return Processor A:","Return processor A takes buflets from input processor D and returns them and any frg_lnked buflets to BLM . FIG. L shows the various states of return processor A state machine(s). The various states are described below:","IDLE: Assert rp_ip_idle and wait for a fragment\/buflet to return to BLM .","if (ip_rp_remove)",{"@attributes":{"id":"p-0885","num":"0000"},"ul":{"@attributes":{"id":"ul0069","list-style":"none"},"li":{"@attributes":{"id":"ul0069-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0070","list-style":"none"},"li":["curr_buf <=ip_rpbuf_ptr","set curr_ff","clear rp_ip_idle"]}}}},"RD_FRG_LNK: Read the frag_lnk field of the buflet pointed to by curr_buf.","If (ma_done)",{"@attributes":{"id":"p-0887","num":"0000"},"ul":{"@attributes":{"id":"ul0071","list-style":"none"},"li":{"@attributes":{"id":"ul0071-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0072","list-style":"none"},"li":["curr_buf <=ifp_rd_data","curr_ff <=(ifp_rd_data !=0)","ifp_free_adr<=curr_buf","ifp_free_bav<=curr_ff"]}}}},"FW_FRG: Release the buflet chain to BLM .","If (ifp_free_bak)",{"@attributes":{"id":"p-0889","num":"0000"},"ul":{"@attributes":{"id":"ul0073","list-style":"none"},"li":{"@attributes":{"id":"ul0073-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0074","list-style":"none"},"li":["if (curr_ff)","set up read of next of frag_lnk field of buflet pointed to by curr_buf","else","set rp_ip_idle"]}}}},"Although the present invention has been described with reference to specific embodiments, these embodiments are illustrative only and not limiting. Many other applications and embodiments of the present invention is apparent in light of this disclosure."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The foregoing features and other features of the present invention will now be described with reference to the drawings of a preferred embodiment. In the drawings, the same components have the same reference numerals. The illustrated embodiment is intended to illustrate, but not to limit the invention. The drawings include the following Figures:",{"@attributes":{"id":"p-0052","num":"0051"},"figref":"FIG. 1A"},{"@attributes":{"id":"p-0053","num":"0052"},"figref":"FIG. 2A"},{"@attributes":{"id":"p-0054","num":"0053"},"figref":"FIGS. 2B-2C"},{"@attributes":{"id":"p-0055","num":"0054"},"figref":"FIGS. 3A-1"},"FIG. A--A (jointly referred to as ) is a block diagram of a system, according to one aspect of the present invention;",{"@attributes":{"id":"p-0057","num":"0056"},"figref":"FIG. 3B"},{"@attributes":{"id":"p-0058","num":"0057"},"figref":["FIG. 3C","FIG. 3B"]},"FIG. C shows an option block state machine diagram used by the input processor of ;","FIG. C is a validation state machine diagram used by the ITP, according to one aspect of the present invention;","FIG. C is a validation state machine diagram for Reset, SYN or invalid state according to one aspect of the present invention;","FIG. C shows a state machine diagram for trimming, as used by the ITP, according to one aspect of the present invention;","FIG. C shows a validation state machine for time stamp functionality, according to one aspect of the present invention;","FIG. C shows an acknowledgement processor used by the ITP, according to one aspect of the present invention;","FIG. C shows a data processing state machine diagram as used by the ITP, according to one aspect of the present invention;","FIG. C shows an in order data processing state machine diagram as used by the ITP, according to one aspect of the present invention;","FIG. C shows an out of order data processing state machine diagram as used by the ITP, according to one aspect of the present invention;","FIG. C is a block diagram of an ITP output processor state machine as used by the ITP, according to one aspect of the present invention;",{"@attributes":{"id":"p-0069","num":"0068"},"figref":"FIG. 3D"},"FIG. D is a block diagram of a timer list state machine, according to one aspect of the present invention;",{"@attributes":{"id":"p-0071","num":"0070"},"figref":"FIG. 3E"},{"@attributes":{"id":"p-0072","num":"0071"},"figref":"FIG. 3F","b":"323"},{"@attributes":{"id":"p-0073","num":"0072"},"figref":"FIG. 3G"},{"@attributes":{"id":"p-0074","num":"0073"},"figref":"FIG. 3H"},{"@attributes":{"id":"p-0075","num":"0074"},"figref":"FIG. 3I"},{"@attributes":{"id":"p-0076","num":"0075"},"figref":"FIG. 3J"},"FIG. J shows a buflet list, according to one aspect of the present invention;",{"@attributes":{"id":"p-0078","num":"0077"},"figref":"FIG. 3K"},{"@attributes":{"id":"p-0079","num":"0078"},"figref":"FIG. 3L"},"FIG. L shows a link list data flow diagram for IP reassembly as performed by IFP, according to one aspect of the present invention;","FIG. L shows an input processor block diagram as used by the IFP, according to one aspect of the present invention;","FIG. L shows a state machine flow diagram for input registers used by the IFP, according to one aspect of the present invention;","FIG. L is a block diagram of the fragment processor used by the IFP, according to one aspect of the present invention;","FIGS. LA-LC show a state machine diagram for the fragment processor of FIG. L, according to one aspect of the present invention;","FIGS. LA-LD show a flow diagram for an IFP place data state machine, according to one aspect of the present invention;","FIGS. LA-LB show a flow diagram for an IFP trim state machine, according to one aspect of the present invention;","FIG. L shows a flow diagram for an IFP hash logic state machine, according to one aspect of the present invention;","FIGS. LA-LB show a flow diagram for a time processor state machine used by the IFP, according to one aspect of the present invention;","FIG. L is a flow diagram for an output processor in the IFP, according to one aspect of the present invention;","FIG. L shows a return processor state machine diagram, according to one aspect of the present invention;",{"@attributes":{"id":"p-0091","num":"0090"},"figref":"FIG. 4A"},{"@attributes":{"id":"p-0092","num":"0091"},"figref":"FIG. 4B"}]},"DETDESC":[{},{}]}
