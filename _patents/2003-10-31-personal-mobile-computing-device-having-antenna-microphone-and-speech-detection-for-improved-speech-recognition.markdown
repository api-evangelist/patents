---
title: Personal mobile computing device having antenna microphone and speech detection for improved speech recognition
abstract: A mobile computing device, adapted to be held in the palm of a user's hand, includes an antenna for transmission of information from the mobile computing device. A first microphone, adapted to convert audible speech from the user into speech signals, is positioned at a distal end of the antenna. The antenna is rotatable, while the mobile computing device is held by the user, into a position which directs the first microphone toward the mouth of the user. A speech sensor outputs a sensor signal indicative of whether the user is speaking.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07120477&OS=07120477&RS=07120477
owner: Microsoft Corporation
number: 07120477
owner_city: Redmond
owner_country: US
publication_date: 20031031
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"p":["The present application is a continuation-in-part of and claims priority of U.S. patent application Ser. No. 09\/447,192, filed Nov. 22, 1999 now U.S. Pat. No. 6,675,027, the content of which is hereby incorporated by reference in its entirety.","Reference is made to co-pending and commonly assigned U.S. patent application Ser. No. 10\/636,176, filed Aug. 7, 2003 and U.S. patent application Ser. No. 10\/629,278, filed Jul. 29, 2003, the contents of which are hereby incorporated by reference in their entirety.","The present invention relates to personal mobile computing devices. More particularly, the present invention relates to an apparatus, system and method for enhancing speech recognition in mobile computing devices.","Mobile devices are small electronic computing devices sometimes referred to as personal digital assistants (PDAs). Many of such mobile devices are handheld devices, or palm-size devices, which comfortably fit within the hand. One commercially available mobile device is sold under the trade name HandHeld PC (or H\/PC) having software provided by Microsoft Corporation of Redmond, Wash.","Generally, the mobile device includes a processor, random access memory (RAM), and an input device such as a keyboard and a display, wherein the keyboard can be integrated with the display, such as a touch sensitive display. A communication interface is optionally provided and is commonly used to communicate with a desktop computer. A replaceable or rechargeable battery powers the mobile device. Optionally, the mobile device can receive power from an external power source that overrides or recharges the built-in battery, such as a suitable AC or DC adapter, or a powered docking cradle.","In one common application, the mobile device is used in conjunction with the desktop computer. For example, the user of the mobile device may also have access to, and use, a desktop computer at work or at home. The user typically runs the same types of applications on both the desktop computer and on the mobile device. Thus, it is quite advantageous for the mobile device to be designed to be coupled to the desktop computer to exchange information with, and share information with, the mobile device.","As the mobile computing device market continues to grow, new developments can be expected. For example, mobile devices can be integrated with cellular or digital wireless communication technology to provide a mobile computing device which also functions as a mobile telephone. Thus, cellular or digital wireless communication technology can provide the communication link between the mobile device and the desktop (or other) computer. Further, speech recognition can be used to record data or to control functions of one or both of the mobile computing device and the desktop computer, with the user speaking into a microphone on the mobile device and with signals being transmitted to the desktop computer based upon the speech detected by the microphone.","Several problems arise when attempting to perform speech recognition, at the desktop computer, of words spoken into a remote microphone such as a microphone positioned on a mobile device. First, the signal-to-noise ratio of the speech signals provided by the microphone drops as the distance between the microphone and the user's mouth increases. With a typical mobile device being held in a user's palm up to a foot from the user's mouth, the resulting signal-to-noise ratio drop may be a significant speech recognition obstacle. Also, internal noise within the mobile device lowers the signal-to-noise ratio of the speech signals due to the close proximity of the internal noise to the microphone which is typically positioned on a housing of the mobile device. Second, due to bandwidth limitations of digital and other wireless communication networks, the speech signals received at the desktop computer will be of lower quality, as compared to speech signals from a desktop microphone. Thus, with different desktop and telephony bandwidths, speech recognition results will vary when using a mobile computing device microphone instead of a desktop microphone.","The aforementioned problems are not limited to speech recognition performed at a desktop computer. In many different speech recognition applications, it is very important, and can be critical, to have a clear and consistent audio input representing the speech to be recognized provided to the automatic speech recognition system. Two categories of noise which tend to corrupt the audio input to the speech recognition system are ambient noise and noise generated from background speech. There has been extensive work done in developing noise cancellation techniques in order to cancel ambient noise from the audio input. Some techniques are already commercially available in audio processing software, or integrated in digital microphones, such as universal serial bus (USB) microphones.","Dealing with noise related to background speech has been more problematic. This can arise in a variety of different, noisy environments. For example, where the speaker of interest is talking in a crowd, or among other people, a conventional microphone often picks up the speech of speakers other than the speaker of interest. Basically, in any environment in which other persons are talking, the audio signal generated from the speaker of interest can be compromised.","One prior solution for dealing with background speech is to provide an on\/off switch on the cord of a headset or on a handset. The on\/off switch has been referred to as a \u201cpush-to-talk\u201d button and the user is required to push the button prior to speaking. When the user pushes the button, it generates a button signal. The button signal indicates to the speech recognition system that the speaker of interest is speaking, or is about to speak. However, some usability studies have shown that this type of system is not satisfactory or desired by users. Thus, incorporating this type of feature in a mobile device may produce unsatisfactory results.","In addition, there has been work done in attempting to separate background speakers picked up by microphones from the speaker of interest (or foreground speaker). This has worked reasonably well in clean office environments, but has proven insufficient in highly noisy environments.","In yet another prior technique, a signal from a standard microphone has been combined with a signal from a throat microphone. The throat microphone registers laryngeal behavior indirectly by measuring the change in electrical impedance across the throat during speaking. The signal generated by the throat microphone was combined with the conventional microphone and models were generated that modeled the spectral content of the combined signals.","An algorithm was used to map the noisy, combined standard and throat microphone signal features to a clean standard microphone feature. This was estimated using probabilistic optimum filtering. However, while the throat microphone is quite immune to background noise, the spectral content of the throat microphone signal is quite limited. Therefore, using it to map to a clean estimated feature vector was not highly accurate. This technique is described in greater detail in Frankco et al., Presentation at the DARPA ROAR Workshop, Orlando, Fla. (2001). In addition, wearing a throat microphone is an added inconvenience to the user.","A mobile computing apparatus includes an antenna for transmission of information from the mobile computing apparatus. A first microphone, adapted to convert audible speech from the user into speech signals, is positioned at a distal end of the antenna. The antenna is rotatable into a position which directs the first microphone toward the mouth of the user. A speech sensor outputs a sensor signal which is indicative of whether the user is speaking, thus allowing the affects of background noise and speakers to be reduced.","In some embodiments of the invention, the antenna is rotatable to a position that, for a particular viewing angle and separation distance of the mobile apparatus relative to the user, minimizes the distance between the first microphone and the mouth of the user. Minimizing this distance increases the signal to noise ratio of the speech signals provided by the first microphone.","In some embodiments, the speech sensor outputs the sensor signal based on a non-audio input generated by speech actions of the user, such as movement of the user's mouth. The speech sensor can be positioned on the antenna, or elsewhere on the mobile computing device. A speech detector component outputs a speech detection signal indicative of whether the user is speaking based on the sensor signal.","The mobile computing device can be a cellular or digital wireless telephone. The mobile computing device can also be adapted to implement speech recognition processing of the speech signals.",{"@attributes":{"id":"p-0034","num":"0033"},"figref":["FIG. 1","FIG. 1"],"b":["10","10","12","14","14","10","12","10","10"]},"In an exemplary embodiment, mobile device  includes a microphone , an analog-to-digital (A\/D) converter  and speech recognition programs . In response to verbal commands, instructions or information from a user of device , microphone  provides speech signals which are digitized by A\/D converter . Speech recognition programs  perform feature extraction functions on the digitized speech signals to obtain intermediate speech recognition results. Using antenna , device  transmit the intermediate speech recognition results over wireless transport  to desktop computer  where additional speech recognition programs are used to complete the speech recognition process.","In other embodiments of the invention, intermediate speech recognition results are not transmitted to desktop computer , but instead programs  complete the speech recognition functions in mobile device . In yet other embodiments of the invention, mobile device  does not include speech recognition programs, and instead transmits the speech signals from microphone  over wireless transport  to desktop computer  or elsewhere. For example, in embodiments in which mobile device  functions as a mobile telephone, mobile device  can transmit the speech signals to other telephones.","In some embodiments, mobile device  includes one or more other application programs  and an object store . The application programs  can be, for example, a personal information manager (PIM) A that stores objects related to a user's electronic mail (e-mail) and scheduling or calendaring information. The application programs  can also include a content viewer B that is used to view information obtained from a wide-area network, such as the Internet. In one embodiment, the content viewer B is an \u201coffline\u201d viewer in that information is stored primarily before viewing, wherein the user does not interact with the source of information in real time. In other embodiments, mobile device  operates in a real time environment wherein the wireless transport  provides two-way communication. PIM A, content viewer B and object store  are not required in all embodiments of the invention.","In embodiments including PIM A, content viewer B and object store , the wireless transport  can also be used to send information to the mobile device  for storage in the object store  and for use by the application programs . The wireless transport  receives the information to be sent from an information source provider , which, for example, can be a source of news, weather, sports, traffic or local event information. Likewise, the information source provider  can receive e-mail and\/or scheduling information from the desktop computer  to be transmitted to the mobile device  through the wireless transport . The information from the desktop computer  can be supplied to the information source provider  through any suitable communication link, such as a direct modem connection. In another embodiment, the desktop computer  and the information source provider  can be connected together forming a local area network (LAN) or a wide area network (WAN). Such networking environments are commonplace in offices, enterprise-wide computer network Intranets and the Internet. If desired, the desktop computer  can also be directly connected to the wireless transport .","It is also worth noting that, in one embodiment, the mobile device  can be coupled to the desktop computer  using any suitable, and commercially available, communication link and using a suitable communications protocol. For instance, in one embodiment, the mobile device  communicates with the desktop computer  with a physical cable which communicates using a serial communications protocol. Other communication mechanisms include infra-red (IR) communication and direct modem communication.","It is also worth noting that the mobile device , in one embodiment, can be synchronized with the desktop computer . In that instance, properties of objects stored in object store  are similar to properties of other instances of the same objects stored in an object store on the desktop computer  or on the mobile device . Thus, for example, when one instance of an object stored in the object store on the desktop computer , the second instance of that object in the object store  of the mobile device  is updated the next time the mobile device  is connected to the desktop computer  so that both instances of the same object contain up-to-date data. This is commonly referred to as synchronization. In order to accomplish synchronization, synchronization components run on both the mobile device  and the desktop computer . The synchronization components communicate with one another through well defined interfaces to manage communication and synchronization.",{"@attributes":{"id":"p-0041","num":"0040"},"figref":["FIG. 2","FIG. 1","FIGS. 3\u20137"],"b":["10","10","20","22","24","26","27","11","10","28","10","17","17","20","15","24","28"]},"Memory  is implemented as non-volatile electronic memory such as random access memory (RAM) with a battery back-up module (not shown) such that information stored in memory  is not lost when the general power to the mobile device  is shut down. A portion of memory  is allocated as addressable memory for program execution, while the remaining portion of memory  can be used for storage, such as to simulate storage on a disk drive.","Memory  includes an operating system , the application programs  (such as PIM A and speech recognition programs  discussed with respect to ) and the object store . During operation, the operating system  is loaded into, and executed by, the processor  from memory . The operating system , in one embodiment, is a Windows CE brand operating system commercially available from Microsoft Corporation. The operating system  can be designed for mobile devices, and implements features which can be utilized by PIM A, content viewer B and speech recognition functions  through a set of exposed application programming interfaces and methods. The objects in object store  are maintained by PIM A, content viewer B and the operating system , at least partially in response to calls to the exposed application programming interfaces and methods.","The I\/O components , in one embodiment, are provided to facilitate input and output operations from the user of the mobile device . The desktop computer communication interface  is optionally provided as any suitable, and commercially available, communication interface. The interface  is used to communicate with the desktop computer  when wireless transceiver  is not used for that purpose.","The wireless transceiver  transmits speech signals or intermediate speech recognition results over wireless transport  using antenna . Wireless transceiver  can also transmit other data over wireless transport . In some embodiments, transceiver  receives information from desktop computer , the information source provider , or from other mobile or non-mobile devices or phones. The wireless transceiver  is coupled to the bus  for communication with the processor  and the object store  to store information received from the wireless transport .","A power supply  includes a battery  for powering the mobile device . Optionally, the mobile device  can receive power from an external power source  that overrides or recharges the built-in battery . For instance, the external power source  can include a suitable AC or DC adapter, or a power docking cradle for the mobile device .",{"@attributes":{"id":"p-0047","num":"0046"},"figref":["FIG. 3","FIG. 3"],"b":["10","11","17","10","32","34","36","85","86","34","36","36","34","32"]},"Microphone  is positioned on a distal end of antenna . Antenna  is in turn adapted to rotate toward the mouth of the user, thereby reducing the distance between the mouth of the user and microphone  while mobile device  is held in the palm of the user's hand. As noted above, reducing this distance helps to increase the signal-to-noise ratio of the speech signals provided by the microphone. Further, placement of microphone  at the tip of antenna  moves the microphone from the housing of mobile device . This reduces the effects of internal device noise on the signal-to-noise ratio.","In some embodiments, mobile device  also includes second microphone , which can be positioned on the housing of mobile device . Providing a second microphone  which is distanced from first microphone  enhances performance of the resulting microphone array when the two microphones are used together. In some embodiments, speaker  is included to allow mobile device  to be used as a mobile telephone.",{"@attributes":{"id":"p-0050","num":"0049"},"figref":["FIG. 4","FIG. 4","FIG. 3","FIG. 4","FIG. 4"],"b":["10","10","10","17","11","86","17","11","17","11"]},"Mobile device  also includes touch sensitive display  which can be used, in conjunction with the stylus , to accomplish certain user input functions. It should be noted that the display  for the mobile devices shown in  can be the same size, or of different sizes, but will typically be much smaller than a conventional display used with a desktop computer. For example, the displays  shown in  may be defined by a matrix of only 240\u00d7320 coordinates, or 160\u00d7160 coordinates, or any other suitable size.","The mobile device  shown in  also includes a number of user input keys or buttons (such as scroll buttons  and\/or keyboard ) which allow the user to enter data or to scroll through menu options or other display options which are displayed on display , without contacting the display . In addition, the mobile device  shown in  also includes a power button  which can be used to turn on and off the general power to the mobile device .","It should also be noted that in the embodiment illustrated in , the mobile device  includes a hand writing area . Hand writing area  can be used in conjunction with the stylus  such that the user can write messages which are stored in memory  for later use by the mobile device . In one embodiment, the hand written messages are simply stored in hand written form and can be recalled by the user and displayed on the display  such that the user can review the hand written messages entered into the mobile device . In another embodiment, the mobile device  is provided with a character recognition module such that the user can enter alpha-numeric information into the mobile device  by writing that alpha-numeric information on the area  with the stylus . In that instance, the character recognition module in the mobile device  recognizes the alpha-numeric characters and converts the characters into computer recognizable alpha-numeric characters which can be used by the application programs  in the mobile device .",{"@attributes":{"id":"p-0054","num":"0053"},"figref":["FIGS. 5 and 6","FIGS. 5 and 6"],"b":["10","17","11","10","85","86","10","90","11","88","17","11","10","90","11","10","90","17","10","10","10"]},{"@attributes":{"id":"p-0055","num":"0054"},"figref":["FIGS. 7 and 8","FIGS. 7 and 8","FIG. 7"],"b":["10","10","34","36","36","34","11","11","90","10","90","34"]},"The embodiment of mobile device  illustrated in  differs from the embodiment shown in  in that speaker  is positioned at the opposite end of the device. Thus, when mobile device  is to be used in a more conventional cordless telephone mode of operation in which display  is not viewable during use, device  can be turned upside-down. This mode of operation is illustrated in . With device  positioned upside down, microphone  can be positioned very close to the mouth of the user with little or no rotation of antenna . At the same time, speaker  can be positioned against the ear of the user. Of course, in this configuration, antenna  faces downward instead of upward as in conventional wireless telephones.",{"@attributes":{"id":"p-0057","num":"0056"},"figref":["FIG. 9","FIG. 9","FIG. 4","FIG. 4","FIG. 9","FIG. 9"],"b":["10","10","10","17","11","86","11","17","17","10","86","10","86","17"]},{"@attributes":{"id":"p-0058","num":"0057"},"figref":"FIG. 10","b":["100","100","10","110","11","11","17","110","90","90"]},{"@attributes":{"id":"p-0059","num":"0058"},"figref":["FIG. 11","FIG. 11","FIGS. 12\u201317"],"b":["100","110","111","110"]},{"@attributes":{"id":"p-0060","num":"0059"},"figref":["FIG. 12","FIG. 1"],"b":["100","100","10","110","120","110"]},"For example, in one embodiment, the transducer is an infrared sensor that is generally aimed at the user's face, notably to the mouth region, and generates a signal indicative of a change in facial movement of the user that corresponds to speech. In another embodiment, the sensor includes a plurality of infrared emitters and sensors aimed at different portions of the user's face.","In still other embodiments, the speech sensor  can include a temperature sensor, such as a thermistor, placed in the breath stream of the user, for example by inclusion of speech sensor  on antenna  near microphone  as shown in . As the user speaks, the exhaled breath causes a change in temperature in the sensor and thus detecting speech. This can be enhanced by passing a small steady state current through the thermistor, heating it slightly above ambient temperature. The breath stream would then tend to cool the thermistor which can be sensed by a change in voltage across the thermistor. In any case, speech sensor  is illustratively highly insensitive to background speech, but strongly indicative of whether the user is speaking. Other types of speech sensors, such as throat microphones, bone vibration sensitive microphones, etc., can also be used.","In the embodiment illustrated in , speech sensor  provides an output indicative of whether the user of mobile device  is speaking. Speech detection module , which like speech recognition program or module  can be executed on processor  as shown in , receives the output of speech sensor  and determines whether the user of mobile device  is speaking. In this embodiment, if it is determined that the user of mobile device  is speaking, speech detection module  generates a microphone control signal to enable microphone . Thus, when the user of mobile device  is speaking, the speech will be provided to speech recognition module  via microphone  and A\/D converter . When speech detection module  determines that the user of mobile device  is not speaking, the microphone control signal disables the microphone , thus preventing sound data from noise or background speakers from being processed by speech recognition module .",{"@attributes":{"id":"p-0064","num":"0063"},"figref":["FIG. 13","FIG. 13","FIG. 12","FIG. 13"],"b":["10","100","100","120","17","120","19","120","100","19","120","100","19"]},"The mobile device block diagrams illustrated in  represent examples of implementation embodiments of the present invention. Many other embodiments, employing other speech detection configurations, can also be used. Referring now to , shown are block diagrams of alternate speech detection systems which can be used in accordance with other embodiments of the mobile device of the present invention.",{"@attributes":{"id":"p-0066","num":"0065"},"figref":"FIG. 14","b":["300","300","301","110","303","17","302","304","20"]},"Capture component  captures signals from conventional microphone  in the form of an audio signal. Component  also captures an input signal from speech transducer  which is indicative of whether a user is speaking. The signal generated from this transducer can be generated from a wide variety of other transducers. For example, in one embodiment, the transducer is an infrared sensor that is generally aimed at the user's face, notably the mouth region, and generates a signal indicative of a change in facial movement of the user that corresponds to speech. In another embodiment, the sensor includes a plurality of infrared emitters and sensors aimed at different portions of the user's face. In still other embodiments, the speech sensor or sensors  can include a throat microphone which measures the impedance across the user's throat or throat vibration. In still other embodiments, the sensor is a bone vibration sensitive microphone which is located adjacent a facial or skull bone of the user (such as the jaw bone) and senses vibrations that correspond to speech generated by the user. This type of sensor can also be placed in contact with the throat, or adjacent to, or within, the user's ear. In another embodiment, a temperature sensor such as a thermistor is placed in the breath stream such as on the same support that holds the regular microphone. As the user speaks, the exhaled breath causes a change in temperature in the sensor and thus detecting speech. This can be enhanced by passing a small steady state current through the thermistor, heating it slightly above ambient temperature. The breath stream would then tend to cool the thermistor which can be sensed by a change in voltage across the thermistor. In any case, the transducer  is illustratively highly insensitive to background speech but strongly indicative of whether the user is speaking.","In one embodiment, component  captures the signals from the transducers  and the microphone  and converts them into digital form, as a synchronized time series of signal samples. Component  then provides one or more outputs to multi-sensory signal processor . Processor  processes the input signals captured by component  and provides, at its output, speech detection signal  which is indicative of whether the user is speaking. Processor  can also optionally output additional signals , such as an audio output signal, or such as speech detection signals that indicate a likelihood or probability that the user is speaking based on signals from a variety of different transducers. Other outputs  will illustratively vary based on the task to be performed. However, in one embodiment, outputs  include an enhanced audio signal that is used in a speech recognition system (such as speech recognition module ).",{"@attributes":{"id":"p-0069","num":"0068"},"figref":["FIG. 15","FIG. 15","FIG. 15"],"b":["304","304","301"]},"In any case,  shows that processor  includes infrared (IR)-based speech detector , audio-based speech detector , and combined speech detection component . IR-based speech detector  receives the IR signal emitted by an IR emitter and reflected off the speaker and detects whether the user is speaking based on the IR signal. Audio-based speech detector  receives the audio signal and detects whether the user is speaking based on the audio signal. The output from detectors  and  are provided to combined speech detection component . Component  receives the signals and makes an overall estimation as to whether the user is speaking based on the two input signals. The output from component  comprises the speech detection signal . In one embodiment, speech detection signal  is provided to background speech removal component . Speech detection signal  is used to indicate when, in the audio signal, the user is actually speaking.","More specifically, the two independent detectors  and , in one embodiment, each generate a probabilistic description of how likely it is that the user is talking. In one embodiment, the output of IR-based speech detector  is a probability that the user is speaking, based on the IR-input signal. Similarly, the output signal from audio-based speech detector  is a probability that the user is speaking based on the audio input signal. These two signals are then considered in component  to make, in one example, a binary decision as to whether the user is speaking.","Signal  can be used to further process the audio signal in component  to remove background speech. In one embodiment, signal  is simply used to provide the speech signal to the speech recognition engine through component  when speech detection signal  indicates that the user is speaking. If speech detection signal  indicates that the user is not speaking, then the speech signal is not provided through component  to the speech recognition engine.","In another embodiment, component  provides speech detection signal  as a probability measure indicative of a probability that the user is speaking. In that embodiment, the audio signal is multiplied in component  by the probability embodied in speech detection signal . Therefore, when the probability that the user is speaking is high, the speech signal provided to the speech recognition engine through component  also has a large magnitude. However, when the probability that the user is speaking is low, the speech signal provided to the speech recognition engine through component  has a very low magnitude. Of course, in another embodiment, the speech detection signal  can simply be provided directly to the speech recognition engine which, itself, can determine whether the user is speaking and how to process the speech signal based on that determination.",{"@attributes":{"id":"p-0074","num":"0073"},"figref":["FIG. 16","FIG. 16"],"b":["304","304","320","320","320","320","306"]},"Regardless of which type of system is used (the system shown in  or that shown in ) the speech detectors can be generated and trained using training data in which a noisy audio signal is provided, along with the IR signal, and also along with a manual indication (such as a push-to-talk signal) that indicates specifically whether the user is speaking.","To better describe this,  shows a plot of an audio signal  and an infrared signal , in terms of magnitude versus time.  also shows speech detection signal  that indicates when the user is speaking. When in a logical high state, signal  is indicative of a decision by the speech detector that the speaker is speaking. When in a logical low state, signal  indicates that the user is not speaking. In order to determine whether a user is speaking and generate signal , based on signals  and , the mean and variance of the signals  and  are computed periodically, such as every 100 milliseconds. The mean and variance computations are used as baseline mean and variance values against which speech detection decisions are made. It can be seen that both the audio signal  and infrared signal  have a larger variance when the user is speaking, than when the user is not speaking. Therefore, when observations are processed, such as every 5\u201310 milliseconds, the mean and variance (or just the variance) of the signal during the observation is compared to the baseline mean and variance (or just the baseline variance). If the observed values are larger than the baseline values, then it is determined that the user is speaking. If not, then it is determined that the user is not speaking. In one illustrative embodiment, the speech detection determination is made based on whether the observed values exceed the baseline values by a predetermined threshold. For example, during each observation, if the infrared signal is not within three standard deviations of the baseline mean, it is considered that the user is speaking. The same can be used for the audio signal.","In accordance with another embodiment of the present invention, the detectors , ,  or  can also adapt during use, such as to accommodate for changes in ambient light conditions, or such as for changes in the head position of the user, which may cause slight changes in lighting that affect the IR signal. The baseline mean and variance values can be re-estimated every 5\u201310 seconds, for example, or using another revolving time window. This allows those values to be updated to reflect changes over time. Also, before the baseline mean and variance are updated using the moving window, it can first be determined whether the input signals correspond to the user speaking or not speaking. The mean and variance can be recalculated using only portions of the signal that correspond to the user not speaking","In addition, from , it can be seen that the IR signal may generally precede the audio signal. This is because the user may, in general, change mouth or face positions prior to producing any sound. Therefore, this allows the system to detect speech even before the speech signal is available.","Although the present invention has been described with reference to various embodiments, workers skilled in the art will recognize that changes may be made in form and detail without departing from the spirit and scope of the invention."],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF ILLUSTRATIVE EMBODIMENTS"],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":["FIG. 2","FIG. 1"]},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIGS. 5 and 6"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIGS. 7 and 8"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0032","num":"0031"},"figref":["FIGS. 15 and 16","FIG. 14"]},{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 17"}]},"DETDESC":[{},{}]}
