---
title: Application programming interface for a multi-pointer indirect touch input device
abstract: To allow a computer platform to provide a consistent interface for applications to use information from multi-point indirect touch input devices, an application programming interface is provided to a software interface layer that manages interaction of the system with a variety of instantiations of multi-pointer indirect touch input devices.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09389679&OS=09389679&RS=09389679
owner: Microsoft Technology Licensing, LLC
number: 09389679
owner_city: Redmond
owner_country: US
publication_date: 20111130
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["Manual input devices used for navigation and spatial control of a computing system have a significant impact on capabilities of the computer system and a user's overall experience. There are several kinds of manual input devices. The most common of these for personal computers include single-pointer, indirect interaction devices, such as a mouse or trackpad, and direct interaction devices, such as touchscreens.","A single-pointer, indirect interaction device sensor detects user interaction with the sensor and maps this interaction to a position on a display. One method of mapping points of input to the display involves one-to-one mapping of sensor extents to the extents of the display, which is called absolute mapping. Examples of devices that employ absolute mapping are pen and touch digitizers. Another method involves mapping device sensor coordinates to a movable subportion of the display, which is called relative mapping.","Examples of devices that employ relative mapping are the mouse and devices that emulate the mouse, such as a trackpad. A mouse senses movement, which displaces a presumed starting position by a distance based on the sensed interaction with the device. A trackpad is commonly used in a manner similar to a mouse. The motion of a contact on the trackpad is sensed, and the sensed motion is treated in a manner similar to a mouse input.","A direct interaction device allows interaction with a device that is visually aligned with a display. A direct interaction device maps between positions on a touch sensitive surface and positions on a display of the same size, using an absolute mapping. For example, when a user touches a point on a touchscreen, an input event may trigger an application response, such as command actuation, in the user interface at a position corresponding to the point on the display touched by the user.","Absolute and relative mapping of spatial input from a multi-pointer input device to a display have selective advantages and disadvantages, depending on physical attributes of the input and display devices, the capabilities of the system, the nature and layout of the application user interface, the type of the task the user is performing, and various ergonomic factors.","Most input devices have buttons in addition to their position information. For example, dual-state mechanical buttons are common on mice. Pen digitizers also typically have some pressure-responsive device in the tip. Most software drivers for pen digitizers implement a form of mouse emulation in process tip-related data. Also, buttons and other mechanisms generally are processed as independent inputs of the input device.","Input devices can be made in variety of shapes and sizes, can have different resolutions from each other, and can provide a variety of data as input to the computer.","This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter.","To allow a computer platform to provide a consistent interface for applications to use information from multi-point indirect touch input devices, an application programming interface is provided to a software interface layer that manages interaction of the system with a variety of instantiations of multi-pointer indirect touch input devices.","Such a host service can provide capabilities such as discovery and binding of touch controller class devices upon physical attachment to the host system; software driver and hardware-only interfaces for communicating device capabilities, touch data, and device mode changes between the controller and the host system; transformation of stateless input events emitted by touch controllers into one or more user or application-selectable touch interaction models; global management of touch controller device, device connection, and user interaction state; enforcement of base system requirements for touch device controllers; common interface for user device configuration settings; support for device-specific, value-add interaction models; management of host and device security contexts; deployment and servicing model for the touch controller host.","In the following description, reference is made to the accompanying drawings which form a part hereof, and in which are shown, by way of illustration, specific example implementations of this technique. It is understood that other embodiments may be utilized and structural changes may be made without departing from the scope of the disclosure.","The following section provides an example operating environment in which an indirect touch input device can be used. After describing an example implementation of the use of a multi-pointer indirect touch device in connection with , an example software architecture will then be described in connection with .","Referring to , a computer system  includes a multi-pointer, indirect input device , having a sensor, connected to a computer platform  (details of an example of which are described below). Such a computer system may be a personal computer, home entertainment system, a projector, a kiosk application, compact personal electronics, or the like. The computer platform has an operating system which manages interaction between one or more applications  and resources of the computer platform , such as its peripheral devices including the multipointer indirect input device.","Within the operating system, data describing multiple sensed input points  is received from the sensor of the multipointer indirect input device . These input points are processed to map them to points on a display .","This mapping process involves determine an initial mapping of the device coordinate system to the display coordinate system, which can be relative or absolute, and then a mapping of each point from the device coordinate system to the display coordinate system. Such initial mapping occurs at the beginning of each input session.","An input session is from a point in time a first input is detected by the sensor to a point in time a last input is removed from the sensor. During an input session, the input points are likely to move. The input points are mapped to from their new locations in the sensor coordinate system to corresponding new locations in the display coordinate system. This movement mapping can take into consideration issues such as bounding and acceleration.","As shown in , the multiple points  are input to a viewport selection module  at the beginning of an input session. The viewport selection module provides, as its output, a viewport size and location  in the display coordinate system. The viewport defines a region in the display coordinate space to which the sensor coordinate space is mapped. In a configuration in which multiple device sensors are connected to the system, each sensor has its own viewport. The viewport may have a shape that corresponds to the shape of the input device sensor. In some implementations however, the viewport may have a different aspect ratio or orientation from the sensor, or even a different shape. For example, an ellipsoid sensor may be mapped to a rectangular viewport. The viewport's shape is typically defined by the host system, but may also be defined by the device or the user. The viewport's size and position are computed when user inputs are detected by the sensor. When no user inputs are detected by the sensor, the size and position of the viewport are undefined. The viewport is typically not displayed to the user. Together, the viewport's shape, size and position represent the mapping of the sensor coordinate system to the display coordinate system. Settings  determine how this mapping is done, such as by relative or absolute mapping, examples of which are described in more detail below.","The multiple points  also are input to an input mapping module  throughout an input session. The input mapping module provides, as its output, multiple points  in the display coordinate system. Settings  determine how each point is mapped, such as by determining device and display reference locations for interpreting relative input positions, applying input motion acceleration, span adjustments, and bounding conditions, examples of which are described in more detail below.","Given the multiple points mapped to a display coordinate system, the multiple points  can be displayed on the display. Each point can be treated by the operating system  and\/or applications  in a manner similar to any single point, e.g., for selection of a displayed item, or in a manner similar to multiple points from direct touch input sensors, e.g., effecting zoom, rotation or movement of an element in the host system user interface. The range of possible uses of the multiple points, once mapped to the display, is not limiting of this invention.","Given this context, an example implementation of the mapping of multiple points to a display will now be described in more detail in connection with .","In , a flowchart describes an example implementation of how the viewport size and location can be selected by the viewport selection module, and how points can be subsequently mapped.","It should be noted that the following implementation is based upon certain design decisions about a desirable user experience. For example, it is assumed that the relative position of each physical input with respect to other physical inputs is retained upon projection to the display. It is also assumed that distances between all inputs are scaled symmetrically.","Another aspect of the user experience is the kind of mapping between the input device and the display. The mapping can be relative or absolute, and can be independent for each axis. For example, a relative mapping can be applied to the y axis, with an absolute mapping applied to the x axis, or vice versa. Also, both axes can use different relative mappings. The mapping also can be based on logical coordinates or physical dimensions of the input device and the display. If the mapping is based on the physical dimensions of the devices, spatial accuracy is improved, providing a more intuitive and cognitively efficient interface. These decisions about the kind of mapping can be optional settings in the system.","Another aspect of the user experience is a bounding policy. In particular, device inputs can be subject to a display bounding policy for the system. For example, all device inputs can be forced to remain within the display, or only one device input from the set can be forced to remain within the display. Another implementation does not use any bounding policy. These decisions about bounding policy can be optional settings in the system.","The viewport size and location is determined at the beginning of each input session. The start of an input session is detected , for example, when one or more input points are detected by the sensor after a period of no user input. The viewport dimensions in each axis may defined by the input device, the host system, or the user. The dimensions can be expressed as either a percentage of the target display device or in physical units of distance. For physical units of distance to be used, the physical and logical (coordinate) extents of both the input sensor and display are provided, by, for example, the device, user input or other means. A position of an output locator, in the display coordinate space, is then retrieved 201. In this implementation, the output locator position is global to the user session (which begins when a user logs in and ends when the user logs off). The output locator position is shared among and updated by multiple single- and multi-pointer input devices connected to the system. The output locator can be a position saved from a previous input session. If there was no previous input session, then the center of the display device, the last position of a mouse or other device, or an alternative default display location can be used as the output locator position.","Next, given known parameters, i.e., coordinates and bounds, of the display device and input device, scaling factors for each axis are determined . These parameters typically are stored in memory. In the case of display device, parameters can be retrieved using system API's. In the case of the input device, parameters can be retrieved via device interrogation. Given the coordinates and bounds of the display and input devices, the scaling factors are determined. If an absolute mapping is used, computations based on physical extents are not necessary, and the x and y axis scale factors are based on a one-to-one ratio of the device and display coordinate extents. If a relative mapping is used, then the x and y axis scale factors are determined by the ratios of the device dimensions to the viewport dimensions in display coordinates. The scale factors can be computed once, stored in memory and retrieved when needed.","The viewport extent, i.e., x and y coordinates of its vertices, in the display coordinate space is determined  using the determined scale factors. The viewport extent is initially determined for an input session using the saved output locator, before a new output locator is computed as follows.","For a scaled viewport using the pixel density of the display, the scale factors Sare non-zero, positive values between 0 and 1, and the extent of the viewport",{"@attributes":{"id":"p-0038","num":"0037"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mi":"Rv","mo":"=","mrow":{"mo":["{","}"],"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":{"msub":{"mi":"L","mrow":{"mi":["V","x"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mn":"0"}},"mo":"-","mrow":{"msub":{"mi":["S","Vx"]},"mo":"\/","mrow":{"mo":["[","]"],"mrow":{"mn":"2","mo":"*","mrow":{"mi":"extent","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["R","Dx"]}}}}}}},"mo":","}}},{"mtd":{"mrow":{"mrow":{"msub":{"mi":"L","mrow":{"mi":["V","x"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mn":"0"}},"mo":"+","mrow":{"msub":{"mi":["S","Vx"]},"mo":"\/","mrow":{"mo":["[","]"],"mrow":{"mn":"2","mo":"*","mrow":{"mi":"extent","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["R","Dx"]}}}}}}},"mo":","}}},{"mtd":{"mrow":{"mrow":{"msub":{"mi":"L","mrow":{"mi":["V","y"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mn":"0"}},"mo":"-","mrow":{"msub":{"mi":["S","Vy"]},"mo":"\/","mrow":{"mo":["[","]"],"mrow":{"mn":"2","mo":"*","mrow":{"mi":"extent","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["R","Dy"]}}}}}}},"mo":","}}},{"mtd":{"mrow":{"msub":{"mi":"L","mrow":{"mi":["V","y"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mn":"0"}},"mo":"+","mrow":{"msub":{"mi":["S","Vy"]},"mo":"\/","mrow":{"mo":["[","]"],"mrow":{"mn":"2","mo":"*","mrow":{"mi":"extent","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["R","Dy"]}}}}}}}}}]}}},"mo":","}}},"br":{},"sub":["V0 ","V ","D"]},"For a viewport using physical dimensions, where the desired size Sis a non-zero, positive value no greater than the physical extent of the target display, and the pixel density D of the display is known via hardware interrogation, the extent of the viewport",{"@attributes":{"id":"p-0040","num":"0039"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"msub":{"mi":["R","v"]},"mo":"=","mrow":{"mo":["{","}"],"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":{"mrow":{"mrow":{"mo":["[","]"],"mrow":{"msub":{"mi":"L","mrow":{"mi":["V","x"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mn":"0"}},"mo":"-","mrow":{"msub":{"mi":["S","Vx"]},"mo":"\/","mrow":{"mo":["[","]"],"mrow":{"mn":"2","mo":"*","mrow":{"mi":"extent","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["R","Dx"]}}}}}}}},"mo":"*","msub":{"mi":["D","x"]}},"mo":"+","msub":{"mi":"R","mrow":{"mi":["Dx","left"],"mo":"."}}},"mo":","}}},{"mtd":{"mrow":{"mo":"[","mrow":{"mrow":{"msub":[{"mi":"L","mrow":{"mi":["V","x"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mn":"0"}},{"mi":"R","mrow":{"mi":["Dx","left"],"mo":"."}}],"mo":["+","+"],"mrow":{"mrow":{"mo":["[","]"],"mrow":{"msub":{"mi":["S","Vx"]},"mo":"\/","mrow":{"mo":["[","]"],"mrow":{"mn":"2","mo":"*","mrow":{"mi":"extent","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["R","Dx"]}}}}}}},"mo":"*","msub":{"mi":["D","x"]}}},"mo":","}}}},{"mtd":{"mrow":{"mo":"[","mrow":{"mrow":{"msub":[{"mi":"L","mrow":{"mi":["V","y"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mn":"0"}},{"mi":"R","mrow":{"mi":["Dy","top"],"mo":"."}}],"mo":["-","+"],"mrow":{"mrow":{"mo":["[","]"],"mrow":{"msub":{"mi":["S","Vy"]},"mo":"\/","mrow":{"mo":["[","]"],"mrow":{"mn":"2","mo":"*","mrow":{"mi":"extent","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["R","Dy"]}}}}}}},"mo":"*","msub":{"mi":["D","y"]}}},"mo":","}}}},{"mtd":{"mrow":{"mrow":{"mrow":{"mo":["[","]"],"mrow":{"msub":{"mi":"L","mrow":{"mi":["V","y"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mn":"0"}},"mo":"+","mrow":{"msub":{"mi":["S","Vy"]},"mo":"\/","mrow":{"mo":["[","]"],"mrow":{"mn":"2","mo":"*","mrow":{"mi":"extent","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["R","Dy"]}}}}}}}},"mo":"*","msub":{"mi":["D","y"]}},"mo":"+","msub":{"mi":"R","mrow":{"mi":["Dy","top"],"mo":"."}}}}}]}}},"mo":","}}}},"Given an initial extent of the viewport, a sensor locator is then determined , initially in the device coordinate system. There are many ways to select the sensor locator, and the particular way chosen is dependent on the desired user interaction. For example, if there is a single input detected by the sensor, the sensor locator can be the coordinates of this single input. If there are multiple inputs, then the sensor locator can be the position of a single \u201cprimary\u201d input, or a point having a particular relationship with other inputs, such as the geometric center of all inputs. The sensor locator is undefined when no input points are detected and is not persisted between input sessions.","When the position of a primary input is used as the sensor locator, one of a variety of methods can be used to select and assign primary status to the input. In general, the \u201cprimary\u201d input is an input point chosen from among the others by any method. For example, the primary input can be the first input, or the last input, detected in the session. This method carries the drawback of forcing an arbitrary selection in the case of multiple inputs arriving simultaneously. A solution is for the primary input to selected by a form of geometric ordering, such as the highest order input according to a geometric sorting formula (which can be interaction dependent). For example, a sorting formula can sort angles formed by each input point with respect to an origin at the geometric center of all inputs and a reference point. The reference point can be, for example, a vertical line with angle measured based on left or right handedness of a user.","Regardless of method, sensor locator determination may be affected by the time of arrival and departure of inputs. To protect against the condition in which the user intends to arrive or depart multiple inputs simultaneously but instead arrives or departs them at slightly different times, a small time window (e.g., 10-60 ms) can be used to delay sensor locator calculation.","Next, the sensor locator position is mapped  from device coordinates to display coordinates. The result is a new output locator position for the frame. This position can be computed by [L\/extent(R)*extent(R)]+R, where Lis the x or y coordinate of the sensor locator, extent(R) is the width or height of the sensor coordinate space, and extent (R) is the width or height of the viewport, and Ris the width or height of the initial viewport. This new output locator is constrained to be within the bounds of the display.","Given the new output locator, the viewport is then positioned in the display coordinate space by obtaining  the viewport locator. For the first frame of the session, the viewport position is determined; in subsequent frames it is retrieved from memory. The position of the viewport is determined logically, meaning that it is optional whether to display the viewport. In fact, in most implementations it is likely to be preferable not to actually display the viewport.","As noted above, the viewport is a projection of the input sensor coordinate space on the display, and the viewport locator position is the geometric center of the viewport, in display coordinates. As also noted above, unlike the output locator, the viewport is undefined when no inputs are detected by the sensor. It is associated with a particular device instance (rather than global to the user session), and its position is updated when the user initially places inputs on the sensor. After an input session starts, and until the input session ends, the viewport remains stationary between frames. If a frame represents a continuance of an input session (lists of input points from both previous and current frames are not empty), then the viewport locator is retrieved from memory. If the frame initiates a new input session, then the viewport locator is obtained by determining an offset between a sensor locator (determined in step ) and the output locator position (determined in step ), as follows.\n\n\u0394\n\n\/extent()*extent()]+\n","Lis then constrained to the bounds of the target display and the extent of the viewport, determined above, is recalculated using the new viewport locator.","After computing the sensor locator, viewport locator, and output locator for a frame, the sensor inputs for that frame are then mapped  to display coordinates, in a manner described in more detail below. If the input session ends, as determined at , then some information about the input session can be saved  (such as the last output locator). If the input session has not ended, and if updated sensor input positions are received (as determined at ), then the process repeats with determining the sensor locator  for the frame, through mapping  these new sensor inputs to the display. However, where the frame is part of a continuing session, the viewport locator is not determined in step , but is retrieved from memory.",{"@attributes":{"id":"p-0049","num":"0048"},"figref":["FIG. 3","FIG. 3"]},"The system receives  a list of input points from the device, each having coordinates in the device coordinate space. Next, the input points are mapped  to their corresponding points in display coordinate space. For example, the coordinates Cin display coordinate space of a point Cin device coordinate space can be computed by [C\/extent (R)*extent (R)]+R.","A bounding box containing the input points is defined . The corners of the bounding box are mapped to and compared  to the visual extent of the display. If none of the corners of the bounding box is outside of the visual area of the display, then the input mapping is retained . Otherwise, an offset to move the bounding box to be within the visual extent of the display is determined . In computing the minimal remedial offset, a displacement vector between previous and current frames of each non-conforming corner of the input bounding box or the individual input define a path and its point of intersection with the visible display boundary. The remedial offset is the displacement between the path's origin and the point of intersection. This offset is applied  to the points to re-map them to new positions within the visual area of the display.","In another implementation, the points are constrained so that at least one input point from the device remains displayed. In , the system receives  a list of input points from the device, each having coordinates in the device coordinate space. Next, the input points are mapped  to their corresponding points in display coordinate space. A bounding box containing the input points is defined . The corners of the bounding box are then compared  to the visual extent of the display. If at least one of the corners of the bounding box remains in the visual area of the display, then the input mapping is retained . Otherwise, a remedial offset to move at least one point of the bounding box to be within the visual extent of the display is determined . Next, an offset of the input nearest the contained corner is determined and applied  to remedial offset. This updated remedial offset is applied  to the points to re-map them to new positions within the visual area of the display.","For multiple monitor displays, the process is similar. There are regular display topologies, where the union of visible areas of the display is a single, rectangular, \u201cvirtual\u201d display with no internal voids. For regular display topologies, bounding of multiple inputs to the boundaries of the virtual display surface is identical to that for a single display. There also can be irregular display topologies, where the union of visible areas of the display is a rectilinear virtual display with convex or concave internal voids. For these display topologies, the foregoing methods can be used to compute and apply remedial offsets.","However, an additional failure case is where a point lies in one of the convex or concave internal voids, a bounding box containing only those points outside of the visual area of the display can be computed, and used to compute a remedial offset. In this case, a bounding box is computed to contain input points that do not map to visible areas of the display, herein called a non-conforming bounding box. A minimal remedial offset is computed by which to ensure at least one corner of a non-conforming bounding box is contained within the visible portion of the display. This remedial offset is applied to the device-to-display transform for all inputs.","A more specific example implementation of boundary conditions for multiple monitors will now be described.","In this example, for each input, a target bounding display (R) is determined in the following way. First, it is determined if the input position Cis not contained within a visible region of the virtual display surface. If it is not, then the coordinates of display Rof the input for the previous frame are retrieved. For a frame representing a new session, these coordinates are substituted with those of the display containing the output locator position L. Next, it is determined whether the input Cremains bounded by Rin either the x or y axis. If a positive test is observed in either axis, then the target bounding display is the display R. Otherwise, the input is out of the bounds of display R. A displacement vector \u0394Sin sensor coordinates is then determined for this input: \u0394S=C\u2014C. The extent of the sensor, extent(R), is retrieved. The dominant axis of displacement is determined. The X axis dominates if |\u0394S\/extent(R)|>=|\u0394S\/extent(R)|. Otherwise the Y axis dominates.","The dominant axis of input displacement is then used to determine a target bounding display. If the X axis is dominant, then the target bounding display Ris the display that satisfies the following conditions: 1. the input falls in the horizontal range of the display; 2. the target display is in the primary moving direction of the input and shares that boundary with the last display; and 3. the last input position falls into the vertical range of the display. If the Y axis is dominant, then the target bounding display Rsatisfies the following conditions: 1. the input falls in the vertical range of the display; 2. the target display is in the primary moving direction of the input and shares that boundary with the last display; and 3. the last input position falls into the horizontal range of the display.","If the target bounding display cannot be determined using the dominant direction, then a search is performed in the non-dominant direction. If the target bounding display is still not found, the target display is the input's previous display.","Given the target bounding display for an input, the input is clamped to that display, and the clamping offset is calculated and stored. This clamping offset is applied to all inputs so that the relative distance among them is maintained. After adjusting the inputs in this manner, they are all tested again to ensure they are on the visible part of the display.","In some interaction modes, a small amount of time is allowed to realize the user's intention to make multiple inputs with the sensor simultaneously. When the first input of a session is observed, a timer is activated and arriving inputs are marked inactive, and sensor locator determination is deferred until timer expiration, or terminated if arriving inputs are removed. Likewise, the user may intend to depart inputs simultaneously. To realize this intention without affecting sensor locator position, a timer can be used. The timer is activated, and departing inputs continue to be included in sensor locator computations until the timer expires.","In the foregoing description, in both relative and absolute mapping modes, the input points are mapped directly to display coordinates. In a relative mapping mode, however, the input device can span only a subset of the target display coordinate space. Therefore, navigation from one display location to another location can involve multiple strokes, unless some form of acceleration of the points is applied as movement of the input points is detected. Conversely, in order to achieve pixel-level, point-to-point targeting precision, a form of deceleration of the points can be applied. Such acceleration and deceleration, sometimes referred to as \u201cpointer ballistics,\u201d can be applied to a multiple input, indirect input device in the following manner. The displacement of input points on the input device is taken into consideration in the mapping of the input points from the device coordinate space to the display coordinate space, to accelerate or decelerate, as the case may be, movement of the points on the display. In general, a measure of displacement for the input points is determined. This displacement is the input to a function that determines, based on the displacement, how to alter the mapping of the input device points to their corresponding display coordinates.","In one implementation, the displacement of each input point is determined. The physical displacement in sensor pixels of the input with the lowest magnitude displacement vector is passed through an acceleration curve transform to produce a single accelerated display displacement, and this is applied to the display displacements of the output locator and all points. The inputs to the acceleration function can be either the vector magnitude or a value for each axis can be input to two different acceleration functions. This implementation will now be described in connection with .","First, input points on an input sensor, from first and second points in time, are received 500. Note that how to uniquely identify and track moving or stationary inputs, known in the art as \u201cinput recognition and tracking,\u201d is device and sensor specific. The invention is not limited to any specific input recognition and tracking technique. Any technique for such recognition and tracking that has been found to be suitable in the art can be used.","The displacement in device coordinates (i.e., pixels) in each dimension of each input within a time interval is then determined . If the time intervals are known to be constant, then the displacement alone can be used. Otherwise the time interval can be used to compute velocity.","For each time interval, or \u201cframe,\u201d of inputs, the input with the smallest magnitude displacement, or velocity, is identified . The input with the lowest magnitude is chosen (rather than the average or maximum, for example) so that inputs held stationary on the input sensor remain stationary when mapped to the display.","The displacement of the identified input can be converted from a displacement in pixels to a physical displacement, using the pixel density of the sensor. The displacement value is used as an input to an acceleration function to transform  the value to an accelerated displacement. The invention is not limited by the specific acceleration formula used. Any reasonable technique currently used in the art, such as used for mouse pointer acceleration, can be used. The invention generally can be applied to any acceleration formula that permits independent acceleration of each coordinate axis (x, y or z). A suitable transform can be implemented using a piece-wise linear function that maps a displacement value to an accelerated displacement value. The accelerated displacement value, if based on physical dimensions can be converted back to pixel coordinates.","The accelerated displacement is then converted  to an accelerated displacement in display coordinate space. For example, the conversion can be expressed by the following: \u0394C=\u0394C\/extent(R)*extent(R)]+R. Each input position mapped to display coordinates is then adjusted  by the accelerated displacement.","For absolutely mapped dimensions, a similar process called span adjustment can be used, as described in connection with . In , the displacement of each input from the sensor locator is determined , in pixels in the device coordinate space. The minimum displacement is selected . This minimum displacement value is converted  to physical dimensions using the pixel density of the device. The minimum displacement value in physical dimensions is transformed  to a span adjustment value, using any appropriate transform. A suitable transform may be similar to the acceleration transform, such as a piece-wise linear function that maps a displacement value to a span adjustment value. This span adjustment value is converted  back to pixel values. Similar to acceleration, the span adjustment value is then transformed  to display pixel values, and each input point is adjusted  using that value.","It should be noted that the acceleration and span adjustment modification to input points is done prior to applying the boundary conditions that ensure that points remain in the visible display area.","Having now described an example implementation, a computing environment in which such a system is designed to operate will now be described. The following description is intended to provide a brief, general description of a suitable computing environment in which this system can be implemented. The system can be implemented with numerous general purpose or special purpose computing hardware configurations. Examples of well known computing devices that may be suitable include, but are not limited to, personal computers, server computers, hand-held or laptop devices (for example, media players, notebook computers, cellular phones, personal data assistants, voice recorders), multiprocessor systems, microprocessor-based systems, set top boxes, game consoles, programmable consumer electronics, network PCs, minicomputers, mainframe computers, distributed computing environments that include any of the above systems or devices, and the like.",{"@attributes":{"id":"p-0071","num":"0070"},"figref":"FIG. 7"},"With reference to , an example computing environment includes a computing machine, such as computing machine . In its most basic configuration, computing machine  typically includes at least one processing unit  and memory . The computing device may include multiple processing units and\/or additional co-processing units such as graphics processing unit . Depending on the exact configuration and type of computing device, memory  may be volatile (such as RAM), non-volatile (such as ROM, flash memory, etc.) or some combination of the two. This most basic configuration is illustrated in  by dashed line . Additionally, computing machine  may also have additional features\/functionality. For example, computing machine  may also include additional storage (removable and\/or non-removable) including, but not limited to, magnetic or optical disks or tape. Such additional storage is illustrated in  by removable storage  and non-removable storage . Computer storage media includes volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information such as computer program instructions, data structures, program modules or other data. Memory , removable storage  and non-removable storage  are all examples of computer storage media. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can accessed by computing machine . Any such computer storage media may be part of computing machine .","Computing machine  may also contain communications connection(s)  that allow the device to communicate with other devices. Communications connection(s)  is an example of communication media. Communication media typically carries computer program instructions, data structures, program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term \u201cmodulated data signal\u201d means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal, thereby changing the configuration or state of the receiving device of the signal. By way of example, and not limitation, communication media includes wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, RF, infrared and other wireless media.","Computing machine  may have various input device(s)  such as a keyboard, mouse, pen, camera, touch input device, and so on. Output device(s)  such as a display, speakers, a printer, and so on may also be included. All of these devices are well known in the art and need not be discussed at length here.","The system can be implemented in the general context of software, including computer-executable instructions and\/or computer-interpreted instructions, such as program modules, being processed by a computing machine. Generally, program modules include routines, programs, objects, components, data structures, and so on, that, when processed by a processing unit, instruct the processing unit to perform particular tasks or implement particular abstract data types. This system may be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment, program modules may be located in both local and remote computer storage media including memory storage devices.","A particular example software architecture will now be described in connection with . One or more indirect touch devices  can be connected to the computer platform through a USB\/HID interface . Each of them has touch controller interface object  associated with it, which can be implemented as a dynamically linked library installed as a user-level service by the operating system. A contact recognition module  can be provided to provide for low level processing of sensor data from the input device to provide, for each frame, a list of contact and attributes of those contacts. This module could be a software library  or a firmware implemented library ","The touch controller interface object  has the following methods. An open method allows the caller to open and initialize the device for data transfer following device attachment. A close method allows the caller to close the device, which involves clean up following physical removal. A query method retrieves device data. The device data is stored one or more data structures by the touch controller interface object. The device data can include information such as supported device modes, sensor dimensions and resolution, a maximum number of contacts, pressure entry\/exit thresholds. A get method causes the object to transfer an array of stateless contact descriptors, state information (such as locked state status), and frame time information. A set method causes the object to enter into one of a set of modes. For example, this method can enable or disable hardware event reporting modes (e.g. HID mouse) during touch interactions, inform the device of interaction mode changes, or assign power state.","The touch controller interface for each connected device is accessed by a touch controller system (TCS) runtime module . Each device connection has a device input\/output (i\/o) thread  that binds touch controller devices to the system, queries and manages touch controller device capabilities, and assigns device modes and attributes of the device in accordance with host conditions and user configuration settings. An input\/output (i\/o) buffer thread  extracts touch data from the device. Each device connection also has an injection thread  that injects information from the touch devices to a core input stack  accessed by applications , to enable a consistent and quality end-user experience for touch controller devices. The TCS runtime module can be implemented as a dynamically linked library installed as a user-level service by the operating system, or can be a kernel-level implementation.","The runtime module performs the various functions for which an example implementation is described above, such as indirect-touch specific interactions, a user interaction model, state transitions based on z-information, mapping modes, acceleration\/deceleration, span adjustment, normalization of data across devices, and power management. A touch controller control panel  is a user interface that allows various settings of the runtime module  to be manipulated by the users.","Any or all of the aforementioned alternate embodiments described herein may be used in any combination desired to form additional hybrid embodiments. It should be understood that the subject matter defined in the appended claims is not necessarily limited to the specific implementations described above. The specific implementations described above are disclosed as examples only. The terms \u201carticle of manufacture\u201d, \u201cprocess\u201d, \u201cmachine\u201d and \u201ccomposition of matter\u201d in the preambles of the appended claims are intended to limit the claims to subject matter deemed to fall within the scope of patentable subject matter defined by the use of these terms in 35 U.S.C. \u00a7101."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 8"}]},"DETDESC":[{},{}]}
