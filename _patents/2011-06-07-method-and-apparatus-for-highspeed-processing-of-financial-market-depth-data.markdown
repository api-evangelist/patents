---
title: Method and apparatus for high-speed processing of financial market depth data
abstract: A variety of embodiments for hardware-accelerating the processing of financial market depth data are disclosed. A coprocessor, which may be resident in a ticker plant, can be configured to update order books based on financial market depth data at extremely low latency. Such a coprocessor can also be configured to enrich a stream of limit order events pertaining to financial instruments with data from a plurality of updated order books.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08768805&OS=08768805&RS=08768805
owner: IP Reservoir, LLC
number: 08768805
owner_city: St. Louis
owner_country: US
publication_date: 20110607
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE AND PRIORITY CLAIM TO RELATED PATENT APPLICATIONS","TERMINOLOGY","BACKGROUND AND SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS"],"p":["This application is a divisional of International Patent Application No. PCT\/US2009\/067935, filed Dec. 14, 2009, and designating the U.S., which claims priority to U.S. patent application 61\/122,673, filed Dec. 15, 2008, entitled \u201cMethod and Apparatus for High-Speed Processing of Financial Market Depth Data\u201d, the entire disclosures of both of which are incorporated herein by reference.","This application is related to (1) U.S. patent application Ser. No. 12\/013,302, filed Jan. 11, 2008, entitled \u201cMethod and System for Low Latency Basket Calculation\u201d, published as U.S. Patent Application Publication 2009\/0182683, (2) U.S. patent application Ser. No. 11\/765,306, filed Jun. 19, 2007, entitled \u201cHigh Speed Processing of Financial Information Using FPGA Devices\u201d, published as U.S. Patent Application Publication 2008\/0243675, (3) U.S. patent application Ser. No. 11\/760,211, filed Jun. 8, 2007, entitled \u201cMethod and System for High Speed Options Pricing\u201d, published as U.S. Patent Application Publication 2007\/0294157, and (4) U.S. patent application Ser. No. 11\/561,615, filed Nov. 20, 2006, entitled \u201cMethod and Apparatus for Processing Financial Information at Hardware Speeds Using FPGA Devices\u201d, published as U.S. Patent Application Publication 2007\/0078837, the entire disclosures of each of which are incorporated herein by reference.","The following paragraphs provide several definitions for various terms used herein. These paragraphs also provide background information relating to these terms.","Financial Instrument: As used herein, a \u201cfinancial instrument\u201d refers to a contract representing an equity ownership, debt, or credit, typically in relation to a corporate or governmental entity, wherein the contract is saleable. Examples of financial instruments include stocks, bonds, options, commodities, currency traded on currency markets, etc. but would not include cash or checks in the sense of how those items are used outside the financial trading markets (i.e., the purchase of groceries at a grocery store using cash or check would not be covered by the term \u201cfinancial instrument\u201d as used herein; similarly, the withdrawal of $100 in cash from an Automatic Teller Machine using a debit card would not be covered by the term \u201cfinancial instrument\u201d as used herein).","Financial Market Data: As used herein, the term \u201cfinancial market data\u201d refers to data contained in or derived from a series of messages that individually represent a new offer to buy or sell a financial instrument, an indication of a completed sale of a financial instrument, notifications of corrections to previously-reported sales of a financial instrument, administrative messages related to such transactions, and the like. Feeds of messages which contain financial market data are available from a number of sources and exist in a variety of feed types\u2014for example, Level 1 feeds and Level 2 feeds as discussed herein.","Basket: As used herein, the term \u201cbasket\u201d refers to a collection comprising a plurality of elements, each element having one or more values. The collection may be assigned one or more Net Values (NVs), wherein a NV is derived from the values of the plurality of elements in the collection. For example, a basket may be a collection of data points from various scientific experiments. Each data point may have associated values such as size, mass, etc. One may derive a size NV by computing a weighted sum of the sizes, a mass NV by computing a weighted sum of the masses, etc. Another example of a basket would be a collection of financial instruments, as explained below.","Financial Instrument Basket: As used herein, the term \u201cfinancial instrument basket\u201d refers to a basket whose elements comprise financial instruments. The financial instrument basket may be assigned one or more Net Asset Values (NAVs), wherein a NAV is derived from the values of the elements in the basket. Examples of financial instruments that may be included in baskets are securities (stocks), bonds, options, mutual funds, exchange-traded funds, etc. Financial instrument baskets may represent standard indexes, exchange-traded funds (ETFs), mutual funds, personal portfolios, etc. One may derive a last sale NAV by computing a weighted sum of the last sale prices for each of the financial instruments in the basket, a bid NAV by computing a weighted sum of the current best bid prices for each of the financial instruments in the basket, etc.","GPP: As used herein, the term \u201cgeneral-purpose processor\u201d (or GPP) refers to a hardware device having a fixed form and whose functionality is variable, wherein this variable functionality is defined by fetching instructions and executing those instructions, of which a conventional central processing unit (CPU) is a common example. Exemplary embodiments of GPPs include an Intel Xeon processor and an AMD Opteron processor.","Reconfigurable Logic: As used herein, the term \u201creconfigurable logic\u201d refers to any logic technology whose form and function can be significantly altered (i.e., reconfigured) in the field post-manufacture. This is to be contrasted with a GPP, whose function can change post-manufacture, but whose form is fixed at manufacture.","Software: As used herein, the term \u201csoftware\u201d refers to data processing functionality that is deployed on a GPP or other processing devices, wherein software cannot be used to change or define the form of the device on which it is loaded.","Firmware: As used herein, the term \u201cfirmware\u201d refers to data processing functionality that is deployed on reconfigurable logic or other processing devices, wherein firmware may be used to change or define the form of the device on which it is loaded.","Coprocessor: As used herein, the term \u201ccoprocessor\u201d refers to a computational engine designed to operate in conjunction with other components in a computational system having a main processor (wherein the main processor itself may comprise multiple processors such as in a multi-core processor architecture). Typically, a coprocessor is optimized to perform a specific set of tasks and is used to offload tasks from a main processor (which is typically a GPP) in order to optimize system performance. The scope of tasks performed by a coprocessor may be fixed or variable, depending on the architecture of the coprocessor. Examples of fixed coprocessor architectures include Graphics Processor Units which perform a broad spectrum of tasks and floating point numeric coprocessors which perform a relatively narrow set of tasks. Examples of reconfigurable coprocessor architectures include reconfigurable logic devices such as Field Programmable Gate Arrays (FPGAs) which may be reconfigured to implement a wide variety of fixed or programmable computational engines. The functionality of a coprocessor may be defined via software and\/or firmware.","Hardware Acceleration: As used herein, the term \u201chardware acceleration\u201d refers to the use of software and\/or firmware implemented on a coprocessor for offloading one or more processing tasks from a main processor to decrease processing latency for those tasks relative to the main processor.","Bus: As used herein, the term \u201cbus\u201d refers to a logical bus which encompasses any physical interconnect for which devices and locations are accessed by an address. Examples of buses that could be used in the practice of the present invention include, but are not limited to the PCI family of buses (e.g., PCI-X and PCI-Express) and HyperTransport buses.","Pipelining: As used herein, the terms \u201cpipeline\u201d, \u201cpipelined sequence\u201d, or \u201cchain\u201d refer to an arrangement of application modules wherein the output of one application module is connected to the input of the next application module in the sequence. This pipelining arrangement allows each application module to independently operate on any data it receives during a given clock cycle and then pass its output to the next downstream application module in the sequence during another clock cycle.","The process of trading financial instruments may be viewed broadly as proceeding through a cycle as shown in . At the top of the cycle is the exchange which is responsible for matching up offers to buy and sell financial instruments. Exchanges disseminate market information, such as the appearance of new buy\/sell offers and trade transactions, as streams of events known as market data feeds. Trading firms receive market data from the various exchanges upon which they trade. Note that many traders manage diverse portfolios of instruments requiring them to monitor the state of multiple exchanges. Utilizing the data received from the exchange feeds, trading systems make trading decisions and issue buy\/sell orders to the financial exchanges. Orders flow into the exchange where they are inserted into a sorted \u201cbook\u201d of orders, triggering the publication of one or more events on the market data feeds.","Exchanges keep a sorted listing of limit orders for each financial instrument, known as an order book. As used herein, a \u201climit order\u201d refers to an offer to buy or sell a specified number of shares of a given financial instrument at a specified price. Limit orders can be sorted based on price, size, and time according to exchange-specific rules. Many exchanges publish market data feeds that disseminate order book updates as order add, modify, and delete events. These feeds belong to a class of feeds known as level 2 data feeds. It should be understood that each exchange may be a little different as to when data is published on the feed and how much normalization the exchange performs when publishing events on the feed, although it is fair to expect that the amount of normalization in the level 2 feed is minimal relative to a level 1 feed. These feeds typically utilize one of two standard data models: full order depth or price aggregated depth. As shown in ), full order depth feeds contain events that allow recipients to construct order books that mirror the order books used by the exchange matching engines. This is useful for trading strategies that require knowledge of the state of specific orders in the market.","As shown in ), price aggregated depth feeds contain events that allow recipients to construct order books that report the distribution of liquidity (available shares) over prices for a given financial instrument.","Order book feeds are valuable to electronic trading as they provide what is generally considered the fastest and deepest insight into market dynamics. The current set of order book feeds includes feeds for order books of equities, equity options, and commodities. Several exchanges have announced plans to provide new order book feeds for derivative instruments such as equity options. Given its explosive growth over the past several years, derivative instrument trading is responsible for the lion's share of current market data traffic. The Options Price Reporting Authority (OPRA) feed is the most significant source of derivatives market data, and it belongs to the class of feeds known as \u201clevel 1\u201d feeds. Level 1 feeds report quotes, trades, trade cancels and corrections, and a variety of summary events. For a given financial instrument, the highest buy price and lowest sell price comprise the \u201cbest bid and offer\u201d (BBO) that are advertised as the quote. As an exchange's sorted order book listing changes due to order executions, modifications, or cancellations, the exchange publishes new quotes. When the best bid and offer prices match in the exchange's order book, the exchange executes a trade and advertises the trade transaction on its level 1 market data feed. Note that some amount of processing is required prior to publishing a quote or trade event because of the latency incurred by the publisher's computer system when processing limit orders to build order books and identify whether trades or quotes should be generated. Thus, level 1 data feeds from exchanges or other providers possess inherent latency relative to viewing \u201craw\u201d order events on order book feeds. A feed of raw limit order data belongs to a class of feeds known as \u201clevel 2\u201d feeds.","In order to minimize total system latency, many electronic trading firms ingest market data feeds, including market data feeds of limit orders, directly into their own computer systems from the financial exchanges. While some loose standards are in place, most exchanges define unique protocols for disseminating their market data. This allows the exchanges to modify the protocols as needed to adjust to changes in market dynamics, regulatory controls, and the introduction of new asset classes. The ticker plant resides at the head of the platform and is responsible for the normalization, caching, filtering, and publishing of market data messages. A ticker plant typically provides a subscribe interface to a set of downstream trading applications. By normalizing data from disparate exchanges and asset classes, the ticker plant provides a consistent data model for trading applications. The subscribe interface allows each trading application to construct a custom normalized data feed containing only the information it requires. This is accomplished by performing subscription-based filtering at the ticker plant.","In traditional market data platforms known to the inventors, the ticker plant may perform some normalization tasks on order book feeds, but the task of constructing sorted and\/or price-aggregated views of order books is typically pushed to downstream components in the market data platform. The inventors believe that such a trading platform architecture increases processing latency and the number of discrete systems required to process order book feeds. As an improvement over such an arrangement, an embodiment of the invention disclosed herein enables a ticker plant to perform order feed processing (e.g., normalization, price-aggregation, sorting) in an accelerated and integrated fashion, thereby increasing system throughput and decreasing processing latency. In an exemplary embodiment, the ticker plant employs a coprocessor that serves as an offload engine to accelerate the building of order books. Financial market data received on a feed into the ticket plant can be transferred on a streaming basis to the coprocessor for high speed processing.","Thus, in accordance with an exemplary embodiment of the invention, the inventors disclose a method for generating an order book view from financial market depth data, the method comprising: (1) maintaining a data structure representative of a plurality of order books for a plurality of financial instruments, and (2) hardware-accelerating a processing of a plurality of financial market depth data messages to update the order books within the data structure. Preferably the hardware-accelerating step is performed by a coprocessor within a ticker plant. The inventors also disclose a system for generating an order book view from financial market depth data, the system comprising: (1) a memory for storing a data structure representative of a plurality of order books for a plurality of financial instruments, and (2) a coprocessor configured to process of a plurality of financial market depth data messages to update the order books within the data structure.","Using these order books, the method and system can also produce views of those order books for ultimate delivery to interested subscribers. The inventors define two general classes of book views that can be produced in accordance with various exemplary embodiments: stream views (unsorted, non-cached) and summary views (sorted, cached). Stream views provide client applications with a normalized stream of updates for limit orders or aggregated price-points for the specified regional symbol, composite symbol, or feed source (exchange). Summary views provide client applications with multiple sorted views of the book, including composite views (a.k.a. \u201cvirtual order books\u201d) that span multiple markets.","In an exemplary embodiment, stream views comprise a normalized stream of updates for limit orders or aggregated price-points for the specified regional symbol, composite symbol, or feed source (exchange). Following the creation of a stream subscription, a ticker plant can be configured to provide a client application with a stream of normalized events containing limit order or price point updates. As stream subscriptions do not provide sorting, it is expected that stream view data would be employed by client applications that construct their own book views or journals from the normalized event stream from one or more specified exchanges.","An example of a stream view that can be generated by various embodiments is an order stream view. An order stream view comprises a stream of normalized limit order update events for one or more specified regional symbols. The normalized events comprise fields such as the type of update (add, modify, delete), the order price, order size, exchange timestamp, and order identifier (if provided by the exchange). Another example of an order stream view is an order exchange stream view that comprises a stream of normalized limit order update events for one or more specified exchanges or clusters of instruments within an exchange. The normalized events comprise fields such as the type of update (add, modify, delete), the order price, order size, exchange timestamp, and order identifier (if provided by the exchange).","Another example of a stream view that can be generated by various embodiments is a price stream view. A price stream view comprises a stream of normalized price level update events for one or more specified regional symbols. The normalized events comprise fields such as the type of update (add, modify, delete), the aggregated price, order volume at the aggregated price, and the order count at the aggregated price. Another example of a price stream view is a price exchange stream view. A price exchange stream view comprises a stream of normalized price level update events for one or more specified exchanges or clusters of instruments within an exchange. The normalized events comprise fields such as the type of update (add, modify, delete), the aggregated price, order volume at the aggregated price, and order count at the aggregated price.","Another example of a stream view that can be generated by various embodiments is an aggregate stream view. An aggregate stream view comprises a stream of normalized price level update events for one or more specified composite symbols. The normalized events comprise fields such as the type of update (add, modify, delete), the (virtual) aggregated price, (virtual) order volume at the aggregated price, and (virtual) order count at the aggregated price.","As explained in the above-referenced and incorporated U.S. Patent Application Publication 2008\/0243675, a regional symbol serves to identify a financial instrument traded on a particular exchange while a composite symbol serves to identify a financial instrument in the aggregate on all of the exchanges upon which it trades. It should be understood that embodiments of the invention disclosed herein may be configured to store both regional and composite records for the same financial instrument in situations where the financial instrument is traded on multiple exchanges.","Summary views provide liquidity insight, and the inventors believe it is highly desirable to obtain such liquidity insight with ultra low latency. In accordance with an embodiment disclosed herein, by offloading a significant amount of data processing from client applications to a ticker plant, the ticker plant frees up client processing resources, thereby enabling those client resources to implement more sophisticated trading applications that retain first mover advantage.","An example of a summary view that can be generated by various embodiments is an order summary view. An order summary view represents a first-order liquidity view of the raw limit order data disseminated by a single feed source. The inventors define an order summary view to be a sorted listing comprising a plurality of individual limit orders for a given financial instrument on a given exchange. The sort order is preferably by price and then by time (or then by size for some exchanges). An example of an order summary view is shown in .","Another example of a summary view that can be generated by various embodiments is a price summary view. A price summary view represents a second-order liquidity view of the raw limit order data disseminated by a single feed source. The inventors define a price summary view to be a sorted listing comprising a plurality of price levels for a given financial instrument on a given exchange, wherein each price level represents an aggregation of same-priced orders from that exchange. The price level timestamp in the summary view preferably reports the timestamp of the most recent event at that price level from that exchange. An example of a price summary view is shown in ). Note that a price summary view produced by an embodiment disclosed herein may be limited to a user-specified number of price points starting from the top of the book.","Another example of a summary view that can be generated by various embodiments is a spliced price summary view. A spliced price summary view represents a second-order, pan-market liquidity view of the raw limit order data disseminated by multiple feed sources. The inventors define a spliced price summary view to be a sorted listing comprising a plurality of price levels for a given financial instrument across all contributing exchanges where each price level represents an aggregation of same-priced orders from a unique contributing exchange. The price level timestamp in the spliced price summary view preferably reports the timestamp of the most recent event at that price level for the specified exchange. An example of a spliced price summary view is shown in ). Note that a spliced price summary view produced by an embodiment disclosed herein may be limited to a user-specified number of price points starting from the top of the book.","Another example of a summary view that can be generated by various embodiments is an aggregate price summary view. An aggregate price summary view represents a third-order, pan-market liquidity view of the raw limit order data disseminated by multiple feed sources. The inventors define an aggregate price summary view to be a sorted listing comprising a plurality of price levels for a given financial instrument where each price level represents an aggregation of same-priced orders from all contributing exchanges. The price level timestamp in the aggregate price summary view preferably reports the timestamp of the most recent event at that price level from any contributing exchange. An example of an aggregate price summary view is shown in ). Note that an aggregate price summary view produced by an embodiment disclosed herein may be limited to a user-specified number of price points starting from the top of the book.","The inventors further note that financial exchanges have continued to innovate in order to compete and to provide more efficient markets. One example of such innovation is the introduction of ephemeral regional orders in several equity markets (e.g., FLASH orders on NASDAQ, BOLT orders on BATS) that provide regional market participants the opportunity to view specific orders prior to public advertisement. Another example of such innovation is implied liquidity in several commodity markets (e.g. CME, ICE) that allow market participants to trade against synthetic orders whose price is derived from other derivative instruments. In order to capture and distinguish this type of order or price level in an order book, the inventors define the concept of attributes and apply this concept to the data structures employed by various embodiments disclosed herein. Each entry in an order book or price book may have one or more attributes. Conceptually, attributes are a vector of flags that may be associated with each order book or price book entry. By default, every order or aggregated price level is \u201cexplicit\u201d and represents a limit order to buy or sell the associated financial instrument entered by a market participant. In some equity markets, an order or price level may be flagged using various embodiments disclosed herein with an attribute to indicate whether the order or price level relates to an ephemeral regional order (ERO). Similarly, in some commodity markets, an order or price level may be flagged using various embodiments disclosed herein to indicate whether the order or price level relates to an implied liquidity.","By capturing such attributes in the data structures employed by exemplary embodiments, the inventors note that these attributes thus provide another dimension to the types of book views that various embodiments disclosed herein generate. For example, one commodity trading application may wish to view a price aggregated book that omits implied liquidity, another commodity trading application may wish to view a price aggregated book with the explicit and implied price levels shown independently (spliced view), while another commodity trading application may wish to view a price aggregated book with explicit and implied entries aggregated by price. These three examples of attribute-based book views are shown in ,  and , respectively.","Thus, in accordance with an exemplary embodiment, the inventors disclose the use of attribute filtering and price level merging to capture the range of options in producing book views for books that contain entries with attributes. Attribute filtering allows applications to specify which entries should be included and\/or excluded from the book view. Price level merging allows applications to specify whether or not entries that share the same price but differing attributes should be aggregated into a single price level.","The inventors also disclose several embodiments wherein a coprocessor can be used to enrich a stream of limit order events pertaining to financial instruments with order book data, both stream view order book data and summary view order book data, as disclosed herein.","These and other features and advantages of the present invention will be described hereinafter to those having ordinary skill in the art.","Examples of suitable platforms for implementing exemplary embodiments of the invention are shown in ) and (). ) depicts a system  employs a hardware-accelerated data processing capability through coprocessor  to process financial market depth data. Within system , a coprocessor  is positioned to receive data that streams into the system  from a network  (via network interface ). In a preferred embodiment, system  is employed to receive financial market limit order data and process financial market depth data. Network  thus preferably comprises a network through which system  can access a source for Level 2 financial data such as the exchanges themselves (e.g., NYSE, NASDAQ, etc.) or a third party provider (e.g., extranet providers such as Savvis or BT Radians). Such incoming data preferably comprises a series of financial market data messages, the messages representing events such as limit orders relating to financial instruments. These messages can exist in any of a number of formats, as is known in the art.","The computer system defined by processor  and RAM  can be any commodity computer system as would be understood by those having ordinary skill in the art. For example, the computer system may be an Intel Xeon system or an AMD Opteron system. Thus, processor , which serves as the central or main processor for system , preferably comprises a GPP.","In a preferred embodiment, the coprocessor  comprises a reconfigurable logic device . Preferably, data streams into the reconfigurable logic device  by way of system bus , although other design architectures are possible (see )). Preferably, the reconfigurable logic device  is a field programmable gate array (FPGA), although this need not be the case. System bus  can also interconnect the reconfigurable logic device  with the processor  as well as RAM . In a preferred embodiment, system bus  may be a PCI-X bus or a PCI-Express bus, although this need not be the case.","The reconfigurable logic device  has firmware modules deployed thereon that define its functionality. The firmware socket module  handles the data movement requirements (both command data and target data) into and out of the reconfigurable logic device, thereby providing a consistent application interface to the firmware application module (FAM) chain  that is also deployed on the reconfigurable logic device. The FAMs of the FAM chain  are configured to perform specified data processing operations on any data that streams through the chain  from the firmware socket module . Examples of FAMs that can be deployed on reconfigurable logic in accordance with a preferred embodiments of the present invention are described below.","The specific data processing operation that is performed by a FAM is controlled\/parameterized by the command data that FAM receives from the firmware socket module . This command data can be FAM-specific, and upon receipt of the command, the FAM will arrange itself to carry out the data processing operation controlled by the received command. For example, within a FAM that is configured to perform an exact match operation between data and a key, the FAM's exact match operation can be parameterized to define the key(s) that the exact match operation will be run against. In this way, a FAM that is configured to perform an exact match operation can be readily re-arranged to perform a different exact match operation by simply loading new parameters for one or more different keys in that FAM. As another example pertaining to baskets, a command can be issued to the one or more FAMs that make up a basket calculation engine to add\/delete one or more financial instruments to\/from the basket.","Once a FAM has been arranged to perform the data processing operation specified by a received command, that FAM is ready to carry out its specified data processing operation on the data stream that it receives from the firmware socket module. Thus, a FAM can be arranged through an appropriate command to process a specified stream of data in a specified manner. Once the FAM has completed its data processing operation, another command can be sent to that FAM that will cause the FAM to re-arrange itself to alter the nature of the data processing operation performed thereby. Not only will the FAM operate at hardware speeds (thereby providing a high throughput of data through the FAM), but the FAMs can also be flexibly reprogrammed to change the parameters of their data processing operations.","The FAM chain  preferably comprises a plurality of firmware application modules (FAMs) , , . . . that are arranged in a pipelined sequence. However, it should be noted that within the firmware pipeline, one or more parallel paths of FAMs can be employed. For example, the firmware chain may comprise three FAMs arranged in a first pipelined path (e.g., FAMs , , ) and four FAMs arranged in a second pipelined path (e.g., FAMs , , , and ), wherein the first and second pipelined paths are parallel with each other. Furthermore, the firmware pipeline can have one or more paths branch off from an existing pipeline path. A practitioner of the present invention can design an appropriate arrangement of FAMs for FAM chain  based on the processing needs of a given application.","A communication path  connects the firmware socket module  with the input of the first one of the pipelined FAMs . The input of the first FAM serves as the entry point into the FAM chain . A communication path  connects the output of the final one of the pipelined FAMs with the firmware socket module . The output of the final FAM serves as the exit point from the FAM chain . Both communication path  and communication path  are preferably multi-bit paths.","The nature of the software and hardware\/software interfaces used by system , particularly in connection with data flow into and out of the firmware socket module are described in greater detail in U.S. Patent Application Publication 2007\/0174841, the entire disclosure of which is incorporated herein by reference.",{"@attributes":{"id":"p-0084","num":"0083"},"figref":["FIG. 8(","FIG. 8("],"i":["b","b"],"b":["800","800","842","806","814","840","842","842","842"]},{"@attributes":{"id":"p-0085","num":"0084"},"figref":["FIG. 9(","FIGS. 8(","FIG. 9("],"i":["a","a","b","a"],"b":["900","806","840","800","802","902","904","902","904"]},{"@attributes":{"id":"p-0086","num":"0085"},"figref":["FIG. 9(","FIG. 9(","FIG. 9("],"i":["b","b","b"],"b":["900","906","908","910","900","804","812","906"]},"It is worth noting that in either the configuration of ) or (), the firmware socket  can make memory  accessible to the bus , which thereby makes memory  available for use by an OS kernel as the buffers for transfers to the FAMs from a data source with access to bus. It is also worth noting that while a single FPGA  is shown on the printed circuit boards of ) and (), it should be understood that multiple FPGAs can be supported by either including more than one FPGA on the printed circuit board  or by installing more than one printed circuit board  in the system .  depicts an example where numerous FAMs in a single pipeline are deployed across multiple FPGAs.",{"@attributes":{"id":"p-0088","num":"0087"},"figref":["FIGS. 11(","FIG. 11(","FIG. 11(","FIG. 11("],"i":["a","c","a","b","c"],"b":["1100","840","1100","1100","1100"]},"In the exemplary embodiments of )-(), a data processing module  for generating a stream view of processed limit order data can be realized via a pipeline . In the example of ), the pipeline comprises a message parsing (MP) module  that receives raw messages . These messages  comprise a stream of financial market data events, of which at least a plurality comprise limit order events. Downstream from the MP module  is a symbol mapping (SM) module , and downstream from the SM module  is an Order Normalization and Price Aggregation (ONPA) module . The ONPA module , as explained below, is configured to generate the stream view of the limit order data contained in limit order events.","The MP module  is configured to parse the incoming stream of raw messages  into a plurality of parsed messages having data fields that can be understood by downstream modules. Exemplary embodiments for such an MP module are described in the above-referenced and incorporated U.S. Patent Application Publication 2008\/0243675. Thus, the MP modules is configured to process incoming raw messages  to create limit order events that can be understood by downstream modules.","The SM module  resolves a unique symbol identifier for the base financial instrument and the associated market center for a received event. Input events may contain a symbol field that uniquely identifies the base financial instrument. In this case, the symbol mapping stage performs a one-to-one translation from the input symbol field to the symbol identifier, which is preferably a minimally-sized binary tag that provides for efficient lookup of associated state information for the financial instrument. Thus, the SM module  operates to map the known symbol for a financial instrument (or set of financial instruments) as defined in the parsed message to a symbology that is internal to the platform (e.g., mapping the symbol for IBM stock to an internal symbol \u201c12345\u201d). Preferably, the internal platform symbol identifier (ID) is an integer in the range 0 to N\u22121, where N is the number of entries in a symbol index memory. Also, the symbol ID may formatted as a binary value of size M=log(N) bits. The format of financial instrument symbols in input exchange messages varies for different message feeds and financial instrument types. Typically, the symbol is a variable-length ASCII character string. A symbology ID is an internal control field that uniquely identifies the format of the symbol string in the message. As shown in , a symbology ID is preferably assigned by a feed handler and present in all incoming messages, as the symbol string format is typically shared by all messages on a given input feed.","An exemplary embodiment of the SM module  maps each unique symbol character string to a unique binary number of size M bits. In such an exemplary embodiment, the symbol mapping FAM performs a format-specific compression of the symbol to generate a hash key of size K bits, where K is the size of the entries in a symbol index memory. The symbology ID may be used to lookup a Key Code that identifies the symbol compression technique that should be used for the input symbol. Preferably, the symbol mapping FAM compresses the symbol using format-specific compression engines and selects the correct compressed symbol output using the key code. Also, the key code can be concatenated with the compressed symbol to form the hash key. In doing so, each compression technique is allocated a subset of the range of possible hash keys. This ensures that hash keys will be unique, regardless of the compression technique used to compress the symbol. An example is shown in  wherein the ASCII symbol for a financial instrument is compressed in parallel by a plurality of different compression operations (e.g., alpha-numeric ticker compression, ISIN compression, and commodity compression). Compression techniques for different symbologies can be selected and\/or devised on an ad hoc basis as desired by a practitioner. A practitioner is free to select a different compression operation as may be appropriate for a given symbology. Based on the value of the key code, the SM module will pass one of the concatenations of the key code and compression results as the output from the multiplexer for use as the hash key.","Alternatively, the format-specific compression engines may be implemented in a programmable processor. The key code may then be used to fetch a sequence of instructions that specify how the symbol should be compressed.","Once the hash key is generated, the SM module  maps the hash key to a unique address in a symbol index memory in the range 0 to N\u22121. The symbol index memory may be implemented in a memory \u201con-chip\u201d (e.g., within the reconfigurable logic device) or in \u201coff-chip\u201d high speed memory devices such as SRAM and SDRAM that are accessible to the reconfigurable logic device. Preferably, this mapping is performed by a hash function. A hash function attempts to minimize the number of probes, or table lookups, to find the input hash key. In many applications, additional meta-data is associated with the hash key. In an exemplary embodiment, the location of the hash key in the symbol index memory is used as the unique internal Symbol ID for the financial instrument.",{"@attributes":{"id":"p-0095","num":"0094"},"figref":["FIG. 14","FIG. 14"],"br":[{},{},{},{}],"in-line-formulae":[{},{},{},{},{},{},{},{}],"i":["H","x","h","x","i*h","x","N ","h","x","A","x","d","x","d","x","T","B","x","h","x","C","x"]},"The operand x is the hash key generated by the previously described compression stage. The function h() is the primary hash function. The value i is the iteration count. The iteration count i is initialized to zero and incremented for each hash probe that results in a collision. For the first hash probe, hash function H(x)=h(), thus the primary hash function determines the first hash probe. The preferred hash function disclosed herein attempts to maximize the probability that the hash key is located on the first hash probe. If the hash probe results in a collision, the hash key stored in the hash slot does not match hash key x, the iteration count is incremented and combined with the secondary hash function h() to generate an offset from the first hash probe location. The modulo N operation ensures that the final result is within the range 0 to N\u22121, where N is the size of the symbol index memory. The secondary hash function h() is designed so that its outputs are prime relative to N. The process of incrementing i and recomputing H(x) continues until the input hash key is located in the table or an empty table slot is encountered. This technique of resolving collisions is known as open-addressing.","The primary hash function, h(), is computed as follows. Compute hash function B(x) where the result is in the range 0 to Q\u22121. Use the result of the B(x) function to lookup a displacement vector d(x) in table T containing Q displacement vectors. Preferably the size of the displacement vector d(x) in bits is equal to M. Compute hash function A(x) where the result is M bits in size. Compute the bitwise exclusive OR, \u2296, of A(x) and d(x). This is one example of near-perfect hashing where the displacement vector is used to resolve collisions among the set of hash keys that are known prior to the beginning of the query stream. Typically, this fits well with streaming financial data where the majority of the symbols for the instruments trading in a given day is known. Methods for computing displacement table entries are known in the art.","The secondary hash function, h(), is computed by computing a single hash function C(x) where the result is always prime relative to N. Hash functions A(x), B(x), and C(x) may be selected from the body of known hash functions with favorable randomization properties. Preferably, hash functions A(x), B(x), and C(x) are efficiently implemented in hardware. The set of H3 hash functions are good candidates. (See Krishnamurthy et al., \u201c\u201d, Proc. of the IEEE 15th Int'l Conf. on Application-Specific Systems, Architectures and Processors, September 2004, pp. 365-375, the entire disclosure of which is incorporated herein by reference).","Once the hash function H(x) produces an address whose entry is equal to the input hash key, the address is passed on as the new Symbol ID to be used internally by the ticker plant to reference the financial instrument. As shown in , the result of the hash key compare function may be used as a valid signal for the symbol ID output.","Hash keys are inserted in the table when an exchange message contains a symbol that was unknown at system initialization. Hash keys are removed from the table when a financial instrument is no longer traded. Alternatively, the symbol for the financial instrument may be removed from the set of known symbols and the hash table may be cleared, recomputed, and initialized. By doing so, the displacement table used for the near-perfect hash function of the primary hash may be optimized. Typically, financial markets have established trading hours that allow for after-hours or overnight processing. The general procedures for inserting and deleting hash keys from a hash table where open-addressing is used to resolve collisions is well-known in the art.","In an exemplary embodiment, the SM module  can also be configured to compute a global exchange identifier (GEID) that maps the exchange code and country code fields in the exchange message to an integer in the range 0 to G\u22121, as shown in . Similar to the symbol field for financial instruments, the exchange code and country code fields uniquely identify the source of the exchange message. Thus, the global exchange identifier (GEID) preferably comprises a binary tag that uniquely identifies a particular exchange for this the message is relevant. The value of G should be selected such that it is larger than the total number of sources (financial exchanges) that will be generating input messages for a given instance of the system. Hashing could be used to map the country codes and exchange codes to the GEID. Alternatively, a \u201cdirect addressing\u201d approach can be used to map country and exchange codes to GEIDs. For example, the exchange code and country codes can each be represented by two character codes, where the characters are 8-bit upper-case ASCII alpha characters. These codes can then be truncated to 5-bit characters in embodiment where only 26 unique values of these codes are needed. For each code, these truncated values are concatenated to generate a 10-bit address that is used to lookup a compressed intermediate value in a stage  table. Then the compressed intermediate values for the exchange and country code can be concatenated to generate an address for a stage  lookup. The result of the stage  lookup is the GEID. The size of the intermediate values and the stage  address will depend on the number of unique countries and the max number of exchanges in any one country, which can be adjusted as new exchanges open in different countries.","The ONPA module  then receives a stream of incoming limit order events , as shown in . The ONPA module accesses a memory  that stores data structures which comprise the various order books tracked by the system to determine (1) how to update the data structures in view of the received limit order event, and (2) how to enrich the limit order event in view of its relation to the tracked order books. The output from the ONPA module  is an outgoing stream of enriched limit order events .",{"@attributes":{"id":"p-0103","num":"0102"},"figref":"FIG. 19","b":["1600","1902","1904","1206","1906","1600","1908","1910","1912","1914","1600","1916","1916","1600","1918","1918","1208"]},"It should be understood that many limit order events  will not have the same fields shown in the example of , whether there be differences in the number of fields and\/or the types of fields. For example, many limit order events will have fields that vary based on the value of the AMD flag field . As another example, some limit order events  will not include a symbol field . In such an instance, the symbol mapping task would be performed by the ONPA module  rather than the SM module . As an example of such a circumstance, in order to conserve bandwidth on data transmission links, several market centers minimize the size of event messages by sending \u201cstatic\u201d information regarding outstanding limit orders only once. Typically, the first message advertising the addition of a new limit order includes all associated information (such as the symbol, source identifier, price and size of the order, etc.) as well as a reference number for the order. Subsequent messages reporting modification or deletion of the order may only include a reference number that uniquely identifies the order (thus omitting a symbol field from the event). In a preferred embodiment, one of the roles for the ONPA module  is to pass complete order information to downstream consumers of the event messages. For limit order event messages, the ONPA module  normalizes the messages by ensuring that all desired fields are present in a regular format and the message consistently communicates the market event. For example, a market center may choose to advertise all events using order modification events. In such a scenario, with a preferred embodiment, it is the responsibility of the ONPA module  to determine if the event results in the addition of a new order, modification of an existing order, or deletion of an existing order. In practice, market centers have disparate data models for communicating limit order events; the ONPA module  ensures that a consistent data model is presented to downstream processing blocks and trading applications. All output events contain a consistent set of fields where each field has a well-defined type. For example, some exchanges may only send the order reference number in a delete event. The ONPA module  fills in missing fields such as the price and size of the deleted order.","In order to resolve the symbol identifier for input events lacking a symbol field, the ONPA module  can use another identifying field (such as an order reference number). In this case, the ONPA module  performs a many-to-one translation to resolve the symbol identifier, as there may be many outstanding orders to buy or sell a given financial instrument. It is important to note that this many-to-one mapping requires maintaining a dynamic set of items that map to a given symbol identifier, items may be added, modified, or removed from the set at any time as orders enter and execute at market centers.","While there are several viable approaches to solve the order normalization problem, the preferred method is to maintain a record for every outstanding limit order advertised by the set of input market data feeds. An example of such a limit order record  is shown in . An event reporting the creation of a new limit order must contain a symbol field, thus when the event arrives at the ONPA module  it will contain the symbol identifier resolved by the symbol mapping stage. If the event is from a market center that uses order reference numbers to minimize subsequent message sizes, the event will also contain an order reference number. The ONPA module  maintains a map to the order record, where the record  may contain the symbol identifier, order reference number, price, size, and other fields provided by the market center. Preferably, the ONPA module  also assigns a unique internal order identifier  that may be used to directly address the order record by other system components.","In the example of , the limit order record  comprises a plurality of fields, such as:\n\n","Once again, however, it should be noted that limit order records  can be configured to have more or fewer and\/or different data fields.","Preferably, the mapping of a received limit order event  to a limit order record  is performed using hashing in order to achieve constant time access performance on average. The hash key may be constructed from the order reference number, symbol identifier, or other uniquely identifying fields. The type of hash key is determined by the type of market center data feed. Upstream feed handlers that perform pre-normalization of the events set flags in the event that notify the ONPA module as to what type of protocol the exchange uses and what fields are available for constructing unique hash keys. For example, this information may be encoded in the GELD field  or in some other field of the limit order event . There are a variety of hash functions that could be used by the ONPA module. In the preferred embodiment, the OPA employs H hash functions as discussed above and in the above-referenced and incorporated U.S. Patent Application Publication 2008\/0243675 due to their efficiency and amenability to parallel hardware implementation. Hash collisions may be resolved in a number of ways. In the preferred embodiment, collisions are resolved via chaining, creating a linked list of entries that map to the same hash slot. A linked list is a simple data structure that allows memory to be dynamically allocated as the number of entries in the set changes.","Once the record is located, the ONPA module updates fields in the record and copies fields from the record to the message, filling in missing fields as necessary to normalize the output message. It is during this step that the ONPA module may modify the type of the message to be consistent with the result of the market event. For example, if the input message is a modify event that specifies that 100 shares should be subtracted from the order (due to a partial execution at the market center) and the outstanding order is for 100 shares, then the OPA will change the type of the message to a delete event, subject to market center rules. Note that market centers may dictate rules such as whether or not zero size orders may remain on an order book. In another scenario, if the outstanding order was for 150 shares, the ONPA module would update the size field  of the limit order record to replace the 150 value with 50 reflect the removal of 100 shares from the order. In general, the ONPA module attempts to present the most descriptive and consistent view of the market data events. Hardware logic within the ONPA module can be configured to provide these updating and normalization tasks.","In addition to normalizing order messages, the ONPA module may additionally perform price aggregation in order to support price aggregated views of the order book. Preferably, the ONPA module maintains an independent set of price point records. In this data structure, a record is maintained for each unique price point in an order book. At minimum, the set of price point records preferably contain the price, volume (sum of order sizes at that price, which can be referred to as the price volume), and order count (total number of orders at that price). Order add events increase the volume and order count fields, order delete events decrease the volume and order count fields, etc. Price point records are created when an order event adds a new price point to the book. Likewise, price point records are deleted when on order event removes the only record with a given price point from the book. Preferably, the ONPA module updates AMD flags in an enriched limit order event  that specify if the event resulted in the addition, modification, or deletion of a price entry in the book (see the price AMD field  in ). This information may be used to optimize downstream sorting engines. Preferably, the ONPA module also assigns a unique internal price identifier to each price record that may be used to directly address the price record by other system components.","Note that mapping a limit order event  to a price point record is also a many-to-one mapping problem. Preferably, the set of price point records is maintained using a hash mapping, similar to the order records. In order to locate the price point record associated with an order event, the hash key is constructed from fields such as the symbol identifier, global exchange identifier, and price. Preferably, hash collisions are resolved using chaining as with the order record data structure. Other data structures may be suitable for maintaining the sets of order and price point records, but hash maps have the favorable property of constant time accesses (on average).","In order to support efficient attribute filtering and price level merging in downstream book views, the ONPA module preferably maintains a price attribute vector as part of the price point records, wherein the price attribute vectors also comprise a vector of volumes and price counts in each price point record. For example, the price point record may include the following fields: price, volume (total shares or lots at this price), order count (total orders at this price), attribute flags, attribute volume 0 (total shares or lots at this price with attribute 0), order count 0 (total orders at this price with attribute 0), attribute volume 1, attribute order count 1, etc. Examples of such price point records are shown in ) and (). In general, the number of unique attributes for a given financial instrument is expected to be small. Preferably, the ONPA module is configurable to allow the number of possible attributes to be defined dynamically for a given financial instrument.","The ONPA module may append the volume, order count, and price attribute to events when creating enriched limit order events . Preferably, the ONPA module maintains price interest vectors that specify if any downstream applications or components require the price aggregated and\/or attribute information. Furthermore, the ONPA module preferably updates flags in the event that specify if the event resulted in the addition, modification, or deletion of a price entry in the book as defined by attribute (see the price AMD field  in ).","The data structure used to store price point records preferably separately maintains regional price point records  and composite price point records  for limit orders. A regional price point record  stores price point information for limit orders pertaining to a financial instrument traded on a specific regional exchange. A composite price point record  stores price point information for limits order pertaining to a financial instrument traded across multiple exchanges. An example of a regional price point record is shown in ), and an example of a composite price point record is shown in ).","An exemplary regional price point record  comprises a plurality of fields, such as:\n\n","An exemplary composite price point record  comprises a plurality of fields, such as:\n\n","Once again, however, it should be noted that regional and composite price point records  and  can be configured to have more or fewer and\/or different data fields.","In the preferred embodiment, parallel engines update and maintain the order and price aggregation data structures in parallel. In one embodiment, the data structures are maintained in the same physical memory. In this case, the one or more order engines and one or more price engines interleave their accesses to memory, masking the memory access latency of the memory technology and maximizing throughput of the system. There are a variety of well-known techniques for memory interleaving. In one embodiment, an engine controller block utilizes a time-slot approach where each engine is granted access to memory at regular intervals. In another embodiment, a memory arbitration block schedules outstanding memory requests on the shared interface and notifies engines when their requests are fulfilled. Preferably, the memory technology is a high-speed dynamic memory such as DDR3 SDRAM. In another embodiment, the order and price data structures are maintained in separate physical memories. As in the single memory architecture, multiple engines may interleave their accesses to memory in order to mask memory access latency and maximize throughput.",{"@attributes":{"id":"p-0120","num":"0149"},"figref":["FIG. 17","FIG. 18"]},"The ONPA module , upon receipt of a limit order event , thus (1) processes data in the limit order event  to access memory  (which may be multiple physical memories) and retrieve a limit order record , regional price point record  and composite price point record  as appropriate, (2) processes data in the limit order event and retrieved records to update the records as appropriate, and (3) enriches the limit order event  with new information to create an enriched limit order event . An example of such an enriched limit order event  is shown in . Preferably, the ONPA module appends a number of fields onto the limit order event  that provide downstream subscribers with valuable information about market depth. In the example of , the enriched limit order event  comprises fields such as:\n\n","Once again, however, it should be noted the ONPA module can be configured to enrich limit order events with more and fewer and\/or different data fields.","The outgoing enriched limit order events  thus serve as the stream view of processed limit order data  that can be produced by the ONPA module  at extremely low latency.","A block diagram of an exemplary embodiment for the ONPA module  is shown in .","The Extractor module is responsible for extracting from an input market event the fields needed by the rest of the modules within the ONPA module and presenting those fields in parallel to downstream modules. The Extractor also forwards the event to the Blender module for message reconstruction.","The Price Normalizer module converts variably typed prices into normalized fixed-sized prices (e.g. 64-bit values). In the preferred embodiment, the new prices are in units of either billionths or 256ths. The power of 2 price conversions may be performed by simple shifts. The power of 10 price conversions take place in a pipeline of shifts and adds.","In the preferred embodiment, the Hash modules are responsible for doing the following:\n\n","With respect to such refresh lists, the inventors note that a refresh event can be used to initialize the book view provided to client applications. Thus, one of the responsibilities of the Level 2 processing pipeline can be to generate book snapshots for client application initialization. At subscription time, a refresh event provides a snapshot of the book at a particular instant in time. It is at this point that the appropriate bits in the interest vector are set in the appropriate data structures. Following the refresh event, incremental update events are delivered to the client application in order to update the client application's view of the book. Refresh events may be processed in-line with incremental update events in the FAM pipeline. In order to minimize the overhead of generating the book snapshot, refresh events may be processed asynchronously. So long as the snapshot of the book is an atomic event that records the event sequence number of the most recent update, the snapshot need not be processed synchronous to all incremental update traffic. Synchronizing buffers in the client API may be used to buffer incremental updates received prior to receipt of the refresh event. When the refresh event is received, incremental updates in the synchronization buffer are processed. Updates with sequence numbers less than or equal to the sequence number noted in the refresh event are discarded.","The Order Engine module is responsible for the following:\n\n","The rice Engine module is responsible for the following:\n\n","The Cache module optimizes performance by maintaining the most recently accessed records in a fast, on-chip memory. The Cache module is responsible for the following:\n\n","The Operation FIFO module is responsible for the following:\n\n","The Refresh Queue module is configured to store refreshes that are received while another refresh is currently being processed. The Blender module may only able to check one order refresh and one price refresh at a time, which limits the number of concurrent refreshes. The SDRAM arbiter module arbitrates accesses from the order and price engines to the two SDRAM interfaces. The Blender module also constructs an outgoing enriched and normalized event using the original event and various fields created by the order and price engines (see ). It also coalesces micro-events from refreshes to create outgoing requested refresh images. The Blender module is also preferably configured to normalize conflicts between the refresh and the event stream.","If desired by a practitioner, the ONPA module's stream view output , comprising the enriched limit order events, may be transmitted directly to clients with appropriate stream view subscriptions. An Interest and Entitlement Filtering (IEF) module  can be located downstream from the ONPA module as shown in  () and (). The IEF module  can be configured to utilize the mapped symbol index to resolve the set of interested and entitled clients for the given enriched limit order event, as described in the above-referenced and incorporated U.S. Patent Application Publication 2008\/0243675. Also, for enriched limit order events  that include an interest vector field , the IEF module  can also utilize such an interest vector to identify interested and entitled clients. The filtering aspect of the IEF module may be extended to filter specific fields from enriched limit order events  based on the type of book view specified by client applications. For example, a sorted order view does not require price aggregated volume and order count fields, and the IEF module  can be configured to remove those fields from enriched limit order events for clients who want a sorted order view. Event transmission latency can be reduced and downstream network bandwidth can be conserved by omitting these fields from update events.","As shown in ), a message formatting (MF) module  can be deployed downstream from the IEF module  to format outgoing enriched limit order events destined for interested and entitled subscribers to a message format expected by those subscribers. An exemplary embodiment for the MF module  is described in the above-referenced and incorporated U.S. Patent Application Publication 2008\/0243675.","As noted above, some clients may prefer to receive a stream view comprising enriched limit order events because they will build their own sorted data structures of the order book data. Thus, in one embodiment of the invention, the output of the pipeline  shown in )-() can be transmitted to a consuming system (client machine) where it is processed by an Application Programming Interface (API) associated with the ticker plant, as shown in connection with . The API performs the sorting and presents the client application with a summary view of the data. The sorting task may be performed using a variety of techniques known in the art.","However, other clients may prefer to receive the summary view from the ticker plant itself. For additional embodiments of the invention, the inventors disclose sorting techniques that can be deployed in pipeline  implemented within the ticker plant coprocessor to create summary views of the order data. However, it should be noted that these sorting techniques could also be performed in an API as shown in  if desired by a practitioner.",{"@attributes":{"id":"p-0138","num":"0199"},"figref":["FIGS. 25(","FIGS. 25("],"i":["a","d","b","d"],"b":["1200","2500","2502","1200","1210","2402"]},"While the SVU module  can be configured to provide sorting functionality via any of a number of techniques, with a preferred embodiment, the SVU module employs sorting engines to independently maintain each side (bid and ask) of each order book. Since input order and price events only affect one side of the order book, accessing each side of the book independently reduces the potential number of entries that must be accessed and provides for parallel access to the sides of the book. Each side of the book is maintained in sorted order in physical memory. The book is \u201canchored\u201d at the bottom of the memory allocation for the book, i.e. the last entry is preferably always stored at the last address in the memory allocation. As a consequence the location of the \u201ctop\u201d of the book (the first entry) varies as the composition of the order book changes. In order to locate the top of the book, the SVU module  maintains a record that contains pointers to the top of the bid and ask side of the book, as well as other meta-data that may describe the book. The record may be located directly by using the symbol map index. Note that inserting an entry into the book moves entries above the insertion location up one memory location. Deleting an entry in the book moves entries above the insertion location down one memory location. While these operations may result in large numbers of memory copies, performance is typically good as the vast majority of order book transactions affect the top positions in the order book. Since the price AMD field  in the enriched limit order event  specifies whether or not a price entry has been inserted or deleted, the sorting engine within the SVU module  can make use of this information to make a single pass through the sorted memory array. Furthermore, since the price aggregation engine within the ONPA module maintains all volume, order count, and attribute information for each price entry in the price point records, the entries in the SVU data structure only need to store the values required for sorting.","For regional order summary views, the SVU module preferably maintains a pair of bid and ask books for each symbol on each exchange upon which it trades. The entries in each book are from one exchange. Since the order engine within the ONPA module maintains all information associated with an order, the SVU data structure only needs to maintain the fields necessary for sorting and the unique order identifier assigned by the ONPA module. In some cases, only the price and timestamp are required for sorting.","For price summary views, the SVU module preferably maintains a pair of spliced bid and ask books for each symbol. The entries in each book are from every exchange upon which the symbol trades. Since the price aggregation engine within the ONPA module maintains all information associated with a price level, the SVU data structure only needs to maintain the fields necessary for sorting (i.e. price) and the unique price identifier assigned by the ONPA module. Composite, spliced, and regional views of the price book may be synthesized from this single spliced book. Attribute filtered and price merged views of the price book may be synthesized in the same way. A price book sorting engine in the SVU module computes the desired views by aggregating multiple regional entries to produce composite entries and positions, and filters unwanted regional price entries to produce regional entries and positions. These computations are performed as the content of each book is streamed from memory through the engine. In order to minimize the memory bandwidth consumed for each update event, the engine requests chunks of memory that are typically smaller in size than the entire order book. Typically, a default memory chunk size is specified at system configuration time. Engines request the default chunk size in order to fetch the top of the book. If additional book entries must be accessed, the engines request the next memory chunk, picking up at the next address relative to the end of the previous chunk. In order to mask the latency of reading chunks of memory, processing, and requesting the next chunk of memory, multiple engines interleave their accesses to memory. As within the ONPA module, interleaving is accomplished by using a memory arbitration block that schedules memory transactions for multiple engines. Note that a time-slot memory controller may also be used. Engines may operate on unique symbols in parallel without affecting the correctness of the data.","In another embodiment of the Sorted View Update module, each side of the book is organized as a hierarchical multi-way tree. The depth of a multi-way tree is dictated by the number of child branches leaving each node. A B+ tree is an example of a multi-way tree where all entries in the tree are stored at the same level, i.e. the leaf level. Typically, the height of a multi-way tree is minimized in order to minimize the number of nodes that must be visited in order to reach a leaf node in the tree, i.e. the desired entry. An example of a B+ tree is shown in . Note that the leaf nodes may be organized as a linked list such that the entire contents of the tree may be accessed by navigating to the leftmost child and following the pointers to the next child node. This feature can be exploited in the SVU module to quickly produce a refresh event, a snapshot of the entire contents of the order book in order to initialize newly subscribed clients. Furthermore, a direct pointer to the leftmost child may be stored along with the root node in order to provide fast access to the top of the sorted book.",{"@attributes":{"id":"p-0143","num":"0204"},"figref":"FIG. 26"},"The SVU module can be configured to utilize hierarchical B+ trees to minimize the number of memory accesses required to update the various book views. As shown in , the SVU module can be configured to maintain a header record that contains pointers to the root of the composite and regional book trees. The root tree is the price aggregated tree. Each leaf of the price aggregated tree contains a pointer to a B+ tree that maintains a sort of the orders at the given price point. With such an embodiment, the SVU module preferably maintains a hierarchical B+ tree for the composite views and each regional view of the book. Note that each side of the book (bid and ask) corresponds to a hierarchical B+ tree. Note that hierarchical B+ trees may also be used in the embodiment where sorting is performed by the API on the client machine. Furthermore, note that a portion of the sorting may be offloaded to the API on the client machine. For example, construction of the spliced view of the book may be offloaded to the API by subscribing to the summary view of all regional books.","Similar to the insertion sorting engines in the previous embodiment, a parallel set of tree traversal engines can operate in parallel and interleave their accesses to memory. Furthermore, the SVU module may optionally cache recently accessed tree nodes in on-chip memory in order to further reduce memory read latency.",{"@attributes":{"id":"p-0146","num":"0207"},"figref":["FIG. 28","FIG. 28"],"b":["2500","2500"]},"The Extractor module provides the same service of extracting necessary fields for processing as described in connection with the ONPA module, and the Extractor module further propagates those fields to the Dispatcher. The Extractor module also preferably propagates the full event to the Blender module.","The Dispatcher module is responsible for fetching the header record that contains pointers to the composite and regional book trees. A cache positioned between the Dispatcher module and the SDRAM Arbiter module provides quick access to recently accessed header records. The operation FIFO module stores event fields and operational state while the Dispatcher module is waiting for memory operations to complete. This allows the Dispatcher module to operate on multiple events in parallel.","When the book pointers have been received from memory, the Dispatcher module passes the event fields and the book pointers to one of several parallel sorting engine modules. All events for a given symbol are preferably processed by the same sorting engine module, but events for different symbols may be processed in parallel by different sorting engine modules. The Dispatcher module may balance the workload across the parallel sorting engines using a variety of techniques well-known in the art. For example, the inventors have found that a random distribution of symbols across sorting engines provides an approximately even load balance on average. Note that a Dispatch Buffer module resides between the Dispatcher module and the sorting engines. This buffer maintains separate queues of pending events for each sorting engine. It reduces the probability of head-of-line blocking when a single sorting engine is backlogged. Pending events for that engine are buffered, while events scheduled for other sorting engines may be processed when the associated sorting engine is ready. The sorting engine may utilize the modified insertion sort or B+ tree sorting data structures described above. In the preferred embodiment, the B+ tree sorting data structure is used. The sorting engine is responsible for:\n\n","The sorting engines include the relative price and order position in outgoing events. For example, the sorting engines can be configured to append data fields onto the order events it processes that identify the sort position for the order and\/or price within the various books maintained by the data structure. Thus, the SVU module can create the summary views by further enriching the limit order events that it receives with sort position information for one or more books maintained by the pipeline. This positional information may be in the form of a scalar position (e.g. 3price level) or a pointer to the previous entry (e.g. pointer to the previous price level). Each sorting engine has an associated cache, operation FIFO, and refresh FIFO. The cache provides fast access to recently accessed memory blocks, which optimizes latency performance for back-to-back operations to the top of the same order book. Each sorting engine may operate on multiple events in parallel by storing in-process fields and state in its operation FIFO. Note that the sorting engines ensure correctness and data structure coherency by monitoring for accesses to the same data structure nodes. Similarly, the sorting engines incrementally process refresh events to service new client subscriptions by employing the refresh queue, similar to the ONPA module.","The output of each sorting engine is passed to the Blender module which constructs the normalized output event by blending the positional information from the sorting engine with the event fields passed by the Extractor module. Note that the Blender maintains a queue for each sorting engine that stores the pending event fields.","Level 2 updates from the ticker plant may be delivered to client applications in the form of discrete events that update the state of one or more book views presented by the client API. For summary views, the API preferably maintains a mirror data structure to the SVU module. For example, if the SVU module employs B+ trees the API preferably maintains a B+ tree. This allows the SVU module to include parent\/child pointers in the update event. Specifically, the SVU module assigns a locally unique identifier to each node in the B+ tree for the given book view. The SVU module enriches the update events with these identifiers to specify the maintenance operations on affected nodes in the data structure. For example, the update event may specify that the given price identifier be added to a given node. This allows the API to perform constant time updates to its data structure using direct addressing and prevents the need for a tree search operation.","Level 2 updates from the ticker plant may also be delivered to client applications in the form of snapshots of the top N levels of a book view, where N is typically on the order of 10 or less. N may be specified by the ticker plant or the subscribing application. In the case that the book view is natively maintained by the SVU module, the snapshot is readily produced by simply reading the first N entries in the sorted data structure. When B+ trees are used, nodes may be sized such that snapshots may be produced in a single memory read. In the case that the SVU module synthesizes the book view (such as composite or attribute filtered views), the SVU preferably reads a sufficient number of entries from the underlying sorted view to produce N entries in the synthesized view. Snapshot delivery from the ticker plant significantly reduces the amount of processing required on client systems by the API to produce the specified book views. The API simply copies the snapshot into a memory array and presents the view to the client application.","In accordance with another embodiment, the pipeline  can leverage its likely ability to generate quote messages before that quote appears in a Level 1 feed published by an exchange. As shown in , the pipeline  can be augmented to pass synthetic quote events from the SVU module to a Value Cache Update (VCU) module  that handles Level 1 traffic. With the embodiment of , the SVU module is configured with logic for detecting when an order or price book update event modifies the top of the book (best bid or offer) (e.g., new price, size, or timestamp). In response to detecting such an event, the SVU module generates a synthetic quote event that contains the new bid or ask price and aggregate order size at that price for the given regional and\/or composite view of the book for the associated symbol. The VCU module  receives this synthetic quote event, updates a Last Value Cache (LVC) data structure that contains the associated Level 1 record for the symbol and transmits a Level 1 quote message to interested and entitled subscribers. Records in the LVC data structure represent a current state of a financial instrument in the market place. As indicated above, because of the extreme low latency provided by the ONPA and SVU modules, it is expected that the SVU module will be able to recognize when order events affect the top of a book before that situation is reflected in a conventional level 1 feed from an exchange. Thus, by recognizing and delivering synthetic quote events to the VCU module , the inventors believe that the pipeline  of  is configured to generate level 1 events from a feed of level 2 events with favorably low latency.","It should be noted that an exemplary embodiment for the VCU module  is described in the above-referenced and incorporated U.S. Patent Application Publication 2008\/0243675 (e.g., see ) and () therein).","In yet another exemplary embodiment, the pipeline  of  can be further configured to include a basket calculation engine (BCE) module  downstream from the VCU module . An exemplary embodiment for the BCE module  is described in the above-referenced and incorporated U.S. Patent Application Publication 2009\/0182683. The BCE module  can be configured to operate on level 1 event resulting from the synthetic quote event to effectively drive Net Asset Value (NAV) computations for baskets of financial instruments from low latency Level 2 data. The inventors believe that this chaining of synthetic quote generation and basket calculation is capable of providing a considerable speed advantage for a number of trading strategies including index arbitrage.","The aforementioned embodiments of the pipeline  may be implemented in a variety of parallel processing technologies including: Field Programmable Gate Arrays (FPGAs), Chip Multi-Processors (CMPs), Application Specific Integrated Circuits (ASICs), Graphics Processing Units (GPUs), and multi-core superscalar processors. Furthermore, such pipelines  may be deployed on coprocessor  of a ticker plant platform  as shown in . It should also be noted that the pipelines disclosed herein can be implemented not only in the ticker plants resident in trading firms but also in the ticker plants resident within the exchanges themselves to accelerate the exchanges' abilities to create and maintain their own order books. Additional details regarding such a ticker plant platform  can be found in the above-referenced and incorporated U.S. Patent Application Publications 2009\/0182683 and 2008\/0243675. In summary, the financial market data from the exchanges is received at the O\/S supplied protocol stack  executing in the kernel space on processor  (see )-()). An upject driver  delivers this exchange data to multi-threaded feed pre-processing software  executing in the user-space on processor . These threads may then communicate data destined for the coprocessor  to the hardware interface driver software  running in the kernel space.","Instructions from client applications may also be communicated to the hardware interface driver  for ultimate delivery to coprocessor  to appropriately configure pipeline  that is instantiated on coprocessor . Such instructions arrive at an O\/S supplied protocol stack  from which they are delivered to a request processing software module . A background and maintenance processing software module  thereafter determines whether the client application has the appropriate entitlement to issue such instructions. If so entitled, the background and maintenance processing block  communicates a command instruction to the hardware interface driver  for delivery to the coprocessor to appropriately update the pipeline  to reflect any appropriate instructions.","The hardware interface driver  then can deliver an interleaved stream of financial market data and commands to the coprocessor  for consumption thereby. Details regarding this stream transfer are described in the above-referenced and incorporated U.S. Patent Application Publication 2007\/0174841. Outgoing data from the coprocessor  returns to the hardware interface driver , from which it can be supplied to MDC driver  for delivery to the client connections (via protocol stack ) and\/or delivery to the background and maintenance processing block .","While the present invention has been described above in relation to its preferred embodiments, various modifications may be made thereto that still fall within the invention's scope. Such modifications to the invention will be recognizable upon review of the teachings herein. Accordingly, the full scope of the present invention is to be defined solely by the appended claims and their legal equivalents."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0041","num":"0040"},"figref":"FIG. 2(","i":"a"},{"@attributes":{"id":"p-0042","num":"0041"},"figref":"FIG. 2(","i":"b"},{"@attributes":{"id":"p-0043","num":"0042"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 4(","i":"a"},{"@attributes":{"id":"p-0045","num":"0044"},"figref":"FIG. 4(","i":"b"},{"@attributes":{"id":"p-0046","num":"0045"},"figref":"FIG. 4(","i":"c"},{"@attributes":{"id":"p-0047","num":"0046"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0050","num":"0049"},"figref":"FIGS. 8(","i":["a","b"]},{"@attributes":{"id":"p-0051","num":"0050"},"figref":"FIGS. 9(","i":["a","b"]},{"@attributes":{"id":"p-0052","num":"0051"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0053","num":"0052"},"figref":"FIGS. 11(","i":["a","c"]},{"@attributes":{"id":"p-0054","num":"0053"},"figref":"FIGS. 12(","i":["a","c"]},{"@attributes":{"id":"p-0055","num":"0054"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0056","num":"0055"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0057","num":"0056"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0058","num":"0057"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0059","num":"0058"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0060","num":"0059"},"figref":"FIG. 18"},{"@attributes":{"id":"p-0061","num":"0060"},"figref":"FIG. 19"},{"@attributes":{"id":"p-0062","num":"0061"},"figref":"FIG. 20"},{"@attributes":{"id":"p-0063","num":"0062"},"figref":"FIG. 21(","i":"a"},{"@attributes":{"id":"p-0064","num":"0063"},"figref":"FIG. 21(","i":"b"},{"@attributes":{"id":"p-0065","num":"0064"},"figref":"FIG. 22"},{"@attributes":{"id":"p-0066","num":"0065"},"figref":"FIG. 23"},{"@attributes":{"id":"p-0067","num":"0066"},"figref":"FIG. 24"},{"@attributes":{"id":"p-0068","num":"0067"},"figref":"FIGS. 25(","i":["a","d"]},{"@attributes":{"id":"p-0069","num":"0068"},"figref":"FIG. 26"},{"@attributes":{"id":"p-0070","num":"0069"},"figref":"FIG. 27"},{"@attributes":{"id":"p-0071","num":"0070"},"figref":"FIG. 28"},{"@attributes":{"id":"p-0072","num":"0071"},"figref":"FIG. 29"},{"@attributes":{"id":"p-0073","num":"0072"},"figref":"FIG. 30"},{"@attributes":{"id":"p-0074","num":"0073"},"figref":"FIG. 31"}]},"DETDESC":[{},{}]}
