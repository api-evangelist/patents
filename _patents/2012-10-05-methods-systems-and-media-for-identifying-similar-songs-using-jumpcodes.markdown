---
title: Methods, systems, and media for identifying similar songs using jumpcodes
abstract: Methods, systems, and media for identifying similar songs using jumpcodes are provided. In some embodiments, methods for a cover song from a query song are provided, the methods comprising: identifying a query song jumpcode for the query song, wherein the query song jumpcode is indicative of changes in prominent pitch over a portion of the query song; identifying a plurality of reference song jumpcodes for a reference song, wherein each of the reference song jumpcodes is indicative of changes in prominent pitch over a portion of the reference song; determining if the query song jumpcode matches any of the plurality of reference song jumpcodes; and upon determining that the query song jumpcode matches at least one of the plurality of reference song jumpcodes, generating an indication that the reference song is a cover song of the query song.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09384272&OS=09384272&RS=09384272
owner: The Trustees of Columbia University in the City of New York
number: 09384272
owner_city: New York
owner_country: US
publication_date: 20121005
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATION","TECHNICAL FIELD","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["This application claims the benefit of U.S. Provisional Patent Application No. 61\/543,739, filed Oct. 5, 2011, which is hereby incorporated by reference herein in its entirety.","The disclosed subject matter relates to methods, systems, and media for identifying similar songs using jumpcodes.","The capability to automatically identity similar songs is a capability with many applications. For example, a music lover may desire to identify cover versions of a song in order to enjoy other interpretations of that song. As another example, copyright holders may want to be able to identify different versions of their songs, copies of their songs, etc., in order to insure proper copyright license revenue. As yet another example, users may want to be able to identify songs with similar sound to a particular song. As still another example, a user listening to a particular song may desire to know the identity of the song or artist performing the song.","While it is generally easy for a human to identify two songs that are similar, automatically doing so with a machine is much more difficult. For example, the two songs can be played in a different key, such that conventional fingerprinting is not accurate. As another example, the two songs can be played at different tempos. As yet another example, a performer playing a cover version may add, remove or rearrange parts of the song. All of this can make it hard to identify a cover version of a song. With millions of songs readily available, having humans compare songs manually is practically impossible. Therefore, there is a need for mechanisms that can automatically identify similar songs.","Methods, systems, and media for identifying similar songs using jumpcodes are provided. In accordance with some embodiments, methods for identifying a cover song from a query song, the methods comprising: identifying, using a hardware processor, a query song jumpcode for the query song, wherein the query song jumpcode is indicative of changes in prominent pitch over a portion of the query song; identifying, using the hardware processor, a plurality of reference song jumpcodes for a reference song, wherein each of the reference song jumpcodes is indicative of changes in prominent pitch over a portion of the reference song; determining, using the hardware processor, if the query song jumpcode matches any of the plurality of reference song jumpcodes; and upon determining that the query song jumpcode matches at least one of the plurality of reference song jumpcodes, generating, using the hardware processor, an indication that the reference song is a cover song of the query song.","In accordance with some embodiments, systems for identifying a cover song from a query song, the systems comprising: a hardware processor that: identities a query song jumpcode for the query song, wherein the query song jumpcode is indicative of changes in prominent pitch over a portion of the query song; identifies a plurality of reference song jumpcodes for a reference song, wherein each of the reference song jumpcodes is indicative of changes in prominent pitch over a portion of the reference song; determines if the query song jumpcode matches any of the plurality of reference song jumpcodes; and upon determining that the query song jumpcode matches at least one of the plurality of reference song jumpcodes, generates an indication that the reference song is a cover song of the query song.","In some embodiments, non-transitory computer-readable media containing computer-executable instructions that, when executed by a processor, cause the processor to perform a method for identifying a cover song from a query song, the method comprising: identifying a query song jumpcode for the query song, wherein the query song jumpcode is indicative of changes in prominent pitch over a portion of the query song; identifying a plurality of reference song jumpcodes for a reference song, wherein each of the reference song jumpcodes is indicative of changes in prominent pitch over a portion of the reference song; determining if the query song jumpcode matches any of the plurality of reference song jumpcodes; and upon determining that the query song jumpcode matches at least one of the plurality of reference song jumpcodes, generating an indication that the reference song is a cover song of the query song.","In accordance with various embodiments, mechanisms for identifying similar songs using jumpcodes are provided. These mechanisms can be used in a variety of applications. For example, cover songs of a query song can be identified. A cover song can include a song performed by one artist that is a version of a song performed by another artist or the same artist at a different time. As another example, similar songs (e,g., two songs with similar sounds, whether unintentional (e.g., due to coincidence) or intentional (e.g., in the case of sampling, copying, or through the creation of a derivative work such as a song parody) can be identified. As yet another example, different songs with common, distinctive features can be identified (e.g., songs from a similar performer, the same performer, a similar style, etc.) for recommending songs to a user by identifying features of a query song. As a still further example, a song being played can be identified (e.g., the mechanisms described herein can allow a user to identify the name of a song on the radio, or the name of a song being, played live by the original performer another performer such as a cover band).","In some embodiments, these mechanisms can receive a song or a portion of a song. For example, songs can be received from a storage device, from a microphone, or from any other suitable device or interface. Beats in the song can then be identified. By identifying beats in the song, variations in tempo between two songs (e.g., between an original recording and a cover) can be normalized. Beat-level descriptors in the song can then be generated using any suitable techniques, as described below in connection with , for example. It should be noted that references to song herein are intended to encompasses a full song as well as portions of a song.","In some embodiments, chroma vectors can be extracted from the song in accordance with musical segments of the song or based on the time periods. A chroma vector can be characterized as having a bin that corresponds to each of twelve semitones (e.g., piano keys) within an octave formed by folding all octaves together (e.g., putting, the intensity of semitone A across all octaves in the same semitone bin I, putting the intensity of semitone B across all octaves in the same semitone bin , putting the intensity of semitone C across all octaves in the same semitone bin , etc.). The semitone bins of a chroma matrix can be numbered from one to twelve such that the lowest pitched semitone can be labeled as bin , and the highest pitched semitone can be labeled as bin . These chroma vectors can then be averaged over each beat to create a beat-level feature array of beat-synchronized chroma vectors.","Chroma vectors can be extracted from a song, from a portion of a song, or from any portion of audio in any suitable way. For example, in some embodiments, an application such as The Echo Nest analyzer API (available at the web page of The Echo Nest, e.g., the.echonest.com) can be used to extract chroma vectors among other information from a portion of audio such as a song or a portion of a song. In some embodiments, the processes described below in connection with  can be used to extract beat synchronized chroma vectors from a portion of audio.","In some embodiments, the beat-synchronized chroma vectors can be normalized. As described below, any suitable technique can be used to normalize chroma vectors.","In some embodiments, normalized beat-synchronized chroma vectors can be averaged over two successive beats to reduce the amount of information to be processed by reducing the number of chroma vectors used to represent a song by half.","In some embodiments, landmarks can be found from an array of normalized (and\/or averaged) beat-synchronized chroma vectors (e.g., a normalized, beat-synchronized chroma matrix). Landmarks can represent prominent pitch information from a chroma vector. For example, if a semitone corresponding to bin  is a prominent semitone in a particular chroma vector, this can be set as a landmark. In some embodiments, there can be more than one landmark allowed at each time frame (e.g., at each beat, averaged over two beats, etc.). In some embodiments, the position of a landmark can be specified by a time-chroma coordinate (time, chroma). For example, a landmark located in bin  at time frame  can be specified as a landmark at (, ). An example of a process for finding landmarks from a beat-synchronized chroma matrix is described below in connection with .","In some embodiments, the extracted landmarks can be used to create jumpcodes that are indicative of changes in landmarks over a window of time frames and\/or beats in a portion of audio, such as a song or a portion of a song. In some embodiments, jumpcodes from a query song can be compared to jumpcodes from one or more reference songs to determine a similarity between the query song and the reference song. For example, a number of jumpcodes common to both a query song and a reference song can be calculated and a song that contains common jumpcodes can be presented as a similar song.","As used herein, a jumpcode can specify changes in prominent pitch of a song over a small portion of the song (e.g., two beats, four beats, five beats, etc. of the song) from an initial prominent pitch at the beginning of the small portion of the song.","Turning to , an example  of a process for comparing a query song  (or portion of a query song) to one or more reference songs in accordance with some embodiments is shown. As shown, process  can include extracting a chroma matrix from a song at , generating a beat-synchronized chroma matrix from the extracted chroma matrix at , setting and storing landmarks based on the beat-synchronized chroma matrix at , calculating and storing jumpcodes in a song database  at , at  comparing jumpcodes calculated at  to jumpcodes stored at , and outputting, results of the comparison  that indicate whether reference songs stored in database  are similar to the query song.","As shown, at  a chroma matrix can be extracted from a song . The chroma matrix can be extracted using any suitable technique, such as, by using The Echo Nest analyzer API, using the processes described in connection with , or any other suitable technique.","At , a vat-synchronized chroma matrix can be generated from the chroma matrix extracted at . In some embodiments, generating a beat-synchronized chroma matrix can include averaging chroma vectors over each beat to create beat-synchronized chroma vectors. Additionally or alternatively, generating a beat-synchronized chroma vectors can include normalizing beat-synchronized chroma vectors using any suitable technique. As one example, the techniques described in connection with  can be used to normalize the beat-synchronized chroma vectors.","In some embodiments, at , successive beat-synchronized chroma vectors can be averaged together. This can allow for the amount of information used to represent a song to be decreased by reducing the number of chroma vectors used to represent the song.","At , landmarks can be set and stored based on the chroma vectors in the beat-synchronized chroma matrix generated at  in some embodiments, landmarks can be indicative of a prominent chroma bin at a specific point in time. For example, a threshold can be set based on prominent chroma bins in some portion of a song, and this threshold can be used in determining other prominent chroma bins in other portions of the song.","In some embodiments, landmarks can be set on both a forward pass through a song (e.g., from the beginning of the song toward the end) and on a backward pass through the song (e.g., from the end of the song toward the beginning). Additionally, the intersection of the landmarks set on the forward pass and landmarks set on a backward pass can be stored as landmarks. In such an embodiment, landmarks that are not common to both the forward pass can be ignored, discarded, etc.","At , jumpcodes for the song can be calculated using the landmarks stored at . In some embodiments, jumpcodes can e calculated by determining a difference between successive landmarks within a time window. The time window can then be moved to a different portion of the song (e.g., moved forward through the song) and additional jumpcodes can be calculated for the landmarks within the moved time window. This can be repeated until the time window is moved through the entire song and jumpcodes are calculated for each position of the time window.","In some embodiments, when jumpcodes are calculated for a song and there are duplicate jumpcodes (e.g., more than one of the same jumpcode is calculated for different portions of the song), a weight can be calculated fin the duplicate jumpcodes based on the number of duplicate jumpcodes in the song.","In some embodiments, the extracted jumpcodes (or the weighted extracted jumpcodes) for a reference song, and\/or a query song can be stored in a database . In some embodiments, the position of specific jumpcodes in a song may not be preserved when the jumpcodes are stored in database . Additionally or alternatively, the key of the stored jumpcodes can be transposed by rotating the jumpcodes about the chroma axis. The jumpcodes for each song can be stored in database  in the original key of the song and in each of the eleven other keys represented in the twelve se In tones of the chroma vectors after transposition of the jumpcodes into those eleven other keys. Any suitable technique can be used to transpose the jumpcodes into other keys, such as the techniques described in connection with .","At , extracted jumpcodes from song  (or a portion of song ) can be compared to jumpcodes of other songs (or portions of songs), such as jumpcodes previously stored in database . The results of the comparison can be presented at  in any suitable fashion, such as presentation on a display of a computing device.","In some embodiments, the mechanism described herein can include a process , shown in  that takes a chroma matrix for a song as an input and either adds jumpcodes calculated from the chroma matrix to database  and\/or compares jumpcodes calculated from the chroma matrix to jumpcodes stored in database .",{"@attributes":{"id":"p-0048","num":"0047"},"figref":["FIG. 2A","FIG. 1"],"b":["200","120"]},"At , a beat-synchronized chroma matrix can be venerated for the chroma matrix from a reference song . In some embodiments, the beat-synchronized chroma matrix can be generated in accordance with the techniques described in connection with .","In some embodiments, chroma matrix  can be received as a beat-synchronous chroma matrix. In such an embodiments,  can be omitted and process  can proceed to .","At , landmarks can be set and stored based on a reference song beat-synchronized chroma matrix. In some embodiments, the landmarks can be set and stored based on the reference song-beat synchronized chroma matrix generated at . In some embodiments, landmarks can be set and stored in accordance with the techniques described in connection with . In some embodiments, the landmarks can be stored in a database in association with the song that the landmarks are derived from. Additionally or alternatively, the landmarks can be stored in memory for use in generating jumpcodes. In some embodiments, landmarks can be set and stored in accordance with the techniques described in connection with .","At , jumpcodes can be calculated and weighted from landmarks for a reference song being analyzed. In some embodiments, jumpcodes can be calculated and\/or weighted in accordance with the techniques described herein. More particularly, jumpcodes can be calculated from landmarks and\/or can be weighted in accordance with the techniques described in connection with  and .","At , jumpcodes and weights associated with the jumpcodes can be stored in a database as reference song jumpcodes. In some embodiments, the jumpcodes that are calculated at  can be transposed into other keys using techniques described herein. For example, the jumpcodes can be transposed using the techniques described in connection with . These transposed jumpcodes can also be stored in a database in association with the reference song jumpcodes from which they were derived. As described below in connection with , each jumpcode stored in the database can be stored as a single value calculated using a hash function with the jumpcode as an input. This can allow for direct comparison between different jumpcodes by comparing a single value rather than a list of values. Further, it can allow for the jumpcodes to be stored in the database efficiently.","In some embodiments, weighted jumpcodes can be stored in a database in association with identification information of the audio from a song that the jumpcodes correspond to. For example, the weighted jumpcodes can be stored in a database along with a corresponding identifier. In a more particular example, in the case of a known song, the artist, title, song writer, etc. can be stored in association with the weighted jumpcodes. In another example, a URL of a video of an unknown song can be stored in association with the weighted jumpcodes. In yet another example, identifying information about an unknown song, such as a source and\/or a location of the unknown song, can be stored in association with the jumpcodes corresponding to the unknown song.","In some embodiments, weighted jumpcodes can be extracted from a collection of known songs. For example, a content owner (e.g., a record company, a performing rights organization, etc.) can extract and weight jumpcodes from songs in the content owner's collection of songs and store re the weighted jumpcodes in a database. In such an example, information (e.g., title, artist, song writer, etc.) identifying the songs can be associated with the weighted jumpcodes.","In some embodiments, the order in time of the jumpcodes in a song is not preserved. Because the specific order of the jumpcodes is not preserved, an identification of songs that contain similar portions to a query song based on a comparison of jumpcodes can be independent of the order in which the jumpcodes are generated from the query song and the reference song. For example, the identification can recognize a query song, that is a medley containing a part of a reference song as being a version of the reference song. In another example, a reference song that contains parts of a query song arranged in a different order than those parts are arranged in the query song can be recognized as being, a version of the query song.",{"@attributes":{"id":"p-0057","num":"0056"},"figref":["FIG. 2B","FIG. 1"],"b":["250","120"]},"Process  can begin by calculating query song jumpcodes from a chroma matrix  for a query song at -, which can be performed as described above in connection with - of process  for calculating jumpcodes of a reference song.","At , weighted jumpcodes calculated at  can be compared to the reference song jumpcodes that were stored in the database at  using process . Additionally, the jumpcodes of the query song can be compared to transposed jumpcodes stored in the database at  in some embodiments.","In some embodiments, all jumpcodes in the database that match at least one jumpcode from the query song can be determined along with the weight corresponding to each of the matching reference jumpcodes.","At , reference songs that are considered similar o the query song can be determined based on the number of reference song jumpcodes from each reference song that match one of the jumpcodes from the query song. In some embodiments, a weighted jumpcode from a reference song can be determined to match a jumpcode from the query song if the weight of the reference jumpcode is within a window, \u03b1, around the weight of the query song, jumpcode. For example, a reference jumpcode can be considered to match a query jumpcode if the weight of the reference jumpcode, w, meets the following conditions: (1\u2212\u03b1)w<w<(1+\u03b1)w, where w can be calculated based on a comparison of the number of times the particular jumpcode appears in a song compared to he total number of jumpcodes in the song. In one particular example, w can be calculated as the total number of times a particular jumpcode appears in a song divided by log (base ) of the total number of jumpcodes in the song. This can diminish the importance of jumpcodes that appear more often if they appear in a larger pool.","In some embodiments, all matching jumpcodes can be determined and the number of jumpcodes associated with a particular reference song that match jumpcodes in the query song can be calculated. A total number of matching jumpcodes from a reference song can be used to rank the reference songs according to how similar they are to the query song. For example, a reference song with more matching jumpcodes can be considered more similar to the query song and, therefore, can be ranked higher than a reference song with less matching jumpcodes. In some embodiments, the mechanisms described herein can identify a reference song as matching when the reference song has a threshold number of matching jumpcodes. For example, if a reference song shares fifty percent (or 75%, or any other suitable threshold) or more of the same jumpcodes as the query song, the mechanisms described herein can identify the reference song as being a matching reference song.","When reference song jumpcodes corresponding to some portion of reference songs stored in the database (including all of the reference songs) have been compared at  to the query song jumpcodes, the results of the comparison can be output at .","In some embodiments, the processes of  can be used together to identify similar songs using process  by comparing query song jumpcodes to jumpcodes stored in a database in accordance with process . For example, a content distributor (e.g., a radio station, a music retailer, etc.) can extract and weight jumpcodes from songs made available by the content distributor. In such an example, information (e.g., title, artist, song writer, etc.) identifying the songs can be associated with the weighted jumpcodes. Further, a query song can be input and songs similar to the query song that are available from the content distributor can be recommended to a user.","In some embodiments, weighted jumpcodes can be extracted from a collection of unknown songs. For example, a user can calculate and weight jumpcodes from soundtracks to videos uploaded (by the user and\/or other users) to a video sharing Web site YOUTUBE). In such an example, information identifying the source of the soundtrack. (e.g., a URL, a username, a reference number, etc.) can be associated with the weighted jumpcodes. The information identifying the source of the soundtracks and associated soundtrack jumpcodes can be used to create a collection of unknown songs. A user can then input a query song and search for different versions of the query song by comparing query song jump:odes to the soundtrack jumpcodes associated with the collection of unknown songs.",{"@attributes":{"id":"p-0066","num":"0065"},"figref":["FIG. 3","FIGS. 10-16"],"b":["300","302","302","300","106","204","254"]},"At , chroma vectors corresponding to each music event can be generated or received and the song can be partititioned into beats. An example, of a musical event can include each time there is a change in pitch in the song. In some embodiments, whenever there is a musical event, a chroma vector can be calculated. Musical events can happen within a beat or can span beats. In some embodiments, the chroma matrix  can already be partitioned into beats, for example, by The Echo Nest analyzer API. Other techniques for partitioning a chroma matrix into beats are described below with reference to .","At , chroma vectors received at  can be averaged over each beat to obtain beat-synchronized chroma vectors. Any suitable technique can be used to average chroma vectors over a beat, including techniques for averaging chroma vectors described below in connection with .","At , the beat-synchronized chroma vectors can be normalized. In some embodiments, the value in each chroma bin of a beat-synchronized chroma vector can be divided by the value in the chroma bin having a maximum value. For example, if chroma bin  of a particular chroma vector has a maximum value of chroma bins  through , then the value of each of chroma bins  through  can be divided by the value of chroma bin . This can result in the maximum value in a chroma bin being equal to one for the normalized beat-synchronous chroma vectors. At , normalized beat-synchronous chroma vectors can be averaged over pairs of successive beats to obtain a beat-synchronized chroma matrix for the chroma matrix , and a beat-synchronized chroma matrix  can be output for use by the identification application.","Turning to , an example  of a process for generating landmarks for a song from a beat-synchronized chroma matrix  in accordance with sonic embodiments is illustrated. At , an initial threshold vector T, where i=1, . . . , 12, can be found. In some embodiments, the initial value for each value of the vector Tcan be the maximum in each chroma dimension (e.g., in each chroma bin) over the first n time frames (e.g., over the first ten time frames), where a time frame is representative of one or more beats. For example, the initial threshold vector Tcan take the maximum value in each chroma dimension i over the first ten time frames, where each time frame is two beats long. In accordance with some embodiments, process  can be used to set and store landmarks at ,  and\/or .","At , a first time frame of the beat-synchronized chroma matrix  can be set as a current time frame t. At , a chroma bin c can be set as a landmark if the chroma value, v, in bin c is above threshold T, and the landmark at the location of chroma bin c, at time t can be stored. For example, in a first time frame a chroma bin  can be set as a landmark if the value, v, in chroma bin  is over threshold T.","In some embodiments, the landmarks identified at  can be stored and the location of the stored landmarks within the song can be identified with a set of coordinates (e.g., (time, chroma)), where time is the time frame location of the landmark, and chroma is the chroma bin of the landmark. Additionally, in some embodiments, a value v can be stored with the landmark for use in deciding which landmarks to use in a case where over a maximum number of landmarks were identified at any one time frame.","At , the threshold vector can be updated based on a landmark set at time t. More particularly, the threshold vector can be updated such that T=v, where v is the value in bin c, at time frame t. Returning to the preceding example, if chroma bin  is set as a landmark at time frame one, the threshold vector can be updated by setting the value at dimension  equal to the value of chroma bin  at time frame one as follows: T=v. The threshold vector value for any chroma dimension that is not set as a landmark can be left unchanged.","At , the current time frame t can be moved to time frame t+1 and the threshold vector can be set for time frame t+1 as follows: T=\u03a8T, where \u03a8 is a decay factor. In some embodiments, \u03a8 can be set any suitable value (e.g., 0.95, 0.995, etc.), where a decay value closer to one may result in less chroma bins being set as landmarks.","At , it can be determined if the current time frame, t, is the final time frame of the pass. For example, if the current pass is a forward pass that started at the first time frame at , it can be determined whether the current time frame is the last time frame of the beat-synchronized chroma matrix . If the current time frame is not the last time frame (\u201cNO\u201d at ), process  can return to . Otherwise, if the current time frame is the last time frame (\u201cYES\u201d at ), process  can proceed to .","At , it can be determined whether landmarks have been stored for a forward pass and a backward pass of the beat-synchronized chroma matrix . If landmarks have not been stored for both a forward pass and a backward pass (\u201cNO\u201d at ), the identification application can proceed to . Otherwise, if landmarks have been stored for both a forward and backward pass (\u201cYES\u201d at ), the identification application can proceed to .","At , last frame of the beat-synchronized chroma matrix can be set as a current frame and a backward pass of the beat-synchronized chroma matrix can be initiated to set landmarks for a backward pass. Process  can return to  to complete the backward pass and process  can proceed to  when landmarks have been stored for both a forward and a backward pass as determined at .","At , the landmarks set on the forward pass and the landmarks set on the backward pass can be compared and the intersection can be kept. More particularly, landmarks that were stored on both the forward pass and the backward pass can be stored as landmarks. In some embodiments, landmarks that were stored on either the forward pass or the backward pass but were not set for both can be ignored or discarded. In some embodiments, landmarks  can be generated for the beat-synchronized chroma matrix  to be used in generating jumpcodes.","In some embodiments, a maximum number of landmarks (e.g., one, two, three, etc.) can be set such that no more than the maximum number of landmarks are kept for any one time frame. More particularly, the most prominent landmarks (up to the maximum number of landmarks) at a particular time frame from among the landmarks identified in process  can be kept. For example, if the maximum number of landmarks is set as two and three landmarks are set at a particular time frame, the two most prominent landmarks can be chosen and kept. In such an example, the value, v, corresponding to each of the landmarks can be checked and the landmarks with the highest corresponding values can be kept as landmarks.",{"@attributes":{"id":"p-0080","num":"0079"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0081","num":"0080"},"figref":"FIG. 6","b":["600","602","420","12"]},"In some embodiments, a jumpcode in a time window of size W can correspond to time differences (where time can be measured synchronously in units of time, e.g., in seconds, milliseconds, etc., or can be measured asynchronously in units of beats) and semitone differences between landmarks in the time window of size W. The difference in semitones between two landmarks can be found by subtracting a chroma bin for a first landmark from a chroma bin for a second landmark. Because this can result in a negative number the result of the subtraction (modulo ) can be used to ensure that the difference will end up in one of the chroma bins being used.","At , it can be determined whether the time window of size W includes the last time frame of the song being analyzed. If the time window of size W does not include the last time frame (\u201cNO\u201d at ), the time window of size W can be advanced toward the end of the song. Otherwise, if the time window of size W includes the last time frame (\u201cYES\u201d at ), process ( can proceed to  where the jumpcodes can be hashed to create a single value corresponding to each jumpcode. In some embodiments, the time window of size W can be advanced by a specified number of time frames at  (e.g., one time frame, two time frames, etc.) between finding jumpcodes at .","Jumpcodes calculated at  can be encoded using the difference in time and the difference in chroma bin of successive landmarks in a time window W. For example, if there are three landmarks located at (,), (, ) and (, ) in a time window IV (where W in this example is at least five time frames), a jumpcode for time window W can be calculated by finding the differences in location between the successive landmarks. In such an example, a difference between landmark one and landmark two can be found by subtracting the time coordinate of landmark one from the time coordinate of landmark two as follows: 201\u2212200=1. As described above, a difference in chroma bins can be found as follows: 7\u22122=5. The time difference between the second landmark and the third landmark can be found similarly, with the time difference being equal to three and the chroma bin being equal to eight as follows: 3\u22127=8. Finally, a jumpcode for the current position of the time window of size W can be specified by the initial chroma bin and the differences between the successive pairs of landmarks, as follows: {2, ((1,5), (3,8))}. Landmark pairs encoded as jumpcodes can provide information relating to musical objects, such as, chords and changes in the melody line.","In some embodiments, where a single time frame in a time window of size W has more than one landmark, one jumpcode can be calculated for each possible combination of landmarks involving each of the landmarks at each particular time frame within time window of size W. For example, if size W is five and there are three time frames within time window of size W that each have one landmark (with the other two time frames having zero landmarks), then a single jumpcode can be calculated for the particular time frames within the time window of size W. In another example, if size W is five and there are two time frames within time window of size W that have one landmark each and a single time frame that has two landmarks, then two jumpcodes can be calculated for the particular time frames within the time window of size W.","At , the jumpcodes calculated at - for the landmarks  can be hashed to create a single value H for each of the jumpcodes. Given a set of k (time, chroma) landmarks, (t,c), (t,c), . . . (t,c) (or using the notation above {c, ((t\u2212t, c\u2212c), . . . (t\u2212t,c\u2212c)}), within a time window of size W, arithmetic and delta coding can be used to store the jumpcode in a single value as follows:",{"@attributes":{"id":"p-0087","num":"0086"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"H","mo":"=","mrow":{"msub":{"mi":"c","mn":"1"},"mo":"+","mrow":{"mn":"12","mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msub":[{"mi":"t","mn":"2"},{"mi":"t","mn":"1"}],"mo":"-"}},{"mi":"W","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msub":[{"mi":"c","mn":"2"},{"mi":"c","mn":"1"}],"mo":"-"}},{"mn":"12","mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msub":[{"mi":"t","mn":"3"},{"mi":"t","mn":"2"}],"mo":"-"}},{"mi":"W","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msub":[{"mi":"c","mn":"3"},{"mi":"c","mn":"2"}],"mo":"-"}},{"mn":"12","mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msub":[{"mi":["t","k"]},{"mi":"t","mrow":{"mi":"k","mo":"-","mn":"1"}}],"mo":"-"}},{"mi":"W","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["c","k"]},{"mi":"c","mrow":{"mi":"k","mo":"-","mn":"1"}}],"mo":"-"}}}}],"mo":"+"}}}],"mo":["+","+"],"mi":"\u2026"}}}],"mo":"+"}}}],"mo":"+"}}}],"mo":"+"}}}}}},{"mrow":{"mo":["(",")"],"mn":"1"}}]}}}},"br":{},"b":["12","3","7"]},"At , a value H for each of the jumpcodes calculated at  and hashed at  can be counted to determine how often each particular hash code appears in the song being analyzed. The counts of the each value of H can then be used to create a weight, w, for each value H, where w is indicative of the frequency that the particular hash code appears in the song associated with the jumpcode, in some embodiments, w can be a number of times that the particular value, H, appears in the song, divided by log (base ) of the total number of jumpcodes in the song. Process  can output weighted jumpcodes  for the song being analyzed. Weighted jumpcodes  can be stored in a database in association with an identification of the song that the jumpcodes were extracted from, or can be used in a comparison with weighted jumpcodes stored in a database of known and\/or unknown songs.","Additionally, each of the jumpcodes in a database can be transposed and the transposed jumpcodes can be stored in the database together with the original jumpcodes. This can allow for jumpcodes to be calculated as though the song was performed in another key. To transpose a jumpcode stored as a hash value into another key, first the initial chroma value for the jumpcode cof equation (1)) can be extracted using the modulo operation H. The result of the modulo operation to H can then be added to a rotation value between 0 and 11 (modulo ) prior to recombination. More particularly the transposed value of H, H, can be found using the following formula:\n\n, \u2003\u2003(2)\n\nwhere T is the number of semitones that over which the song is to be transposed. As H=c, equation (2) can be rewritten as follows:\n\n\u2003\u2003(3)\n\nIn this way, each song can be associated with jumpcodes in keys corresponding to each semitone. This can allow the identification application to identify songs that are a different version of the query song performed in a different key.\n",{"@attributes":{"id":"p-0090","num":"0089"},"figref":"FIG. 7A","b":["700","700","710","710","710","708","706","705","702"]},"System  can include one or more servers . Server  can be any suitable server for providing access to or a copy of the application, such as a processor, a computer, a data processing device, or any suitable combination of such devices. For example, the application can be distributed into multiple backend components and multiple frontend components or interfaces. In a more particular example, backend components, such as data collection and data distribution can be performed on one or more servers .","More particularly, for example, each of the computing devices  and server  can be any of a general purpose device such as a computer or a special purpose device such as a client, a server, etc. Any of these general or special purpose devices can include any suitable components such as a hardware processor (which can be a microprocessor, digital signal processor, a controller, etc.), memory, communication interfaces, display controllers, input devices, etc. For example, computing device  can be implemented as a smartphone, a tablet computer, a personal data assistant (PDA), a personal computer, a laptop computer, a multimedia terminal, a special purpose device, a game console, etc.","Referring back to , communications network  can be any suitable computer network including the Internet, an intranet, a wide-area network (WAN), a local-area network (LAN), a wireless network, a digital subscriber line (DSL) network, a frame relay network, an asynchronous transfer mode (ATM) network a virtual private network (VPN), or any suitable combination of any of such networks. Communications links  and  can be any communications links suitable for communicating data between mobile devices  and server , such as network links, dial-up links, wireless links, hard-wired links, any other suitable communications links, or any suitable combination of such links. Mobile devices  can enable a user to execute the application that allows the features of the mechanisms to be used. Computing devices  and server  can be located at an suitable location.","System  can also include a content owner server  that can include hardware similar to server . Content owner server  can be operated by, for example, a record company, a copyright licensing organization, etc. In some embodiments, content owner server  can use songs owned by the content owner\u2014or a party associated with the content owner, such as an agent, a copyright licensing organization, etc.\u2014as query songs. Using the mechanisms described herein the content owner can automatically search for cover versions of the songs that are owned by the content owner. For example, the content owner can search a database of songs available using content server . Content server  can be a server, or multiple servers, that are part of a service (e.g., YOUTUBE, etc.) allowing users to upload user generated content (including content copied from another source by a user, not only content created by a user). Using the mechanisms described herein can allow the content owner to search a database containing unknown songs for alternate versions of a song owned by the copyright owner.","In some embodiments, a party providing a service associated with content saver  can maintain a database of beat-synchronized chroma matrices and\/or jumpcodes of songs uploaded to content server . Content server  can then allow users to input a query song, and the content server can identify different versions of the song and\/or similar songs to the user. This can be provided as part of a service to all users and\/or as a service to content owners and copyright licensing organizations, such as BMI or ASCAP.",{"@attributes":{"id":"p-0096","num":"0095"},"figref":["FIG. 7B","FIG. 7A","FIG. 7B"],"b":["700","710","712","714","716","718","718","712"]},"Hardware processor  can use the computer program to present on display  an interface that allows a user to interact with the application and to send and receive data through communication link . It should also be noted that data received through communications link  or any other communications links can be received from any suitable source. In some embodiments, hardware processor  can send and receive data through communication link  or any other communication links using, for example, a transmitter, receiver, transmitter\/receiver, transceiver, or any other suitable communication device. Input device  can be a computer keyboard, a cursor-controller, dial, switchbank, lever, touchscreen, or any other suitable input device as would be used by a designer of input systems or process control systems.","Server  can include hardware processor , display , input device , and memory , which can be interconnected. In some embodiments, memory  can include a storage device for storing data received through communications link  or through other links, and also receives commands and values transmitted by one or more users. The storage device can further include a server program for controlling hardware processor .","In one particular embodiment, the application can include client-side software, hardware, or both. For example, the application can encompass a computer program written in a programming language recognizable by the computing device executing the application (e.g., a program written in a programming language, such as, Java, C, Objective-C, C++, C#, Javascript, Visual Basic, or any other suitable approaches).","In some embodiments, the identification application with a user interface and mechanisms for identifying similar songs, and other functions, can be delivered to computing device  and installed, as illustrated in the example shown in . The computing device  can be, for example, a mobile computing device such as a mobile phone or a tablet computer. At , weighted jumpcodes for one or more reference songs can be stored in a database by server . In one example, the extraction and weighting of the jumpcodes can be done by server . In another example, the extraction and weighting of the jumpcodes can be performed using any suitable device and can be uploaded to server  in any suitable manner. At , the weighted jumpcodes stored at  can be transmitted to computing, device  as part of the application for utilizing the mechanisms described herein. It should be noted that transmitting the application to the computing device can be done from any suitable device and is not limited to transmission from server . It should also be noted that transmitting the application to computing, device  can involve intermediate steps, such as, downloading the application to a personal computer or other device, and\/or recording the application in memory or storage, such as a hard drive, a flash memory, a SIM card, a memory card, or any other suitable device for temporarily or permanently storing an application.","Computing device  can receive the application and weighted jumpcodes from server  at . After the application is received at computing device , the application can be installed and can be used to receive audio data for a query song  at  as described herein. The application executing on computing device  can extract jumpcodes from the query song  at  in accordance with process  and can compare the query jumpcodes to the reference jumpcodes in accordance with  of process  and determine if there is a match at  in accordance with  of process , and generate and output results at  in accordance with  of process  if matching songs are determined from the database (\u201cYES\u201d at ). If the identification application running on computing device  determines that there are no matching jumpcodes (e.g., the number of matching jumpcodes is not over a threshold), the identification application can cause a message to be output that there were no similar songs in the database of reference songs.","In some embodiments, the identification application with a user interface and mechanisms for receiving query song data (e.g., audio data for a song or a portion of a song) and transmitting query song data, and other user interface functions can be transmitted to computing device  (e.g., a mobile computing device), but the jumpcodes for the reference songs can be kept on server , as illustrated in the example shown in . Similarly to the example in , at , extracted and weighted jumpcodes can be stored in a database in accordance with the mechanisms described herein. Server  can transmit the identification application (or at least a portion of the identification application) to computing device  at . Computing device  can receive the application at , and start receiving and transmitting query song data (e.g., audio data for a query song) to server  at . In some embodiments, audio data is transmitted to server . Additionally or alternatively, chroma vectors, a chroma matrix, a beat-synchronized chroma matrix, weighted jumpcodes, or any other suitable data about the query song, can be received and\/or generated by computing device  and can be transmitted to server  at . Computing device  can proceed to , where computing device  can receive a list of similar songs (e.g., one or more songs identified as similar to the query song) from server , and proceed to .","At , server  can receive query song data (e.g., audio, weighted jumpcodes, etc.) from computing device , extract weighted jumpcodes in accordance with  of process  (if computing device  has not already done so), and compare the query song jumpcodes to reference song jumpcodes in accordance with  of with process . Server  can determine if there is a match between the query song jumpcodes and the reference song jumpcodes at  in accordance with  of process , and if there are any acceptable matches (e.g., number of weighted matching jumpcodes is over a threshold) proceed to . If there is not a match at , server  can return to  and continue to receive query song data transmitted from mobile device .","At , server  can generate an alert based on the presence of a match between the query song jumpcodes extracted at  and reference song jumpcodes stored at , and can transmit a list of similar reference songs to computing device  (this can include an indication that no similar songs were found among the reference songs). In some embodiments, server  can transmit audio and\/or video of the similar songs (or a link to audio and\/or video of the similar songs) at . After receiving and transmitting query song data at , computing, device  can proceed to  where it can be put into a state to receive a list of similar songs from the server, and can move to  to check if a list of songs has been received from server . If a list of similar songs has been received (\u201cYES\u201d at ), computing device  can proceed to  where it can provide the list of similar songs to a user of the computing device in accordance with process  and\/or process . If a list of similar songs has not been received (\u201cNO\u201d at ), computing device  can output a message to a user to inform the user that no similar songs were found and process  can return to  where it can receive and transmit query song data.","In some embodiments, a hybrid process can combine conventional song fingerprinting and the mechanisms described herein. Such a hybrid process can be used to identify similar songs based on a query song. In such a hybrid process, a user can record a portion of a song for identification using a mobile device, for example. That portion can be used in a song fingerprinting process to attempt to find an close match to the query song among songs in a database. Song fingerprinting, can include generating landmarks in a specific sequence (e.g., preserving the timing of the landmarks) and attempting to identify the song from the portion of the song being analyzed by matching the specific sequence of landmarks to a specific sequence of landmarks in a known song.","In a hybrid process, a database of reference songs used for fingerprinting can include jumpcodes for each the reference songs that have been calculated in accordance with the processes described herein. In the hybrid process the portion of the recorded audio can be transmitted to a server for a fingerprinting analysis that can compare the query song to a database of songs. Alternatively, a portion of the processing for fingerprinting analyses can be performed on a user's computing device to generate the landmarks used for fingerprinting.","The portion of audio can be used for fingerprinting, and based on the fingerprinting it can be determined if any reference songs in the database match the query song. If a reference song matches the query song, it can be determined if the matching song is a known song (e.g., whether a title and artist for the song are known). If the title and artist for the matching song are unknown, the jumpcodes stored in association with the song in the database can be used to search the database for similar songs using the mechanisms described herein.","In some embodiments, if the song identified by fingerprinting is known, and the song has previously been identified as a cover (e.g., as a version of another song) the original song can be associated in the database with the identified cover song.","In the hybrid process, if there is no matching song in the database based on fingerprinting, the mechanisms described herein can be used to calculate query song jumpcodes. These query song jumpcodes can then be compared the reference song jumpcodes as described herein. Using the hybrid system to first identify whether the jumpcodes for the query song have already been calculated and are available on the server, the query song can be identified more quickly by eliminating the need to calculate jumpcodes for the query song.",{"@attributes":{"id":"p-0110","num":"0109"},"figref":"FIGS. 10-16","b":["102","100","200","300","400","600"]},"In accordance with some embodiments, in order to track beats for extracting and calculating, chroma vectors, all or a portion of a song can be converted into an onset strength envelope O(t)  as illustrated in process  in . As part of this process, the song (or portion of the song)  can be sampled or re-sampled (e.g., at 8 kHz or any other suitable rate) at  and then the spectrogram of the short-term Fourier transform (STFT) calculated for time intervals in the song (e.g., using 32 ms windows and 4 ms advance between frames or any other suitable window and advance) at . An approximate auditory representation of the song can then be formed at  by mapping to 40 Mel frequency bands or any other suitable number of bands) to balance the perceptual importance of each frequency band. This can be accomplished, for example, by calculating each Mel bin as a weighted average of the FFT bins ranging from the center frequencies of the two adjacent Mel bins, with linear weighting to give a triangular weighting window. The Mel spectrogram can then be converted to dB at , and the first-order difference along time is calculated for each band at . Then, at , negative values in the first-order differences can be set to zero (half-wave rectification), and the remaining, positive differences can be summed across all of the frequency bands. The summed differences can then be passed through a high-pass filter (e.g., with a cutoff around 0.4 Hz) and smoothed (e.g., by convolving with a Gaussian envelope about 20 ms wide) at . This gives a one-dimensional onset strength envelope  as a function of time (e.g., O(t) that responds to proportional increase in energy summed across approximately auditory frequency bands.","In some embodiments, the onset envelope for each musical excerpt can then be normalized by dividing by its standard deviation.",{"@attributes":{"id":"p-0113","num":"0112"},"figref":"FIG. 11","b":["1100","1102","1104","1104","1106","1108","1104"]},"In some embodiments, a tempo estimate \u03c4for the song (or portion of the song) can next be calculated using process  as illustrated in . Given an onset strength envelope O(t) , autocorrelation can be used to reveal any regular, periodic structure in the envelope. For example, autocorrelation can be performed at  to calculate the inner product of the envelope with delayed versions of itself. For delays that succeed in lining up many of the peaks, a large correlation can occur. For example, such an autocorrelation can be represented as:",{"@attributes":{"id":"p-0115","num":"0114"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"munder":{"mo":"\u2211","mi":"t"},"mo":"\u2062","mrow":{"mrow":[{"mi":"O","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"mi":"O","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["t","\u03c4"],"mo":"-"}}}],"mo":"\u2062"}}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}}}}},"Because there can be large correlations at various integer multiples of a basic period (e.g., as the peaks line up with the peaks that occur two or more beats later), it can be difficult to choose a single best peak among many correlation peaks of comparable magnitude. However, human tempo perception (as might be examined by asking subjects to tap along in time to a piece of music) is known to have a bias towards 120 beats per minute (BPM). Therefore, in some embodiments, a perceptual weighting window can be applied at  to the raw autocorrelation to down-weight periodicity peaks that are far from this bias. For example, such a perceptual weighting window W(\u03c4) can be expressed as a Gaussian weighting function on a log-time axis, such as:",{"@attributes":{"id":"p-0117","num":"0116"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"W","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03c4"}},{"mi":"exp","mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":{"mo":"-","mfrac":{"mn":["1","2"]}},"mo":"\u2062","msup":{"mrow":{"mo":["(",")"],"mfrac":{"mrow":{"msub":{"mi":"log","mn":"2"},"mo":"\u2062","mrow":{"mi":"\u03c4","mo":"\/","msub":{"mi":"\u03c4","mn":"0"}}},"msub":{"mi":["\u03c3","\u03c4"]}}},"mn":"2"}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"5"}}]}}}},"br":{},"sub":["0 ","\u03c4"]},"By applying this perceptual weighting window W(\u03c4) to the autocorrelation above, a tempo period strength  can be represented as:",{"@attributes":{"id":"p-0119","num":"0118"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"TPS","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03c4"}},{"mrow":[{"mi":"W","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03c4"}},{"munder":{"mo":"\u2211","mi":"t"},"mo":"\u2062","mrow":{"mrow":[{"mi":"O","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"mi":"O","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["t","\u03c4"],"mo":"-"}}}],"mo":"\u2062"}}],"mo":"\u2062"}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"6"}}]}}}}},"Tempo period strength , for any given period \u03c4, can be indicative of the likelihood of a human choosing that period as the underlying tempo of the input sound. A primary tempo period estimate \u03c4 can therefore be determined at  by identifying the \u03c4 for which TPS(\u03c4) is largest.",{"@attributes":{"id":"p-0121","num":"0120"},"figref":["FIG. 13","FIG. 11"],"b":["1302","1304","1306","1210"],"sub":"p "},"In some embodiments, rather than simply choosing the largest peak in the base TPS, a process  of  can be used to determine \u03c4. As shown, two further functions can be calculated at  and  by re-sampling TPS to one-half and one-third, respectively, of its original length, adding this to the original TPS, then choosing the largest peak across both of these new sequences as shown below:\n\n2(\u03c4)=(\u03c4)+0.5(2\u03c4)+0.25(2\u03c4\u22121)+0.25(2\u03c4+1) \u2003\u2003(7)\n\n3(\u03c4)=(\u03c4)+0.33(3\u03c4)+0.33(3\u03c4\u22121)+0.33(3\u03c4+1) \u2003\u2003(8)\n","Whichever sequence (7) or (8) results in a larger peak value TPS(\u03c4) or TPS(\u03c4) determines at  whether the tempo is considered duple  or triple , respectively. The value of \u03c4or \u03c4corresponding to the larger peak value is then treated as the faster target tempo metrical level at  or , with one-half or one-third of that value as the adjacent metrical level at  or . TPS can then be calculated twice using the faster target tempo metrical level and adjacent metrical level using equation (6) at . In some embodiments, at \u03c3 of 0.9 octaves (or any other suitable value) can be used instead of an \u03c3 of 1.4 octaves in performing the calculations of equation (6). The larger value of these two TPS values can then be used at  to indicate that the faster target tempo metrical level or the adjacent metrical level, respectively, is the primary tempo period estimate \u03c4.","Using the onset strength envelope and the tempo estimate, a sequence of beat times that correspond to perceived onsets in the audio signal and constitute a regular, rhythmic pattern can be generated using process  as illustrated in connection with  using the following equation:",{"@attributes":{"id":"p-0125","num":"0124"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"C","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mo":["{","}"],"msub":{"mi":["t","i"]}}}},{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mrow":{"mi":"O","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["t","i"]}}}},{"mi":"\u03b1","mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"2"},"mi":"N"},"mo":"\u2062","mrow":{"mi":"F","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":[{"mi":["t","i"]},{"mi":"t","mrow":{"mi":"i","mo":"-","mn":"1"}}],"mo":"-"},"mo":",","msub":{"mi":["\u03c4","p"]}}}}}}],"mo":"+"}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"9"}}]}}}},"br":{},"sub":["i","p","p ","p"]},{"@attributes":{"id":"p-0126","num":"0125"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"F","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["\u0394","t"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mo":",","mi":"\u03c4"}}},{"mo":"-","msup":{"mrow":{"mo":["(",")"],"mrow":{"mi":"log","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mfrac":{"mrow":{"mi":["\u0394","t"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mi":"\u03c4"}}},"mn":"2"}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"10"}}]}}}},"br":{}},"A property of the objective function C(t) is that the best-scoring time sequence can be assembled recursively to calculate the best possible score C*(t) of all sequences that end at time t. The recursive relation can be defined as:",{"@attributes":{"id":"p-0128","num":"0127"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msup":{"mi":"C","mo":"*"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"mrow":[{"mi":"O","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"munder":{"mi":"max","mrow":{"mi":"\u03c4","mo":"=","mrow":{"mrow":{"mn":"0","mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mi":["\u2026","t"]},"mo":"-","mn":"1"}}},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"\u03b1","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"F","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["t","\u03c4"],"mo":"-"},"mo":",","msub":{"mi":["\u03c4","p"]}}}}},{"msup":{"mi":"C","mo":"*"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03c4"}}],"mo":"+"}}}],"mo":"+"}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"11"}}]}}}}},"This equation is based on the observation that the best score for time t is the local onset strength, plus the best score to the preceding beat time \u03c4 that maximizes the sum of that best score and the transition cost from that time. While calculating C*, the actual preceding beat time that gave the best score can also be recorded as:",{"@attributes":{"id":"p-0130","num":"0129"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msup":{"mi":"P","mo":"*"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mi":"max","mrow":{"mi":"\u03c4","mo":"=","mrow":{"mrow":{"mn":"0","mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mi":["\u2026","t"]},"mo":"-","mn":"1"}}},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"\u03b1","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"F","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["t","\u03c4"],"mo":"-"},"mo":",","msub":{"mi":["\u03c4","p"]}}}}},{"msup":{"mi":"C","mo":"*"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03c4"}}],"mo":"+"}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"12"}}]}}}}},"In some embodiments, a limited range of \u03c4 can be searched instead of the full range because the rapidly growing penalty term F will make it unlikely that the best predecessor time lies far from t\u2212\u03c4. Thus, a search can be limited to \u03c4=t\u22122\u03c4. . . t\u2212\u03c4\/2 as follows:",{"@attributes":{"id":"p-0132","num":"0131"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":[{"msup":{"mi":"C","mo":"*"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"mrow":[{"mi":"O","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"munder":{"mi":"max","mrow":{"mi":"\u03c4","mo":"=","mrow":{"mi":"i","mo":["-","-"],"mrow":[{"mn":"2","mo":["\u2062","\u2062","\u2062","\u2062"],"msub":{"mi":["\u03c4","p"]},"mi":["\u2026","t"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"msub":{"mi":["\u03c4","p"]},"mo":"\/","mn":"2"}]}}},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"\u03b1","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"F","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["t","\u03c4"],"mo":"-"},"mo":",","msub":{"mi":["\u03c4","p"]}}}}},{"msup":{"mi":"C","mo":"*"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03c4"}}],"mo":"+"}}}],"mo":"+"}],"mo":"="}},{"mrow":{"mo":["(",")"],"msup":{"mn":"11","mi":"\u2032"}}}]},{"mtd":[{"mrow":{"mrow":[{"msup":{"mi":"P","mo":"*"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mi":"max","mrow":{"mi":"\u03c4","mo":"=","mrow":{"mi":"i","mo":["-","-"],"mrow":[{"mn":"2","mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"msub":{"mi":["\u03c4","p"]},"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mi":["\u2026","t"]},{"msub":{"mi":["\u03c4","p"]},"mo":"\/","mn":"2"}]}}},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"\u03b1","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"F","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["t","\u03c4"],"mo":"-"},"mo":",","msub":{"mi":["\u03c4","p"]}}}}},{"msup":{"mi":"C","mo":"*"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03c4"}}],"mo":"+"}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"msup":{"mn":"12","mi":"\u2032"}}}]}]}}}},"To find the set of beat times that optimize the objective function for a given onset envelope. C*(t) and P*(t) can be calculated at  for every time starting from the beginning of the range zero at  via . The largest value of C* which will typically be within \u03c4of the end of the time range) can be identified at . This largest value of C* is the final beat instant t\u2014where N, the total number of beats, is still unknown at this point. The beats leading up to C* can be identified by \u2018back tracing\u2019 via P* at , finding the preceding beat time t=P*(t), and progressively working backwards via  until the beginning of the song (or portion of a song) is reached. This produces the entire optimal beat sequence {t}* .","In order to accommodate slowly varying tempos, \u03c4can be updated dynamically during the progressive calculation of C*(t) and P*(t). For instance, \u03c4(t) can be set to a weighted average (e.g., so that times further in the past have progressively less weight) of the best inter-beat-intervals found in the max search for times around t. For example, as C*(t) and P*(t) are calculated at , \u03c4(t) can be calculated as:\n\n\u03c4()=\u03b7(*())+(1\u2212\u03b7)\u03c4(*()) \u2003\u2003(13)\n\nwhere \u03b7 is a smoothing constant having a value between 0 and 1 (e.g., 0.1 or any other suitable value) that is based on how quickly the tempo can change. During the subsequent calculation of C*(t+1), the term F(t\u2212\u03c4, \u03c4) can be replaced with F(t\u2212\u03c4, \u03c4(\u03c4)) to take into account the new local tempo estimate.\n","In order to accommodate several abrupt Changes in tempo, several different \u03c4values can be used in calculating C*( ) and P*( ) in some embodiments. In some of these embodiments, a penalty factor can be included in the calculations of C*( ) and P*( ) to down-weight calculations that favor frequent shifts between tempo. For example, a number of different tempos can be used in parallel to add a second dimension to C*( ) and P*( ) to find the best sequence ending, at time t and with a particular tempo \u03c4. For example, C*( ) and P*( ) can be represented as:",{"@attributes":{"id":"p-0136","num":"0135"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":[{"msup":{"mi":"C","mo":"*"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"t","mo":",","msub":{"mi":["\u03c4","pi"]}}}},{"mrow":[{"mi":"O","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"munder":{"mi":"max","mrow":{"mi":"\u03c4","mo":"=","mrow":{"mrow":{"mn":"0","mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mi":["\u2026","t"]},"mo":"-","mn":"1"}}},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"\u03b1","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"F","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["t","\u03c4"],"mo":"-"},"mo":",","msub":{"mi":["\u03c4","pi"]}}}}},{"msup":{"mi":"C","mo":"*"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03c4"}}],"mo":"+"}}}],"mo":"+"}],"mo":"="}},{"mrow":{"mo":["(",")"],"msup":{"mn":"11","mi":"\u2033"}}}]},{"mtd":[{"mrow":{"mrow":[{"msup":{"mi":"P","mo":"*"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"t","mo":",","msub":{"mi":["\u03c4","pi"]}}}},{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mi":"max","mrow":{"mi":"\u03c4","mo":"=","mrow":{"mrow":{"mn":"0","mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mi":["\u2026","t"]},"mo":"-","mn":"1"}}},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"\u03b1","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"F","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["t","\u03c4"],"mo":"-"},"mo":",","msub":{"mi":["\u03c4","pi"]}}}}},{"msup":{"mi":"C","mo":"*"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03c4"}}],"mo":"+"}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"msup":{"mn":"12","mi":"\u2033"}}}]}]}}}},"This approach is able to find an optimal spacing of beats even in intervals where there is no acoustic evidence of any beats. This \u201cfilling in\u201d emerges naturally from the back trace and may be beneficial in cases in which music contains silence or long sustained notes.","Using the optimal beat sequence {t}*, the song or a portion of the song) can next be used to generate a single feature vector per beat as beat-level descriptors, in accordance with  of . These beat-level descriptors can be used to represent both the dominant note (typically melody) and the broad harmonic accompaniment in the song (or portion of the song) (e.g., when using chroma features as described below.","In some embodiments, beat-level descriptors are generated as the intensity associated with each of 12 semitones (e.g., piano keys) within an octave formed by folding all octaves together (e.g., putting the intensity of semitone A across all octaves in the same semitone bin A, putting the intensity of semitone  across all octaves in the same semitone bin B, putting the intensity of semitone C across all octaves in the same semitone bin C, etc.).","In generating these beat-level descriptors, phase-derivatives (instantaneous frequencies) of FFT bins can be used both to identify strong tonal components in the spectrum (indicated by spectrally adjacent bins with close instantaneous frequencies) and to get a higher-resolution estimate of the underlying frequency. For example, a 1024 point Fourier transform can be applied to 10 seconds of the song (or the portion of the song) sampled (or re-sampled) at 11 kHz with 93 ms overlapping windows advanced by 10 ms. This results in 513 frequency bins per FFT window and 1000 FFT windows.","To reduce these 513 frequency bins over each of 1000 windows to 12 (for example) chroma bins per beat, the 513 frequency bins can first be reduced to 12 chroma bins. This can be done by removing non-tonal peaks by keeping only bins where the instantaneous frequency is within 25% (or any other suitable value) over three (or any other suitable number) adjacent bins, estimating the frequency that each energy peak relates to from the energy peak's instantaneous frequency, applying a perceptual weighting function to the frequency estimates so frequencies closest to a given frequency (e.g., 400 Hz) have the strongest contribution to the chroma vector, and frequencies below a lower frequency (e.g., 100 Hz, 2 octaves below the given frequency, or any other suitable value) or above an upper frequency (e.g., 1600 Hz, 2 octaves above the given frequency, or any other suitable value) are strongly down-weighted, and sum up all the weighted frequency components by putting their resultant magnitude into the chroma bin with the nearest frequency.","As mentioned above, in some embodiments, each chroma bin can correspond to the same semitone in all octaves. Thus, each chroma bin can correspond to multiple frequencies (i.e., the particular semitones of the different octaves). In some embodiments, the different frequencies (f) associated with each chroma bin i can be calculated by applying the following formula to different values of r:\n\n*2\u2003\u2003(14)\n\nwhere r is an integer value representing the octave relative to ffor which the specific frequency fis to be determined r=\u22121 indicates to determine ffor the octave immediately below 440 Hz), N is the total number of chroma bins (e.g., 12 in this example), and fis the \u201ctuning center\u201d of the set of chroma bins (e.g., 440 Hz or any other suitable value).\n","Once there are 12 chroma bins over 1000 windows, in the example above, the 1000 windows can be associated with corresponding beats, and then each of the windows for a beat combined to provide a total of 12 chroma bins per beat. The windows for a beat can be combined, in some embodiments, by averaging each chroma bin i across all of the windows associated with a beat. In some embodiments, the windows for a beat can be combined by taking the largest value or the median value of each chroma bin i across all of the windows associated with a beat. In some embodiments, the windows for a beat can be combined by taking the N-th root of the average of the values, raised to the N-th power, for each chroma bin i across all of the windows associated with a beat.","In some embodiments, the Fourier transform can be weighted (e.g., using Gaussian weighting) to emphasize energy a couple of octaves (e.g., around two with a Gaussian half-width of 1 octave) above and below 400 Hz.","In some embodiments, instead of using a phase-derivative within FFT bins in order to generate beat-level descriptors as chroma bins, the STFT bins calculated in determining the onset strength envelope O(t) can be mapped directly to chroma bins by selecting spectral peaks. For example, the magnitude of each FFT bin can be compared with the magnitudes of neighboring bins to determine if the bin is larger. The magnitudes of the non-larger bins can be set to zero, and a matrix containing, the FFT bins multiplied by a matrix of weights that map each FFT bin to a corresponding chroma bin. This results in having 12 chroma bins per each of the FFT windows calculated in determining the onset strength envelope. These 12 bins per window can then be combined to provide 12 bins per beat in a similar manner as described above for the phase-derivative-within-FFT-bins approach to generating beat-level descriptors.","In some embodiments, the mapping of frequencies to chroma bins can be adjusted for each song (or portion of a song) by up to \u00b10.5 semitones (or any other suitable value) by making the single strongest frequency peak from a long FFT window (e.g., 10 seconds or any other suitable value) of that song (or portion of that song) line up with a chroma bin center.","In some embodiments, the magnitude of the chroma bins can be compressed by applying a square root function to the magnitude to improve performance of the correlation between songs.","In some embodiments, each chroma bin can be normalized to have zero mean and unit variance within each dimension (i.e., the chroma bin dimension and the beat dimension). In some embodiments, the chroma bins are also high-pass filtered in the time dimension to emphasize changes. For example, a first-order high-pass filter with a 3 dB cutoff at around 0.1 radians\/sample can be used.","In some embodiments, in addition to the beat-level descriptors described above for each beat (e.g., 12 chroma bins), other beat-level descriptors can additionally be generated and used in comparing songs (or portions of songs). For example, such other beat-level descriptors can include the standard deviation across the windows of beat-level descriptors within a beat, and\/or the slope of a straight-line approximation to the time-sequence of values of beat-level descriptors for each window within a beat. Note, that if transposition of the chroma bins is performed as discussed below, the mechanism for doing so can be modified to insure that the chroma dimension of any matrix in which the chroma bins are stored is symmetric or to account for any asymmetry in the chroma dimension.","In some of these embodiments, only components of the song (or portion of the song) up to 1 kHz are used in forming the beat-level descriptors. In other embodiments, only components of the song (or portion of the song) up to 2 kHz are used in forming the beat-level descriptors.","The lower two panes  and  of  show beat-level descriptors as chroma bins before and after averaging into beat-length segments.","Accordingly, methods, systems, and media for identifying similar songs using jumpcodes are provided.","In some embodiments, any suitable computer readable media can be used for storing instructions for performing the functions and\/or processes described herein. For example, in some embodiments, computer readable media can be transitory or non-transitory. For example, non-transitory computer readable media can include media such as magnetic media (such as hard disks, floppy disks, etc.), optical media (such as compact discs, digital video discs, Blu-ray discs, etc.), semiconductor media (such as flash memory, electrically programmable read only memory (EPROM), electrically erasable programmable read only memory (EEPROM), etc.), any suitable media that is not fleeting or devoid of any semblance of permanence during transmission, and\/or any suitable tangible media. As another example, transitory computer readable media can include signals on networks, in wires, conductors, optical fibers, circuits, any suitable media that is fleeting and devoid of any semblance of permanence during transmission, and\/or any suitable intangible media.","It should be noted that, as used herein, the term mechanism can encompass hardware, software, firmware, or any suitable combination thereof.","It should be understood that the above described steps of the processes of  ,  and  can be executed or performed in any order or sequence not limited to the order and sequence shown and described in the figures. Also, some of the above steps of the processes of  can be executed or performed substantially simultaneously where appropriate or in parallel to reduce latency and processing times.","Although the invention has been described, and illustrated in the foregoing illustrative embodiments, it is understood that the present disclosure has been made only by way of example, and that numerous changes in the details of implementation of the invention can be made without departing from the spirit and scope of the invention, which is limited only by the claims that follow. Features of the disclosed embodiments can be combined and rearranged in various ways."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The above and other objects and advantages of the invention will be apparent upon consideration of the following detailed description, taken in conjunction with the accompanying drawings, in which like reference characters refer to like parts throughout, and in which:",{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 2A"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":["FIG. 2B","FIG. 2A"]},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 7A"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":["FIG. 7B","FIG. 7A"]},{"@attributes":{"id":"p-0019","num":"0018"},"figref":["FIG. 8","FIG. 2"]},{"@attributes":{"id":"p-0020","num":"0019"},"figref":["FIG. 9","FIG. 2"]},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 16"}]},"DETDESC":[{},{}]}
