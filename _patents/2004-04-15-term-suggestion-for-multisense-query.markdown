---
title: Term suggestion for multi-sense query
abstract: Systems and methods for related term suggestion are described. In one aspect, term clusters are generated as a function of calculated similarity of term vectors. Each term vector having been generated from search results associated with a set of high frequency of occurrence (FOO) historical queries previously submitted to a search engine. Responsive to receiving a term/phrase from an entity, the term/phrase is evaluated in view of terms/phrases in the term clusters to identify one or more related term suggestions.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07428529&OS=07428529&RS=07428529
owner: Microsoft Corporation
number: 07428529
owner_city: Redmond
owner_country: US
publication_date: 20040415
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["RELATED APPLICATIONS","TECHNICAL FIELD","BACKGROUND","SUMMARY","DETAILED DESCRIPTION","Keyword Mining and Feature Extraction","Term Clustering","Term Matching","Classification of Low FOO Terms","CONCLUSION"],"p":["This patent application is related to the following patent applications, each of which are commonly assigned to assignee of this application, and hereby incorporated by reference:\n\n","Systems and methods of the invention pertain to data mining.","A keyword or phrase is a word or set of terms submitted by a Web surfer to a search engine when searching for a related Web page\/site on the World Wide Web (WWW). Search engines determine the relevancy of a Web site based on the keywords and keyword phrases that appear on the page\/site. Since a significant percentage of Web site traffic results from use of search engines, Web site promoters know that proper keyword\/phrase selection is vital to increasing site traffic to obtain desired site exposure. Techniques to identify keywords relevant to a Web site for search engine result optimization include, for example, evaluation by a human being of Web site content and purpose to identify relevant keyword(s). This evaluation may include the use of a keyword popularity tool. Such tools determine how many people submitted a particular keyword or phrase including the keyword to a search engine. Keywords relevant to the Web site and determined to be used more often in generating search queries are generally selected for search engine result optimization with respect to the Web site.","After identifying a set of keywords for search engine result optimization of the Web site, a promoter may desire to advance a Web site to a higher position in the search engine's results (as compared to displayed positions of other Web site search engine results). To this end, the promoter bids on the keyword(s) to indicate how much the promoter will pay each time a Web surfer clicks on the promoter's listings associated with the keyword(s). In other words, keyword bids are pay-per-click bids. The larger the amount of the keyword bid as compared to other bids for the same keyword, the higher (more prominently with respect to significance) the search engine will display the associated Web site in search results based on the keyword.","In view of the above, systems and methods to better identify keywords relevant to Web site content would be welcomed by Web site promoters. This would allow the promoters to bid user preferred terms. Ideally, these systems and methods would be independent of the need for a human being to evaluate Web site content to identify relevant keywords for search engine optimization and keyword bidding.","Systems and methods for related term suggestion are described. In one aspect, term clusters are generated as a function of calculated similarity of term vectors. Each term vector having been generated from search results associated with a set of high frequency of occurrence (FOO) historical queries previously submitted to a search engine. Responsive to receiving a term\/phrase from an entity, the term\/phrase is evaluated in view of terms\/phrases in the term clusters to identify one or more related term suggestions.","Overview","It may appear that the simplest way to suggest related term\/phrase is to use a substring matching approach, which judges two terms\/phrases as related when one term\/phrase includes some or all of the words of another term\/phrase. However, this technique is substantially limited. This method may neglect many semantically related terms, because related terms need not to contain common words. For example, consider that a footwear corporation wants to know related terms for \u201cshoe\u201d. If the conventional matching approach is used, only \u201cwomen's shoes\u201d, \u201cdiscount shoes\u201d, etc. will be suggested. However, many other related terms exist such as \u201csneakers\u201d, \u201chiking boots\u201d, \u201cNike\u201d, etc.","The following systems and methods for related term suggestion for a multi-sense query address these limitations of conventional substring matching techniques. To this end, the systems and methods mine search engine results for terms\/phrases that are semantically related to an end-user (e.g., Web site promoter, advertiser, etc.) submitted terms\/phrases. The semantic relationship is constructed by mining context (e.g., text, and\/or so on) from the search engine results, the context surrounding a term\/phrase that can cast light on term\/phrase meaning. More particularly, a set of query terms is aggregated from a historical query log, with their frequency of occurrence (FOO) being counted. These query terms are submitted, one-by-one, to the search engine. In one implementation, the submitted the historical query log terms have a relatively high frequency of occurrence as compared to the frequency of occurrence of other historical query log terms.","Responsive to receiving respective ones of the submitted queries, the search engine returns a ranked list of search results, including URLs, result titles, and short descriptions of each result and\/or context surrounding the submitted query. As search engine results are received, the systems and methods extract a set of features (keywords and the corresponding weights that are calculated using known TFIDF techniques) from select ones of the returned search results (e.g., one or more top-ranked results). After extracting features of corresponding search engine results from the submitted search queries, the extracted features are normalized. The normalized features are used to represent each submitted queries, and are used in a text clustering algorithm to group submitted query terms into clusters.","Responsive to receiving the term\/phrase from the end-user, the term\/phrase is compared to respective ones of the terms\/phrases in the term clusters. Since the term clusters include terms that are contextually related to one another, when the term\/phrase is compared to the terms within the clusters, the term phrase is evaluated in view of any multiple related contexts, or \u201csenses.\u201d In one implementation, if a term\/phrase matches a term from a cluster, the cluster is returned to the end-user in a suggested term list. The suggested term list includes terms\/phrases determined to be semantically and\/or contextually related to the term\/phrase, respective term\/phrase to term\/phrase similarity measurements (confidence values), and respective term\/phrase frequency of occurrence (FOO). The returned list is ordered by a combination of FOO and confidence value. If the term\/phrase matches terms in more than a single term cluster, multiple suggested term lists are generated. The lists are ordered by the cluster sizes; and the terms within each list are ordered by a combination of FOO and confidence value. If no matching clusters are identified, the query term is further matched against expanded clusters generated from query terms with low FOO.","In one implementation, query terms with low FOO are clustered by training a classifier (e.g., a K-nearest neighbor classifier) for the term clusters generated from the high frequency of occurrence historical query log terms. Historical query terms determined to have low frequency of occurrence are submitted, one-by-one, to the search engine. Features are then extracted from select ones (e.g., a first top-ranked Web page, and\/or so on) of the returned search results. The extracted features are normalized and used to represent the query terms with low FOO. The query terms are then classified into existing clusters to generate expanded clusters based on the trained classifier. The end-user submitted term\/phrase is then evaluated in view of these expanded clusters to identify and return a suggested term list to the end-user.","These and other aspects of the systems and methods for related term\/keyword suggestion for a multi-sense query are now described in greater detail.","An Exemplary System","Turning to the drawings, wherein like reference numerals refer to like elements, the systems and methods for related term suggestion for multi-sense query are described and shown as being implemented in a suitable computing environment. Although not required, the invention is described in the general context of computer-executable instructions (program modules) being executed by a personal computer. Program modules generally include routines, programs, objects, components, data structures, etc., that perform particular tasks or implement particular abstract data types. While the systems and methods are described in the foregoing context, acts and operations described hereinafter may also be implemented in hardware.",{"@attributes":{"id":"p-0020","num":"0021"},"figref":"FIG. 1","b":["100","100","102","104","106","108","106","102","102","110","106","108","104","100","106"]},"A suggested term list  includes, for example, terms\/phrases determined to be related to the term\/phrase , respective term\/phrase to term\/phrase  similarity measurements (confidence values), and respective term\/phrase frequency of occurrence (FOO)\u2014frequency in the historical query log. Techniques for identifying related terms\/phrases, generating similarity measurements, and generating FOO values are described in greater detail below in reference to sections titled keyword mining, feature extraction, and term clustering.","TABLE 1 shows an exemplary suggested term list  of terms determined to be related to a term\/phrase  of \u201cmail.\u201d Terms related to term\/phrase  are shown in this example in column 1, titled \u201cSuggested Term.\u201d",{"@attributes":{"id":"p-0023","num":"0024"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 1"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"AN EXEMPLARY SUGGESTED TERM LIST FOR"},{"entry":"THE BID TERM \u201cMAIL\u201d"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"1","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"63pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Suggested Term","Similarity","Frequency","<Context>"]},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"1","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"63pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["hotmail","0.246142","93161","online e-mail"]},{"entry":["yahoo","0.0719463","165722","related"]},{"entry":["mail.com","0.352664","1455"]},{"entry":["yahoo mail","0.0720606","39376"]},{"entry":["www.mail.com","0.35367","711"]},{"entry":["email.com","0.484197","225"]},{"entry":["www.hot","0.186565","1579"]},{"entry":["www.msn.com","0.189117","1069"]},{"entry":["mail.yahoo.com","0.0968248","4481"]},{"entry":["free email","0.130611","1189"]},{"entry":["www.aolmail.com","0.150844","654"]},{"entry":["check mail","0.221989","66"]},{"entry":["check email","0.184565","59"]},{"entry":["msn passport","0.12222","55"]},{"entry":["www.webmail.aol.com","0.0800538","108"]},{"entry":["webmail.yahoo.com","0.08789","71"]},{"entry":["free email account","0.0836481","65"]},{"entry":["mail","1","2191","Traditional mail"]},{"entry":["usps","0.205141","4316","related"]},{"entry":["usps.com","0.173754","779"]},{"entry":["united parcel service","0.120837","941"]},{"entry":["postal rates","0.250423","76"]},{"entry":["stamps","0.156702","202"]},{"entry":["stamp collecting","0.143618","152"]},{"entry":["state abbreviations","0.104614","300"]},{"entry":["postal","0.185255","66"]},{"entry":["postage","0.180112","55"]},{"entry":["postage rates","0.172722","51"]},{"entry":["usps zip codes","0.138821","78"]},{"entry":["us postmaster","0.109844","58"]},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}}]}}]}}},"Referring to TABLE 1, note that terms in the suggested term list are mapped to term similarity values (see, column 2, titled \u201cSimilarity\u201d) and frequency of occurrence scores (see, column 3, titled \u201cFrequency\u201d). Each term similarity value, calculated as described below in the section titled \u201cTerm Clustering\u201d, provides a similarity measure between a corresponding suggested term (column 1) and the term\/phrase , which is \u201cmail\u201d in this example. Each frequency value, or score, indicates the number of times that the suggested term occurs in the historical query log. The suggested term list is sorted as a function of term similarity, and\/or frequency of occurence scores as a function of business goals.","Any given term\/phrase  (e.g., mail, etc.) may have more than a single context within which the bid term may be used. To account for this, STS model  provides an indication in suggested term list  of which suggested terms correspond to which of the multiple contexts of term\/phrase . For example, referring to TABLE 1, the term\/phrase  of \u201cmail\u201d has two (2) contexts: (1) traditional off-line mail and (2) online e-mail. Note that a respective list of related terms is shown for each of these two bid term contexts.","Additionally, suggested terms for any term\/phrase  may be more than synonyms of the bid term. For instance, referring to TABLE 1, the suggested term \u201cusps\u201d is an acronym for an organization that handles mail, not a synonym for the bid term \u201cmail.\u201d However, \u201cusps\u201d is also a term very related to a \u201cmail\u201d bid term, and thus, is shown in the suggested term list . In one implementation, STS model  determines the relationship between a related term R (e.g. \u201cusps\u201d) and a target term T (e.g. \u201cmail\u201d) as a function of the following association rule: itr(T)\u2192itr(R), wherein \u201citr\u201d represents \u201cinterested in\u201d. If a user (advertiser, Web site promoter, and\/or the like) is interested in R, the user will also be interested in T.","EVS  includes a number of computer-program modules to generate suggested term list . The computer-program modules include, for example, search term suggestion (STS) module  and classification module . STS module  retrieves a set of historical queries  from query log . The historical queries include search query terms previously submitted to a search engine. STS module  evaluates historical queries  as a function of frequency of occurrence to identify high frequency of occurrence (FOO) search terms  and relatively lower frequency of occurrence search terms . In this implementation, a configurable threshold value is used to determine whether a historical query has a relatively higher or low frequency of occurrence. For example, search query terms in historical queries  that occur at least a threshold number of times are said to have a high frequency of occurrence. Analogously, search query terms in historical queries  that occur less than the threshold number of time are said to have a low frequency of occurrence. For purposes of illustration, such a threshold value is shown as a respective portion of \u201cother data\u201d .","STS module  mines semantic\/contextual meaning high frequency of occurrence query terms  by submitting each query, one-by-one (search query ), to search engine . Responsive to receiving search query , search engine , returns a ranked listing (whose number is configurable) in search result  to STS module . The ranked listing includes URLs, result titles, and short descriptions and\/or contexts of query term related to the submitted search query . The ranked listing is stored in the search results . Such search result retrieval is done for each search query .","STS module  parses Web page Hypertext Markup Language (HTML) to extract the URLs, result titles and short descriptions and\/or contexts of the query term for each query term  from each retrieved search result . The URLs, result titles, short descriptions and\/or contexts of the query term, and the search query  used to obtain the retrieved Search result  are stored by STS module  in a respective record of extracted features .","After parsing search results  for the high frequency of occurrence query terms , STS module  performs text preprocessing operations on extracted features  to generate linguistic tokens (tokenize) from the extracted features into individual keywords. To reduce dimensionality of the tokens, STS module  removes any stop-words (e.g., \u201cthe\u201d, \u201ca\u201d, \u201cis\u201d, etc.) and removes common suffixes to normalize the keywords, for example, using a known Porter stemming algorithm. STS module  arranges the resulting extracted features  into one or more term vectors .","Each term vector  has dimensions based on term frequency and inverted document frequency (TFIDF) scores. A weight for the ivector's jkeyword is calculated as follows:\n\n\u00d7log()\n\nwherein TFrepresents term frequency (the number of occurrences of keyword j in the irecord), N is the total number of query terms, and DFis the number of records that contain keyword j.\n","STS module  groups similar terms to generate term clusters  from term vectors . To this end, and in this implementation, given the vector representation of each term, a cosine function is used to measure the similarity between a pair of terms (recall that the vectors were normalized):",{"@attributes":{"id":"p-0033","num":"0034"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"sim","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["q","j"]},{"mi":["q","k"]}],"mo":","}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"d"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":[{"mi":["w","ij"]},{"mi":["w","ik"]}],"mo":"\u00b7"}}],"mo":"="}}},"br":[{},{},{}],"in-line-formulae":[{},{}],"i":["q",", q","q",", q"],"sub":["j","k","j","k"],"b":["124","110"]},"STS module  uses the calculated similarity measurement(s) to cluster\/group terms represented by keyword vectors  into term cluster(s) . More particularly, and in this implementation, STS module  uses a known density-based clustering algorithm (DBSCAN) to generate term cluster(s) . DBSCAN uses two parameters: Eps and MinPts. Eps represents a maximum distance between points in a cluster . Here points are equivalent of vectors because each vector can be represented by the point of the vector's head when its tail is moved to the origin. MinPts represents a minimum number of points in a cluster . To generate a cluster , DBSCAN starts with an arbitrary point p and retrieves all points density-reachable from p with respect to Eps and MinPts. If p is a core point, this procedure yields a cluster  with respect to Eps and MinPts. If p is a border point, no points are density-reachable from p and DBSCAN visits the next point.","Responsive to receiving the term\/phrase  from an end-user (e.g., an advertiser, Web site promoter, etc), STS module  compares the term\/phrase  to respective ones of the terms\/phrases in the term clusters . Since term clusters  include terms that are contextually related to one another, the term\/phrase  is evaluated in view of multiple related and historical contexts, or \u201csenses.\u201d In one implementation, if STS module  determines that a term\/phrase  matches a term\/phrase from a cluster , search term suggestion module  generates suggested term list  from the cluster . In this implementation, a match may be an exact match or a match with a small number of variations such as singular\/plural forms, misspellings, punctuation marks, etc. The returned list is ordered by a combination of FOO and confidence value.","If STS module  determines that a term\/phrase  matches terms in multiple term clusters , search term suggestion module  generates multiple suggested term lists  from terms in the multiple ones of term clusters . The lists are ordered by the cluster sizes; and the terms within each list are ordered by a combination of FOO and confidence value.","Classification module  generates suggested term list  when term clusters  generated from high frequency of occurrence (FOO) query terms  do not include same terms to end-user input term\/phrase . To this end, classification module  generates trained classifier  from term clusters  generated from high frequency of occurrence (FOO) query log terms . The terms in term clusters  already have corresponding keyword vectors in a vector space model suitable for classification operations. Additionally, stop-word removal and word stemming (suffix removal) reduced dimensionality of term vectors  (upon which clusters  are based). In one implementation, additional dimensionality reduction techniques, for example, feature selection or re-parameterization, may be employed.","In this implementation, to classify a class-unknown query term , classification module  uses the k-Nearest Neighbor classifier algorithm to find k most similar neighbors in all class-known query terms , relying on their corresponding feature vectors, and uses the a weighted majority of class labels of the neighbors to predict the class of the new query term. Here each query term already in term clusters  is assigned a label same to their corresponding clusters' label, while each cluster  is labeled by simple sequence numbers. These neighbors are weighted using the similarity of each neighbor to X, where similarity is measured by Euclidean distance or the cosine value between two vectors. The cosine similarity is as follows:",{"@attributes":{"id":"p-0039","num":"0040"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mi":"sim","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"X","mo":",","msub":{"mi":["D","j"]}}}},"mo":"=","mfrac":{"mrow":[{"munder":{"mo":"\u2211","mrow":{"msub":{"mi":["t","i"]},"mo":"\u2208","mrow":{"mo":["(",")"],"mrow":{"mi":"x","mo":"\u22c2","msub":{"mi":["D","j"]}}}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":[{"mi":["x","i"]},{"mi":["d","ij"]}],"mo":"\u00b7"}},{"msub":[{"mrow":{"mo":["\uf605","\uf606"],"mi":"X"},"mn":"2"},{"mrow":{"mo":["\uf605","\uf606"],"msub":{"mi":["D","j"]}},"mn":"2"}],"mo":"\u00b7"}]}}}},"br":{},"sub":["j ","i ","j","i ","i ","ij ","i ","j","2","2","2","3","j","2 ","j"],"sup":["2","2","2"]},{"@attributes":{"id":"p-0040","num":"0041"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"label","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"X"}},{"munder":{"mrow":{"mi":["arg","max"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"msub":{"mi":["l","i"]}},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"munder":{"mo":"\u2211","mrow":{"mrow":{"mi":["All","where"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"msub":{"mi":["D","j"]},"mrow":{"mi":"lable","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["D","j"]}}}},"mo":"=","msub":{"mi":["l","i"]}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"sim","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"X","mo":",","msub":{"mi":["D","i"]}}}}}}}],"mo":"="}}}},"In another implementation, a different statistical classification and machine learning technique (e.g., including regression models, Bayesian classifiers, decision trees, neural networks, and support vector machines) other than a nearest-neighbor classification technique is used to generate trained classifier .","Classification module  submits low frequency of occurrence (FOO) query terms , one-by-one (via a respective search query ), to search engine . Responsive to receiving search result  associated with a particular search query , and using techniques already described, classification module  extracts features (extracted features ) from one or more retrieved search results  identified by the search result . In this implementation, features are extracted from a first top-ranked Search result . For each retrieved and parsed Search result , classification module  stores the following information in a respective record of extracted features : the URLs, result titles, short descriptions and\/or contexts of the query term, and search query  used to obtain the retrieved Search result . Next, classification module  tokenizes, reduces dimensionality, and normalizes extracted features  derived from low FOO query terms  to generate term vectors . Then, classification  clusters the query terms into a respective set of clusters . This clustering operation is performed using trained classifier  (generated from high FOO query terms ).","Classification  evaluates end-user submitted term\/phrase  in view of these expanded term clusters (generated based on low FOO query terms ) to identify and return one or more suggested term lists  to the end-user. An exemplary such procedure is described above in paragraphs [0032] and [0033], and in the following section.","An Exemplary Procedure",{"@attributes":{"id":"p-0044","num":"0045"},"figref":["FIG. 2","FIG. 1","FIG. 1"],"b":["200","202","112","116","120","112","116","204","112","120","132","130","206","112","130","134","136","136","120"]},"At block , STS module  applies a clustering algorithm to group substantially similar terms based on term vectors  into term clusters . At block , responsive to receiving a term\/phrase  from an end-user, STS module  generates a suggested term list  from any keywords\/key phrases from the term clusters  determined to be substantially similar to the term\/phrase . At block , STS module  determines whether any keywords\/phrases from keyword clusters  were determined to be substantially similar to term\/phrase . If so, the procedure continues at block , where in STS module  sends the suggested term list  to the end-user. Otherwise, the procedure continues at block  of  as shown by on-page reference \u201cA\u201d.",{"@attributes":{"id":"p-0046","num":"0047"},"figref":["FIG. 3","FIG. 2","FIG. 1"],"b":["300","300","200","302","112","140","138","120","304","112","122","132","130","306","112","134","130","136","308","112","136","122","140","138"]},"At block , STS module  generates a suggested term list  from the keywords\/key phrases from term clusters  based on the low frequency of occurrence prairie terms  that are determined to be substantially similar to the term\/phrase . At block , STS module  sends the suggested term list  to the end-user.","An Exemplary Operating Environment",{"@attributes":{"id":"p-0048","num":"0049"},"figref":["FIG. 4","FIG. 1","FIGS. 2 and 3"],"b":["400","100","400","400","400"]},"The methods and systems described herein are operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of well-known computing systems, environments, and\/or configurations that may be suitable for use include, but are not limited to, personal computers, server computers, multiprocessor systems, microprocessor-based systems, network PCs, minicomputers, mainframe computers, distributed computing environments that include any of the above systems or devices, and so on. Compact or subset versions of the framework may also be implemented in clients of limited resources, such as handheld computers, or other computing devices. The invention is practiced in a distributed computing environment where tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment, program modules may be located in both local and remote memory storage devices.","With reference to , an exemplary system for related term suggestion for multi-sense query includes a general purpose computing device in the form of a computer . The following described aspects of computer  are exemplary implementations of client computing device PSS server  () and\/or client computing device . Components of computer  may include, but are not limited to, processing unit(s) , a system memory , and a system bus  that couples various system components including the system memory to the processing unit . The system bus  may be any of several types of bus structures including a memory bus or memory controller, a peripheral bus, and a local bus using any of a variety of bus architectures. By way of example and not limitation, such architectures may include Industry Standard Architecture (ISA) bus, Micro Channel Architecture (MCA) bus, Enhanced ISA (EISA) bus, Video Electronics Standards Association (VESA) local bus, and Peripheral Component Interconnect (PCI) bus also known as Mezzanine bus.","A computer  typically includes a variety of computer-readable media. Computer-readable media can be any available media that can be accessed by computer  and includes both volatile and nonvolatile media, removable and non-removable media. By way of example, and not limitation, computer-readable media may comprise computer storage media and communication media. Computer storage media includes volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information such as computer-readable instructions, data structures, program modules or other data. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by computer .","Communication media typically embodies computer-readable instructions, data structures, program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism, and includes any information delivery media. The term \u201cmodulated data signal\u201d means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example and not limitation, communication media includes wired media such as a wired network or a direct-wired connection, and wireless media such as acoustic, RF, infrared and other wireless media. Combinations of the any of the above should also be included within the scope of computer-readable media.","System memory  includes computer storage media in the form of volatile and\/or nonvolatile memory such as read only memory (ROM)  and random access memory (RAM) . A basic input\/output system  (BIOS), containing the basic routines that help to transfer information between elements within computer , such as during start-up, is typically stored in ROM . RAM  typically contains data and\/or program modules that are immediately accessible to and\/or presently being operated on by processing unit . By way of example and not limitation,  illustrates operating system , application programs , other program modules , and program data . In one implementation, wherein computer  is a PSS server . In this scenario, application programs  comprise search term suggestion model , and classification model . In this same scenario, program data  comprises term\/phrase , suggested term list , historical queries , search query , search result , search results , extracted features , term vectors , keyword clusters , trained classifier , and other data .","The computer  may also include other removable\/non-removable, volatile\/nonvolatile computer storage media. By way of example only,  illustrates a hard disk drive  that reads from or writes to non-removable, nonvolatile magnetic media, a magnetic disk drive  that reads from or writes to a removable, nonvolatile magnetic disk , and an optical disk drive  that reads from or writes to a removable, nonvolatile optical disk  such as a CD ROM or other optical media. Other removable\/non-removable, volatile\/nonvolatile computer storage media that can be used in the exemplary operating environment include, but are not limited to, magnetic tape cassettes, flash memory cards, digital versatile disks, digital video tape, solid state RAM, solid state ROM, and the like. The hard disk drive  is typically connected to the system bus  through a non-removable memory interface such as interface , and magnetic disk drive  and optical disk drive  are typically connected to the system bus  by a removable memory interface, such as interface .","The drives and their associated computer storage media discussed above and illustrated in , provide storage of computer-readable instructions, data structures, program modules and other data for the computer . In , for example, hard disk drive  is illustrated as storing operating system , application programs , other program modules , and program data . Note that these components can either be the same as or different from operating system , application programs , other program modules , and program data . Operating system , application programs , other program modules , and program data  are given different numbers here to illustrate that they are at least different copies.","A user may enter commands and information into the computer  through input devices such as a keyboard  and pointing device , commonly referred to as a mouse, trackball or touch pad. Other input devices (not shown) may include a microphone, joystick, game pad, satellite dish, scanner, or the like. These and other input devices are often connected to the processing unit  through a user input interface  that is coupled to the system bus , but may be connected by other interface and bus structures, such as a parallel port, game port or a universal serial bus (USB).","A monitor  or other type of display device is also connected to the system bus  via an interface, such as a video interface . In addition to the monitor, computers may also include other peripheral output devices such as speakers  and printer , which may be connected through an output peripheral interface .","The computer  operates in a networked environment using logical connections to one or more remote computers, such as a remote computer . The remote computer  may be a personal computer, a server, a router, a network PC, a peer device or other common network node, and as a function of its particular implementation, may include many or all of the elements described above relative to the computer , although only a memory storage device  has been illustrated in . The logical connections depicted in  include a local area network (LAN)  and a wide area network (WAN) , but may also include other networks. Such networking environments are commonplace in offices, enterprise-wide computer networks, intranets and the Internet.","When used in a LAN networking environment, the computer  is connected to the LAN  through a network interface or adapter . When used in a WAN networking environment, the computer  typically includes a modem  or other means for establishing communications over the WAN , such as the Internet. The modem , which may be internal or external, may be connected to the system bus  via the user input interface , or other appropriate mechanism. In a networked environment, program modules depicted relative to the computer , or portions thereof, may be stored in the remote memory storage device. By way of example and not limitation,  illustrates remote application programs  as residing on memory device . The network connections shown are exemplary and other means of establishing a communications link between the computers may be used.","Although the systems and methods for related term suggestion for multi-sense query have been described in language specific to structural features and\/or methodological operations or actions, it is understood that the implementations defined in the appended claims are not necessarily limited to the specific features or actions described. Accordingly, the specific features and actions are disclosed as exemplary forms of implementing the claimed subject matter."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["In the figures, the left-most digit of a component reference number identifies the particular figure in which the component first appears.",{"@attributes":{"id":"p-0009","num":"0010"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0010","num":"0011"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0011","num":"0012"},"figref":["FIG. 3","FIG. 2"]},{"@attributes":{"id":"p-0012","num":"0013"},"figref":"FIG. 4"}]},"DETDESC":[{},{}]}
