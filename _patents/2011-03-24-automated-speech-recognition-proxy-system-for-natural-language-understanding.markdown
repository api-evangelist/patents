---
title: Automated speech recognition proxy system for natural language understanding
abstract: An interactive response system mixes HSR subsystems with ASR subsystems to facilitate overall capability of voice user interfaces. The system permits imperfect ASR subsystems to nonetheless relieve burden on HSR subsystems. An ASR proxy is used to implement an IVR system, and the proxy decides, based on a set of rules, to route an utterance solely to one ASR, to route it to an HSR in addition to at least one ASR, to route it solely to one or more HSR subsystem, to reroute an utterance originally sent to an ASR over to an HSR, to use HSRs to help tune and train one or more ASRs, and to use multiple ASRs to increase reliability of results.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08484031&OS=08484031&RS=08484031
owner: Interactions Corporation
number: 08484031
owner_city: Franklin
owner_country: US
publication_date: 20110324
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["RELATED APPLICATION","FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","SUMMARY","DETAILED DESCRIPTION"],"p":["This application is a continuation-in-part of, and claims priority under 35 USC \u00a7120 to, commonly owned co-pending U.S. patent application Ser. No. 12\/985,174 entitled \u201cAutomated Speech Recognition System for Natural Language Understanding\u201d, filed Jan. 5, 2011, the contents of which are hereby incorporated by reference as if fully set forth herein.","This invention relates to the field of interactive response communication systems, and, more particularly to an interactive response communications system that selectively routes utterances to automated speech recognition (ASR) processor(s), human speech recognition (HSR) resource(s), or both ASR and HSR facilities.","Many companies interact with their customers via electronic means (most commonly via telephone, e-mail, and online text chat). Such electronic systems save the companies a large amount of money by limiting the number of customer service or support agents needed. These electronic systems, however, generally provide a less than satisfactory customer experience. The customer experience may be acceptable for simple transactions, but are frequently inconsistent or downright frustrating if the customer is not adept at talking to or interacting with a computer.","Such interactive response systems are well known in the art. For example, providing customer service via telephone using an interactive voice response (IVR) system is one such system. An example of customer service systems utilizing IVR technology is described in U.S. Pat. No. 6,411,686. An IVR system typically communicates with customers using a set of prerecorded phrases, responds to some spoken input and touch-tone signals, and can route or transfer calls. A drawback to such IVR systems is that they are normally built around a \u201cmenu\u201d structure, which presents callers with just a few valid options at a time and require a narrow range of responses from callers.","Many of these IVR systems now incorporate speech recognition technology. An example of a system incorporating speech recognition technology is described in U.S. Pat. No. 6,499,013. The robustness of the speech recognition technology used by IVR systems vary, but often have a predetermined range of responses that they listen for and can understand, which limits the ability of the end user to interact with the system in everyday language. Therefore, the caller will often feel that they are being forced to speak to the system \u201cas though they are talking to a computer.\u201d Moreover, even when interacting with a system that utilizes speech recognition, customer input is often either not recognized or incorrectly determined, causing the customer to seek a connection to a human customer service agent as soon as possible.","Human customer service agents continue to be used for more involved customer service requests. These agents may speak to the customer over the phone, respond to customer e-mails, and chat with customers online. Agents normally answer customer questions or respond to customer requests. Companies have customer service groups, which are sometimes outsourced to businesses that specialize in \u201ccustomer relations management.\u201d Such businesses run centers staffed by hundreds of agents who spend their entire working day on the phone or otherwise interacting with customers. An example of such system is described in U.S. Pat. No. 5,987,116.","The typical model of customer service interaction is for one agent to assist a customer for the duration of the customer's interaction. At times, one agent (for example, a technical support representative) may transfer the customer to another agent (such as a sales representative) if the customer needs help with multiple requests. But in general, one agent spends his or her time assisting that one customer for the full duration of the customer's call or chat session, or is occupied resolving the customer's issue via e-mail. Most call centers also expect the agent to take the time to log (document) the call. Deficiencies in this heavy agent interface model is (1) there is a high agent turnover rate and (2) a great deal of initial and ongoing agent training is usually required, which all add up to making customer service a significant expense for these customer service providers.","In order to alleviate some of the expenses associated with agents, some organizations outsource their customer service needs. One trend in the United States in recent years, as high-speed fiber optic voice and data networks have proliferated, is to locate customer service centers overseas to take advantage of lower labor costs. Such outsourcing requires that the overseas customer service agents be fluent in English. In cases where these agents are used for telephone-based support, the agent's ability to understand and speak clearly in English is often an issue. An unfortunate result of off shore outsourcing is misunderstanding and a less than satisfactory customer service experience for the person seeking service.","Improved interactive response systems blend computer-implemented speech recognition with intermittent use of human agents. To some extent, this has been done for years; U.S. Pat. No. 5,033,088 addresses a system using both a human attendant and an automated speech recognizer. Likewise, U.S. Pat. No. 7,606,718 discloses a system in which a human agent is presented with only portions of a call requiring human interpretation of a user's utterance. The contents of these patents, as well as all other art referred to herein, is hereby incorporated by reference as is fully set forth herein. Interest in such systems is enhanced if they are relatively low in cost, which generally calls for limited human interaction. To achieve such limited human interaction, it would be desirable to have a system that required minimal initial training and for which results continued to improve over time. In particular, a learning\/training system that provides \u201cday-one\u201d performance that is suitable for production use and that improves in efficiency quickly over time would be particularly valuable.","Many existing ASR systems suffer from serious training constraints such as the need to be trained to recognize the voice of each particular user of the system or the need to severely limit recognized vocabulary in order to provide reasonable results. Such systems are readily recognizable by users as being artificial. Consider the difference between the typical human prompt, \u201cHow can I help you?\u201d and the artificial prompt, \u201cSay MAKE if you want to make a reservation, STATUS if you would like to check on status of a reservation, or CANCEL to cancel a reservation.\u201d","Systems that are more ambitious, such as Natural Language Understanding (NLU) systems, require extensive machine learning periods in order to get usable results from larger grammars and vocabularies. Particularly in environments in which vocabulary may be dynamic (such as a system to take ticket orders for a new play or for a concert by a new musical group), the learning period may be far too long to provide satisfactory results. Inclusion of accents, dialects, regional differences in grammar and the like further complicate the task of teaching such systems so that they can achieve reasonable thresholds of recognition accuracy.","ASR systems currently available are effective at recognizing simple spoken utterances such as numbers, data, and simple grammars (i.e., a small set of words). However, to date ASR systems have not provided a high enough level of understanding to create a voice interface that provides a free-flowing conversation. Additionally, ASR performance degrades not only with accents and dialects as noted above, but also with background noise and, in many cases, female rather than male voices. ASR performance is improving over time, with some systems using statistical language models intended to recognize an extremely wide range of responses from callers, such that callers can be recognized even when they speak naturally rather than in a highly constrained manner. Even so, ASR performance has not yet rivaled actual interaction between humans, and the ASR systems that provide the highest levels of performance are time consuming and expensive to build and to tune for specific applications.","Tuning of grammars by considering statistical probabilities of various expected answers, as well as synonyms, is one technique used to improve ASR performance. Another is development of statistical language models, which can involve significant efforts transcribing recordings of utterances of live phone conversations with live operators. ASR performance is quite acceptable in certain applications but is not yet suitable for others, so known ASR-based systems continue to lack capability for understanding natural unconstrained utterances.","Therefore, there remains a need in the art for an interactive system that provides a consistently high-quality experience without the limitations of constituent ASR components.","An interactive response system mixes HSR subsystems with ASR subsystems to facilitate natural language understanding and improve overall capability of voice user interfaces. The system permits imperfect ASR subsystems to nonetheless relieve burden on HSR subsystems. An ASR proxy is used to implement an IVR system, and the proxy decides, based on a set of rules, to route an utterance solely to one ASR, to route it to an HSR in addition to at least one ASR, to route it solely to one or more HSR subsystem, to reroute an utterance originally sent to an ASR over to an HSR, to use HSRs to help tune and train one or more ASRs, and to use multiple ASRs to increase reliability of results.","In one aspect, the ASR proxy includes a recognition decision engine and a results decision engine. In a related aspect, these two engines facilitate recognition performance, natural language understanding and recognition and grammar tuning.","In a further aspect, the ASR proxy selects ASR and\/or HSR resources based on application criteria, historical results, and recognition experienced with a particular user's voice.","In yet another aspect, the ASR proxy is configurable based on various parameters, such as maximizing use of ASR, or making communication more or less \u201chuman-like\u201d.","In still another aspect, selection of ASR or HSR resources by the ASR proxy is transparent to a software application calling upon the ASR proxy for voice recognition.","Those skilled in the art will recognize that a particular configuration addressed in this disclosure can be implemented in a variety of other ways. Unless otherwise defined, all technical and scientific terms used herein have the same meaning as commonly understood by one of ordinary skill in the art to which this disclosure belongs.","The features described above may be used alone or in combination without departing from the scope of this disclosure. Other features, objects, and advantages of the systems and methods disclosed herein will be apparent from the following detailed description and figures.","Description of operation of an interactive response system, and of related machine learning systems and processes, is provided first, according to . Description of operation of an ASR proxy system and processes related thereto are described thereafter, according to . Note that unless otherwise evident, the terms \u201cintent\u201d and \u201cmeaning\u201d used herein refer to the contextual reason corresponding to an utterance (for instance, having a system determine a caller's business intent to make a new flight reservation). In contrast, the term \u201crecognize\u201d and its derivatives is generally used herein for the process of converting a sound to its corresponding word.",{"@attributes":{"id":"p-0044","num":"0043"},"figref":["FIG. 1","FIG. 1"],"b":["102","100","101","102","103","104","102","100","101","102","102","100"]},"In this architecture of , in various embodiments a number of different types of devices are used to implement each of the interactions platform  and communications links . Interactions platform  may be implemented by any device capable of communicating with the customer . For example, interactions platform  is in one embodiment a telephony server in interactive response system  where the customer is calling by telephone. The telephony server handles answering, transferring and disconnecting incoming calls. The telephony server is also a storehouse for prerecorded audio clips so that it can play any welcome prompt and as other audio clips as directed by iRouter .","A telephony server in accordance with this embodiment is assembled from off-the-shelf components, for example Windows for an operating system, a central processor, such as a Pentium processor, and an Intel Dialogic voice board. Using this architecture, the communications link  is implemented by any means of providing an interface between the customer's telephone and the telephony server. For example, communications link  is in various embodiments a dial-up connection or a two-way wireless communication link.","In another exemplary embodiment, interactions platform  is a gateway server in interactive response system . In accordance with this exemplary embodiment, the customer interacts with the interactive response server by e-mail, interactive text chats or VOIP. The gateway server runs customized open source e-mail, www server software or SIP. Further, a gateway server in accordance with this exemplary embodiment is designed to conduct e-mail, interactive text chat, or VOIP transactions with customers, while also forwarding and receiving data to other elements of the system. Using this architecture, the communications link  is implemented by any means of providing an interface between the customer's computer and the gateway server. For example, communications link  is in various embodiments a dedicated interface, a single network, a combination of networks, a dial-up connection or a cable modem.","While only one interactions platform  is illustrated in , one skilled in the art will appreciate that multiple interactions platforms  may be used in this system after studying this specification. With multiple interactions platforms , an interactive response system may communicate via voice and text data with a customer. Further, multiple customer bases may be accommodated by a dedicated interactions platform  for each of the customer bases. In this manner, a workflow (as will be described further, below) is selected by determining which of the multiple interactions platforms  initiated the interaction.","In the architecture of , the iRouter  comprises software to control interactive response system . iRouter  \u201cowns\u201d the interaction with customer  from beginning to end by coordinating activity among other components and managing the transaction. iRouter  manages interactions with customer  according to one or more programmable scripts, called, according to this exemplary embodiment, \u201cworkflows.\u201d In general, a workflow comprises an interaction flow wherein the path through the workflow depends upon intent input from the customer. Workflows are preprogrammed by system engineers and, advantageously, periodically \u201ctweaked\u201d in order to improve customer satisfaction, speed, accuracy, etc. In accordance with this exemplary embodiment, iRouter  is almost always \u201cin charge\u201d of selecting the next step or path in the workflow.","iRouter  receives interaction input from interactions platform  in the form of audio clips, email, text data or other interaction type\u2014depending on the form of customer communication\u2014and forwards the input to one or more human agents  (sometimes referred to as \u201cIntent Analysts\u201d or \u201cIAs\u201d), speech recognition engines or expert systems (collectively , and sometimes referred to as \u201cautomated speech recognizers\u201d or \u201cASRs\u201d) and uses the responses to advance its current workflow. When human interpretation (or translation) of the input is necessary, iRouter  directs human agent desktop software to display an appropriate visual context of the current workflow. Once iRouter  understands the input, iRouter  advances through the workflow and directs interactions platform  to respond appropriately to customer .","In an exemplary embodiment wherein interactions platform  comprises a telephony server, iRouter  delivers sound clips to play back to a customer, send text-to-speech clips or both. Alternatively, interactions platform  may store sound clips, have text-to-speech capability or both. In this embodiment, iRouter directs interactions platform  as to what to play to a customer and when.","iRouter  comprises, in this exemplary embodiment, a networked, off-the-shelf commercially available processor running an operating system such as Windows or Linux. Further, iRouter  software includes a modified open Voice XML (VXML) browser and VXML script incorporating objects appropriate to the specific application. One skilled in the art will understand how to construct these objects after studying this specification.","In accordance with the exemplary architecture of , interactive response system  includes at least one pool of human agents . A pool of human agents  is often located at a contact center site. Human agents , in accordance with the present embodiment of this invention, use specialized desktop software specific to system  (as will be described further, below, in connection with , B and B) that presents a collection of possible intents on their screen (their user interface)\u2014along with a history or context of the customer interaction to that point. The human agent or agents  interpret the input and select an appropriate customer intent, data or both.","For telephone interactions, human agents  wear headphones and hear sound clips (\u201cutterances\u201d) streamed from the telephony server  at the direction of iRouter . In accordance with one aspect of this invention, a single human agent  will not handle the entire transaction for customer . Rather, human agent  handles some piece of the transaction that has been designated by the workflow designer as requiring human interpretation of customer's  utterance. IRouter  can send the same customer  interaction to any number of human agents , and may distribute pieces of a given interaction to many different human agents .","In accordance with the exemplary embodiment of this invention, human agents  are preferably off-site. Further, human agents  may be in diverse geographic areas of the world, such as India, the Philippines and Mexico. Human agents  may be in groups in a building or may be working from home. In applications that require 24\/7 human agent support, human agents  may be disposed around the world so that each human agent  may work during suitable business hours.","Interactive response system  of the present invention employs custom human agent application software. Human agents  use a custom application developed in Java and running on a standard call center computer network workstation. Generally speaking, interactive response system  applies human intelligence towards interpretation of customer  input into \u201cintent\u201d (what the customer wants) and data (any input required to determine what the customer wants). The interpretation normally comprises selecting the most-correct interpretation of what was said from a list of choices, in this exemplary embodiment. In an alternate embodiment, computer-aided data entry (e.g., auto-completion of text entry or entry of an email address) is used in connection with agent processing.","Workflow server  of the present invention, an off-the-shelf component, is an archive of the workflows used by the Interactions router. Workflow server  is in one embodiment built with off-the-shelf hardware using a commercially available processor running a standard server operating system, with the workflow documents written in XML in this exemplary embodiment. Workflow server  maintains a compilation of business rules that govern the behavior of iRouter .","Interactive response system  employs a workflow designer used by a business analyst or process engineer to map out workflows. A workflow serves as the map that iRouter  follows in a given interaction, with speech recognition or human agents. The workflow \u201csteers\u201d iRouter  along a path in the workflow in response to customer input. A place in the workflow, along with data collected to that point is called a \u201ccontext.\u201d","The workflow designer builds instructions for human agent  into the workflow in order to guide human agent  in interpreting intent. The workflow designer may include a version of Eclipse software development environment customized to focus on building XML documents. However, one skilled in the art will be able to develop a workflow designer after studying this specification.","Performance and interactions archive  of the present invention comprises a database that can be maintained on any common computer server hardware. Performance and interactions archive  contains both archival data of system transactions with customers  (i.e., a repository of sound clips, e-mails, chats, etc. from interactions with customer ) as well as performance data for human agents .","This exemplary embodiment employs \u201creporter\u201d software to generate statistics about a group of interactions or to display performance ranking for human agent . Reporter software can also reconstruct an interaction with customer  from sound clips, e-mails, or chat text that constituted customer's  contact stored in interactions archive . Reporter software is a series of simple scripts, and can run on any common server hardware.","This exemplary embodiment also includes manager\/administrator software, usually run from the same station as reporter software. Manager\/administrator software sets operating parameters for interactive response system . Such operating parameters include, but are not limited to, business rules for load balancing, uploading changes in workflow, and other administrative changes. In one particular embodiment, manager\/administrator software is a small custom Java application running on a standard call center computer workstation.","Support system  consists of numerous databases and customer proprietary systems (also including off-the-shelf automated speech recognition (ASR) software such as Nuance) that may be employed in responding to customer  requests. For example, support system  may include a database for customer information or a knowledge base. Speech recognition software is, in this exemplary embodiment, an off-the-shelf component used to interpret customer  utterances. Support system  may also include a text-to-speech capability, often off-the-shelf software that reads text to customer .","Company agents  of the present invention consist of human agents that handle customer  requests that the workflow refers to them. For example, should customer  intend to obtain assistance with a company matter, and an outsourced human agent  identifies that intent, the workflow may direct interactive response system  to transfer the call to company agent .","The elements of interactive response system  communicate over a TCP\/IP network in this exemplary embodiment. Communication is driven by the workflow that iRouter  follows. \u201cDatabase\u201d in the present embodiment can be a flat file database, a relational database, an object database, or some combination thereof.","Turning now to , these figures illustrate an example of how information is retrieved and handled by interactive response system  when a customer interacts with the interactive response system  via telephone. The example shown in  presupposes that all required hardware, software, networking and system integration is complete, and that a business analyst has mapped out the possible steps in a customer interaction using the graphic workflow designer. The business analyst also has scripted the text for anything that the interactive response system may say to a customer , including, but not limited to, the initial prompt (e.g., \u201cThank you for calling, how can I help you today?\u201d), response(s) to a customer, requests for additional information, \u201cstutter speech\u201d (sounds sent to the customer while the iRouter  is determining a response), and a closing statement. Either text-to-speech software or voice talent records the server-side speech pieces as written by the business analyst. This workflow is then loaded into the interactive response system  where it is available to the iRouter .","As shown in block , the interaction begins with the customer  calling the customer service telephone number of a company. The interactions platform , in this case a telephony server, answers the telephone call and retrieves the appropriate workflow stored in the workflow database, based on either (1) ANI\/DNIS information of the caller or (2) other business rules (e.g., line or trunk the call came in on), as illustrated at block . The telephony server then plays the appropriate welcome prompt as illustrated at block  and the customer then responds to that prompt (block ).","For purpose of example, an imaginary airline, Interair, provides customer service via an interactive response system in accordance with a call center embodiment of this invention. The interaction platform  is therefore a telephony interface and iRouter  selects a workflow appropriate to Interair.","A first point or context in the workflow is shown in the illustrative workflow of . There is no customer utterance, thus no intent or data to capture (and respond to). The only response is the greeting and the prompt for customer input.","Processing proceeds to box  in the flowchart of . The telephony server begins digitizing the customer's spoken input and connects to the iRouter. At this point, workflow or business rules determine if the interactive response to the customer needs to be handled by a human agent or speech recognition software. That is, the iRouter selects the appropriate workflow for the call from the workflow repository and follows the workflow rules to conduct a conversation with the customer.","To interpret customer speech, iRouter  uses ASR from the support systems or has the customer's audio streamed to human agents  in contact centers as appropriate, as illustrated in block . If human agents  are required by the workflow, iRouter  identifies available human agents by applying a load balancing algorithm, triggers a pop-up on their screens (as illustrated in the initially blank pop-up screen, ), presents several selectable intent options, and begins streaming customer audio to the identified human agents, as shown at block . This load balancing, at various times, includes identifying more or fewer human agents for interpreting the utterance based on any of a variety of factors, as will occur to those skilled in the art given the present disclosure. The human agent(s) hear the customer utterance in headphones, and computer software prompts for an interpretation of the utterance as shown in blocks  and .","In accordance with the exemplary workflow of , the customer utterance that the human agent or agents hear is \u201cI need to check my flight from Chicago to London this afternoon.\u201d The agents' screen indicates the current context (or point in the workflow) as illustrated in . In this illustrative screen shot, there are 12 possible requests (including unanswerable and terminate) that the human agent can select. In operation, there are several hundred possible interpretations available to the agents. Such multiplicity of selection allows the agents interpretive flexibility, which enables the iRouter to jump around in its workflow according to the interpreted intent. Thus, in accordance with one aspect of this invention, the iRouter can respond appropriately even if the customer changes subjects in midstream.","In each case, each agent selects what he or she feels is the best fit interpretation of the customer utterance in the current context of the workflow. In example of , the human agent(s) selects \u201cCFT\u201d (Check Flight Time) and enters or selects from drop down menus the departure and arrival cities (or other, preprogrammed information that the customer could possibly utter).","Note that, in blocks  and , human agents can elect to apply acceleration to the customer audio clip(s) received at the station in order to compensate for any response delay (usually due to lag time in application set-up\u2014the time it will take for human agent desktop software to accept the streaming audio and display the appropriate workflow). Network latency might be around 0.2 seconds, where application delay could be more in the 1+second range. To compensate for the application delay, the interactive response system accelerates the voice clip (although not to the point of discernible distortion). The purpose is to strive for a more \u201creal-time\u201d conversational interaction, so that the customer does not experience a notable delay while awaiting a response. The acceleration is applied to the speech as it is streaming from the telephony server. The acceleration can never overcome the inherent latency of the link but will allow human agents to \u201crecover\u201d any application set-up time and reduce the amount of lag time in the interaction, ideally up to the limits imposed by latency in the network. However, acceleration is optional, wherein a novice agent may need a slower playback, while a more experienced agent may apply acceleration.","In test , the iRouter evaluates the accuracy, in real time, of the customer audio interpretation and updates each agent's speed\/accuracy profile. Next, in block , the iRouter processes the interpretation and performs the next step(s) in the workflow (e.g., database lookup based on input data) and then forwards an appropriate response  to the customer through the telephony server (if the interpretation is deemed accurate). If the iRouter determines the interpretation is accurate, it directs the playback of responses to the customer from the telephony server based on the interpretation of either the speech recognition software or by applying key algorithms to the responses of one or more human agents. In this example, the response is given in the last block of screen , .","To determine accuracy, the iRouter compares the interpretation of two human agents, and, if no consensus is reached, plays the customer audio clip for a third human agent for a further interpretation (i.e., \u201cmajority rule\u201d determines which is the accurate response). Other business rules may also be used to determine the accurate interpretation. For example, an interpretation from the agent with the best accuracy score may be selected. Alternatively, one of the interpretations may be selected and played back to the customer (\u201cI understood you to say . . . \u201d) and the customer response determines whether the interpretation was correct. Further, the interpretations may be selected from known data (e.g., two interpretations of an email address could be compared against a database of customer email addresses, only one of two interpretations of a credit card number will pass a checksum algorithm, etc.).","The interactive response system allows for virtually any number of human agents to handle to same customer interaction at once. That is, an interactive response system could have two agents listening during a busy time or have seven human agents listening during a more idle time. Moreover, during times of high call volume, accuracy can be decreased by removing the \u201cdouble-checking\u201d rule to maintain high response time. An agent assigned a high trust ranking based on the agent's speed\/accuracy profile may be asked to work without the double-checking. In addition to trading off accuracy for quicker system availability, a steady flow of audio clips is flowing by each agent, thereby decreasing human agent \u201cslack\u201d time.","Returning to the flowchart of , either the customer will respond again as seen in block , the call will be transferred (if so directed by a step in the workflow or by business rules), or the customer terminates the call, as shown in block . If the interpretation is deemed inaccurate in block , the iRouter  plays a stall speech to the customer (block ) and sends the audio clip to additional human agents for another interpretation (block ) and then reevaluate its accuracy.","The iRouter manages interaction with the customer to call completion, using the workflow as its guide. The iRouter may stream customer utterances to human agents for interpretation at numerous points in the call. Once the call has concluded, a snapshot of the customer interaction is preserved in the archive database. Human agents' speed\/accuracy profiles are constantly updated and maintained.","If human intervention is not needed to interpret customer's request, ASR interprets the audio clip and the iRouter determines the appropriate response as shown in blocks  and .","Continuing with the Interair example, the captured customer utterance, as seen in , has two requests: food and entertainment queries. In accordance with another aspect of this invention, the human agent captures two intents: meal and movie. There is no relevant data to enter because the interactive response system already knows the flight information from the previous data entered in  (this data is visible in ). As seen in , the human agent enters \u201cGeneral\u201d and \u201cMeal\u201d from an on-screen display of possible intents. The human agent also enters \u201cMovie.\u201d As seen in , the interactive response system then provides the appropriate response. As seen in , if the customer requests further information regarding the meal or movie such as: \u201cwhat meal is offered?\u201d, \u201cAre their special meals?\u201d, \u201cWhat is the movie rated?\u201d, the appropriate human agent interpretation options are located on the computer screen.",{"@attributes":{"id":"p-0082","num":"0081"},"figref":"FIG. 6","b":["601","602","602","101","603","604","605","606","101","101","101","607","608","101","609","101"]},"The discussion of an interactive response system and its constituent processes above in connection with  includes operation of one or more speech recognition and related subsystems . In practice, implementation of IVR system  requires such subsystems  to be capable of recognizing a significant portion of the customers' utterances in order to minimize the need for human interaction.","Referring now to , a training subsystem  is included as a part of IVR System . In operation, training subsystem  selectively provides machine learning capabilities to real-time ASRs in subsystems  to allow them to very quickly adapt to new or changed customer interactions. For instance, when an IVR system  is first installed for a company, the generic capabilities of an embedded ASR may not be very usable for actual customer interactions, particularly if those interactions include many industry-specific terms (e.g., an electrician calling to order a ground fault circuit interrupter would typically use the acronym \u201cGFCI\u201d, which few ASRs would recognize easily). Likewise, when a new offering becomes available, existing ASR capabilities may begin to fail even though they were previously successful (e.g., an ASR that correctly identified \u201ciPod\u201d in past uses may begin to fail upon introduction of another product with a similar name, such as \u201ciPad\u201d). In some applications, these changes may be infrequent, while in others, they may occur on a regular basis. For example, an application for selling tickets to rock concerts will need to adapt to new customer requests for band names on a regular basis.","In one embodiment, training takes place based on the indicated need for such training. For an existing system in which ASR accuracy is well above a threshold of acceptability, training may occur only rarely, if at all. In such instances, training could occur, for example, only during periods of extremely low call volume during which IAs  are otherwise relatively idle. Where a system is new or whenever ASR success is dropping below acceptable limits, more training may be called for and so training subsystem  is active more often.","A non real-time training ASR  of training subsystem  receives as input a customer utterance from iRouter  and a corresponding intent from IA . In practice, multiple training ASRs  may be used as described below.","As with real-time production processing, processing for purposes of non real-time training includes in some embodiments inputs from single IAs and in others inputs from multiple IAs. Differences in intent selected by different IA's are quite helpful in training an ASR, as they may indicate a particularly nuanced utterance that calls for extensive additional training In the simplest form, where a business intent may have a small grammar with very few options, such as \u201cyes\u201d or \u201cno\u201d, and where an ASR comes with a pre-packaged understanding of the utterances in \u201cyes\u201d and \u201cno\u201d, training may consist of building a statistical model that can be used for grammar tuning. In more complex training, the ASR is assisted in the recognition of words with domain knowledge, in order to build the statistical language model of the utterances that may be said.","In a preferred embodiment, IVR system  is implemented using multiple available real-time ASRs in support systems . In practice, each ASR is found to have strengths and weaknesses, and success in particular areas is usable by iRouter  to determine which ASR to use in a particular circumstance, as well as by training subsystem  to determine which ASR could benefit from training in a particular circumstance. Currently available ASRs include those from Carnegie Mellon University (Sphinx), Nuance, Dragon, Luquendo, Lumenvox, AT&T, SRI International, Nexidia, Microsoft and Google. As only select ASRs are available for no cost (e.g., under open source licenses), financial considerations may limit the number of ASRs to include in support systems . Because iRouter  can selectively route production requests to an ASR that is expected to perform well in any specific context, and because training subsystem  can likewise selectively train real-time ASRs based on expected improvement in their performance, it will often be advantageous to select a group of ASRs with performance characteristics that are somewhat orthogonal to one another. In that manner, one ASR can be expected to make up for weaknesses in another ASR. For example, an ASR optimized for processing telephonic speech may have performance characteristics quite different than one designed for speech from dictation equipment.","In order to increase accuracy of the real-time ASRs used with IVR system , training subsystem  facilitates machine learning by providing the real-time ASRs with training that is specific to the meaning of each received utterance, based on non real-time operation of training ASR .","Common ASRs are trained in several different aspects. First, ASRs must be able to classify audio streams, and portions of audio streams, into components that can help lead to recognition of a word that is being spoken. Typically, this involves identifying, within an audio stream, a set of similar sound classes known as \u201cphones,\u201d sound transitions or combinations known as \u201cdiphones,\u201d and potentially more complex waveform portions referred to generally as \u201csenones.\u201d Commonly, utterances are divided wherever periods of silence are detected. Features are derived from utterances by dividing the utterance frames (such as 10-millisecond timeframes) and extracting various different characterizing aspects of the audio within that timeframe, such as whether amplitude and frequency are increasing, constant or decreasing. In the Sphinx ASR available from Carnegie Mellon University, 39 features are extracted to represent speech as a \u201cfeature vector.\u201d Typically, ASR engines come with this aspect of their recognition fixed and users of such systems cannot change which features are analyzed or how they are analyzed.","ASRs use various models to proceed from raw audio waveform to a prediction of the word corresponding to the utterance. An acoustic model determines most probable features\/feature vectors for received senones. A phonetic model maps phones and words, with the words coming either from a fixed dictionary or from a vocabulary (or \u201cgrammar\u201d) derived by machine learning. A language model restricts candidate word choices based on some context, such as a previously recognized word. ASRs typically use a combination of these models to predict which words correspond to utterances. It is the latter two models, i.e., phonetic models and language models, that are the focus of training in the embodiments discussed below, although the concepts addressed herein could readily be applied to other models used in speech recognition.","In many instances, training an ASR can be more effectively accomplished by using context, either from previously recognized words or, for processing that is not in real time (i.e., later-recognized words in the same customer discourse). Such training is described below.","Turning first to phonetic models, consider the following user utterance: \u201cI would like to fly roundtrip between Boston and San Diego.\u201d An \u201coff-the-shelf\u201d ASR may have some difficulty recognizing some of these words across a variety of speakers. For example, in pronouncing the word \u201croundtrip\u201d some speakers may conflate the \u201cd\u201d and \u201ct\u201d consonant sounds into one sound (\u201crountrip\u201d), while others may enunciate them separately (as if they were the two words \u201cround\u201d and \u201ctrip\u201d).","In one embodiment, training subsystem  provides machine learning to non-real time training ASR  by addressing each of these issues. First, training subsystem  selects a target vocabulary based on a business meaning that corresponds to the utterance as determined by an IA  when the utterance was initially received. In this instance, the IA likely selected \u201cNew Reservation\u201d as the business meaning. Whereas the word \u201croundtrip\u201d may have been one word out of 40,000 in a general grammar, with a very low statistical rate of occurrence, it may be one word out of only 1,000 in a grammar specific to the \u201cNew Reservation\u201d intent, and may have a statistical rate of occurrence that is far higher. Thus, training subsystem , by changing the applicable grammar, significantly increases the probability that training ASR  will accept the word \u201croundtrip\u201d as what was spoken, even if the feature vectors vary significantly from a standardized model of that word. Furthermore, as additional utterances of \u201croundtrip\u201d become associated with the \u201cNew Reservation\u201d intent, those utterances likely will more closely match at least some of the previously recognized instances in which \u201croundtrip\u201d was spoken. Thus, over time both the likelihood of the word \u201croundtrip\u201d occurring in a \u201cNew Reservation\u201d intent and the variations in pronunciation of that word will lead to two results: (a) greater certainty in recognizing the word (which can be propagated to other grammars that include the same word, such as a grammar associated with the \u201cCancel Reservation\u201d intent); and (b) better ability to predict business intent by refined statistics as to how often the word is associated with a particular intent.","Returning to the utterance example used above, fast-talking speakers may blur the distinction between \u201cBoston\u201d and the following word \u201cand,\u201d and may fail to articulate all of the sounds such that training ASR  may be trying to analyze a sound, \u201cBostonan.\u201d Likewise, the city name \u201cSan Diego\u201d may be pronounced by some speakers in a manner that sounds more like \u201cSandy A-go.\u201d The selection of a \u201cNew Reservation\u201d-specific grammar rather than a generalized grammar would again likely dramatically increase the statistical likelihood that recognition of \u201cBoston\u201d and \u201cSan Diego\u201d will be achieved with confidence. As a further refinement, training subsystem  employs iterative passes through the utterances of an entire user discourse to improve training even further. In the example given above, it may be that later in the discourse the caller says \u201cBoston\u201d at the end of a sentence, in a manner readily recognized by training ASR . That speaker's acoustical signature for \u201cBoston\u201d is included in the ASR's mapping, so that on a second pass, the same speaker's \u201cBostonan\u201d utterance will be considered a better match for \u201cBoston\u201d than it was before. Similarly, the speaker may say \u201cSan Diego\u201d a second time in a manner that provides more distinction between \u201cSan\u201d and \u201cDiego,\u201d thereby providing learning that upon an iterative recognition attempt will lead to a greater likelihood of successful recognition of the first, blurred utterance. For extensive customer discourses, multiple iterations may lead to significant improvement in overall recognition, as the caller's voice characteristics become better understood through the words that the system can recognize.","Referring now also to , in one embodiment the actual time of recognition by an intent analyst is used to decompose an audio stream into separate utterances for recognition (e.g., by training ASR ). Specifically, the time of recognition of the utterance intent \u201cI want to take a flight from\u201d (, ), the time of recognition of the data portion \u201cBoston\u201d (, ), and the time of recognition of the data portion \u201cSan Diego\u201d (, ) are all sufficiently distinct that the timeframes themselves are usable to facilitate decomposition of the audio into separate utterances for recognition. In some instances, an IA may provide recognition before (or after) the utterance is complete (e.g., as shown in  at , \u201cSan Diego\u201d is recognized by the IA before the final \u201co\u201d sound), so in such cases time frames are adjusted to end at suitable pauses after (or before) the IA-provided recognition. The number of possible business intents and typical words used to express them are usable to narrow the intent recognition grammar, and the type of data collected (e.g., city names) are usable to narrow the data recognition grammar.","Moving on to language models, training system  again takes advantage of business intent to assist with training. For instance, where an IA has indicated a business intent of \u201cNew Reservation\u201d it may be statistically quite likely that at least one instance of the word \u201cand\u201d in the utterance will be preceded by one city name and followed by another city name. Likewise, if the words \u201cfrom\u201d or \u201cto\u201d are recognized, it may be statistically very probable that a city name follow those words. In contrast, if a business intent determined by an IA is \u201cseat assignment,\u201d those same words \u201cfrom\u201d and \u201cto\u201d may rarely correlate with an adjacent city name but a nearby number-letter pair instead (e.g., \u201cI would like to change from seat B to seat A.\u201d).","Such language model training also allows for ready adaptation to changing user phrasings. For example, if an airline begins service to England, it may suddenly start receiving requests using different language than was used before, for the same business meaning. For instance, the prior example of \u201cI would like to fly roundtrip between Boston and San Diego\u201d might be spoken by a British customer as \u201cI would like to book a return trip between Boston and London.\u201d Initially, the word \u201cbook\u201d and would not appear with high probability in the \u201cNew Reservation\u201d grammar, but statistical usage of that word in that grammar quickly increases with additional British customers. Likewise, use of the term \u201creturn\u201d changes with the addition of a British customer base, and the \u201cNew Reservation\u201d grammar is adjusted accordingly to recognize this.","Training subsystem  also adjusts statistics for recognition candidates based on a combination of business intent and adjacent recognized words in the discourse. Consider the example in which a business intent has been determined as \u201cNew Reservation\u201d and only one utterance in a user's discourse is not initially recognizable with a usable level of confidence. If the discourse is recognized to have included only one city name, the probability that the unrecognized utterance is another city name is quite high; the probability that it is a city name served by the airline using the system is higher yet. Changing the probabilities for candidate words within a grammar to recognize the partial recognition may well drop some candidates words from further consideration and may bring only one candidate (presumably a city name) to a usable level of certainty. Machine learning then incorporates that particular user's enunciation of the city into the ASR's model so that subsequent instances of similar utterances are more readily recognized.","Maintenance of separate grammars for each allowable business intent facilitates training subsystem  to provide more rapid teaching of ASRs than would otherwise be possible. For example, there are strong phonetic similarities in the utterances \u201cbook,\u201d \u201cnotebook\u201d and \u201cBucharest.\u201d Determining which of these meanings corresponds to a user's utterance is greatly enhanced by considering the business intent. For example, if the business intent is \u201cLost & Found,\u201d then \u201cbook\u201d (in its noun sense) and notebook (as in \u201cnotebook computer\u201d) may appear with much higher likelihood than in other contexts. If the business intent is \u201cNew Reservation,\u201d then \u201cbook\u201d (in its sense as a verb) may also appear with fairly high likelihood. Similarly, if the business intent is \u201cNew Reservation,\u201d then \u201cBucharest\u201d may appear with higher likelihood than if the business intent were, for instance, \u201cSeat Selection.\u201d","Once training ASR  has itself been sufficiently trained, correlations between business intents and language models can be developed in a very robust manner. For instance, one exemplary portion of a mapping for similar-sounding words might be as follows:",{"@attributes":{"id":"p-0102","num":"0101"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"84pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"98pt","align":"left"}}],"thead":{"row":[{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]},{"entry":[{},"Business Intent","Words and Probability"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"New Reservation","Book (0.8)"]},{"entry":[{},{},"Bucharest (0.1)"]},{"entry":[{},{},"Brook (0.1)"]},{"entry":[{},"Lost & Found","Book (0.7)"]},{"entry":[{},{},"Notebook (0.2)"]},{"entry":[{},{},"Bucharest (0.1)"]},{"entry":[{},"Seat Selection","Bulkhead (0.8)"]},{"entry":[{},{},"Bucharest (0.1)"]},{"entry":[{},{},"Book (0.1)"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}}},"Training ASR  is particularly well-suited to develop language model statistics because it has two advantages over real-time ASRs from support systems . First, since it is not used for production operations, it does not need to operate in real time and can therefore take advantage of more complex recognition algorithms that could not, at least on relatively modest computing platforms, perform recognition quickly enough to be used for real time processing. This allows training ASR  to recognize utterances that real time ASRs in support systems  would not be able to recognize. Second, training ASR  can take advantage not only of a priori information from a customer discourse, but a posteriori information as well. Thus, it can wait until all utterances in an interaction are analyzed and then take multiple passes at recognition, presumably with greater likelihood of success on subsequent iterations. As noted above, an initial user utterance that sounds like \u201cBostonan\u201d may be far more readily recognized after a second utterance of \u201cBoston.\u201d","Training ASR  builds, over time, a set of statistics related to language elements used with each associated business meaning. In one embodiment, multiple training ASRs  are used, each one contributing to the overall statistics. In some embodiments, the statistics include measures of certainty regarding recognition, based on multiple instances of recognition by a single training ASR , on agreement between multiple training ASRs , or both.","The statistics developed in this manner are usable by any of the real-time ASRs in support systems . Each of the different ASRs that may be used for real-time recognition in support systems typically has its own mechanism for training and corresponding specifications for how language models may be input to it for training. In a preferred embodiment, training subsystem  formats the statistics it develops for each of the ASRs in support systems  so that each of those ASRs can take advantage of the statistics generated by training subsystem . In practice, ASRs vary widely in the mechanisms they support for training, and training algorithm  is therefore readily configurable to collect, format and provide to ASRs training data in a manner appropriate for each existing ASR as well as new ASRs that may be added to support systems . As the performance of a real-time ASR improves with training, the quality of its recognition may allow it to replace the function of an IA  in processing ,.","Training subsystem  also works in conjunction with the capabilities of each ASR to ensure that the ASR training is leveraged maximally for use in IVR system . For example, an ASR may support a threshold determination for when sufficient portions of an utterance are recognized to be usable to perform statistical analysis, such as using sentence trees, and training algorithm  is configured to be compatible with such features so as to determine training progress.","The real-time ASRs in support systems  are used in two different ways calling for different statistical processing. In a first manner, they are used to recognize a process once an IA has determined a corresponding business intent. For example, one or more IAs  may select \u201cNew Reservation\u201d as a business intent for a sentence spoken by a caller, and based on that one or more real-time ASRs in support systems  will attempt to recognize the specific words spoken by the caller.","In a second manner, a real-time ASR rather than an IA is used to determine the business intent. This is a different recognition task than determining the specific words spoken by the caller. For instance, determining whether a business intent may be \u201cNew Reservation\u201d or \u201cSeat Request\u201d may involve recognition of a small number of highly likely key words specific to each intent, such as the words \u201cfrom\u201d and \u201cto\u201d for \u201cNew Reservation,\u201d and the words \u201caisle\u201d and \u201cwindow\u201d for \u201cSeat Request.\u201d One type of ASR in support systems  may be better suited to determine business intent, and another may be better suited to recognize words based on a that business intent. In one embodiment, the format of training statistics for each real-time ASR provided by training subsystem  is adjusted based on whether the real-time ASR is to be optimized for determining intent or recognizing words based on a determined intent.","Part of the training process includes determining how effective machine learning has been for the real-time ASRs in support systems . This is referred to as validation. In a preferred embodiment, validation is performed by training subsystem ; in alternate embodiments validation is performed by iRouter  or a dedicated validation processor (not shown). In validation, ASRs are operated in parallel with one another and with IAs to determine how their performance compares. Each training instance provides more information that is used to develop statistical models and probabilities of grammar uses for each business meaning provided by the IAs. In some situations, historical data from IAs also determine the anticipated level of automation that may be available with respect to an utterance. If IAs routinely provide multiple meanings for an utterance, it may be that an ASR will only be usable if it is capable of significant contextual training Those ASRs that have robust context processing may be able to properly process such utterances while those that are not contextually strong may be incapable of meeting a minimum threshold regardless of how much training is provided. As an example, the utterance \u201cIP\u201d could mean \u201cInternet Protocol\u201d or \u201cIntellectual Property.\u201d If used in an application where both meanings are common, mistakes in processing accuracy are to be expected unless an ASR is capable, after training, of deriving which of the two meanings is the appropriate one.","As training proceeds, the performance of a real-time ASR improves. At a point of statistical stabilization that meets the needs of the particular use of the ASR within IVR system , the ASR is placed into production operation. For example, an ASR intended to determine a business meaning for an utterance may operate in a non-production mode in parallel with an IA until such point as it has been trained enough that its performance approaches that of the IA, at which time it is switched to production operation to relieve the load on the IAs in processing , .","In a typical embodiment, in both real time production processing and training processing, input from two IAs is provided to two ASRs to increase accuracy. Should input from two IAs for the same utterance in the same user discourse differ, in some embodiments the utterance is submitted to a third IA (in some instances selected based on a measure of IA quality) for determination of meaning.","When an ASR reaches a level of accuracy above a certain threshold, as determined through validation and based on the specifics of the environment, training processing transitions. In one exemplary environment, the ASR is used for production processing but training continues as described above. In a less demanding environment, or in one with fewer available resources, training ceases altogether. In a third environment, training continues but at a reduced priority (e.g., training processing occurs only when there is a certain amount of available processing capacity or when the performance of the ASR is found to have deteriorated to a certain degree).","In some embodiments, a validation processor is configured to test ASRs to determine their performance levels. In some embodiments, validation follows a training phase, while in others it is performed concurrently with training. Based on results from validation, iRouter  changes its allocation of utterances to ASRs and IAs. For instance, if an ASR is found to perform sufficiently well in comparison with an IA in determining a business meaning, iRouter  routes utterances to that ASR far more often than to the IA. Advantageously, such routing is highly adaptable and configurable. Following the example used in connection with , based on performance statistics, iRouter  may favor an IA for response interpretation immediately after a welcome message (), favor a first ASR for response interpretation for movies or meals (), and favor a second ASR for response interpretation for seat assignments, plane info, and select other choices shown in . In some embodiments, two ASRs (as in , ) are selected for each specific area of interpretation to ensure accuracy. If both provide the same interpretation, the corresponding response is provided to the user. If the ASRs differ, then the utterance is provided to an IA to select a meaning through adjudication as in .","As a result, human IAs are required only at specific times when ASRs fail to adequately perform, and processing may flow back to the ASRs immediately after IA intervention depending on business criteria, with no need for an IA to stay connected to the customer discourse. Where training can improve ASRs, it does so without imposing significant additional cost or other overhead on the overall IVR system . Human interaction does not need to be any more involved than listening to a single user utterance and selecting a meaning, or intent, of the user from a drop-down list of predetermined options so that an appropriate automated response is provided to the user.","Referring now to , an exemplary processing flow  for ASR training is illustrated. A digitized audio stream with a user utterance is provided  to one or more IAs  and, if the IAs are able to provide a usable intent response as described in connection with , to training ASRs . If a training ASR  cannot sufficiently recognize  the utterance so as to convert the audio to a textual counterpart, the utterance is discarded and not used for training.","If the ASR  can sufficiently recognize  the utterance, statistical models\/tuning grammars (e.g., grammars corresponding with IA-provided meanings and data) are built  as described above in connection with . For some of those utterances below a certain confidence threshold determined by the ASR , an additional verification loop for an IA to verify the recognition by the ASR  of the intent or data can be utilized. If the recognition is verified, processing proceeds as described for , but if not, the recognition results are discarded.","Next, a test is made to determine  whether performance of the training ASR  is now sufficient. The performance threshold may depend on the criticality of the application. A health care application may be much less tolerant of errors, for instance, than a free tourist information service might be. The performance threshold may also depend on the rate that new words or phrases are added to the statistical model. If the performance is not sufficient, processing returns to prepare for further utterances that can be digitized  and used for additional training. If performance is sufficient, the results of training are applied to con the real time ASRs of support systems  with the models that have resulted from the training, and those real time ASRs are then validated and, if appropriate, used for production processing.","In some embodiments, training is then considered complete. The ASR is brought on-line initially in a provisional mode, i.e., as a shadow to the IAs. If the ASR meets a level of quality as determined by business criteria (e.g., by comparing results from the ASR and one or more of the IAs), it is brought into full production use so that it replaces an IA in processing . Likewise, performance of a second ASR is measured and, if it produces sufficient quality in recognition, it is brought on line to replace a second IA in processing . In other embodiments, further testing  is done, at times dependent upon the specific environment, to see whether performance of an ASR has dropped below some applicable minimum threshold. If it has, flow returns to  for additional training. If performance is acceptable, processing loops back to  to repeat testing at an appropriate time. Should performance not reach an acceptable threshold after numerous attempts, in some embodiments training is abandoned.",{"@attributes":{"id":"p-0119","num":"0118"},"figref":"FIG. 9","b":["200","902","904","904","920","922","906","912","920","918","912","908","910","914","916","922","900","906","902","910","912","914","918","200"]},"The storage device  is a computer-readable storage medium such as a hard drive, compact disc read-only memory (CD-ROM), DVD, or a solid-state memory device. The memory  holds instructions and data used by the processor . The pointing device  is a mouse, track ball, or other type of pointing device, and is used in combination with the keyboard  to input data into the computer system . The graphics adapter  displays images and other information on the display device . The network adapter  couples the computer system  to the Internet . Some embodiments of the computer  have different and\/or other components than those shown in .","The computer  is adapted to execute computer program modules for providing functionality described herein. As used herein, the term \u201cmodule\u201d refers to computer program instructions and other logic used to provide the specified functionality. Thus, a module can be implemented in hardware, firmware, and\/or software. In one embodiment, program modules formed of executable computer program instructions are stored on the storage device , loaded into the memory , and executed by the processor .","The types of computers  used by the components described herein vary depending upon the embodiment and the processing power used by the entity. For example, a customer's computer  typically has limited processing power. The iRouter , in contrast, may comprise multiple servers working together to provide the functionality described herein. In some applications a single processor (or sets of processors) may implement both real time ASRs in support systems  as well as training ASR  and other functions of training subsystem . In those applications, determining how much training to do and when allows relatively inexpensive and modestly powerful computers to be used for both training and production ASR processing.","The systems and methods described above are applicable not only to voice interactions, but in certain embodiments is usable with, e.g., video, text, email, chat, photographs, and other images. These other embodiments are usable in applications such as on-line chat, security surveillance, theme park concierge services, and device help, for example. As a specific example, a consumer device such as the iPhone or iPad devices offered by Apple, Inc. can be provided with help facilities in which open-ended questions are interpreted and processed in the manner set forth above. Likewise, recognition of video streams and images can also be facilitated using the techniques as described above.","As is evident from the discussion above, ASR subsystems are sometimes more appropriate for handling a portion of a customer interaction than are HSR subsystems. In order to provide the best possible user experience, where an application program (such as stored in workflow repository ) seeks speech recognition resources, benefits can be achieved by optimizing selection of the resources used for such recognition (i.e., ASR or HSR, as well as selection of particular ASR\/HSR resources that are best suited for the current recognition task).","Referring now to , there is shown a block diagram of operation of an ASR Proxy  to accomplish such selection of appropriate processing resources. More specifically, the functions described below are implemented in various embodiments by one or more of encapsulation in Media Resource Control Protocol (MRCP) within a Voice eXtensible Markup Language (VXML) browser, a web service, and an Application Programming Interface (API, for instance written in the Java or CSharp languages). In a specific embodiment, common ASRs from various vendors use MRCP as a standard interface to VXML platforms (browsers), and in this environment the ASR proxy  is configured to appear to a software application  running with the VXML platform as an ASR engine, but instead acts as a proxy between a VXML application and speech recognition functions by providing speech recognition resources from both ASR and HSR subsystems.","As set forth in greater detail below, ASR proxy  is configured to freely select one or more ASR subsystems  (such as described earlier in connection with discussion of support systems ) or HSR subsystems  (such as described earlier in connection with discussion of off-site agents ). Based on a database subsystem of statistics , ASR proxy  communicates with a recognition decision engine  (the operation of which is further described in connection with ) and a results decision engine  (the operation of which is further described in connection with ) to make determinations about which ASR\/HSR resources ,  to employ at any particular time. If any HSR resources are selected for use, corresponding user interface information is provided to an appropriate HSR desktop workstation  as described above in connection with off-site agents .","ASR proxy  alleviates the need for the developer of software application  to consider whether an utterance is to be recognized by an ASR or an HSR. Accordingly, such a software developer can construct (and assume availability of) more human like voice user interfaces than have traditionally been used with computers.","With more specific reference now to , in various embodiments software application  served various purposes. In one embodiment, software application  is an IVR system for toll-free caller assistance; in another it is an interactive help application on a tablet computer. Software application  directs ASR proxy  by telling it what to recognize (i.e., providing it a grammar) as well as providing it with an utterance, typically an audio file, such as a .wav file or a real-time audio stream (e.g., an MRCP Real-Time Protocol stream). ASR proxy  responds, as expected, with the \u201ctext\u201d or meaning of what it recognizes, along with a confidence score that indicates the ASR's confidence that it has correctly recognized the utterance.","Because ASR proxy  may have capabilities that differ from a conventional ASR, ASR proxy  may require additional information in for example grammar meta-tags for statistics and decisions, such as a unique way of identifying the prompt and grammar, a unique way of identifying the current session, a unique way of identifying the \u201cvoice\u201d or user (to continue to learn the acoustic model of the speaker), and thresholds to specify the behavior of the ASR proxy . In some applications, grammars are predefined or built-in; in others, grammars are not built-in such that meta-information relating to grammar, such as user interface information to frame and guide the decision of the agent, is provided to better define possible responses (e.g., for an HSR subsystem).","When software application  requests ASR proxy  to recognize an utterance, ASR proxy  passes processing over to recognition decision engine , which is responsible for deciding how to recognize the utterance. For instance, parameters and confidence thresholds provided by software application  may impact this determination. As a specific example, if an application calls for extremely high recognition quality, recognition decision engine  may direct that recognition be accomplished solely by HSR resources . On the other hand, the application may consider cost to be of paramount importance, and as a result may dictate that ASR resources  be used exclusively as a default, reserving HSR resource  usage only for instances in which ASR usage is resulting in numerous errors.","In one embodiment, ASR proxy  automatically and dynamically makes similar decisions, varying appropriate thresholds to satisfy particular requirements of the application. Thus, a high quality threshold may be used for a high asset bank customer, while a utility bill inquiry from a consumer is given a lower acceptable threshold. In this embodiment, thresholds are based on historical statistics calculated based on past recognition attempts.","It is found that beneficial results are obtained by not merely selecting between use of ASR and HSR resources, but also by allowing selection of combinations of such resources. For example, one set of parameters may best be met by submitting an utterance for recognition by multiple ASR resources; another may best be met by submitting to a single particular ASR; and still another may best be met by submitting to a mix of ASR and HSR resources. In practice, issues such as the extent to which ASRs have been trained or tuned (per the training discussion above, for instance), whether an ASR has been validated for a particular grammar, whether a cost for multiple recognition paths is acceptable, and historical results are all helpful in determining which resources to apply in any specific situation.","Similarly, a security meta-tag relating to an utterance is helpful to determine the most appropriate recognition resource; for instance, an meta-tag indicating an utterance is a Social Security Number might be sent for processing by an ASR resource to avoid the potential for a human to obtain personal information about an individual.","Another parameter considered in certain embodiments is the level of activity of various system resources. If human staff is backlogged by a high volume of requests, that backlog is usable as a parameter to favor increased use of ASR resources.","Multiple resources, whether of the same or differing types, are in some embodiments used to provide dual-checking of results.","In yet another embodiment, recognition decision engine  dynamically keeps track of the length of a current audio stream and compares it with lengths of expected utterances as defined by the corresponding grammar. For instance, if an utterance is expected to have a grammar consisting only of one of the three colors, \u201cred\u201d, \u201cgreen\u201d and \u201cblue\u201d and the actual utterance is three seconds long, a previous decision to have the utterance recognized by an ASR resource may be changed to an HSR resource in addition to or instead of the ASR, based on an anticipation that the utterance will NOT be one of the expected single syllable colors in the grammar. Such an approach is found to minimize the ultimate time to recognize \u201csurprising\u201d utterances and therefore increases the overall efficiency of the ASR proxy .","As previously mentioned, operation of ASR proxy  and the corresponding engines ,  make extensive use of statistics, thresholds, and other unique information for personalization of a system to correspond to the needs of software application . This information is stored in a statistics database , as illustrated in . For example, the results of operation of an ASR are stored as a confidence score statistic in database , and the aggregate statistics for that ASR is considered with respect to whether it is usable under the applicable business or other rules needed by software application . Furthermore, any and all statistics about the utterance, such as the speaker, the prompt, the grammar, the application, the recognition methods (e.g., ARS, HSR, single ASR, multiple ASR, multiple HSR), the confidence, no-match or no-input, and training\/tuning, are stored by ASR proxy  in database .","In a manner similar to what was described in connection with prior figures, if an ASR fails to provide usable results for an utterance, it is sent to HSR resources for recognition\/resolution of disparities. Statistics are maintained not only for ASRs, but for HSRs as well, and statistics are further maintained on the basis of an individual speaker. Thus if an ASR is found to be particularly effective at recognition for a particular speaker, the statistics are maintained and updated so as to increase the likely use of that ASR for later utterances from the same speaker. Likewise, statistics are maintained on an individual grammar basis, again to maximize the likelihood that recognition decision engine chooses an appropriate resource to use based on the expected grammar or prompt\/grammar combination. For example, a \u201cyes\/no\u201d grammar may be more effective for a simple prompt recognition by an ASR such as \u201cAre you John Smith?\u201d, but less effective for a more complex question, such as \u201cFor today, relative to the same day last week, are you feeling better?\u201d.","Generalizing from the above, statistics are generated on various bases, and are maintained so that intelligent decisions are made regarding when to use a particular ASR\/HSR resource. Based on confidence levels, grammars capable of high confidence ASR recognition can even be used more often by software application . For example, a \u201cyes\u201d or \u201cno\u201d grammar may have very high confidence with a simple ASR resource. Statistics are recorded about the prompt\/grammar combination from simple confirmation statements such as \u201cI have your phone number as (555) 123-4567. Is that correct?\u201d to more complex communications such as, \u201cIf you have been feeling OK over the past week say \u2018yes\u2019 and if you have been feeling bad at all say \u2018no\u2019.\u201d","Discussions herein regarding grammar are expandable and generalizable to combinations of grammar with prompts. One statistic, for example, relates to overall confidence for a set of utterances of the current speaker in the current session (i.e., over multiple prompts). If ASR recognition is failing for a speaker regardless of the prompt\/grammar combination, that indicates ASR proxy  would do better to resort to HSR for this speaker than even try ASR. On the other hand, if a particular speaker's utterances are routinely showing strong confidence, the ASR proxy uses ASR as the preferred recognition method. In order to generalize beyond a particular session, a unique speaker reference ID allows the system to recognize a particular speaker (e.g., based on phone number used to connect with the system) so as to choose an appropriate ASR or HSR resource.","Software application  provides thresholds as the software developer may find appropriate for a particular situation, and in some situations are generated over time based on prior recognition experiences. For example, where statistics can be generated via dual checking or confirmation via an HSR resource, those statistics are collected and stored in database . The mean, standard deviation and mode information from such statistics are applied to a range of thresholds depending on the needs determined by the software developer of software application , based on the overall goals of such application.","Furthermore, statistics are usable to determine when further reliance on an ASR resource would not be effective. For example, if a significant sample size of recognition quality for an ASR and a specific grammar shows that performance is unlikely to rise over an acceptable recognition threshold, that ASR is removed from future consideration for that particular recognition task, this recognition task could require more training (tuning), though through multiple training\/tuning attempts, which prove unsuccessful, that particular recognition attempt is permanently removed from consideration until changes occur, such as an adjustment to the prompt\/grammar or the use of a new ASR or a new version of an ASR.","Statistics are also usable for tuning ASRs. Tuning grammars are sometimes purely statistical, such as the percent of time \u201cred\u201d is used in the grammar \u201cred, green or blue\u201d, or can include synonyms such as \u201cturquoise\u201d for \u201cblue.\u201d In the latter case, tuning is facilitated through use of HSR resources for \u201cout-of-grammar\u201d recognizers (e.g., to confirm that in a particular instance \u201cturquoise\u201d should be considered synonymous with \u201cblue\u201d). Immediately after such tuning, it may be desirable in certain applications to introduce the tuned ASR in a \u201csilent\u201d limited test basis rather than a product basis to ensure performance is above acceptable thresholds. In one embodiment, HSRs are employed to verify that an ASR is capable of recognizing a grammar of interest; to calculate confidence threshold statistics during the validation phase referenced above; and to calculate confidence threshold statistics in the case of invalid recognition by the ASR. Even after validation, random dual checking with ASR or HSR resources provides ongoing checking of validity of a selected recognition method. The frequency of such checking is in one embodiment based on statistical deviations between correct and incorrect ASR recognitions. As a specific example, consider a situation in which an average confidence of a correct recognition is  and an average confidence of an incorrect recognition is . If the standard deviations are small (e.g., 8), this would suggest that there is little practical confusion between correct and incorrect recognitions, so dual checking does not need to be used very frequently. However, if the standard deviations are larger (e.g., 12), more frequent dual checking may be required to more finely tune the grammar confidence thresholds.","Over time, statistics may suggest that ASR proxy  change its initial operations. For example, statistical suggestion of very good success may suggest changing from dual checking of two ASRs to only one; or with poor success from ceasing attempts to train or tune for a particularly difficult grammar, using HSR only instead.","Both initial training, and subsequent tuning, of ASRs share common characteristics and can be implemented similarly. However, in many instances training involves more subtle issues than initial tuning, larger vocabularies and statistical language models, so techniques that work well in tuning may not be optimal for training Training may call for significantly larger sample sizes, greater use of HSRs, and reliance on out-of-grammar ASR resources.","Particularly complex grammars may call for consistent dual-checking by two ASRs with different recognition models (from different vendors), with differing outcomes being adjudicated by an HSR. Reliance on multiple HSRs (for instance, two with a third acting to resolve differences) may in some instances provide further benefits. See, e.g., U.S. Pat. No. 7,606,718, the contents of which are incorporated by reference as if fully set forth herein. ASR proxy  is configurable, via software application , to address any of these possibilities.","Turning now to , a particular recognition decision engine  operates as follows. This engine automatically decides how to process an utterance depending on historical statistics (e.g., for a speaker, session, grammar and application) and other factors, and is configurable using various settings. In the instance illustrated in , as an initial step software application  may direct that an ASR not be used until it is trained or tuned. A check  is made to determine that. If so, a check  is made to determine whether such tuning\/training has already been completed. Otherwise, a quick check  is made to determine whether the grammar is simple enough so that training is not required (for example, the grammar has only a very small number of terminals). If the grammar is not simple, processing again flows to check . If the grammar is sufficiently simple, processing flows to check . Check , referenced above, reviews stored statistics on the ASR success for the grammar, and for whether the ASR was previously tuned\/trained for the grammar (whether in the same application  or others that may have similar goals and corresponding thresholds of confidence). If those statistics indicate sufficient training\/tuning, check  passes processing on to check . Otherwise, processing is directed to HSR processing .","Check  uses confidence statistics stored in database  and a threshold that the ASR is capable of understanding a specific grammar, and a second statistic in the ongoing confidence of recognizing a speaker within a session. For those simple grammars that are not tuned or trained, on-going statistics of how well the ASR is performing the recognition task are compared with an anticipated recognition confidence threshold provided by the application or through a calculated threshold by the proxy. For instances in which a first recognition is being performed, the threshold can be set so that it is automatically considered not met, forcing recognition by an HSR to allow initial calculation of threshold by the proxy; in some embodiments the threshold is augmented by historical information regarding the current grammar. Additionally, if the ability of the ASR to recognize the speaker suggests a confidence above a threshold, ASR processing will be used and processing flows to check . Otherwise, HSR processing  is used. For example, a threshold may be set as the number of times the ASR recognition falls below the confidence (or adjusted confidence, e.g., high value speaker). In some applications, this is set as low as one ASR recognition below confidence to force subsequent recognitions to be performed by HSR.","Check  determines whether software application  or another component (e.g. requirements for training or validation) require that a dual check be used for recognition. If such is not required, processing flows to step  where a single ASR is used for recognition.","If a dual check is needed, processing flows to check  to determining whether the dual check can be done by two or more ASRs (for instance, because there are two or more trained and otherwise acceptable ASRs available). If so, processing flows to step  in which recognition is performed by such multiple ASRs. If not, for example where the ASR is not suitable for the recognition or to perform ASR validation, processing flows to steps  and  so that recognition is performed by both ASR and HSR resources.","When an ASR or HSR completes recognition, statistics regarding the recognition are stored in statistics database .","As noted above in connection with , ASR proxy  also communicates with a results decision engine . The purpose of such an engine is to evaluate results of a recognition process by ASR\/HSR resources. Referring now to , there is shown a particular results decision engine , the operation of which is described as follows. Results decision engine  reviews the results of recognition from one or more ASR\/HSR resources and determines appropriate next steps. First, a check  is made to determine whether a reported confidence levels meet a recognition threshold set by software application  or calculated by ASR proxy . If so, a validation statistic is updated  to reflect successful recognition and operation of results decision engine  is complete. Otherwise, a \u201cfiller\u201d prompt is provided  to the user, as further processing is required. For instance, the caller may be told, \u201cPlease wait, still working on it.\u201d The specific message provided to the caller may be a default message such as this, or a more specific message provided and determined by software application through some form of reference .","Processing then flows to recognition by one or more HSR resources  and then a check to determine whether the HSR recognition agrees with that of the ASR. If so, statistics are again updated, this time prorated because the recognition required HSR as well. In one embodiment, the proration is a deduction of \u2153 from the score that would have been provided had the confidence threshold been cleared.","If the results between the HSR and ASR recognition differ, a check  is made to determine if a dual HSR was used, in which case the results from the dual HSR are used  and the statistics tracking successful ASR recognition are decremented. Otherwise, an additional filler message is played  and additional HSR recognition is undertaken . If the HSR results do not agree, a third attempt to use HSR is performed in some embodiments. If there is no consensus among HSRs, a \u201cNO MATCH\u201d result is returned, indicating none of the recognizers understands the speaker (and thus no bias is indicated for the ASR(s)). Depending on current load conditions, it may not be practical to perform second or third HSRs, in which case the single HSR result is used, again without bias to the ASR(s). In such embodiments, similar processing is used in connection with operation of results decision engines discussed in connection with ,  and  as well. If the ASR is determined to match the HSR recognition, processing is complete; otherwise processing flows back to  to apply the HSR recognition and update the statistics as discussed above.","It should be noted that in one implementation, and ASR does not need to select from the grammar as a result of recognition; it can also return a \u201cNO MATCH\u201d, \u201cNO INPUT\u201d, or \u201cNOISE\u201d result, in which case further HSR processing is used as described above, again depending on criteria established by the application.","Referring now to , there is shown a particular results decision engine , the operation of which is described as follows. Results decision engine  reviews the results of recognition from two or more ASR resources and determines appropriate next steps. First, a check  is made to determine whether the results from the two ASR resources agree. If they do, a check  is made to determine whether the confidence is above an appropriate threshold. In one embodiment, each ASR has its own threshold, and confidence is considered sufficient if either ASR is above its confidence threshold. In that case, validation statistics are incremented  for the recognizer(s) that are above threshold (with neither an increment or a decrement in statistics for any agreeing but below-threshold ASR) and processing is complete.","If the results do not agree, or if the confidence level is not high enough, filler is played  for the caller and HSR resources are called in  to do the recognition. Then, a check  is made to determine whether at least one of the ASR results agrees with the HSR results. If not, a check  is made to see whether the HSR was dual check HSR. If it was not, filler is again played  and additional HSR recognition  is performed. If the HSR agrees with an ASR, or if the HSR was dual check, or if a second HSR  has been performed, processing moves to use the agreeing HSR results , which includes decrementing statistics from disagreeing ASRs and also decrements (though at a prorated amount, \u2153 in one embodiment) statistics from any agreeing but below-threshold ASRs. Next, any agreeing above-threshold ASR validation statistics are incremented , and processing is complete.",{"@attributes":{"id":"p-0158","num":"0157"},"figref":"FIG. 15","b":["1501","1502","1503","1504"]},"If the results do not agree, a check  is made to determine whether dual-check HSR was used and if not, filler is played  while a second HSR recognition  is performed. Then, the HSR results, assuming they agree, are used  and statistics for disagreeing ASRs decremented as discussed above. If HSR results do not agree, processing continues as described above in connection with . In Validation statistics are then incremented , either fully or in prorated manner as previously discussed, for any agreeing ASRs. Processing is then complete.","Referring now to , processing of a particular results decision engine  is shown in the case where solely HSR resources are used. An initial check  determines whether dual-check HSR was used (assuming it was required by the calling application). If dual check was not used, filler is played  and then a second HSR recognition  is performed to make sure the recognition is correct.","A check  is then made to determine whether the results of the HSRs are in agreement. If not, processing is complete and in one embodiment, further processing outside the scope of this process, such as a third HSR recognition (not shown), will be required to satisfy the requirements of the calling application. In such instance, if there is no convergence after the third recognition, a \u201cno match\u201d situation is declared indicating that recognition attempts have failed. Otherwise, the results of the at least two agreeing HSRs are used.","If the two HSR results in check  do agree, then processing is complete, and the recognized utterance can, for example, be added to a group for tuning\/training purposes, for instance as described above.","The present invention has been described in particular detail with respect to various possible embodiments. Those of skill in the art will appreciate that the invention may be practiced in other embodiments. First, the particular naming of the components, capitalization of terms, the attributes, data structures, or any other programming or structural aspect is not mandatory or significant, and the mechanisms that implement the invention or its features may have different names, formats, or protocols. Further, the system may be implemented via a combination of hardware and software, as described, or entirely in hardware elements. Also, the particular division of functionality between the various system components described herein is merely exemplary, and not mandatory; functions performed by a single system component may instead be performed by multiple components, and functions performed by multiple components may instead performed by a single component.","Some portions of above description present the features of the present invention, process steps, and instructions in terms of algorithms and symbolic representations of operations on information. These algorithmic descriptions and representations are the means used by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art. These operations, while described functionally or logically, could be embodied in software, firmware or hardware, and when embodied in software, could be downloaded to reside on and be operated from different platforms used by real time network operating systems.","Furthermore, it has also proven convenient at times, to refer to these arrangements of operations as modules or by functional names, without loss of generality.","Unless specifically stated otherwise or as apparent from the above discussion, it is appreciated that throughout the description, discussions utilizing terms such as \u201cdetermining\u201d or the like, refer to the action and processes of a computer system, or similar electronic computing device, that manipulates and transforms data represented as physical (electronic) quantities within the computer system memories or registers or other such information storage, transmission or display devices.","The present invention also relates to an apparatus for performing the operations herein. This apparatus may be specially constructed for the required purposes, or it may comprise a general-purpose computer selectively activated or reconfigured by a computer program stored on a computer readable medium that can be accessed by the computer and run by a computer processor. Such a computer program may be stored in a non-transitory computer readable storage medium, such as, but is not limited to, any type of disk including floppy disks, optical disks, CD-ROMs, magnetic-optical disks, read-only memories (ROMs), random access memories (RAMs), EPROMs, EEPROMs, magnetic or optical cards, application specific integrated circuits (ASICs), or any type of media suitable for storing electronic instructions, and each coupled to a computer system bus. Furthermore, the computers referred to in the specification may include a single processor or may be architectures employing multiple processor designs for increased computing capability.","In addition, the present invention is not described with reference to any particular programming language. It is appreciated that a variety of programming languages may be used to implement the teachings of the present invention as described herein, and any references to specific languages are provided for enablement and best mode of the present invention.","The present invention is well suited to a wide variety of computer network systems over numerous topologies. Within this field, the configuration and management of large networks comprise storage devices and computers that are communicatively coupled to dissimilar computers and storage devices over a network, such as the Internet.","Finally, it should be noted that the language used in the specification has been principally selected for readability and instructional purposes, and may not have been selected to delineate or circumscribe the inventive subject matter. Accordingly, the disclosure of the present invention is intended to be illustrative, but not limiting, of the scope of the invention."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["Further features of the invention, its nature and various advantages will be more apparent from the following detailed description of the preferred embodiment, taken in conjunction with the accompanying drawings, in which like reference characters refer to like parts throughout, and in which:",{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":["FIG. 3A","FIG. 2"]},{"@attributes":{"id":"p-0027","num":"0026"},"figref":["FIG. 3B","FIG. 2"]},{"@attributes":{"id":"p-0028","num":"0027"},"figref":["FIG. 4A","FIG. 2"]},{"@attributes":{"id":"p-0029","num":"0028"},"figref":["FIG. 4B","FIG. 2"]},{"@attributes":{"id":"p-0030","num":"0029"},"figref":["FIG. 5A","FIG. 2"]},{"@attributes":{"id":"p-0031","num":"0030"},"figref":["FIG. 5B","FIG. 2"]},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 8","b":"800"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 9","b":"200"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0041","num":"0040"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0042","num":"0041"},"figref":"FIG. 16"}]},"DETDESC":[{},{}]}
