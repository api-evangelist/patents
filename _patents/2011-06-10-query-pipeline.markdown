---
title: Query pipeline
abstract: A query pipeline is created () from a query request. The query pipeline includes multiple query operations including multiple query operators. A first query operator and a second query operator perform first and second query operations on a database () and on data outside the database (). A result from the first query operation in the query pipeline is fed to the second query operation in the query pipeline.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09009139&OS=09009139&RS=09009139
owner: Hewlett-Packard Development Company, L.P.
number: 09009139
owner_city: Houston
owner_country: US
publication_date: 20110610
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["PRIORITY","BACKGROUND","DETAILED DESCRIPTION OF EMBODIMENTS","EXAMPLE (1)"],"p":["The present application claims priority to U.S. Provisional Patent Application Ser. No. 61\/353,559, filed Jun. 10, 2010, which is incorporated by reference in its entirety.","Network security management is generally concerned with collecting data from network devices that reflects network activity and operation of the devices, and analyzing the data to enhance security. For example, the data can be analyzed to identify an attack on the network or a network device, and determine which user or machine is responsible. If the attack is ongoing, a countermeasure can be performed to thwart the attack or mitigate the damage caused by the attack. The data that is collected usually originates in a message (such as an event, alert, or alarm) or an entry in a log file, which is generated by a network device. Examples of network devices include firewalls, intrusion detection systems, servers, switches, routers, etc.","Each message or log file entry is stored for future use. The stored messages or log file entries may be queried to identify information relevant to the attack or analysis. To facilitate the searching of the messages or log file entries, the data may be stored in a relational database. Upon receiving the queries, the relational database may identify and return relevant messages or log files. The returned messages or log files may be processed (e.g., filtered or sorted) and then presented to a user.","The relational database is capable of handling a large amount of data. However, the relational database can perform certain types of queries but not others. Hence, a user must carefully craft query operators and conditions to be compatible with the relational database. In some occasions, the user may have to perform subsequent processing on a query result obtained from the relational database to obtain a desired result. Moreover, the relational database often requires access to a slower secondary storage device (e.g., hard disk) in order to perform a query.","The figures depict an embodiment for purposes of illustration only. One skilled in the art will readily recognize from the following description that alternative embodiments of the structures and methods illustrated herein may be employed without departing from the principles described herein.","According to an embodiment, a computer system is for collecting data from disparate devices across a computer network, normalizing the data to a common schema, and consolidating the normalized data. The data (\u201cevents\u201d) can then be monitored, analyzed, and used for investigation and remediation in a centralized view. Events can be cross-correlated with rules to create meta-events. Correlation includes, for example, discovering the relationships between events, inferring the significance of those relationships (e.g., by generating metaevents), prioritizing the events and meta-events, and providing a framework for taking action. The system (one embodiment of which is manifest as machine readable instructions executed by computer hardware such as a processor) enables aggregation, correlation, detection, and investigative tracking of suspicious network activities. The system also supports response management, ad-hoc query resolution, reporting and replay for forensic analysis, and graphical visualization of network threats and activity.","Embodiments also include a logging system that generates a sequence of query operations which involves at least one query operation on a relational database and at least one query operation based on stream processing. The logging system receives a query request and identifies query operations to be performed on the relational database and query operations to be performed by stream processing. The query operations for the relational database are converted into SQL (structured query language) commands and are provided to the relational database. The logging system receives query results from the relational database and performs subsequent query operations based on the relational database or stream processing. The logging system leverages the capability of the relational database to handle a large amount of data and the flexibility of the stream processing to perform various types of query commands.","Stream processing described herein refers to performing multiple operations on a stream of data in a sequential manner within primary data storage. In stream processing, processed data from an earlier operation are fed to a subsequent operation for processing sequentially.","The system is described with respect to examples, which should not be read to limit the broader spirit and scope of the embodiments. For example, the examples presented herein describe distributed agents, managers and consoles, which are but one embodiment. The general concepts and reach are much broader and may extend to any computer-based or network-based security system. Also, examples of the messages that may be passed to and from the components of the system and the data schemas that may be used by components of the system are given, but are not meant to be all-inclusive examples and should not be regarded as such.",{"@attributes":{"id":"p-0017","num":"0016"},"figref":["FIG. 1","FIG. 1"],"b":["100","110","110","110"]},"Types of data sources  include security detection and proxy systems, access and policy controls, core service logs and log consolidators, network hardware, encryption devices, and physical security. Examples of security detection and proxy systems include IDSs, IPSs, multipurpose security appliances, vulnerability assessment and management, anti-virus, honeypots, threat response technology, and network monitoring. Examples of access and policy control systems include access and identity management, virtual private networks (VPNs), caching engines, firewalls, and security policy management. Examples of core service logs and log consolidators include operating system logs, database audit logs, application logs, log consolidators, web server logs, and management consoles. Examples of network hardware includes routers and switches. Examples of encryption devices include data security and integrity. Examples of physical security systems include card-key readers, biometrics, burglar alarms, and fire alarms.","The SIEM system  includes one or more agents , one or more managers , one or more databases , one or more online archives , one or more user interfaces , and one or more logging systems . In some embodiments, these modules are combined in a single platform or distributed in two, three, or more platforms (such as in ). The use of this multi-tier architecture supports scalability as a computer network or system grows.","An agent  provides an interface to a data source . For example, the agent  collects data (\u201craw events\u201d) from a data source , processes the data, and sends the processed data (\u201cevents\u201d) to a manager . The agent  can operate anywhere, such as at a separate device communicating via a protocol such as simple network management protocol (SNMP) traps, at a consolidation point within the network, or at the data source . For example, if the data source  is a software application, the agent  can be co-hosted on the device that hosts the data source.","Processing can include normalization, aggregation, and filtering. For example, individual raw events are parsed and normalized for use by the manager . Normalization can involve normalizing values (such as severity, priority, and time zone) into a common format and\/or normalizing a data structure into a common schema. Events can be categorized using a common, human-readable format. This format makes it easier for users to understand the events and makes it easier to analyze the events using filters, rules, reports, and data monitors. In one embodiment, the common format is the Common Event Format (CEF) log management standard from ArcSight, Inc.","Aggregation and filtering reduce the volume of events sent to the manager , which saves network bandwidth and storage space, increases the manager's efficiency and accuracy, and reduces event processing time. The agent  sends events to the manager  in batches based on the expiration of a time period or based on a threshold number of events being reached.","The agent  can also send commands to the data source  and\/or execute commands on the local host, such as instructing a scanner to run a scan. These actions can be executed manually or through automated actions from rules and data monitors. The agent  can also add information to the data that it has collected, such as by looking up an Internet Protocol (IP) address and\/or hostname in order to resolve IP\/hostname lookup at the manager .","The agent  is configured via an associated configuration file (not shown). The agent  can include one or more software modules including a normalizing component, a time correction component, an aggregation component, a batching component, a resolver component, a transport component, and\/or additional components. These components can be activated and\/or deactivated through appropriate commands in the configuration file. During configuration, the agent  is registered to the manager  and configured with characteristics based on its data source  and desired behavior. The agent  is further configurable through both manual and automated processes. For example, the manager  can send to the agent  a command or configuration update.","The manager  provides analysis capabilities, case management workflow capabilities, and services capabilities. Communications between the manager  and the agent  can be bi-directional (e.g., to enable the manager  to transmit a command to the platform hosting the agent ) and encrypted. In some installations, the manager  can act as a concentrator for multiple agents  and can forward information to other managers  (e.g., managers deployed at a corporate headquarters). To perform its tasks, the manager  uses a variety of filters, rules, reports, data monitors, dashboards, and network models. In one embodiment, the manager  is a Java-based server.","Analysis can include detection, correlation, and escalation. For example, the manager  cross-correlates the events received from the agents  using a rules engine (not shown), which evaluates each event with network model and vulnerability information to develop realtime threat summaries. Regarding case management, the manager  can maintain reports regarding the status of security incidents and their resolution. Services can include administration, notification, and reporting. The manager  can also provide access to a knowledge base.","As events are received by the manager , they are stored in a database . Storing the events enables them to be used later for analysis and reference. In one embodiment, the database  is a relational database management system.","In one embodiment, the database  stores data in partitions, which are chronological slices of the database. For example, one new partition is created each day to store that day's events. A partition can be compressed and stored in an online archive  for later retrieval. In one embodiment, partition management is provided by the SmartStorage archiving and retrieval component of the Security Lifecycle Information Management (SLIM) product from ArcSight, Inc.","A user interacts with the manager  via a user interface . The user interface  enables the user to navigate the features and functions of the manager . A single manager  can support multiple user interface instances. The features and functions that are available to the user can depend on the user's role and permissions and\/or the manager's configuration. In one embodiment, access control lists enable multiple security professionals to use the same manager  and database  but each professional has his own views, correlation rules, alerts, reports, and knowledge bases appropriate to his responsibilities. Communication between the manager  and the user interface  is bi-directional and can be encrypted.","In one embodiment, there are two types of user interfaces : a workstation-based interface and a web browser-based interface. The workstation interface is a standalone software application that is intended for use by full-time security staff in a Security Operations Center (SOC) or similar security monitoring environment. The workstation interface includes an authoring tool for creating and modifying filters, rules, reports, pattern discovery, dashboards, and data monitors. The workstation interface also enables a user to administer users, database partitions, and workflow (e.g., incident investigation and reporting). For example, the workstation interface enables a user to perform routine monitoring, build complex correlation and long sequence rules, and perform routine administrative functions. In one embodiment, the workstation interface is the ESM Console product from ArcSight, Inc.","The web interface is an independent and remotely installable web server that provides a secure interface with the manager  for web browser clients. The web interface is intended for use as a streamlined interface for customers of Managed Service Security Providers (MSSPs), SOC operators, and users who need to access the manager  from outside the protected network. Because the web server can be installed at a location remote from the manager , the web server can operate outside the firewall that protects the manager . The web interface provides event monitoring and drill-down capabilities. In one embodiment, as a security feature, the web interface does not enable authoring or administrative functions. In one embodiment, the web interface is the ArcSight Web product from ArcSight, Inc.","In one embodiment, a logging system  is an event data storage appliance that is optimized for extremely high event throughput. The logging system  stores security events (sometimes referred to as \u201clog data\u201d). In one embodiment, the security events are stored in compressed form. However, the logging system  can retrieve these events on demand and restore them to their original, unmodified form, which may be used for subsequent forensics analysis. Multiple logging systems  can work together to scale up to support high sustained input rates when storing events. Event queries can be distributed across a peer network of logging systems . A user can configure the logging system  via a user interface (not shown).","The logging system  can receive both processed events (e.g., events adhering to the Common Event Format) and raw events. In one embodiment, raw events are received directly from data sources  (such as syslog messages and log files), and processed events are received from agents  or managers . The logging system  can also send both raw events and processed events. In one embodiment, raw events are sent as syslog messages (to any device; not shown), and processed events are sent to the manager . The logging system  will be further described below.","Through the above-described architecture, the SIEM system  can support a centralized or decentralized environment. This is useful because an organization may want to implement a single instance of the SIEM system  and use an access control list to partition users. Alternatively, the organization may choose to deploy separate SIEM systems  for each of a number of groups and consolidate the results at a \u201cmaster\u201d level. Such a deployment can also achieve a \u201cfollow-the-sun\u201d arrangement where geographically dispersed peer groups collaborate with each other by passing primary oversight responsibility to the group currently working standard business hours. SIEM systems  can also be deployed in a corporate hierarchy where business divisions work separately and support a rollup to a centralized management function.","Described herein are systems and methods for storing log data efficiently while supporting querying. \u201cLog data,\u201d as used herein, can be generated by various data sources, including both devices and applications. These sources include, for example, the data sources  described above as well as network systems, computers, operating systems, anti-virus systems, databases, physical infrastructure, identity management systems, directory services, system health information systems, web traffic, legacy systems, proprietary systems, mainframes, mainframe applications, security systems, physical devices, and SIEM sources (such as agents  and managers ).","A system can obtain log data in many ways. For example, log data can be received (e.g., according to the syslog protocol). Alternatively, log data can be accessed (e.g., by reading a file that is stored locally or remotely). Other methods include, for example, Open Database Connectivity (ODBC), Simple Network Management Protocol (SNMP) traps, NetFlow, and proprietary Application Programming Interfaces (APIs). Log data can also be input by a user (e.g., using a command line interface (CLI)).","Log data can be in any format. One such format is CEF. Other formats are, for example, specific to the data sources  that generated the log data. Log data is comprised of one or more data instances called \u201cevents.\u201d An event can be, for example, an entry in a log file, an entry in a syslog server, an alert, an alarm, a network packet, an email, or a notification page. In general, an event is generated once and does not change afterwards.","In one embodiment, an event includes implicit meta-data and a message. Implicit meta-data can include information about, for example, the device or application that generated the event (\u201cevent source\u201d) and when the event was received from the event source (\u201creceipt time\u201d). In one embodiment, the receipt time is a date\/time stamp, and the event source is a network endpoint identifier (e.g., an IP address or Media Access Control (MAC) address) and\/or a description of the source, possibly including information about the product's vendor and version.","The message represents what was received from the event source and can be in any form (binary data, alphanumeric data, etc.). In one embodiment, the message is free-form text that describes a noteworthy scenario or change. In another embodiment, the message also includes explicit meta-data. Explicit meta-data is obtained, for example, by parsing the message. When an event source generates an event, the event usually includes information that indicates when the event occurred (\u201cevent occurrence time\u201d). The event occurrence time, which is usually a date\/time stamp, is an example of explicit meta-data and is frequently used for analysis. Different event sources often produce non-uniform explicit meta-data (e.g., priority or criticality of event, devices\/applications\/users affected by event, and which user triggered event).","In one embodiment, if an event does not include an occurrence time, an implicit timestamp generated by an event receiver when it received the event is treated as the original occurrence timestamp. As an event is processed and potentially forwarded through various systems, each system usually has an implicit notation of event receipt time.","In one embodiment, an event represents a data structure that includes one or more fields, where each field can contain a value (sometimes referred to as an \u201cattribute\u201d). The size of this data structure usually falls within the range of 100 bytes to 10 kilobytes.",{"@attributes":{"id":"p-0042","num":"0041"},"figref":"FIG. 2","b":["200","170","100","200","202","204","204","206","208","210","212","214","216","204","218","212"]},"The secondary storage  is any device capable of holding data, like a hard drive, compact disk read-only memory (CD-ROM), DVD, or a solid-state memory device. The primary storage  holds instructions and data used by the processor . The primary storage  is often referred to simply as \u201cmemory\u201d and may be embodied, for example, as a random-access memory (RAM). The primary storage  has a faster access speed but limited storage capacity compared to the secondary storage . Hence, operations performed with no access or limited access to the secondary storage  would be faster and more efficient than operations that need frequent access to the secondary storage .","The pointing device  may be a mouse, track ball, or other type of pointing device, and is used in combination with the keyboard  to input data into the computer . The graphics adapter  displays images and other information on the display . The network adapter  couples the computer  to a local or wide area network.","As is known in the art, the computer  can have different and\/or other components than those shown in . In addition, the computer  can lack certain illustrated components. For example, the computer  acting as a logging system  can lack a keyboard , pointing device , graphics adapter , and\/or display . Moreover, the storage device  can be local and\/or remote from the computer  (such as embodied within a storage area network (SAN)).","The computer system  may execute, by the processor  or other hardware processing circuit, the methods, functions and other processes described herein. These methods, functions and other processes may be embodied as machine readable instructions stored on computer readable medium, which may be non-transitory, such as hardware storage devices.",{"@attributes":{"id":"p-0047","num":"0046"},"figref":["FIG. 3A","FIG. 3B"],"b":["170","100","206","170","310","320","314","310","170","310","310","208","350","350","385","208","208","324"]},"The event receiver  receives log data , processes the log data , and outputs a data \u201cchunk\u201d . The event receiver  includes a control system , a set of one or more buffers , and a metadata structure . The control system  is communicatively coupled to the set of one or more buffers  and the metadata structure . The control system  controls the operation of the event receiver  and is further described below in detail with reference to .","Each buffer  stores information regarding one or more events. In one embodiment, a buffer's size is fixed but the size itself is configurable. If different events include the same types of fields, then the events can be organized in a table. Each row of the table would represent a different event, and each column of the table would represent a different field. In one embodiment, each buffer  is associated with a particular field and includes values from that field (\u201cattributes\u201d) from one or more events. In another embodiment, each buffer  also includes an identifier (\u201cIndexID\u201d) that indicates which field is associated with the buffer.","The metadata structure  stores metadata about the contents of the set of buffers . In one embodiment, this metadata includes the unique identifier associated with the event receiver  that received the events, the number of events in the set of buffers, and, for each of one or more \u201cfields of interest,\u201d a minimum value and a maximum value that reflect the range of values of that field over all of the events in the set of buffers. The metadata structure  acts as a search index when querying event data.","In one embodiment, a field of interest is not an event field per se. Instead, it is a \u201cderived\u201d value that is determined based on the values stored in one or more fields of an event.","The storage manager  receives data chunks  and stores them in the secondary storage  so that they can be queried. In one embodiment, the storage manager  generates or extracts metadata of the chunk , and stores the metadata in chunks table  in the relational database . The chunks table  stores information about the metadata stored in the chunk (described below) and the location of the chunk (e.g., the unique identifier associated with the datafile that stores the chunk and the location within the datafile where the chunk is stored (e.g., as an offset)).","The storage manager  also generates datafiles table  for storing information about the one or more datafiles . The datafiles table  is also stored in the relational database . In one embodiment, each entry in the datafiles table  represents one datafile  for which space has been allocated, and the entry includes a unique identifier associated with the datafile and the location of the datafile (e.g., a file system, a path therein, and a file name). A datafile  listed in the datafiles table  may or may not contain data (e.g., chunks ). In one embodiment, datafiles  are allocated before they are needed. In this embodiment, a list of these pre-allocated datafiles  (called a \u201cfree list\u201d) is maintained.","A datafile  stores multiple chunks . In one embodiment, all datafiles are the same size (e.g., 1 gigabyte) and are organized in time order.","The relational database  includes a plugin  that enables the relational database  to work in conjunction with executable components in the primary storage . In one embodiment, the relational database  is a commercially available database or open source database. The plugin  receives datafiles table  and the chunks table  and stores them so that these tables can be accessed during a query.",{"@attributes":{"id":"p-0056","num":"0055"},"figref":["FIG. 4","FIG. 4"],"b":["400","310","355","320"]},"In one embodiment, before the method  begins, the set of buffers  and the metadata structure  are initialized. For example, the control system  stores, in each buffer, the appropriate IndexID. The control system  also stores, in the metadata structure , the unique identifier associated with the event receiver .","The method  begins when the event receiver  receives  log data . In one embodiment, the log data  is received in the form of a stream.","The control system  separates  the log data into one or more events and determines  when each event was received by the event receiver .","The control system  parses  the events into their field values and stores the field values and receipt times in the appropriate buffers. The control system  also updates  the metadata structure . For example, the number of events in the buffer will have increased. The minimum and maximum values for the field(s) of interest may also need to be updated. In one embodiment, data write operations and metadata write operations are synchronized in order to avoid possible inconsistency if a system crash occurs. For example, a transactional database system is used so that if field values are stored in the buffer , the metadata structure  is guaranteed to be updated accordingly, even if the underlying system crashes in between the two steps.","At some point in time (see below), the control system  generates  data chunks  based on the metadata structure  and the contents of the buffers . For example, one chunk is generated for each buffer. Different chunks can have different sizes. Chunk sizes can differ due to, for example, the type of field values stored in a chunk (and the compression algorithm applied to them) and the type of trigger that caused the chunk to be generated. In one embodiment, a maximum chunk size can be specified.","In one embodiment, each chunk includes the metadata structure , the contents of the associated buffer, a chunk identifier (ChunkID), a stripe identifier (StripeID), and a set of index location identifiers (IndexLocationIDs). The field of interest, which concerns the meta-data portion of a chunk, and the field associated with the buffer, which concerns the \u201cpayload\u201d portion of a chunk, need not be the same field. The ChunkID uniquely identifies the chunk with respect to other chunks. The StripeID, which is shared among the set of chunks, is used to associate the chunks with each other (since all of the chunks concern the same set of events). The next time the control system  generates  data chunks , the chunks will concern a different set of events, so a different StripeID will be used. The set of IndexLocationIDs includes one IndexLocationID for each field value in the buffer\/chunk. The IndexLocationID is used to access a field value in a different chunk that corresponds to the same event. In one embodiment, the IndexLocationID includes the StripeID and an offset identifier (OffsetID). The OffsetID indicates which field value (within a buffer\/chunk) corresponds to the desired event.","In one embodiment, the contents of the associated buffer  are compressed before they are stored in the chunk . Compressing the buffer contents makes this approach a costeffective choice for long-term storage of data. The compressed version of the contents can be generated using any data compression algorithm.","In one embodiment, the chunk  also includes a \u201cmagic number\u201d and a version identifier. The magic number, sometimes called a file signature, is a short sequence of bytes that identifies the data type of the chunk. For example, the magic number is reasonably unique (i.e., unique with a high probability) across other data and file formats, including other chunks. Thus, when a chunk is read, it is easy to determine whether the chunk is in the expected format. If the chunk's actual magic number differs from the expected magic number, then the chunk is \u201cwrong\u201d (e.g., corrupted). The magic number thereby helps detect data corruption and resynchronize data chunk boundaries in order to recover corrupt data. (If the actual magic number matches the expected magic number, then data that occurs later in the chunk might still be wrong. However, the matching magic number excludes this possibility for the majority of common situations.) The version identifier enables the accommodation of data and file formats that have changed. For example, when a chunk is read, the version identifier can be used in conjunction with the magic number to indicate additional information about the data or file format.","In another embodiment (also not shown), the control system  also generates a message digest of the contents of a buffer . For example, the control system  applies a cryptographic hash function to the bytes stored in the buffer . Any cryptographic hash function can be used, such as Message-Digest algorithm 5 (MD5) or an algorithm in the Secure Hash Algorithm family (e.g., SHA-256). In one embodiment, the digest value is stored in the chunk . This value can later be used to determine whether the buffer data that is stored in the chunk (in compressed form) has been changed or tampered with. This helps guarantee the integrity of stored events by making it noticeable when events have been changed. Also, when the chunk  arrives at the storage manager , the digest value can be stored in the chunks table  along with the chunk's metadata. That way, if the chunk is later tampered with (or corrupted) while it is stored in a datafile , the message digest of the tampered chunk will not match the message digest that was previously stored in the chunks table .","The set of buffers  and the metadata structure  are then re-initialized , thereby flushing the buffers . In one embodiment, the set of buffers  includes additional buffers that can be used to store incoming events while other buffers are full or are being flushed.","In one embodiment, step  is performed (\u201ctriggered\u201d) when any one of the buffers  is full. In another embodiment, step  is performed (triggered) when a particular period of time (a \u201ctimeout window\u201d) has elapsed, during which no events were received by the event receiver .","The control system  sends  the data chunks  to the storage manager . The storage manager  receives  the chunks  and stores  the chunks in one or more datafiles  (see below). In one embodiment, a chunk is encrypted before it is stored for security purposes. The storage manager  also updates  the chunks table . For example, the storage manager  adds to the table information regarding the chunks  that it just stored in the datafile(s) .","After a chunk  has been stored in a datafile , the field values within the chunk can be queried. A query can be executed by itself or as part of handling an interactive search or generating a report. A query is represented as an expression that can be evaluated against an event. The expression includes one or more search operators. A search operator concerns the contents of an event, such as, a particular field and the value of that field.",{"@attributes":{"id":"p-0070","num":"0069"},"figref":["FIG. 3B","FIGS. 3A and 4"],"b":["170","170","314","324","208","314","206","324","208","208","385","324","206","208","204"]},"The querying module  includes, among other components, a parser , an optimizer , an execution engine  and a result renderer . The parser  receives a query request and parses the request to identify query operators. In one embodiment, a query request includes multiple query operators and a separator for distinguishing the query operators. The separator may be the ASCII vertical bar character \u201c|\u201d. An example query request is as follows:","failed login time=\u201cthe last two hours\u201d|rex \u201cextract srcIP\u201d|top srcIP|head ","The above query request includes four separate query operators. The first operator \u201cfailed login time=\u2018the last two hours\u2019\u201d indicates search for \u2018failed login\u201d attempts for last two hours at a specified event source. The vertical bar character \u201c|\u201d following the first operator indicates that the first operator is finished and a string of characters related to a second operator will follow. The second operator (rex \u201cextract srcIP\u201d) is a regular expression for extracting a first value (here, source IP addresses) from events obtained by the first operator (here, failed login attempts that occurred during the last two hours). The second operator is followed by another vertical bar character \u201c|\u201d to separate the second operator from the third operator. The third operator (\u201ctop srcIP\u201d) operator sorts matching entries by a field in the event (in this case, source IP address) by matching counts. The result of the \u201ctop\u201d operator is a table with two columns: the first column including source IP addresses, and the second column indicating event count (representing a number of failed login attempt events from that source IP address). The third operator is followed by another vertical bar character \u201c|\u201d to separate the third operator from the fourth operator. Finally, the fourth operator (\u201chead \u201d) determines top 5 results from the matching events.","The parser  parses the query request and converts the query request into a query pipeline of multiple query operators. A query pipeline herein refers to a set of multiple query operations for processing in a sequential order. The result of a query operation in the query pipeline is provided to a subsequent query operation in the query pipeline. After undergoing all operations in sequence, the final query result is obtained from the last query operation.","In one embodiment, the parser  identifies the user name of the query request and modifies the query pipeline to limit the results based on access granted to the user. For example, if the user \u201cjoe\u201d is allowed to access only a certain type of events based on regular expression, a new operator \u201cregex \u2018joe access regular expression\u2019\u201d is added between the first operator and the second operator in the above example (1) to limit the expression that the user \u2018joe\u2019 is allowed to access.","In one embodiment, the query pipeline generated by the parser  is provided to the optimizer  to rearrange, eliminate, or rewrite the operators in the query pipeline to render the query pipeline more efficient or amendable to query operations. In one example, the optimizer  moves the positions of the operators within the query pipeline in order to achieve better performance. For example, a query \u201cfailed login|cef srcIP|regex \u2018magic regex expression\u2019|cef dst|top dst\u201d may be rearranged to \u201cfailed login|cef srcIP dst|regex \u2018magic regex expression\u2019|top dst\u201d. CEF is an open log management standard that improves the interoperability of security-related information from different security and network devices and applications. \u201ccef\u201d operator extracts a field value from a CEF compliant event. \u201ccef\u201d operator may use extensive resources since the entire event needs to be parsed. The modified query pipeline performs parsing only once instead of twice. In one embodiment, optimization may be performed by relocating \u201ccef\u201d operator inside first query operator. The modified query would read as \u201cfailed login (cef srcIP dst)|regex \u2018magic regex expression\u2019|top dst.\u201d In this case, the expensive cef parsing is completely eliminated.","The optimizer  may also eliminate operators that are not required. For example, in the query request \u201cfailed login|cef srcIP|cef dst|top dst,\u201d the results list events sharing the same values in the \u201cdst\u201d fields. The \u201ccef srcIP\u201d operator is not relevant in this query pipeline because the results depend on values in \u201cdst\u201d field and not on values in \u201csrcIP\u201d field. Hence, the optimizer  removes the \u201ccef srcIP\u201d operator and updates the query pipeline to \u201cfailed login|cef dst|top dst\u201d. The updated query pipeline does not include the \u201ccef srcIP\u201d operator.","The optimizer  may also rewrite one or more pipeline operators into a more efficient form. For example, in the query request \u201cfailed login|cef srcIP|chart_count by srcIP|sort-srcIP,\u201d chart_count and sort operators may be combined into a single \u201ctop\u201d operator. The chart_count operator generates a chart indicating the number of occurrences of certain entry value representing a criterion (e.g., the source IP address). The sort operator sorts the entries in the chart based on a certain criterion (e.g., the source IP address). The \u201ctop\u201d operator is a single operator that performs the operation of the chart_count operator and the sort operator. Hence, the optimizer  rewrites the query request to \u201cfailed login|cef srcIP|top srcIP.\u201d","The optimized query pipeline is then fed to the execution engine . The execution engine  includes, among others, a database interface  and a stream processor . A first set of operators is predefined for searching using the relational database , and a second set of operators is predefined for processing by the stream processor  or the database interface . The database interface  converts any of the first operators (that uses the relational database ) into a database query command (e.g., SQL command). The database query command is then forwarded to the relational database  to perform a querying operation. In contrast, the second set of operators is forwarded to the stream processor  for stream processing. In example (1), the query operators \u201cfailed login\u201d and \u201ctop\u201d are better suited for processing by the relational database , while other query operators are all processed by the stream processor . Any operators that are likely to consume a large amount of memory and operators configured for efficient performance on a relational database are performed using the relational database, whereas other operations are performed using stream processing.","Example operators and details of the operators are \u201cchart,\u201d \u201cwhere,\u201d \u201crare\u201d and \u201ctop\u201d operators which may be used for operations on the relational database . Conversely \u201ccef,\u201d \u201ceval,\u201d \u201cfields,\u201d \u201chead,\u201d \u201cregex,\u201d \u201crex,\u201d \u201csort,\u201d \u201ctail,\u201d \u201crename,\u201d \u201creplace,\u201d \u201cextract,\u201d and \u201ckeys\u201d operators are processed at the stream processor .","The execution engine  executes a series of query operations according to the query pipeline as received from the optimizer . For example, the execution engine  performs a series of query operations corresponding to the query operators in a sequence as defined in the query pipeline.","The first operator in the query pipeline is called a query source. A query source generates multiple matching data entries and uses accesses to the relational database . In the above example (1), the query source is \u201cfailed login.\u201d Following the query source are supporting operators. The supporting operators include at least two types of query operators. One is a single event operator and another is an aggregated event operator. When executed, a single event operator takes one event, processes the event, and outputs a processed event or nothing. In contrast, an aggregated event operator takes multiple events and outputs the summary of the multiple events. In the above example (1), \u201crex\u201d operator is a single event operator, whereas \u201ctop\u201d operator and \u201chead\u201d operator are aggregated event operators.","A query pipeline may include operators accessing the relational database  as support operators. Some operators are more efficient or reliable when performed using the relational database  even though it is possible to perform these operations by stream processing. If an operator results in a large number of result sets, such an operator may take up excessive memory space; and hence, may be inappropriate for stream processing. Further, relational databases are very efficient at performing operations such as \u201ctop,\u201d \u201cjoin,\u201d \u201csubquery,\u201d \u201cset\u201d operations and various indexing operations. Hence, such operations are performed using the relational database  instead of stream processing. By using the relational database to perform such query operations, the querying module  can provide higher performance and scalability.","To perform a search using the relational database , the database interface  sends a database query command  to the relational database via the plugin . The relational database  may need to issue a RPC (remote procedure call)  to the storage manager  (or other components) in the logging system  via the plugin  to identify the datafile table  or the chunks table  relevant to a query command . If so, the relational database  receives a response (not shown) to the RPC  from the storage manager  or other components that enables the relational database  to retrieve relevant chunks ,  directly or via other components.","In one embodiment, the storage manager  manages metadata associated with the chunks stored in the datafiles . After receiving the query command  from the database interface , the relational database  issues a RPC  to the storage manager . The storage manager  decides if all the data chunks relevant to the query command  are indexed in chunks table  by making a query to the relational database  to search the chunks table . If all relevant data chunks are indexed, the storage manager  identifies relevant metadata entries by performing search on the chunks table , and sends the identified metadata entries to the relational database .","In one embodiment, the querying operation on the relational database  is performed in multiple phases. If at least some relevant data chunks are not indexed (determined by determining presence of corresponding entries in the chunks tables ), the storage manager  accesses chunks in the datafiles  to identify relevant events using the datafiles table . The storage manager  retrieves compressed chunk  including relevant events from the datafiles , decompresses the chunk , and sends the decompressed chunk  or selected events to the relational database . The relational database  performs further processes based on the query command, and sends the database query result  to the database interface . The database query result  then becomes the basis of further downstream query operations at the execution engine .","In one embodiment, the relational database  performs querying operations without issuing a RPC  to the storage manager . In this embodiment, the relational database  accesses the datafiles tables  and\/or the chunks table  directly, reads compressed chunks  directly from the datafiles  and uncompresses the chunk before processing, as needed.","For each operation in the query pipeline for stream processing, the stream processor  creates a stream process instance and executes corresponding query operations. The stream process instance does not make any queries to the relational database  and is performed using memory space of the primary storage . The processed result from a stream process instance corresponding to a query operator is fed to a subsequent stream process instance corresponding to a subsequent query operator in the query pipeline.","Some stream process instances can perform operations based on partial results. For such stream process instances, the stream processing can be performed in parallel by executing multiple stream process instances simultaneously. A first stream process instance earlier in the pipeline receives input, processes the input and sends the processed result to a second stream process instance following the first stream process instance. If the second stream process instance is a single event operator, the second stream process instance executes and outputs data as each processed event is received from the first stream process instance. If the second stream process is an aggregated event operator, the second stream process executes and outputs a result after receiving multiple processed events from the first stream process instance.","The result renderer  receives the query result from the execution engine . The result renderer  then processes the query result for transmission or display and sends to a user device.",{"@attributes":{"id":"p-0091","num":"0090"},"figref":"FIG. 5","b":["316","510","316","514","316","514","318","518"]},"The execution engine  then selects  the first operator in the query pipeline. Then, the execution engine  determines  whether the first operator is to be used for a query on the relational database . If the first operator is for querying the relational database , then the database interface  of the execution engine  converts  the query operator into a database query command  and sends  the database query command  to the relational database . The relational database  performs operations according to the database query command and returns the DB query result  to the database interface .","The first query operator performs query on the relational database  using the database interface . It is then determined  if the first operator is the last query operator in the query pipeline. By definition, there is at least one other query operator in the query pipeline; and hence, the method proceeds  to the next operator in the query pipeline. After proceeding to the next query operator, the process of determining whether the query operator needs a query on the relational database  and subsequent steps are repeated.","If it is determined that the  that a query may be performed by stream processing, the stream processing is performed  at the stream processor .","For stream processing, multiple stream processing instances may be created for simultaneous operations. A pipeline of stream processing instances may be created so that the result from a stream processing instance is fed to a subsequent stream processing instance. In this way, the stream processing can be performed more efficiently and promptly. Stream processing may include executing operations on the primary memory device outside of the secondary storage device.","If it is determined  that the next query operator needs a query on the relational database , the database interface  converts  the query operator into a database query command . The database query command  is then sent  to the relational database . The query is executed on the relational database , and the querying module  receives  the database query result  of the query from the relational database . Then the process proceeds to determine  if the previous operator was the last operator in the query pipeline, and reiterates the subsequent processes.","If it is determined  that the previous query operator was the last query operator in the query pipeline, then the execution engine  sends the query result to the result render . Then, the result renderer  processes  the query result for transmission or display to the user.","In one embodiment, the execution engine provides partial query results to the result renderer . This enables some results to be displayed to the user before the entire query operations have been performed.","The sequence and steps described above with reference to  are merely illustrative. For example, in cases where the query operators are already in the format of a database query command, step  may be omitted. Moreover, step  of optimizing the query pipeline may also be omitted."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF DRAWINGS","p":[{"@attributes":{"id":"p-0006","num":"0005"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0007","num":"0006"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 3A"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 3B"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 5"}]},"DETDESC":[{},{}]}
