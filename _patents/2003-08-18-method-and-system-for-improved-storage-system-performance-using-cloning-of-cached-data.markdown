---
title: Method and system for improved storage system performance using cloning of cached data
abstract: A method for improving storage system performance is disclosed. The method includes cloning information stored in a first unit of storage in a second unit of storage. The first unit of storage is stored in a first cache maintained by an upper-level system, while the second unit of storage is stored in a second cache.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07426611&OS=07426611&RS=07426611
owner: Symantec Operating Corporation
number: 07426611
owner_city: Cupertino
owner_country: US
publication_date: 20030818
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"p":["Portions of this patent application contain materials that are subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document, or the patent disclosure, as it appears in the Patent and Trademark Office file or records, but otherwise reserves all copyright rights whatsoever.","1. Field of the Invention","This invention relates to the field of data storage devices, and more particularly relates to a method and system for improving the performance of storage systems via the cloning of units of storage.","2. Description of the Related Art","Information drives business. A disaster affecting a data center can cause days or even weeks of unplanned downtime and data loss that could threaten an organization's productivity. For businesses that increasingly depend on data and information for their day-to-day operations, this unplanned downtime can also hurt their reputations and bottom lines. Businesses are becoming increasingly aware of these costs and are taking measures to plan for and recover from disasters. This is particularly true of the data storage systems that maintain such businesses information.","With this growing focus on data storage, it has become desirable to implement commercial-grade storage performance and reliability akin to those of mainframe disk subsystems in a more cost-effective manner. In response to this need, techniques have been developed that abstract multiple disks into single storage objects, using commodity disks (such as SCSI and IDE drives) and system buses (such as ISA, EISA, PCI, and SBus). A data storage system employing this abstraction of multiple disks into a single storage object is referred to generically as a Redundant Arrays of Independent Disks (RAID) or RAID array.","A number of RAID types (referred to as RAID levels) have been defined, each offering a unique set of performance and data-protection characteristics. Originally, several RAID configurations (often called RAID levels) were proposed. (RAID levels are often abbreviated as RAID-x; for example, RAID level 5 may be abbreviated as RAID-5.) In addition to RAID levels 2 through 6, which use parity calculations to provide redundancy, two other disk configurations were retroactively labeled as RAID: striping, or interleaving data across disks with no added redundancy, was identified as RAID level 0, and mirroring, maintaining full redundant data copies, was identified as RAID level 1.","The important characteristics of each major RAID level are now presented. While RAID-0 offers no increased reliability, it can supply performance acceleration at no increased storage cost by sharing I\/O accesses among multiple disks. By contrast, RAID-1 provides the highest performance for redundant storage, because read-modify-write cycles are not required when updating data (as required in RAID-5 storage systems). Moreover, multiple copies of data may be used in order to accelerate read-intensive applications. However, RAID-1 requires at least double the disk capacity (and therefore, at least double the disk expenditures) of a non-RAID-1 solution. RAID-1 is most advantageous in high-performance and write-intensive applications. Also, since more than two copies may be used, RAID-1 arrays can be constructed to withstand loss of multiple disks without suffering an interruption in service.","Use of mirroring (or RAID level 1) increases data availability and read I\/O performance, at the cost of sufficient storage capacity for fully redundant copies. RAID levels 2 through 5 address data redundancy by storing a calculated value (commonly called parity, or parity information), which can be used to reconstruct data after a drive failure or system failure, and to continue servicing input\/output (I\/O) requests for the failed drive.","In order to increase reliability while preserving the performance benefits of striping, it is possible to configure objects which are both striped and mirrored. While not explicitly numbered as standard RAID configurations, such a combination is sometimes called RAID-1+0, RAID-0+1, or RAID-10. This configuration is achieved by striping several disks together, then mirroring the striped sets to each other, producing mirrored stripes. When striped objects are mirrored together, each striped object is viewed as if it were a single disk. If a disk becomes unavailable due to error, that entire striped object is disabled. A subsequent failure on the surviving copy would make all data unavailable. It is, however, extremely rare that this would occur before the disk could be serviced. In addition, use of hot spares makes this even less likely.","Among the parity RAID configurations, RAID-2 uses a complex Hamming code calculation for parity, and is not typically found in commercial implementations. RAID levels 3, 4 and 5 are, by contrast, often implemented. Each uses an exclusive-or (XOR) calculation to check and correct missing data. RAID-3 distributes bytes across multiple disks. RAID-4 and RAID-5 arrays compute parity on an application-specific block size, called an interleave or stripe unit, which is a fixed-size data region that is accessed contiguously. All stripe units at the same depth on each drive (called the altitude) are used to compute parity. This allows applications to be optimized to overlap read access by reading data off a single drive while other users access a different drive in the RAID. These types of parity striping require write operations to be combined with read and write operations for disks other than the ones actually being written, in order to update parity correctly. RAID-4 stores parity on a single disk in the array, while RAID-5 removes a possible bottleneck on the parity drive by rotating parity across all drives in the set.","RAID 5 protects the data for n disks with a single disk that is the same size as the smallest disk in the array. RAID 5 usable capacity equals s*[n\u22121], where s is the capacity of the smallest disk in the array and n is the total number of disks in the array. Not only does a RAID 5 array offer a very efficient way to protect data, such an array also has read performance similar to a RAID 0 array, although write performance suffers in comparison to a single disk (due to the read\/modify\/write cycle for writes, discussed subsequently). Because of its combination of data protection and performance, RAID 5 is popular for general-purpose servers such as file and Web servers.","The parity information generated is simply the result of an XOR operation on all the data elements in the stripe. Because XOR is an associative and commutative operation, to find the XOR result of multiple operands, one starts by simply performing the XOR operation of any two operands. Subsequently, one performs an XOR operation on the result with the next operand, and so on with all of the operands, until the final result is reached. Additionally, parity rotation is implemented to improve performance, as discussed subsequently.","A RAID 5 volume can thus tolerate the loss of any one disk without data loss. The missing data for any stripe is easily determined by performing an XOR operation on all of the remaining data elements for that stripe. If the host requests a RAID controller to retrieve data from a disk array that is in a degraded state, the RAID controller first reads all of the other data elements on the stripe, including the parity data element. It then performs all of the XOR calculations before returning the data that would have resided on the failed disk. All of this happens without the host being aware of the failed disk, and array access continues.","The RAID 5 write operation is responsible for generating the requisite parity data, an operation which is typically referred to as a read\/modify\/write operation (alternatively, a read\/modify\/log\/write operation, if logging is implemented). This process, as will be appreciated, is time-consuming, in comparison to simply writing data to disk. This is substantial overhead, even in the case where the entire stripe is being written (a full stripe write).",{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 1A","b":["100","110","120"]},"The overhead associated with this kind of operation is even more onerous in the case of a partial stripe write, in terms of overhead per unit of data written. Consider a stripe composed of a number of strips (i.e., stripe units) of data and one strip of parity information, as is the normal case. Suppose the host wants to change just a small amount of data that takes up the space on only one strip within the stripe. The RAID controller cannot simply write that small portion of data and consider the request complete. It must also update the parity data. One must remember that the parity data is calculated by performing XOR operations on every strip within the stripe. So when one or more strips change, parity needs to be recalculated using all strips, regardless of the amount of data actually changed. This mandates reading the information from the other (unchanging) strips, even though the data read, once used, will simply be discarded.",{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 1B","b":["130","140","150","160"]},"In greater detail, the read\/modify\/write operation can be broken down into the following actions:\n\n","It will be noted that the parity disk is involved in every write operation (steps 3 and 7). This is why parity is rotated to a different disk with each stripe. If the parity were all stored on the same disk all of the time, that disk could become a performance bottleneck.","Because of its combination of data protection and performance, RAID-5 is a popular alternative for a variety of commercial applications. However, reads are actually an integral part of a RAID-5 write (as well as reads, naturally), making read performance an important criteria in implementing a RAID-5 array. This issue of read performance also impacts the performance of storage systems implementing copy-on-write snapshots, as will be appreciated from the following discussion in connection with .",{"@attributes":{"id":"p-0023","num":"0030"},"figref":"FIG. 1C","b":["170","180","190"]},"As will be appreciated, therefore, it is desirable to improve the performance of storage systems, such as those described above, for a variety of reasons. What is therefore needed is a technique that addresses the delays inherent in such storage systems, and, in particular, delays related to the read operations that must be performed in supporting techniques such as those discussed above. Moreover, such a solution should preferably do so without affecting the basic storage paradigm employed.","The present invention provides a method and system that addresses the foregoing limitations by avoiding the need to perform a read operation in certain circumstances. The present invention achieves this by cloning a unit of storage (e.g., a page or region), caching the existing data within that unit of storage in another unit of storage (e.g., in a \u201ccloned page cache\u201d or \u201cold data cache\u201d (i.e., a cache for old data)) prior to the unit of storage being written to. For example, in a storage system of the present invention, an existing page's old data is copied to a free page in an old data cache (resulting in a cloned page), prior to the existing page being modified.","This is particularly effective when the data is already cached (e.g., in a filesystem cache or volume manager cache), as the data is then immediately available for cloning. For example, if an upper-level system (e.g., a filesystem, a database application, a firmware module or the like) caches pages, those pages can be cloned into pages kept in a separate cache referred to herein as an old data cache. This old data cache (e.g., a cloned page cache) can be maintained by the upper-level system, a lower-level storage module (e.g., a volume manager), or by some other stand-alone module. Once cached in an old data cache, the information in the cloned page can be used in situations in which that information would typically be read from a storage system. Thus, in the examples discussed previously, if the requisite page has been cloned (and is thus available from the old data cache), the cloned page can be used rather than reading that page from the storage system, thereby avoiding that initial read operation by the lower-level storage module.","In one embodiment, a method for improving storage system performance is disclosed. The method includes cloning information stored in a first unit of storage in a second unit of storage. The first unit of storage is stored in a first cache maintained by an upper-level system, while the second unit of storage is stored in a second cache. This second cache is referred to herein as an old data cache.","The foregoing is a summary and thus contains, by necessity, simplifications, generalizations and omissions of detail; consequently, those skilled in the art will appreciate that the summary is illustrative only and is not intended to be in any way limiting. Other aspects, inventive features, and advantages of the present invention, as defined solely by the claims, will become apparent in the non-limiting detailed description set forth below.","The use of the same reference symbols in different drawings indicates similar or identical items.","For a thorough understanding of the subject invention, refer to the following detailed description, including the appended claims, in connection with the above-described drawings. Although the present invention is described in connection with several embodiments, the invention is not intended to be limited to the specific forms set forth herein. On the contrary, it is intended to cover such alternatives, modifications, and equivalents as can be reasonably included within the scope of the invention as defined by the appended claims.","In the following description, for purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of the invention. It will be apparent to one skilled in the art, however, that the present invention can be employed without these specific details.","References in the specification to \u201cone embodiment\u201d or \u201can embodiment\u201d means that a particular feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment of the invention. The appearances of the phrase \u201cin one embodiment\u201d in various places in the specification are not necessarily all referring to the same embodiment, nor are separate or alternative embodiments mutually exclusive of other embodiments. Moreover, various features are described which may be exhibited by some embodiments and not by others. Similarly, various requirements are described which may be requirements for some embodiments but not other embodiments.","An Example Storage Architecture",{"@attributes":{"id":"p-0054","num":"0061"},"figref":"FIG. 2","b":["200","200","210","220","220","230","210","230","230"]},"It also will be appreciated that, although referred to in terms of disks, disk storage  can be implemented using any storage technology, including hard disks, RAM disks, optical disks, tape or other media. Also included in storage architecture  is an old data cache  (also referred to herein as a cloned page cache). Old data cache  can be accessed by both upper-level system  and lower-level storage module . In various embodiments, upper-level system  can be a filesystem, a database application, a firmware module, an application-specific integrated circuit (ASIC) or some other hardware or software module that employs lower-level storage module  in accessing disk storage . Lower-level storage module  can be, for example, a volume manager or some other hardware or software module capable of managing disk storage .","In various embodiments of the present invention, old data cache  can be primarily associated with upper-level system  or lower-level storage module . Alternatively, old data cache  can be implemented as a separate caching module, and configured to service requests for the creation of cache pages, writes to\/reads from cache pages, the destruction of cache pages, and the like. Thus, in certain embodiments, upper-level system  includes old data cache , and provides access to old data cache  to lower-level storage module . In other embodiments, old data cache  is primarily under the control of lower-level storage module , which provides upper-level system  with access thereto. In still other embodiments, old data cache  is an independent caching module, and so is not under the primary control of upper-level system  or lower-level storage module .","Also depicted in  is a parity cache . Parity cache  is used by lower-level storage module  in applications in which disk storage  is implemented as a RAID array. In such implementation, parity cache  is used to maintain parity information regarding information stored in disk storage , prior to the writing of that parity information to disk storage .",{"@attributes":{"id":"p-0058","num":"0065"},"figref":"FIG. 2A","b":["260","270","260","240"]},"General Examples of Partial Stripe Writes and Copy-On-Write Snapshot Processes",{"@attributes":{"id":"p-0059","num":"0066"},"figref":"FIG. 3A","b":["230","240","300","310","230","320"]},{"@attributes":{"id":"p-0060","num":"0067"},"figref":["FIG. 3B","FIG. 3A"],"b":["240","350","360"]},"It will be noted that various processes according to embodiments of the present invention are discussed herein (e.g., with reference to ). It is appreciated that operations discussed herein may consist of directly entered commands by a computer system user or by steps executed by application specific hardware modules, but the preferred embodiment includes steps executed by software modules. The functionality of steps referred to herein may correspond to the functionality of modules or portions of modules.","These operations may be modules or portions of modules (e.g., software, firmware or hardware modules). For example, although the described embodiment includes software modules and\/or includes manually entered user commands, the various example modules may be application specific hardware modules. The software modules discussed herein may include script, batch or other executable files, or combinations and\/or portions of such files. The software modules may include a computer program or subroutines thereof encoded on computer-readable media.","Additionally, those skilled in the art will recognize that the boundaries between modules are merely illustrative and alternative embodiments may merge modules or impose an alternative decomposition of functionality of modules. For example, the modules discussed herein may be decomposed into submodules to be executed as multiple computer processes, and, optionally, on multiple computers. Moreover, alternative embodiments may combine multiple instances of a particular module or submodule. Furthermore, those skilled in the art will recognize that the operations described in example embodiment are for illustration only. Operations may be combined or the functionality of the operations may be distributed in additional operations in accordance with the invention.","The software modules described herein may be received by a computer system, for example, from computer readable media. The computer readable media may be permanently, removably or remotely coupled to the computer system. Such computer readable media can include, for example: magnetic storage media including disk and tape storage media; optical storage media such as compact disk media (e.g., CD-ROM, CD-R, etc.) and digital video disk storage media; nonvolatile memory storage memory including semiconductor-based memory units such as FLASH memory, EEPROM, EPROM, ROM or application specific integrated circuits; volatile storage media including registers, buffers or caches, main memory, RAM, and the like; and data transmission media including computer network, point-to-point telecommunication, and carrier wave transmission media. In a UNIX-based embodiment, the software modules may be embodied in a file which may be a device, a terminal, a local or remote file, a socket, a network connection, a signal, or other expedient of communication or state change. Other new and various types of computer-readable media can be used to store and\/or transmit the software modules discussed herein.","Alternatively, such actions may be embodied in the structure of circuitry that implements such functionality, such as the micro-code of a complex instruction set computer (CISC), firmware programmed into programmable or erasable\/programmable devices, the configuration of a field-programmable gate array (FPGA), the design of a gate array or full-custom application-specific integrated circuit (ASIC), or the like. Each of the processes described herein can be executed by a module (e.g., a software module) or a portion of a module or a computer system user using, for example, a computer system such as that discussed subsequently in connection with .","Example Processes for Partial Stripe Writes Using a Volume Manager Cache",{"@attributes":{"id":"p-0066","num":"0073"},"figref":"FIG. 4","b":["400","410","230","420"]},"However, if the contents of the dirtied page are valid, a signal is sent to the volume manager by the filesystem, indicating that the volume manager should read and cache the parity blocks corresponding to the old data (step ). It will be noted that parity information is only needed in the case where the underlying storage strategy requires such information. An example would be a RAID array (e.g., RAID-3, RAID-4, RAID-5 or RAID-6). Thus, parity information will be needed in the case of the management of a RAID-5 volume. However, only the old data will be necessary if no parity information is required (e.g., in the case of a copy-on-write snapshot methodology). The reading and caching of parity information is discussed in further detail in connection with . The page that will be dirtied is also written to the old data cache (e.g., old data cache ) (step ). The writing of the page to an old data cache is discussed in further detail with regard to . As will be appreciated, the volume manager will then complete the write, carrying out the modify\/log\/write phases of the write operation. The performance of these phases is discussed in further detail in connection with .","Using a technique according to the present invention, whenever a filesystem dirties a page having valid contents (i.e., contents that are synchronized with the data stored in the storage system), the filesystem can clone the page, copying the existing contents into the new page before modifying the old page. The filesystem also signals the volume manager, indicating that the volume manager should read and cache the corresponding parity information (parity block(s)). The cloned page is then used by the volume manager in completing the other phases of the write operation being performed. Subsequent modifications to the paged data go to the original page. When this dirty (original) page is flushed by the filesystem, resulting in a volume write, the volume manager can completely eliminate the read phase of the write operation, since the volume manager now has at its disposal the old data (in the cloned page) and old parity (in the cached parity blocks).","A technique according to the present invention also avoids unnecessary data duplication, since the cached contents (i.e. the cloned pages) are the ones that are going to be useful when the dirty page flushes (and hence the RAID-5 volume writes) eventually take place. This ability to achieve what is basically a 100% cache hit rate is a significant advantage of techniques according to the present invention. Moreover, as noted elsewhere, a technique according to the present invention can easily be extended to any application running on top of a lower-level storage management application (e.g., a volume manager) that performs its own cache management, such as database applications, by providing an application programming interface for the management of cloned pages and the upper-level system's interactions with the lower-level storage management application.","As will be appreciated, the cloned page can be freed after the modified data has been successfully flushed to the disk, depending on the given implementation (primarily, whether the data in the cloned page is expected to be needed subsequently). A set of interfaces is therefore needed by the volume manager to use the cloned pages. The management of the cloned pages, in one embodiment, is handled by the filesystem. The filesystem exports a set of interfaces to the volume manager, which the volume manager can invoke in order to use the cloned pages. The management of such interfaces is controlled by the filesystem in this scenario. Alternatively, these cloned pages can also be managed by the volume manager (with the set of interfaces exported to the filesystem), or even by a caching module that is independent of the filesystem and volume manager (with the set of interfaces exported to both the volume manager and the filesystem).","In the case where volume manager maintains a pool of pages that the filesystem can use to store old or existing data (an old data cache such as a cloned page cache, as described above), the filesystem can employ, for example, the following interfaces to perform the requisite functions:\n\n","Additionally, the volume manager needs to maintain all standard cache management primitives required for maintaining the cache (old data cache, or simply cache, depending if taking the perspective of the functionality associated with the present invention, or that conventionally available in the volume manager).","Typically, in the case of memory mapped writes, the filesystem is not notified of the occurrence of such writes, and so is unaware that any action is required. The filesystem therefore cannot be tasked with the cloning of the requisite information in such a case. Thus, in certain operating systems, such situations may require interfacing with the virtual memory system of the operating system in order to ensure that the modified page is cloned prior to its being modified. In this case, the cloning is performed by the virtual memory system (e.g., rather than the filesystem). It will also be noted that, in the case where the volume manager maintains the cache, the cloned pages are keyed on device identifiers and offsets within them, and not file identifiers and offsets therein.",{"@attributes":{"id":"p-0074","num":"0087"},"figref":"FIG. 5","b":["500","510","250","520"]},{"@attributes":{"id":"p-0075","num":"0088"},"figref":"FIG. 6","b":["240","600","610","620"]},{"@attributes":{"id":"p-0076","num":"0089"},"figref":"FIG. 7","b":"240"},"The process of modifying\/logging\/writing performed by the volume manager begins with the computation of parity information, using the new data, the cloned page and cached parity information, assuming that such caching is feasible (step ). It will be noted that, in fact, given the possibility of read operations being required notwithstanding the techniques of the present invention, these parity computations may need to be performed on the pages and parity information thus read. As before, once the parity calculations have been performed, data is logged to a data log (step ). Also as before, data and parity information are then written to disk storage (step ).",{"@attributes":{"id":"p-0078","num":"0091"},"figref":"FIG. 8","b":["220","240"]},"A memory mapped access process according to the present invention begins with a determination as to whether a write page fault has occurred (step ). Once a write page fault occurs, a determination is made as to whether the contents of the faulted page are valid (step ). If the contents of the faulted page are not valid, a signal is sent to the volume manager, indicating that the volume manager should read the old data from disk storage, as would normally be the case (step ). If needed (e.g., in the case of a write to a RAID-5 volume (as opposed to a volume having copy-on-write snapshots)), the volume manager also reads parity information at this point.","However, if the page's contents are valid, a determination is made as to whether the write-faulted page is already in the volume manager-managed old data cache (step ). If the faulted page is already in the volume manager-managed old data cache, the volume manager already has access to the faulted page's data, and so is at liberty to complete the remaining phases of the write, once the write to the volume manager actually occurs. That being the case, the process of page cloning is complete.","If the faulted page is not in the volume manager's old data cache (step ), the faulted page needs to be cloned. To accomplish this, the filesystem first acquires a free page from the old data cache managed by the volume manager (old data cache ) (step ). Next, the filesystem copies the faulted page's existing information into the free page (step ). It is at this point that the free page becomes a cloned page, having the existing data written thereto. The filesystem then identifies the cloned page by the device's vnode (in contrast to the file vnode) and the offset to which the device vnode corresponds (step ). Finally, the filesystem releases the cloned page back into the old data cache (step ). In doing so, the filesystem makes the cloned page available to the volume manager in a manner that is transparent to the volume manager (given that the volume manager is tasked with managing the old data cache (and so, by definition, has access to the old data cache)).",{"@attributes":{"id":"p-0082","num":"0095"},"figref":"FIG. 9","b":["900","910","920"]},{"@attributes":{"id":"p-0083","num":"0096"},"figref":"FIG. 10","b":["1000","1010","1020"]},"If the write is not to a hole, a determination is made as to whether the kernel has completed the mapping of the relevant sections of the file being written to into memory before those sections are dirtied and written to disk (step ). Once the kernel has completed the mapping of the relevant sections of the file being written to into memory, a determination is made as to whether the data to be modified is cached in the old data cache (step ). If the data in question is already cached in the old data cache, no further action need be taken in this regard, and the kernel can proceed with writing the relevant file sections into memory and flushing that information to disk.","If the requisite data has not yet been cached in the old data cache, however, the filesystem acquires the necessary free page(s) from the old data cache (step ). The filesystem then copies the existing information from the file sections, into the free page(s) (step ). Having written the existing information into the free page(s) (thus making the page(s) into cloned page(s)), the filesystem releases the cloned page(s) back into the old data cache (step ).",{"@attributes":{"id":"p-0086","num":"0099"},"figref":"FIG. 11","b":["1100","1110"]},"Example Processes for Partial Stripe Writes Using Page Cloning",{"@attributes":{"id":"p-0087","num":"0100"},"figref":"FIG. 12","b":["1200","1210","1220","1230"]},"Thus, if the faulted page is not in the old data cache, the filesystem proceeds with cloning the page. The filesystem accomplishes this goal by first acquiring a free page from the old data cache (step ). The filesystem then copies the existing information from the faulted page into the free page (thus making the free page a cloned page) (step ). The filesystem then identifies the cloned page by a file vnode and an offset corresponding to the file vnode (step ). Finally, the filesystem puts the cloned page back into the old data cache (step ). More generally, as will be appreciated, the scenarios discussed herein with regard to the use of a volume manager-managed old data cache (e.g., a memory mapped situation) can be addressed with equal success using an old data cache maintained by the filesystem, such as that just discussed in connection with .","Example Processes for Storage Systems Employing Copy-on-Write Snapshots","Writes to Volumes Subject to Copy-on-Write Snapshots",{"@attributes":{"id":"p-0089","num":"0102"},"figref":"FIG. 13A"},"The process of page cloning in a copy-on-write snapshot scenario begins with a determination as to whether a write has occurred (step ). Once a write has occurred, a determination is made as to whether the faulted page is valid (step ). If the page is not valid, the filesystem sends a signal to the volume manager indicating that the volume manager should read the old data from disk storage, as it normally would, and cache the old data in the old data cache (step ). If the page is valid, a determination is made as to whether the requisite page already exists in the old data cache (step ). If the requisite page is already in the old data cache, no further actions need be taken with regard to caching that information. This allows the copying of the page's existing data, and the writing of the new application data to be performed by volume manager using the previously-cached page.","If the requisite data is not in the old data cache, the filesystem performs the operations necessary to clone the page, and so cache the old contents. First, the filesystem acquires a free page from the old data cache (step ). The filesystem then copies the existing information from the page to be written to, to the free page (step ). The filesystem then puts the cloned page back into the old data cache (step ).",{"@attributes":{"id":"p-0092","num":"0105"},"figref":"FIG. 13B","b":["1370","1380"]},"Reads to Copy-on-Write Snapshot Volumes","The process of an application (e.g., a filesystem) reading from a snapshot volume in a copy-on-write snapshot scenario can also benefit from techniques according to the present invention. In a volume having copy-on-write snapshots, the volume containing the original data is referred to as a source volume (as it is the source of the original data). In the conventional case, a read from a snapshot volume involves a determination as to whether the (latest) data for those blocks reside on the snapshot volume, or on the source volume. The latter would be the case, for example, where there had been no writes to those blocks on the source volume. As will be appreciated, if the desired information is on the source volume, the source volume must be read to retrieve (the most recent version of) the desired information. However, as in the other scenarios discussed herein, if the desired information has already been cloned (e.g., by a filesystem mounted on the source volume), that information is readily available, thus avoiding the need to perform a read operation on the disk storage. Thus, in the case where a read is performed on the snapshot volume and the most recent version of the desired data resides on the source volume, the source volume need not be read if the desired information has already been cloned (and is thus available from the old data cache).","Partial Region Writes to Copy-on-Write Snapshot Volumes","The process of a partial region write to a snapshot volume in a copy-on-write snapshot scenario can also benefit from techniques according to the present invention. Typically, data in storage systems is dealt with on the basis of regions (which, in turn, contain some standard number of blocks), or some other unit of storage of the storage unit. For example, a region might contain 64 KB of data. Also typically, some sort of metadata is kept, indicating whether the most current version of a region's data is currently kept in the source volume or the snapshot volume. The present invention can be used to improve the performance of partial writes (e.g., partial region writes) to the snapshot volume of such systems (e.g., in the situation where a decision support system writes a partial region to the snapshot volume).","It will be appreciated that situations will exist in which something less than a full region (e.g., even a single block) may need to be written to a snapshot volume (e.g., an application may write data amounting to something less than a full region). As noted, such a scenario is referred to as a partial region write (or, more generally, a partial write). However, writes to a snapshot volume are conventionally limited to a minimum unit of storage (e.g., a region), due to the burdensome nature of maintaining metadata using smaller units of storage. In writing to a snapshot volume, then, the copying of data from the source volume to the snapshot volume is normally performed on a region-by-region basis (as a result of the limitations mandated the copy-on-write snapshot paradigm).","Conventionally, in the case where the most recent version of data in a given region is currently kept on the source volume (as indicated by the log) and new data is to be written to a partial region on the snapshot volume (corresponding to the aforementioned region on the source volume), the region on the source volume (containing the most recent version of the data) is read. The given region of the snapshot volume is then written with either the new data (on the appropriate portion of the region) or the most recent version of the data from the source volume (on the remaining portion of the region). Thus, if the most current data resides on a region of the source volume and the corresponding region of the snapshot volume is to be written, that region of the source volume must be read in order to properly complete the write to the snapshot volume.","By contrast, in a storage architecture according to the present invention, the reading of the most recent version of the data from the source volume need not be performed if that information has already been cloned in the old data cache. In that case, the volume manager can write the new data to the appropriate portion of the region of the snapshot volume (as it normally would), and the old data from the old data cache to the remaining portion of the region of the snapshot volume, thus completing the writing of that region of the snapshot volume without the need for reading from the source volume.","An Example Computing and Network Environment",{"@attributes":{"id":"p-0098","num":"0111"},"figref":"FIG. 14","b":["1410","1410","1412","1410","1416","1417","1418","1420","1422","1424","1426","1428","1430","1432","1433","1434","1437","1438","1435","1490","1435","1439","1440","1442","1446","1412","1428","1447","1412","1430","1448","1412"]},"Bus  allows data communication between central processor  and system memory , which may include read-only memory (ROM) or flash memory (neither shown), and random access memory (RAM) (not shown), as previously noted. The RAM is generally the main memory into which the operating system and application programs are loaded and typically affords at least 154 megabytes of memory space. The ROM or flash memory may contain, among other code, the Basic Input-Output system (BIOS) which controls basic hardware operation such as the interaction with peripheral components. Applications resident with computer system  are generally stored on and accessed via a computer readable medium, such as a hard disk drive (e.g., a fixed disk ), an optical drive (e.g., optical disk drive ), floppy disk unit  or other storage medium. Additionally, applications may be in the form of electronic signals modulated in accordance with the application and data communication technology when accessed via network modem  or network interface .","Storage interface , as with the other storage interfaces of computer system , may connect to a standard computer readable medium for storage and\/or retrieval of information, such as fixed disk drive . Fixed disk drive  may be a part of computer system  or may be separate and accessed through other interface systems. It will be apparent that a virtual loader of the present invention can be implemented, for example, using a hard disk drive such as fixed disk . Modem  may provide a direct connection to a remote server via a telephone link or to the Internet via an internet service provider (ISP). Network interface  may provide a direct connection to a remote server via a direct network link to the Internet via a POP (point of presence). Network interface  may provide such connection using wireless techniques, including digital cellular telephone connection, Cellular Digital Packet Data (CDPD) connection, digital satellite data connection or the like.","Many other devices or subsystems (not shown) may be connected in a similar manner (e.g., bar code readers, document scanners, digital cameras and so on). Conversely, it is not necessary for all of the devices shown in  to be present to practice the present invention. The devices and subsystems may be interconnected in different ways from that shown in . The operation of a computer system such as that shown in  is readily known in the art and is not discussed in detail in this application. Code to implement the present invention may be stored in computer-readable storage media such as one or more of system memory , fixed disk , optical disk , or floppy disk . Additionally, computer system  may be any kind of computing device, and so includes personal data assistants (PDAs), network appliance, X-window terminal or other such computing devices. The operating system provided on computer system  may be MS-DOS\u00ae, MS-WINDOWS\u00ae, OS\/2\u00ae, UNIX\u00ae, Linux\u00ae, or another known operating system. Computer system  also supports a number of Internet access tools, including, for example, an HTTP-compliant web browser having a JavaScript interpreter, such as Netscape Navigator\u00ae, Microsoft Explorer\u00ae, and the like.","Moreover, regarding the signals described herein, those skilled in the art will recognize that a signal may be directly transmitted from a first block to a second block, or a signal may be modified (e.g., amplified, attenuated, delayed, latched, buffered, inverted, filtered, or otherwise modified) between the blocks. Although the signals of the above described embodiment are characterized as transmitted from one block to the next, other embodiments of the present invention may include modified signals in place of such directly transmitted signals as long as the informational and\/or functional aspect of the signal is transmitted between blocks. To some extent, a signal input at a second block may be conceptualized as a second signal derived from a first signal output from a first block due to physical limitations of the circuitry involved (e.g., there will inevitably be some attenuation and delay). Therefore, as used herein, a second signal derived from a first signal includes the first signal or any modifications to the first signal, whether due to circuit limitations or due to passage through other circuit elements which do not change the informational and\/or final functional aspect of the first signal.","The foregoing described embodiment wherein the different components are contained within different other components (e.g., the various elements shown as components of computer system ). It is to be understood that such depicted architectures are merely examples, and that, in fact, many other architectures can be implemented which achieve the same functionality. In an abstract, but still definite sense, any arrangement of components to achieve the same functionality is effectively \u201cassociated\u201d such that the desired functionality is achieved. Hence, any two components herein combined to achieve a particular functionality can be seen as \u201cassociated with\u201d each other such that the desired functionality is achieved, irrespective of architectures or intermediate components. Likewise, any two components so associated can also be viewed as being \u201coperably connected,\u201d or \u201coperably coupled,\u201d to each other to achieve the desired functionality.",{"@attributes":{"id":"p-0104","num":"0117"},"figref":"FIG. 15","b":["1500","1510","1520","1530","1540","1540","1410","1550","1540","1560","1","1540","1560","1","1540","1540","1570","1570","1580","1","1540","1540","1510","1520","1530","1550","1590","1570","1560","1","1560","1","1580","1","1570"]},"It will be noted that the variable identifier \u201cN\u201d is used in several instances in  to more simply designate the final element of a series of related or similar elements. The repeated use of such variable identifiers is not meant to imply a correlation between the sizes of such series of elements, although such correlation may exist. The use of such variable identifiers does not require that each series of elements has the same number of elements as another series delimited by the same variable identifier. Rather, in each instance of use, the variable identified by \u201cN\u201d may hold the same or a different value than other instances of the same variable identifier.","With reference to computer system , modem , network interface  or some other method can be used to provide connectivity from each of client computer systems ,  and  to network . Client systems ,  and  are able to access information on storage server A or B using, for example, a web browser or other client software (not shown). Such a client allows client systems ,  and  to access data hosted by storage server A or B or one of storage devices A()-(N), B()-(N), ()-(N) or intelligent storage array .  depicts the use of a network such as the Internet for exchanging data, but the present invention is not limited to the Internet or any particular network-based environment.","While particular embodiments of the present invention have been shown and described, it will be apparent to those skilled in the art that, based upon the teachings herein, changes and modifications may be made without departing from this invention and its broader aspects and, therefore, the appended claims are to encompass within their scope all such changes and modifications as are within the true scope of this invention. Moreover, while the invention has been particularly shown and described with reference to these specific embodiments, it will be understood by those skilled in the art that the foregoing and other changes in the form and details may be made therein without departing from the scope of the invention."],"heading":["BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION"],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The present invention may be better understood, and its numerous objects, features, and advantages made apparent to those skilled in the art by referencing the accompanying drawings.",{"@attributes":{"id":"p-0030","num":"0037"},"figref":"FIG. 1A"},{"@attributes":{"id":"p-0031","num":"0038"},"figref":"FIG. 1B"},{"@attributes":{"id":"p-0032","num":"0039"},"figref":"FIG. 1C"},{"@attributes":{"id":"p-0033","num":"0040"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0034","num":"0041"},"figref":"FIG. 2A"},{"@attributes":{"id":"p-0035","num":"0042"},"figref":"FIG. 3A"},{"@attributes":{"id":"p-0036","num":"0043"},"figref":"FIG. 3B"},{"@attributes":{"id":"p-0037","num":"0044"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0038","num":"0045"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0039","num":"0046"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0040","num":"0047"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0041","num":"0048"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0042","num":"0049"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0043","num":"0050"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0044","num":"0051"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0045","num":"0052"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0046","num":"0053"},"figref":"FIG. 13A"},{"@attributes":{"id":"p-0047","num":"0054"},"figref":"FIG. 13B"},{"@attributes":{"id":"p-0048","num":"0055"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0049","num":"0056"},"figref":"FIG. 15"}]},"DETDESC":[{},{}]}
