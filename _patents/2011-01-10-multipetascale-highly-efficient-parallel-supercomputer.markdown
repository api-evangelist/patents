---
title: Multi-petascale highly efficient parallel supercomputer
abstract: A Multi-Petascale Highly Efficient Parallel Supercomputer of 100 petaOPS-scale computing, at decreased cost, power and footprint, and that allows for a maximum packaging density of processing nodes from an interconnect point of view. The Supercomputer exploits technological advances in VLSI that enables a computing model where many processors can be integrated into a single Application Specific Integrated Circuit (ASIC). Each ASIC computing node comprises a system-on-chip ASIC utilizing four or more processors integrated into one die, with each having full access to all system resources and enabling adaptive partitioning of the processors to functions such as compute or messaging I/O on an application by application basis, and preferably, enable adaptive partitioning of functions in accordance with various algorithmic phases within an application, or if I/O or other processors are underutilized, then can participate in computation or communication nodes are interconnected by a five dimensional torus network with DMA that optimally maximize the throughput of packet communications between nodes and minimize latency.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09081501&OS=09081501&RS=09081501
owner: INTERNATIONAL BUSINESS MACHINES CORPORATION
number: 09081501
owner_city: Armonk
owner_country: US
publication_date: 20110110
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","STATEMENT OF GOVERNMENT INTEREST","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["The present disclosure claims priority from U.S. Provisional Patent Application Ser. No. 61\/293,611, filed on Jan. 8, 2010, and additionally claims priority from U.S. Provisional Application Ser. No. 61\/295,669, filed Jan. 15, 2010, and additionally claims priority from U.S. Provisional Application Ser. No. 61\/299,911, filed Jan. 29, 2010 the entire contents and disclosure of each of which is expressly incorporated by reference herein as if fully set forth herein.","The present invention further relates to following commonly-owned, co-pending U.S. patent applications, the entire contents and disclosure of each of which is expressly incorporated by reference herein as if fully set forth herein. U.S. Pat. No. 8,275,954, for \u201cUSING DMA FOR COPYING PERFORMANCE COUNTER DATA TO MEMORY\u201d; U.S. Pat. No. 8,275,964 for \u201cHARDWARE SUPPORT FOR COLLECTING PERFORMANCE COUNTERS DIRECTLY TO MEMORY\u201d; U.S. patent application Ser. No. 12\/684,190 for \u201cHARDWARE ENABLED PERFORMANCE COUNTERS WITH SUPPORT FOR OPERATING SYSTEM CONTEXT SWITCHING\u201d; U.S. Pat. No. 8,468,275, for \u201cHARDWARE SUPPORT FOR SOFTWARE CONTROLLED FAST RECONFIGURATION OF PERFORMANCE COUNTERS\u201d; U.S. Pat. No. 8,347,001, for \u201cHARDWARE SUPPORT FOR SOFTWARE CONTROLLED FAST MULTIPLEXING OF PERFORMANCE COUNTERS\u201d; U.S. patent application Ser. No. 12\/697,799, for \u201cCONDITIONAL LOAD AND STORE IN A SHARED CACHE\u201d; U.S. Pat. No. 8,595,389, for \u201cDISTRIBUTED PERFORMANCE COUNTERS\u201d; U.S. Pat. No. 8,103,910, for \u201cLOCAL ROLLBACK FOR FAULT-TOLERANCE IN PARALLEL COMPUTING SYSTEMS\u201d; U.S. Pat. No. 8,447,960, for \u201cPROCESSOR WAKE ON PIN\u201d; U.S. Pat. No. 8,268,389, for \u201cPRECAST THERMAL INTERFACE ADHESIVE FOR EASY AND REPEATED, SEPARATION AND REMATING\u201d; U.S. Pat. No. 8,359,404, for \u201cZONE ROUTING IN A TORUS NETWORK\u201d; U.S. patent application Ser. No. 12\/684,852, for \u201cPROCESSOR WAKEUP UNIT\u201d; U.S. Pat. No. 8,429,377, for \u201cTLB EXCLUSION RANGE\u201d; U.S. Pat. No. 8,356,122, for \u201cDISTRIBUTED TRACE USING CENTRAL PERFORMANCE COUNTER MEMORY\u201d; U.S. patent application Ser. No. 13\/008,602, for \u201cPARTIAL CACHE LINE SPECULATION SUPPORT\u201d; U.S. Pat. No. 8,473,683, for \u201cORDERING OF GUARDED AND UNGUARDED STORES FOR NO-SYNC I\/O\u201d; U.S. Pat. No. 8,458,267, for \u201cDISTRIBUTED PARALLEL MESSAGING FOR MULTIPROCESSOR SYSTEMS\u201d; U.S. Pat. No. 8,086,766, for \u201cSUPPORT FOR NON-LOCKING PARALLEL RECEPTION OF PACKETS BELONGING TO THE SAME MESSAGE\u201d; U.S. Pat. No. 8,571,834, for \u201cOPCODE COUNTING FOR PERFORMANCE MEASUREMENT\u201d; U.S. patent application Ser. No. 12\/684,776, for \u201cMULTI-INPUT AND BINARY REPRODUCIBLE, HIGH BANDWIDTH FLOATING POINT ADDER IN A COLLECTIVE NETWORK\u201d; U.S. Pat. No. 8,533,399, for \u201cCACHE DIRECTORY LOOK-UP REUSE\u201d; U.S. Pat. No. 8,621,478, for \u201cMEMORY SPECULATION IN A MULTI LEVEL CACHE SYSTEM\u201d; U.S. patent application Ser. No. 13\/008,583, for \u201cMETHOD AND APPARATUS FOR CONTROLLING MEMORY SPECULATION BY LOWER LEVEL CACHE\u201d; U.S. patent application Ser. No. 12\/984,308, for \u201cMINIMAL FIRST LEVEL CACHE SUPPORT FOR MEMORY SPECULATION MANAGED BY LOWER LEVEL CACHE\u201d; U.S. patent application Ser. No. 12\/984,329, for \u201cPHYSICAL ADDRESS ALIASING TO SUPPORT MULTI-VERSIONING IN A SPECULATION-UNAWARE CACHE\u201d; U.S. Pat. No. 8,255,633, for \u201cLIST BASED PREFETCH\u201d; U.S. Pat. No. 8,347,039, for \u201cPROGRAMMABLE STREAM PREFETCH WITH RESOURCE OPTIMIZATION\u201d; U.S. patent application Ser. No. 13\/004,005, for \u201cFLASH MEMORY FOR CHECKPOINT STORAGE\u201d; U.S. Pat. No. 8,359,367, for \u201cNETWORK SUPPORT FOR SYSTEM INITIATED CHECKPOINTS\u201d; U.S. Pat. No. 8,327,077, for \u201cTWO DIFFERENT PREFETCH COMPLEMENTARY ENGINES OPERATING SIMULTANEOUSLY\u201d; U.S. Pat. No. 8,364,844, for \u201cDEADLOCK-FREE CLASS ROUTES FOR COLLECTIVE COMMUNICATIONS EMBEDDED IN A MULTI-DIMENSIONAL TORUS NETWORK\u201d; U.S. Pat. No. 8,549,363, for \u201cIMPROVING RELIABILITY AND PERFORMANCE OF A SYSTEM-ON-A-CHIP BY PREDICTIVE WEAR-OUT BASED ACTIVATION OF FUNCTIONAL COMPONENTS\u201d; U.S. Pat. No. 8,571,847, for \u201cA SYSTEM AND METHOD FOR IMPROVING THE EFFICIENCY OF STATIC CORE TURN OFF IN SYSTEM ON CHIP (SoC) WITH VARIATION\u201d; U.S. patent application Ser. No. 12\/697,043, for \u201cIMPLEMENTING ASYNCHRONOUS COLLECTIVE OPERATIONS IN A MULTI-NODE PROCESSING SYSTEM\u201d; U.S. patent application Ser. No. 13\/008,546, for \u201cMULTIFUNCTIONING CACHE\u201d; U.S. patent application Ser. No. 12\/697,175 for \u201cI\/O ROUTING IN A MULTIDIMENSIONAL TORUS NETWORK\u201d; U.S. Pat. No. 8,370,551 for ARBITRATION IN CROSSBAR FOR LOW LATENCY; U.S. Pat. No. 8,312,193 for EAGER PROTOCOL ON A CACHE PIPELINE DATAFLOW; U.S. Pat. No. 8,521,990 for EMBEDDED GLOBAL BARRIER AND COLLECTIVE IN A TORUS NETWORK; U.S. Pat. No. 8,412,974 for GLOBAL SYNCHRONIZATION OF PARALLEL PROCESSORS USING CLOCK PULSE WIDTH MODULATION; U.S. patent application Ser. No. 12\/796,411 for IMPLEMENTATION OF MSYNC; U.S. patent application Ser. No. 12\/796,389 for NON-STANDARD FLAVORS OF MSYNC; U.S. patent application Ser. No. 12\/696,817 for HEAP\/STACK GUARD PAGES USING A WAKEUP UNIT; U.S. Pat. No. 8,527,740 for MECHANISM OF SUPPORTING SUB-COMMUNICATOR COLLECTIVES WITH O(64) COUNTERS AS OPPOSED TO ONE COUNTER FOR EACH SUB-COMMUNICATOR; and U.S. Pat. No. 8,595,554 for REPRODUCIBILITY IN BGQ.","This invention was made with Government support under subcontract number B554331 awarded by the Department of Energy. The Government has certain rights in this invention.","The present invention relates generally relates to the formation of a 100 petaflop scale, low power, and massively parallel supercomputer.","This invention relates generally to the field of high performance computing (HPC) or supercomputer systems and architectures of the type such as described in the IBM Journal of Research and Development, Special Double Issue on Blue Gene, Vol. 49, Numbers 2\/3, March\/May 2005; and, IBM Journal of Research and Development, Vol. 52, 49, Numbers 1 and 2, January\/March 2008, pp. 199-219.","Massively parallel computing structures (also referred to as \u201csupercomputers\u201d) interconnect large numbers of compute nodes, generally, in the form of very regular structures, such as mesh, torus, and tree configurations. The conventional approach for the most cost\/effective scalable computers has been to use standard processors configured in uni-processors or symmetric multiprocessor (SMP) configurations, wherein the SMPs are interconnected with a network to support message passing communications. Today, these supercomputing machines exhibit computing performance achieving 1-3 petaflops (see http:\/\/www.top500.org\/June 2009). However, there are two long standing problems in the computer industry with the current cluster of SMPs approach to building supercomputers: (1) the increasing distance, measured in clock cycles, between the processors and the memory (the memory wall problem) and (2) the high power density of parallel computers built of mainstream uni-processors or symmetric multi-processors (SMPs').","In the first problem, the distance to memory problem (as measured by both latency and bandwidth metrics) is a key issue facing computer architects, as it addresses the problem of microprocessors increasing in performance at a rate far beyond the rate at which memory speeds increase and communication bandwidth increases per year. While memory hierarchy (caches) and latency hiding techniques provide excellent solutions, these methods necessitate the applications programmer to utilize very regular program and memory reference patterns to attain good efficiency (i.e., minimizing instruction pipeline bubbles and maximizing memory locality).","In the second problem, high power density relates to the high cost of facility requirements (power, cooling and floor space) for such peta-scale computers.","It would be highly desirable to provide a supercomputing architecture that will reduce latency to memory, as measured in processor cycles, exploit locality of node processors, and optimize massively parallel computing at \u02dc100 petaOPS-scale at decreased cost, power, and footprint.","It would be highly desirable to provide a supercomputing architecture that exploits technological advances in VLSI that enables a computing model where many processors can be integrated into a single ASIC.","It would be highly desirable to provide a supercomputing architecture that comprises a unique interconnection of processing nodes for optimally achieving various levels of scalability.","It would be highly desirable to provide a supercomputing architecture that comprises a unique interconnection of processing nodes for efficiently and reliably computing global reductions, distribute data, synchronize, and share limited resources.","A novel massively parallel supercomputer capable of achieving 107 petaflop with up to 8,388,608 cores, or 524,288 nodes, or 512 racks is provided. It is based upon System-On-a-Chip technology, where each processing node comprises a single Application Specific Integrated Circuit (ASIC). The ASIC nodes are interconnected by a five-dimensional torus networks that optimally maximize packet communications throughput and minimize latency. The 5-D network includes a DMA (direct memory access) network interface.","In one aspect, there is provided a new class of massively-parallel, distributed-memory scalable computer architectures for achieving 100 peta-OPS scale computing and beyond, at decreased cost, power and footprint.","In a further aspect, there is provided a new class of massively-parallel, distributed-memory scalable computer architectures for achieving 100 peta-OPS scale computing and beyond that allows for a maximum packaging density of processing nodes from an interconnect point of view.","In a further aspect, there is provided an unprecedented-scale supercomputing architecture that exploits technological advances in VLSI that enables a computing model where many processors can be integrated into a single ASIC. Preferably, simple processing cores are utilized that have been optimized for minimum power consumption and capable of achieving superior price\/performance to those obtainable current architectures, while having system attributes of reliability, availability, and serviceability expected of large servers. Particularly, each computing node comprises a system-on-chip ASIC utilizing four or more processors integrated into one die, with each having full access to all system resources. Many processors on a single die enables adaptive partitioning of the processors to functions such as compute or messaging I\/O on an application by application basis, and preferably, enable adaptive partitioning of functions in accordance with various algorithmic phases within an application, or if I\/O or other processors are underutilized, then can participate in computation or communication.","In a further aspect, there is provided an ultra-scale supercomputing architecture that incorporates a plurality of network interconnect paradigms. Preferably, these paradigms include a five dimensional torus with DMA. The architecture allows parallel processing message-passing.","In a further aspect, there is provided in an highly scalable computer architecture, key synergies that allow new and novel techniques and algorithms to be executed in the massively parallel processing arts.","In a further aspect, there is provided I\/O nodes for filesystem I\/O wherein I\/O communications and host communications are carried out. The application can perform I\/O and external interactions without unbalancing the performance of the 5-D torus nodes.","Moreover, these techniques also provide for partitioning of the massively parallel supercomputer into a flexibly configurable number of smaller, independent parallel computers, each of which retain all of the features of the larger machine. Given the tremendous scale of this supercomputer, these partitioning techniques also provide the ability to transparently remove, or map around, any failed racks or parts of racks referred to herein as \u201cmidplanes,\u201d so they can be serviced without interfering with the remaining components of the system.","In a further aspect, there is added serviceability such as Ethernet addressing via physical location, and JTAG interfacing to Ethernet.","According to yet another aspect of the invention, there is provided a scalable, massively parallel supercomputer comprising: a plurality of processing nodes interconnected in n-dimensions, each node including one or more processing elements for performing computation or communication activity as required when performing parallel algorithm operations; and, the n-dimensional network meets the bandwidth and latency requirements of a parallel algorithm for optimizing parallel algorithm processing performance.","In one embodiment, the node architecture is based upon System-On-a-Chip (SOC) Technology wherein the basic building block is a complete processing \u201cnode\u201d comprising a single Application Specific Integrated Circuit (ASIC). When aggregated, each of these processing nodes is termed a \u2018Cell\u2019, allowing one to define this new class of massively parallel machine constructed from a plurality of identical cells as a \u201cCellular\u201d computer. Each node preferably comprises a plurality (e.g., four or more) of processing elements each of which includes a central processing unit (CPU), a plurality of floating point processors, and a plurality of network interfaces.","The SOC ASIC design of the nodes permits optimal balance of computational performance, packaging density, low cost, and power and cooling requirements. In conjunction with novel packaging technologies, it further enables scalability to unprecedented levels The system-on-a-chip level integration allows for low latency to all levels of memory including a local main store associated with each node, thereby overcoming the memory wall performance bottleneck increasingly affecting traditional supercomputer systems. Within each node, each of multiple processing elements may be used individually or simultaneously to work on any combination of computation or communication as required by the particular algorithm being solved or executed at any point in time.","At least three modes of operation are supported. In the full virtual node mode, each of the processing cores will perform its own MPI (message passing interface) process independently. Each core is running four thread\/process, and it uses a sixteenth of the memory (L2 and SDRAM) of the node, while coherence among the 64 processes within the node and across the nodes is maintained by MPI. In the full SMP, one MPI task with 64 threads (4 threads per core) is running, using the whole node memory capacity. The third mode called the mixed mode. Here 2, 4, 8, 16, and 32 processes are running 32, 16, 8, 4, and 2 threads, respectively.","Because of the torus' DMA feature, internode communications can overlap with computations running concurrently on the nodes.","With respect to the Torus network, it is configured, in one embodiment, as a 5-dimensional design supporting hyper-cube communication and partitioning. A 4-Dimensional design allows a direct mapping of computational simulations of many physical phenomena to the Torus network. However, higher dimensionality, 5 or 6-dimensional Toroids, which allow shorter and lower latency paths at the expense of more chip-to-chip connections and significantly higher cabling costs have been implemented in the past.","Further independent networks include an external Network (such as a 10 Gigabit Ethernet) that provides attachment of input\/output nodes to external server and host computers; and a Control Network (a combination of 1 Gb Ethernet and a IEEE 1149.1 Joint Test Access Group (JTAG) network) that provides complete low-level debug, diagnostic and configuration capabilities for all nodes in the entire machine, and which is under control of a remote independent host machine, called the \u201cService Node\u201d. Preferably, use of the Control Network operates with or without the cooperation of any software executing on the nodes of the parallel machine. Nodes may be debugged or inspected transparently to any software they may be executing. The Control Network provides the ability to address all nodes simultaneously or any subset of nodes in the machine. This level of diagnostics and debug is an enabling technology for massive levels of scalability for both the hardware and software.","Novel packaging technologies are employed for the supercomputing system that enables unprecedented levels of scalability, permitting multiple networks and multiple processor configurations. In one embodiment, there is provided multi-node \u201cNode Cards\u201d including a plurality of Compute Nodes, plus optionally one or two I\/O Node where the external I\/O Network is enabled. In this way, the ratio of computation to external input\/output may be flexibly selected by populating \u201cmidplane\u201d units with the desired number of I\/O nodes. The packaging technology permits sub-network partitionability, enabling simultaneous work on multiple independent problems. Thus, smaller development, test and debug partitions may be generated that do not interfere with other partitions.","Connections between midplanes and racks are selected to be operable based on partitioning. Segmentation creates isolated partitions; each partition owning the full bandwidths of all interconnects, providing predictable and repeatable performance. This enables fine-grained application performance tuning and load balancing that remains valid on any partition of the same size and shape. In the case where extremely subtle errors or problems are encountered, this partitioning architecture allows precise repeatability of a large scale parallel application. Partitionability, as enabled by the present invention, provides the ability to segment so that a network configuration may be devised to avoid, or map around, non-working racks or midplanes in the supercomputing machine so that they may be serviced while the remaining components continue operation.","The present invention is directed to a next-generation massively parallel supercomputer, hereinafter referred to as \u201cBluGene\u201d or \u201cBluGene\/Q\u201d. The previous two generations were detailed in the IBM Journal of Research and Development, Special Double Issue on Blue Gene, Vol. 49, Numbers 2\/3, March\/May 2005; and, IBM Journal of Research and Development, Vol. 52, 49, Numbers 1 and 2, January\/March 2008, pp. 199-219, the whole contents and disclosures of which are incorporated by reference as if fully set forth herein. The system uses a proven Blue Gene architecture, exceeding by over 15\u00d7 the performance of the prior generation Blue Gene\/P per dual-midplane rack. Besides performance, there are addition several novel enhancements which will be described herein below.",{"@attributes":{"id":"p-0287","num":"0286"},"figref":"FIG. 1-0","b":["50","100","50","50","1","50"],"i":"n"},"A compute node of this present massively parallel supercomputer architecture and in which the present invention may be employed is illustrated in . The compute nodechip  is a single chip ASIC (\u201cNodechip\u201d) based on low power processing core architecture, though the architecture can use any low power cores, and may comprise one or more semiconductor chips. In the embodiment depicted, the node employs PowerPC\u00ae A2 at 1600 MHz, and support a 4-way multi-threaded 64b PowerPC implementation. Although not shown, each A2 core has its own execution unit (XU), instruction unit (IU), and quad floating point unit (QPU or FPU) connected via an AXU (Auxiliary eXecution Unit). The QPU is an implementation of a quad-wide fused multiply-add SIMD QPX floating point instruction set architecture, producing, for example, eight (8) double precision operations per cycle, for a total of 128 floating point operations per cycle per compute chip. QPX is an extension of the scalar PowerPC floating point architecture. It includes multiple, e.g., thirty-two, 32B-wide floating point registers per thread.","More particularly, the basic nodechip  of the massively parallel supercomputer architecture illustrated in  includes multiple symmetric multiprocessing (SMP) cores , each core being 4-way hardware threaded supporting transactional memory and thread level speculation, and, including the Quad Floating Point Unit (FPU)  on each core. In one example implementation, there is provided sixteen or seventeen processor cores , plus one redundant or back-up processor core, each core operating at a frequency target of 1.6 GHz providing, for example, a 563 GB\/s bisection bandwidth to shared L2 cache  via an interconnect device , such as a full crossbar or SerDes switches. In one example embodiment, there is provided 32 MB of shared L2 cache , each of sixteen cores core having associated 2 MB of L2 cache  in the example embodiment. There is further provided external DDR SDRAM (e.g., Double Data Rate synchronous dynamic random access) memory , as a lower level in the memory hierarchy in communication with the L2. In one embodiment, the compute node employs or is provided with 8-16 GB memory\/node. Further, in one embodiment, the node includes 42.6 GB\/s DDR3 bandwidth (1.333 GHz DDR3) (2 channels each with chip kill protection).","Each FPU  associated with a core  provides a 32B wide data path to the L1-cache  of the A2, allowing it to load or store 32B per cycle from or into the L1-cache . Each core  is directly connected to a private prefetch unit (level-1 prefetch, L1P) , which accepts, decodes and dispatches all requests sent out by the A2. The store interface from the A2 core  to the L1P  is 32B wide, in one example embodiment, and the load interface is 16B wide, both operating at processor frequency. The L1P  implements a fully associative, 32 entry prefetch buffer, each entry holding an L2 line of 128B size, in one embodiment. The L1P provides two prefetching schemes for the private prefetch unit : a sequential prefetcher, as well as a list prefetcher.","As shown in , the shared L2  may be sliced into 16 units, each connecting to a slave port of the crossbar switch device (XBAR) switch . Every physical address is mapped to one slice using a selection of programmable address bits or a XOR-based hash across all address bits. The L2-cache slices, the L1Ps and the L1-D caches of the A2s are hardware-coherent. A group of four slices may be connected via a ring to one of the two DDR3 SDRAM controllers .","Network packet I\/O functionality at the node is provided and data throughput increased by implementing MU . Each MU at a node includes multiple parallel operating DMA engines, each in communication with the XBAR switch, and a Network Interface unit . In one embodiment, the Network interface unit of the compute node includes, in a non-limiting example: 10 intra-rack and inter-rack interprocessor links , each operating at 2.0 GB\/s, that, in one embodiment, may be configurable as a 5-D torus, for example); and, one I\/O link  interfaced with the Network interface Unit  at 2.0 GB\/s (i.e., a 2 GB\/s I\/O link (to an I\/O subsystem)) is additionally provided.","The system is expandable to 512 compute racks, each with 1024 compute node ASICs (BQC) containing 16 PowerPC A2 processor cores at 1600 MHz. Each A2 core has associated a quad-wide fused multiply-add SIMD floating point unit, producing 8 double precision operations per cycle, for a total of 128 floating point operations per cycle per compute chip. Cabled as a single system, the multiple racks can be partitioned into smaller systems by programming switch chips, termed the BG\/Q Link ASICs (BQL), which source and terminate the optical cables between midplanes. Each compute rack consists of 2 sets of 512 compute nodes. Each set is packaged around a doubled-sided backplane, or midplane, which supports a five-dimensional torus of size 4\u00d74\u00d74\u00d74\u00d72 which is the communication network for the compute nodes which are packaged on 16 node boards. This tori network can be extended in 4 dimensions through link chips on the node boards, which redrive the signals optically with an architecture limit of 64 to any torus dimension. The signaling rate is 10 Gb\/s, 8\/10 encoded), over \u02dc20 meter multi-mode optical cables at 850 nm. As an example, a 96-rack system is connected as a 16\u00d716\u00d716\u00d712\u00d72 torus, with the last x2 dimension contained wholly on the midplane. For reliability reasons, small torus dimensions of 8 or less may be run as a mesh rather than a torus with minor impact to the aggregate messaging rate.","The Blue Gene\/Q platform includes four kinds of nodes: compute nodes (CN), I\/O nodes (ION), login nodes (LN), and service nodes (SN). The CN and ION share the same Blue Gene\/Q compute ASIC.","Microprocessor Core and Quad Floating Point Unit of CN and ION","The basic node of this present massively parallel supercomputer architecture is illustrated in . As shown in , each includes 16+1 (symmetric multiprocessing) cores (SMP), each core being 4-way hardware threaded supporting transactional memory and thread level speculation, and, including a Quad floating point unit on each core (204.8 GF peak node). The core operating frequency target is 1.6 GHz and a 563 GB\/s bisection bandwidth to shared L2 cache (32 MB of shared L2 cache in the embodiment depicted). There is further provided 42.6 GB\/s DDR3 bandwidth (1.333 GHz DDR3) (2 channels each with chip kill protection); 10 intra-rack interprocessor links each at 2.0 GB\/s (i.e., 10*2 GB\/s intra-rack & inter-rack (e.g., configurable as a 5-D torus in one embodiment); one I\/O link at 2.0 GB\/s (2 GB\/s I\/O link (to I\/O subsystem)); and, 8-16 GB memory\/node. The ASIC may consume up to about 30 watts chip power.","The node here is based on a low power A2 PowerPC cores, though the architecture can use any low power cores. The A2 is a 4-way multi-threaded 64b PowerPC implementation. Each A2 core has its own execution unit (XU), instruction unit (IU), and quad floating point unit (QPU) connected via the AXU (Auxiliary eXecution Unit) (). The QPU is an implementation of the 4-way SIMD QPX floating point instruction set architecture. QPX is an extension of the scalar PowerPC floating point architecture. It defines 32 32B-wide floating point registers per thread instead of the traditional 32 scalar 8B-wide floating point registers. Each register contains 4 slots, each slot storing an 8B double precision floating point number. The leftmost slot corresponds to the traditional scalar floating point register. The standard PowerPC floating point instructions operate on the left-most slot to preserve the scalar semantics as well as in many cases also on the other three slots. Programs that are assuming only the traditional FPU ignore the results generated by the additional three slots. QPX defines, in addition to the traditional instructions new load, store, arithmetic instructions, rounding, conversion, compare and select instructions that operate on all 4 slots in parallel and deliver 4 double precision floating point results concurrently. The load and store instructions move 32B from and to main memory with a single instruction. The arithmetic instructions include addition, subtraction, multiplication, various forms of multiply-add as well as reciprocal estimates and reciprocal square root estimates.",{"@attributes":{"id":"p-0297","num":"0296"},"figref":"FIG. 2-0"},"Compute ASIC Node","The compute chip implements 18 PowerPC compliant A2 cores and 18 attached QPU floating point units. In one embodiment, seventeen (17) cores are functional. The 18th \u201credundant\u201d core is in the design to improve chip yield. Of the 17 functional units, 16 will be used for computation leaving one to be reserved for system function.","I\/O Node","Besides the 1024 compute nodes per rack, there are associated I\/O nodes. These I\/O nodes are in separate racks, and are connected to the compute nodes through an 11th port (an I\/O port such as shown in ). The I\/O nodes are themselves connected in a 5D torus with an architectural limit. I\/O nodes include an associated PCIe 2.0 adapter card, and can exist either with compute nodes in a common midplane, or as separate I\/O racks connected optically to the compute racks; the difference being the extent of the torus connecting the nodes. The SN and I\/ONs are accessed through an Ethernet control network. For this installation the storage nodes are connected through a large IB (InfiniBand) switch to I\/O nodes.","Memory Hierarchy\u2014L1 and L1P","The QPU has a 32B wide data path to the L1-cache of the A2, allowing it to load or store 32B per cycle from or into the L1-cache. Each core is directly connected to a private prefetch unit (level-1 prefetch, L1P), which accepts, decodes and dispatches all requests sent out by the A2. The store interface from the A2 to the L1P is 32B wide and the load interface is 16B wide, both operating at processor frequency. The L1P implements a fully associative, 32 entry prefetch buffer. Each entry can hold an L2 line of 128B size. The L1P provides two prefetching schemes: a sequential prefetcher as used in previous Blue Gene architecture generations, as well as a novel list prefetcher. The list prefetcher tracks and records memory requests, sent out by the core, and writes the sequence as a list to a predefined memory region. It can replay this list to initiate prefetches for repeated sequences of similar access patterns. The sequences do not have to be identical, as the list processing is tolerant to a limited number of additional or missing accesses. This automated learning mechanism allows a near perfect prefetch behavior for a set of important codes that show the required access behavior, as well as perfect prefetch behavior for codes that allow precomputation of the access list.","A system, method and computer program product is provided for improving a performance of a parallel computing system, e.g., by prefetching data or instructions according to a list including a sequence of prior cache miss addresses.","In one embodiment, a parallel computing system operates at least an algorithm for prefetching data and\/or instructions. According to the algorithm, with software (e.g., a compiler) cooperation, memory access patterns can be recorded and\/or reused by at least one list prefetch engine (e.g., a software or hardware module prefetching data or instructions according to a list including a sequence of prior cache miss address(es)). In one embodiment, there are at least four list prefetch engines. A list prefetch engine allows iterative application software (e.g., \u201cwhile\u201d loop, etc.) to make an efficient use of general, but repetitive, memory access patterns. The recording of patterns of physical memory access by hardware (e.g., a list prefetch engine  in FIG. --) enables virtual memory transactions to be ignored and recorded in terms of their corresponding physical memory addresses.","A list describes an arbitrary sequence (i.e., a sequence not necessarily arranged in an increasing, consecutive order) of prior cache miss addresses (i.e., addresses that caused cache misses before). In one embodiment, address lists which are recorded from L1 (level one) cache misses and later loaded and used to drive the list prefetch engine may include, for example, 29-bit, 128-byte addresses identifying L2 (level-two) cache lines in which an L1 cache miss occurred. Two additional bits are used to identify, for example, the 64-byte, L1 cache lines which were missed. In this embodiment, these 31 bits plus an unused bit compose the basic 4-byte record out of which these lists are composed.","FIG. -- illustrates a system diagram of a list prefetch engine  in one embodiment. The list prefetch engine  includes, but is not limited to: a prefetch unit , a comparator , a first array referred to herein as \u201cListWrite array\u201d , a second array referred to herein as \u201cListRead array\u201d , a first module , a read module  and a write module . In one embodiment, there may be a plurality of list prefetch engines. A particular list prefetch engine operates on a single list at a time. A list ends with \u201cEOL\u201d (End of List). In a further embodiment, there may be provided a micro-controller (not shown) that requests a first segment (e.g., 64-byte segment) of a list from a memory device (not shown). This segment is stored in the ListRead array .","In one embodiment, a general approach to efficiently prefetching data being requested by a L1 (level-one) cache is to prefetch data and\/or instructions following a memorized list of earlier access requests. Prefetching data according to a list works well for repetitive portions of code which do not contain data-dependent branches and which repeatedly make the same, possibly complex, pattern of memory accesses. Since this list prefetching (i.e., prefetching data whose addresses appear in a list) can be understood at an application level, a recording of such a list and its use in subsequent iterations may be initiated by compiler directives placed in code at strategic spots. For example, \u201cstart_list\u201d (i.e., a directive for starting a list prefetch engine) and \u201cstop_list\u201d (i.e., a directive for stopping a list prefetch engine) directives may locate those strategic spots of the code where first memorizing, and then later prefetching, a list of L1 cache misses may be advantageous.","In one embodiment, a directive called start_list causes a processor core to issue a memory mapped command (e.g., input\/output command) to the parallel computing system. The command may include, but not limited to:\n\n","The first module  receives a current cache miss address (i.e., an address which currently causes a cache miss) and evaluates whether the current cache miss address is valid. A valid cache miss address refers to a cache miss address belonging to a class of cache miss addresses for which a list prefetching is intended In one embodiment, the first module  evaluates whether the current cache miss address is valid or not, e.g., by checking a valid bit attached on the current cache miss address. The list prefetch engine  stores the current cache miss address in the ListWrite array  and\/or the history FIFO. In one embodiment, the write module  writes the contents of the array  to a memory device when the array  becomes full. In another embodiment, as the ListWrite Array  is filled, e.g., by continuing L1 cache misses, the write module  continually writes the contents of the array  to a memory device and forms a new list that will be used on a next iteration (e.g., a second iteration of a \u201cfor\u201d loop, etc.).","In one embodiment, the write module  stores the contents of the array  in a compressed form (e.g., collapsing a sequence of adjacent addresses into a start address and the number of addresses in the sequence) in a memory device (not shown). In one embodiment, the array  stores a cache miss address in each element of the array. In another embodiment, the array  stores a pointer pointing to a list of one or more addresses. In one embodiment, there is provided a software entity (not shown) for tracing a mapping between a list and a software routine (e.g., a function, loop, etc.). In one embodiment, cache miss addresses, which fall within an allowed address range, carry a proper pattern of translation lookaside buffer (TLB) user bits and are generated, e.g., by an appropriate thread. These cache miss addresses are stored sequentially in the ListWrite array .","In one embodiment, a processor core may allow for possible list miss-matches where a sequence of load commands deviates sufficiently from a stored list that the list prefetch engine  uses. Then, the list prefetch engine  abandons the stored list but continues to record an altered list for a later use.","In one embodiment, each list prefetch engine includes a history FIFO (not shown). This history FIFO can be implemented, e.g., by a 4-entry deep, 4 byte-wide set of latches, and can include at least four most recent L2 cache lines which appeared as L1 cache misses. This history FIFO can store L2 cache line addresses corresponding to prior L1 cache misses that happened most recently. When a new L1 cache miss, appropriate for a list prefetch engine, is determined as being valid, e.g., based on a valid bit associated with the new L1 cache miss, an address (e.g., 64-byte address) that caused the L1 cache miss is compared with the at least four addresses in the history FIFO. If there is a match between the L1 cache miss address and one of the at least four addresses, an appropriate bit in a corresponding address field (e.g., 32-bit address field) is set to indicate the half portion of the L2 cache line that was missed, e.g., the 64-byte portion of the 128-byte cache line was missed. If a next L1cache miss address matches none of the at least four addresses in the history FIFO, an address at a head of the history FIFO is written out, e.g., to the ListWrite array , and this next address is added to a tail of the history FIFO.","When an address is removed from one entry of the history FIFO, it is written into the ListWrite array . In one embodiment, this ListWrite array  is an array, e.g., 8-deep, 16-byte wide array, which is used by all or some of list prefetch engines. An arbiter (not shown) assigns a specific entry (e.g., a 16-btye entry in the history FIFO) to a specific list prefetch engine. When this specific entry is full, it is scheduled to be written to memory and a new entry assigned to the specific list prefetch engine.","The depth of this ListWrite array  may be sufficient to allow for a time period for which a memory device takes to respond to this writing request (i.e., a request to write an address in an entry in the history FIFO to the ListWrite array ), providing sufficient additional space that a continued stream of L1 cache miss addresses will not overflow this ListWrite array . In one embodiment, if 20 clock cycles are required for a 16-byte word of the list to be accepted to the history FIFO and addresses can be provided at the rate at which L2 cache data is being supplied (one L1 cache miss corresponds to 128 bytes of data loaded in 8 clock cycles), then the parallel computing system may need to have a space to hold 20\/8\u22483 addresses or an additional 12 bytes. According to this embodiment, the ListWrite array  may be composed of at least four, 4-byte wide and 3-word deep register arrays. Thus, in this embodiment, a depth of 8 may be adequate for the ListWrite array  to support a combination of at least four list prefetch engines with various degrees of activity. In one embodiment, the ListWrite array  stores a sequence of valid cache miss addresses.","The list prefetch engine  stores the current cache miss address in the array . The list prefetch engine  also provides the current cache miss address to the comparator . In one embodiment, the engine  provides the current miss address to the comparator  when it stores the current miss address in the array . In one embodiment, the comparator  compares the current cache miss address and a list address (i.e., an address in a list; e.g., an element in the array ). If the comparator  does not find a match between the current miss address and the list address, the comparator  compares the current cache miss address with the next list addresses (e.g., the next eight addresses listed in a list; the next eight elements in the array ) held in the ListRead Array  and selects the earliest matching address in these addresses (i.e., the list address and the next list addresses). The earliest matching address refers to a prior cache miss address whose index in the array  is the smallest and which matches with the current cache miss address. An ability to match a next address in the list with the current cache miss address is a fault tolerant feature permitting addresses in the list which do not reoccur as L1 cache misses in a current running of a loop to be skipped over.","In one embodiment, the comparator  compares addresses in the list and the current cache miss address in an order. For example, the comparator  compares the current cache miss address and the first address in the list. Then, the comparator may compare the current cache miss address and the second address in the list. In one embodiment, the comparator  synchronizes an address in the list which the comparator  matches with the current cache miss address with later addresses in the list for which data is being prefetched. For example, the list prefetch engine  finds a match between a second element in the array , then the list prefetch engine  prefetches data whose addresses are stored in the second element and subsequent elements of the array . This separation between the address in the list which matches the current cache miss address and the address in the list being prefetched is called the prefetch depth and in one embodiment this depth can be set, e.g., by software (e.g., a compiler). In one embodiment, the comparator  includes a fault-tolerant feature. For example, when the comparator  detects a valid cache miss address that does not match any list address with which it is compared, that cache miss address is dropped and the comparator  waits for next valid address. In another embodiment, a series of mismatches between the cache miss address and the list address (i.e., addresses in a list) may cause the list prefetch engine to be aborted. However, a construction of a new list in the ListWrite array  will continue. In one embodiment, loads (i.e., load commands) from a processor core may be stalled until a list has been read from a memory device and the list prefetch engine  is ready to compare () subsequent L1 cache misses with at least or at most eight addresses of the list.","In one embodiment, lists needed for a comparison () by at least four list prefetch engines are loaded (under a command of individual list prefetch engines) into a register array, e.g., an array of 24 depth and 16-bytes width. These registers are loaded according to a clock frequency with data coming from the memory (not shown). Thus, each list prefetch engine can access at least 24 four-byte list entries from this register array. In one embodiment, a list prefetch engine may load these list entries into its own set of, for example, 8, 4-byte comparison latches. L1 cache miss addresses issued by a processor core can then be compared with addresses (e.g., at least or at most eight addresses) in the list. In this embodiment, when a list prefetch engine consumes 16 of the at least 24 four-byte addresses and issues a load request for data (e.g., the next 64-byte data in the list), a reservoir of the 8, 4-byte addresses may remain, permitting a single skip-by-eight (i.e., skipping eight 4-byte addresses) and subsequent reload of the 8, 4-byte comparison latches without requiring a stall of the processor core.","In one embodiment, L1 cache misses associated with a single thread may require data to be prefetched at a bandwidth of the memory system, e.g., one 32-byte word every two clock cycles. In one embodiment, if the parallel computing system requires, for example, 100 clock cycles for a read command to the memory system to produce valid data, the ListRead array  may have sufficient storage so that 100 clock cycles can pass between an availability of space to store data in the ListRead array  and a consumption of the remaining addresses in the list. In this embodiment, in order to conserve area in the ListReady array , only 64-byte segments of the list may be requested by the list prefetch engine . Since each L1 cache miss leads to a fetching of data (e.g., 128-byte data), the parallel computing system may consume addresses in an active list at a rate of one address every particular clock cycles (e.g., 8 clock cycles). Recognizing a size of an address, e.g., as 4 bytes, the parallel computing system may calculate that a particular lag (e.g., 100 clock cycle lag) between a request and data in the list may require, for example, 100\/8*4 or a reserve of 50 bytes to be provided in the ListRead array . Thus, a total storage provided in the ListRead array  may be, for example, 50+64\u2248114 bytes. Then, a total storage (e.g., 32+96=128 bytes) of the ListRead array  may be close to a maximum requirement.","The prefetch unit  prefetches data and\/or instruction(s) according to a list if the comparator  finds a match between the current cache miss address and an address on the list. The prefetch unit  may prefetch all or some of the data stored in addresses in the list. In one embodiment, the prefetch unit  prefetches data and\/or instruction(s) up to a programmable depth (i.e., a particular number of instructions or particular amount of data to be prefetched; this particular number or particular amount can be programmed, e.g., by software).","In one embodiment, addresses held in the comparator  determine prefetch addresses which occur later in the list and which are sent to the prefetch unit  (with an appropriate arbitration between the at least four list prefetch engines). Those addresses (which have not yet been matched) are sent off for prefetching up to a programmable prefetch depth (e.g., a depth of 8). If an address matching (e.g., an address comparison between an L1 cache miss address and an address in a list) proceeds with a sufficient speed that a list address not yet prefetched matches the L1 cache miss address, this list address may trigger a demand to load data in the list address and no prefetch of the data is required. Instead, a demand load of the data to be returned directly to a processor core may be issued. The address matching may be done in parallel or in sequential, e.g., by the comparator .","In one embodiment, the parallel computing system can estimate the largest prefetch depth that might be needed to ensure that prefetched data will be available when a corresponding address in the list turns up as an L1 cache miss address (i.e., an address that caused an L1 cache miss). Assuming that a single thread running in a processor core is consuming data as fast as the memory system can provide to it (e.g., a new 128-byte prefetch operation every 8 clock cycles) and that a prefetch request requires, for example, 100 clock cycles to be processed, the parallel computing system may need to have, for example, 100\/8\u224812 prefetch active commands; that is, a depth of 12, which may be reasonably close to the largest available depth (e.g., a depth of 8).","In one embodiment, the read module  stores a pointer pointing to a list including addresses whose data may be prefetched in each element. The ListRead array  stores an address whose data may be prefetched in each element. The read module  loads a plurality of list elements from a memory device to the ListRead array . A list loaded by the read module  includes, but is not limited to: a new list (i.e., a list that is newly created by the list prefetch engine ), an old list (i.e., a list that has been used by the list prefetch engine ). Contents of the ListRead array  are presented as prefetch addresses to a prefetch unit  to be prefetched. This presence may continue until a pre-determined or post-determined prefetching depth is reached. In one embodiment, the list prefetch engine  may discard a list whose data has been prefetched. In one embodiment, a processor (not shown) may stall until the ListRead array  is fully or partially filled.","In one embodiment, there is provided a counter device in the prefetching control (not shown) which counts the number of elements in the ListRead array  between that most recently matched by the comparator  and the latest address sent to the prefetch unit . As a value of the counter device decrements, i.e., the number of matches increments, while the matching operates with the ListRead array , prefetching from later addresses in the ListRead array  may be initiated to maintain a preset prefetching depth for the list.","In one embodiment, the list prefetch engine  may be implemented in hardware or reconfigurable hardware, e.g., FPGA (Field Programmable Gate Array) or CPLD (Complex Programmable Logic Device), using a hardware description language (Verilog, VHDL, Handel-C, or System C). In another embodiment, the list prefetch engine  may be implemented in a semiconductor chip, e.g., ASIC (Application-Specific Integrated Circuit), using a semi-custom design methodology, i.e., designing a chip using standard cells and a hardware description language. In one embodiment, the list prefetch engine  may be implemented in a processor (e.g., IBM\u00ae PowerPC\u00ae processor, etc.) as a hardware unit(s). In another embodiment, the list prefetch engine  may be implemented in software (e.g., a compiler or operating system), e.g., by a programming language (e.g., Java\u00ae, C\/C++, .Net, Assembly language(s), Pearl, etc.).","FIG. -- illustrates a flow chart illustrating method steps performed by the list prefetch engine  in one embodiment. At step , a parallel computing system operates at least one list prefetch engine (e.g., a list prefetch engine ). At step , a list prefetch engine  receives a cache miss address and evaluates whether the cache miss address is valid or not, e.g., by checking a valid bit of the cache miss address. If the cache miss address is not valid, the control goes to step  to receive a next cache miss address. Otherwise, at step , the list prefetch engine  stores the cache miss address in the ListWrite array .","At step , the list prefetch engine evaluates whether the ListWrite array  is full or not, e.g., by checking an empty bit (i.e., a bit indicating that a corresponding slot is available) of each slot of the array . If the ListWrite array  is not full, the control goes to step  to receive a next cache miss address. Otherwise, at step , the list prefetch engine stores contents of the array  in a memory device.","At step , the parallel computing system evaluates whether the list prefetch engine needs to stop. Such a command to stop would be issued when running list control software (not shown) issues a stop list command (i.e., a command for stopping the list prefetch engine ). If such a stop command has not been issued, the control goes to step  to receive a next cache miss address. Otherwise, at step , the prefetch engine flushes contents of the ListWrite array . This flushing may set empty bits (e.g., a bit indicating that an element in an array is available to store a new value) of elements in the ListWrite array  to high (\u201c1\u201d) to indicate that those elements are available to store new values. Then, at step , the parallel computing system stops this list prefetch engine (i.e., a prefetch engine performing the steps -).","While operating steps -, the prefetch engine  may concurrently operate steps -. At step , the list prefetch engine  determines whether the current list has been created by a previous use of a list prefetch engine or some other means. In one embodiment, this is determined by a \u201cload list\u201d command bit set by software when the list engine prefetch  is started. If this \u201cload list\u201d command bit is not set to high (\u201c1\u201d), then no list is loaded to the ListRead array  and the list prefetch engine  only records a list of the L1 cache misses to the history FIFO or the ListWrite array  and does no prefetching.","If the list assigned to this list prefetch engine  has not been created, the control goes to step  to not load a list into the ListRead array  and to not prefetch data. If the list has been created, e.g., a list prefetch engine or other means, the control goes to step . At step , the read module  begins to load the list from a memory system.","At step , a state of the ListRead array  is checked. If the ListRead array  is full, then the control goes to step  for an analysis of the next cache miss address. If the ListRead array  is not full, a corresponding processor core is held at step  and the read module  continues loading prior cache miss addresses into the ListRead array  at step .","At step , the list prefetch engine evaluates whether the received cache miss address is valid, e.g., by checking a valid bit of the cache miss address. If the cache miss address is not valid, the control repeats the step  to receive a next cache miss address and to evaluate whether the next cache miss address is valid. A valid cache miss address refers to a cache miss address belonging to a class of cache miss addresses for which a list prefetching is intended Otherwise, at step , the comparator  compares the valid cache miss address and address(es) in list in the ListRead array . In one embodiment, the ListRead array  stores a list of prior cache miss addresses. If the comparator  finds a match between the valid cache miss address and an address in a list in the ListRead array, the list prefetch engine resets a value of a counter device which counts the number of mismatches between the valid cache miss address and addresses in list(s) in the ListRead array .","Otherwise, at step , the list prefetch engine compares the value of the counter device to a threshold value. If the value of the counter device is greater than the threshold value, the control goes to step  to let the parallel computing system stop the list prefetch engine . Otherwise, at step , the list prefetch engine  increments the value of the counter device and the control goes back to the step .","At step , the list prefetch engine prefetches data whose addresses are described in the list which included the matched address. The list prefetch engine prefetches data stored in all or some of the addresses in the list. The prefetched data whose addresses may be described later in the list, e.g., subsequently following the match address. At step , the list prefetch engine evaluates whether the list prefetch engine reaches \u201cEOL\u201d (End of List) of the list. In other words, the list prefetch engine  evaluates whether the prefetch engine  has prefetched all the data whose addresses are listed in the list. If the prefetch engine does not reach the \u201cEOL,\u201d the control goes back to step  to load addresses (in the list) whose data have not been prefetched yet into the ListRead array . Otherwise, the control goes to step . At step , the parallel computing system stops operating the list prefetch engine .","In one embodiment, the parallel computing system allows the list prefetch engine to memorize an arbitrary sequence of prior cache miss addresses for one iteration of programming code and subsequently exploit these addresses by prefetching data stored in this sequence of addresses. This data prefetching is synchronized with an appearance of earlier cache miss addresses during a next iteration of the programming code.","In a further embodiment, the method illustrated in FIG. -- may be extended to include the following variations when implementing the method steps in FIG. --:","The list prefetch engine can prefetch data through a use of a sliding window (e.g., a fixed number of elements in the ListRead array ) that tracks the latest cache miss addresses thereby allowing to prefetch data stored in a fixed number of cache miss addresses in the sliding window. This usage of the sliding window achieves a maximum performance, e.g., by efficiently utilizing a prefetch buffer which is a scarce resource. The sliding window also provides a degree of tolerance in that a match in the list is not necessary as long as the next L1 cache miss address is within a range of a width of the sliding window.","A list of addresses can be stored in a memory device in a compressed form to reduce an amount of storage needed by the list.","Lists are indexed and can be explicitly controlled by software (user or compiler) to be invoked.","Lists can optionally be simultaneously saved while a current list is being utilized for prefetching. This feature allows an additional tolerance to actual memory references, e.g., by effectively refreshing at least one list on each invocation.","Lists can be paused through software to avoid loading a sequence of addresses that are known not relevant (e.g., the sequence of addresses are unlikely be re-accessed by a processor unit). For example, data dependent branches such as occur during a table lookup may be carried out while list prefetching is paused.","In one embodiment, prefetching initiated by an address in a list is for a full L2 (Level-two) cache line. In one embodiment, the size of the list may be minimized or optimized by including only a single 64-byte address which lies in a given 128-byte cache line. In this embodiment, this optimization is accomplished, e.g., by comparing each L1 cache miss with previous four L1 cache misses and adding a L1 cache miss address to a list only if it identifies a 128-byte cache line different from those previous four addresses. In this embodiment, in order to enhance a usage of the prefetch data array, a list may identify, in addition to an address of the 128-byte cache line to be prefetched, those 64-byte portions of the 128-byte cache line which corresponded to L1 cache misses. This identification may allow prefetched data to be marked as available for replacement as soon as portions of the prefetched data that will be needed have been hit.","There is provided a system, method and computer program product for prefetching of data or instructions in a plurality of streams while adaptively adjusting prefetching depths of each stream.","Further the adaptation algorithm may constrain that the total depth of all prefetched streams is predetermined and consistent with the available storage resources in a stream prefetch engine.","In one embodiment, a stream prefetch engine (e.g., a stream prefetch engine  in FIG. --) increments a prefetching depth of a stream when a load request for the stream has a corresponding address in a prefetch directory (e.g., a PFD  in FIG. --) but the stream prefetch engine has not received corresponding data from a memory device. Upon incrementing the prefetching depth of the stream, the stream prefetch engine decrements a prefetching depth of a victim stream (e.g., a least recently used stream).","In one embodiment, a parallel computing system operates at least one prefetch algorithm as follows:","Stream prefetching: a plurality of concurrent data or instruction streams (e.g., 16 data streams) of consecutive addresses can be simultaneously prefetched with a support up to a prefetching depth (e.g., eight cache lines can be prefetched per stream) with a fully adaptive depth selection. An adaptive depth selection refers to an ability to change a prefetching depth adaptively. A stream refers to sequential data or instructions. An MPEG (Moving Picture Experts Group) movie file or a MP3 music file is an example of a stream.\n\n","In one embodiment, there are provided rules for adaptively adjusting the prefetching depth. These rules may govern a performance of the stream prefetch engine (e.g., a stream prefetch engine  in FIG. --) when dealing with varying stream counts and avoid pathological thrashing of many streams. A thrashing refers to a computer activity that makes little or no progress because a storage resource (e.g., a prefetch data array  in FIG. --) becomes exhausted or limited to perform operations.","Rule 1: a stream may increase its prefetching depth in response to a prefetch to a demand fetch conversion event that is an indicative of bandwidth starvation. A demand fetch conversion event refers to a hit on a line that has been established in a prefetch directory but not yet had data returned from a switch or a memory device. The prefetch directory is described in detail below in conjunction with FIG. --.","Rule 2: this depth increase is performed at an expense of a victim stream whenever a sum of all prefetching depths equals a maximum capacity of the stream prefetch engine. In one embodiment, the victim stream selected is the least recently used stream with non-zero prefetching depth. In this way, less active or inactive streams may have their depths taken by more competitive hot streams, similar to stale data being evicted from a cache. This selection of a victim stream has at least two consequences: First, that victim's allowed depth is decreased by one. Second, when an additional prefetching is performed for the stream whose depth has been increased, it is possible that all or some prefetch registers may be allocated to active streams including the victim stream since the decrease in the depth of the victim stream does not imply that the actual data footprint of that stream in the prefetch data array may correspondingly shrink. Prefetch registers refer to registers working with the stream prefetch engine. Excess data resident in the prefetch data array for the victim stream may eventually be replaced by new cache lines of more competitive hot streams. This replacement is not necessarily immediate, but may eventually occur.","In one embodiment, there is provided a free depth counter which is non-zero when a sum of all prefetching depths is less than the capacity of the stream prefetch engine. In one embodiment, this counter has value 32 on reset, and per-stream depth registers are reset to zero. These per-stream depth registers store a prefetching depth for each active stream. Thus, the contents of the per-stream depth registers are changed as a prefetching depth of a stream is changed. When a stream is invalidated, its depth is returned to the free depth counter.","FIG. -- illustrates a system diagram of a stream prefetch engine  in one embodiment. The stream prefetch engine  includes, but is not limited to, a first table  called prefetch directory, an array or buffer  called prefetch data array, a queue  call hit queue, a stream detect engine , a prefetch unit , a second table  called DFC (Demand Fetch Conversion) table, a third table  called adaptive control block. These tables ,  and  may be implemented as any data structure including, but is not limited to, an array, buffer, list, queue, vector, etc. The stream prefetch engine  is capable of maintaining a plurality of active streams of varying prefetching depths. An active stream refers to a stream being processed by a processor core. A prefetching depth refers to the number of instructions or an amount of data to be prefetched ahead (e.g., 10 clock cycles before the instructions or data are needed by a processor core). The stream prefetch engine  dynamically adapts prefetching depths of streams being prefetched, e.g., according to method steps illustrated in FIG. --. These method steps in FIG. -- are described in detail below.","The prefetch directory (PFD)  stores tag information (e.g., valid bits) and meta data associated with each cache line stored in the prefetch data array (PDA) . The prefetch data array  stores cache lines (e.g., L2 (Level two) cache lines and\/or L1 (Level one) cache lines) prefetched, e.g., by the stream prefetch unit . In one embodiment, the stream prefetch engine  supports diverse memory latencies and a large number (e.g., 1 million) of active threads run in the parallel computing system. In one embodiment, the stream prefetching makes use of the prefetch data array  which holds up to, for example, 32 128-byte level-two cache lines.","In one embodiment, an entry of the PFD  includes, but is not limited to, an address valid (AVALID) bit(s), a data valid (DVALID) bit, a prefetching depth (DEPTH) of a stream, a stream ID (Identification) of the stream, etc. An address valid bit indicates whether the PFD  has a valid cache line address corresponding to a memory address requested in a load request issued by the processor. A valid cache line address refers to a valid address of a cache line. A load request refers to an instruction to move data from a memory device to a register in a processor. When an address is entered as valid into the PFD , corresponding data may be requested from a memory device but may be not immediately received. The data valid bit indicates whether the stream prefetch engine  has received data corresponding to a AVALID bit from a memory device . In other words, DVALID bit is set to low (\u201c0\u201d) to indicate pending data, i.e., the data that has been requested to the memory device  but has not been received by the prefetch unit . When the prefetch unit  establishes an entry in the prefetch directory  with setting the AVALID bit to high (\u201c1\u201d) to indicate the entry has a valid cache line address corresponding to a memory address requested in a load request, the prefetch unit  may also request corresponding data (e.g., L1 or L2 cache line corresponding to the memory address) from a memory device  (e.g., L1 cache memory device, L2 cache memory device, a main memory device, etc.) and set corresponding DVALID bit to low. When a AVALID bit is set to high and a corresponding DVALID bit is set to low, the prefetch unit  places a corresponding load request associated with these AVALID and DVALID bits in the DFC table  to wait until the corresponding data that is requested by the prefetch unit  comes from the memory device . Once the corresponding data arrives from the memory device , the stream prefetch engine  stores the data in the PDA  and sets the DVALID bit to high in a corresponding entry in the PFD . Then, the load request, for which there exists a valid cache line in the PDA  and a valid cache line address in the PFD , are forwarded to the hit queue , e.g., by the prefetch unit . In other words, once the DVALID bit and the AVALID bit are set to high in an entry in the PFD , a load request associated with the entry is forwarded to the hit queue .","A valid address means that a request for the data for this address has been sent to a memory device, and that the address has not subsequently been invalidated by a cache coherence protocol. Consequently, a load request to that address may either be serviced as an immediate hit, for example, to the PDA  when the data has already been returned by the memory device (DVALID=1), or may be serviced as a demand fetch conversion (i.e., obtaining the data from a memory device) with the load request placed in the DFC table  when the data is still in flight from the memory device (DVALID=0).","Valid data means that an entry in the PDA  corresponding to the valid address in the PFD  is also valid. This entry may be invalid when the data is initially requested from a memory device and may become valid when the data has been returned by the memory device.","In one embodiment, the stream fetch engine  is triggered by hits in the prefetch directory . As a prefetching depth can vary from a stream to another stream, a stream ID field (e.g., 4-bit field) is held in the prefetch directory  for each cache line. This stream ID identifies a stream for which this cache line was prefetched and is used to select an appropriate prefetching depth.","A prefetch address is computed, e.g., by selecting the first cache line within the prefetching depth that is not resident (but is a valid address) in the prefetch directory . A prefetch address is an address of data to be prefetched. As this address is dynamically selected from a current state of the prefetch directory , duplicate entries are avoided, e.g., by comparing this address and addresses that stored in the prefetch directory . Some tolerance to evictions from the prefetch directory  is gained.","An actual data prefetching, e.g., guided by the prefetching depth, is managed as follows: When a stream is detected, e.g., by detecting subsequent cache line misses, a sequence of \u201cN\u201d prefetch requests is issued in \u201cN\u201d or more clock cycles, where \u201cN\u201d is a predetermined integer between 1 and 8. Subsequent hits to this stream (whether or not the data is already present in the prefetch data array ) initiate a single prefetch request, provided that an actual prefetching depth of this stream is less than its allowed depth. Increases in this allowed depth (caused by hits to cache lines being prefetched but not yet resident in the prefetch data array ) can be exploited by this one-hit\/one-prefetch policy because the prefetch line length is twice the L1 cacheline length: two hits will occur to the same prefetch line for sequential accesses. This allows two prefetch lines to be prefetched for every prefetch line consumed and depth can be extended. One-hit\/one-prefetch policy refers to a policy initiating a prefetch of data or instruction in a stream per a hit in that stream.","The prefetch unit  stores in a demand fetch conversion (DFC) table  a load request for which a corresponding cache line has an AVALID bit set to high but a DVALID bit not (yet) set to high. Once a valid cache line returns from the memory device , the prefetch unit  places the load request into the hit queue . In one embodiment, a switch (not shown) provides the data to the prefetch unit  after the switch retrieves the data from the memory device. This (i.e., receiving data from the memory device or the switch and placing the load request in the hit queue ) is known as demand fetch conversion (DFC). The DFC table  is sized to match a total number of outstanding load requests supported by a processor core associated with the stream prefetch engine .","In one embodiment, the demand fetch conversion (DFC) table  includes, but is not limited to, an array of, for example, 16 entries\u00d713 bits representing at least 14 hypothetically possible prefetch to demand fetch conversions. A returning prefetch from the switch is compared against this array. These entries may arbitrate for access to the hit queue, waiting for free clock cycles. These entries wait until the cache line is completely entered before requesting an access to the hit queue.","In one embodiment, the prefetch unit  is tied quite closely to the prefetch directory  on which the prefetch unit  operates and is implemented as part of the prefetch directory . The prefetch unit  generates prefetch addresses for a data or instruction stream prefetch. If a stream ID of a hit in the prefetch directory  indicates a data or instruction stream, the prefetch unit  processes address and data vectors representing \u201chit\u201d, e.g., by following steps - in FIG. --.","When either a hit or DFC occurs, the next \u201cN\u201d cache line addresses may be also matched in the PFD  where \u201cN\u201d is a number described in the DEPTH field of a cache line that matched with the memory address. A hit refers to finding a match between a memory address requested in a load request and a valid cache line address in the PFD . If a cache line within the prefetching depth of a stream is not present in the PDA , the prefetch unit  prefetches the cache line from a cache memory device (e.g., a cache memory ). Before prefetching the cache line, the prefetch unit  may establish a corresponding cache line address in the PFD  with AVALID bit set to high. Then, the prefetch unit  requests data load from the cache memory device . Data load refers to reading the cache line from the cache memory device . When prefetching the cache line, the prefetch unit  assigns to the prefetched cache line a same stream ID which is inherited from a cache line whose address was hit. The prefetch unit  looks up a current prefetching depth of that stream ID in the adaptive control block  and inserts this prefetching depth in a corresponding entry in the PFD  which is associated with the prefetched cache line. The adaptive control block  is described in detail below.","The stream detect engine  memorizes a plurality of memory addresses that caused cache misses before. In one embodiment, the stream detect engine  memories the latest sixteen memory addresses that causes load misses. Load misses refer to cache misses caused by load requests. If a load request demands an access to a memory address which resides in a next cache line of a cache line that caused a prior cache miss, the stream detect engine  detects a new stream and establishes a stream. Establishing a stream refers to prefetching data or instruction in the stream according to a prefetching depth of the stream. Prefetching data or instructions in a stream according to a prefetch depth refers to fetching a certain number of instructions or a certain amount of data in the stream within the prefetching data before they are needed. For example, if the stream detect engine  is informed a load from \u201cM1\u201d memory address is a missed address, it will memorise the corresponding cacheline \u201cC1\u201d. Later, if a processor core issues a load request reading data in \u201cM1+N\u201d memory address and \u201cM1+N\u201d address corresponds to a cache line \u201cC1+1\u201d which is subsequent to the cache line \u201cC1\u201d, the stream detect engine  detects a stream which includes the cache line \u201cC1\u201d, the cache line \u201cC1+1\u201d, a cache line \u201cC1+2\u201d, etc. Then, the prefetch unit  fetches \u201cC1+1\u201d and prefetches subsequent cache lines (e.g., the cache line \u201cC1+2\u201d, a cache line \u201cC1+3,\u201d etc.) of the stream detected by the stream detect engine  according to a prefetching depth of the stream. In one embodiment, the stream detect engine establishes a new stream whenever a load miss occurs. The number of cache lines established in the PFD  by the stream detect engine  is programmable.","In one embodiment, the stream prefetch engine  operates three modes where a stream is initiated on each of the following events:\n\n","Each of these modes can be enabled\/disabled independently via MMIO registers. The optimistic mode and DCBT instruction share hardware logic (not shown) with the stream detect engine . In order for a use of the DCBT instruction, which is only effective to a L2 cache memory device and does not unnecessarily fill a load queue (i.e., a queue storing load requests) in a processor core, the stream prefetch engine  may trigger an immediate return of dummy data allowing the DCBT instruction to be retired without incurring latency associated with a normal extraction of data from a cache memory device as this DCBT instruction only affects a L2 cache memory operation and the data may not be held in a L1 cache memory device by the processor core. A load queue refers to a queue for storing load requests.","In one embodiment, the stream detect engine  is performed by comparing all cache misses to a table of at least 16 expected 128-byte cache line addresses. A hit in this table triggers a number n of cache lines to be established in the prefetch directory  on the following n clock cycles. A miss in this table causes a new entry to be established with a round-robin victim selection (i.e., selecting a cache line to be replaced in the table with a round-robin fashion).","In one embodiment, a prefetching depth does not represent an allocation of prefetched cache lines to a stream. The stream prefetch engine  allows elasticity (i.e., flexibility within certain limits) that can cause this depth to differ (e.g., by up to 8) between streams. For example, when a processor core  aggressively issues load requests, the processor core can catch up with a stream, e.g., by hitting prefetched cache lines whose data has not yet been returned by the switch. These prefetch-to-demand fetch conversion cases may be treated as normal hits by the stream detect engine  and additional cache lines are established and fetched. A prefetch-to-demand fetch conversion case refers to a case in which a hit on a line that has been established in the prefetch directory  but not yet had data returned from a switch or a memory device. Thus, the number of prefetch lines used by a stream in the prefetch directory  can exceed the prefetching depth of a stream. However, the stream prefetch engine  will have the number of cache lines for each stream equal to that stream's prefetching depth once all pending requests are satisfied and the elasticity removed.","The adaptive control block  includes at least two data structures: 1. Depth table storing a prefetching depth of each stream which are registered in the PFD  with its stream ID; 2. LRU (Least Recently Used) table identifying the least recently used streams among the registered streams, e.g., by employing a known LRU replacement algorithm. The known LRU replacement algorithm may update the LRU table whenever a hit in an entry in the PFD  and\/or DFC (Demand Fetch Conversion) occurs. In one embodiment, when a DFC occurs, the stream prefetch engine  increments a prefetching depth of a stream associated with the DFC.","This increment allows a deep prefetch (e.g., prefetching data or instructions in a stream according to a prefetching depth of 8) to occur when only one or two streams are being prefetched, e.g., according to a prefetching depth of up to 8. Prefetching data or instructions according to a prefetching depth of a stream refers to fetching data or instructions in the stream within the prefetching depth ahead. For example, if a prefetching depth of a stream which comprises data stored in \u201cK\u201d cache line address, \u201cK+1\u201d cache line address, \u201cK+2\u201d cache line address, . . . , and \u201cK+1000\u201d cache line address is a depth of 2 and the stream detect engine  detects this stream when a processor core requests data in \u201cK1+1\u201d cache line address, then the stream prefetch engine  fetches data stored in \u201cK+1\u201d cache line address and \u201cK1+2\u201d cache line address. In one embodiment, an increment of a prefetching depth is only made in response to an indicator that loads from a memory device for this stream are exceeding the rate enabled by a current prefetching depth of the stream. For example, although the stream prefetch engine  prefetches data or instructions, the stream may face demand fetch conversions because the stream prefetch engine  fails to prefetch enough data or instructions ahead. Then, the stream prefetch engine  increases the prefetching depth of the stream to fetch data or instruction further ahead for the stream. A load refers to reading data and\/or instructions from a memory device. However, by only doing this increase in response to an indicator of data starvation, the stream prefetch engine  avoids unnecessary deep prefetch. For example, when only hits (e.g., a match between an address in a current load request and an address in the PFD ) are taken, a prefetching depth of a stream associated with the current cache miss address is not increased. Unless PFD  has a AVALID bit set to high and a corresponding DVALID bit set to low, the prefetch unit  may not increase a prefetching depth of a corresponding stream. Because depth is stolen in competition with other active streams, the stream prefetch engine  can also automatically adapt to optimally support concurrent data or instruction streams (e.g., 16 concurrent streams) with a small storage capability (e.g., a storage capacity storing only 32 cache lines) and a shallow prefetching depth (e.g., a depth of 2) for each stream.","As a capacity of the PDA  is limited, it is essential that active streams do not try to exceed the capacity (e.g., 32 L2 cache lines) of the PDA  to prevent thrashing and substantial performance degradation. This capacity of the PDA  is also called a capacity of the stream prefetch engine . The stream prefetch engine adaptation algorithm  constrains a total depth of all streams across all the streams to remain as a predetermined value.","When incrementing a prefetching depth of a stream, the stream prefetch engine  decrements a prefetching depth of a victim stream. A victim stream refers to a stream which is least recently used and has non-zero prefetching depth. Whenever a current active stream needs to acquire one more unit of its prefetching depth (e.g., a depth of 1), the victim stream releases one unit of its prefetching depth, thus ensuring the constraint is satisfied by forcing streams to compete for their prefetching depth increments. The constraint includes, but is not limited to: fixing a total depth of all streams.","In one embodiment, there is provided a victim queue (not shown) implemented, e.g., by a collection of registers. When a stream of a given stream ID is hit, that stream ID is inserted at a head of the victim queue and a matching entry is eliminated from the victim queue. The victim queue may list streams, e.g., by a reverse time order of an activity. A tail of this victim queue may thus include the least recently used stream. A stream ID may be used when a stream is detected and a new stream reinserted in the prefetch directory . Stale data is removed from the prefetch directory  and corresponding cache lines are freed.","The stream prefetch engine  may identify the least recently used stream with a non-zero depth as a victim stream for decrementing a depth. An empty bit in addition to stream-ID is maintained in a LRU (Least Recently Used) queue (e.g., 16\u00d75 bit register array). The empty bit is set to 0 when a stream ID is hit and placed at a head of the queue. If decrementing a prefetching depth of a victim stream results in a prefetching depth of the victim stream becoming zero, the empty bit of the victim stream is set to 1. A stream ID of a decremented-to-zero-depth stream is distributed to the victim queue. One or more comparator(s) matches this stream ID and sets the empty bit appropriately. A decremented-to-zero-depth stream refers to a stream whose depth is decremented to zero.","In one embodiment, a free depth register is provided for storing depths of invalidated streams. This register stores a sum of all depth allocations matching the capacity of the prefetch data array , ensuring a correct book keeping.","In one embodiment, the stream prefetch engine  may require elapsing a programmable number of clock cycles between adaptation events (e.g., the increment and\/or the decrement) to rate control such adaptation events. For example, this elapsing gives a tunable rate control over the adaptation events.","In one embodiment, the Depth table does not represent an allocation of a space for each stream in the PDA . As the prefetch unit  changes a prefetching depth of a stream, a current prefetching depth of the stream may not immediately reflect this change. Rather, if the prefetch unit  recently increased a prefetching depth of a stream, the PFD  may reflect this increase after the PFD  receives a request for this increase and prefetched data of the stream is grown. Similarly, if the prefetch unit  decreases a prefetching depth of a stream, the PFD  may include too much data (i.e., data beyond the prefetching depth) for that stream. Then, when a processor core issues subsequent load requests for this stream, the prefetch unit  may not trigger further prefetches and at a later time an amount of the prefetched data may represent a shrunk depth. In one embodiment, the Depth table includes a prefetching depth for each stream. An additional counter is implemented as the free depth register for spare prefetching depth. This free depth register can semantically be thought of as a dummy stream and is essentially treated as a preferred victim for purposes of depth stealing. In one embodiment, invalidated stream IDs return their depths to this free depth register. This return may require a full adder to be implemented in the free depth register.","If a look-up address hits in the prefetch directory , a prefetch is generated for the lowest address that is within a prefetching depth of a stream ID associated with the look-up address and which misses, for example, an eight-bit lookahead vector over the next 8 cache line addresses identifying which of these are already present in PFD . A look-up address refers to an address associated with a request or command. A condition called underflow occurs when the look-up address is present with a valid address (and hence has been requested from a memory device) but corresponding data has not yet become valid. This underflow condition triggers a hit stream to increment its depth and decrement a depth of a current depth of a victim stream. A hit stream refers to a stream whose address is found in the prefetch directory . As multiple hits can occur for each prefetched cache line, depths of hit streams can grow dynamically. The stream prefetch engine  keeps a capacity of foot prints of all or some streams fixed, avoiding many pathological performance conditions that the dynamic growing could introduce. In one embodiment, the stream prefetch engine  performs a less aggressive prefetch, e.g., by stealing depths from less active streams.","Due to outstanding load requests issued from a processor core, there is elasticity between issued requests, and those queued, pending or returned. Thus, even with the algorithm described above, a capacity of the stream prefetch engine  can be exceeded by additional 4, 6 or 12 requests. The prefetching depths may be viewed as a \u201cdrive to\u201d target depths whose sum is constrained not to exceed the capacity of a cache memory device when the processor core has no outstanding loads tying up slots of the cache memory. While the PFD  does not immediately or automatically include precisely the number of cache lines for each stream corresponding to the depth of each stream, the stream prefetch engine  makes its decisions about when to prefetch to try to get closer to a prefetching depth (drives towards it).","FIG. -- illustrates a flow chart illustrating method steps performed by a stream prefetch engine (e.g., a stream prefetch engine  in FIG. --) in a parallel computing system in one embodiment. A stream prefetch engine refers to a hardware or software module for performing fetching of data in a plurality of streams before the data is needed. The parallel computing system includes a plurality of computing nodes. A computing node includes at least one processor and at least one memory device. At step , a processor issues a load request (e.g., a load instruction). The stream prefetch engine  receives the issued load request. At step , the stream prefetch engine searches the PFD  to find a cache line address corresponding to a first memory address in the issued load request. In one embodiment, the PFD  stores a plurality of memory addresses whose data have been prefetched, or requested to be prefetched, by the stream prefetch engine . In this embodiment, the stream prefetch engine  evaluates whether the first address in the issued load request is present and valid in the PFD . To determine whether a memory address in the PFD  is valid or not, the stream prefetch engine  may check an address valid bit of that memory address.","If the first memory address is present and valid in the PFD  or there is a valid cache line address corresponding to the first memory address in the PFD , at step , the stream prefetch engine  evaluates whether there exists valid data (e.g., valid L2 cache line) corresponding to the first memory address in the PDA . In other words, if there is a valid cache line address corresponding to the first memory address in the PFD , the stream prefetch engine  evaluates whether the corresponding data is valid yet. If the data is not valid, then the corresponding data is pending, i.e., corresponding data is requested to the memory device  but has not been received by the stream prefetch engine . At step , if the first memory address is not present or not valid in the PFD , the control goes to step . At step , to evaluate whether there already exists the valid data in the PDA , the stream prefetch engine  may check a data valid bit associated with the first memory address or the valid cache line address in the PFD .","If there is no valid data corresponding to the first memory address in the PDA , at step , the stream prefetch engine  inserts the issued load request to the DFC table  and awaits a return of the data from the memory device . Then, the control goes to step . In other words, if the data is pending, at step , the stream prefetch engine  inserts the issued load request to the DFC table , the stream prefetch engine  awaits the data to be returned by the memory device (since the address was valid, the data has already been requested but not returned) and the control goes to step . Otherwise, the control goes to step . At step , the stream prefetch engine  increments a prefetching depth of a first stream that the first memory address belongs to. While incrementing the prefetching depth of the first stream, at step , the stream prefetch engine  determines a victim stream among streams registered in the PFD  and decrements a prefetching depth of the victim stream. The registered streams refers to streams whose stream IDs are stored in the PFD . To determine the victim stream, the stream prefetch engine  searches the least recently used stream having non-zero prefetching depth among the registered streams. The stream prefetch engine  sets the least recently used stream having non-zero prefetching depth as the victim stream in a purpose of a reallocation of a prefetching depth of the victim stream.","In one embodiment, a total prefetching depth of the registered streams is a predetermined value. The parallel computing system operating the stream prefetch engine  can change or program the predetermined value representing the total prefetching depth.","Returning to FIG. --, at step , the stream prefetch engine  evaluates whether prefetching of additional data (e.g., subsequent cache lines) is needed for the first stream. For example, the stream prefetch engine  perform parallel address comparisons to check whether all memory addresses or cache line addresses within a prefetching depth of the first stream are present in the PFD . If all the memory addresses or cache line addresses within the prefetching depth of the first stream are present, i.e., all the cache line addresses within the prefetching depth of the first stream are present and valid in the PFD , then the control goes to step . Otherwise, the control goes to step .","At step , the stream prefetch engine  prefetches the additional data. Upon determining that prefetching of additional data is necessary, the stream prefetch engine  may select the nearest address to the first address that is not present but is a valid address in the PFD  within a prefetching depth of a stream corresponding to the first address and starts to prefetch data from the nearest address. The stream prefetch engine  may also prefetch subsequent data stored in subsequent addresses of the nearest address. The stream prefetch engine  may fetch at least one cache line corresponding to a second memory address (i.e., a memory address or cache line address not being present in the PFD ) within the prefetching depth of the first stream. Then, the control goes to step .","At step , the stream prefetch engine  attempts to detect a stream (e.g., the first stream that the first memory address belongs to). In one embodiment, the stream prefetch engine  stores a plurality of third memory addresses that caused load misses before. A load miss refers to a cache miss caused by a load request. The stream prefetch engine  increments the third memory addresses. The stream prefetch engine  compares the incremented third memory addresses and the first memory address. The stream prefetch engine  identifies the first stream if there is a match between an incremented third memory address and the first memory address.","If the stream prefetch engine  succeeds to detect a stream (e.g., the first stream), at step , the stream prefetch engine  starts to prefetch data and\/or instructions in the stream (e.g., the first stream) according to a prefetching depth of the stream. Otherwise, the control goes to step . At step , the stream prefetch engine  returns prefetched data and\/or instructions to a processor core. The stream prefetch engine  stores the prefetched data and\/or instructions, e.g., in PDA , before returning the prefetched data and\/or instructions to the processor core. At step , the stream prefetch engine  inserts the issued load request to the DFC table . At step , the stream prefetch engine receives a new load request issued from a processor core.","In one embodiment, the stream prefetch engine  adaptively changes prefetching depths of streams. In a further embodiment, the stream prefetch engine  sets a minimum prefetching depth (e.g., a depth of zero) and\/or a maximum prefetching depth (e.g., a depth of eight) that a stream can have. The stream prefetch engine  increments a prefetching depth of a stream associated with a load request when a memory address in the load request is valid (e.g., its address valid bit has been set to high in the PFD ) but data (e.g., L2 cache line stored in the PDA ) corresponding to the memory address is not yet valid (e.g., its data valid bit is still set to low (\u201c0\u201d) in the PFD ). In other words, the stream prefetch engine  increments the prefetching depth of the stream associated with the load request when there is no valid cache line data present in the PDA  corresponding to the valid memory address in the PFD (due to the data being in flight from the cache memory). To increment the prefetching depth of the stream, the stream prefetch engine  decrements a prefetching depth of the least recently used stream having non-zero prefetching depth. For example, the stream prefetch engine  first attempts to decrement a prefetching depth of the least recently used stream. If the least recently used stream already has zero prefetching depth (i.e., a depth of zero), the stream prefetch engine  attempts to decrement a prefetching depth of a second least recently used stream, and so on. In one embodiment, as described above, the adaptive control block  includes the LRU table that traces least recently used streams according to hits on streams.","In one embodiment, the stream prefetch engine  may be implemented in hardware or reconfigurable hardware, e.g., FPGA (Field Programmable Gate Array) or CPLD (Complex Programmable Logic deviceDevice), using a hardware description language (Verilog, VHDL, Handel-C, or System C). In another embodiment, the stream prefetch engine  may be implemented in a semiconductor chip, e.g., ASIC (Application-Specific Integrated Circuit), using a semi-custom design methodology, i.e., designing a chip using standard cells and a hardware description language. In one embodiment, the stream prefetch engine  may be implemented in a processor (e.g., IBM\u00ae PowerPC\u00ae processor, etc.) as a hardware unit(s). In another embodiment, the stream prefetch engine  may be implemented in software (e.g., a compiler or operating system), e.g., by a programming language (e.g., Java\u00ae, C\/C++, .Net, Assembly language(s), Pearl, etc.).","In one embodiment, the stream prefetch engine  operates with at least four threads per processor core and a maximum prefetching depth of eight (e.g., eight L2 (level two) cache lines). In one embodiment, the prefetch data array  may store 128 cache lines. In this embodiment, the prefetch data array stores 32 cache lines and, by adapting the prefetching depth according to a system load, the stream prefetch engine  can support the same dynamic range of memory accesses. By adaptively changing the capacity of the PDA , the prefetch data array  whose capacity is 32 cache lines can also operate as an array with 128 cache lines.","In one embodiment, an adaptive prefetching is necessary to both support efficient low stream count (e.g., a single stream) and efficient high stream count (e.g., 16 streams) prefetching with the stream prefetch engine . An adaptive prefetching is a technique adaptively adjusting prefetching depth per a stream as described in the steps - in FIG. --.","In one embodiment, the stream prefetch engine  counts the number of active streams and then divides the PFD  and\/or the PDA  equally among these active streams. These active streams may have an equal prefetching depth.","In one embodiment, a total depth of all active streams is predetermined and not exceeding a PDA capacity of the stream prefetch engine  to avoid thrashing. An adaptive variation of a prefetching depth allows a deep prefetch (i.e., a depth of eight) for low numbers of streams (i.e., two streams), while a shallow prefetch (i.e., a depth of 2) is used for large numbers of streams (i.e., 16 streams) to maintain the usage of PDA  optimal under a wide variety of load requests.","There is provided a system, method and computer program product for improving a performance of a parallel computing system, e.g., by operating at least two different prefetch engines associated with a processor core.","FIG. -- illustrates a flow chart for responding to commands issued by a processor when prefetched data may be available because of an operation of one or more different prefetch engines in one embodiment. A parallel computing system may include a plurality of computing nodes. A computing node may include, without limitation, at least one processor and\/or at least one memory device. At step , a processor (e.g., IBM\u00ae PowerPC\u00ae, A2 core  in FIG. --, etc.) in a computing node in the parallel computing system issues a command. A command includes, without limitation, an instruction (e.g., Load from and\/or Store to a memory device, etc.) and\/or a prefetching request (i.e., a request for prefetching of data or instruction(s) from a memory device). A command also refers to a request, vice versa. A command and a request are interchangeably used in this disclosure. A command or request includes, without limitation, instruction codes, addresses, pointers, bits, flags, etc.","At step , a look-up engine (e.g., a look-up engine  in FIG. --) evaluates whether a prefetch request has been issued for first data (e.g., numerical data, string data, instructions, etc.) associated with the command. The prefetch request (i.e., a request for prefetching data) may be issued by a prefetch engine (e.g., a stream prefetch engine  or a list prefetch engine  in FIG. --). In one embodiment, to make the determination, the look-up engine compares a first address in the command and second addresses for which prefetch requests have been issued or that have been prefetched. Thus, the look-up engine may include at least one comparator. The parallel computing system may further include an array or table (e.g., a prefetch directory  in FIG. --) for storing the addresses for which prefetch requests have been previously issued by the one or more simultaneously operating prefetch engines. The stream prefetch engine  and the list prefetch engine  are described in detail below.","At step , if the look-up engine determines that a prefetch request has not been issued for the first data, e.g., the first data address is not found in the prefetch directory , at step , then a normal load command is issued to a memory system.","At step , if the look-up engine determines that a prefetch request has been issued for the first data, then the look-up engine determines whether the first data is present in a prefetch data array (e.g., a prefetch data array  in FIG. --), e.g., by examining a data present bit (e.g., a bit indicating whether data is present in the prefetch data array) in step . If the first data has already been prefetched and is resident in the prefetch data array, at step , then the first data is passed directly to the processor, e.g., by a prefetch system  in FIG. --. If the first data has not yet been received and is not yet in the prefetch data array, at step , then the prefetch request is converted to a demand load command (i.e., a command requesting data from a memory system) so that when the first data is returned from the memory system it may be transferred directly to the processor rather than being stored in the prefetch data array awaiting a later processor request for that data.","The look-up engine also provides the command including an address of the first data to two at least two different prefetch engines simultaneously. These two different prefetch engines include, without limitation, at least one stream prefetch engine (e.g., a stream prefetch engine  in FIG. --) and one or more list prefetch engine, e.g., at least four list prefetch engines (e.g., a list prefetch engine  in FIG. --). A stream prefetch engine uses the first data address to initiate a possible prefetch command for second data (e.g., numerical data, string data, instructions, etc.) associated with the command. For example, the stream prefetch engine fetches ahead (e.g., 10 clock cycles before when data or an instruction is expected to be needed) one or more 128 byte L2 cache lines of data and\/or instruction according to a prefetching depth. A prefetching depth refers to a specific amount of data or a specific number of instructions to be prefetched in a data or instruction stream.","In one embodiment, the stream prefetch engine adaptively changes the prefetching depth according to a speed of each stream. For example, if a speed of a data or instruction stream is faster than speeds of other data or instruction streams (i.e., that faster stream includes data which is requested by the processor but is not yet resident in the prefetch data directory), the stream prefetch engine runs the step  to convert a prefetch request for the faster stream to a demand load command described above. The stream prefetch engine increases a prefetching depth of the fastest data or instruction stream. In one embodiment, there is provided a register array for specifying a prefetching depth of each stream. This register array is preloaded by software at the start of running the prefetch system (e.g., the prefetch system  in FIG. --) and then the contents of this register array vary as faster and slower streams are identified. For example, if a first data stream includes an address which is requested by a processor and corresponding data is found to be resident in the prefetch data array and a second data stream includes an address for which prefetched data which has not yet arrived in the prefetch data array. The stream prefetch engines reduces a prefetching depth of the first stream, e.g., by decrementing a prefetching depth of a first stream in the register array. The stream prefetch engine increases a prefetching depth of the second stream, e.g., by incrementing a prefetching depth of a second stream in the register array. If a speed of a data or instruction stream is slower than speeds of other data or instruction streams, the stream prefetch engine decreases a prefetching depth of the slowest data or instruction stream. In another embodiment, the stream prefetch engine increases a prefetching depth of a stream when the command has a valid address of a cache line but there is no valid data corresponding to the cache line. To increase a prefetching depth of a stream, the stream prefetch engine steals and decreases a prefetching depth of a least recently used stream having a non-zero prefetching depth. In one embodiment, the stream prefetch engine prefetches at least sixteen data or instruction streams. In another embodiment, the stream prefetch engine prefetches at most or at least sixteen data or instruction streams. A detail of the stream prefetch engines is described in connection with FIGS. -- and --. In an embodiment described in FIG. --, the stream prefetch engine prefetches second data associated with the command according to a prefetching depth. For example, when a prefetching depth of a stream is set to two, a cache line miss occurs at a cache line address \u201cL1\u201d and another cache line miss subsequently occurs at a cache line address \u201cL1+1,\u201d the stream prefetch engine prefetch cache lines addressed at \u201cL1+2\u201d and \u201cL1+3.\u201d","The list prefetch engine(s) prefetch(es) third data associated with the command. In one embodiment, the list prefetch engine(s) prefetch(es) the third data (e.g., numerical data, string data, instructions, etc.) according to a list describing a sequence of addresses that caused cache misses. The list prefetch engine(s) prefetches data or instruction(s) in a list associated with the command. In one embodiment, there is provided a module for matching between a command and a list. A match would be found if an address requested in the command and an address listed in the list are same. If there is a match, the list prefetch engine(s) prefetches data or instruction(s) in the list up to a predetermined depth ahead of where the match has been found. A detail of the list prefetch engine(s) is described in described in connection with FIGS. -- and --.","The third data prefetched by the list prefetch engine or the second data prefetched by the stream prefetch engine may include data that may subsequently be requested by the processor. In other words, even if one of the engines (the stream prefetch engine and the list prefetch engine) fails to prefetch this subsequent data, the other engine succeeds to prefetch this subsequent data based on the first data that both prefetch engines use to initiate further data prefetches. This is possible because the stream prefetch engine is optimized for data located in consecutive memory locations (e.g., streaming movie) and the list prefetch engine is optimized for a block of randomly located data that is repetitively accessed (e.g., loop). The second data and the third data may include different set of data and\/or instruction(s).","In one embodiment, the second data and the third data are stored in an array or buffer without a distinction. In other words, data prefetched by the stream prefetch engine and data prefetched by the list prefetch engine are stored together without a distinction (e.g., a tag, a flag, a label, etc.) in an array or buffer.","In one embodiment, each of the list prefetch engine(s) and the stream prefetch engine(s) can be turned off and\/or turned on separately. In one embodiment, the stream prefetch engine(s) and\/or list prefetch engine(s) prefetch data and\/or instruction(s) that have not been prefetched before and\/or have not listed in the prefetch directory .","In one embodiment, the parallel computing system operates the list prefetch engine occasionally (e.g., when a user bit(s) are set). A user bit(s) identify a viable address to be used, e.g., by a list prefetch engine. The parallel computing system operates the stream prefetch engine all the time.","In one embodiment, if the look-up engine determines that the first data has not been prefetched, at step , the parallel computing system immediately issues the load command for this first data to a memory system. However, it also provides an address of this first data to the stream prefetch engine and\/or at least one list prefetch engine which use this address to determine further data to be prefetched. The prefetched data may be consumed by the processor core  in subsequent clock cycles. A method to determine and\/or identify whether the further data needs to be prefetched is described herein above. Upon determining and\/or identifying the further data to be prefetched, the stream prefetch engine may establish a new stream and prefetch data in the new stream or prefetch additional data in an existing stream. At the same time, upon determining and\/or identifying the further data to be prefetched, the list prefetch engine may recognize a match between the address of this first data and an earlier L1 cache miss address (i.e., an address caused a prior L1 cache miss) in a list and prefetch data from the subsequent cache miss addresses in the list separated by a predetermined \u201clist prefetch depth\u201d, e.g., a particular number of instructions and\/or a particular amount of data to be prefetched by the list prefetch engine.","A parallel computing system which has at least one stream and at least one list prefetch engine may run more efficiently if both types of prefetch engines are provided. In one embodiment, the parallel computing system allows these two different prefetch engines (i.e., list prefetch engines and stream prefetch engines) to run simultaneously without serious interference. The parallel computing system can operate the list prefetch engine, which may require a user intervention, without spoiling benefits for the stream prefetch engine.","In one embodiment, the stream prefetch engine  and\/or the list prefetch engine  is implemented in hardware or reconfigurable hardware, e.g., FPGA (Field Programmable Gate Array) or CPLD (Complex Programmable Logic deviceDevice), using a hardware description language (Verilog, VHDL, Handel-C, or System C). In another embodiment, the stream prefetch engine  and\/or the list prefetch engine  is implemented in a semiconductor chip, e.g., ASIC (Application-Specific Integrated Circuit), using a semi-custom design methodology, i.e., designing a chip using standard cells and a hardware description language. In one embodiment, the stream prefetch engine  and\/or the list prefetch engine  is implemented in a processor (e.g., IBM\u00ae PowerPC\u00ae processor, etc.) as a hardware unit(s). In another embodiment, the stream prefetch engine  and\/or the list prefetch engine  is\/are implemented in software (e.g., a compiler or operating system), e.g., by a programming language (e.g., Java\u00ae, C\/C++, .Net, Assembly language(s), Pearl, etc.). When the stream prefetch engine  is implemented in a compiler, the compiler adapts the prefetching depth of each data or instruction stream.","FIG. -- illustrates a system diagram of a prefetch system for improving performance of a parallel computing system in one embodiment. The prefetch system  includes, but is not limited to: a plurality of processor cores (e.g., A2 core , IBM\u00ae PowerPC\u00ae), at least one boundary register (e.g., a latch ), a bypass engine , a request array , a look-up queue , at least two write-combine buffers (e.g., a write-combine buffers  and ), a store data array , a prefetch directory , a look-up engine , a multiplexer , an address compare engine , a stream prefetch engine , a list prefetch engine , a multiplexer , a stream detect engine , a fetch conversion engine , a hit queue , a prefetch data array , a switch request table , a switch response handler , a switch , at least one local control register , a multiplexer , an interface logic .","The prefetch system  is a module that provides an interface between the processor core  and the rest of the parallel computing system. Specifically, the prefetch system  provides an interface to the switch  and an interface to a computing node's DCR (Device Control Ring) and local control registers special to the prefetch system . The system  performs performance critical tasks including, without limitations, identifying and prefetching memory access patterns, managing a cache memory device for data resulting from this identifying and prefetching. In addition, the system  performs write combining (e.g., combining four or more write commands into a single write command) to enable multiple writes to be presented as a single write to the switch , while maintaining coherency between the write combine arrays.","The processor core  issue at least one command including, without limitation, an instruction requesting data. The at least one register  buffers the issued command, at least one address in the command and\/or the data in the commands. The bypass engine  allows a command to bypass the look-up queue  when the look-up queue  is empty.","The look-up queue  receives the commands from the register  and also outputs the earliest issued command among the issued commands to one or more of: the request array , the stream detect engine , the switch request table  and the hit queue . In one embodiment, the queue  is implemented in as a FIFO (First In First Out) queue. The request array  receives at least one address from the register  associated with the command. In one embodiment, the addresses in the request array  are indexed to the corresponding command in the look-up queue . The look-up engine  receives the ordered commands from the bypass engine  or the request array  and compares an address in the issued commands with addresses in the prefetch directory . The prefetch directory  stores addresses of data and\/or instructions for which prefetch commands have been issued by one of the prefetch engines (e.g., a stream prefetch  and a list prefetch engine ).","The address compare engine  receives addresses that have been prefetched from the at least one prefetch engine (e.g., the stream prefetch engine  and\/or the list prefetch engine ) and prevents the same data from being prefetched twice by the at least one prefetch engine. The address compare engine  allows a processor core to request data not present in the prefetch directory . The stream detect engine  receives address(es) in the issued commands from the look-up engine  and detects at least one stream to be used in the stream prefetch engine . For example, if the addresses in the issued commands are \u201cL1\u201d and \u201cL1+1,\u201d the stream prefetch engine may prefetch cache lines addressed at \u201cL1+2\u201d and \u201cL1+3.\u201d","In one embodiment, the stream detect engine  stores at least one address that caused a cache miss. The stream detect engine  detects a stream, e.g., by incrementing the stored address and comparing the incremented address with an address in the issued command. In one embodiment, the stream detect engine  can detect at least sixteen streams. In another embodiment, the stream detect engine can detect at most sixteen streams. The stream detect engine  provides detected stream(s) to the stream prefetch engine . The stream prefetch engine  issues a request for prefetching data and\/instructions in the detected stream according to a prefetching depth of the detected stream.","The list prefetch engine  issues a request for prefetching data and\/or instruction(s) in a list that includes a sequence of address that caused cache misses. The multiplexer  forwards the prefetch request issued by the list prefetch engine  or the prefetch request issued by the stream prefetch engine  to the switch request table . The multiplexer  forwards the prefetch request issued by the list prefetch engine  or the prefetch request issued by the stream prefetch engine  to the prefetch directory . A prefetch request may include memory address(es) where data and\/or instruction(s) are prefetched. The prefetch directory  stores the prefetch request(s) and\/or the memory address(es).","The switch request table  receives the commands from the look-up queue  and the forwarded prefetch request from the multiplexer . The switch request table  stores the commands and\/or the forwarded request. The switch  retrieves the commands and\/or the forwarded request from the table , and transmits data and\/instructions demanded in the commands and\/or the forwarded request to the switch response handler . Upon receiving the data and\/or instruction(s) from the switch , the switch response handler  immediately delivers the data to the processor core , e.g., via the multiplexer  and the interface logic . At the same time, if the returned data or instruction(s) is the result of a prefetch request the switch response handler  delivers the data or instruction(s) from the switch  to the prefetch conversion engine  and delivers the data and\/or instruction(s) to the prefetch data array .","The prefetch conversion engine  receives the commands from the look-up queue  and\/or information bits accompanying data or instructions returned from the switch response handler . The conversion engine  converts prefetch requests to demand fetch commands if the processor requests data that were the target of a prefetch request issued earlier by one of the prefetch units but has not yet been fulfilled. The conversion engine  will then identify this prefetch request when it returns from the switch  through the switch response handler  as a command that was converted from a prefetch request to a demand load command. This returning prefetch data from the switch response handler  is then routed to the hit queue  so that it is quickly passed through the prefetch data array  on the processor core . The hit queue  may also receive the earliest command (i.e., the earliest issued command by the processor core ) from the look-up queue  if that command requests data that is already present in the prefetch data array . In one embodiment, when issuing a command, the processor core  attaches generation bits (i.e., bits representing a generation or age of a command) to the command. Values of the generation bits may increase as the number of commands issued increases. For example, the first issued command may have \u201c0\u201d in the generation bits. The second issued command may be \u201c1\u201d in the generation bits. The hit queue  outputs instructions and\/or data that have been prefetched to the prefetch data array .","The prefetch data array  stores the instructions and\/or data that have been prefetched. In one embodiment, the prefetch data array  is a buffer between the processor core  and a local cache memory device (not shown) and stores data and\/or instructions prefetched by the stream prefetch engine  and\/or list prefetch engine . The switch  may be an interface between the local cache memory device and the prefetch system .","In one embodiment, the prefetch system  combines multiple candidate writing commands into, for example, four writing commands when there is no conflict between the four writing commands. For example, the prefetch system  combines multiple \u201cstore\u201d instructions, which could be instructions to various individual bytes in the same 32 byte word, into a single store instruction for that 32 byte word. Then, the prefetch system  stores these coalesced single writing commands to at least two arrays called write-combine buffers  and . These at least two write-combine buffers are synchronized with each other. In one embodiment, a first write-combine buffer  called write-combine candidate match array may store candidate writing commands that can be combined or concatenated immediately as they are issued by the processor core . The first write-combine buffer  receives these candidate writing commands from the register . A second write-combine buffer  called write-combine buffer flush receives candidate writing commands that can be combined from the bypass engine  and\/or the request array  and\/or stores the single writing commands that combine a plurality of writing commands when these (uncombined) writing commands reach the tail of the look-up queue . When these write-combine arrays become full or need to be flushed to make the contents of a memory system be up-to-date, these candidate writing commands and\/or single writing commands are stored in an array  called store data array. In one embodiment, the array  may also store the data from the register  that is associated with these single writing commands.","The switch  can retrieve the candidate writing commands and\/or single writing commands from the array . The prefetch system  also transfers the candidate writing commands and\/or single writing commands from the array  to local control registers  or a device command ring (DCR), i.e., a register storing control or status information of the processor core. The local control register  controls a variety of functions being performed by the prefetch system . This local control register  as well as the DCR can also be read by the processor core  with the returned read data entering the multiplexer . The multiplexer  receives, as inputs, control bits from the local control register , the data and\/or instructions from the switch response handler  and\/or the prefetched data and\/or instructions from the prefetch data array . Then, the multiplexer  forwards one of the inputs to the interface logic . The interface logic  delivers the forwarded input to the processor core . All of the control bits as well as I\/O commands (i.e., an instruction for performing input\/output operations between a processor and peripheral devices) are memory mapped and can be accessed either using memory load and store instructions which are passed through the switch  or are addressed to the DCR or local control registers .","Look-Up Engine","FIG. -- illustrates a state machine  that operates the look-up engine  in one embodiment. In one embodiment, inputs from the look-up queue  are latched in a register (not shown). This register holds its previous value if a \u201chold\u201d bit is asserted by the state machine  and preserved for use when the state machine  reenters a new request processing state. Inputs to the state machine  includes, without limitation, a request ID, a valid bit, a request type, a request thread, a user defining the request, a tag, a store index, etc.","By default, the look-up engine  is in a ready state  (i.e., a state ready for performing an operation). Upon receiving a request (e.g., a register write command), the look-up engine  goes to a register write state  (i.e., a state for updating a register in the prefetch system ). In the register write state , the look-up engine  stays in the state  until receiving an SDA arbitration input  (i.e., an input indicating that the write data from the SDA has been granted access to the local control registers ). Upon completing the register update, the look-up engine  goes back to the ready state . Upon receiving a DCR write request (i.e., a request to write in the DCR) from the processor core , the look-up engine  goes from the register write state  to a DCR write wait state  (i.e., a state for performing a write to DCR). Upon receiving a DCR acknowledgement from the DCR, the look-up engine  goes from the DCR write wait state  to the ready state .","The look-up engine  goes from the ready state  to a DCR read wait  (i.e., a state for preparing to read contents of the DCR) upon receiving a DCR ready request (i.e., a request for checking a readiness of the DCR). The look-up engine  stays in the DCR read wait state  until the look-up engine  receives the DCR acknowledgement  from the DCR. Upon receiving the DCR acknowledgement, the look-up engine  goes from the DCR read wait state  to a register read state . The look-up engine  stays in the register read state  until a processor core reload arbitration signal  (i.e., a signal indicating that the DCR read data has been accepted by the interface ) is asserted.","The look-up engine  goes from the ready state  to the register read state  upon receiving a register read request (i.e., a request for reading contents of a register). The look-up engine  comes back to ready state  from the register read state  upon completing a register read. The look-up engine  stays in the ready state  upon receiving one or more of: a hit signal (i.e., a signal indicating a \u201chit\u201d in an entry in the prefetch directory ), a prefetch to demand fetch conversion signal (i.e., a signal for converting a prefetch request to a demand to a switch or a memory device), a demand load signal (i.e., a signal for loading data or instructions from a switch or a memory device), a victim empty signal (i.e., a signal indicating that there is no victim stream to be selected by the stream prefetch engine ), a load command for data that must not be put in cache (a non-cache signal), a hold signal (i.e., a signal for holding current data), a noop signal (i.e., a signal indicating no operation).","The look-up engine  goes to the ready state  to a WCBF evict state  (i.e., a state evicting an entry from the WCBF array ) upon receiving a WCBF evict request (i.e., a request for evicting the WCBF entry). The look-up engine  goes back to the ready state  from the WCBF evict state  upon completing an eviction in the WCBF array . The look-up engine  stays in the WCBF evict state  while a switch request queue (SRQ) arbitration signal  is asserted.","The look-up engine  goes from the ready state  to a WCBF flush state  upon receiving a WCBF flush request (i.e., a request for flushing the WCBF array ). The look-up engine  goes back to the ready state  from the WCBF flush state  upon a completion of flushing the WCBF array . The look-up engine  stays in the ready state  while a generation change signal (i.e., a signal indicating a generation change of data in an entry of the WCBF array ) is asserted.","In one embodiment, most state transitions in the state machine  are done in a single cycle. Whenever a state transition is scheduled, a hold signal is asserted to prevent further advance of the look-up queue  and to ensure that a register at a boundary of the look-up queue  retains its value. This state transition is created, for example, by a read triggering two write combine array evictions for coherency maintenance. Generation change triggers a complete flush of the WCBF array  over multiple clock cycles.","The look-up engine  outputs the following signals going to the hit queue , SRT (Switch Request Table) , demand fetch conversion engine , and look-up queue : critical word, a tag (bits attached by the processor core  to allow it to identify a returning load command) indicating thread ID, 5-bit store index, a request index, a directory index indicating the location of prefetch data for the case of a prefetch hit, etc.","In one embodiment, a READ combinational logic (i.e., a combinational logic performing a memory read) returns a residency of a current address and next consecutive addresses. A STORE combinational logic (i.e., a combinational logic performing a memory write) returns a residency of a current address and next consecutive addresses and deasserts an address valid bit for any cache lines matching this current address.","Hit Queue","In one exemplary embodiment, the hit queue  is implemented, e.g., by 12 entry\u00d712-bit register array holds pending hits (hits for prefetched data) for a presentation to the interface  of the processor core. Read and write pointers are maintained in one or two clock cycle domain. Each entry of the hit queue includes, without limitation, a critical word, a directory index and a processor core tag.","Prefetch Data Array","In one embodiment, the prefetch data array  is implemented, e.g., by a dual ported 32\u00d7128-byte SRAM operating in one or two clock cycle domain. A read port is driven, e.g., by the hit queue and the write port is driven, e.g., by switch response handler .","Prefetch Directory","The prefetch directory  includes, without limitation, a 32\u00d748-bit register array storing information related to the prefetch data array . It is accessed by the look-up engine  and written by the prefetch engines  and . The prefetch directory  operates in one or two clock cycle domain and is timing and performance critical. There is provided a combinatorial logic associated with this prefetch directory  including a replication count of address comparators.","Each prefetch directory entry includes, without limitation, an address, an address valid bit, a stream ID, data representing a prefetching depth. In one embodiment, the prefetch directory  is a data structure and may be accessed for a number of different purposes.","Look-Up and Stream Comparators","In one embodiment, at least two 32-bit addresses associated with commands are analyzed in the address compare engine  as a particular address (e.g., 35bit to 3bit) and their increments. A parallel comparison is performed on both of these numbers for each prefetch directory entry. The comparators evaluate both carry and result of the particular address (e.g., 2bit to 0bit)+0, 1, . . . , or 7. The comparison bits (e.g., 35bit to 3bit in the particular address) with or without a carry and the first three bits (e.g., 2bit to 0th bit in the particular address) are combined to produce a match for lines N, N+1 to N+7 in the hit queue . This match is used by look-up engine  for both read, and write coherency and for deciding which line to prefetch for the stream prefetch engine . If a write signal is asserted by the look-up engine , a matching address is invalidated and subsequent read look-ups (i.e., look-up operations in the hit queue  for a read command) cannot be matched. A line in the hit queue  will become unlocked for reuse once any pending hits, or pending data return if the line was in-flight, have been fulfilled.","LIST Prefetch Comparators","In one embodiment, address compare engine  includes, for example, 32\u00d735-bit comparators returning \u201chit\u201d (i.e., a signal indicating that there exists prefetched data in the prefetch data array  or the prefetch directory ) and \u201chit index\u201d (i.e., a signal representing an index of data being \u201chit\u201d) to the list prefetch engine  in one or two clock cycle period(s). These \u201chit\u201d and \u201chit index\u201d are used to decide whether to service or discard a prefetch request from the list prefetch engine . The prefetch system  does not establish the same cache line twice. The prefetch system  discards prefetched data or instruction(s) if it collides with an address in a write combine array (e.g., array  or ).","Automatic Stream Detection, Manual Stream Touch","All or some of the read commands that cause a miss when looked up in the prefetch directory  are snooped by the stream detect engine . The stream detect engine  includes, without limitation, a table of expected next aligned addresses based on previous misses to prefetchable addresses. If a confirmation (i.e., a stream is detected, e.g., by finding a match between an address in the table and an address forwarded by the look-up engine) is obtained (e.g., by a demand fetch issued on a same cycle), the look-up queue  is stalled on a next clock cycle and a cache line is established in the prefetch data array  starting from an (aligned) address to the aligned address. The new stream establishment logic is shared with at least 16 memory mapped registers, one for each stream that triggers a sequence of four cache lines to be established in the prefetch data array  with a corresponding stream ID, starting with the aligned address written to the register.","When a new stream is established the following steps occur\n\n","In one embodiment, the demand fetch conversion engine  includes, without limitation, an array of, for example, 16 entries\u00d713 bits representing at least 14 hypothetically possible prefetch to demand fetch conversions (i.e., a process converting a prefetch request to a demand for data to be returned immediately to the processor core ). The information bits of returning prefetch data from the switch  is compared against this array. If this comparison determines that this prefetch data has been converted to demand fetch data (i.e., data provided from the switch  or a memory system), these entries will arbitrate for access to the hit queue , waiting for free clock cycles. These entries wait until the cache line is completely entered before requesting an access to the hit queue . Each entry in the array in the engine  includes, without limitation, a demand pending bit indicating a conversion from a prefetch request to a demand load command when set, a tag for the prefetch, an index identifying the target location in the prefetch data array  for the prefetch and a critical word associated with the demand.","ECC and Parity","In one embodiment, data paths and\/or prefetch data array  will be ECC protected, i.e., errors in the data paths and\/or prefetch data array may be corrected by ECC (Error Correction Code). In one embodiment, the data paths will be ECC protected, e.g., at the level of 8-byte granularity. Sub 8-byte data in the data paths will by parity protected at a byte level, i.e., errors in the data paths may be identified by a parity bit. Parity bit and\/or interrupts may be used for the register array  which stores request information (e.g., addresses and status bits). In one embodiment, a parity bit is implemented on narrower register arrays (e.g., an index FIFO, etc.). There can be a plurality of latches in this module that may affect a program function. Unwinding logical decisions made by the prefetch system  based on detected soft errors in addresses and request information may impair latency and performance. Parity bit implementation on the bulk of these decisions is possible. An error refers to a signal or datum with a mistake.","FIG. -- depicts, in greater detail, a plurality of processing unit (PU), . . . , , one of which, PU shown including at least one processor core , such as the A2 core, the quad floating point unit (QPU) and an optional L1P pre-fetch cache . The PU, in one embodiment, includes a 32B wide data path to an associated L1-cache , allowing it to load or store 32B per cycle from or into the L1-cache. In a non-limiting embodiment, each core  is directly connected to an optional private prefetch unit (level-1 prefetch, L1P) , which accepts, decodes and dispatches all requests sent out by the A2 processor core. In one embodiment, a store interface from the A2 to the L1P is 32B wide and the load interface is 16B wide, both operating at processor frequency, for example. The L1P implements a fully associative, 32 entry prefetch buffer, each entry holding cache lines of 128B size, for example. Each PU is connected with the L2 cache  via a master port (a Master device) of full crossbar switch . In one example embodiment, the shared L2 cache is 32 MB sliced into 16 units, with each 2 MB unit connecting to a slave port of the switch (a Slave device). Every physical address issued via a processor core is mapped to one slice using a selection of programmable address bits or a XOR-based hash across all issued address bits. The L2-cache slices, and the L1 caches of the A2s are hardware-coherent. A group of four slices may be connected via a ring to one of the two DDR3 SDRAM controllers  ().","As shown in FIG. --, each PU's . . . , , where M is the number of processors cores, and ranges from 0 to 17, for example, connects to the central low latency, high bandwidth crossbar switch  via a plurality of master ports including master data ports  and corresponding master control ports . The central crossbar  routes requests received from up to M processor cores via associated pipeline latches . . . , where they are input to respective data path latch devices . . . , in the crossbar  to write data from the master ports to the slave ports  via data path latch devices . . . , in the crossbar  and respective pipeline latch devices . . . , , where S is the number of L2 cache slices, and may comprise an integer number up to 15, in an example embodiment. Similarly, central crossbar  routes return data read from memory  via associated pipeline latches and data path latches back to the master ports. A write data path of each master and slave port is 16B wide, in example embodiment. A read data return port is 32B wide, in an example embodiment.","As further shown in FIG. --, the cross-bar includes arbitration device  implementing one or more state machines for arbitrating read and write requests received at the crossbar  from each of the PU's, for routing to\/from the L2 cache slices .","In the multiprocessor system on a chip , the \u201cM\u201d processors (e.g., 0 to M\u22121) are connected to the centralized crossbar switch  through one or more pipe line latch stages. Similarly, \u201cS\u201d cache slices (e.g., 0 to S\u22121) are also connected to the crossbar switch  through one or more pipeline stages. Any master \u201cM\u201d intending to communicate with a slave \u201cS\u201d, sends a request  to the crossbar indicating its need to communicate with the slave \u201cS\u201d. The arbitrations device  arbitrates among the multiple requests competing for the same slave \u201cS\u201d.","Processor core connects to the arbitration device  via a plurality of Master data ports  and Master control ports . At a Master control port , a respective processor signal  requests routing of data latched at a corresponding Master data port  to a Slave device associated with a cache slice. Processor request signals  are received and latched at the corresponding Master control pipeline latch devices . . . , for routing to the arbiter every clock cycle. Arbitration device issues arbitration grant signals  to the respective requesting processor core  from the arbiter . Grant signals  are latched corresponding Master control pipeline latch devices . . . , prior to transfer back to the processor. The arbitration device  further generates corresponding Slave control signals  that are communicated to slave ports  via respective Slave control pipeline latch devices . . . , , in an example embodiment. Slave control port signals inform the slaves of the arrival of the data through a respective slave data port . . . , in accordance with the arbitration scheme issued at that clock cycle. In accordance with arbitration grants selecting a Master Port  and Slave Port  combination in accordance with an arbitration scheme implemented, the arbitration device  generates, in every clock cycle, multiplexor control signals  for receipt at a respective multiplexor devices . . . , to control, e.g., select by turning on, a respective multiplexor. A selected multiplexor enables forwarding of data from master data path latch device . . . , associated with a selected Master Port to the selected Slave Port  via a corresponding connected slave data path latch device . . . , . In FIG. --, for example, two multiplexor control signals and are shown issued simultaneously for controlling routing of data via multiplexor devices and .","In one example embodiment, the arbitrations device  arbitrates among the multiple requests competing for the same slave \u201cS\u201d using a two step mechanism: 1): There are \u201cS\u201d slave arbitration slices. Each slave arbitration slice includes arbitration logic that receives all the pending requests of various Masters to access it. It then uses a round robin mechanism that uses a single round robin priority vector, e.g., bits, to select one Master as the winner of the arbitration. This is done independently by each of the S slave arbitration slices in a clock cycle; 2): There are \u201cM\u201d Master arbitration slices. It is possible that multiple Slave arbitration slices have chosen the same Master in the previous step. Each master arbitration slice uses a round robin mechanism to choose one such slave. This is done independently by each of the \u201cM\u201d master arbitration slices. Though FIG. -- depicts processing at a single arbitration unit , it is understood that both Master arbitration slice and Slave arbitrations slice state machine logic may be distributed within the crossbar switch.","This method ensures fairness, as shown in the signal timing diagram of arbitration device signals of FIG. -- and depicted in Table 1 below. For example, assuming that Masters 1 through 4 have chosen to access Slave 4. Assuming also that master 0 has pending requests to slaves 0 through 4. It is possible that each of the Slaves 0 through 4 choose master 0 (e.g., in cycle 1). Now Master 0 chooses one of the slaves. Masters 1 through 4 find that no slave has chosen them and hence they do not participate in the arbitration process. Master 0 using a round robin mechanism chooses slave 0 in cycle 1. Slaves 1 through 4, implementing a single round robin priority vector, continue to choose master 0 in cycle 2. Master 0 chooses slave 1 in cycle 2, slave 2 in cycle 3, slave 3 in cycle 4 and slave 4 in cycle 5. Only after slave 4 is chosen in cycle 5, will Slave 4 choose another master using the round robin mechanism. Even though requests were pending from Masters 1 through 4 to slave 4, slave 4 implementing a single round robin priority vector, continued to choose master 0 for cycles 1 through 5. The following describes the cycle and choice and winner via this mechanism using round robin priority:",{"@attributes":{"id":"p-0444","num":"0459"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"77pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"91pt","align":"left"}}],"thead":{"row":[{"entry":[{},"TABLE 1"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]},{"entry":[{},"Cycle","Choice of Slave 4","Winner"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"1","Master 0","Master 0 to Slave 0"]},{"entry":[{},"2","Master 0","Master 0 to Slave 1"]},{"entry":[{},"3","Master 0","Master 0 to Slave 2"]},{"entry":[{},"4","Master 0","Master 0 to Slave 3"]},{"entry":[{},"5","Master 0","Master 0 to Slave 4 (slave 4"]},{"entry":[{},{},{},"wins)"]},{"entry":[{},"6","Master 1","Master 1 to Slave 4 (slave 4"]},{"entry":[{},{},{},"wins)"]},{"entry":[{},"7","Master 2","Master 2 to Slave 4 (slave 4"]},{"entry":[{},{},{},"wins)"]},{"entry":[{},"8","Master 3","Master 3 to Slave 4 (slave 4"]},{"entry":[{},{},{},"wins)"]},{"entry":[{},"9","Master 4","Master 4 to Slave 4 (slave 4"]},{"entry":[{},{},{},"wins)"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]}}}}},"In this example, it takes at least 5 clock cycles  before the request for Master 1 had even been granted to a slave due to the round robin scheme implemented. However, all transactions to slave 4 are scheduled by cycle 9.","This throughput performance through crossbar  may be improved in a further embodiment: rather than each slave using a single round robin priority vector, each slave uses two or more round robin priority vectors. The slave cycles the use of these priority vectors every clock cycle. Thus, in the above example, slave 4 having chosen Master 0 in cycle 1, will choose Master 1 in cycle 2 using a different round robin priority vector. In cycle 2, Master 1 would choose slave 4 as it is the only slave requesting it.",{"@attributes":{"id":"p-0447","num":"0462"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"77pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"91pt","align":"left"}}],"thead":{"row":[{"entry":[{},"TABLE 2"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]},{"entry":[{},"Cycle","Chosen by slave 4","Winner"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"1","Master 0","Master 0 to Slave 0"]},{"entry":[{},"2","Master 1","Master 0 to Slave 1;"]},{"entry":[{},{},{},"Master 1 to Slave 4 (slave 4"]},{"entry":[{},{},{},"wins)"]},{"entry":[{},"3","Master 0","Master 0 to Slave 2"]},{"entry":[{},"4","Master 2","Master 0 to Slave 3;"]},{"entry":[{},{},{},"Master 2 to Slave 4 (slave 4"]},{"entry":[{},{},{},"wins)"]},{"entry":[{},"5","Master 0","Master 0 to Slave 4 (slave 4"]},{"entry":[{},{},{},"wins)"]},{"entry":[{},"6","Master 3","Master 3 to Slave 4 (slave 4"]},{"entry":[{},{},{},"wins)"]},{"entry":[{},"7","Master 4","Master 4 to Slave 4 (slave 4"]},{"entry":[{},{},{},"wins)"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]}}}}},"FIG. -- depicts the first step processing  performed by the arbiter . The process  is performed by each slave arbitration slice, i.e., arbitration logic executed at each slice (for each Slave 0 to S\u22121). At , each Slave arbitration slice receives all the pending requests of various Masters requesting access to it, e.g., Slave S1, for example. Using a priority vector SP1, the Slave S1 arbitration slice chooses one of the masters (e.g., M1) at . The Slave arbitration slice then sends this information to the master arbitration slice M1 at . Then, as a result of the arbitration scheme implemented the chosen Master, e.g., Master 1, a determination is made as to whether the M1 has accepted the Slave S1 at  or other slaves at that clock cycle. If at  it is determined that the M1 has accepted the Slave (e.g., Slave 1), then the priority vector SP1 is updated at step  and the process proceeds to . Otherwise, if it is determined that the M1 has not accepted the Slave (e.g., Slave 1) the process continues directly to step . Then, in the subsequent cycle, as shown at , the Slave arbitration slice examines requests from various Masters to Slave S1 and, at , uses a second priority vector SP2 to choose one of the Masters (e.g., M2). Continuing, at , this information is transmitted to the Master arbitration slice, e.g., for Master M2. Then, at , a further determination is made as to whether the Master arbitration for M2 has accepted the Slave S1. If the Master arbitration for M2 has accepted the Slave S1, then at , the priority vector is updated to SP2 and the process returns to  for continuing arbitration for that Slave slice.","In a similar vein, each Master can have two or more priority vectors and can cycle among their use every clock cycle to further increase performance. FIG. -- depicts the second step processing performed by the arbiter . The process  is performed by each master arbitration slice, i.e., arbitration logic executed at each slice (for each Master 0 to M\u22121). Each Master arbitration slice waits until a Slave arbitration slice has selected it (Slave arbitration has selected a Master) at . Then, at  using a priority vector MP1, Master arbitration slice chooses one of the slaves (e.g., S1). This information is sent to the corresponding Slave arbitration slice S1 at . Then, priority vector MP1 is updated at . Then, in the subsequent cycle, at , the Master arbitration slice waits again for the slave arbitration slices to make a master selection. Using a priority vector MP2, the Master arbitration slice at  chooses one of the slaves (e.g., S2). Then, the Master arbitration slice transmits this information to the slave arbitration slice S2 at . Finally, the priority vector MP2 is updated at  and the process returns to  for continuing arbitration for that Master slice.","In one example embodiment, the priority vector used by the slave, e.g., SP1, is M bits long (0 to M\u22121), as the slave arbitration has to choose one of M masters. Hence, only one bit would be set per cycle as the lowest priority bit, in the example. For example, if a bit  of the priority vector is set, then the Master 5 has the lowest priority and the Master 6 would have the highest priority, Master 7 has the second highest priority, etc. The order from highest priority to lowest priority is 6, 7, 8 . . . M\u22121, 0, 1, 2, 3, 4, 5 in this example priority vector. Further, for example, the Masters arbitration slices 7, 8 and 9 request the slave and Master 7 wins. The priority vector SP1 would be updated so that bit  would be set\u2014resulting in priority order from highest to lowest as 8, 9, 10, . . . M\u22121, 0, 1, 2, 3, 4, 5, 6, 7 in the updated vector. A similar bit vector scheme is further used by the Master arbitration logic devices in determining priority values of slaves to be selected for access within a clock cycle.","The usage of multiple priority vectors both by the masters and slaves and cycling among them result in increased performance. For example, as a result of implementing processes at the arbitration Slave and Master arbitration slices of the example depicted in FIG. --, it is seen that all transactions to slave S4 are scheduled by the seventh clock cycle , thus improving performance as compared to the case of FIG. --.","A method and system are described that reduce latency between masters (e.g., processors) and slaves (e.g., devices having memory\/cache\u2014L2 slices) communicating with one another through a central cross bar switch.","FIG. -- is a diagram illustrating communications between masters and slaves via a cross bar switch. In a multiprocessor system on a chip (e.g., in integrated circuit such as an application specific integrated circuit (ASIC)), \u201cM\u201d processors (e.g., 0 to M\u22121) are connected to a centralized crossbar switch  through one or more pipe line latch stages . Similarly, \u201cS\u201d slave devices, for example, cache slices (e.g., 0 to S\u22121) are also connected to the crossbar switch through one or more pipeline stages .","Any master \u201cm\u201d desiring to communicate with a slave \u201cs\u201d goes through the following steps:\n\n","The latency expected for communicating among the masters, the cross bar , and the slaves are shown in FIG. --. Let us assume that there are p1 pipeline stages between a master and the crossbar switch and p2 pipeline stages between the crossbar switch and a slave. Following is a typical latency calculation for a request assuming that there is no contention for the slave. A master sending a request (\u201creq_r1\u201d) to the cross bar may take p1 cycles, for example, as shown at . Crossbar arbitrating multiples requests from multiple masters may take A1 cycles, for example, as shown at . Cross bar sending a schedule command (e.g., \u201csked_r1\u201d) may take p1 cycles, for example, as shown at . Master sending the information to the crossbar (e.g., \u201cinfo_r1\u201d) may take p1 cycles, for example, as shown at . Crossbar sending the information (e.g., \u201cinfo_r1\u201d) to the slave may take p2 cycles, for example, as shown at . The number of cycles spent in sending information from a master to a slave totals to 3*(p1)+A+p2 cycles in this example.","Referring back to FIG. --, the method and system in one embodiment of the present disclosure reduce the latency or number of cycles it takes in communicating between a master and a slave. In one aspect, this is accomplished without buffering information, for example, to keep the area or needed resources such as buffering devices to a minimum. A master, for example, master \u201cm\u201d sends a request (\u201creq_r1\u201d) to the cross bar  indicating its intention to communicate with slave \u201cs\u201d, for example, via a pipe line latch . The master \u201ceagerly\u201d sends the information (e.g., \u201cinfo_r1\u201d) to be transferred to the slave \u201cA\u201d cycles after sending the request, for example, via pipe line latch unless there is information to be sent in response to a \u201cschedule\u201d command. The master continues to drive the information to be transferred to the slave unless there is a \u201cschedule\u201d command or \u201cA\u201d or more cycles have elapsed after a later request (e.g., \u201creq_r2\u201d) has been issued.","The cross bar switch  arbitrates among the multiple requests competing for the same slave \u201cs\u201d. In one embodiment, the cross bar switch  may include an arbiter logic , which makes decisions as to which master can talk to which slave. The cross bar switch  may include an arbiter for each master and each slave slice, for instance, a slave arbitration slice for each slave 0 to S\u22121, and a master arbitration slice for each master 0 to M\u22121. Once it has determined that a slot is available for transferring the information from \u201cm\u201d to \u201cs\u201d, the crossbar  sends the information (\u201cinfo_r1\u201d) to the slave \u201cs\u201d, for example, via a pipe line latch . The crossbar  also sends an acknowledgement back to the master \u201cm\u201d that the \u201ceager\u201d scheduling has succeeded, for example, via a pipe line latch ","Eager scheduling latency is shown in FIG. -- which illustrates the cycles incurred in communicating between a master and a slave with the above-described eager scheduling protocol. A master sending a request (\u201creq_r1\u201d) to the cross bar may take p1 cycles as shown at . Arbitration by the crossbar may take A cycles, for example, as shown at . The crossbar sending the information (\u201cinfo_r1\u201d) to the slave may take p2 cycles. Thus, it takes a total of 1*(p1)+A+p2 cycles to send information or data from a master to a slave. Compared with the non-eager scheduling shown in FIG. --, eager scheduling has reduced the latency by 2*p1 cycles. Eager scheduling protocol sends the information only after waiting the number of cycles the crossbar takes to arbitrate, for example, shown at . Thus, the cycle time taken for sending the information (e.g., shown at  and ) overlaps with the time the spent in transferring the request and the time spent by the crossbar in arbitrating (e.g., shown at  and ).","FIG. -- is a flow diagram illustrating a core or processor to crossbar scheduling in one embodiment of the present disclosure. At , a master device, for example, a processor or a core, determines whether there is a new request to send to the cross bar switch. If there is no new request, the logic flow continues at . If there is a new request, then at , request is sent to the cross bar switch. The logic flow then continues to .","At , the master device checks whether a request to schedule information has been received from the cross bar switch. If there is no request to schedule information, the logic flows to . If a request to schedule the information has been received, the master sends the information associated with this request to schedule to the cross bar switch at . The logic flow then continues to .","At , it is determined whether a request was sent to the crossbar \u201carbitration delay\u201d cycles before the current cycle. If so, at , the master device \u201ceagerly\u201d sends the information or data associated with the request that was sent \u201carbitration delay\u201d cycles before the current cycle. The logic then continues to  where it is again determined whether there is a new request to send information to the cross bar switch.","At , if no request was sent to the crossbar \u201carbitration delay\u201d cycles before the current cycle, then the master device drives or sends to the cross bar switch the information associated with the latest request that was sent at least \u201carbitration cycles\u201d before the current cycle. At , the master device proceeds to the next cycle and the logic returns to continue at .","The master continues to drive the information associated with the latest request sent at least \u201cA\u201d cycles before. So as long as no new requests are sent to the switch by that master, eager scheduling success is possible even in later cycles than the one indicated in FIG. --.","As an implementation example, each of the slave arbitration slices may maintain M counters (counter 0 to counter M\u22121). Counter[m][s] signals the number of pending requests from master \u201cm\u201d to slave \u201cs\u201d. When a master \u201cm\u201d sends a request to a slave \u201cs\u201d, counter[m][s] is incremented by that slave. When a request to that master gets scheduled (eager or non eager), the counter gets decremented. Each of the master arbitration slices also maintains the identifier of the slave that is last sent by the master. When a request to a master \u201cm\u201d gets scheduled to slave s, the identifier of the slave that is last sent by that master is matched with \u201cs\u201d. If there is a match, then eager scheduling is possible. Other implementations are possible to perform the eager scheduling described herein, and the present invention is not limited to one specific implementation.","FIG. -- is a flow diagram illustrating functionality of the cross bar switch in one embodiment of the present disclosure. A cross bar switch may include an arbiter logic, e.g., shown in FIG. -- at , which makes decisions as to which master can talk to which slave. The cross bar switch may include an arbiter which performs distributed arbitration. For instance, there may be arbitration logic for each slave, for instance, a slave arbitration slice for each slave 0 to S\u22121. Similarly, there may be arbitration logic for each master, for instance, a master arbitration slice for each master 0 to M\u22121. FIG. -- illustrates functions of an arbitration slice for one slave device, for example, slave s1.","At , an arbiter, for example, a slave arbitration slice for s1 examines one or more requests from one or more masters to slave s1. At , a master is selected. For instance, if there is more than one master desiring to talk to slave s1, the slave arbitration slice for s1 may use a predetermined protocol or rule to select one master. If there is only one master requesting to talk to this slave device, arbitrating for a master is not needed. Rather, that one master is selected. The predetermined protocol or rule may to use round robin priority selection method. Other protocols or rules may be employed for selecting a master from a plurality of masters.","At , the slave arbitration slice sends the information that it selected a master, for example, master ml to the master arbitration slice responsible for master ml. At , it is determined whether the selected master accepted the slave arbitration slice's decision. It may be that this master has received selections or other requests to talk from more than one slave. In such cases the master may not accept the slave arbitration slice's decision to talk to it. If the selected master does not accept, for example, for that reason or other reasons, the logic flow returns to  where the slave arbitration slice examines more requests.","At , if the selected master has accepted the slave arbitration slice's decision to talk to it, then the priority vector of may be updated to indicate that this master has been selected, for example, so that in the next selection process, this master does not get the highest priority of selection and another master may be selected.","Once the slot between the selected master and this slave has been made available or established for example according to the previous steps for communication, it is determined at  whether the eager scheduling can succeed. That is, the slave arbitration slice determines whether the information or data is available from this master that it can send to the slave device. The information or data may be available at the cross bar switch, if the selected master has sent the information \u201ceagerly\u201d after waiting for an arbitration delay period even without an acknowledgment from the cross bar switch to send the information.","If at , it is determined that the information can be sent to the slave, the information from the selected master is sent to the slave at . The arbitration slice sends a notification to the master arbitration slice that the eager scheduling succeeded. The master arbitration slice then sends the eager scheduling success notice to the selected master. The logic returns to  to continue to the next request.","If at , it is determined that the information is not available to send to the slave currently, slave arbitration slice sends a notification or request to schedule the information or data to the master at , for example, via the master's arbitration slice at the cross bar switch. The logic returns to  to continue to the next request.","FIG. -- illustrates functions of an arbitration slice for one master device in one embodiment of the present disclosure. As explained above, the cross bar switch may include an arbitration slice for each master device, for example, master 0 to master M\u22121 on an integrated chip. At , an arbitration slice for a master device waits for slave arbitration slices to select a master. At , the arbitration slice may use a predetermine protocol or rule such as a round robin selection protocol or others to select a slave among the slaves that have selected this master to communicate with. If only one slave has selected this master currently, the master arbitration slice need not arbitrate for a slave, rather the master arbitration slice may accept that slave.","At , the master arbitration slice notifies the slave selected for communication. This establishes the communication or slot between the master and the slave. At , a priority vector or the like may be updated to indicate that this slave has been selected, for example, so that this slave does not get the highest priority for selection in the next round of selections. Rather, other slaves a given a chance to communicate with this master in the next round.","Processing Unit","The complex consisting of A2, QPU and L1P is called processing unit (PU, see ). Each PU connects to the central low latency, high bandwidth crossbar switch via a master port. The central crossbar routes requests and write data from the master ports to the slave ports and read return data back to the masters. The write data path of each master and slave port is 16B wide. The read data return port is 32B wide.","FIG. -- is an overview of a memory management unit  (MMU) utilized by in a multiprocessor system, such as IBM's BlueGene parallel computing system. Further details about the MMU  are provided in IBM's \u201cPowerPC RISC Microprocessor Family Programming Environments Manual v2.0\u201d (hereinafter \u201cPEM v2.0\u201d) published Jun. 10, 2003 which is incorporated by reference in its entirety. The MMU  receives data access requests from the processor (not shown) through data accesses  and receives instruction access requests from the processor (not shown) through instruction accesses . The MMU  maps effective memory addresses to physical memory addresses to facilitate retrieval of the data from the physical memory. The physical memory may include cache memory, such as L1 cache, L2 cache, or L3 cache if available, as well as external main memory, e.g., DDR3 SDRAM.","The MMU  comprises an SLB , an SLB search logic device , a TLB , a TLB search logic device , an Address Space Register (ASR) , an SDR1 , a block address translation (BAT) array , and a data block address translation (DBAT) array . The SDR1  specifies the page table base address for virtual-to-physical address translation. Block address translation and data block address translation are one possible implementation for translating an effective address to a physical address and are discussed in further detail in PEM v2.0 and U.S. Pat. No. 5,907,866.","Another implementation for translating an effective address into a physical address is through the use of an on-chip SLB, such as SLB , and an on-chip TLB, such as TLB . Prior art SLBs and TLBs are discussed in U.S. Pat. No. 6,901,540 and U.S. Publication No. 20090019252, both of which are incorporated by reference in their entirety. In one embodiment, the SLB  is coupled to the SLB search logic device  and the TLB  is coupled to the TLB search logic device . In one embodiment, the SLB  and the SLB search logic device  function to translate an effective address (EA) into a virtual address. The function of the SLB is further discussed in U.S. Publication No. 20090019252. In the PowerPC\u2122 reference architecture, a 64 bit effective address is translated into an 80 bit virtual address. In the A2 implementation, a 64 bit effective address is translated into an 88 bit virtual address.","In one embodiment of the A2 architecture, both the instruction cache and the data cache maintain separate \u201cshadow\u201d TLBs called ERATs (effective to real address translation tables). The ERATs contain only direct (IND=0) type entries. The instruction I-ERAT contains 16 entries, while the data D-ERAT contains 32 entries. These ERAT arrays minimize TLB  contention between instruction fetch and data load\/store operations. The instruction fetch and data access mechanisms only access the main unified TLB  when a miss occurs in the respective ERAT. Hardware manages the replacement and invalidation of both the I-ERAT and D-ERAT; no system software action is required in MMU mode. In ERAT-only mode, an attempt to access an address for which no ERAT entry exists causes an Instruction (for fetches) or Data (for load\/store accesses) TLB Miss exception.","The purpose of the ERAT arrays is to reduce the latency of the address translation operation, and to avoid contention for the TLB  between instruction fetches and data accesses. The instruction ERAT (I-ERAT) contains sixteen entries, while the data ERAT (D-ERAT) contains thirty-two entries, and all entries are shared between the four A2 processing threads. There is no latency associated with accessing the ERAT arrays, and instruction execution continues in a pipelined fashion as long as the requested address is found in the ERAT. If the requested address is not found in the ERAT, the instruction fetch or data storage access is automatically stalled while the address is looked up in the TLB . If the address is found in the TLB , the penalty associated with the miss in the I-ERAT shadow array is 12 cycles, and the penalty associated with a miss in the D-ERAT shadow array is 19 cycles. If the address is also a miss in the TLB , then an Instruction or Data TLB Miss exception is reported.","When operating in MMU mode, the on-demand replacement of entries in the ERATs is managed by hardware in a least-recently-used (LRU) fashion. Upon an ERAT miss which leads to a TLB  hit, the hardware will automatically cast-out the oldest entry in the ERAT and replace it with the new translation. The TLB  and the ERAT can both be used to translate an effective or virtual address to a physical address. The TLB  and the ERAT may be generalized as \u201clookup tables\u201d.","The TLB  and TLB search logic device  function together to translate virtual addresses supplied from the SLB  into physical addresses. A prior art TLB search logic device  is shown in FIG. --. A TLB search logic device  according to one embodiment of the invention is shown in FIG. --. The TLB search logic device  facilitates the optimization of page entries in the TLB  as discussed in further detail below.","Referring to FIG. --, the TLB search logic device  controls page identification and address translation, and contains page protection and storage attributes. The Valid (V), Effective Page Number (EPN), Translation Guest Space identifier (TGS), Translation Logical Partition identifier (TLPID), Translation Space identifier (TS), Translation ID (TID), and Page Size (SIZE) fields of a particular TLB entry identify the page associated with that TLB entry. In addition, the indirect (IND) bit of a TLB entry identifies it as a direct virtual to real translation entry (IND=0), or an indirect (IND=1) hardware page table pointer entry that requires additional processing. All comparisons using these fields should match to validate an entry for subsequent translation and access control processing. Failure to locate a matching TLB page entry based on the criteria for instruction fetches causes a TLB miss exception which results in issuance of an Instruction TLB error interrupt. Failure to locate a matching TLB page entry based on this criteria for data storage accesses causes a TLB miss exception which may result in issuance of a data TLB error interrupt, depending on the type of data storage access. Certain cache management instructions do not result in an interrupt if they cause an exception; these instructions may result in a no-op.","Page identification begins with the expansion of the effective address into a virtual address. The effective address is a 64-bit address calculated by a load, store, or cache management instruction, or as part of an instruction fetch. In one embodiment of a system employing the A2 processor, the virtual address is formed by prepending the effective address with a 1-bit \u2018guest space identifier\u2019, an 8-bit \u2018logical partition identifier\u2019, a 1-bit \u2018address space identifier\u2019 and a 14-bit\u2032process identifier\u2032. The resulting 88-bit value forms the virtual address, which is then compared to the virtual addresses contained in the TLB page table entries. For instruction fetches, cache management operations, and for non-external PID storage accesses, these parameters are obtained as follows. The guest space identifier is provided by Machine State Register MACHINE STATE REGISTER[GS]. The logical partition identifier is provided by the Logical Partition ID (LPID) register. The process identifier is included in the Process ID (PID) register. The address space identifier is provided by MACHINE STATE REGISTER[IS] for instruction fetches, and by MACHINE STATE REGISTER[DS] for data storage accesses and cache management operations, including instruction cache management operations.","For external PID type load and store accesses, these parameters are obtained from the External PID Load Context (EPLC) or External PID Store Context (EPSC) registers. The guest space identifier is provided by EPL\/SC[EGS] field. The logical partition identifier is provided by the EPL\/SC[ELPID] field. The process identifier is provided by the EPL\/SC[EPID] field, and the address space identifier is provided by EPL\/SC[EAS].","The address space identifier bit differentiates between two distinct virtual address spaces, one generally associated with interrupt-handling and other system-level code and\/or data, and the other generally associated with application-level code and\/or data. Typically, user mode programs will run with MACHINE STATE REGISTER[IS,DS] both set to 1, allowing access to application-level code and data memory pages. Then, on an interrupt, MACHINE STATE REGISTER[IS,DS] are both automatically cleared to 0, so that the interrupt handler code and data areas may be accessed using system-level TLB entries (i.e., TLB entries with the TS field=0).","FIG. -- is an overview of the translation of a 64 bit EA  into an 80 bit VA  as implemented in a system employing the PowerPC architecture. In one embodiment, the 64 bit EA  comprises three individual segments: an \u2018effective segment ID\u2019 , a \u2018page index\u2019 , and a \u2018byte offset\u2019 . The \u2018effective segment ID\u2019  is passed to the SLB search logic device  which looks up a match in the SLB  to produce a 52 bit virtual segment ID (VSID) . The \u2018page index\u2019  and byte offset  remain unchanged from the 64 bit EA , and are passed through and appended to the 52 bit VSID . In one embodiment, the \u2018page index\u2019  is 16 bits and the byte offset  is 12 bits. The \u2018byte offset\u2019  is 12 bits and allows every byte within a page to be addressed. A 4 KB page requires a 12 bit page offset to address every byte within the page, i.e., 2=4 KB. The VSID  and the \u2018page index\u2019  are combined into a Virtual Page Number (VPN), which is used to select a particular page from a table entry within a TLB (TLB entries may be associated with more than one page). Thus, the VSID  and the \u2018page index\u2019  is and the byte offset  are combined to form an 80 bit VA . A virtual page number (VPN) is formed from the VSID  and \u2018page index\u2019 . In one embodiment of the PowerPC architecture, the VPN comprises 68 bits. The VPN is passed to the TLB search logic device  which uses the VPN to look up a matching physical page number (RPN)  in the TLB . The RPN  together with the 12 bit byte offset form a 64 bit physical address .","FIG. -- is a TLB logic device  for matching a virtual address to a physical address. A match between a virtual address and the physical address is found by the TLB logic device  when all of the inputs into \u2018AND\u2019 gate  are true, i.e., all of the input bits are set to 1. Each virtual address that is supplied to the TLB  is checked against every entry in the TLB .","The TLB logic device  comprises logic blocks  and logic block . Logic block  comprises \u2018AND\u2019 gates  and , comparators , , , , ,  and , and \u2018OR\u2019 gates  and . \u2018AND\u2019 gate  that receives input from TLBentry[ThdID(t)] (thread identifier)  and \u2018thread t valid\u2019 . TLBentry[ThdID(t)]  identifies a hardware thread and in one implementation there are 4 thread ID bits per TLB entry. \u2018Thread t valid\u2019  indicates which thread is requesting a TLB lookup. The output of AND\u2032 gate  is 1 when the input of \u2018thread t valid\u2019  is 1 and the value of \u2018thread identifier\u2019 is 1.  The output of AND\u2032 gate  is coupled to \u2018AND\u2019 gate .","Comparator  compares the values of inputs TLBentry[TGS]  and \u2018GS\u2019 . TLBentry[TGS]  is a TLB guest state identifier and \u2018GS\u2019  is the current guest state of the processor. The output of comparator  is only true, i.e., a bit value of 1, when both inputs are of equal value. The output of comparator  is coupled to \u2018AND\u2019 gate .","Comparator  determines if the value of the \u2018logical partition identifier\u2019  in the virtual address is equal to the value of the TLPID field  of the TLB page entry. Comparator  determines if the value of the TLPID field  is equal to 0 (non-guest page). The outputs of comparators  and  are supplied to an \u2018OR\u2019 gate . The output of \u2018OR\u2019 gate  is supplied to \u2018AND\u2019 gate . The \u2018AND\u2019 gate  also directly receives an input from \u2018validity bit\u2019 TLBentry[V] . The output of \u2018AND\u2019 gate  is only valid when the \u2018validity bit\u2019  is set to 1.","Comparator  determines if the value of the \u2018address space\u2019 identifier  is equal to the value of the \u2018TS\u2019 field  of the TLB page entry. If the values match, then the output is 1. The output of the comparator  is coupled to \u2018AND\u2019 gate .","Comparator  determines if the value of the \u2018Process ID\u2019  is equal to the \u2018TID\u2019 field  of the TLB page entry indicating a private page, or comparator  determines if the value of the TID field is 0, indicating a globally shared page. The output of comparators  and  are coupled to \u2018OR\u2019 gate . The output of \u2018OR\u2019 gate  is coupled to \u2018AND\u2019 gate .","Comparator  determines if the value in the \u2018effective page number\u2019 field  is equal to the value stored in the \u2018EPN\u2019 field  of the TLB page entry. The number of bits N in the \u2018effective page number\u2019  is calculated by subtracting logof the page size from the bit length of the address field. For example, if an address field is 64 bits long, and the page size is 4 KB, then the effective address field length is found according to equation 1:\n\nEA=0 to \u22121, where =Address Field Length\u2212log(page size)\u2003\u2003(1)\n\nor by subtracting log(2) or 12 from 64. Thus, only the first 52 bits, or bits  to  of the effective address are used in matching the \u2018effective address\u2019 320 field to the \u2018EPN field\u2019 . The output of comparator  is coupled to \u2018AND\u2019 gate .\n","Logic block  comprises comparators  and  and \u2018OR\u2019 gate . Comparator  determines if the value of bits \u2018n:51\u2019  of the effective address (where n=64\u2212log(page size)) is greater than the value of bits n:51 of the \u2018EPN\u2019 field  in the TLB entry. Normally, the LSB are not utilized in translating the EA to a physical address. When the value of bits n:51 of the effective address is greater than the value stored in the EPN field, the output of comparator  is 1. Comparator  determines if the TLB entry \u2018exclusion bit\u2019  is set to 1. If the \u2018exclusion bit\u2019  is set to 1, than the output of comparator  is 1. The \u2018exclusion bit\u2019  functions as a signal to exclude a portion of the effective address range from the current TLB page. Applications or the operating system may then map subpages (pages smaller in size than the current page size) over the excluded region. In one example embodiment of an IBM BlueGene parallel computing system, the smallest page size is 4 KB and the largest page size is 1 GB. Other available page sizes within the IBM BlueGene parallel computing system include 64 KB, 16 MB, and 256 MB pages. As an example, a 64 KB page may have a 16 KB range excluded from the base of the page. In other implementations, the comparator may be used to excluded a memory range from the top of the page. In one embodiment, an application may map additional pages smaller in page size than the original page, i.e., smaller than 16 KB into the area defined by the excluded range. In the example above, up to four additional 4 KB pages may be mapped into the excluded 16 KB range. Note that in some embodiments, the entire area covered by the excluded range is not always available for overlapping additional pages. It is also understood that the combination of logic gates within the TLB search logic device  may be replaced by any combination of gates that result in logically equivalent outcomes.","A page entry in the TLB  is only matched to an EA when all of the inputs into the \u2018AND\u2019 gate  are true, i.e., all the input bits are 1. Referring back to FIG. --, the page table entry (PTE)  matched to the EA by the TLB search logic device  provides the physical address  in memory where the data requested by the effective address is stored.","FIGS. -- and -- together illustrate how the TLB search logic device  is used to optimize page entries in the TLB . One of the limiting properties of prior art TLB search logic devices is that, for a given page size, the page start address must be aligned to the page size. This requires that larger pages are placed adjacent to another in a contiguous memory range or that the gaps between large pages are filled in with numerous smaller pages. This requires the use of more TLB page entries to define a large contiguous range of memory.","FIG. -- is a table that provides which bits within a virtual address are used by the TLB search logic device  to match the virtual address to a physical address and which \u2018exclusion range\u2019 bits are used to map a \u2018hole\u2019 or an exclusion range into an existing page. FIGS. -- and -- are based on the assumption that the processor core utilized is a PowerPC\u2122 A2 core, the EA is 64 bits in length, and the smallest page size is 4 KB. Other processor cores may implement effective addresses of a different length and benefit from additional page sizes.","Referring now to FIG. ---, column  of the table lists the available page sizes in the A2 core used in one implementation of the BlueGene parallel computing system. Column  lists all the calculated values of log(page size). Column  lists the number of bits, i.e. MSB, required by the TLB search logic device  to match the virtual address to a physical address. Each entry in column  is found by subtracting log(page size) from 64.","Column  lists the \u2018effective page number\u2019 (EPN) bits associated with each page size. The values in column  are based on the values calculated in column . For example, the TLB search logic device  requires all 52 bits (bits 0:51) of the EPN to look up the physical address of a 4 KB page in the TLB . In contrast, the TLB search logic device  requires only 34 bits (bits 0:33) of the EPN to look up the physical address of a 1 GB page in the TLB . Recall that in one example embodiment, the EPN is formed by a total of 52 bits. Normally, all of the LSB (the bits after the EPN bits) are set to 0. Exclusion ranges may be carved out of large size pages in units of 4 KB, i.e., when TLBentry[X] bit  is 1, the total memory excluded from the effective page is 4 KB*((value of Exclusion range bits )+1). When the exclusion bit is set to 1 (X=1), even if the LSBs in the virtual page number are set to 0, a 4 KB page is still excluded from a large size page.","A 64 KB page only requires bits 0:47 within the EPN field to be set for the TLB search logic device  to find a matching value in the TLB . An exclusion range within the 64 KB page can be provided by setting LSBs 48:51 to any value except all \u20181\u2019s. Note that the only page size smaller than 64 KB is 4 KB. One or more 4 KB pages can be mapped by software into the excluded memory region covered by the 64 KB page when the TLBentry[X] (exclusion) bit is set to 1. When the TLB search logic device  maps a virtual address to a physical address and the TLB exclusion bit is also set to 1, the TLB search logic device  will return a physical address that maps to the 64 KB page outside the exclusion range. If the TLB exclusion bit is set to 0, the TLB search logic device  will return a physical address that maps to the whole area of the 64 KB page.","An application or the operating system may access the non excluded region within a page when the \u2018exclusion bit\u2019  is set to 1. When this occurs, the TLB search logic device  uses the MSB to map the virtual address to a physical address that corresponds to an area within the non excluded region of the page. When the \u2018exclusion bit\u2019  is set to 0, then the TLB search logic device  uses the MSB to map the virtual address to a physical address that corresponds to a whole page.","In one embodiment of the invention, the size of the exclusion range is configurable to M\u00d74 KB, where M=1 to (TLB entry page size in bytes\/2)\u22121. The smallest possible exclusion range is 4 KB, and successively larger exclusion ranges are multiples of 4 KB. In another embodiment of the invention, such as in the A2 core, for simplicity, M is further restricted to 2, where n=0 to log(TLB entry page size)\u221213, i.e., the possible excluded ranges are 4 KB, 8 KB, 16 KB, up to (page size)\/2. Additional TLB entries may be mapped into the exclusion range. Pages mapped into the exclusion range cannot overlap and pages mapped in the exclusion range must be collectively fully contained within the exclusion range. The pages mapped into the exclusion range are known as subpages.","Once a TLB page table entry has been deleted from the TLB  by the operating system, the corresponding memory indicated by the TLB page table entry becomes available to store new or additional pages and subpages. TLB page table entries are generally deleted when their corresponding applications or processes are terminated by the operating system.","FIG. -- is an example of how page table entries are created in a TLB  in accordance with the prior art. For simplification purposes only, the example assumes that only two page sizes, 64 KB and 1 MB are allowable. Under the prior art, once a 64 KB page is created in a 1 MB page, only additional 64 KB page entries may be used to map the remaining virtual address in the 1 MB page until a contiguous 1 MB area of memory is filled. This requires a total of 16 page table entries, i.e., , to in the TLB .","FIG. -- is an example of how page table entries are created in a TLB  in accordance with the present invention. Different size pages may be used next to one another. For example, PTE  is a 64 KB page table entry and PTE  is a 1 MB page table entry. In one embodiment, PTE  has a 64 KB \u2018exclusion range\u2019  excluded from the base corresponding to the area occupied by PTE . The use of an exclusion range allows the 1 MB memory space to be covered by only 2 page table entries in the TLB , whereas in FIG. -- sixteen page table entries were required to cover the same range of memory. In one embodiment, when the \u2018exclusion bit\u2019 is set, the first 64 KB of the 1 MB page specified by PTE  will not match the virtual address, i.e., this area is excluded. In other embodiments of the invention, the excluded range may begin at the top of the page.","Referring now to FIG. --, there is shown the overall architecture of a multiprocessor compute node  implemented in a parallel computing system in which the present invention may be implemented. In one embodiment, the multiprocessor system implements a BLUEGENE\u2122 torus interconnection network, which is further described in the journal article \u2018Blue Gene\/L torus interconnection network\u2019 N. R. Adiga, et al., IBM J. Res. & Dev. Vol. 49, 2005, the contents of which are incorporated by reference in its entirety. Although the BLUEGENE\u2122\/L torus architecture comprises a three-dimensional torus, it is understood that the present invention also functions in a five-dimensional torus, such as implemented in the BLUEGENE\u2122\/Q massively parallel computing system comprising compute node ASICs (BQC), each compute node including multiple processor cores.","In this embodiment, the compute node  is a single chip (\u2018nodechip\u2019) based on low power A2 PowerPC cores, though the architecture can use any low power cores, and may comprise one or more semiconductor chips. In the embodiment depicted, the node includes 16 PowerPC A2 cores running at 1600 MHz.","More particularly, the basic compute node  of the massively parallel supercomputer architecture illustrated in FIG. -- includes in one embodiment seventeen (16+1) symmetric multiprocessing (PPC) cores , each core being 4-way hardware threaded and supporting transactional memory and thread level speculation, including a memory management unit (MMU)  and Quad Floating Point Unit (FPU)  on each core (204.8 GF peak node). In one implementation, the core operating frequency target is 1.6 GHz providing, for example, a 563 GB\/s bisection bandwidth to shared L2 cache  via a full crossbar switch . In one embodiment, there is provided 32 MB of shared L2 cache , each core having an associated 2 MB of L2 cache . There is further provided external DDR SDRAM (i.e., Double Data Rate synchronous dynamic random access) memory , as a lower level in the memory hierarchy in communication with the L2. In one embodiment, the node includes 42.6 GB\/s DDR3 bandwidth (1.333 GHz DDR3) (2 channels each with chip kill protection).","Each MMU  receives data accesses and instruction accesses from their associated processor cores  and retrieves information requested by the core  from memory such as the L1 cache , L2 cache , external DDR3 , etc.","Each FPU  associated with a core  has a 32B wide data path to the L1-cache , allowing it to load or store 32B per cycle from or into the L1-cache . Each core  is directly connected to a prefetch unit (level-1 prefetch, L1P) , which accepts, decodes and dispatches all requests sent out by the core . The store interface from the core  to the L1P  is 32B wide and the load interface is 16B wide, both operating at the processor frequency. The L1P  implements a fully associative, 32 entry prefetch buffer. Each entry can hold an L2 line of 328B size. The L1P provides two prefetching schemes for the prefetch unit : a sequential prefetcher as used in previous BLUEGENE\u2122 architecture generations, as well as a list prefetcher. The prefetch unit is further disclosed in U.S. patent application Ser. No. 11\/767,717, which is incorporated by reference in its entirety.","As shown in FIG. --, the 32 MB shared L2 is sliced into 16 units, each connecting to a slave port of the switch . Every physical address is mapped to one slice using a selection of programmable address bits or a XOR-based hash across all address bits. The L2-cache slices, the L1Ps and the L1-D caches of the A2s are hardware-coherent. A group of 4 slices is connected via a ring to one of the two DDR3 SDRAM controllers .","By implementing a direct memory access engine referred to herein as a Messaging Unit, \u2018MU\u2019 such as MU , with each MU including a DMA engine and a Network Device  in communication with the crossbar switch , chip I\/O functionality is provided. In one embodiment, the compute node further includes, in a non-limiting example: 10 intra-rack interprocessor links , each operating at 2.0 GB\/s, i.e., 10*2 GB\/s intra-rack & inter-rack (e.g., configurable as a 5-D torus in one embodiment); and, one I\/O link  interfaced with the MU  at 2.0 GB\/s (2 GB\/s I\/O link (to I\/O subsystem)) is additionally provided. The system node  employs or is associated and interfaced with an 8-16 GB memory\/node (not shown).","Although not shown, each A2 processor core  has associated a quad-wide fused multiply-add SIMD floating point unit, producing 8 double precision operations per cycle, for a total of 328 floating point operations per cycle per compute node. A2 is a 4-way multi-threaded 64b PowerPC implementation. Each A2 processor core  has its own execution unit (XU), instruction unit (IU), and quad floating point unit (QPU) connected via the AXU (Auxiliary eXecution Unit). The QPU is an implementation of the 4-way SIMD QPX floating point instruction set architecture. QPX is an extension of the scalar PowerPC floating point architecture. It defines 32 32B-wide floating point registers per thread instead of the traditional 32 scalar 8B-wide floating point registers.","FIG. -- is an overview of the A2 processor core organization. The A2 core includes a concurrent-issue instruction fetch and decode unit with attached branch unit, together with a pipeline for complex integer, simple integer, and load\/store operations. The A2 core also includes a memory management unit (MMU); separate instruction and data cache units; Pervasive and debug logic; and timer facilities.","The instruction unit of the A2 core fetches, decodes, and issues two instructions from different threads per cycle to any combination of the one execution pipeline and the AXU interface (see \u201cExecution Unit\u201d below, and Auxiliary Processor Unit (AXU) Port on page 49). The instruction unit includes a branch unit which provides dynamic branch prediction using a branch history table (BHT). This mechanism greatly improves the branch prediction accuracy and reduces the latency of taken branches, such that the target of a branch can usually be run immediately after the branch itself, with no penalty.","The A2 core contains a single execution pipeline. The pipeline consists of seven stages and can access the five-ported (three read, two write) GPR file. The pipeline handles all arithmetic, logical, branch, and system management instructions (such as interrupt and TLB management, move to\/from system registers, and so on) as well as arithmetic, logical operations and all loads, stores and cache management operations. The pipelined multiply unit can perform 32-bit\u00d732-bit multiply operations with single-cycle throughput and single-cycle latency. The width of the divider is 64 bits. Divide instructions dealing with 64 bit operands recirculate for 65 cycles, and operations with 32 bit operands recirculate for 32 cycles. No divide instructions are pipelined, they all require some recirculation. All misaligned operations are handled in hardware, with no penalty on any operation which is contained within an aligned 32-byte region. The load\/store pipeline supports all operations to both big endian and little endian data regions.","The A2 core provides separate instruction and data cache controllers and arrays, which allow concurrent access and minimize pipeline stalls. The storage capacity of the cache arrays 16 KB each. Both cache controllers have 64-byte lines, with 4-way set-associativity I-cache and 8-way set-associativity D-cache. Both caches support parity checking on the tags and data in the memory arrays, to protect against soft errors. If a parity error is detected, the CPU will force a L1 miss and reload from the system bus. The A2 core can be configured to cause a machine check exception on a D-cache parity error. The PowerISA instruction set provides a rich set of cache management instructions for software-enforced coherency.","The ICC delivers up to four instructions per cycle to the instruction unit of the A2 core. The ICC also handles the execution of the PowerISA instruction cache management instructions for coherency.","The DCC handles all load and store data accesses, as well as the PowerISA data cache management instructions. All misaligned accesses are handled in hardware, with cacheable load accesses that are contained within a double quadword (32 bytes) being handled as a single request and with cacheable store or caching inhibited loads or store accesses that are contained within a quadword (16 bytes) being handled as a single request. Load and store accesses which cross these boundaries are broken into separate byte accesses by the hardware by the micro-code engine. When in 32 Byte store mode, all misaligned store or load accesses contained within a double quadword (32 bytes) are handled as a single request. This includes cacheable and caching inhibited stores and loads. The DCC interfaces to the AXU port to provide direct load\/store access to the data cache for AXU load and store operations. Such AXU load and store instructions can access up to 32 bytes (a double quadword) in a single cycle for cacheable accesses and can access up to 16 bytes (a quadword) in a single cycle for caching inhibited accesses. The data cache always operates in a write-through manner. The DCC also supports cache line locking and \u201ctransient\u201d data via way locking. The DCC provides for up to eight outstanding load misses, and the DCC can continue servicing subsequent load and store hits in an out-of-order fashion. Store-gathering is not performed within the A2 core.","The A2 Core supports a flat, 42-bit (4 TB) real (physical) address space. This 42-bit real address is generated by the MMU, as part of the translation process from the 64-bit effective address, which is calculated by the processor core as an instruction fetch or load\/store address. Note: In 32-bit mode, the A2 core forces bits 0:31 of the calculated 64-bit effective address to zeroes. Therefore, to have a translation hit in 32-bit mode, software needs to set the effective address upper bits to zero in the ERATs and TLB. The MMU provides address translation, access protection, and storage attribute control for embedded applications. The MMU supports demand paged virtual memory and other management schemes that require precise control of logical to physical address mapping and flexible memory protection. Working with appropriate system level software, the MMU provides the following functions:\n\n","The translation lookaside buffer (TLB) is the primary hardware resource involved in the control of translation, protection, and storage attributes. It consists of 512 entries, each specifying the various attributes of a given page of the address space. The TLB is 4-way set associative. The TLB entries may be of type direct (IND=0), in which case the virtual address is translated immediately by a matching entry, or of type indirect (IND=1), in which case the hardware page table walker is invoked to fetch and install an entry from the hardware page table.","The TLB tag and data memory arrays are parity protected against soft errors; if a parity error is detected during an address translation, the TLB and ERAT caches treat the parity error like a miss and proceed to either reload the entry with correct parity (in the case of an ERAT miss, TLB hit) and set the parity error bit in the appropriate FIR register, or generate a TLB exception where software can take appropriate action (in the case of a TLB miss).","An operating system may choose to implement hardware page tables in memory that contain virtual to logical translation page table entries (PTEs) per Category E.PT. These PTEs are loaded into the TLB by the hardware page table walker logic after the logical address is converted to a real address via the LRAT per Category E.HV.LRAT. Software must install indirect (IND=1) type TLB entries for each page table that is to be traversed by the hardware walker. Alternately, software can manage the establishment and replacement of TLB entries by simply not using indirect entries (i.e. by using only direct IND=0 entries). This gives system software significant flexibility in implementing a custom page replacement strategy. For example, to reduce TLB thrashing or translation delays, software can reserve several TLB entries for globally accessible static mappings. The instruction set provides several instructions for managing TLB entries. These instructions are privileged and the processor must be in supervisor state in order for these instructions to be run.","The first step in the address translation process is to expand the effective address into a virtual address. This is done by taking the 64-bit effective address and prepending to it a 1-bit \u201cguest state\u201d (GS) identifier, an 8-bit logical partition ID (LPID), a 1-bit \u201caddress space\u201d identifier (AS), and the 14-bit Process identifier (PID). The 1-bit \u201cindirect entry\u201d (IND) identifier is not considered part of the virtual address. The LPID value is provided by the LPIDR register, and the PID value is provided by the PID register.","The GS and AS identifiers are provided by the Machine State Register which contains separate bits for the instruction fetch address space (MACHINE STATE REGISTER[S]) and the data access address space (MACHINE STATE REGISTER[DS]). Together, the 64-bit effective address, and the other identifiers, form an 88-bit virtual address. This 88-bit virtual address is then translated into the 42-bit real address using the TLB.","The MMU divides the address space (whether effective, virtual, or real) into pages. Five direct (IND=0) page sizes (4 KB, 64 KB, 1 MB, 16 MB, 1 GB) are simultaneously supported, such that at any given time the TLB can contain entries for any combination of page sizes. The MMU also supports two indirect (IND=1) page sizes (1 MB and 256 MB) with associated sub-page sizes. In order for an address translation to occur, a valid direct entry for the page containing the virtual address must be in the TLB. An attempt to access an address for which no TLB direct exists results in a search for an indirect TLB entry to be used by the hardware page table walker. If neither a direct or indirect entry exists, an Instruction (for fetches) or Data (for load\/store accesses) TLB Miss exception occurs.","To improve performance, both the instruction cache and the data cache maintain separate \u201cshadow\u201d TLBs called ERATs. The ERATs contain only direct (IND=0) type entries. The instruction I-ERAT contains 16 entries, while the data D-ERAT contains 32 entries. These ERAT arrays minimize TLB contention between instruction fetch and data load\/store operations. The instruction fetch and data access mechanisms only access the main unified TLB when a miss occurs in the respective ERAT. Hardware manages the replacement and invalidation of both the I-ERAT and D-ERAT; no system software action is required in MMU mode. In ERAT-only mode, an attempt to access an address for which no ERAT entry exists causes an Instruction (for fetches) or Data (for load\/store accesses) TLB Miss exception.","Each TLB entry provides separate user state and supervisor state read, write, and execute permission controls for the memory page associated with the entry. If software attempts to access a page for which it does not have the necessary permission, an Instruction (for fetches) or Data (for load\/store accesses) Storage exception will occur.","Each TLB entry also provides a collection of storage attributes for the associated page. These attributes control cache policy (such as cachability and write-through as opposed to copy-back behavior), byte order (big endian as opposed to little endian), and enabling of speculative access for the page. In addition, a set of four, user-definable storage attributes are provided. These attributes can be used to control various system level behaviors.","L2 Cache","The 32 MB shared L2 () is sliced into 16 units, each connecting to a slave port of the switch. Every physical address is mapped to one slice using a selection of programmable address bits or a XOR-based hash across all address bits. The L2-cache slices, the L1Ps and the L1-D caches of the A2s are hardware-coherent. A group of 4 slices is connected via a ring to one of the two DDR3 SDRAM controllers. Each of the four rings is 16B wide and clocked at half processor frequency. The SDRAM controllers drive each a 16B wide SDRAM port at 1333 or 1600 Mb\/s\/pin. The SDRAM interface uses an ECC across 64B with chip-kill correct capability as will be explained in greater detail herein below. Both the chip-kill capability and direct soldered DRAMs and enhanced error correction codes, are used to achieve ultra reliability targets.","The BGQ Compute ASIC incorporates support for thread-level speculative execution (TLS). This support utilizes the L2 cache to handle multiple versions of data and detect memory reference patterns from any core that violates sequential consistency. The L2 cache design tracks all loads to cache a cache line and checks all stores against these loads. This BGQ compute ASIC has up to 32 MB of speculative execution state storage in L2 cache. The design supports for the following speculative execution mechanisms. If a core is idle and the system is running in a speculative mode, the target design provides a low latency mechanism for the idle core to obtain a speculative work item and to cancel that work and invalidate its internal state and obtain another available speculative work item if sequential consistency is violated. Invalidating internal state is extremely efficient: updating a bit in a table that indicates that the thread ID is now in the \u201cInvalid\u201d state. Threads can have one of four states: Primary non-speculative; Speculative, valid and in progress; Speculative, pending completion of older dependencies before committing; and Invalid, failed.","In one embodiment, there is allowed out of order issuance of store instructions and process the store instructions in a parallel computing system without using an msync instruction as is done in the art.","FIG. -- illustrates a computing node  of a parallel computing system (e.g., IBM\u00ae Blue Gene\u00ae L\/P\/Q, etc.) in one embodiment. The computing node  includes, but is not limited to: a plurality of processor cores (e.g., a processor core ), a plurality of local cache memory devices (e.g., L1 (Level 1) cache memory device ) associated with the processor cores, a plurality of first request queues (not shown) located at output ports of the processor cores, a plurality of second request queues (e.g., FIFOs (First In First Out queues)  and ) associated with the local cache memory devices, a plurality of shared cache memory devices (e.g., L2 (Level 2) cache memory device ), a plurality of third request queues (e.g., FIFOs  and ) associated with the shared cache memory devices, a messaging unit (MU)  that includes DMA capability, at least one fourth request queue (e.g., FIFO ) associated with the messaging unit , and a switch  connecting the FIFOs. A processor core may be a single processor unit such as IBM\u00ae PowerPC\u00ae or Intel\u00ae Pentium. There may be at least one local cache memory device per a processor core. In a further embodiment, a processor core may include at least one local cache memory device. A request queue includes load instructions (i.e., instructions for loading a content of a memory location to a register) and store instructions and other requests (e.g., prefetch request). A request queue may be implemented as an FIFO (First In First Out) queue. Alternatively, a request queue is implemented as a memory buffer operating (i.e., inputting and outputting) out-of-order (i.e., operating regardless of an order). In a further embodiment, a local cache memory device (e.g., L1 cache memory device ) includes at least two second request queues (e.g., FIFOs  and ). An FIFO (First In First Out) is a storage device that holds requests (e.g., load instructions and\/store instructions) and coherence management operation (e.g., an operation for invalidating speculative and\/or invalid data stored in a local cache memory device associated with that FIFO). A shared cache memory device may include third request queues (e.g., FIFOs  and ). In a further embodiment, the messaging unit (MU)  is a processing core that does not include a local cache memory device. The messaging unit  is described in detail below in conjunction with FIGS. -----. In one embodiment, the switch  implemented as a crossbar switch. The switch may be implemented as an optical and reconfigurable crossbar switch. In one embodiment, the switch is unbuffered, i.e., the switch cannot store requests (e.g., load and store instructions) or invalidations (i.e., operations or instructions for invalidating of requests or data) but transfer these requests and invalidations in a predetermined amount of cycles between processor cores. In an alternative embodiment, the switch  includes at least one internal buffer that may hold the requests and coherence management operations (e.g., an operation invalidating a request and\/or data). The buffered switch  can hold the requests and operations for a period time (e.g., 1,000 clock cycles) even without a limit of how long the switch  can hold the requests and operations.","In FIG. --, an arrow labeled Ld\/St (Load\/Store) (e.g., an arrow ) is a request from a processor core to the at least one shared cache memory device (e.g., L2 cache memory device ). The request includes, but is not limited to: a load instruction, a store instruction, a prefetch request, an atomic update (e.g., an operation for updating registers), cache line locking, etc. An arrow labeled Inv (e.g., an arrow ) is a coherence management operation that invalidates data in the at least one local cache memory device (e.g., L1 cache memory device ). The coherence management operation includes, but is not limited to: an ownership notification (i.e., a notification claiming an ownership of a datum held in the at least one local cache memory device), a flush request (i.e., a request draining a queue), etc.","FIG. -- illustrates a flow chart describing method steps for processing at least one store instruction in one embodiment. The computing node  allows out-of-order issuances of store instructions by processing cores and\/or guarantees in-order processing the issued store instructions, e.g., by running method steps - in FIG. --. At step , a processor core of a computing node issues a store instruction. At step , the processor core updates the shared cache memory device  according to the issued store instruction. For example, the processor core overwrites data in a certain cache line of the shared cache memory device  which corresponds to a memory address or location included in the store instruction. At step , processor core sets a flag bit on data in the shared cache memory device  updated by the store instruction. In this embodiment, the flag bit indicates whether corresponding data is valid or not. In a further embodiment, a position of flag bit in data is pre-determined. At step , the MU  looks at the flag bit based on a memory location or address specified in the store instruction, validates the updated data if determined that the flag bit on the updated data is set, and sends the updated data to other processor cores or other computing nodes that the MU does not belong to. In one embodiment, the MU  monitors load instructions and store instructions issued by processor cores, e.g., by accessing an instruction queue.","In one embodiment, a processor core issued the store instruction is a producer (i.e., a component producing or generating data). That processor core hands off the produced or generated data to, e.g., a register in, the MU  (FIGS. -----) which is another processor core having no local cache memory device. Thus, in this embodiment, the MU  is a consumer (i.e., a component receiving data from the producer).","In one embodiment, other processor cores access the updated data upon seeing the flag bit set, e.g., by accessing the updated data by using a load instruction specifying a memory location of the updated data. The store instruction may be a guarded store instruction or an unguarded store instruction. The guarded store instruction is not processed speculatively and\/or run when its operation is guaranteed safe. The unguarded store instruction is processed speculatively and\/or assumes no side effect (e.g., speculatively overwriting data in a memory location does not affect a true output) in accessing the shared cache memory device . The parallel computing system run the method steps - without an assistance of a synchronization instruction (e.g., mysnc instruction).","FIG. -- illustrates a flow chart for processing at least one store instruction in a parallel computing system in one embodiment. The parallel computing system may include a plurality of computing nodes. A computing node may include a plurality of processor cores and at least one shared cache memory device. The computing node allows out-of-order issuances of store instructions by processing cores and\/or guarantees in-order processing of the issued store instructions, e.g., by running method steps - in FIG. --. A first processor core (e.g., a processor core  in FIGS. -----) may include at least one local cache memory device. At step , a processor core issues a store instruction. At step , a first request queue associated with the processor core receives and stores the issued store instruction. In one embodiment, the first request queue is located at an output port of the first processor core. At step , a second request queue, associated with at least one local cache memory device of the first processor core, receives and stores the issued store instruction from the first processor core. In one embodiment, the second request queue is an internal queue or buffer of the at least one local cache memory device . The first processor core updates data in its local cache memory device  (i.e., the at least one local cache memory device of the first processor core) according to the store instruction. At step , a third request queue, associated with the shared cache memory device, receives and stores the store instruction from the first processor core, the first request queue or the second request queue. In one embodiment, the third request queue is an internal queue or buffer of the shared cache memory device .","At step  in FIG. --, the first processor core invalidates data, e.g., by unsetting a valid bit associated with that data, in the shared cache memory device  associated with the store instruction. The first processor core may also invalidate data, e.g., by unsetting a valid bit associated with that data, in other local cache memory device(s) of other processor core(s) associated with the store instruction. At step , the first processor core flushes the first request queue. The first processor does not flush other request queues. Thus, the parallel computing system allows the other request queues (i.e., request queues not flushed) to hold invalid requests (e.g., invalid store or load instruction). In this embodiment described in FIG. --, the processor cores and MU  do not use a synchronization instruction (e.g., msync instruction issued by a processor core) to process store instructions. The synchronization instruction may flush all the queues.","In a further embodiment, a fourth request queue, associated with the MU , also receives and stores the issued store instruction. The first processor may not flush this fourth request queue when flushing the first request queue. The synchronization instruction issued by a processor core may flush this fourth request queue when flushing all other request queues.","In a further embodiment, the first, second, third and forth request queues concurrently receive the issued store instruction from the first processor core. Alternatively, the first, second, third and fourth request queues receive the issued store instruction in a sequential order.","In a further embodiment, some of the method steps described in FIG. -- runs concurrently. The method steps described in FIG. -- does not need to run sequentially as depicted in FIG. --.","In one embodiment, the method steps in FIGS. -- and -- are implemented in hardware or reconfigurable hardware, e.g., FPGA (Field Programmable Gate Array) or CPLD (Complex Programmable Logic Device), using a hardware description language (Verilog, VHDL, Handel-C, or System C). In another embodiment, the method steps in FIGS. -- and -- are implemented in a semiconductor chip, e.g., ASIC (Application-Specific Integrated Circuit), using a semi-custom design methodology, i.e., designing a chip using standard cells and a hardware description language. Thus, the hardware, reconfigurable hardware or the semiconductor chip operates the method steps described in FIGS. -- and --.","Generally, in field of synchronizing memory accesses in a multi-processor, parallel computing system parallel computing, application programs are split into \u201cthreads\u201d that can run \u201cspeculatively\u201d in parallel. The terms \u201cspeculative,\u201d \u201cspeculatively,\u201d \u201cexecution\u201d and \u201cspeculative execution\u201d as used herein are terms of art that do not imply mental steps or manual operation. Instead, they refer to computer processors running segments of code automatically. Some segments of code are known as \u201cthreads.\u201d If the execution of code is \u201cspeculative,\u201d this means that the thread is run in the computer as a sort of gamble. The gamble is that any given thread will be able to do something meaningful without altering data after some other thread altering the same data in a way that would make results from the given thread invalid. All of the operations are undertaken within the hardware on an automated basis.","There is further provided an instruction set and supporting hardware for a multiprocessor system that support speculative execution by improving synchronization of memory accesses.","Advantageously, a multiprocessor system will include a special msync unit for supporting memory synchronization requests. This unit will have a mechanism for keeping track of generations of requests and for delaying requests that exceed a maximum count of generations in flight.","Advantageously, also various different levels or methods of memory synchronization will be supported responsive to the msync unit.","The following description mentions a number of instruction and function names such as \u201cmsync,\u201d \u201chwsync,\u201d \u201clwsync,\u201d and \u201ceieio;\u201d \u201cTLBsync,\u201d \u201cMbar,\u201d \u201cfull sync,\u201d \u201cnon-cumulative barrier,\u201d \u201cproducer sync,\u201d \u201cgeneration change sync,\u201d \u201cproducer generation change sync,\u201d \u201cconsumer sync,\u201d and \u201clocal barrier,\u201d These names are arbitrary and for convenience of understanding. An instruction might equally well be given any name as a matter of preference without altering the nature of the instruction or without taking the instruction or the hardware supporting it outside of the scope of the claims.","Generally implementing an instruction will involve creating specific computer hardware that will cause the instruction to run when computer code requests that instruction. The field of Application Specific Integrated Circuits (\u201cASIC\u201ds) is a well-developed field that allows implementation of computer functions responsive to a formal specification. Accordingly, no specific implementation will be discussed here. Instead the functions of instructions and units will be discussed.","As described herein, the use of the letter \u201cB\u201d represents a Byte quantity, e.g., 2B, 8.0B, 32B, and 64B represent Byte units. Recitations \u201cGB\u201d represent Gigabyte quantities. Throughout this disclosure a particular embodiment of a multi-processor system will be discussed. This embodiment includes various numerical values for numbers of components, bandwidths of interfaces, memory sizes and the like. These numerical values are not intended to be limiting, but only examples. One of ordinary skill in the art might devise other examples as a matter of design choice.",{"@attributes":{"id":"p-0551","num":"0575"},"figref":"FIG. 1-0","b":"50"},"The compute node  is a single chip (\u201cnodechip\u201d) is based on low power A2 PowerPC cores, though any compatible core might be used. While the commercial embodiment is built around the PowerPC architecture, the invention is not limited to that architecture. In the embodiment depicted, the node includes 17 cores , each core being 4-way hardware threaded. There is a shared L2 cache  accessible via a full crossbar switch , the L2 including 16 slices . There is further provided external memory , in communication with the L2 via DDR-3 controllers \u2014DDR being an acronym for Double Data Rate.","A messaging unit (\u201cMU\u201d)  includes a direct memory access (\u201cDMA\u201d) engine, a network interface , a Peripheral Component Interconnect Express (\u201cPCIe\u201d) unit. The MU is coupled to interprocessor links  and i\/o link .","Each FPU  associated with a core  has a data path to the L1-data cache . Each core  is directly connected to a supplementary processing agglomeration , which includes a private prefetch unit. For convenience, this agglomeration  will be referred to herein as \u201cUP\u201d\u2014meaning level 1 prefetch\u2014or \u201cprefetch unit;\u201d but many additional functions are lumped together in this so-called prefetch unit, such as write combining. These additional functions could be illustrated as separate modules, but as a matter of drawing and nomenclature convenience the additional functions and the prefetch unit will be illustrated herein as being part of the agglomeration labeled \u201cUP.\u201d This is a matter of drawing organization, not of substance. Some of the additional processing power of this L1P group is shown in FIGS. -- and --. The L1P group also accepts, decodes and dispatches all requests sent out by the core .","In this embodiment, the L2 Cache units provide the bulk of the memory system caching. Main memory may be accessed through two on-chip DDR-3 SDRAM memory controllers , each of which services eight L2 slices.","To reduce main memory accesses, the L2 advantageously serves as the point of coherence for all processors within a nodechip. This function includes generating L1 invalidations when necessary. Because the L2 cache is inclusive of the L1s, it can remember which processors could possibly have a valid copy of every line, and can multicast selective invalidations to such processors. In the current embodiment the prefetch units and data caches can be considered part of a memory access pathway.","FIG. -- shows features of the control portion of an L2 slice. Broadly, this unit includes coherence tracking at , a request queue at , a write data buffer at , a read return buffer at , a directory pipe , EDRAM pipes , a reservation table , and a DRAM controller. The functions of these elements are explained in more detail in US provisional patent application Ser. No. 61\/299,911 filed Jan. 29, 2010, which is incorporated herein by reference.","The units  and  have outputs relevant to memory synchronization, as will be discussed further below with reference to FIG. --B.","FIG. --A shows a simple example of a producer thread \u03b1 and a consumer thread \u03b2. In this example, \u03b1 seeks to do a double word write . After the write is finished, it sets a 1 bit flag , also known as a guard location. In parallel, \u03b2 reads the flag . If the flag is zero, it keeps reading . If the flag is not zero, it again reads the flag . If the flag is one, it reads data written by \u03b1.","FIG. -- shows conceptually where delays in the system can cause problems with this exchange. Thread \u03b1 is running on a first core\/L1 group . Thread \u03b2 is running on a second core\/L1 group . Both of these groups will have a copy of the data and flag relating to the thread in their L1D caches. When a does the data write, it queues a memory access request at , which passes through the crossbar switch  and is hashed to a first slice  of the L2, where it is also queued at  and eventually stored.","The L2, as point of coherence, detects that the copy of the data resident in the L1D for thread \u03b2 is invalid. Slice  therefore queues an invalidation signal to the queue  and then, via the crossbar switch, to the queue  of core\/L1 group .","When \u03b1 writes the flag, this again passes through queue  to the crossbar switch , but this time the write is hashed to the queue  of a second slice  of the L2. This flag is then stored in the slice and queued at  to go to through the crossbar  to queue  and then to the core\/L1 group . In parallel, thread \u03b2, is repeatedly scanning the flag in its own L1D.","Traditionally, multiprocessor systems have used consistency models called \u201csequential consistency\u201d or \u201cstrong consistency\u201d. Pursuant to this type of model, if unit  first writes data and then writes the flag, this implies that if the flag has changed, then the data has also changed. It is not possible for the flag to be changed before the data. The data change must be visible to the other threads before the flag changes. This sequential model has the disadvantage that threads are kept waiting, sometimes unnecessarily, slowing processing.","To speed processing, PowerPC architecture uses a \u201cweakly consistent\u201d memory model. In that model, there is no guarantee whatsoever what memory access request will first result in a change visible to all threads. It is possible that \u03b2 will see the flag changing, and still not have received the invalidation message from slice , so \u03b2 may still have old data in its L1D.","To prevent this unfortunate result, the PowerPC programmer can insert msync instructions  and  as shown in FIG. --B. This will force a full sync, or strong consistency, on these two threads, with respect to this particular data exchange. In PowerPC architecture, if a core executes an msync, it means that all the writes that have happened before the msync are visible to all the other cores before any of the memory operations that happened after the msync will be seen. In other words, at the point of time when the msync completes, all the threads will see the new write data. Then the flag change is allowed to happen. In other words, until the invalidation goes back to group , the flag cannot be set.","In accordance with the embodiment disclosed herein, to support concurrent memory synchronization instructions, requests are tagged with a global \u201cgeneration\u201d number. The generation number is provided by a central generation counter. A core executing a memory synchronization requests the central unit to increment the generation counter and then waits until all memory operations of the previously current generation and all earlier generations have completed.","A core's memory synchronization request is complete when all requests that were in flight when the request began have completed. In order to determine this, the L1P monitors a reclaim pointer that will be discussed further below. Once it sees the reclaim pointer moving past the generation that was active at the point of the start of the memory synchronization request, then the memory synchronization request is complete.","FIG. --A shows a view of the memory synchronization central unit. In the current embodiment, the memory synchronization generation counter unit  is a discrete unit placed relatively centrally in the chip , close to the crossbar switch . It has a central location as it needs short distances to a lot of units. L1P units request generation increments, indicate generations in flight, and receive indications of generations completed. The L2's provide indications of generations in flight. The OR-tree  receives indications of generations in flight from all units queuing memory access requests, Tree  is a distributed structure. Its parts are scattered across the entire chip, coupled with the units that are queuing the memory access requests. The components of the OR reduce tree are a few OR gates at every fork of the tree. These gates are not inside any unit. Another view of the OR reduce tree is discussed with respect to FIG. --, below.","A number of units within the nodechip queue memory access requests, these include:","L1P","L2","DMA","PCIe","Every such unit can contain some aspect of a memory access request in flight that might be impacted by a memory synchronization request. FIG. --B shows an abstracted view of one of these units at , a generic unit that issues or processes memory requests via a queue. Each such unit includes a queue  for receiving and storing memory requests. Each position in the queue includes bits  for storing a tag that is a three bit generation number. Each of the sets of three bits is coupled to a three-to-eight binary decoder . The outputs of the binary decoders are OR-ed bitwise at  to yield the eight bit output vector , which then feeds the OR-reduce tree of FIG. --. A clear bit in the output vector means that no request associated with that generation is in flight. Core queues are flushed prior to the start of the memory synchronization request and therefore do not need to be tagged with generations. The L1D need not queue requests and therefore may not need to have the unit of FIG. --B.","The global OR tree  per FIG. -- receives\u2014from all units  issuing and queuing memory requests\u2014an eight bit wide vector , per FIG. --BB at . Each bit of the vector indicates for one of the eight generations whether this unit is currently holding any request associated with that generation. The numbers 3, 2, and 2 in units  indicate that a particular generation number is in flight in the respective unit. This generation number is shown as a bit within vectors . While the present embodiment has 8 bit vectors, more or less bits might be used by the designer as needed for particular applications. FIG. -- actually shows these vectors as having more than eight bits, based on the ellipsis and trailing zeroes. This is an alternative embodiment. The Global OR tree reduces each bit of the vector individually, creating one resulting eight bit wide vector , each bit of which indicates if any request of the associated generation is in flight anywhere in the node. This result is sent to the global generation counter  and thence broadcasted to all core units , as shown in FIG. -- and also at  of FIG. --. FIG. -- is a simplified figure. The actual OR gates are not shown and there would, in the preferred embodiment, be many more than three units contributing to the OR reduce tree.","Because the memory subsystem has paths\u2014especially the crossbar\u2014through which requests pass without contributing to the global OR reduce tree of FIG. --, the memory synchronization exit condition is a bit more involved. All such paths have a limited, fixed delay after which requests are handed over to a unit  that contributes to the global OR. Compensating for such delays can be done in several alternative ways. For instance, if the crossbar has a delay of six cycles, the central unit can wait six cycles after disappearance of a bit from the OR reduce tree, before concluding that the generation is no longer in flight. Alternatively, the L1P might keep the bit for that generation turned on during the anticipated delay.","Memory access requests tagged with a generation number may be of many types, including:\n\n","The memory synchronization unit  shown in FIG. -- allows grouping of memory accesses into generations and enables ordering by providing feedback when a generation of accesses has completed. The following functions are implemented in FIG. --:\n\n","In the current embodiment, the generation counter is used to determine whether a requested generation change is complete, while the reclaim pointer is used to infer what generation has completed.","The module  of FIG. -- broadcasts via  a signal defining the current generation number to all memory synchronization interface units, which in turn tag their accesses with that number. Each memory subsystem unit that may hold such tagged requests flags per FIG. --B for each generation whether it holds requests for that particular generation or not.","For a synchronization operation, a unit can request an increment of the current generation and wait for previous generations to complete.","The central generation counter uses a single counter  to determine the next generation. As this counter is narrow, for instance 3 bits wide, it wraps frequently, causing the reuse of generation numbers. To prevent using a number that is still in flight, there is a second, reclaiming counter  of identical width that points to the oldest generation in flight. This counter is controlled by a track and control unit  implemented within the memory synchronization unit. Signals from the msync interface unit, discussed with reference to FIGS. -- and -- below, are received at . These include requests for generation change.","FIG. -- illustrates conditions under which the generation counter may be incremented and is part of the function of the track and control unit . At  it is tested whether a request to increment is active and the request specifies the current value of the generation counter plus one. If not, the unit must wait at . If so, the unit tests at  whether the reclaim pointer is equal to the current generation pointer plus one. If so, again the unit must wait and retest in accordance with . If not, it is tested at  whether the generation counter has been incremented in the last two cycles, if so, the unit must wait at . If not, the generation counter may be incremented at .","The generation counter can only advance if doing so would not cause it to point to the same generation as the reclaim pointer per in the next cycle. If the generation counter is stalled by this condition, it can still receive incoming memory synchronization requests from other cores and process them all at once by broadcasting the identical grant to all of them, causing them all to wait for the same generations to clear. For instance, all requests for generation change from the hardware threads can be OR'd together to create a single generation change request.","The generation counter (gen_cnt)  and the reclaim pointer (rcl_ptr)  both start at zero after reset. When a unit requests to advance to a new generation, it indicates the desired generation. There is no request explicit acknowledge sent back to the requestor, the requestor unit determines at whether its request has been processed based on the global current generation , . As the requested generation can be at most the gen_cnt+1, requests for any other generation at are assumed to have already been completed.","If the requested generation is equal to gen_cnt+1 and equal to rcl_ptr at, an increment is requested because the next generation value is still in use. The gen_cnt will be incremented as soon as the rcl_ptr increments.","If the requested generation is not equal to gen_cnt+1, it is assumed completed and is ignored.","If the requested generation is equal to gen_cnt+1 and not equal to rcl_ptr, gen_cnt is incremented at; but gen_cnt is incremented at most every 2 cycles, allowing units tracking the broadcast to see increments even in the presence of single cycle upset events.","Per FIG. --, which is implemented in box , the reclaim counter is advanced at  if\n\n","The PowerPC architecture defines three levels of synchronization:","heavy-weight sync, also called hwsync, or msync,","lwsync (lightweight sync) and","eieio (also called mbar, memory barrier).","Generally it has been found that programmers overuse the heavyweight sync in their zealousness to prevent memory inconsistencies. This results in unnecessary slowing of processing. For instance, if a program contains one data producer and many data consumers, the producer is the bottleneck. Having the producer wait to synchronize aggravates this. Analogously, if a program contains many producers and only one consumer, then the consumer can be the bottleneck and forcing it to wait should be avoided where possible.","In implementing memory synchronization, it has been found advantageous to offer several levels of synchronization programmable by memory mapped I\/O. These levels can be chosen by the programmer in accordance with anticipated work distribution. Generally, these levels will be most commonly used by the operating system to distribute workload. It will be up to the programmer choosing the level of synchronization to verify that different threads using the same data have compatible synchronization levels.","Seven levels or \u201cflavors\u201d of synchronization operations are discussed herein. These flavors can be implemented as alternatives to the msync\/hwsync, lwsync, and mbar\/eieio instructions of the PowerPC architecture. In this case, program instances of these categories of Power PC instruction can all be mapped to the strongest sync, the msync, with the alternative levels then being available by memory-mapped i\/o. The scope of restrictions imposed by these different flavors is illustrated conceptually in the Venn diagram of FIG. --. While seven flavors of synchronization are disclosed herein, one of ordinary skill in the art might choose to implement more or less flavors as a matter of design choice. In the present embodiment, these flavors are implemented as a store to a configuration address that defines how the next msync is supposed to be interpreted.","The seven flavors disclosed herein are:","Full Sync ","The full sync provides sufficient synchronization to satisfy the requirements of all PowerPC msync, hwsync\/lwsync and mbar instructions. It causes the generation counter to be incremented regardless of the generation of the requestor's last access. The requestor waits until all requests complete that were issued before its generation increment request. This sync has sufficient strength to implement the PowerPC synchronizing instructions.","Non-Cumulative Barrier ","This sync ensures that the generation of the last access of the requestor has completed before the requestor can proceed. This sync is not strong enough to provide cumulative ordering as required by the PowerPC synchronizing instructions. The last load issued by this processor may have received a value written by a store request of another core from the subsequent generation. Thus this sync does not guarantee that the value it saw prior to the store is visible to all cores after this sync operation. More about the distinction between non-cumulative barrier and full sync is illustrated by FIG. --. In this figure there are three core processors , , and . The first processor  is running a program that includes three sequential instructions: a load , an msync , and a store . The second processor  is running a second set of sequential instructions: a store , a load , and a load . It is desired for\n\n","This sync ensures that the generation of the last store access before the sync instruction of the requestor has completed before the requestor can proceed. This sync is sufficient to separate the data location updates from the guard location update for the producer in a producer\/consumer queue. This type of sync is useful where the consumer is the bottleneck and where there are instructions that can be carried out between the memory access and the msync that do not require synchronization. It is also not strong enough to provide cumulative ordering as required by the PowerPC synchronizing instructions.","Generation Change Sync ","This sync ensures only that the requests following the sync are in a different generation than the last request issued by the requestor. This type of sync is normally requested by the consumer and puts the burden of synchronization on the producer. This guarantees that load and stores are completed. This might be particularly useful in the case of atomic operations as defined in co-pending application 61\/299,911 filed Jan. 29, 2010, which is incorporated herein by reference, and where it is desired to verify that all data is consumed.","Producer Generation Change Sync ","This sync is designed to slow the producer the least. This sync ensures only that the requests following the sync are in a different generation from the last store request issued by the requestor. This can be used to separate the data location updates from the guard location update for the producer in a producer\/consumer queue. However, the consumer has to ensure that the data location updates have completed after it sees the guard location change. This type does not require the producer to wait until all the invalidations are finished. The term \u201cguard location\u201d here refers to the type of data shown in the flag of FIGS. --A and --B. Accordingly, this type might be useful for the types of threads illustrated in those figures. In this case the consumer has to know that the flag being set does not mean that the data is ready. If the flag has been stored with generation X, the data has been stored with x\u22121 or earlier. The consumer just has to make sure that the current generation \u22121 has completed.","Consumer Sync ","This request is run by the consumer thread. This sync ensures that all requests belonging to the current generation minus one have completed before the requestor can proceed. This sync can be used by the consumer in conjunction with a producer generation change sync by the producer in a producer\/consumer queue.","Local Barrier ","This sync is local to a core\/L1 group and only ensures that all its preceding memory accesses have been sent to the switch.","FIG. -- shows how the threads of FIG. --B can use the generation counter and reclaim pointer to achieve synchronization without a full sync. At , thread \u03b1\u2014the producer\u2014writes data. At  thread \u03b1 requests a generation increment pursuant to a producer generation change sync. At  thread \u03b1 monitors the generation counter until it increments. When the generation increments, it sets the data ready flag.","At  thread \u03b2\u2014the consumer\u2014tests whether the ready flag is set. At , thread \u03b2 also tests, in accordance with a consumer sync, whether the reclaim pointer has reached the generation of the current synchronization request. When both conditions are met at , then thread \u03b2 can use the data at .","In addition to the standard addressing and data functions , , when the L1P  shown in FIG. --\u2014sees any of these synchronization requests at the interface from the core , it immediately stops write combining\u2014responsive to the decode function  and the control unit \u2014for all currently open write combining buffers  and enqueues the request in its request queue . During the lookup phase of the request, synchronizing requests will advantageously request an increment of the generation counter and wait until the last generation completes, executing a Full Sync. The L1P will then resume the lookup and notify the core  of its completion.","To invoke the synchronizing behavior of synchronization types other than full sync, at least two implementation options are possible:","1. Synchronization Caused by Load and Store Operations to Predefined Addresses","Synchronization levels are controlled by memory-mapped I\/O accesses. As store operations can bypass load operations, synchronization operations that require preceding loads to have completed are implemented as load operations to memory mapped I\/O space, followed by a conditional branch that depends on the load return value. Simple use of load return may be sufficient. If the sync does not depend on the completion of preceding loads, it can be implemented as store to memory mapped I\/O space. Some implementation issues of one embodiment are as follows. A write access to this location is mapped to a sync request which is sent to the memory synchronization unit. The write request stalls the further processing of requests until the sync completes. A load request to the location causes the same type of requests, but only the full and the consumer request stall. All other load requests return the completion status as value back, a  for sync not yet complete, a  for sync complete. This implementation does not take advantage all of the built in PowerPC constraints of a core implementing PowerPC architecture. Accordingly, more programmer attention to order of memory access requests is needed.\n\n2. Configuring the Semantics of the Next Synchronizations Instruction, e.g. the PowerPC Msync, Via Storing to a Memory Mapped Configuration Register.\n","In this implementation, before every memory synchronization instruction, a store is executed that deposits a value that selects a synchronization behavior into a memory mapped register. The next executed memory synchronization instruction invokes the selected behavior and restores the configuration back to the Full Sync behavior. This reactivation of the strongest synchronization type guarantees correct execution if applications or subroutines that do not program the configuration register are executed.","Memory Synchronization Interface Unit","FIG. -- illustrates operation of the memory synchronization interface unit  associated with a prefetch unit group  of each processor . This unit mediates between the OR reduce end-point, the global generation counter unit and the synchronization requesting unit. The memory synchronization interface unit  includes a control unit  that collects and aggregates requests from one or more clients  (e.g.,  thread memory synchronization controls of the L1P via decoder ) and requests generation increments from the global generation counter unit  illustrated in  and receives current counts from that unit as well. The control unit  includes a respective set of registers  for each hardware thread. These registers may store information such as\n\n","The register storing configuration will sometimes be referred to herein as \u201cconfiguration register.\u201d This control unit  notifies the core  via  when the msync is completed. The core issuing the msync drains all loads and stored, stops taking loads and stores and stops the issuing thread until the msync completion indication is received.","This control unit also exchanges information with the global generation counter module . This information includes a generation count. In the present embodiment, there is only one input per L1P to the generation counter, so the L1P aggregates requests for increment from all hardware threads of the processor . Also, in the present embodiment, the OR reduce tree is coupled to the reclaim pointer, so the memory synchronization interface unit gets information from the OR reduce tree indirectly via the reclaim pointer.","The control unit also tracks the changes of the global generation (gen_cnt) and determines whether a request of a client has completed. Generation completion is detected by using the reclaim pointer that is fed to observer latches in the L1P. The core waits for the L1P to handle the msyncs. Each hardware thread may be waiting for a different generation to complete. Therefore each one stores what the generation for that current memory synchronization instruction was. Each then waits individually for its respective generation to complete.","For each client , the unit implements a group  of three generation completion detectors shown at , , , per FIG. --. Each detector implements a 3 bit latch , ,  that stores a generation to track, which will sometimes be the current generation, gen_cnt, and sometimes be the prior generation, last_gen. Each detector also implements a flag , ,  that indicates if the generation tracked has still requests in flight (ginfl_flag). The detectors can have additional flags, for instance to show that multiple generations have completed.","For each store request generated by a client, the first  of the three detectors sets its ginfl_flag  and updates the last_gen_latch  with the current generation. This detector is updated for every store, and therefore reflects whether the last store has completed or not. This is sufficient, since prior stores will have generations less than or equal to the generation of the current store. Also, since the core is waiting for memory synchronization, it will not be making more stores until the completion indication is received.","For each memory access request, regardless whether load or store, the second detector  is set correspondingly. This detector is updated for every load and every store, and therefore its flag indicates whether the last memory access request has completed.","If a client requests a full sync, the third detector  is primed with the current generation, and for a consumer sync the third detector is primed with the current generation-1. Again, this detector is updated for every full or consumer sync.","Since the reclaim pointer cannot advance without everything in that generation having completed and because the reclaim pointer cannot pass the generation counter, the reclaim pointer is an indication of whether a generation has completed. If the rcl_ptr  moves past the generation stored in last gen, no requests for the generation are in flight anymore and the ginfl_flag is cleared.","Full Sync","This sync completes if the ginfl_flag  of the third detector  is cleared. Until completion, it requests a generation change to the value stored in the third detector plus one.","Non-Cumulative Barrier","This sync completes if the ginfl_flag  of the second detector  is cleared. Until completion, it requests a generation change to the value that is held in the second detector plus one.","Producer Sync","This sync completes if the ginfl_flag  of the first detector  is cleared. Until completion, it requests a generation change to the value held in the first detector plus one.","Generation Change Sync","This sync completes if either the ginfl_flag  of the second detector  is cleared or the if the last_gen  of the second detector is different from gen_cnt . If it does not complete immediately, it requests a generation change to the value stored in the second detector plus one. The purpose of the operation is to advance the current generation (value of gen_cnt) to at least one higher than the generation of the last load or store. The generation of the last load or store is stored in the last_gen register of the second detector.\n\n","This sync completes if either the ginfl_flag  of the first detector  is cleared or if the last_gen  of the first detector is different from gen_cnt . If it does not complete immediately, it requests a generation change to of the value stored in the first detector plus one. This operates similarly to the generation change sync except that it uses the generation of the last store, rather than load or store.","Consumer Sync","This sync completes if the ginfl_flag  of the third detector  is cleared. Until completion, it requests a generation change to of the value stored in the third detector plus one.","Local Barrier","This sync is executed by the L1P, it does not involve generation tracking.","From the above discussion, it can be seen that a memory synchronization instruction actually implicates a set of sub-tasks. For a comprehensive memory synchronization scheme, those sub-tasks might include one or more of the following:\n\n","In implementing the various levels of synchronization herein, sub-sets of this set of sub-tasks can be viewed as partial synchronization tasks to be allocated between threads in an effort to improve throughput of the system. Therefore address formats of instructions specifying a synchronization level effectively act as parameters to offload sub-tasks from or to the thread containing the synchronization instruction. If a particular sub-task implicated by the memory synchronization instruction is not performed by the thread containing the memory synchronization instruction, then the implication is that some other thread will pick up that part of the memory synchronization function. While particular levels of synchronization are specified herein, the general concept of distributing synchronization sub-tasks between threads is not limited to any particular instruction type or set of levels.","Physical Design","The Global OR tree needs attention to layout and pipelining, as its latency affects the performance of the sync operations.","In the current embodiment, the cycle time is 1.25 ns. In that time, a signal will travel 2 mm through a wire. Where a wire is longer than 2 mm, the delay will exceed one clock cycle, potentially causing unpredictable behavior in the transmission of signals. To prevent this, a latch should be placed at each position on each wire that corresponds to 1.25 ns, in other words approximately every 2 mm. This means that every transmission distance delay of 4 ns will be increased to 5 ns, but the circuit behavior will be more predictable. In the case of the msync unit, some of the wires are expected to be on the order of 10 mm meaning that they should have on the order of five latches.","Due to quantum mechanical effects, it is advisable to protect latches holding generation information with Error Correcting Codes (\u201cECC\u201d) (4b per 3b counter data). All operations may include ECC correction and ECC regeneration logic.","The global broadcast and generation change interfaces may be protected by parity. In the case of a single cycle upset, the request or counter value transmitted is ignored, which does not affect correctness of the logic.","Software Interface","The Msync unit will implement the ordering semantics of the PPC hwsync, lwsync and mbar instruction by mapping these operations to the full sync.","FIG. -- shows a mechanism for delaying incrementation if too many generations are in flight. At , the outputs of the OR reduce tree are multiplexed, to yield a positive result if all possible generations are in flight. A counter  holds the current generation, which is incremented at . A comparator  compares the current generation plus one to the requested generation. A comparison result is ANDed at  with an increment request from the core. A result from the AND at  is ANDed at  with an output of multiplexer .","The flowchart and block diagrams in the Figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods and computer program products according to various embodiments of the present invention. In this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of code, which comprises one or more executable instructions for implementing the specified logical function(s). It should also be noted that, in some alternative implementations, the functions noted in the block may occur out of the order noted in the figures. For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse order, depending upon the functionality involved. It will also be noted that each block of the block diagrams and\/or flowchart illustration, and combinations of blocks in the block diagrams and\/or flowchart illustration, can be implemented by special purpose hardware-based systems that perform the specified functions or acts, or combinations of special purpose hardware and computer instructions.","The word \u201ccomprising\u201d, \u201ccomprise\u201d, or \u201ccomprises\u201d as used herein should not be viewed as excluding additional elements. The singular article \u201ca\u201d or \u201can\u201d as used herein should not be viewed as excluding a plurality of elements. Unless the word \u201cor\u201d is expressly limited to mean only a single item exclusive from other items in reference to a list of at least two items, then the use of \u201cor\u201d in such a list is to be interpreted as including (a) any single item in the list, (b) all of the items in the list, or (c) any combination of the items in the list. Ordinal terms in the claims, such as \u201cfirst\u201d and \u201csecond\u201d are used for distinguishing elements and do not necessarily imply order of operation.","There is further provided a system and method for managing the loading and storing of data conditionally in memories of multi-processor systems.","A conventional multi-processor computer system includes multiple processing units (a.k.a. processors or processor cores) all coupled to a system interconnect, which typically comprises one or more address, data and control buses. Coupled to the system interconnect is a system memory, which represents the lowest level of volatile memory in the multiprocessor computer system and which generally is accessible for read and write access by all processing units. In order to reduce access latency to instructions and data residing in the system memory, each processing unit is typically further supported by a respective multi-level cache hierarchy, the lower level(s) of which may be shared by one or more processor cores.","Cache memories are commonly utilized to temporarily buffer memory blocks that might be accessed by a processor in order to speed up processing by reducing access latency introduced by having to load needed data and instructions from system memory. In some multiprocessor systems, the cache hierarchy includes at least two levels. The level one (L1), or upper-level cache is usually a private cache associated with a particular processor core and cannot be directly accessed by other cores in the system. Typically, in response to a memory access instruction such as a load or store instruction, the processor core first accesses the upper-level cache. If the requested memory block is not found in the upper-level cache or the memory access request cannot be serviced in the upper-level cache (e.g., the L1 cache is a store-though cache), the processor core then accesses lower-level caches (e.g., level two (L2) or level three (L3) caches) to service the memory access to the requested memory block. The lowest level cache (e.g., L2 or L3) is often shared among multiple processor cores.","A coherent view of the contents of memory is maintained in the presence of potentially multiple copies of individual memory blocks distributed throughout the computer system through the implementation of a coherency protocol. The coherency protocol, entails maintaining state information associated with each cached copy of the memory block and communicating at least some memory access requests between processing units to make the memory access requests visible to other processing units.","In order to synchronize access to a particular granule (e.g., cache line) of memory between multiple processing units and threads of execution, load-reserve and store-conditional instruction pairs are often employed. For example, load-reserve and store-conditional instructions referred to as LWARX and STWCX have been implemented. Execution of a LWARX (Load Word And Reserve Indexed) instruction by a processor loads a specified cache line into the cache memory of the processor and typically sets a reservation flag and address register signifying the processor has interest in atomically updating the cache line through execution of a subsequent STWCX (Store Word Conditional Indexed) instruction targeting the reserved cache line. The cache then monitors the storage subsystem for operations signifying that another processor has modified the cache line, and if one is detected, resets the reservation flag to signify the cancellation of the reservation. When the processor executes a subsequent STWCX targeting the cache line reserved through execution of the LWARX instruction, the cache memory only performs the cache line update requested by the STWCX if the reservation for the cache line is still pending. Thus, updates to shared memory can be synchronized without the use of an atomic update primitive that strictly enforces atomicity.","Individual processors usually provide minimal support for load-reserve and store-conditional. The processors basically hand off responsibility for consistency and completion to the external memory system. For example, a processor core may treat load-reserve like a cache-inhibited load, but invalidate the target line if it hits in the L1 cache. The returning data goes to the target register, but not to the L1 cache. Similarly, a processor core may treat store-conditional as a cache-inhibited store and also invalidate the target line in the L1 cache if it exists. The store-conditional instruction stalls until success or failure is indicated by the external memory system, and the condition code is set before execution continues. The external memory system is expected to maintain load-reserve reservations for each thread, and no special internal consistency action is taken by the processor core when multiple threads attempt to use the same lock.","In a traditional, bus-based multiprocessor system, the point of memory system coherence is the bus itself. That is, coherency between the individual caches of the processors is resolved by the bus during memory accesses, because the accesses are effectively serialized. As a result, the shared main memory of the system is unaware of the existence of multiple processors. In such a system, support for load-reserve and store-conditional is implemented within the processors or in external logic associated with the processors, and conflicts between reservations and other memory accesses are resolved during bus accesses.","As the number of processors in a multiprocessor system increases, a shared bus interconnect becomes a performance bottleneck. Therefore, large-scale multiprocessors use some sort of interconnection network to connect processors to shared memory (or a cache for shared memory). Furthermore, an interconnection network encourages the use of multiple shared memory or cache slices in order to take advantage of parallelism and increase overall memory bandwidth.  shows the architecture of such a system, consisting of eighteen processors , a crossbar switch interconnection network , and a shared L2 cache consisting of sixteen slices . In such a system, it may be difficult to maintain memory consistency in the network, and it may be necessary to move the point of coherence to the shared memory (or shared memory cache when one is present). That is, the shared memory is responsible for maintaining a consistent order between the servicing of requests coming from the multiple processors and responses returning to them.","It is desirable to implement synchronization based on load-reserve and store-conditional in such a large-scale multiprocessor, but it is no longer efficient to do so at the individual processors. What is needed is a mechanism to implement such synchronization at the point of coherence, which is the shared memory. Furthermore, the implementation must accommodate the individual slices of the shared memory. A unified mechanism is needed to insure proper consistency of lock reservations across all the processors of the multiprocessor system.","In the embodiment described above, each A2 processor core has four independent hardware threads sharing a single L1 cache with a 64-byte line size. Every memory line is stored in a particular L2 cache slice, depending on the address mapping. That is, the sixteen L2 slices effectively comprise a single L2 cache, which is the point of shared memory coherence for the compute node. Those skilled in the art will recognize that the invention applies to different multiprocessor configurations including a single L2 cache (i.e. one slice), a main memory with no L2 cache, and a main memory consisting of multiple slices.","Each L2 slice has some number of reservation registers to support load-reserve\/store-conditional locks. One embodiment that would accommodate unique lock addresses from every thread simultaneously is to provide 68 reservation registers in each slice, because it is possible for all 68 threads to simultaneously use lock addresses that fall into the same L2 slice. Each reservation register would contain an N-bit address (specifying a unique 64-byte L1 line) and a valid bit, as shown in FIG. --. Note that the logic shown in FIG. -- is implemented in each slice of the L2 cache. The number of address bits stored in each reservation register is determined by the size of the main memory, the granularity of lock addresses, and the number of L2 slices. For example, a byte address in a 64 GB main memory requires 36 bits. If memory addresses are reserved as locks at an 8-byte granularity, then a lock address is 33 bits in size. If there are 16 L2 slices, then 4 address bits are implied by the memory reference steering logic that determines a unique L2 slice for each address. Therefore, each reservation register would have to accommodate a total of 29 address bits (i.e. N equals 29 in FIG. --).","When a load-reserve occurs, the reservation register corresponding to the ID (i.e. the unique thread number) of the thread that issued the load-reserve is checked to determine if the thread has already made a reservation. If so, the reservation address is updated with the load-reserve address. If not, the load-reserve address is installed in the register and the valid bit is set. In both cases, the load-reserve continues as an ordinary load and returns data.","When a store-conditional occurs, the reservation register corresponding to the ID of the requesting thread is checked to determine if the thread has a valid reservation for the lock address. If so, then the store-conditional is considered a success, a store-conditional success indication is returned to the requesting processor core, and the store-conditional is converted to an ordinary store (updating the memory and causing the necessary invalidations to other processor cores by the normal coherence mechanism). In addition, if the store-conditional address matches any other reservation registers, then they are invalidated. If the thread issuing the store-conditional has no valid reservation or the address does not match, then the store-conditional is considered a failure, a store-conditional failure indication is returned to the requesting processor core, and the store-conditional is dropped (i.e. the memory update and associated invalidations to other cores and other reservation registers does not occur).","Every ordinary store to the shared memory searches all valid reservation address registers and simply invalidates those with a matching address. The necessary back-invalidations to processor cores will be generated by the normal coherence mechanism.","In general, a thread is not allowed to have more than one load-reserve reservation at a time. If the processor does not track reservations, then this restriction must be enforced by additional logic outside the processor. Otherwise, a thread could issue load-reserve requests to more than one L2 slice and establish multiple reservations. FIG. -- shows one embodiment of logic that can enforce the single-reservation constraint on behalf of the processor. There are four lock reservation registers, one for each thread (assuming a processor that implements four threads). Each register stores a reservation address  for its associated thread and a valid bit . When a thread executes load-reserve, the memory address is stored in the appropriate register and the valid bit is set. If the thread executes another load-reserve, the register is simply overwritten. In both cases, the load-reserve continues on to the L2 as described above.","When the thread executes store-conditional, the address will be matched against the appropriate register. If it matches and the register is valid, then the store-conditional protocol continues as described above. If not, then the store-conditional is considered a failure, the core is notified, and only a special notification is sent to the L2 slice holding the reservation in order to cancel that reservation. This embodiment allows the processor to continue execution past the store-conditional very quickly. However, a failed store-conditional requires the message to be sent to the L2 in order to invalidate the reservation there. The memory system must guarantee that this invalidation message acts on the reservation before any subsequent store-conditional from the same processor is allowed to succeed.","Another embodiment, shown in FIG. --, is to store an L2 slice index (4 bits for 16 slices), represented at , together with a valid bit, represented at . In this case, an exact store-conditional address match can only be performed at an L2 slice, requiring a roundtrip message before execution on the processor continues past the store-conditional. However, the L2 slice index of the store-conditional address is matched to the stored index and a mismatch avoids the roundtrip for some (perhaps common) cases, where the address falls into a different L2 slice than the reservation. In the case of a mismatch, the store-conditional is guaranteed to be a failure and the special failure notification is sent to the L2 slice holding the reservation (as indicated by the stored index) in order to cancel the reservation.","A similar tradeoff exists for load-reserve followed by load-reserve, but the performance of both storage strategies is the same. That is, the reservation resulting from the earlier load-reserve address must be invalidated at L2, which can be done with a special invalidate message. Then the new reservation is established as described previously. Again, the memory system must insure that no subsequent store-conditional can succeed before that invalidate message has had its effect.","When a load-reserve reservation is invalidated due to a store-conditional by some other thread or an ordinary store, all L2 reservation registers storing that address are invalidated. While this guarantees correctness, performance could be improved by invalidating matching lock reservation registers near the processors (FIGS. -- and --) as well. This is simply a matter of having the reservation logic of FIG. -- (or FIG. --) snoop L1 invalidations, but it does require another datapath (invalidates) to be compared (by way of the Core Address in FIG. -- or the L2 Index in FIG. --).","As described above, the L2 cache slices store the reservation addresses of all valid load-reserve locks. Because every thread could have a reservation and they could all fall into the same L2 slice, one embodiment, shown in FIG. --, provides 68 lock reservation registers, each with a valid bit.","It is desirable to compare the address of a store-conditional or store to all lock reservation addresses simultaneously for the purpose of rapid invalidation. Therefore, a conventional storage array such as a static RAM or register array is preferably not used. Rather, discrete registers that can operate in parallel are needed. The resulting structure has on the order of N*68 latches and requires a 68-way fanout for the address and control buses. Furthermore, it is replicated in all sixteen L2 slices.","Because load-reserve reservations are relatively sparse in many codes, one way to address the power inefficiency of the large reservation register structure is to use clock-gated latches. Another way, as illustrated in FIG. --, is to block the large buses behind AND gates  that are only enabled when at least one of the reservation registers contains a valid address (the uncommon case), as determined by an OR  of all the valid bits. Such logic will save power by preventing the large output bus (Bus Out) from switching when there are no valid reservations.","Although the reservation register structure in the L2 caches described thus far will accommodate any possible locking code, it would be very unusual for 68 threads to all want a unique lock since locking is done when memory is shared. A far more likely, yet still remote, possibility is that 34 pairs of threads want unique locks (one per pair) and they all happen to fall into the same L2 slice. In this case, the number of registers could be halved, but a single valid bit no longer suffices because the registers must be shared. Therefore, each register would, as represented in FIG. --, store a 7-bit thread ID  and the registers would no longer be dedicated to specific threads. Whenever a new load-reserve reservation is established, an allocation policy is used to choose one of the 34 registers, and the ID of the requesting thread is stored in the chosen register along with the address tag.","With this embodiment, a store-conditional match is successful only if both the address and thread ID are the same. However, an address-only match is sufficient for the purpose of invalidation. This design uses on the order of 34*M latches and requires a 34-way fanout for the address, thread ID, and control buses. Again, the buses could be shielded behind AND gates, using the structure shown in FIG. --, to save switching power.","Because this design cannot accommodate all possible lock scenarios, a register selection policy is needed in order to cover the cases where there are no available lock registers to allocate. One embodiment is to simply drop new requests when no registers are available. However, this can lead to deadlock in the pathological case where all the registers are reserved by a subset of the threads executing load-reserve, but never released by store-conditional. Another embodiment is to implement a replacement policy such as round-robin, random, or LRU. Because, in some embodiments, it is very likely that all 34 registers in a single slice may be used, a policy that has preference for unused registers and then falls back to simple round-robin replacement will, in many cases provided excellent results.","Given the low probability of having many locks within a single L2 slice, the structure can be further reduced in size at the risk of a higher livelock probability. For instance, even with only 17 registers per slice, there would still be a total of 272 reservation registers in the entire L2 cache; far more than needed, especially if address scrambling is used to spread the lock addresses around the L2 cache slices sufficiently.","With a reduced number of reservation registers, the thread ID storage could be modified in order to allow sharing and accommodate the more common case of multiple thread IDs per register (since locks are usually shared). One embodiment is to replace the 7-bit thread ID with a 68-bit vector specifying which threads share the reservation. This approach does not mitigate the livelock risk when the number of total registers is exhausted.","Another compression strategy, which may be better in some cases, is to replace the 7-bit thread ID with a 5-bit processor ID (assuming 17 processors) and a 4-bit thread vector (assuming 4 threads per processor). In this case, a single reservation register can be used by all four threads of a processor to share a single lock. With this strategy, seventeen reservation registers would be sufficient to accommodate all 68 threads reserving the same lock address. Similarly, groups of threads using the same lock would be able to utilize the reservation registers more efficiently if they shared a processor (or processors), reducing the probability of livelock. At the cost of some more storage, the processor ID can be replaced by a 4-bit index specifying a particular pair of processors and the thread vector could be extended to 8 bits. As will be obvious to those skilled in the art, there is an entire spectrum of choices between the full vector and the single index.","As an example, one embodiment for the 17-processor multiprocessor is 17 reservation registers per L2 slice, each storing an L1 line address together with a 5-bit core ID and a 4-bit thread vector. This results in bus fanouts of 17.","While the embodiment herein disclosed describes a multiprocessor with the reservation registers implemented in a sliced, shared memory cache, it should be obvious that the invention can be applied to many types of shared memories, including a shared memory with no cache, a sliced shared memory with no cache, and a single, shared memory cache.","The disclosure further relates to managing speculation with respect to cache memory in a multiprocessor system with multiple threads, some of which may execute speculatively.","In a multiprocessor system with generic cores, it becomes easier to design new generations and expand the system. Advantageously, speculation management can be moved downstream from the core and first level cache. In such a case, it is desirable to devise schemes of accessing the first level cache without explicitly keeping track of speculation.","There may be more than one modes of keeping the first level cache speculation blind. Advantageously, the system will have a mechanism for switching between such modes.","One such mode is to evict writes from the first level cache, while writing through to a downstream cache. The embodiments described herein show this first level cache as being the physically first in a data path from a core processor; however, the mechanisms disclose here might be applied to other situations. The terms \u201cfirst\u201d and \u201csecond,\u201d when applied to the claims herein are for convenience of drafting only and are not intended to be limiting to the case of L1 and L2 caches.","As described herein, the use of the letter \u201cB\u201d\u2014other than as part of a figure number\u2014represents a Byte quantity, while \u201cGB\u201d represents Gigabyte quantities. Throughout this disclosure a particular embodiment of a multi-processor system will be discussed. This discussion includes various numerical values for numbers of components, bandwidths of interfaces, memory sizes and the like. These numerical values are not intended to be limiting, but only examples. One of ordinary skill in the art might devise other examples as a matter of design choice.","The term \u201cthread\u201d is used herein. A thread can be either a hardware thread or a software thread. A hardware thread within a core processor includes a set of registers and logic for executing a software thread. The software thread is a segment of computer program code. Within a core, a hardware thread will have a thread number. For instance, in the A2, there are four threads, numbered zero through three. Throughout a multiprocessor system, such as the nodechip  of , software threads can be referred to using speculation identification numbers (\u201cIDs\u201d). In the present embodiment, there are 128 possible IDs for identifying software threads.","These threads can be the subject of \u201cspeculative execution,\u201d meaning that a thread or threads can be started as a sort of wager or gamble, without knowledge of whether the thread can complete successfully. A given thread cannot complete successfully if some other thread modifies the data that the given thread is using in such a way as to invalidate the given thread's results. The terms \u201cspeculative,\u201d \u201cspeculatively,\u201d \u201cexecute,\u201d and \u201cexecution\u201d are terms of art in this context. These terms do not imply that any mental step or manual operation is occurring. All operations or steps described herein are to be understood as occurring in an automated fashion under control of computer hardware or software.","If speculation fails, the results must be invalidated and the thread must be re-run or some other workaround found.","Three modes of speculative execution are to be supported: Speculative Execution (SE) (also referred to as Thread Level Speculation (\u201cTLS\u201d)), Transactional Memory (\u201cTM\u201d), and Rollback.","SE is used to parallelize programs that have been written as sequential program. When the programmer writes this sequential program, she may insert commands to delimit sections to be executed concurrently. The compiler can recognize these sections and attempt to run them speculatively in parallel, detecting and correcting violations of sequential semantics.","When referring to threads in the context of Speculative Execution, the terms older\/younger or earlier\/later refer to their relative program order (not the time they actually run on the hardware).","In Speculative Execution, successive sections of sequential code are assigned to hardware threads to run simultaneously. Each thread has the illusion of performing its task in program order. It sees its own writes and writes that occurred earlier in the program. It does not see writes that take place later in program order even if (because of the concurrent execution) these writes have actually taken place earlier in time.","To sustain the illusion, the L2 gives threads private storage as needed, accessible by software thread ID. It lets threads read their own writes and writes from threads earlier in program order, but isolates their reads from threads later in program order. Thus, the L2 might have several different data values for a single address. Each occupies an L2 way, and the L2 directory records, in addition to the usual directory information, a history of which thread IDs are associated with reads and writes of a line. A speculative write is not to be written out to main memory.","One situation that will break the program-order illusion is if a thread earlier in program order writes to an address that a thread later in program order has already read. The later thread should have read that data, but did not. The solution is to kill the later software thread and invalidate all the lines it has written in L2, and to repeat this for all younger threads. On the other hand, without such interference a thread can complete successfully, and its writes can move to external main memory when the line is cast out or flushed.","Not all threads need to be speculative. The running thread earliest in program order can be non-speculative and run conventionally; in particular its writes can go to external main memory. The threads later in program order are speculative and are subject to be killed. When the non-speculative thread completes, the next-oldest thread can be committed and it then starts to run non-speculatively.","The following sections describe the implementation of the speculation model in the context of addressing.","When a sequential program is decomposed into speculative tasks, the memory subsystem needs to be able to associate all memory requests with the corresponding task. This is done by assigning a unique ID at the start of a speculative task to the thread executing the task and attaching the ID as tag to all its requests sent to the memory subsystem.","As the number of dynamic tasks can be very large, it may not be practical to guarantee uniqueness of IDs across the entire program run. It is sufficient to guarantee uniqueness for all IDs concurrently present in the memory system. More about the use of speculation ID's, including how they are allocated, committed, and invalidated, appears in the incorporated applications.","Transactions as defined for TM occur in response to a specific programmer request within a parallel program. Generally the programmer will put instructions in a program delimiting sections in which TM is desired. This may be done by marking the sections as requiring atomic execution. According to the PowerPC architecture: \u201cAn access is single-copy atomic, or simply \u201catomic\u201d, if it is always performed in its entirety with no visible fragmentation.\u201d","To enable a TM runtime system to use the TM supporting hardware, it needs to allocate a fraction of the hardware resources, particularly the speculation IDs that allow hardware to distinguish concurrently executed transactions, from the kernel (operating system), which acts as a manager of the hardware resources. The kernel configures the hardware to group IDs into sets called domains, configures each domain for its intended use, TLS, TM or Rollback, and assigns the domains to runtime system instances.","At the start of each transaction, the runtime system executes a function that allocates an ID from its domain, and programs it into a register that starts marking memory access as to be treated as speculative, i.e., revocable if necessary.","When the transaction section ends, the program will make another call that ultimately signals the hardware to do conflict checking and reporting. Based on the outcome of the check, all speculative accesses of the preceding section can be made permanent or removed from the system.","The PowerPC architecture defines an instruction pair known as larx\/stcx. This instruction type can be viewed as a special case of TM. The larx\/stcx pair will delimit a memory access request to a single address and set up a program section that ends with a request to check whether the instruction pair accessed the memory location without interfering access from another thread. If an access interfered, the memory modifying component of the pair is nullified and the thread is notified of the conflict More about a special implementation of larx\/stcx instructions using reservation registers is to be found in co-pending application Ser. No. 12\/697,799 filed Jan. 29, 2010, which is incorporated herein by reference. This special implementation uses an alternative approach to TM to implement these instructions. In any case, TM is a broader concept than larx\/stcx. A TM section can delimit multiple loads and stores to multiple memory locations in any sequence, requesting a check on their success or failure and a reversal of their effects upon failure.","Rollback occurs in response to \u201csoft errors\u201d, temporary changes in state of a logic circuit. Normally these errors occur in response to cosmic rays or alpha particles from solder balls. The memory changes caused by a programs section executed speculatively in rollback mode can be reverted and the core can, after a register state restore, replay the failed section.","Referring now to , there is shown an overall architecture of a multiprocessor computing node  implemented in a parallel computing system in which the present embodiment may be implemented. The compute node  is a single chip (\u201cnodechip\u201d) based on PowerPC cores, though the architecture can use any cores, and may comprise one or more semiconductor chips.","More particularly, the basic nodechip  of the multiprocessor system illustrated in  includes (sixteen or seventeen) 16+1 symmetric multiprocessing (SMP) cores , each core being 4-way hardware threaded supporting transactional memory and thread level speculation, and, including a Quad Floating Point Unit (FPU)  associated with each core. The 16 cores  do the computational work for application programs.","The 17core is configurable to carry out system tasks, such as\n\n","In other words, it offloads all the administrative tasks from the other cores to reduce the context switching overhead for these.","In one embodiment, there is provided 32 MB of shared L2 cache , accessible via crossbar switch . There is further provided external Double Data Rate Synchronous Dynamic Random Access Memory (\u201cDDR SDRAM\u201d) , as a lower level in the memory hierarchy in communication with the L2. Herein, \u201clow\u201d and \u201chigh\u201d with respect to memory will be taken to refer to a data flow from a processor to a main memory, with the processor being upstream or \u201chigh\u201d and the main memory being downstream or \u201clow.\u201d","Each FPU  associated with a core  has a data path to the L1-cache  of the CORE, allowing it to load or store from or into the L1-cache . The terms \u201cL1\u201d and \u201cL1D\u201d will both be used herein to refer to the L1 data cache.","Each core  is directly connected to a supplementary processing agglomeration , which includes a private prefetch unit. For convenience, this agglomeration  will be referred to herein as \u201cL1P\u201d\u2014meaning level 1 prefetch\u2014or \u201cprefetch unit;\u201d but many additional functions are lumped together in this so-called prefetch unit, such as write combining. These additional functions could be illustrated as separate modules, but as a matter of drawing and nomenclature convenience the additional functions and the prefetch unit will be grouped together. This is a matter of drawing organization, not of substance. Some of the additional processing power of this L1P group is shown in FIGS. --,-- and --. The L1P group also accepts, decodes and dispatches all requests sent out by the core .","By implementing a direct memory access (\u201cDMA\u201d) engine referred to herein as a Messaging Unit (\u201cMU\u201d) such as MU , with each MU including a DMA engine and Network Card interface in communication with the XBAR switch, chip I\/O functionality is provided. In one embodiment, the compute node further includes: intra-rack interprocessor links  which may be configurable as a 5-D torus; and, one I\/O link  interfaced with the interfaced with the MU. The system node employs or is associated and interfaced with a 8-16 GB memory\/node, also referred to herein as \u201cmain memory.\u201d","The term \u201cmultiprocessor system\u201d is used herein. With respect to the present embodiment this term can refer to a nodechip or it can refer to a plurality of nodechips linked together. In the present embodiment, however, the management of speculation is conducted independently for each nodechip. This might not be true for other embodiments, without taking those embodiments outside the scope of the claims.","The compute nodechip implements a direct memory access engine DMA to offload the network interface. It transfers blocks via three switch master ports between the L2-cache slices  (). It is controlled by the cores via memory mapped I\/O access through an additional switch slave port. There are 16 individual slices, each of which is assigned to store a distinct subset of the physical memory lines. The actual physical memory addresses assigned to each cache slice are configurable, but static. The L2 has a line size such as 128 bytes. In the commercial embodiment this will be twice the width of an L1 line. L2 slices are set-associative, organized as 1024 sets, each with 16 ways. The L2 data store may be composed of embedded DRAM and the tag store may be composed of static RAM.","The L2 has ports, for instance a 256b wide read data port, a 128b wide write data port, and a request port. Ports may be shared by all processors through the crossbar switch .","In this embodiment, the L2 Cache units provide the bulk of the memory system caching on the BQC chip. Main memory may be accessed through two on-chip DDR-3 SDRAM memory controllers , each of which services eight L2 slices.","The L2 slices may operate as set-associative caches while also supporting additional functions, such as memory speculation for Speculative Execution (SE), which includes different modes such as: Thread Level Speculations (\u201cTLS\u201d), Transactional Memory (\u201cTM\u201d) and local memory rollback, as well as atomic memory transactions.","The L2 serves as the point of coherence for all processors. This function includes generating L1 invalidations when necessary. Because the L2 cache is inclusive of the L1s, it can remember which processors could possibly have a valid copy of every line, and slices can multicast selective invalidations to such processors.","FIG. -- shows a cache slice. It includes arrays of data storage , and a central control portion .","FIG. -- shows various address versions across a memory pathway in the nodechip . One embodiment of the core  uses a 64 bit virtual address  in accordance with the PowerPC architecture. In the TLB , that address is converted to a 42 bit \u201cphysical\u201d address  that actually corresponds to 64 times the architected maximum main memory size 80, so it includes extra bits that can be used for thread identification information. The address portion used to address a location within main memory will have the canonical format of FIG. --, prior to hashing, with a tag  that matches the address tag field of a way, an index  that corresponds to a set, and an offset  that corresponds to a location within a line. The addressing varieties shown, with respect to the commercial embodiment, are intended to be used for the data pathway of the cores. The instruction pathway is not shown here. The \u201cphysical\u201d address is used in the L1D . After arriving at the L1P, the address is stripped down to 36 bits for addressing of main memory at .","Address scrambling per FIG. -- tries to distribute memory accesses across L2-cache slices and within L2-cache slices across sets (congruence classes). Assuming a 64 GB main memory address space, a physical address dispatched to the L2 has 36 bits, numbered from 0 (MSb) to 35 (LSb) (a(0 to 35)).","The L2 stores data in 128B wide lines, and each of these lines is located in a single L2-slice and is referenced there via a single directory entry. As a consequence, the address bits  to  only reference parts of an L2 line and do not participate in L2 slice or set selection.","To evenly distribute accesses across L2-slices for sequential lines as well as larger strides, the remaining address bits - are hashed to determine the target slice. To allow flexible configurations, individual address bits can be selected to determine the slice as well as an XOR hash on an address can be used: The following hashing is used at  in the present embodiment:\n\n","For each of the slices, 25 address bits are a sufficient reference to distinguish L2 cache lines mapped to that slice.","Each L2 slice holds 2 MB of data or 16K cache lines. At 16-way associativity, the slice has to provide 1024 sets, addressed via 10 address bits. The different ways are used to store different addresses mapping to the same set as well as for speculative results associated with different threads or combinations of threads.","Again, even distribution across set indices for unit and non-unit strides is achieved via hashing, to wit:","Set index:=(\u201c00000\u201d & a(0 to 4)) xor a(5 to 14) xor a(15 to 24).","To uniquely identify a line within the set, using a(0 to 14) is sufficient as a tag.","Thereafter, the switch provides addressing to the L2 slice in accordance with an address that includes the set and way and offset within a line, as shown in FIG. --. Each line has 16 ways.","FIG. -- shows the role of the Translation Lookaside Buffer (\u201cTLB\u201d). The role of this unit is explained in further detail herein. FIG. -- shows a four piece address space also described in more detail herein.","Long and Short Running Speculation","The L2 accommodates two types of L1 cache management in response to speculative threads. One is for long running speculation and the other is for short running speculation. The differences between the mode support for long and short running speculation is described in the following two subsections.","For long running transactions mode, the L1 cache needs to be invalidated to make all first accesses to a memory location visible to the L2 as an L1-load-miss. A thread can still cache all data in its L1 and serve subsequent loads from the L1 without notifying the L2 for these. This mode will use address aliasing as shown in FIG. --, with the four part address space in the L1P, as shown in FIG. --, and as further described in detail herein.","To reduce overhead in short running speculation mode, the embodiment herein eliminates the requirement to invalidate L1. The invalidation of the L1 allowed tracking of all read locations by guaranteeing at least one L1 miss per accessed cache line. For small transactions, the equivalent is achieved by making all load addresses within the transaction visible to the L2, regardless of L1 hit or miss, i.e. by operating the L1 in \u201cread\/write through\u201d mode. In addition, data modified by a speculative thread is in this mode evicted from the L1 cache, serving all loads of speculatively modified data from L2 directly. In this case, the L1 does not have to use a four piece mock space as shown in FIG. --, since no speculative writes are made to the L1. Instead, it can use a single physical addressing space that corresponds to the addresses of the main memory.","FIG. -- shows a switch for choosing between these addressing modes. The processor  chooses\u2014responsive to computer program code produced by a programmer\u2014whether to evict on write for short running speculation or do address aliasing for long-running speculation per FIGS. --, -- and --.","In the case of switching between memory access modes here, a register  at the entry of the L1P receives an address field from the processor , as if the processor  were requesting a main memory access, i.e., a memory mapped input\/output operation (MMIO). The L1P diverts a bit called ID_evict  from the register and forwards it both back to the processor  and also to control the L1 caches.","A special purpose register SPR  also takes some data from the path , which is then AND-ed at  to create a signal that informs the L1D , i.e. the data cache whether write on evict is to be enabled. The instruction cache, L1I  is not involved.","FIG. -- is a flowchart describing operations of the short running speculation embodiment. At , memory access is requested. This access is to be processed responsive to the switching mechanism of FIG. --. This switch determines whether the memory access is to be in accordance with a mode called \u201cevict on write\u201d or not per .","At , it is determined whether current memory access is responsive to a store by a speculative thread. If so, there will be a write through from L1 to L2 at , but the line will be deleted from the L1 at .","If access is not a store by a speculative thread, there is a test as to whether the access is a load at . If so, the system must determine at  whether there is a hit in the L1. If so, data is served from L1 at  and L2 is notified of the use of the data at .","If there is not a hit, then data must be fetched from L2 at . If L2 has a speculative version per , the data should not be inserted into L1 per . If L2 does not have a speculative version, then the data can be inserted into L1 per .","If the access is not a load, then the system must test whether speculation is finished at . If so, the speculative status should be removed from L2 at .","If speculation is not finished, and none of the other conditions are met, then default memory access behavior occurs at .","A programmer will have to determine whether or not to activate evict on write in response to application specific programming considerations. For instance, if data is to be used frequently, the addressing mechanism of FIG. -- will likely be advantageous.","If many small sections of code without frequent data accesses are to be executed in parallel, the mechanism of short running speculation will likely be advantageous.","L1\/L1P Hit Race Condition","FIG. -- shows a simplified explanation of a race condition. When the L1P prefetches data, this data is not flagged by the L2 as read by the speculative thread. The same is true for any data residing in L1 when entering a transaction in TM.","In case of a hit in L1P or L1 for TM at , a notification for this address is sent to L2 , flagging the line as speculatively accessed. If a write from another core at  to that address reaches the L2 before the L1\/L1P hit notification and the write caused invalidate request has not reached the L1 or L1P before the L1\/L1P hit, the core could have used stale data and while flagging new data to be read in the L2. The L2 sees the L1\/L1P hit arriving after the write at  and cannot deduce directly from the ordering if a race occurred. However, in this case a use notification arrives at the L2 with the coherence bits of the L2 denoting that the core did not have a valid copy of the line, thus indicating a potential violation. To retain functional correctness, the L2 invalidates the affected speculation ID in this case at .","Coherence","A thread starting a long-running speculation always begins with an invalidated L1, so it will not retain stale data from a previous thread's execution. Within a speculative domain, L1 invalidations become unnecessary in some cases:\n\n","A thread using short running speculation evicts the line it writes to from its L1 due to the proposed evict on speculative write. This line is evicted from other L1 caches as well based on the usual coherence rules. Starting from this point on, until the speculation is deemed either to be successful or its changes have been reverted, L1 misses for this line will be served from the L2 without entering the L1 and therefore no incoherent L1 copy can occur.","Between speculative domains, the usual multiprocessor coherence rules apply. To support speculation, the L2 routinely records thread IDs associated with reads; on a write, the L2 sends invalidations to all processors outside the domain that are marked as having read that address.","Access Size Signaling from the L1\/L1p to the L2","Memory write accesses footprints are always precisely delivered to L2 as both L1 as well as L1P operate in write-through.","For reads however, the data requested from the L2 does not always match its actual use by a thread inside the core. However, both the L1 as well as the L1P provide methods to separate the actual use of the data from the amount of data requested from the L2.","The L1 can be configured such that it provides on a read miss not only the 64B line that it is requesting to be delivered, but also the section inside the line that is actually requested by the load instruction triggering the miss. It can also send requests to the L1P for each L1 hit that indicate which section of the line is actually read on each hit. This capability is activated and used for short running speculation. In long running speculation, L1 load hits are not reported and the L2 has to assume that the entire 64B section requested has been actually used by the requesting thread.","The L1P can be configured independently from that to separate L1P prefetch requests from actual L1P data use (L1P hits). If activated, L1P prefetches only return data and do not add IDs to speculative reader sets. L1P read hits return data to the core immediately and send to the L2 a request that informs the L2 about the actual use of the thread.","This disclosure arose in the course of development of a new generation of the IBM\u00ae BluGene\u00ae system. This new generation included several concepts, such as managing speculation in the L2 cache, improving energy efficiency, and using generic cores that conform to the PowerPC architecture usable in other systems such as PCs; however, the invention need not be limited to this context.","An addressing scheme can allow generic cores to be used for a new generation of parallel processing system, thus reducing research, development and production costs. Also creating a system in which prefetch units and L1D caches are shared by hardware threads within a core is energy and floor plan efficient.","FIG. -- shows the role of the Translation Look-aside Buffers (TLB)  in the address mapping process. The goal of the mapping process is to isolate each thread's view of the memory state inside the L1D. This is necessary to avoid making speculative memory changes of one thread visible in the L1D to another thread. It is achieved by assigning for a given virtual address different physical addresses to each thread. These addresses differ only in the upper address bits that are not used to distinguish locations within the smaller implemented main memory space. The left column  shows a table with a column representing the virtual address matching component of the TLB. It matches the hardware thread ID (TID) of the thread executing the memory access and a column directed to the virtual address, in other words the 64 bit address used by the core. In this case, both thread ID 1 and thread ID 2 are seeking to access a virtual address, A. The right column  shows the translation part of the TLB, a \u201cphysical address,\u201d in other words an address to the four piece address space shown in FIG. --. In this case, the hardware thread with ID 1 is accessing a \u201cphysical address\u201d that includes the main memory address A\u2032, corresponding to the virtual address A, plus an offset, n, indicating the first hardware thread. The hardware thread with ID 2 is accessing the \u201cphysical address\u201d that includes the main memory address A\u2032 plus an offset, n, indicating the second hardware thread. Not only does the TLB keep track of a main memory address A\u2032, which is provided by a thread, but it also keeps track of a thread number (0, n, n, n). This table happens to show two threads accessing the same main memory address A\u2032 at the same time, but that need not be the case. The hardware thread number\u2014as opposed to the thread ID\u2014combined with the address A\u2032, will be treated by the L1P as addresses of a four piece \u201caddress space\u201d as shown in FIG. --. This is not to say that the L1P is actually maintaining 256 GB of memory, which would be four times the main memory size. This address space is the conceptual result of the addressing scheme. The L1P acts as if it can address that much data in terms of addressing format, but in fact it targets considerably less cache lines than would be necessary to store that much data.","This address space will have at least four pieces, , , , and , because the embodiment of the core has four hardware threads. If the core had a different number of hardware threads, there could be a different number of pieces of the address space of the L1P. This address space allows each hardware thread to act as if it is running independently of every other thread and has an entire main memory to itself. The hardware thread number indicates to the L1P, which of the pieces is to be accessed.","When a line has been established by a speculative thread or a transaction, the rules for enforcing consistency change. When running purely non-speculative, only write accesses change the memory state; in the absence of writes the memory state can be safely assumed to be constant. When a speculatively running thread commits, the memory state as observed by other threads may also change. The memory subsystem does not have the set of memory locations that have been altered by the speculative thread instantly available at the time of commit, thus consistency has to be ensured by means other than sending invalidates for each affected address. This can be accomplished by taking appropriate action when memory writes occur.","Access Size Signaling from the L1\/L1p to the L2","Memory write accesses footprints are always precisely delivered to L2 as both L1 as well as L1P operate in write-through.","For reads however, the data requested from the L2 does not always match its actual use by a thread inside the core. However, both the L1 as well as the L1P provide methods to separate the actual use of the data from the amount of data requested from the L2.","The L1 can be configured such that it provides on a read miss not only the 64B line that it is requesting to be delivered, but also the section inside the line that is actually requested by the load instruction triggering the miss. It can also send requests to the L1P for each L1 hit that indicate which section of the line is actually read on each hit. This capability is activated and used for short running speculation. In long running speculation, L1 load hits are not reported and the L2 has to assume that the entire 64B section requested has been actually used by the requesting thread.","The L1P can be configured independently from that to separate L1P prefetch requests from actual L1P data use (L1P hits). If activated, L1P prefetches only return data and do not add IDs to speculative reader sets. L1P read hits return data to the core immediately and send to the L2 a request that informs the L2 about the actual use of the thread.","The inventor here has discovered, that, surprisingly, given the extraordinary size of this type of supercomputer system, the caches, originally sources of efficiency and power reduction, have become significant power consumers\u2014so that they themselves must be scrutinized to see how they can be improved.","The architecture of the current version of IBM\u00ae Blue Gene\u00ae supercomputer includes coordinating speculative execution at the level of the L2 cache, with results of speculative execution being stored by hashing a physical main memory address to a specific cache set\u2014and using a software thread identification number along with upper address bits to direct memory accesses to corresponding ways of the set. The directory lookup for the cache becomes the conflict checking mechanism for speculative execution.","In a cache that has 16 ways, each memory access request for a given cache line, requires searching all 16 ways of the selected set along with elaborate conflict checking. When multiplied by the thousands of caches in the system, these lookups become energy inefficient\u2014especially in the case where several sequential, or nearly sequential, lookups access the same line.","Thus the new generation of supercomputer gave rise to an environment where directory lookup becomes a significant component of the energy efficiency of the system. Accordingly, it would be desirable to save results of lookups in case they are needed by subsequent memory access requests.","The following document relates to write piggybacking in the context of DRAM controllers:\n\n","It would be desirable to reduce directory SRAM accesses to reduce power and increase throughput in accordance with one or both of the following methods:\n\n","These methods are especially effective if the memory access request generating unit can provide a hint whether this location might be accessed soon or if the access request type implies that other cores will access this location soon, e.g., atomic operation requests for barriers.","Throughout this disclosure a particular embodiment of a multi-processor system will be discussed. This discussion may include various numerical values. These numerical values are not intended to be limiting, but only examples. One of ordinary skill in the art might devise other examples as a matter of design choice.","The present invention arose in the context of the IBM\u00ae Blue Gene\u00ae project, which is further described in the applications incorporated by reference above. FIG. -- is a schematic diagram of an overall architecture of a multiprocessor system in accordance with this project, and in which the invention may be implemented. At , there are a plurality of processors operating in parallel along with associated prefetch units and L1 caches. At , there is a switch. At , there are a plurality of L2 slices. At , there is a main memory unit. It is envisioned, for the preferred embodiment, that the L2 cache should be the point of coherence.","FIG. -- shows a cache slice. It includes arrays of data storage , and a central control portion .","FIG. -- shows features of an embodiment of the control section  of a cache slice .","Coherence tracking unit  issues invalidations, when necessary. These invalidations are issued centrally, while in the prior generation of the Blue Gene\u00ae project, invalidations were achieved by snooping.","The request queue  buffers incoming read and write requests. In this embodiment, it is 16 entries deep, though other request buffers might have more or less entries. The addresses of incoming requests are matched against all pending requests to determine ordering restrictions. The queue presents the requests to the directory pipeline  based on ordering requirements.","The write data buffer  stores data associated with write requests. This buffer passes the data to the eDRAM pipeline  in case of a write hit or after a write miss resolution.","The directory pipeline  accepts requests from the request queue , retrieves the corresponding directory set from the directory SRAM , matches and updates the tag information, writes the data back to the SRAM and signals the outcome of the request (hit, miss, conflict detected, etc.).","The L2 implements four parallel eDRAM pipelines  that operate independently. They may be referred to as eDRAM bank 0 to eDRAM bank 3. The eDRAM pipeline controls the eDRAM access and the dataflow from and to this macro. If writing only subcomponents of a doubleword or for load-and-increment or store-add operations, it is responsible to schedule the necessary RMW cycles and provide the dataflow for insertion and increment.","The read return buffer  buffers read data from eDRAM or the memory controller  and is responsible for scheduling the data return using the switch . In this embodiment it has a 32B wide data interface to the switch. It is used only as a staging buffer to compensate for backpressure from the switch. It is not serving as a cache.","The miss handler  takes over processing of misses determined by the directory. It provides the interface to the DRAM controller and implements a data buffer for write and read return data from the memory controller.","The reservation table  registers and invalidates reservation requests.","In the current embodiment of the multi-processor, the bus between the L1 to the L2 is narrower than the cache line width by a factor of 8. Therefore each write of an entire L2 line, for instance, will require 8 separate transmissions to the L2 and therefore 8 separate lookups. Since there are 16 ways, that means a total of 128 way data retrievals and matches. Each lookup potentially involves all this conflict checking that was just discussed, which can be very energy-consuming and resource intensive.","Therefore it can be anticipated that\u2014at least in this case\u2014an access will need to be retained. A prefetch unit can annotate its request indicating that it is going to access the same line again to inform the L2 slice of this anticipated requirement.","Certain instruction types, such as atomic operations for barriers, might result in an ability to anticipate sequential memory access requests using the same data.","One way of retaining a lookup would be to have a special purpose register in the L2 slice that would retain an identification of the way in which the requested address was found. Alternatively, more registers might be used if it were desired to retain more accesses.","Another embodiment for retaining a lookup would be to actually retain data associated with a previous lookup to be used again.","An example of the former embodiment of retaining lookup information is shown in FIG. --. The L2 slice  includes a request queue . At , a cascade of modules tests whether pending memory access requests will require data associated with the address of a previous request, the address being stored at . These tests might look for memory mapped flags from the L1 or for some other identification. A result of the cascade  is used to create a control input at  for selection of the next queue entry for lookup at , which becomes an input for the directory look up module . These mechanisms can be used for reordering, analogously to the Shao article above, i.e., selecting a matching request first. Such reordering, together with the storing of previous lookup results, can achieve additional efficiencies.","FIG. -- shows more about the interaction between the directory pipe  and the directory SRAM . The vertical lines in the pipe represent time intervals during which data passes through a cascade of registers in the directory pipe. In a first time interval T1, a read is signaled to the directory SRAM. In a second time interval T2, data is read from the directory SRAM. In a third time interval, T3, the directory matching phase may alter directory data and a table lookup informs writes WR and WR DATA to the directory SRAM provide it via the Write and Write Data ports to the directory SRAM. In general, table lookup will govern the behavior of the directory SRAM to control cache accesses responsive to speculative execution. Only one table lookup is shown at T3, but more might be implemented. More detail about the lookup is to be found in the applications incorporated by reference herein, but, since coherence is primarily implemented in this lookup, it is an elaborate process. In particular, in the current embodiment, speculative results from different concurrent processes may be stored in different ways of the same set of the cache. Records of memory access requests and line evictions during concurrent speculative execution will be retained this directory. Moreover, information from cache lines, such as whether a line is shared by several cores, may be retained in the directory. Conflict checking will include checking these records and identifying an appropriate way to be used by a memory access request. Retaining lookup information can reduce use of this conflict checking mechanism.","A traditional store-operate instruction reads from, modifies, and writes to a memory location as an atomic operation. The atomic property allows the store-operate instruction to be used as a synchronization primitive across multiple threads. For example, the store- and instruction atomically reads data in a memory location, performs a bitwise logical- and operation of data (i.e., data described with the store-add instruction) and the read data, and writes the result of the logical- and operation into the memory location. The term store-operate instruction also includes the fetch-and-operate instruction (i.e., an instructions that returns a data value from a memory location and then modifies the data value in the memory location). An example of a traditional fetch-and-operate instruction is the fetch-and-increment instruction (i.e., an instruction that returns a data value from a memory location and then increments the value at that location).","In a multi-threaded environment, the use of store-operate instructions may improve application performance (e.g., better throughput, etc.). Because atomic operations are performed within a memory unit, the memory unit can satisfy a very high rate of store-operate instructions, even if the instructions are to a single memory location. For example, a memory system of IBM\u00ae Blue Gene\u00ae\/Q computer can perform a store-operate instruction every 4 processor cycles. Since a store-operate instruction modifies the data value at a memory location, it traditionally invokes a memory coherence operation to other memory devices. For example, on the IBM\u00ae Blue Gene\u00ae\/Q computer, a store-operate instruction can invoke a memory coherence operation on up to 15 level-1 (L1) caches (i.e., local caches). A high rate (e.g., every 4 processor cycles) of traditional store-operate instructions thus causes a high rate (e.g., every 4 processor cycles) of memory coherence operations which can significantly occupy computer resources and thus reduce application performance.","The present disclosure further describes a method, system and computer program product for performing various store-operate instructions in a parallel computing system that reduces the number of cache coherence operations and thus increases application performance.","In one embodiment, there are provided various store-operate instructions available to a computing device to reduce the number of memory coherence operations in a parallel computing environment that includes a plurality of processors, at least one cache memory and at least one main memory. These various provided store-operate instructions are variations of a traditional store-operate instruction that atomically modify the data (e.g., bytes, bits, etc.) at a (cache or main) memory location. These various provided store-operate instructions include, but are not limited to: StoreOperateCoherenceOnValue instruction, StoreOperateCoherenceThroughZero instruction and StoreOperateCoherenceOnPredecessor instruction. In one embodiment, the term store-operate instruction(s) also includes the fetch-and-operate instruction(s). These various provided fetch-and-operate instructions thus also include, but are not limited to: FetchAndOperateCoherenceOnValue instruction, FetchAndOperateCoherenceThroughZero instruction and FetchAndOperateCoherenceOnPredecessor instruction.","In one aspect, a StoreOperateCoherenceOnValue instruction is provided that improves application performance in a parallel computing environment (e.g., IBM\u00ae Blue Gene\u00ae computing devices L\/P, etc. such as described in herein incorporated US Provisional Application Ser. No. 61\/295,669), by reducing the number of cache coherence operations invoked by a functional unit (e.g., a functional unit  in FIG. --). The StoreOperateCoherenceOnValue instruction invokes a cache coherence operation only when the result of a store-operate instruction is a particular value or set of values. The particular value may be given by the instruction issued from a processor in the parallel computing environment. The StoreOperateCoherenceThroughZero instruction invokes a cache coherence operation only when data (e.g., a numerical value) in a (cache or main) memory location described in the StoreAddCoherenceThroughZero instruction changes from a positive value to a negative value, or vice versa. The StoreOperateCoherenceOnPredecessor instruction invokes a cache coherence operation only when the result of a StoreOperateCoherenceOnPredecessor instruction is equal to data (e.g., a numerical value) stored in a preceding memory location of a logical memory address described in the StoreOperateCoherenceOnPredecessor instruction. These instructions are described in detail in conjunction with FIGS. --A---B.","The FetchAndOperateCoherenceOnValue instruction invokes a cache coherence operation only when a result of the fetch-and-operate instruction is a particular value or set of values. The particular value may be given by the instruction issued from a processor in the parallel computing environment. The FetchAndOperateCoherenceThroughZero instruction invokes a cache coherence operation only when data (e.g., a numerical value) in a (cache or main) memory location described in the fetch-and-operate instruction changes from a positive value to a negative value, or vice versa. The FetchAndOperateCoherenceOnPredecessor instruction invokes a cache coherence operation only when the result of a fetch-and-operate instruction (i.e., the read data value in a memory location described in the fetch-and-operate instruction) is equal to particular data (e.g., a particular numerical value) stored in a preceding memory location of a logical memory address described in the fetch-and-operate instruction.","FIG. -- illustrates a portion of a parallel computing environment  employing the system and method of the present invention in one embodiment. The parallel computing environment may include a plurality of processors (Processor 1 (), Processor 2 (), . . . , and Processor N ()). In one embodiment, these processors are heterogeneous (e.g., a processor is IBM\u00ae PowerPC\u00ae, another processor is Intel\u00ae Core). In another embodiment, these processors are homogeneous (i.e., identical each other). A processor may include at least one local cache memory device. For example, a processor 1 () includes a local cache memory device . A processor 2 () includes a local cache memory device .","A processor N () includes a local cache memory device . In one embodiment, the term processor may also refer to a DMA engine or a network adaptor  or similar equivalent units or devices. One or more of these processors may issue load or store instructions. These load or store instructions are transferred from the issuing processors, e.g., through a cross bar switch , to an instruction queue  in a memory or cache unit . A functional unit (FU)  fetches these instructions from the instruction queue , and runs these instructions. To run one or more of these instructions, the FU  may retrieve data stored in a cache memory  or in a main memory (not shown) via a main memory controller . Upon completing the running of the instructions, the FU  may transfer outputs of the run instructions to the issuing processor or network adaptor via the network  and\/or store outputs in the cache memory  or in the main memory (not shown) via the main memory controller . The main memory controller  is a traditional memory controller that manages data flow between the main memory device and other components (e.g., the cache memory device , etc.) in the parallel computing environment .","FIGS. --A---B illustrates operations of the FU  to run the StoreOperateCoherenceOnValue instruction in one embodiment. The FU  fetches an instruction  from the instruction queue . FIG. -- illustrates composition of the instruction  in one embodiment. The instruction  includes an Opcode  specifying what is to be performed by the FU  (e.g., reading data from a memory location, storing data to a memory location, store-add, store, max or other store-operate instruction, fetch-and-increment, fetch-and-decrement or other fetch-and-operate instruction, etc.). The Opcode  may include further information e.g., the width of an operand value . The instruction  also includes a logical address  specifying a memory location from which data is to be read and\/or stored. In the case of a store instruction, the instruction  includes the operand value  to be stored to the memory location. Similarly, in the case of a store-operate instruction, the instruction  includes the operand value  to be used in an operation with an existing memory value with an output value to be stored to the memory location. Similarly, in the case of a fetch-and-operate instructions, the instruction  may include an operand value  to be used in an operation with the existing memory value with an output value to be stored to the memory location. Alternatively, the operand value  may correspond to a unique identification number of a register. The instruction  may also include an optional field  whose value is used by a store-operate or fetch-and-operate instruction to determine if a cache coherence operation should be invoked. In one embodiment, the instruction , including the optional field  and the Opcode  and the logical address , but excluding the operand value , has a width of 32 bits or 64 bits or other widths. The operand value  typically has widths of 1 byte, 4 byte, 8 byte, 16 byte, 32 byte, 64 byte, 128 byte or other widths.","In one embodiment, the instruction  specifies at least one condition under which a cache coherence operation is invoked. For example, the condition may specifies a particular value, e.g., zero.","Upon fetching the instruction  from the instruction queue , the FU  evaluates  whether the instruction  is a load instruction, e.g., by checking whether the Opcode  of the instruction  indicates that the instruction  is a load instruction. If the instruction  is a load instruction, the FU  reads  data stored in a (cache or main) memory location corresponding to the logical address  of the instruction , and uses the crossbar  to return the data to the issuing processor. Otherwise, the FU  evaluates  whether the instruction  is a store instruction, e.g., by checking whether the Opcode  of the instruction  indicates that the instruction  is a store instruction. If the instruction  is a store instruction, the FU  transfers  the operand value  of the instruction ) to a (cache or main) memory location corresponding to the logical address  of the instruction . Because a store instruction changes the value at a memory location, the FU  invokes , e.g. via cross bar , a cache coherence operation on other memory devices such as L1 caches \u2014 in processors\u2014-. Otherwise, the FU  evaluates  whether the instruction  is a store-operate or fetch-and-operate instruction, e.g., by checking whether the Opcode  of the instruction  indicates that the instruction  is a store-operate or fetch-and-operate instruction.","If the instruction  is a store-operate instruction, the FU  reads  data stored in a (cache or main) memory location corresponding to the logical address  of the instruction , modifies  the read data with the operand value  of the instruction, and writes  the result of the modification to the (cache or main) memory location corresponding to the logical address  of the instruction. Alternatively, the FU modifies  the read data with data stored in a register (e.g., accumulator) corresponding to the operand value , and writes  the result to the memory location. Because a store-operate instruction changes the value at a memory location, the FU  invokes , e.g. via cross bar , a cache coherence operation on other memory devices such as L1 caches - in processors\u2014-.","If the instruction  is a fetch-and-operate instruction, the FU  reads  data stored in a (cache or main) memory location corresponding to the logical address  of the instruction  and return, via the crossbar , the data to the issuing processor. The FU then modifies  the data, e.g., with an operand value  of the instruction , and writes  the result of the modification to the (cache or main) memory location. Alternatively, the FU modifies  the data stored in the (cache or main) memory location, e.g., with data stored in a register (e.g., accumulator) corresponding to the operand value , and writes the result to the memory location. Because a fetch-and-operate instruction changes the value at a memory location, the FU  invokes , e.g. via cross bar , a cache coherence operation on other memory devices such as L1 caches - in processors -.","Otherwise, the FU  evaluates  whether the instruction  is a StoreOperateCoherenceOnValue instruction or FetchAndOperateCoherenceOnValue instruction, e.g., by checking whether the Opcode  of the instruction  indicates that the instruction  is a StoreOperateCoherenceOnValue instruction. If the instruction  is a StoreOperateCoherenceOnValue instruction, the FU  performs operations  which is shown in detail in FIG. --B. The StoreOperateCoherenceOnValue instruction  includes the Store Operate operation  described above. The StoreOperateCoherenceOnValue instruction  invokes a cache coherence operation on other memory devices when the condition specified in the StoreOperateCoherenceOnValue instruction is satisfied. As shown in FIG. --B, upon receiving from the instruction queue  the StoreOperateCoherenceOnValue instruction, the FU  performs the store-operate operation described in the StoreOperateCoherenceOnValue instruction. The FU  evaluates  whether the result  of the store-operate operation is a particular value. In one embodiment, the particular value is implicit in the Opcode , for example, a value zero. In one embodiment, as shown in FIG. --, the instruction may include an optional field  that specifies this particular value. The FU  compares the result  to the particular value implicit in the Opcode  or explicit in the optional field  in the instruction . If the result is the particular value, the FU  invokes , e.g. via cross bar , a cache coherence operation on other memory devices such as L1 caches - in processors -. Otherwise, if the result  is not the particular value, the FU  does not invoke  a cache coherence operation on other memory devices.","If the instruction  is a FetchAndOperateCoherenceOnValue instruction, the FU  performs operations  which is shown in detail in FIG. --B. The FetchAndOperateCoherenceOnValue instruction  includes the FetchAndOperate operation  described above. The FetchandOperateCoherenceOnValue instruction  invokes a cache coherence operation on other memory devices only if a condition specified in the FetchandOperateCoherenceOnValue instruction  is satisfied. As shown in FIG. --B, upon receiving from the instruction queue  the FetchAndOperateCoherenceOnValue instruction , the FU  performs a fetch-and-operate operation described in the FetchAndOperateCoherenceOnValue instruction. The FU  evaluates  whether the result  of the fetch-and-operate operation is a particular value. In one embodiment, the particular value is implicit in the Opcode , for example, a numerical value zero. In one embodiment, as shown in FIG. --, the instruction may include an optional field  that includes this particular value. The FU  compares the result value  to the particular value implicit in the Opcode  or explicit in the optional field  in the instruction . If the result value  is the particular value, the FU  invokes  e.g. via cross bar , a cache coherence operation on other memory devices, e.g., L1 caches - in processors -. Otherwise, if the result is not the particular value, the FU  does not invoke  the cache coherence operation on other memory devices.","In one embodiment, the StoreOperateCoherenceOnValue  instruction described above is a StoreAddInvalidateCoherenceOnZero instruction. The value in a memory location at the logical address  is considered to be an integer value. The operand value  is also considered to be an integer value. The StoreAddInvalidateCoherenceOnZero instruction adds the operand value to the previous memory value and stores the result of the addition as a new memory value in the memory location at the logical address . In one embodiment, a network adapter  may use the StoreAddInvalidateCoherenceOnZero instruction. In this embodiment, the network adaptor  interfaces the parallel computing environment  to a network  which may deliver a message as out-of-order packets. A complete reception of a message can be recognized by initializing a counter to the number of bytes in the message and then having the network adaptor decrement the counter by the number of bytes in each arriving packet. The memory device  is of a size that allows any location in a (cache) memory device to serve as such a counter for each message. Applications on the processors - poll the counter of each message to determine if a message has completely arrived. On reception of each packet, the network adaptor can issue a StoreAddInvalidateCoherenceOnZero instruction  to the memory device . The Opcode  specifies the StoreAddInvalidateCoherenceOnZero instruction. The logical address  is that of the counter. The operand value  is a negative value of the number of received bytes in the packet. In this embodiment, only when the counter reaches the value 0, the memory device  invokes a cache coherence operation to the level-1 (L1) caches of the processors -. This improves the performance of the application, since the application demands the complete arrival of each message and is uninterested in a message for which all packets have not yet arrived and only invokes the cache coherence operation only when all packets of the message arrives at the network adapter . By contrast, the application performance on the processors - may be decreased if the network adaptor  issues a traditional Store-Add instruction, since then each of the processors - would then receive and serve an unnecessary cache coherence operation upon the arrival of each packet.","In one embodiment, the FetchAndOperateCoherenceOnZero instruction  described above is a FetchAndDecrementCoherenceOnZero instruction. The value in a memory location at the logical address  is considered to be an integer value. There is no accompanying operand value . The FetchAndIncrementCoherenceOnZero instruction returns the previous value of the memory location and then increments the value at the memory location. In one embodiment, the processors - may use the FetchAndIncrementCoherenceOnZero instruction to implement a barrier (i.e., a point where all participating threads must arrive, and only then can the each thread proceed with its execution). The barrier uses a memory location in the memory device  (e.g., a shared cache memory device) as a counter. The counter is initialized with the number of threads to participate in the barrier. Each thread, upon arrival at the barrier issues a FetchAndDecrementCoherenceOnZero instruction  to the memory device . The Opcode  specifies the FetchAndDecrementCoherenceOnZero instruction. The memory location of the logical address  stores a value of the counter. The value \u201c1\u201d is returned by the FetchAndDecrementCoherenceOnZero instruction to the last thread arriving at the barrier and the value \u201c0\u201d is stored to the memory location and a cache coherence operation is invoked. Given this value \u201c1\u201d, the last thread knows all threads have arrived at the barrier and thus the last thread can exit the barrier. For the other earlier threads to arrive at the barrier, the value \u201c1\u201d is not returned by the FetchAndDecrementCoherenceOnZero. So, each of these threads polls the counter for the value 0 indicating that all threads have arrived. Only when the counter reaches the value \u201c0,\u201d the FetchAndDecrementCoherenceOnZero instruction causes the memory device  to invoke a cache coherence operation to the level-1 (L1) caches - of the processors -. This FetchAndDecrementCoherenceOnZero instruction thus helps reduce computer resource usage in a barrier and thus helps improve the application performance. The polling mainly uses the L1-cache (local cache memory device in a processor; local cache memory devices -) of each processor -. By contrast, the barrier performance may be decreased if the barrier used a traditional Fetch-And-Decrement instruction, since then each of the processors - would then receive and serve an unnecessary cache coherence operation on the arrival of each thread into the barrier and thus would cause polling to communicate more with the memory device  and communicate less with local cache memory devices.","FIGS. --A---B illustrate operations of the FU  to run a StoreOperateCoherenceOnPredecessor instruction or FetchAndOperateCoherenceOnPredecessor instruction in one embodiment. FIGS. --A---B are similar to FIGS. --A---B except that the FU evaluates  whether the instruction  is the StoreOperateCoherenceOnPredecessor instruction or FetchAndOperateCoherenceOnPredecessor instruction, e.g., by checking whether the Opcode  of the instruction  indicates that the instruction  is a StoreOperateCoherenceOnPredecessor instruction. If the instruction  is a StoreOperateCoherenceOnPredecessor instruction, the FU  performs operations  which is shown in detail in FIG. --B. The StoreOperateCoherenceOnPredecessor instruction  is similar to the StoreOperateCoherenceOnValue operation  described above, except that the StoreOperateCoherenceOnPredecessor instruction  uses a different criterion to determine whether or not to invoke a cache coherence operation on other memory devices. As shown in FIG. --B, upon receiving from the instruction queue  the StoreOperateCoherenceOnPredecessor instruction, the FU  performs the store-operate operation described in the StoreOperateCoherenceOnPredecessor instruction. The FU  evaluates  whether the result  of the store-operate operation is equal to the value stored in the preceding memory location (i.e., logical address-1). If equal, the FU  invokes , e.g. via cross bar , a cache coherence operation on other memory devices (e.g., local cache memories in processors -). Otherwise, if the result  is not equal to the value in the preceding memory location, the FU  does not invoke  a cache coherence operation on other memory devices.","If the instruction  is a FetchAndOperateCoherenceOnPredecessor instruction, the FU  performs operations  which is shown in detail in FIG. --B. The FetchAndOperateCoherenceOnPredecessor instruction  is similar to FetchAndOperateCoherenceOnValue operation  described above, except that the FetchAndOperateCoherenceOnPredecessor operation  uses a different criterion to determine whether or not to invoke a cache coherence operation on other memory devices. As shown in FIG. --B, upon receiving from the instruction queue  the FetchAndOperateCoherenceOnPredecessor instruction, the FU  performs the fetch-and-operate operation described in the FetchAndOperateCoherenceOnPredecessor instruction. The FU  evaluates  whether the result  of the fetch-and-operate operation is equal to the value stored in the preceding memory location. If equal, the FU  invokes , e.g. via cross bar , a cache coherence operation on other memory devices (e.g., L1 cache memories in processors -). Otherwise, if the result  is not equal to the value in the preceding memory location, the FU  does not invoke  a cache coherence operation on other memory devices.","FIGS. --A---B illustrate operations of the FU  to run a StoreOperateCoherenceThroughZero instruction or FetchAndOperateCoherenceThroughZero instruction in one embodiment. FIGS. --A---B are similar to FIGS. --A---B except that the FU evaluates  whether the instruction  is the StoreOperateCoherenceThroughZero instruction or FetchAndOperateCoherenceThroughZero instruction, e.g., by checking whether the Opcode  of the instruction  indicates that the instruction  is a StoreOperateCoherenceThroughZero instruction. If the instruction  is a StoreOperateCoherenceThroughZero instruction, the FU  performs operations  which is shown in detail in FIG. --B. The StoreOperateCoherenceThroughZero operation  is similar to the StoreOperateCoherenceOnValue operation  described above, except that the StoreOperateCoherenceThroughZero operation  uses a different criterion to determine whether or not to invoke a cache coherence operation on other memory devices. As shown in FIG. --B, upon receiving from the instruction queue  the StoreOperateCoherenceThroughZero instruction, the FU  performs the store-operate operation described in the StoreOperateCoherenceThroughZero instruction. The FU  evaluates  whether a sign (e.g., positive (+) or negative (\u2212)) of the result  of the store-operate is an opposite to a sign of an original value in the memory location corresponding to the logical address . If opposite, the FU  invokes , e.g. via cross bar , a cache coherence operation on other memory devices (e.g., L1 caches - in processors -). Otherwise, if the result  does not have the opposite sign of the original value in the memory location, the FU  does not invoke  a cache coherence operation on other memory devices.","If the instruction  is a FetchAndOperateCoherenceThroughZero instruction, the FU  performs operations  which is shown in detail in FIG. --B. The FetchAndOperateCoherenceThroughZero operation  is similar to the FetchAndOperateCoherenceOnValue operation  described above, except that the FetchAndOperateCoherenceThroughZero operation  uses a different criterion to determine whether or not to invoke a cache coherence operation on other memory devices. As shown in FIG. --B, upon receiving from the instruction queue  the FetchAndOperateCoherenceThroughZero instruction, the FU  performs the fetch-and-operate operation described in the FetchAndOperateCoherenceThroughZero instruction. The FU  evaluates  whether a sign of the result  of the fetch-and-operate operation is opposite to the sign of an original value in the memory location. If opposite, the FU  invokes , e.g. via cross bar , a cache coherence operation on other memory devices (e.g., in processors -). Otherwise, if the result  does not have the opposite sign of the original value in the memory location, the FU  does not invoke  a cache coherence operation on other memory devices.","In one embodiment, the store-operate operation described in the StoreOperateCoherenceOnValue or StoreOperateCoherenceOnPredecessor or StoreOperateCoherenceThroughZero includes one or more of the following traditional operations that include, but are not limited to: StoreAdd, StoreMin and StoreMax, each with variations for signed integers, unsigned integers or floating point numbers, Bitwise StoreAnd, Bitwise StoreOr, Bitwise StoreXor, etc.","In one embodiment, the Fetch-And-Operate operation described in the FetchAndOperateCoherenceOnValue or FetchAndOperateCoherenceOnPredecessor or FetchAndOperateCoherenceThroughZero includes one or more of the following traditional operations that include, but are not limited to: FetchAndIncrement, FetchAndDecrement, FetchAndClear, etc.","In one embodiment, the width of the memory location operated by the StoreOperateCoherenceOnValue or StoreOperateCoherenceOnPredecessor or StoreOperateCoherenceThroughZero or FetchAndOperateCoherenceOnValue or FetchAndOperateCoherenceOnPredecessor or FetchAndOperateCoherenceThroughZero includes, but is not limited to: 1 byte, 2 byte, 4 byte, 8 byte, 16 byte, and 32byte, etc.","In one embodiment, the FU  performs the evaluations -,  and  sequentially. In another embodiment, the FU  performs the evaluations -,  and  concurrently, i.e., in parallel. For example, FIG. -- illustrates the FU  performing these evaluations in parallel. The FU  fetches the instruction  from the instruction . The FU  provides the same fetched instruction  to comparators - (i.e., comparators that compares the Opcode  of the instruction  to a particular instruction set). In one embodiment, a comparator implements an evaluation step (e.g., the evaluation  shown in FIG. --A). For example, a comparator  compares the Opcode  of the instruction  to a predetermined Opcode corresponding to a load instruction. In one embodiment, there are provided at least six comparators, each of which implements one of these evaluations -,  and . The FU  operates these comparators in parallel. When a comparator finds a match between the Opcode  of the instruction  and a predetermined Opcode in an instruction set (e.g., a predetermined Opcode of StoreOperateCoherenceOnValue instruction), the FU performs the corresponding operation (e.g., the operation ). In one embodiment, per an instruction, only a single comparator finds a match between the Opcode of that instruction and a predetermined Opcode in an instruction set.","In one embodiment, threads or processors concurrently may issue one of these instructions (e.g., StoreOperateCoherenceOnValue instruction, StoreOperateCoherenceThroughZero instruction, StoreOperateCoherenceOnPredecessor instruction, FetchAndOperateCoherenceOnValue instruction, FetchAndOperateCoherenceThroughZero instruction, FetchAndOperateCoherenceOnPredecessor instruction) to a same (cache or main) memory location. Then, the FU  may run these concurrently issued instructions every few processor clock cycles, e.g., in parallel or sequentially. In one embodiment, these instructions (e.g., StoreOperateCoherenceOnValue instruction, StoreOperateCoherenceThroughZero instruction, StoreOperateCoherenceOnPredecessor instruction, FetchAndOperateCoherenceOnValue instruction, FetchAndOperateCoherenceThroughZero instruction, FetchAndOperateCoherenceOnPredecessor instruction) are atomic instructions that atomically implement operations on cache lines.","In one embodiment, the FU  is implemented in hardware or reconfigurable hardware, e.g., FPGA (Field Programmable Gate Array) or CPLD (Complex Programmable Logic Device), e.g., by using a hardware description language (Verilog, VHDL, Handel-C, or System C). In another embodiment, the FU  is implemented in a semiconductor chip, e.g., ASIC (Application-Specific Integrated Circuit), e.g., by using a semi-custom design methodology, i.e., designing a chip using standard cells and a hardware description language.","It would be desirable to allow for multiple modes of speculative execution concurrently in a multiprocessor system.","In one embodiment, a computer method includes carrying out operations in a multiprocessor system. The operations include:\n\n","In another embodiment, the operations include\n\n","In yet another embodiment, a multiprocessor system includes:\n\n","It would be desirable to prevent speculative memory accesses from going to main memory to improve efficiency of a multiprocessor system.","In one embodiment, a method for managing memory accesses in a multiprocessor system includes carrying out operations within the system. The operations include:\n\n","In another embodiment, a cache memory for use in a multiprocessor system includes:\n\n","Yet another embodiment is a cache control system for use in a multiprocessor system including\n\n","The term \u201cthread\u201d is used herein. A thread can be either a hardware thread or a software thread. A hardware thread within a core processor includes a set of registers and logic for executing a software thread. The software thread is a segment of computer program code. Within a core, a hardware thread will have a thread number. For instance, in the A2, there are four threads, numbered zero through three. Throughout a multiprocessor system, such as the nodechip  of , 68 software threads can be executed concurrently in the present embodiment.","These threads can be the subject of \u201cspeculative execution,\u201d meaning that a thread or threads can be started as a sort of wager or gamble, without knowledge of whether the thread can complete successfully. A given thread cannot complete successfully if some other thread modifies the data that the given thread is using in such a way as to invalidate the given thread's results. The terms \u201cspeculative,\u201d \u201cspeculatively,\u201d \u201cexecute,\u201d and \u201cexecution\u201d are terms of art in this context. These terms do not imply that any mental step or manual operation is occurring. All operations or steps described herein are to be understood as occurring in an automated fashion under control of computer hardware or software.","Speculation Model","This section describes the underlying speculation ID based memory speculation model, focusing on its most complex usage mode, speculative execution (SE), also referred to as thread level speculation (TLS). When referring to threads, the terms older\/younger or earlier\/later refer to their relative program order (not the time they actually run on the hardware).","Multithreading Model","In Speculative Execution, successive sections of sequential code are assigned to hardware threads to run simultaneously. Each thread has the illusion of performing its task in program order. It sees its own writes and writes that occurred earlier in the program. It does not see writes that take place later in program order even if, because of the concurrent execution, these writes have actually taken place earlier in time.","To sustain the illusion, the memory subsystem, in particular in the preferred embodiment the L2-cache, gives threads private storage as needed. It lets threads read their own writes and writes from threads earlier in program order, but isolates their reads from threads later in program order. Thus, the L2 might have several different data values for a single address. Each occupies an L2 way, and the L2 directory records, in addition to the usual directory information, a history of which threads have read or written the line. A speculative write is not to be written out to main memory.","One situation will break the program-order illusion\u2014if a thread earlier in program order writes to an address that a thread later in program order has already read. The later thread should have read that data, but did not. A solution is to kill the later thread and invalidate all the lines it has written in L2, and to repeat this for all younger threads. On the other hand, without this interference a thread can complete successfully, and its writes can move to external main memory when the line is cast out or flushed.","Not all threads need to be speculative. The running thread earliest in program order can execute as non-speculative and runs conventionally; in particular its writes can go to external main memory. The threads later in program order are speculative and are subject to being killed. When the non-speculative thread completes, the next-oldest thread can be committed and it then starts to run non-speculatively.","The following sections describe a hardware implementation embodiment for a speculation model.","Speculation IDs","Speculation IDs constitute a mechanism for the memory subsystem to associate memory requests with a corresponding task, when a sequential program is decomposed into speculative tasks. This is done by assigning an ID at the start of a speculative task to the software thread executing the task and attaching the ID as tag to all requests sent to the memory subsystem by that thread. In SE, a speculation ID should be attached to a single task at a time.","As the number of dynamic tasks can be very large, it is not practical to guarantee uniqueness of IDs across the entire program run. It is sufficient to guarantee uniqueness for all IDs assigned to TLS tasks concurrently present in the memory system.","The BG\/Q memory subsystem embodiment implements a set of 128 such speculation IDs, encoded as 7 bit values. On start of a speculative task, a thread requests an ID currently not in use from a central unit, the L2 CENTRAL unit. The thread then uses this ID by storing its value in a core-local register that tags the ID on all requests sent to the L2-cache.","After a thread has terminated, the changes associated with its ID are either committed, i.e., merged with the persistent main memory state, or they are invalidated, i.e., removed from the memory subsystem, and the ID is reclaimed for further allocation. But before a new thread can use the ID, no valid lines with that thread ID may remain in the L2. It is not necessary for the L2 to identify and mark these lines immediately because the pool of usable IDs is large. Therefore, cleanup is gradual.","Life Cycle of a Speculation ID","FIG. -- illustrates the life cycle of a speculation ID. When a speculation ID is in the available state at , it is unused and ready to be allocated. When a thread requests an ID allocation from L2 CENTRAL, the ID selected by L2 CENTRAL changes state to speculative at , its conflict register is cleared and its A-bit is set at .","The thread starts using the ID with tagged memory requests at . Such tagging may be implemented by the runtime system programming a register to activate the tagging. The application may signal the runtime system to do so, especially in the case of TM. If a conflict occurs at , the conflict is noted in the conflict register of FIG. --E at  and the thread is notified via an interrupt at . The thread can try to resolve the conflict and resume processing or invalidate its ID at . If no conflict occurs until the end of the task per , the thread can try to commit its ID by issuing a try_commit, a table of functions appears below, request to L2 CENTRAL at . If the commit is successful at , the ID changes to the committed state at . Otherwise, a conflict must have occurred and the thread has to take actions similar to a conflict notification during the speculative task execution.","After the ID state change from speculative to committed or invalid, the L2 slices start to merge or invalidate lines associated with the ID at . More about merging lines will be described with reference to FIGS. --E and -- below. The ID does not switch to available until at  all references to the ID have been cleared from the cache and software has explicitly cleared the A-bit per .","In addition to the SE use of speculation, the proposed system can support two further uses of memory speculation: Transactional Memory (\u201cTM\u201d), and Rollback. These uses are referred to in the following as modes.","TM occurs in response to a specific programmer request. Generally the programmer will put instructions in a program delimiting sections in which TM is desired. This may be done by marking the sections as requiring atomic execution. According to the PowerPC architecture: \u201cAn access is single-copy atomic, or simply \u201catomic\u201d, if it is always performed in its entirety with no visible fragmentation\u201d. Alternatively, the programmer may put in a request to the runtime system for a domain to be allocated to TM execution This request will be conveyed by the runtime system via the operating system to the hardware, so that modes and IDs can be allocated. When the section ends, the program will make another call that ultimately signals the hardware to do conflict checking and reporting. Reporting means in this context: provide conflict details in the conflict register and issue an interrupt to the affected thread. The PowerPC architecture has an instruction type known as larx\/stcx. This instruction type can be implemented as a special case of TM. The larx\/stcx pair will delimit a memory access request to a single address and set up a program section that ends with a request to check whether the memory access request was successful or not. More about a special implementation of larx\/stcx instructions using reservation registers is to be found in co-pending application Ser. No. 12\/697,799 filed Jan. 29, 2010, which is incorporated herein by reference. This special implementation uses an alternative approach to TM to implement these instructions. In any case, TM is a broader concept than larx\/stcx. A TM section can delimit multiple loads and stores to multiple memory locations in any sequence, requesting a check on their success or failure and a reversal of their effects upon failure. TM is generally used for only a subset of an application program, with program sections before and after executing in speculative mode.","Rollback occurs in response to \u201csoft errors,\u201d normally these errors occur in response to cosmic rays or alpha particles from solder balls.","Referring back to , there is shown an overall architecture of a multiprocessor computing node  implemented in a parallel computing system in which the present embodiment may be implemented. The compute node  is a single chip (\u201cnodechip\u201d) based on PowerPC cores, though the architecture can use any cores, and may comprise one or more semiconductor chips.","More particularly, the basic nodechip  of the multiprocessor system illustrated in  includes (sixteen or seventeen) 16+1 symmetric multiprocessing (SMP) cores , each core being 4-way hardware threaded supporting transactional memory and thread level speculation, and, including a Quad Floating Point Unit (FPU)  associated with each core. The 16 cores  do the computational work for application programs.","The 17core is configurable to carry out system tasks, such as\n\n","In other words, it offloads all the administrative tasks from the other cores to reduce the context switching overhead for these.","In one embodiment, there is provided 32 MB of shared L2 cache , accessible via crossbar switch . There is further provided external Double Data Rate Synchronous Dynamic Random Access Memory (\u201cDDR SDRAM\u201d) , as a lower level in the memory hierarchy in communication with the L2.","Each FPU  associated with a core  has a data path to the L1-cache  of the CORE, allowing it to load or store from or into the L1-cache . The terms \u201cL1\u201d and \u201cL1D\u201d will both be used herein to refer to the L1 data cache.","Each core  is directly connected to a supplementary processing agglomeration , which includes a private prefetch unit. For convenience, this agglomeration  will be referred to herein as \u201cUP\u201d\u2014meaning level 1 prefetch\u2014or \u201cprefetch unit;\u201d but many additional functions are lumped together in this so-called prefetch unit, such as write combining. These additional functions could be illustrated as separate modules, but as a matter of drawing and nomenclature convenience the additional functions and the prefetch unit will be illustrated herein as being part of the agglomeration labeled \u201cUP.\u201d This is a matter of drawing organization, not of substance. Some of the additional processing power of this L1P group includes write combining. The L1P group also accepts, decodes and dispatches all requests sent out by the core .","By implementing a direct memory access (\u201cDMA\u201d) engine referred to herein as a Messaging Unit (\u201cMU\u201d) such as MU , with each MU including a DMA engine and Network Card interface in communication with the XBAR switch, chip I\/O functionality is provided. In one embodiment, the compute node further includes: intra-rack interprocessor links  which may be configurable as a 5-D torus; and, one I\/O link  interfaced with the interfaced with the MU The system node employs or is associated and interfaced with a 8-16 GB memory\/node, also referred to herein as \u201cmain memory.\u201d","The term \u201cmultiprocessor system\u201d is used herein. With respect to the present embodiment this term can refer to a nodechip or it can refer to a plurality of nodechips linked together. In the present embodiment, however, the management of speculation is conducted independently for each nodechip. This might not be true for other embodiments, without taking those embodiments outside the scope of the claims.","The compute nodechip implements a direct memory access engine DMA to offload the network interface. It transfers blocks via three switch master ports between the L2-cache slices  (). It is controlled by the cores via memory mapped I\/O access through an additional switch slave port. There are 16 individual slices, each of which is assigned to store a distinct subset of the physical memory lines. The actual physical memory addresses assigned to each cache slice is configurable, but static. The L2 will have a line size such as 128 bytes. In the commercial embodiment this will be twice the width of an L1 line. L2 slices are set-associative, organized as 1024 sets, each with 16 ways. The L2 data store may be composed of embedded DRAM and the tag store may be composed of static RAM.","The L2 will have ports, for instance a 256b wide read data port, a 128b wide write data port, and a request port. Ports may be shared by all processors through the crossbar switch .","FIG. --A shows some software running in a distributed fashion, distributed over the cores of node . An application program is shown at . If the application program requests TLS or TM, a runtime system  will be invoked. This runtime system is particularly to manage TM and TLS execution and can request domains of IDs from the operating system . The runtime system can also request allocation of and commits of IDs. The runtime system includes a subroutine that can be called by threads and that maintains a data structure for keeping track of calls for speculative execution from threads. The operating system configures domains and modes of execution. \u201cDomains\u201d in this context are numerical groups of IDs that can be assigned to a mode of speculation. In the present embodiment, an L2 central unit will perform functions such as defining the domains, defining the modes for the domains, allocating speculative ids, trying to commit them, sending interrupts to the cores in case of conflicts, and retrieving conflict information. FIG. -- shows schematically a number of CORE processors . Thread IDs  are assigned centrally and a global thread state  is maintained.","FIG. --B shows a timing diagram explaining how TM execution might work on this system. At  the program starts executing. At the end of block , a call for TM is made. In  the run time system receives this request and conveys it to the operating system. At , the operating system confirms the availability of the mode. The operating system can accept, reject, or put on hold any requests for a mode. The confirmation is made to the runtime system at . The confirmation is received at the application program at . If there had been a refusal, the program would have had to adopt a different strategy, such as serialization or waiting for the domain with the desired mode to become available. Because the request was accepted, parallel sections can start running at the end of . The runtime system gets speculative IDs from the hardware at 146 and transmits them to the application program at , which then uses them to tag memory accesses. The program knows when to finish speculation at the end of . Then the run time system asks for the ID to commit at . Any conflict information can be transmitted back to the application program at , which then may try again or adopt other strategies. If there is a conflict and an interrupt is raised by the L2 central, the L2 will send the interrupt to the hardware thread that was using the ID. This hardware thread then has to figure out, based on the state the runtime system is in and the state the L2 central provides indicating a conflict, what to do in order to resolve the conflict. For example, it might execute the transactional memory section again which causes the software to jump back to the start of the transaction.","If the hardware determines that no conflict has occurred, the speculative results of the associated thread can be made persistent.","In response to a conflict, trying again may make sense where another thread completed successfully, which may allow the current thread to succeed. If both threads restart, there can be a \u201clifelock,\u201d where both keep failing over and over. In this case, the runtime system may have to adopt other strategies like getting one thread to wait, choosing one transaction to survive and killing others, or other strategies, all of which are known in the art.","FIG. --B- shows a timing diagram for TLS mode. In this diagram, an application program is running at . A TLS runtime system intervenes at . The runtime system requests the operating system to configure a domain in TLS mode at . The operating system returns control to the runtime system at . The runtime system then allocates at least one ID and starts using that ID at . The application program then runs at , with the runtime system tagging memory access requests with the ID. When the TLS section completes, the runtime system commits the ID at  and TLS mode ends.","FIG. --C shows a timing diagram for rollback mode. More about the implementation of rollback is to be found in the co-pending application Ser. No. 12\/696,780, which is incorporated herein by reference. In the case of rollback, an application program is running at  without knowing that any speculative execution is contemplated. The operating system requests an interrupt immediately after . At the time of this interrupt, it stores a snapshot at  of the core register state to memory; allocates an ID in rollback mode; and starts using that ID in accessing memory. In the case of a soft error, during the subsequent running of the application program , the operating system receives an interrupt indicating an invalid state of the processor, resets the affected core, invalidates the last speculation ID, restores core registers from memory, and jumps back to the point where the snapshot was taken. If no soft error occurs, the operating system at the end of  will receive another interrupt and take another snapshot at .","Once an ID is committed, the actions taken by the thread under that ID become irreversible.","In the current embodiment, a hardware thread can only use one speculation ID at a time and that ID can only be configured to one domain of IDs. This means that if TM or TLS is invoked, which will assign an ID to the thread, then rollback cannot be used. In this case, the only way of recovering from a soft error might be to go back to system states that are stored to disk on a more infrequent basis. It might be expected in a typical embodiment that a rollback snapshot might be taken on the order of once every millisecond, while system state might be stored to disk only once every hour or two. Therefore rollback allows for much less work to be lost as a result of a soft error. Soft errors increase in frequency as chip density increases. Executing in TLS or TM mode therefore entails a certain risk.","Generally, recovery from failure of any kind of speculative execution in the current embodiment relates to undoing changes made by a thread. If a soft error occurred that did not relate to a change that the thread made, then it may nevertheless be necessary to go back to the snapshot on the disk.","As shown in , a 32 MB shared L2 (see also FIG. --) is sliced into 16 units , each connecting to a slave port of the switch . The L2 slice macro area shown in FIG. -- is dominated by arrays. The 8 256 KB eDRAM macros  are stacked in two columns, each 4 macros tall. In the center , the directory Static Random Access Memories (\u201cSRAMs\u201d) and the control logic are placed.","FIG. -- shows more features of the L2. In FIG. --, reference numerals repeated from  refer to the same elements as in the earlier figure. Added to this diagram with respect to FIG. -- are L2 counters , Device Bus (\u201cDEV BUS\u201d) , and L2 CENTRAL. . Groups of 4 slices are connected via a ring, e.g. , to one of the two DDR3 SDRAM controllers .","FIG. -- shows various address versions across a memory pathway in the nodechip . One embodiment of the core , uses a 64 bit virtual address as part of instructions in accordance with the PowerPC architecture. In the TLB , that address is converted to a 42 bit \u201cphysical\u201d address that actually corresponds to 64 times the size of the main memory , so it includes extra bits for thread identification information. The term \u201cphysical\u201d is used loosely herein to contrast with the more elaborate addressing including memory mapped i\/o that is used in the PowerPC core . The address portion will have the canonical format of FIG. --B, prior to hashing, with a tag  that corresponds to a way, an index  that corresponds to a set, and an offset  that corresponds to a location within a line. The addressing varieties shown here, with respect to the commercial embodiment, are intended to be used for the data pathway of the cores. The instruction pathway is not shown here. After arriving at the L1P, the address is converted to 36 bits.","Address scrambling tries to distribute memory accesses across L2-cache slices and within L2-cache slices across sets (congruence classes). Assuming a 64 GB main memory address space, a physical address dispatched to the L2 has 36 bits, numbered from 0 (MSb) to 35 (LSb) (a(0 to 35)).","The L2 stores data in 128B wide lines, and each of these lines is located in a single L2-slice and is referenced there via a single directory entry. As a consequence, the address bits  to  only reference parts of an L2 line and do not participate in L2 or set selection.","To evenly distribute accesses across L2-slices for sequential lines as well as larger strides, the remaining address bits are hashed to determine the target slice. To allow flexible configurations, individual address bits can be selected to determine the slice as well as an XOR hash on an address can be used: The following hashing is used in the present embodiment:\n\n","For each of the slices, 25 address bits are a sufficient reference to distinguish L2 cache lines mapped to that slice.","Each L2 slice holds 2 MB of data or 16K cache lines. At 16-way associativity, the slice has to provide 1024 sets, addressed via 10 address bits. The different ways are used to store different addresses mapping to the same set as well as for speculative results associated with different threads or combinations of threads.","Again, even distribution across set indices for unit and non-unit strides is achieved via hashing, to wit:","Set index:=(\u201c00000\u201d & a(0 to 4)) xor a(5 to 14) xor a(15 to 24).","To uniquely identify a line within the set, using a(0 to 14) is sufficient as a tag.","Thereafter, the switch provides addressing to the L2 slice in accordance with an address that includes the set and way and offset within a line, as shown in FIG. --B. Each line has 16 ways.","L2 as Point of Coherence","In this embodiment, the L2 Cache provides the bulk of the memory system caching on the BQC chip. To reduce main memory accesses, the L2 caches serve as the point of coherence for all processors. This function includes generating L1 invalidations when necessary. Because the L2 caches are inclusive of the L1s, they can remember which processors could possibly have a valid copy of every line. Memory consistency is enforced by the L2 slices by means of multicasting selective L1 invalidations, made possible by the fact that the L1s operate in write-through mode and the L2s are inclusive of the L1s.","Per the article on \u201cCache Coherence\u201d in Wikipedia, there are several ways of monitoring speculative execution to see if some resource conflict is occurring, e.g.\n\n","The prior version of the IBM\u00ae BluGene\u00ae processor used snoop filtering to maintain cache coherence. In this regard, the following patent is incorporated by reference: U.S. Pat. No. 7,386,685, issued 10 Jun. 2008.","The embodiment discussed herein uses directory based coherence.","FIG. -- shows features of an embodiment of the control section  of a cache slice .","Coherence tracking unit  issues invalidations, when necessary.","The request queue  buffers incoming read and write requests. In this embodiment, it is 16 entries deep, though other request buffers might have more or less entries. The addresses of incoming requests are matched against all pending requests to determine ordering restrictions. The queue presents the requests to the directory pipeline  based on ordering requirements.","The write data buffer  stores data associated with write requests. This embodiment has a 16B wide data interface to the switch  and stores 16 16B wide entries. Other sizes might be devised by the skilled artisan as a matter of design choice. This buffer passes the data to the eDRAM pipeline  in case of a write hit or after a write miss resolution. The eDRAMs are shown at  in FIG. --.","The directory pipeline  accepts requests from the request queue , retrieves the corresponding directory set from the directory SRAM , matches and updates the tag information, writes the data back to the SRAM and signals the outcome of the request (hit, miss, conflict detected, etc.). Operations illustrated at FIGS. --F, --G, --H, --I-, and --I- are conducted within the directory pipeline .","In parallel,\n\n","The L2 implements two eDRAM pipelines  that operate independently. They may be referred to as eDRAM bank 0 and eDRAM bank 1. The eDRAM pipeline controls the eDRAM access and the dataflow from and to this macro. If writing only subcomponents of a doubleword or for load-and-increment or store-add operations, it is responsible to schedule the necessary Read Modify Write (\u201cRMW\u201d) cycles and provide the dataflow for insertion and increment.","The read return buffer  buffers read data from eDRAM or the memory controller  (FIG. --) and is responsible for scheduling the data return using the switch . In this embodiment it has a 32B wide data interface to the switch. It is used only as a staging buffer to compensate for backpressure from the switch. It is not serving as a cache.","The miss handler  takes over processing of misses determined by the directory. It provides the interface to the DRAM controller and implements a data buffer for write and read return data from the memory controller,","The reservation table  registers reservation requests, decides whether a STWCX can proceed to update L2 state and invalidates reservations based on incoming stores.","Also shown in FIG. -- are a pipeline control unit  and EDRAM queue decoupling buffer .","The L2 implements a multitude of decoupling buffers for different purposes.\n\n","FIG. --. The L2 slice  includes a request queue . At , a cascade of modules tests whether pending memory access requests will require data associated with the address of a previous request, the address being stored at . These tests might look for memory mapped flags from the L1 or for some other identification. A result of the cascade  is used to create a control input at  for selection of the next queue entry for lookup at , which becomes an input for the directory look up module .","FIG. -- shows more about the interaction between the directory pipe  and the directory SRAM . The vertical lines in the pipe represent time intervals during which data passes through a cascade of registers in the directory pipe. In a first time interval T1, a read is signaled to the directory SRAM. In a second time interval T2, data is read from the directory SRAM. In a third time interval, T3, a table lookup informs writes WR and WR DATA to the directory SRAM. In general, table lookup will govern the behavior of the directory SRAM to control cache accesses responsive to speculative execution. Only one table lookup is shown at T3, but more might be implemented. More about the contents of the directory SRAM is shown in FIG. --C and --D, discussed further below. More about the action of the table lookup will be disclosed with respect to aspects of conflict checking and version aggregation.","The L2 central unit  is illustrated in FIG. --A. It is accessed by the cores via its interface  to the device bus\u2014DEV BUS . The DEV Bus interface is a queue of requests presented for execution. The state table that keeps track of the state of thread ID's is shown at . More about the contents of this block will be discussed below, with respect to FIG. --B.","The L2 counter units  of FIG. -- track the number of ID references\u2014directory entries that store an ID\u2014in a group of four slices. These counters periodically\u2014in the current implementation every 4 cycles\u2014send a summary of the counters to the L2 central unit. The summaries indicate which ID has zero references and which have one or more references. The \u201creference tracking unit\u201d  in the L2 CENTRAL UNIT  aggregates the summaries of all four counter sets and determines which IDs have zero references in all counter sets. IDs that have been committed or invalidated and that have zero references in the directory can be reused for a new speculation task.","A command execution unit  coordinates operations with respect to speculation ID's. Operations associated with FIGS. --C, --, --, --, --, --, --, and --A are conducted in unit . It decodes requests received from the DEV BUS. If the command is an ID allocation, the command execution unit goes to the ID state table  and looks up an ID that is available, changes the state to speculative and returns the value back via the DEV BUS. It sends commands at  to the core , such as when threads need to be invalidated and switching between evict on write and address aliasing. The command execution unit also sends out responses to commands to the L2 via the dedicated interfaces. An example of such a command might be to update the state of a thread.","The L2 slices  communicate to the central unit at  typically in the form of replies to commands, though sometimes the communications are not replies, and receive commands from the central unit at . Other examples of what might be transmitted via the bus labeled \u201cL2 replies\u201d include signals from the slices indicating if a conflict has happened. In this case, a signal can go out via a dedicated broadcast bus to the cores indicating the conflict to other devices, that an ID has changed state and that an interrupt should be generated.","The L2 slices receive memory access requests at  from the L1D at a request interface . The request interface forwards the request to the directory pipe  as shown in more detail in FIG. --.","Support for such functionalities includes additional bookkeeping and storage functionality for multiple versions of the same physical memory line.","FIG. --B shows various registers of the ID STATE table . All of these registers can be read by the operating system.","These registers include 128 two bit registers , each for storing the state of a respective one of the 128 possible thread IDs. The possible states are:",{"@attributes":{"id":"p-0886","num":"0997"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"126pt","align":"center"}}],"thead":{"row":[{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]},{"entry":[{},"STATE","ENCODING"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"AVAILABLE","00"]},{"entry":[{},"SPECULATIVE","01"]},{"entry":[{},"COMMITTED","10"]},{"entry":[{},"INVALID","11"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}}},"By querying the table on every use of an ID, the effect of instantaneous ID commit or invalidation can be achieved by changing the state associated with the ID to committed or invalid. This makes it possible to change a thread's state without having to find and update all the thread's lines in the L2 directory; also it saves directory bits.","Another set of 128 registers  is for encoding conflicts associated with IDs. More detail of these registers is shown at FIG. --E. There is a register of this type for each speculation ID. This register contains the following fields:\n\n","Another register  has 5 bits and is for indicating how many domains have been created.","A set of 16 registers  indicates an allocation pointer for each domain. A second set of 16 registers  indicates a commit pointer for each domain. A third set of 16 registers  indicates a reclaim pointer for each domain. These three pointer registers are seven bits each.","FIG. --C shows a flowchart for an ID allocation routine. At  a request for allocating an ID is received. At , a determination is made whether the ID is available. If the ID is not available, the routine returns the previous ID at . If the ID is available, the routine returns the ID at  and increments the allocation pointer at , wrapping at domain boundaries.","FIG. --D shows a conceptual diagram of allocation of IDs within a domain. In this particular example, only one domain of 127 IDs is shown. An allocation pointer is shown at  pointing at speculation ID 3. Order of the IDs is of special relevance for TLS. Accordingly, the allocation pointer points at the oldest speculation ID , with the next oldest being at . The point where the allocation pointer is pointing is also the wrap point for ordering, so the youngest and second youngest are shown at  and .","ID Ordering for Speculative Execution","The numeric value of the speculation ID is used in Speculative Execution to establish a younger\/older relationship between speculative tasks. IDs are allocated in ascending order and a larger ID generally means that the ID designates accesses of a younger task.","To implement in-order allocation, the L2 CENTRAL at  maintains an allocation pointer . A function ptr_try_allocate tries to allocate the ID the pointer points to and, if successful, increments the pointer. More about this function can be found in a table of functions listed below.","As the set of IDs is limited, the allocation pointer  (FIG. --B) will wrap at some point from the largest ID to the smallest ID. Following this, the ID ordering is no longer dependent on the ID values alone. To handle this case, in addition to serving for ID allocation, the allocation pointer also serves as pointer to the wrap point of the currently active ID space. The ID the allocation pointer points to will be the youngest ID for the next allocation. Until then, if it is still active, it is the oldest ID of the ID space. The (allocation pointer\u22121) ID is the ID most recently allocated and thus the youngest. So the ID order is defined as:","Alloc_pointer+0: oldest ID","Alloc_pointer+1: second oldest ID",". . .","Alloc_pointer\u22122: second youngest ID","Alloc_pointer\u22121: youngest ID","The allocation pointer is a 7b wide register. It stores the value of the ID that is to be allocated next. If an allocation is requested and the ID it points to is available, the ID state is changed to speculative, the ID value is returned to the core and the pointer content is incremented.","The notation means: if the allocation pointer is, e.g., 10, then ID 0 is the oldest, 11 second oldest, . . . , 8 second youngest and 9 youngest ID.","Aside from allocating IDs in order for Speculative Execution, the IDs must also be committed in order. L2 CENTRAL provides a commit pointer  that provides an atomic increment function and can be used to track what ID to commit next, but the use of this pointer is not mandatory.","Per FIG. --, when an ID is ready to commit at , i.e., its predecessor has completed execution and did not get invalidated, a ptr_try_commit can be executed . In case of success, the ID the pointer points to gets committed and the pointer gets incremented at . At that point, the ID can be released by clearing the A-bit at .","If the commit fails or the ID was already invalid before the commit attempt at , the ID the commit pointer points to needs to be invalidated along with all younger IDs currently in use at . Then the commit pointer must be moved past all invalidated IDs by directly writing to the commit pointer register . Then, the A-bit for all invalidated IDs the commit pointer moved past can be cleared and thus released for reallocation at . The failed speculative task then needs to be restarted.","Speculation ID Reclaim","To support ID cleanup, the L2 cache maintains a Use Counter within units  for each thread ID. Every time a line is established in L2, the use counter corresponding to the ID of the thread establishing the line is incremented. The use counter also counts the occurrences of IDs in the speculative reader set. Therefore, each use counter indicates the number of occurrences of its associated ID in the L2.","At intervals programmable via DCR the L2 examines one directory set for lines whose thread IDs are invalid or committed. For each such line, the L2 removes the thread ID in the directory, marks the cache line invalid or merges it with the non-speculative state respectively, and decrements the use counter associated with that thread ID. Once the use counter reaches zero, the ID can be reclaimed, provided that its A bit has been cleared. The state of the ID will switch to available at that point. This is a type of lazy cleanup. More about lazy evaluation can be found the in Wikipedia article entitled \u201cLazy Evaluation.\u201d","Domains","Parallel programs are likely to have known independent sections that can run concurrently. Each of these parallel sections might, during the annotation run, be decomposed into speculative threads. It is convenient and efficient to organize these sections into independent families of threads, with one committed thread for each section. The L2 allows for this by using up to the four most significant bits of the thread ID to indicate a speculation domain. The user can partition the thread space into one, two, four, eight or sixteen domains. All domains operate independently with respect to allocating, checking, promoting, and killing threads. Threads in different domains can communicate if both are non-speculative; no speculative threads can communicate outside their domain, for reasons detailed below.","Per FIG. --B, each domain requires its own allocation  and commit pointers , which wrap within the subset of thread IDs allocated to that domain.","Transactional Memory","The L2's speculation mechanisms also support a transactional-memory (TM) programming model, per FIG. --. In a transactional model, the programmer replaces critical sections with transactional sections at , which can manipulate shared data without locking.","The implementation of TM uses the hardware resources for speculation. A difference between TLS and TM is that TM IDs are not ordered. As a consequence, IDs can be allocated at  and committed in any order . The L2 CENTRAL provides a function that allows allocation of any available ID from a pool (try_alloc_avail) and a function that allows an ID to be atomically committed regardless of any pointer state (try_commit) . More about these functions appears in a table presented below.","The lack of ordering means also that the mechanism to forward data from older threads to younger threads cannot be used and both RAW as well as WAR accesses must be flagged as conflicts at . Two IDs that have speculatively written to the same location cannot both commit, as the order of merging the IDs is not tracked. Consequently, overlapping speculative writes are flagged as WAW conflicts .","A transaction succeeds  if, while the section executes, no other thread accesses to any of the addresses it has accessed, except if both threads are only reading per . If the transaction does not succeed, hardware reverses its actions : its writes are invalidated without reaching external main memory. The program generally loops on a return code and reruns failing transactions.","Mode Switching","Each of the three uses of the speculation facilities","1. TLS","2. TM","3. Rollback Mode","require slightly different behavior from the underlying hardware. This is achieved by assigning to each domain of speculation IDs one of the three modes. The assignment of modes to domains can be changed at run time. For example, a program may choose TLS at some point of execution, while at a different point transactions supported by TM are executed. During the remaining execution, rollback mode should be used.","FIG. -- shows starting with one of the three modes at . Then a speculative task is executed at . If a different mode is needed at , it cannot be changed if any of the IDs of the domain is still in the speculative state per . If the current mode is TLS, the mode can in addition not be changed while any ID is still in the committed state, as lines may contain multiple committed versions that rely on the TLS mode to merge their versions in the correct order. Once the IDs are committed, the domain can be chosen at .","Memory Consistency","This section describes the basic mechanisms used to enforce memory consistency, both in terms of program order due to speculation and memory visibility due to shared memory multiprocessing, as it relates to speculation.","The L2 maintains the illusion that speculative threads run in sequential program order, even if they do not. Per FIG. --, to do this, the L2 may need to store unique copies of the same memory line with distinct thread IDs. This is necessary to prevent a speculative thread from writing memory out of program order.","At the L2 at , the directory is marked to reflect which threads have read and written a line when necessary. Not every thread ID needs to be recorded, as explained with respect to the reader set directory, see e.g. FIG. --D.","On a read at , the L2 returns the line that was previously written by the thread that issued the read or else by the nearest previous thread in program order ; if the address is not in L2 , the line is fetched  from external main memory.","On a write , the L2 directory is checked for illusion-breaking reads\u2014reads by threads later in program order. More about this type of conflict checking is explained with reference to FIGS. --C through --I-. That is, it checks all lines in the matching set that have a matching tag and an ID smaller or equal  to see if their read range contains IDs that are greater than the ID of the requesting thread . If any such line exists, then the oldest of those threads and all threads younger than it are killed , , , . If no such lines exist, the write is marked with the requesting thread's ID . The line cannot be written to external main memory if the thread ID is speculative .","To kill a thread (and all younger threads), the L2 sends an interrupt  to the corresponding core. The core receiving the interrupt has to notify the cores running its successor threads to terminate these threads, too per . It then has to mark the corresponding hread IDs invalid  and restart its current speculative thread .","Commit Race Window Handling","Per FIG. --, when a speculative TLS or TM ID's status is changed to committed state per , the system has to ensure that a condition that leads to an invalidation has not occurred before the change to committed state has reached every L2 slice. As there is a latency from the point of detection of a condition that warrants an invalidation until this information reaches the commit logic, as well as there is a latency from the point of initiating the commit until it takes effect in all L2 slices, it is possible to have a race condition between commit and invalidation.","To close this window, the commit process is managed in TLS, TM mode, and rollback mode , , . Rollback mode requires equivalent treatment to transition IDs to the invalid state.","Transition to Committed State","To avoid the race, the L2 gives special handling to the period between the end of a committed thread and the promotion of the next. Per  and FIG. --, for TLS, after a committed thread completes at , the L2 keeps it in committed state  and moves the oldest speculative thread to transitional state . L2_central has a register that points to the ID currently in transitional state (currently committing). The state register of the ID points during this time to the speculative state. Newly arriving writes  that can affect the fate of the transitional thread\u2014writes from outside the domain and writes by threads older than the transitional thread \u2014are blocked when detected  inside the L2. After all side effects, e.g. conflicts, from writes initiated before entering the transitional state have completed \u2014if none of them cause the transitional thread to be killed \u2014the transitional thread is promoted  and the blocked writes are allowed to resume . If side effects cause the transitional thread to fail, at , the thread is invalidated, a signal sent to the core, and the writes are also unblocked at .","In the case of TM, first the thread to be committed is set to a transitional state at . Then accesses from other speculative threads or non-speculative writes are blocked at . If any such speculative access or non-speculative write are active, then the system has to wait at . Otherwise conflicts must be checked for at . If none are present, then all side effects must be registered at , before the thread may be committed and writes resumed at .","Thread ID Counters","A direct implementation of the thread ID use counters would require each of the 16 L2's to maintain 128 counters (one per thread ID), each 16 bits (to handle the worst case where all 16 ways in all 1024 sets have a read and a write by that thread). These counters would then be ORd to detect when a count reached zero.","Instead, groups of L2's manipulate a common group-wide-shared set of counters . The architecture assigns one counter set to each set of 4 L2-slices. The counter size is increased by 2 bits to handle directories for 4 caches, but the number of counters is reduced 4-fold. The counters become more complex because they now need to simultaneously handle combinations of multiple decrements and increments.","As a second optimization, the number of counters is reduced a further 50% by sharing counters among two thread IDs. A nonzero count means that at least one of the two IDs is still in use. When the count is zero, both IDs can potentially be reclaimed; until then, none can be reclaimed. The counter size remains the same, since the 4 L2's still can have at most 4*16*1024*3 references total.","A drawback of sharing counters is that IDs take longer to be reused\u2014none of the two IDs can be reused until both have a zero count. To mitigate this, the number of available IDs is made large (128) so free IDs will be available even if several generations of threads have not yet fully cleared.","After a thread count has reached zero, the thread table is notified that those threads are now available for reuse.","Conflict Handling","Conflict Recording","To detect conflicts, the L2 must record all speculative reads and writes to any memory location.","Speculative writes are recorded by allocating in the directory a new way of the selected set and marking it with the writer ID. The set contains 16 dirty bits that distinguish which double word of the 128B line has been written by the speculation ID. If a sub-double word write is requested, the L2 treats this as a speculative read of a double word, insertion of the write data into that word followed by full a double word write.","FIG. --C shows the formats of 4 directory SRAMs included at , to wit:\n\n","In the base directory, , there are 15 bits that represent the upper 15b address bits of the line stored at . Then there is a seven bit speculative writer ID field  that indicates which speculation ID wrote to this line and a flag  that indicates whether the line was speculatively written. Then there is a two bit speculative read flag field  indicating whether to invoke the speculative reader directory , and a one bit \u201ccurrent\u201d flag . The current flag  indicates whether the current line is assembled from more than one way or not. The core  does not know about the fields -. These fields are set by the L2 directory pipeline.","If the speculative writer flag is checked, then the way has been written speculatively, not taken from main memory and the writer ID field will say what the writer ID was. If the flag is clear, the writer ID field is irrelevant.","The LRU directory indicates \u201cage\u201d, a relative ordering number with respect to last access. This directory is for allocating ways in accordance with the Least Recently Used algorithm.","The COH\/dirty directory has two uses, and accordingly two possible formats. In the first format, , known as \u201cCOH,\u201d there are 17 bits, one for each core of the system. This format indicates, when the writer flag is not set, whether the corresponding core has a copy of this line of the cache. In the second format, \u2032, there are 16 bits. These bits indicate, if the writer flag is set in the base directory, which part of the line has been modified speculatively. The line has 128 bytes, but they are recorded at \u2032 in groups of 8 bytes, so only 16 bits are used, one for each group of eight bytes.","Speculative reads are recorded for each way from which data is retrieved while processing a request. As multiple speculative reads from different IDs for different sections of the line need to be recorded, the L2 uses a dynamic encoding that provides a superset representation of the read accesses.","In FIG. --C, the speculative reader directory  has fields PF for parameters , left boundary , right boundary , a first speculative ID , and a second ID . The speculative reader directory is invoked in response to flags in field .","FIG. --D relates to an embodiment of use of the reader set directory. The left column of FIG. --D illustrates seven possible formats of the reader set directory, while the right column indicates what the result in the cache line would be for that format. Formats , , and  can be used for TLS, while formats - can be used for TM.","Format  indicates that no speculative reading has occurred.","If only a single TLS or TM ID has read the line, the L2 records the ID along with the left and right boundary of the line section so far accessed by the thread. Boundaries are always rounded to the next double word boundary. Format  uses two bit code \u201c01\u201d to indicate that a single seven bit ID, \u03b1, has read in a range delimited by four bit parameters denoted \u201cleft\u201d and \u201cright\u201d.","If two IDs in TM have accessed the line, the IDs along with the gap between the two accessed regions are recorded. Format  uses two bit code \u201c11\u201d to indicate that a first seven bit ID denoted \u201c\u03b1\u201d has read from a boundary denoted with four bits symbolized by the word \u201cleft\u201d to the end of the line; while a seven bit second ID, denoted \u201c\u03b2\u201d has read from the beginning of the line to a boundary denoted by four bits symbolized by the word \u201cright.\u201d","Format  uses three bit code \u201c001\u201d to indicate that three seven bit IDs, denoted \u201c\u03b1,\u201d \u201c\u03b2,\u201d and \u201c\u03b3,\u201d have read the entire line. In fact, when the entire line is indicated in this figure, it might be that less than the entire line has been read, but the encoding of this embodiment does not keep track at the sub-line granularity for more than two speculative IDs. One of ordinary skill in the art might devise other encodings as a matter of design choice.","Format  uses five bit code \u201c00001\u201d to indicate that several IDs have read the entire line. The range of IDs is indicated by the three bit field denoted \u201cID up\u201d. This range includes the sixteen IDs that share the same upper three bits. Which of the sixteen IDs have read the line is indicated by respective flags in the sixteen bit field denoted \u201cID set.\u201d","If two or more TLS IDs have accessed the line, the youngest and the oldest ID along with the left and right boundary of the aggregation of all accesses are recorded.","Format  uses the eight bit code \u201c00010000\u201d to indicate that a group of IDs has read the entire line. This group is defined by a 16 bit field denoted \u201cIDgroupset.\u201d","Format  uses the two bit code \u201c10\u201d to indicate that two seven bit IDs, denoted \u201c\u03b1\u201d and \u201c\u03b2\u201d have read a range delimited by boundaries indicated by the four bit fields denoted \u201cleft\u201d and \u201cright.\u201d","When doing WAR conflict checking, per FIG. --I- and FIG. --I- below, the formats of FIG. --D are used.","Rollback ID reads are not recorded.","If more than two TM IDs, a mix of TM and TLS IDs or TLS IDs from different domains have been recorded, only the 64 byte access resolution for the aggregate of all accesses is recorded.","FIG. --E shows assembly of a cache line, as called for in element  of FIG. --. In one way, there is unspecified data NSPEC at . In another way, ID1 has written version 1 of the data at , leaving undefined data at  and . In another way, ID2 has written version 2 of data  leaving undefined areas  and . Ultimately, these three ways can be combined into an assembled way, having some NSPEC fields , , and , version 1 at  and Version 2 at . This assembled way will be signaled in the directory, because it will have the current flag, , set. This is version aggregation is required whenever a data item needs to read from a speculative version, e.g., speculative loads or atomic RMW operations.","FIG. -- shows a flow of version aggregation, per . At , the procedure starts in the pipe control unit  with a directory lookup at . If there are multiple versions of the line, further explained with reference to FIG. --E and --G, this will be treated as a cache miss and referred to the miss handler . The miss handler will treat the multiple versions as a cache miss per  and block further accesses to the EDRAM pipe at . Insert copy operations will then be begun at  to aggregate the versions into the EDRAM queue. When aggregation is complete at , the final version is inserted into the EDRAM queue at , otherwise - repeat.","In summary, then, the current bit  of FIG. --C indicates whether data for this way contains only the speculatively written fields as written by the speculative writer indicated in the spec id writer field (current flag=0) or if the other parts of the line have been filled in with data from the non-speculative version or\u2014if applicable\u2014older TLS versions for the address (current flag=1). If the line is read using the ID that matches the spec writer ID field and the flag is set, no extra work is necessary and the data can be returned to the requestor (line has been made current recently). If the flag is clear in that case, the missing parts for the line need to be filled in from the other aforementioned versions. Once the line has been completed, the current flag is set and the line data is returned to the requestor.","Conflict Detection","For each request the L2 generates a read and write access memory footprint that describes what section of the 128B line is read and\/or written. The footprints are dependent on the type of request, the size info of the request as well as on the atomic operation code.","For example, an atomic load-increment-bounded from address A has a read footprint of the double word at A as well as the double word at A+8, and it has a write footprint of the double word at address A. The footprint is used matching the request against recorded speculative reads and writes of the line.","Conflict detection is handled differently for the three modes.","Per FIG. --F, due to the out-of-order commit and missing order of the IDs in TM, all RAW, WAR and WAW conflicts with other IDs are flagged as conflicts. With respect to FIG. --H, for WAW and RAW conflicts, the read and write footprints are matched against the 16b dirty set of all speculative versions and conflicts with the recorded writer IDs are signaled for each overlap.","With respect to FIG. --I-, for WAR conflicts, the left and the right boundary of the write footprint are matched against the recorded reader boundaries and a conflict is reported for each reader ID with an overlap.","Per FIG. --F, in TLS mode, the ordering of the ID and the forwarding of data from older to younger threads requires only WAR conflicts to be flagged. WAR conflicts are processed as outlined for TM.","In Rollback mode, any access to a line that has a rollback version signals a conflict and commits the rollback ID unless the access was executed with the ID of the existing rollback version.","With respect to FIG. --I-, if TLS accesses encounter recorded IDs outside their domain and if TM accesses encounter recorded IDs that are non-TM IDs, all RAW, WAR and WAW cases are checked and conflicts are reported.","FIG. --F shows an overview of conflict checking, which occurs  of FIG. --. At  of FIG. --F a memory access request is received that is either TLS or TM. At , it is determined whether the access is a read or a write or both. It should be noted that both types can exist in the same instruction. In the case of a read, it is then tested whether the access is TM at . If it is TLS, no further checks are required before recording the read at . If it is TM, a Read After Write (\u201cRAW\u201d) check must be performed at  before recording the read at . In the case of a write, it is also tested whether the access is TLS or TM at . If it is a TLS access, then control passes to the Write After Read (\u201cWAR\u201d) check . WAW is not necessarily a conflict for TLS, because the ID ordering can resolve conflicting writes. If it is a TM access then control passes to the Write After Write (\u201cWAW\u201d) check  before passing to the WAR check . Thereafter the write can be recorded at .","FIG. --G shows an aspect of conflict checking. First, a write request comes in at . This is a request from the thread with ID 6 for a double word write across the 8 byte groups 6, 7, and 8 of address A. In the base directory , three ways are found that have speculative data written in them for address A. These ways are shown at , , . Way  was written for address A, by the thread with speculative ID number 5. The corresponding portion of the \u201cdirty directory\u201d  is shown at  indicates that this ID wrote at double words 6, 7 and 8. This means there is a potential conflict between ID's 5 and 6. Way  was written for address A by the thread with speculative ID number 6. This is not a conflict, because the speculative ID number matches that of the current write request. As a result the corresponding bits from the \u201cdirty directory\u201d at  are irrelevant. Way  was written for address A by the thread with speculative ID number 7; however the corresponding bits from the \u201cdirty directory\u201d at  indicate that only double word 0 was written. As a result, there is no conflict between speculative IDs numbered 6 and 7 for this write.","FIG. --H shows the flow of WAW and RAW conflict checking. At , ways with matching address tags are searched to retrieve at  a set that has been written, along with the ID's that have written them. Then two checks are performed. The first at  is whether the writer ID is not equal to the access ID. The second at  is whether the access footprint overlaps the dirty bits of the retrieved version. In order for a conflict to be found at , both tests must come up in the affirmative per .","FIG. --I- shows a first aspect of WAR conflict checking. There is a difference between the way this checking is done for TM and TLS, so the routine checks which are present at . For TM, WAR is only done on non-speculative versions at . For TLS, WAR is done both on non-speculative versions at  and also on speculative versions with younger, i.e. larger IDs at . More about ID order is described with respect to FIG. --E.","FIG. --I- shows a second aspect of WAR conflict checking. This aspect is done for the situations found in both  and . First the reader representation is read at . More about the reader representation is described with respect to FIG. --D. The remaining parts of the procedure are performed with respect to all IDs represented in the reader representation per . At , it is checked whether the footprints overlap. If they do not, then there is no conflict . If they do, then there is also additional checking, which may be performed simultaneously. At , accesses are split into TM or TLS. For TM, there is a conflict if the reading ID is not the ID currently requesting access at . For TLS, there is a conflict if the reading ID was from a different domain or younger than the ID requesting access. If both relevant conditions for the type of speculative execution are met, then a conflict is signaled at .","TLS\/TM\/Rollback Management","The TLS\/TM\/Rollback capabilities of the memory subsystem are controlled via a memory-mapped I\/O interface.","Global Speculation ID Management:","The management of the ID state is done at the L2 CENTRAL unit. L2 CENTRAL also controls how the ID state is split into domains and what attributes apply to each domain. The L2 CENTRAL is accessed via MMIO by the cores. All accesses to the L2 CENTRAL are performed with cache inhibited 8B wide, aligned accesses.","The following functions are defined in the preferred embodiment:",{"@attributes":{"id":"p-0975","num":"1096"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"84pt","align":"left"}}],"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}},{"entry":[{},"number of ",{},{}]},{"entry":["Name","instances","Access","Function"]},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"84pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["NumDomains","1","RD","Returns current number "]},{"entry":[{},{},{},"of domains"]},{"entry":[{},{},"WR","Set number of domains. "]},{"entry":[{},{},{},"Only values 1, 2, 4, 8, 16 "]},{"entry":[{},{},{},"are valid. Clears all domain "]},{"entry":[{},{},{},"pointers. Not permitted to "]},{"entry":[{},{},{},"be changed if not all IDs "]},{"entry":[{},{},{},"are in available state"]},{"entry":["IdState","1","RD only","Returns vector of 128 bit "]},{"entry":[{},{},{},"pairs indicating the state "]},{"entry":[{},{},{},"of all 128 IDs"]},{"entry":[{},{},{},"00b: Available"]},{"entry":[{},{},{},"01b: Speculative"]},{"entry":[{},{},{},"10b: Committed"]},{"entry":[{},{},{},"11b: Invalid"]},{"entry":["TryAllocAvail","1","RD only","Allocates an available ID "]},{"entry":[{},{},{},"from the set of IDs"]},{"entry":[{},{},{},"specified by groupmask. "]},{"entry":[{},{},{},"Returns ID on success, \u22121 "]},{"entry":[{},{},{},"otherwise. On success, "]},{"entry":[{},{},{},"changes state of ID to"]},{"entry":[{},{},{},"speculative, clears conflict "]},{"entry":[{},{},{},"register and sets A bit in"]},{"entry":[{},{},{},"conflict register."]},{"entry":[{},{},{},"Groupmask is a 16b bit set, "]},{"entry":[{},{},{},"bit i = 1 indicating to"]},{"entry":[{},{},{},"include IDs 8*I to 8*i + 7 "]},{"entry":[{},{},{},"into the set of selectable IDs"]},{"entry":["Per domain:",{},{},{}]},{"entry":["DomainMode","16","RD\/WR","Bit 61:63: mode"]},{"entry":[{},{},{},"000b: long running TLS"]},{"entry":[{},{},{},"001b: short running TLS"]},{"entry":[{},{},{},"011b: short running TM"]},{"entry":[{},{},{},"100b: rollback mode"]},{"entry":[{},{},{},"Bit 60: invalidate on conflict,"]},{"entry":[{},{},{},"Bit 59: interrupt on conflict,"]},{"entry":[{},{},{},"Bit 58: interrupt on commit,"]},{"entry":[{},{},{},"Bit 57: interrupt on invalidate"]},{"entry":[{},{},{},"Bit 56: 0: commit to id 00; "]},{"entry":[{},{},{},"1: commit to id 01"]},{"entry":["AllocPtr","16","RD\/WR","Read and write allocation "]},{"entry":[{},{},{},"pointer. Allocation pointer"]},{"entry":[{},{},{},"is used to define ID wrap "]},{"entry":[{},{},{},"point for TLS and next ID"]},{"entry":[{},{},{},"to allocate using "]},{"entry":[{},{},{},"TryPtrAlloc. Should never "]},{"entry":[{},{},{},"be changed if domain is "]},{"entry":[{},{},{},"TLS and any ID in domain"]},{"entry":[{},{},{},"is not available"]},{"entry":["CommitPtr","16","RD\/WR","Read and write commit "]},{"entry":[{},{},{},"pointer. The commit pointer"]},{"entry":[{},{},{},"is used in PtrTryCommit"]},{"entry":[{},{},{},"and has no function"]},{"entry":[{},{},{},"otherwise. When using "]},{"entry":[{},{},{},"PtrTryCommit in TLS, use"]},{"entry":[{},{},{},"this function to step "]},{"entry":[{},{},{},"over invalidated IDs."]},{"entry":["ReclaimPtr","16","RD\/WR","Read and write reclaim "]},{"entry":[{},{},{},"pointer. The reclaim pointer"]},{"entry":[{},{},{},"is an approximation on "]},{"entry":[{},{},{},"which IDs could be reclaimed"]},{"entry":[{},{},{},"assuming their A bits were"]},{"entry":[{},{},{},"clear. The reclaim pointer"]},{"entry":[{},{},{},"value has no effect on any "]},{"entry":[{},{},{},"function of the L2"]},{"entry":[{},{},{},"CENTRAL."]},{"entry":["PtrTryAlloc","0x104+ ","RD only","Same function as "]},{"entry":[{},"domain*0x10",{},"TryAllocAvail, but set of"]},{"entry":[{},{},{},"selectable IDs limited to "]},{"entry":[{},{},{},"ID pointed to by allocation"]},{"entry":[{},{},{},"pointer. On success, "]},{"entry":[{},{},{},"increments additionally the"]},{"entry":[{},{},{},"allocation pointer."]},{"entry":["PtrForceCommit","16","N\/A","Reserved, not implemented"]},{"entry":["PtrTryCommit","16","RD only","Same function as "]},{"entry":[{},{},{},"TryCommit, but targets ID "]},{"entry":[{},{},{},"pointed to by commit "]},{"entry":[{},{},{},"pointer. Additionally, "]},{"entry":[{},{},{},"increments commit "]},{"entry":[{},{},{},"pointer on success."]},{"entry":["Per ID:",{},{},{}]},{"entry":["IdState","128","RD\/WR","Read or set state of ID:"]},{"entry":[{},{},{},"00b: Available"]},{"entry":[{},{},{},"01b: Speculative"]},{"entry":[{},{},{},"10b: Committed"]},{"entry":[{},{},{},"11b: Invalid"]},{"entry":[{},{},{},"This function should be "]},{"entry":[{},{},{},"used to invalidate IDs for"]},{"entry":[{},{},{},"TLS\/TM and to commit "]},{"entry":[{},{},{},"IDs for Rollback. These"]},{"entry":[{},{},{},"changes are not allowed "]},{"entry":[{},{},{},"while a TryCommit is in"]},{"entry":[{},{},{},"flight that may change "]},{"entry":[{},{},{},"this ID."]},{"entry":["Conflict","128","RD\/WR","Read or write "]},{"entry":[{},{},{},"conflict register:"]},{"entry":[{},{},{},"bit 57:63 conflicting ID, "]},{"entry":[{},{},{},"qualified by 1C bit"]},{"entry":[{},{},{},"bit 56: 1C bit, at least one "]},{"entry":[{},{},{},"ID is in conflict with this"]},{"entry":[{},{},{},"ID. Qualifies bits 57:63. "]},{"entry":[{},{},{},"Cleared if ID in 57:63 is"]},{"entry":[{},{},{},"invalidated"]},{"entry":[{},{},{},"bit 55: A bit, if set, ID "]},{"entry":[{},{},{},"can not be reclaimed"]},{"entry":[{},{},{},"bit 56: M bit, more than "]},{"entry":[{},{},{},"one ID with this ID in"]},{"entry":[{},{},{},"conflict"]},{"entry":[{},{},{},"bit 53: N bit, conflict with "]},{"entry":[{},{},{},"non-speculative access"]},{"entry":[{},{},{},"bit 52: R bit, invalidate due "]},{"entry":[{},{},{},"to resource conflict"]},{"entry":[{},{},{},"The conflict register is "]},{"entry":[{},{},{},"cleared on allocation of ID,"]},{"entry":[{},{},{},"except for the A bit. The A "]},{"entry":[{},{},{},"bit is set on allocation."]},{"entry":[{},{},{},"The A bit must be cleared "]},{"entry":[{},{},{},"explicitly by software to"]},{"entry":[{},{},{},"enable reclaim of this ID."]},{"entry":[{},{},{},"An ID can only be "]},{"entry":[{},{},{},"committed if the 1C, M, "]},{"entry":[{},{},{},"N and R bits are clear."]},{"entry":["ConflictSC","128","WR only","Write data is interpreted as "]},{"entry":[{},{},{},"mask, each bit set in the"]},{"entry":[{},{},{},"mask clears the "]},{"entry":[{},{},{},"corresponding bit in the "]},{"entry":[{},{},{},"conflict register, all other "]},{"entry":[{},{},{},"bits are left unchanged."]},{"entry":["TryCommit ","128","RD only","Tries to commit an ID for "]},{"entry":[{},{},{},"TLS\/TM and to invalidate"]},{"entry":[{},{},{},"an ID for Rollback."]},{"entry":[{},{},{},"Guarantees atomicity using "]},{"entry":[{},{},{},"a two-phase transaction. "]},{"entry":[{},{},{},"Succeeds if ID is speculative"]},{"entry":[{},{},{},"and 1C, M, N and R bit of "]},{"entry":[{},{},{},"conflict registers are clear"]},{"entry":[{},{},{},"at the end of the first phase."]},{"entry":[{},{},{},"Returns ID on success, "]},{"entry":[{},{},{},"\u22121 on fail."]},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}}]}}]}},"br":{}},"For each thread, a speculation ID register  in FIG. -- implemented next to the core provides a speculation ID to be attached to memory accesses of this thread.","When starting a transaction or speculative thread, the thread ID provided by the ID allocate function of the Global speculation ID management has to be written into the thread ID register of FIG. --. this register. All subsequent memory accesses for which the TLB attribute U is set are tagged with this ID. Accesses for which U is not set are tagged as non-speculative accesses. The PowerPC architecture specifies 4 TLB attributes bits U to U that can be used for implementation specific purposes by a system architect. See PPC spec 2.06 on http\/\/www.power.org\/resources\/downloads\/PowerISA_V2.0613 V2_PUBLIC.pdf, page 947.","In the latest IBM\u00ae Blue Gene\u00ae architecture, the point of coherence is a directory lookup mechanism in a cache memory. It would be desirable to guarantee a hierarchy of atomicity options within that architecture.","In one embodiment, a multiprocessor system includes a plurality of processors, a conflict checking mechanism, and an instruction implementation mechanism. The processors are adapted to carry out speculative execution in parallel. The conflict checking mechanism is adapted to detect and protect results of speculative execution responsive to memory access requests from the processors. The instruction implementation mechanism cooperates with the processors and conflict checking mechanism adapted to implement an atomic operation that includes load, modify, and store with respect to a single memory location in an uninterruptible fashion.","In another embodiment, a system includes a plurality of processors and at least one cache memory. The processors are adapted to issue atomicity related operations. The operations include at least one atomic operation and at least one other type of operation. The atomic operation includes sub-operations including a read, a modify, and a write. The other type of operation includes at least one atomicity related operation. The cache memory includes an cache data array access pipeline and a controller. The controller is adapted to prevent the other types operations from entering the cache data array access pipeline, responsive to an atomic operation in the pipeline, when those other types of operation compete with the atomic operation in the pipeline for a memory resource.","In yet another embodiment, a multiprocessor system includes a plurality of processors, a central conflict checking mechanism, and a prioritizer. The processors are adapted to implement parallel speculative execution of program threads and to implement a plurality of atomicity related techniques. The central conflict checking mechanism resolves conflicts between the threads. The prioritizer prioritizes at least one atomicity related technique over at least one other atomicity related technique.","In a further embodiment, a computer method includes issuing an atomic operation, recognizing the atomic operation, and blocking other operations. The atomic operation is issued from one of the processors in a multi-processor system and defines sub-operations that include reading, modifying, and storing with respect to a memory resource. A directory based conflict checking mechanism recognizes the atomic operation. Other operations seeking to access the memory resource are blocked until the atomic operation has completed.","Three modes of speculative execution are supported in the current embodiment: Thread Level Speculation (\u201cTLS\u201d), Transactional Memory (\u201cTM\u201d), and Rollback.","TM occurs in response to a specific programmer request. Generally the programmer will put instructions in a program delimiting sections in which TM is desired. This may be done by marking the sections as requiring atomic execution. \u201cAn access is single-copy atomic, or simply \u201catomic\u201d, if it is always performed in its entirety with no visible fragmentation.\u201d IBM\u00ae Power ISA\u2122 Version 2.06, Jan. 30, 2009. In a transactional model, the programmer replaces critical sections with transactional sections at  (FIG. --), which can manipulate shared data without locking. When the section ends, the program will make another call that ultimately signals the hardware to do conflict checking and reporting.","Normally TLS occurs when a programmer has not specifically requested parallel operation. Sometimes a compiler will ask for TLS execution in response to a sequential program. When the programmer writes this sequential program, she may insert commands delimiting sections. The compiler can recognize these sections and attempt to run them in parallel.","Rollback occurs in response to \u201csoft errors,\u201d normally these errors occur in response to cosmic rays or alpha particles from solder balls. Rollback is discussed in more detail in co-pending application Ser. No. 12\/696,780, which is incorporated herein by reference.","The present invention arose in the context of the IBM\u00ae Blue Gene\u00ae project, which is further described in the applications incorporated by reference above. FIG. -- is a schematic diagram of an overall architecture of a multiprocessor system in accordance with this project, and in which the invention may be implemented. At , there are a plurality of processors operating in parallel along with associated prefetch units and L1 caches. At , there is a switch. At , there are a plurality of L2 slices. At , there is a main memory unit. It is envisioned, for the present embodiment, that the L2 cache should be the point of coherence.","FIG. --A shows some software running in a distributed fashion, distributed over the cores of node .","The application program  can also request various operation types, for instance as specified in a standard such as the PowerPC architecture. These operation types might include larx\/stcx pairs or atomic operations, to be discussed further below.",{"@attributes":{"id":"p-0990","num":"1111"},"figref":"FIG. 1B"},"FIG. -- shows a cache slice.","FIG. -- shows features of an embodiment of the control section  of a cache slice .","As described above, FIG. -- shows an example of retaining lookup information.","As described above, FIG. -- shows more about the interaction between the directory pipe  and the directory SRAM .","As described above, FIG. --C shows the formats of 4 directory SRAMs included at .","In FIG. -- the operation of the pipe control unit  and the EDRAM queue decoupling buffer  will be described more below with reference to FIG. --.","The L2 implements a multitude of decoupling buffers for different purposes.\n\n","The L2 caches may operate as set-associative caches while also supporting additional functions, such as memory speculation for Speculative Execution (SE), Transactional Memory (TM) and local memory rollback, as well as atomic memory transactions. Support for such functionalities includes additional bookkeeping and storage functionality for multiple versions of the same physical memory line.","To reduce main memory accesses, the L2 cache may serve as the point of coherence for all processors. In performing this function, an L2 central unit will have responsibilities such as defining domains of speculation IDs, assigning modes of speculation execution to domains, allocating speculative IDS to threads, trying to commit the IDs, sending interrupts to the cores in case of conflicts, and retrieving conflict information. This function includes generating L1 invalidations when necessary. Because the L2 caches are inclusive of the L1s, they can remember which processors could possibly have a valid copy of every line, and they can multicast selective invalidations to such processors. The L2 caches are advantageously a synchronization point, so they coordinate synchronization instructions from the PowerPC architecture, such as larx\/stcx.","Larx\/Stcx","The larx and stcx. instructions used to perform a read-modify-write operation to storage. If the store is performed, the use of the larx and stcx instruction pair ensures that no other processor or mechanism has modified the target memory location between the time the larx instruction is executed and the time the stcx. instruction completes.","The lwarx (Load Word and Reserve Indexed) instruction loads the word from the location in storage specified by the effective address into a target register. In addition, a reservation on the memory location is created for use by a subsequent stwcx. instruction.","The stwcx (Store Word Conditional Indexed) instruction is used in conjunction with a preceding lwarx instruction to emulate a read-modify-write operation on a specified memory location.","The L2 caches will handle lwarx\/stwcx reservations and ensure their consistency. They are a natural location for this responsibility because software locking is dependent on consistency, which is managed by the L2 caches.","The A2 core basically hands responsibility for lwarx\/stwcx consistency and completion off to the external memory system. Unlike the 450 core, it does not maintain an internal reservation and it avoids complex cache management through simple invalidation. Lwarx is treated like a cache-inhibited load, but invalidates the target line if it hits in the L1 cache. Similarly, stwcx is treated as a cache-inhibited store and also invalidates the target line in L1 if it exists.","The L2 cache is expected to maintain reservations for each thread, and no special internal consistency action is taken by the core when multiple threads attempt to use the same lock. To support this, a thread is blocked from issuing any L2 accesses while a lwarx from that thread is outstanding, and it is blocked completely while a stwcx is outstanding. The L2 cache will support lwarx\/stwcx as described in the next several paragraphs.","Each L2 slice has 17 reservation registers. Each reservation register consists of a 25-bit address register and an 9-bit thread ID register that identifies which thread has reserved the stored address and indicates whether the register is valid (i.e. in use).","When a lwarx occurs, the valid reservation thread ID registers are searched to determine if the thread has already made a reservation. If so, the existing reservation is cleared. In parallel, the registers are searched for matching addresses. If found, the thread ID is tried to be added to the thread identifier. If either no address is found or the thread ID could not be added to reservation registers with matching addresses, a new reservation is established. If a register is available, it is used, otherwise a random existing reservation is evict and a new reservation is established in its place. The larx continues as an ordinary load and returns data.","Every store searches the valid reservation address registers. All matching registers are simply invalidated. The necessary back-invalidations to cores will be generated by the normal coherence mechanism.","When a stcx occurs, the valid reservation registers  are searched for entries with both a matching address and a matching thread ID. If both of these conditions are met, then the stcx is considered a success. Stcx success is returned to the requesting core and the stcx is converted to an ordinary store (causing the necessary invalidations to other cores by the normal coherence mechanism). If either condition is not met, then the stcx is considered a failure. Stcx fail is returned to the requesting core and the stcx is dropped. In addition, for every stcx any pending reservation for the requesting thread is invalidated.","To allow more than 17 reservations per slice, the actual thread ID field is encoded by the core ID and a vector of 4 bits, each representing a thread of the indicated core. If a reservation is established, first a check for matching address and core number n any register is made. If a register has both matching address and matching core, the corresponding thread bit is activated. Only if all bits are clear, the entire register is assumed invalidated and available for reallocation without eviction.","Atomic Operations","The L2 supports multiple atomic operations on 8B entities. These operations are sometimes of the type that perform read, modify, and write back atomically\u2014in other words that combine several frequently used instructions and guarantee that they can perform successfully. The operation is selected based on address bits as defined in the memory map and the type of access. These operations will typically require RAW, WAW, and WAR checking. The directory lookup phase will be somewhat different from other instructions, because both read and write are contemplated.","FIG. -- shows aspects of the L2 cache data array access pipeline, implemented as EDRAM pipeline  in the preferred embodiment, pertinent to atomic operations. In this pipeline, data is typically ready after five cycles. At , some read data is ready. Error correcting codes (ECC) are used to make sure that the read data is error free. Then read data can be sent to the core at . If it is one of these read\/modify\/write atomic operations, the data modification is performed at , followed by a write back to eDRAM at , which feeds back to the beginning of the pipeline per , while other matching requests are blocked from the pipeline, guaranteeing atomicity. Sometimes, two such compound instructions will be carried out sequentially. In such a case, any number of them can be linked using a feedback at . To assemble a line, several iterations of this pipeline structure may be undertaken. More about assembling lines can be found in the provisional applications incorporated by reference above. Thus atomic operations, which reserve the EDRAM pipeline, can achieve performance results that a sequence of operations cannot while guaranteeing atomicity.","It is possible to feed two atomic operations to two different addresses together through the EDRAM pipe: read a, read b, then write a and b.","FIG. -- shows a comparison between approaches to atomicity. At  a thread executing pursuant to a TM model is shown. At  a block of code protected by a larx\/stcx pair is shown. At  an atomic operation is shown.","Thread  includes three parts,","a first part  that involves at least one load instruction;","a second part  that involves at least one store instruction; and","a third part  where the system tries to commit the thread.","Arrow  indicates that the reader set directory is active for that part. Arrow  indicates that the writer set directory is active for that part.","Code block  is delimited by a larx instruction  and a stcx instruction . Arrow  indicates that the reservation table  is active. When the stcx instruction executes, if there has been any read or write conflict, the whole block  fails.","Atomic operation  is one of the types indicated in table below, for instance \u201cload increment.\u201d The arrows at  show the arrival of the atomic operation during the periods of time delimited by double arrows at  and . The atomic operation is guaranteed to complete due to the block on the EDRAM pipe for the relevant memory accesses. Accordingly, if there is a concurrent use by a TM thread  and\/or by a block of code protected by LARX\/STCX , and if those uses access the same memory location as the atomic operation , a conflict will be signaled and results of the code blocks  and  will be invalidated. A uninterruptible, persistent atomic operation will be given priority over a reversible operation, e.g. TM transaction, or an interruptible operation, e.g., a LARX\/STCX pair.","As between blocks  and , which is successful and which invalidates will depend on the order of operations, if they compete for the same memory resource. For instance, in the absence of , if the stcx instruction  completes before the commit attempt , the larx\/stcx box will succeed while the TM thread will fail. Alternatively, also in the absence of , if the commit attempt  completes before the stcx instruction , then the larx\/stcx block will fail. The TM thread can actually function a bit like multiple larx\/stcx pairs together.","FIG. -- shows some issues relating to queuing operations. At , an atomic operation issues from a processor. It takes the form of a memory access with the lower bits indicating an address of a memory location and the upper bits indicating which operation is desired. At , the L1D and L1P treat this operation as an ordinary memory access to an address that is not cached. At , in the pipe control unit of the L2 cache slice, the operation is recognized as an atomic operation responsive to a directory lookup. The directory lookup also determines whether there are multiple versions of the data accessed by the atomic operation. At , if there are multiple versions, control is transferred to the miss handler.","At , the miss handler treats the existence of multiple versions as a cache miss. It blocks further accesses to that set and prevents them from entering the queue, by directing them to the EDRAM decoupling buffer. With respect to the set, the EDRAM pipe is then made to carry out copy\/insert operations at  until the aggregation is complete at . This version aggregation loop is used for ordinary memory accesses to cache lines that have multiple versions.","Once the aggregation is complete, or if there are not multiple versions, control passes to  where the current access is inserted into the EDRAM queue. If there is already an atomic operation relating to this line of the cache at , then, at , the current operation must wait in the EDRAM decoupling buffer. Non atomic operations will similarly have to be decoupled if they seek to access a cache line that is currently being accessed by an atomic operation in the EDRAM queue. If there are no atomic operations relating to this line in the queue, then control passes to  where the current operation is transferred to the EDRAM queue. Then, at , the atomic operation traverses the EDRAM queue twice, once for the read and modify and once for the write. During this traversal, other operations seeking to access the same line may not enter the EDRAM pipe, and will be decoupled into the decoupling buffer.","The following atomic operations are examples that are supported in the preferred embodiment, though others might be implemented. These operations are implemented in addition to the memory mapped i\/o operations in the PowerPC architecture.",{"@attributes":{"id":"p-1027","num":"1153"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"4","colwidth":"98pt","align":"left"}},{"@attributes":{"colname":"5","colwidth":"77pt","align":"left"}}],"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}},{"entry":[{},"Load\/",{},{},{}]},{"entry":["Opcode","Store","Operation","Function","Comment"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["000","Load","Load","Load the current value",{}]},{"entry":["001","Load","Load Clear","Fetch current value and store zero",{}]},{"entry":["010","Load","Load","Fetch current value and increment ","0xFFFF FFFF FFFF"]},{"entry":[{},{},"Increment","storage","FFFF rolls over to 0."]},{"entry":[{},{},{},{},"So when sw uses the"]},{"entry":[{},{},{},{},"counter as unsigned,"]},{"entry":[{},{},{},{},"+2{circumflex over (\u2009)}64 \u2212 1 rolls over to 0."]},{"entry":[{},{},{},{},"Thanks to two's"]},{"entry":[{},{},{},{},"complement, sw can"]},{"entry":[{},{},{},{},"use the counter as"]},{"entry":[{},{},{},{},"signed or unsigned."]},{"entry":[{},{},{},{},"When using as signed,"]},{"entry":[{},{},{},{},"+2{circumflex over (\u2009)}63 \u2212 1 rolls over to "]},{"entry":[{},{},{},{},"\u22122{circumflex over (\u2009)}63."]},{"entry":["011","Load","Load","Fetch current value and","0 rolls over to to"]},{"entry":[{},{},"Decrement","decrement storage","0xFFFF FFFF FFFF"]},{"entry":[{},{},{},{},"FFFF. So when sw uses"]},{"entry":[{},{},{},{},"the counter as"]},{"entry":[{},{},{},{},"unsigned, 0 rolls over"]},{"entry":[{},{},{},{},"to +2{circumflex over (\u2009)}64 \u2212 1."]},{"entry":[{},{},{},{},"Thanks to two's"]},{"entry":[{},{},{},{},"complement, sw can"]},{"entry":[{},{},{},{},"use the counter as"]},{"entry":[{},{},{},{},"signed or unsigned."]},{"entry":[{},{},{},{},"When using as signed,"]},{"entry":[{},{},{},{},"\u22122{circumflex over (\u2009)}63 rolls over to"]},{"entry":[{},{},{},{},"2{circumflex over (\u2009)}63 \u2212 1."]},{"entry":["100","Load","Load","The counter is the address given","The 8B counter and its"]},{"entry":[{},{},"Increment","and the boundary is the","8B boundary efficiently"]},{"entry":[{},{},"Bounded","SUBSEQUENT address.","support"]},{"entry":[{},{},{},"If counter and boundary values","producer\/consumer"]},{"entry":[{},{},{},"differ, increment counter and","queue\/stack\/deque with"]},{"entry":[{},{},{},"return old value, else return","multiple producers and"]},{"entry":[{},{},{},"0x8000 0000 0000 0000.","multiple consumers."]},{"entry":[{},{},{},"if","The counter and"]},{"entry":[{},{},{},"(*ptrCounter==*(ptrCounter+1)){","boundary pair must be"]},{"entry":[{},{},{},"\u2002return 0x8000 0000 0000 0000;","within a 32 Byte line."]},{"entry":[{},{},{},"\u2002\/\/ +2{circumflex over (\u2009)}63 unsigned","Rollover and"]},{"entry":[{},{},{},"\u2002\/\/ \u22122{circumflex over (\u2009)}63 signed","signed\/nusigned"]},{"entry":[{},{},{},"} else {","software use are as for"]},{"entry":[{},{},{},"\u2002oldValue = *ptrCounter;","\u2018load increment\u2019"]},{"entry":[{},{},{},"\u2002++*ptrCounter;","instruction."]},{"entry":[{},{},{},"\u2002return oldValue;","On boundary, 0x8000"]},{"entry":[{},{},{},"}","0000 0000 0000 is"]},{"entry":[{},{},{},{},"returned."]},{"entry":[{},{},{},{},"So unsigned use is also"]},{"entry":[{},{},{},{},"restricted to the upper"]},{"entry":[{},{},{},{},"value 2{circumflex over (\u2009)}63 \u2212 1, instead"]},{"entry":[{},{},{},{},"of the optimal 2{circumflex over (\u2009)}64 \u2212 1."]},{"entry":[{},{},{},{},"This factor 2 loss is not"]},{"entry":[{},{},{},{},"expected to be a"]},{"entry":[{},{},{},{},"problem in practice."]},{"entry":["101","Load","Load","The counter is the address given","Comments as for \u2018Load"]},{"entry":[{},{},"Decrement","and the boundary is the","Increment Bounded\u2019"]},{"entry":[{},{},"Bounded","PREVIOUS address.",{}]},{"entry":[{},{},{},"If counter and boundary values",{}]},{"entry":[{},{},{},"differ, decrement counter and",{}]},{"entry":[{},{},{},"return old value, else return",{}]},{"entry":[{},{},{},"0x8000 0000 0000 0000.",{}]},{"entry":[{},{},{},"if (*ptrCounter==*(ptrCounter-",{}]},{"entry":[{},{},{},"1)){",{}]},{"entry":[{},{},{},"\u2002return 0x8000 0000 0000 0000;",{}]},{"entry":[{},{},{},"\u2002\/\/ +2{circumflex over (\u2009)}63 unsigned",{}]},{"entry":[{},{},{},"\u2002\/\/ \u22122{circumflex over (\u2009)}63 signed",{}]},{"entry":[{},{},{},"} else {",{}]},{"entry":[{},{},{},"\u2002oldValue = *ptrCounter;",{}]},{"entry":[{},{},{},"\u2002--*ptrCounter;",{}]},{"entry":[{},{},{},"\u2002return oldValue;",{}]},{"entry":[{},{},{},"}",{}]},{"entry":["110","Load","Load","The counter is the address given","The 8B counter and its"]},{"entry":[{},{},"Increment if","and the compare value is the","8B compare value"]},{"entry":[{},{},"equal","SUBSEQUENT address.","efficiently support"]},{"entry":[{},{},{},"If counter and boundary values","trylock operations for"]},{"entry":[{},{},{},"are equal, increment counter and","mutex locks."]},{"entry":[{},{},{},"return old value, else return","The counter and"]},{"entry":[{},{},{},"0x8000 0000 0000 0000.","boundary pair must be"]},{"entry":[{},{},{},"if","within a 32 Byte line."]},{"entry":[{},{},{},"(*ptrCounter!=*(ptrCounter+1)){","Rollover and"]},{"entry":[{},{},{},"\u2002return 0x8000 0000 0000 0000;","signed\/nusigned"]},{"entry":[{},{},{},"\u2002\/\/ +2{circumflex over (\u2009)}63 unsigned","software use are as for"]},{"entry":[{},{},{},"\u2002\/\/ \u22122{circumflex over (\u2009)}63 signed","\u2018load increment\u2019"]},{"entry":[{},{},{},"} else {","instruction."]},{"entry":[{},{},{},"\u2002oldValue = *ptrCounter;","On mismatch, 0x8000"]},{"entry":[{},{},{},"\u2002++*ptrCounter;","0000 0000 0000 is"]},{"entry":[{},{},{},"\u2002return oldValue;","returned."]},{"entry":[{},{},{},"}","So unsigned use is also"]},{"entry":[{},{},{},{},"restricted to the upper"]},{"entry":[{},{},{},{},"value 2{circumflex over (\u2009)}63 \u2212 1, instead"]},{"entry":[{},{},{},{},"of the optimal 2{circumflex over (\u2009)}64 \u2212 1."]},{"entry":[{},{},{},{},"This factor 2 loss is not"]},{"entry":[{},{},{},{},"expected to be a"]},{"entry":[{},{},{},{},"problem in practice."]},{"entry":["000","Store","Store","Store the given value",{}]},{"entry":["001","Store","StoreTwin","Store 8B value to 8B address","Used for fast deque"]},{"entry":[{},{},{},"given and to the SUBSEQUENT","implementations"]},{"entry":[{},{},{},"8B address, if these two locations","The address pair must"]},{"entry":[{},{},{},"previously had the equal values. ","be within a 32 Byte"]},{"entry":[{},{},{},{},"line."]},{"entry":["010","Store ","Store Add","Add store value to storage","0xFFFF FFFF FFFF"]},{"entry":[{},{},{},{},"FFFF and earlier rolls"]},{"entry":[{},{},{},{},"over to 0 and beyond."]},{"entry":[{},{},{},{},"Vice versa in the other"]},{"entry":[{},{},{},{},"direction."]},{"entry":[{},{},{},{},"So when sw uses the"]},{"entry":[{},{},{},{},"counter as unsigned,"]},{"entry":[{},{},{},{},"+2{circumflex over (\u2009)}64 \u2212 1 and earlier"]},{"entry":[{},{},{},{},"rolls over to 0 and"]},{"entry":[{},{},{},{},"beyond."]},{"entry":[{},{},{},{},"Thanks to two's"]},{"entry":[{},{},{},{},"complement, sw can"]},{"entry":[{},{},{},{},"use the counter and"]},{"entry":[{},{},{},{},"\u2018store value\u2019 as signed"]},{"entry":[{},{},{},{},"or unsigned. When"]},{"entry":[{},{},{},{},"using as signed, and"]},{"entry":[{},{},{},{},"adding a positive store"]},{"entry":[{},{},{},{},"value, then \u2032+2{circumflex over (\u2009)}63 \u2212 1"]},{"entry":[{},{},{},{},"and earlier rolls over to"]},{"entry":[{},{},{},{},"\u22122{circumflex over (\u2009)}63 and beyond."]},{"entry":[{},{},{},{},"Vice versa, when"]},{"entry":[{},{},{},{},"adding a negative store"]},{"entry":[{},{},{},{},"value."]},{"entry":["011","Store ","Store","As Store Add, but do not keep",{}]},{"entry":[{},{},"Add\/Coherence","L1-caches coherent unless",{}]},{"entry":[{},{},"on Zero","storage value reaches zero",{}]},{"entry":["100","Store","Store Or","Logical OR value to storage",{}]},{"entry":["101","Store","Store Xor","Logical XOR value to storage",{}]},{"entry":["110","Store","Store Max","Store Max of value and storage,",{}]},{"entry":[{},{},"Unsigned","values are interpreted as unsigned",{}]},{"entry":[{},{},{},"binary",{}]},{"entry":["111","Store","Store Max","Store Max of value and storage,","Allows Max of floating"]},{"entry":[{},{},"Sign\/Value","values are interpreted as 1b sign","point numbers"]},{"entry":[{},{},{},"and 63b absolute value",{}]},{"entry":[{},{},{},"If the encoding of either operand",{}]},{"entry":[{},{},{},"represents a NaN, the operand is",{}]},{"entry":[{},{},{},"assumed to be positive for",{}]},{"entry":[{},{},{},"comparison purposes."]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}}}}},"For example load increment acts similarly to a load. This instruction provides a destination address to be loaded and incremented. In other words, the load gets a special modification that tells the memory subsystem not to simply load the value, but also increment it and write the incremented data back to the same location. This instruction is useful in various contexts. For instance if there is a workload to be distributed to multiple threads, and it is not known how many threads will share the workload or which one is ready, then the workload can be divided into chunks. A function can associate a respective integer value to each of these chunks. Threads can use load-increment to get a workload by number and process it.","Each of these operations acts like a modification of main memory. If any of the core\/L1 units has a copy of the modified value, it will get a notification that the memory value has changed\u2014and it evicts and invalidates its local copy. The next time the core\/L1 unit needs the value, it has to fetch it from the 12. This process happens each time the location is modified in the 12.","A common pattern is that some of the core\/L1 units will be programmed to act when a memory location modified by atomic operations reaches a specific value. When polling for the value, repeated L1 misses, fetches from L2 followed by L1 invalidations due to atomic operations occur.","Store_add_coherence_on_zero reduces the events of the local cache being invalidated and a new copy gotten from the 12 cache. With this atomic operation, L1 cache lines will be left incoherent and not invalidated unless the modified value reaches zero The threads waiting for zero can then keep checking whatever their local value its L1 cache is even if that local value is inaccurate, until the value is actually zero. This means that one thread might modify the value as far as the L2 is concerned, without generating a miss for other threads.","In general, the operations in the above table, called \u201catomic\u201d have an effect that the regular load and store does not have. They load, read, modify and write back in one atomic operation, even within the context of speculation. This type of operation works in the context of speculation, because of the loop back in the EDRAM pipeline. It executes conflict checking equivalent to a sequence of a load and a store. Before the atomic operation is loading, it does the version aggregation discussed further in the provisional applications incorporated by reference above.","In a further aspect, a device and method for copying performance counter data are provided. The device, in one aspect, may include at least one processor core, a memory, and a plurality of hardware performance counters operable to collect counts of selected hardware-related activities. A direct memory access unit includes a DMA controller operable to copy data between the memory and the plurality of hardware performance counters. An interconnecting path connects the processor core, the memory, the plurality of hardware performance counters, and the direct memory access unit.","A method of copying performance counter data, in one aspect, may include establishing a path between a direct memory access unit to a plurality of hardware performance counter units, the path further connecting to a memory device. The method may also include initiating a direct memory access unit to copy data between the plurality of hardware performance counter units and the memory device.","Multicore chips are those computer chips with more than a single core. The extra cores may be used to offload the work of setting up a transfer of data between the performance counters and memory without perturbing the data being generated from the running application. A direct memory access (DMA) mechanism allows software to specify a range of memory to be copied from and to, and hardware to copy all of the memory in the specified range. Many chip multiprocessors (CMP) and systems on a chip (SoC) integrate a DMA unit. The DMA engine is typically used to facilitate data transfer between network devices and the memory, or between I\/O devices and memory, or between memory and memory.","Many chip architectures include a performance monitoring unit (PMU). This unit contains a number of performance counters that count a number of events in the chip. The performance counters are typically programmable to select particular events to count. This unit can count events from some or all of the processors and from other components in the system, such as the memory system, or the network system.","If software wants to use the values from performance counters, it has to read performance counters. Counters are read out using a software program which reads the memory area where performance counters are mapped by reading counters sequentially. For a system with large number of counters or with large counter access latency, executing the code to get these counter values has a substantial impact on program performance.","The mechanism of the present disclosure combines hardware and software that allows for efficient, non-obtrusive movement of hardware performance counter data between the registers that hold that data and a set of memory locations. To be able to utilize a hardware DMA unit available on the chip for copying performance counters into the memory, the hardware DMA unit is connected via paths to the hardware performance counters and registers. The DMA is initialized to perform data copy in the same way it is initialized to perform the copy of any other memory area, by specifying the starting source address, the starting destination address, and the data size of data to be copied. By offloading data copy from a processor to the DMA engine, the data transfer may occur without disturbing the core on which the measured computation or operation (i.e., monitoring and gathering performance counter data) is occurring.","A register\/memory location provides the start memory location of the first destination memory address. For example, the software, or an operating system, or the like pre-allocates memory area to provide space for writing and storing the performance counter data. Additional register and\/or memory location provides the start memory location of the first source memory address. This source address corresponds to the memory address of the first performance counter to be copied. Additional register and\/or memory location provides the size of data to be copied, or number of performance counters to be copied.","On a multicore chip, for example, the software running on an extra core, i.e., one not dedicated to gather performance data, may decide which of the performance counters to copy, utilize the DMA engine by setting up the copy, initiate the copy, and then proceed to perform other operations or work.","FIG. -- illustrates an architectural diagram showing using DMA for copying performance counter data to memory. DMA unit , performance counter unit , and L2 cache or another type of memory device  are connected on the same interconnect . A performance counter unit  may be built into a microprocessor and includes a plurality of hardware performance counters , which are registers used to store the counts of hardware-related activities within a computer. Examples of activities of which the counters  may store counts may include, but are not limited to, cache misses, translation lookaside buffer (TLB) misses, the number of instructions completed, number of floating point instructions executed, processor cycles, input\/output (I\/O) requests, and other hardware-related activities and events. A memory device , which may be an L2 cache or other memory, stores various data related to the running of the computer system and its applications.","Both the performance counter unit  and the memory  are accessible from the DMA unit . An operating system or software may allocate an area in memory  for storing the counter data of the performance counters . The operating system or software may decide which performance counter data to copy, whether the data is to be copied from the performance counters  to the memory  or the memory  to the performance counters , and may prepare a packet for DMA and inject the packet into the DMA unit , which initiates memory-to-memory copy, i.e., between the counters  and memory . In one aspect, the control packet for DMA may contain a packet type identification, which specifies that this is a memory-to-memory transfer, a starting source address of data to be copied, size in bytes of data to be copied, and a destination address where the data are to be copied. The source addresses may map to the performance counter device , and destination address may map to the memory device  for data transfer from the performance counters to the memory.","In another aspect, data transfer can be performed in both directions, not only from the performance counter unit to the memory, but also from the memory to the performance counter unit. Such a transfer may be used for restoring the value of the counter unit, for example.","Multiple cores  may be running different processes, and in one aspect, the software that prepares the DMA packet and initiates the DMA data transfer may be running on a core that is separate from the process running on another core that is gathering the hardware performance monitoring data. In this way, the core running a measure computation, i.e., that gathers the hardware performance monitoring data, need not be disturbed or interrupted to perform the copying to and from the memory .","FIG. -- is a flow diagram illustrating a method for using DMA for copying performance counter data to memory. At , software sets up a DMA packet that specifies at least which performance counters are involved in copying, the memory location in memory device that is involved in copying. At , the software injects the DMA packet into the DMA unit, which invokes the DMA unit to perform the specified copy. At , the software is free to perform its other tasks. At , asynchronous to the software performing other tasks, the DMA unit performs the instructed copy between the performance counters and the memory as directed in the DMA packet. In one embodiment, the software that prepares and injects the DMA packet runs on one core on a microprocessor, and is a separate process from the process that may be gathering the measurement data for the performance counters, which may be running on a different core.","FIG. -- is a flow diagram illustrating a method for using DMA for copying performance counter data to memory in another aspect. At , destination address and source address are specified. The operating system or another software may specify the destination address and source address, for example, in a DMA packet. At , data size and number of counters are specified. Again, the operating system or another software may specify the data size and number of counters to copy in the DMA packet. At , a DMA device checks the address range specified in the packet and if not correct, an error signal is generated at . The DMA device then waits for next packet. If the address range is correct at , the DMA device starts copying the counter data at . At , the DMA device performs a store to the specified memory address. At , the destination address is incremented by the length of counter data copied. At , if not all counters have been copied, the control returns to  to perform the next copy. If all counters have been copied, the control returns to .","A device and method for hardware supported performance counter data collection are provided. The device, in one aspect, may include a plurality of performance counters operable to collect one or more counts of one or more selected activities. A first storage element may be operable to store an address of a memory location, and a second storage element may be operable to store a value indicating whether the hardware should begin copying. A state machine is operable to detect the value in the second storage element and trigger hardware copying of data in selected one or more of the plurality of performance counters to the memory location whose address is stored in the first storage element.","The present disclosure, in one aspect, describes hardware support to facilitate transferring the performance counter data between the hardware performance counters and memory. One or more hardware capability and configurations are disclosed that allow software to specify a memory location and have the hardware engine copy the counters without the software getting involved. In another aspect, the software may specify a sequence of memory locations and have the hardware perform a sequence of copies from the hardware performance counter registers to the sequence of memory locations specified by software. In this manner, the hardware need not interrupt the software.","The mechanism of the present disclosure combines hardware and software capabilities to allow for efficient movement of hardware performance counter data between the registers that hold that data and a set of memory locations. The following description of the embodiments uses the term \u201chardware\u201d interchangeably with the state machine and associated registers used for controlling the automatic copying of the performance counter data to memory. Further, the term \u201csoftware\u201d may refer to the hypervisor, operating system, or another tool that either of those layers has provided direct access to. For example the operating system could set up a mapping, allowing a tool with the correct permission, to interact directly with the hardware state machine.","A direct memory engine (DMA) may be used to copy the values of performance monitoring counters from the performance monitoring unit directly to the memory without intervention of software. The software may specify the starting address of the memory where the counters are to be copied, and a number of counters to be copied.","After initialization of the DMA engine in the performance monitoring unit by software, other functions are performed by hardware. Events are monitored and counted, and an element such as a timer keeps track of time. After a time interval expires, or another triggering event, the DMA engine starts copying counter values to the predestined memory locations. For each performance counter, the destination memory address is calculated, and a set of signals for writing the counter value into the memory is generated. After all counters are copied to memory, the timer (or another triggering event) may be reset.","FIG. -- is a diagram illustrating a hardware unit with a series of control registers. The hardware unit  includes hardware performance counters , which may be implemented as registers, and collect information on various activities and events occurring on the processor.","The device  may be built into a microprocessor and includes a plurality of hardware performance counters , which are registers used to store the counts of hardware-related activities within a computer. Examples of activities of which the counters  may store counts may include, but are not limited to, cache misses, translation lookaside buffer (TLB) misses, the number of instructions completed, number of floating point instructions executed, processor cycles, input\/output (I\/O) requests, and other hardware-related activities and events.","Other examples may include, but are not limited to, events related to the network activity, like number of packets sent or received in each of networks links, errors when sending or receiving the packets to the network ports, or errors in the network protocol, events related to the memory activity, for example, number of cache misses for any or all cache level L1, L2, L3, or the like, or number of memory requests issued to each of the memory banks for on-chip memory, or number of cache invalidates, or any memory coherency related events. Yet more examples may include, but are not limited to, events related to one particular processor's activity in a chip multiprocessor systems, for example, instructions issued and completed, integer and floating-point, for the processor 0, or for any other processor, the same type of counter events but belonging to different processors, for example, the number of integer instructions issued in all N processors. Those are some of the examples activities and events the performance counters may collect.","A register or a memory location  may specify the frequency at which the hardware state machine should copy the hardware performance counter registers  to memory. Software, such as the operating system, or a performance tool the operating system has enabled to directly access the hardware state machine control registers, may set this register to frequency at which it wants the hardware performance counter registers  sampled.","Another register or memory location  may provide the start memory location of the first memory address . For example, the software program running in address space A, may have allocated memory to provide space to write the data. A segmentation fault may be generated if the specific memory location is not mapped writable into the user address space A, that interacted with the hardware state machine  to set up the automatic copying.","Yet another register or memory location  may indicate the length of the memory region to be written to. For each counter to be copied, hardware calculates the destination address, which is saved in the register .","For the hardware to automatically and directly perform copy of data from the performance counters  to store in the memory area , the software may set a time interval in the register . The time interval value is copied into the timer  that counts down, which upon reaching zero, triggers a state machine  to invoke copying of the data to the address of memory specified in register . For each new value to be stored, the current address in register  is calculated. When the interval timer reaches zero, the hardware may perform the copying automatically without involving the software.","In addition, or instead of using the time interval register  and timer , an external signal  generated outside of the performance monitoring unit may be used to start direct copying. For example, this signal may be an interrupt signal generated by a processor, or by some other component in the system.","Optionally, a register or memory location  may contain a bit mask indicating which of the hardware performance counter registers  should be copied to memory. This allows software to choose a subset of the registers of critical registers. Copying and storing only a selected set of hardware performance counters may be more efficient in terms of the amount of the memory consumed to gather the desired data.","In one aspect, hardware may be responsible for ensuring that memory address is valid. In this embodiment, state machine  checks for each address if it is within the memory area specified by the starting address, as specified in , and length value, as specified in . In the case the address is beyond that boundary, an interrupt signal for segmentation fault may be generated for the operating system.","In another aspect, software may be responsible to keep track of the available memory and to provide sufficient memory for copying performance counters. In this embodiment, for each counter to be copied, hardware calculates the next address without making any address boundary checks.","Another register or memory location  may store a value that specifies the number of times to write the above specified hardware performance counters to memory . This register may be decremented every time a DMA engine starts its copying all, or selected counters to the memory. After this register reached zero, the counters are no more copied until the next re-programming by software. Alternatively or additionally, the value may include an on or off bit which indicates whether the hardware should collect data or not.","The memory location for writing and collecting the counter data may be a pre-allocated block  at the memory  such as L2 cache or another with a starting address (e.g., specified in ) and a predetermined length (e.g., specified in ). In one embodiment, the block  may be written once until the upper boundary is reached, after which an interrupt signal may be initialized, and further copying is stopped. In another embodiment, memory block  is arranged as a circular buffer, and it is continuously overwritten each time the block is filled. In this embodiment, another register  or memory location may be used to store an indication as to whether the hardware should wrap back to the beginning of the area, or stop when it reaches the end of the memory region or block specified by software. Memory device  that stores the performance counter data may be an L2 cache, L3 cache, or memory.","FIG. -- is a diagram illustrating a hardware unit with a series of control registers that support collecting of hardware counter data to memory in another embodiment of the present disclosure. The performance counter unit  includes a plurality of performance counters  collecting processor or hardware related activities and events.","A time interval register  may store a value that specifies the frequency of copying to be performed, for example, a time value that specifies to perform a copy every certain time interval. The value may be specified in seconds, milliseconds, instruction cycles, or others. A software entity such as an operating system or another application may write the value in the register . The time interval value  is set in the timer  for the timer  to being counting the time. Upon expiration of the time, the timer  notifies the state machine  to trigger the copying.","The state machine  reads the address value of  and begins copying the data of the performance counters specified in the counter list register  to the memory location  of the memory  specified in the address register . When the copying is done, the timer  is reset with the value specified in the time interval , and the timer  begins to count again.","The register  or another memory location stores the list of performance counters, whose data should be copied to memory . For example, each bit stored in the register  may correspond to one of the performance counters. If a bit is set, for example, the associated performance counter should be copied. If a bit is not set, for example, the associated performance counter should not be copied.","The memory location for writing and collecting the counter data may be a set of distinct memory blocks specified by set of addresses and lengths. Another set of registers or memory locations  may provide the set of start memory locations of the memory blocks . Yet another set of registers or memory locations  may indicate the lengths of the set of memory blocks  to be written to. The starting addresses  and lengths  may be organized as a list of available memory locations.","A hardware mechanism, such as a finite state machine  in the performance counter unit  may point from memory region to memory region as each one gets filled up. The state machine may use current pointer register or memory location  to indicate where in the multiple specified memory regions the hardware is currently copying to, or which of the pairs of start address  and length  it is currently using from the performance counter unit .","The state machine  uses the current address and length registers, as specified in , to calculate the destination address . The value in  stays unchanged until the state machine identifies that the memory block is full. This condition is identified by comparing the destination address  to the sum of the start address  and the memory block length . Once a memory block is full, the state machine  increments the current register  to select a different pair of start register  and length register .","Another register or memory location  may be used to store an indication as to whether the hardware should wrap back to the beginning of the area, or stop when it reaches the end of the memory region or block specified by software.","Another register or memory location  may store a value that specifies the number of times to write the above specified hardware performance counters to memory . Each time the state machine  initiates copying and\/or storing, the value of the number of writes  is decremented. If the number reaches zero, the copying is not performed. Further copying from the performance counters  to memory  may be re-established after an intervention by software.","In another aspect, an external interrupt  or another signal may trigger the state machine  or another hardware component to start the copying. The external signal  may be generated outside of the performance monitoring unit  to start direct copying. For example, this signal may be an interrupt signal generated by a processor, or by some other component in the system.","FIG. -- is a flow diagram illustrating a hardware support method for collecting hardware performance counter data in one embodiment of the present disclosure. At , a software thread writes time interval value into a designated register. At , a hardware thread reads the value and transfers the value into a timer register. At , the timer register counts down the time interval value, and when the timer count reaches zero, notifies a state machine. Any other method of detecting expiration of the timer value may be utilized. At , the state machine triggers copying of all or selected performance counter register values to specified address in memory. At , hardware thread copies the data to memory. At , the hardware thread checks whether more copying should be performed, for example, by checking a value in another register. If more copying is to be done, then the processing returns to .","FIG. -- is a flow diagram illustrating a hardware support method for collecting hardware performance counter data in another embodiment of the present disclosure. At , a state machine or another like hardware waits, for example, for a signal to start performing copies of the performance counters. The signal may be an external interrupt initiated by another device or component, or another notification. The state machine need not be idle while waiting. For example, the state machine may be performing other tasks while waiting. At , the state machine receives an interrupt or another signal. At , the state machine or another hardware triggers copying of hardware performance counter data to memory. At , performance counter data is copied to memory. At , it is determined whether there is more copying to be done. If there is more copying to be done, the step proceeds to . If all copies are done, method stops.","While the above description referred to a timer element that detects the time expiration for triggering the state machine for, it should be understood that other devices, elements, or methods may be utilized for triggering the state machine. For instance, an interrupt generated by another element or device may trigger the state machine to begin copying the performance counter data.","As shown with respect to FIG. --, there is further provided the ability for software-initiated automatic saving and restoring of the data associated with the performance monitoring unit including the entire set of control registers and associated counter values. Automatic refers to the fact that the hardware goes through each of the control registers and data values of the hardware performance counter information and stores them all into memory rather than requiring the operating system or other such software (for example, one skilled in the art would understand how to apply the mechanisms described herein to a hypervisor environment) to read out the values individually and store the values itself.","While there are many operations that need to occur as part of a context switch, this disclosure focuses the description on those that pertain to the hardware performance counter infrastructure. In preparation for performing a context switch, the operating system, which knows of the characteristics and capabilities of the computer, will have set aside memory associated with each process commensurate with the number of hardware performance control registers and data values.","One embodiment of the hardware implementation to perform the automatic saving and restoring of data may utilize two control registers associated with the infrastructure, i.e., the hardware performance counter unit. One register, R1 (for convenience of naming), , is designated to hold the memory address that data is to be copied to or from. Another register, for example, a second register R2, , indicates whether and how the hardware should perform the automatic copying process. The value of second register is normally a zero. When the operating system wishes to initiate a copy of the hardware performance information to memory it writes a value in the register to indicate this mode. When the operating system wishes to initiate a copy of the hardware performance values from memory it writes another value in the register that indicates this mode. For example, when the operating system wishes to initiate a copy of the hardware performance information to memory it may write a \u201c1\u201d to the register, and when the operating system wishes to initiate a copy of the hardware performance values from memory it may write a \u201c2\u201d to the register. Any other values to indications may be utilized. This may be an asynchronous operation, i.e., the hardware and the operating system may operate or function asynchronously. An asynchronous operation allows the operating system to continue performing other tasks associated with the context switch while the hardware automatically stores the data associated with the performance monitoring unit and sets an indication when finished that the operating system can check to ensure the process was complete. Alternatively, in another embodiment, the operation may be performed synchronously by setting a third register. For example, R3,  can be set to \u201c1\u201d indicating that the hardware should not return control to the operating system after the write to R2 until the copying operation has completed.","FIG. -- illustrates an architectural diagram showing hardware enabled performance counters with support for operating system context switching in one embodiment of the present disclosure. A performance counter unit  may be built into a microprocessor, or in a multiprocessor system, and includes a plurality of hardware performance counters , which are registers used to store the counts of hardware-related activities within a computer. Examples of activities of which the counters  may store counts may include, but are not limited to, cache misses, translation lookaside buffer (TLB) misses, the number of instructions completed, number of floating point instructions executed, processor cycles, input\/output (I\/O) requests, and other hardware-related activities and events.","A memory device , which may be an L2 cache or other memory, stores various data related to the running of the computer system and its applications. A register  stores an address location in memory  for storing the hardware performance counter information associated with the switched out process. For example, when the operating system determines it needs to switch out a given process A, it looks up in its data structures the previously allocated memory addresses (e.g., in ) for process A's hardware performance counter information and writes the beginning value of that address range into a register . A register  stores an address location in memory  for loading the hardware performance counter information associated with the switched in process. For example, when the operating system determines it needs to switch in a given process B, it looks up in its data structures the previously allocated memory addresses (e.g., in ) for process B's hardware performance counter information and writes the beginning value of that address range into a register .","Context switch register  stores a value that indicates the mode of copying, for example, whether the hardware should start copying, and if so, whether the copying should be from the performance counters  to memory , or from the memory  to the performance counters , for example, depending on whether the process is being context switched in or out. Table 1 for examples shows possible values that may be stored by or written into the context switch  as an indication for copying. Any other values may be used.",{"@attributes":{"id":"p-1084","num":"1210"},"tables":{"@attributes":{"id":"TABLE-US-00006","num":"00006"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"189pt","align":"left"}}],"thead":{"row":[{"entry":"TABLE 1"},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}},{"entry":["Value","Meaning of the value"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["0","No copying needed"]},{"entry":["1","Copy the current values from the performance counters to "]},{"entry":[{},"the memory location indicated in the context address current "]},{"entry":[{},"register, and then copy values from the memory location "]},{"entry":[{},"indicated in the context address new to the performance counters"]},{"entry":["2","Copy from the performance counters to the memory location "]},{"entry":[{},"indicated in the context address register"]},{"entry":["3","Copy from the memory location indicated in context address "]},{"entry":[{},"register to the performance counters"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},"The operating system for example writes those values into the register , according to which the hardware performs its copying.","A control state machine  starts the context switch operation of the performance counter information when the register  holds values that indicate that the hardware should start copying. If the value in the register  is 1 or 2, the circuitry of the performance counter unit  stores the current context (i.e., the information in the performance counters ) of the counters  to the memory area  specified in the context address register . This actual data copying can be performed by a simple direct memory access engine (DMA), not shown in the picture, which generates all bus signals necessary to store data to the memory. Alternatively, this functionality can be embedded in the state machine . All performance counters and their configurations are saved to the memory starting at the address specified in the register . The actual arrangement of counter values and configuration values in the memory addresses can be different for different implementations, and does not change the scope of this invention.","If the value in the register  is 3, or is 1 and the copy-out step described above is completed, the copy-in step starts. The new context (i.e., hardware performance counter information associated with the process being switched in) is loaded from the memory area  indicated in the context address . In addition, the values of performance counters are copied from the memory back to the performance counters . The exact arrangement of counter values and configurations values does not change the scope of this invention.","When the copying is finished, the state machine  sets the context switch register to a value (e.g., \u201c0\u201d) that indicates that the copying is completed. In another embodiment, the performance counters may generate an interrupt to signal the completion of copying. The interrupt may be used to notify the operating system that the copying has completed. In one embodiment, the hardware clears the context switch register . In another embodiment, the operating system resets the context switch register value  (e.g., \u201c0\u201d) to indicate no copying.","The state machine  copies the memory address stored in the context address register  to the context address register . Thus, the new context address is free to be used in the future for the next context switch, and the current context will be copied back to its previous memory location.","In another embodiment of the implementation, the second context address register  may not be needed. That is, the operating system may use one context address register  for indicating the memory address to copy to or to copy from, for context switching out or context switching in, respectively. Thus, for example, register  may be also used for indicating a memory address from where to context switch in the hardware performance counter information associated with a process being context switched in, when the operating system is context switching back in a process that was context switched out previously.","Additional number of registers or the like, or different configurations for hardware performance counter unit may be used to accomplish the automatic saving of storing and restoring of contexts by the hardware, for example, while the operating system may be performing other operations or tasks, and\/or, so that the operating system or the software or the like need not individually read the counters and associated controls.","FIG. -- is a flow diagram illustrating a method for hardware enabled performance counters with support for operating system context switching in one embodiment of the present disclosure. While the method shown in FIG. -- illustrates a specific steps for invoke the automatic copying mechanisms using several registers, it should be understood that other implementation of the method and any number of registers or the like may be used for the operating system or the like to invoke an automatic copying of the counters to memory and memory to counters by the hardware, for instance, so that the operating system or the like does not have to individually read the counters and associated controls.","Referring to FIG. --, at  when the operating system determines it needs to switch out a given process A, it looks up in its data structures the previously allocated memory addresses for process A's hardware performance counter information and writes the beginning value of that range into a register, e.g., register R1. At , the operating system or the like then writes a value in another register, e.g., register R2 to indicate that copying from the performance counters to the memory should begin. For instance, the operating system or the like writes \u201c1\u201d to R2. At , the hardware identifies that the value in register R2 or the like indicates data copy-out command, and based on the value performs copying. For example, writing values 1 or 2 in the register R2 generates a signal \u201cstart copying data\u201d which causes the state machine to enter the state \u201ccopy data\u201d. In this state, for example, data are stored to the memory starting at the specified memory location, and respecting the implemented bus protocol. This step may include driving bus control signals to specify store operation, driving address lines with destination address and driving data lines with data values to be stored. The exact memory writing protocol of the particular implementation may be followed, i.e., how many cycles these bus signals need to be driven, and if there is an acknowledgement signal from the memory that writing succeeded. The exact bus protocol and organization does not change the scope of this invention. The data store operation is performed for all values which need to be copied.","The operating system or the like may proceed in performing other operations while the hardware copies that data from the hardware performance control and data registers. At , after the hardware finishes copying, the hardware resets the value at register R1, for example, to \u201c0\u201d to indicate that the copying is done. At , prior to completing the context switch, the operating system or the like checks the value of register R2 to make sure it is \u201c0\u201d or another value, which indicates that the hardware has finished the copy.","For context switching back in process B, the operating system or the like may perform the similar procedure. For example, the operating system writes the beginning of the range of addresses used for storing hardware performance counter information associated with process B into register R1 (or another such designated memory location), writes a value (e.g., \u201c3\u201d) into register R2 to indicate to the hardware to start copying from the memory location specified in register R1 to the hardware performance counters. The operating system or the like may proceed with other context restoring operation. Prior to returning control to the process, the operating system verifies that the hardware finished its copying function, for example, by checking the value in R2 (in this example, checking for \u201c0\u201d value). In this way, the copying of the hardware performance counter information with the other operations needed when performing a context switch can be performed in parallel, or substantially in parallel.","In another embodiment, rather than having the operating system check a register to determine whether the hardware completed its copying, another register, R3, may be used to indicate to the hardware whether and when the control to the operating system should be returned. For instance, if this register is set to a predetermined value, e.g., \u201c1\u201d, the hardware will not return control to the operating system until the copy is complete. For example, this register, or a bit in another control register, is labeled \u201cinterrupt enabled\u201d, and it specifies that an interrupt signal should be raised when data copy is complete. Operating system performs operations which are part of context switching in parallel. Once this interrupt is received, operating system is informed that all data copying of the performance counters is completed.","FIG. -- is a flow diagram illustrating hardware enabled performance counters with support for operating system context switching using a register setting in one embodiment of the present disclosure. At , if the register value is not zero, the method may proceed to . At , if the register value is one or three, configuration registers and counter values are copied to memory at . At  if all configuration registers and counter values have been copied, the method may proceed to . At , if the register value is one, the method proceeds to , otherwise the method proceeds to . Also at  if the register value was not one and not three, then the method proceeds to . At , values from the memory are copied into configuration registers and counter values. At , new configuration address is copied into the current configuration address. At , the register value is set to zero.","The above described examples used the register values as being set to \u201c0\u201d, \u201c1\u201d, and \u201c2\u201d in explaining the different modes indicated in the register value. It should be understood, however, that any other values may be used to indicate the different modes of copying.","There is further provided hardware support to facilitate the efficient hardware switching and storing of counters. Particularly, in one aspect, the hardware support of the present disclosure allows specification of a set of groups of hardware performance counters, and the ability to switch between those groups without software intervention.","In one embodiment, hardware and software is combined that allows for the ability to set up a series of different configurations of hardware performance counter groups. The hardware may automatically switch between the different configurations at a predefined interval. For the hardware to automatically switch between the different configurations, the software may set an interval timer that counts down, which upon reaching zero, switches to the next configuration in the stored set of configurations. For example, the software may set up the set of configurations that it wants the hardware to switch between and also set a count of the number of hardware configurations it has set up. When the interval timer reaches zero, the hardware may update the currently collected set of hardware counters automatically without involving the software and set up a new group of hardware performance counters to start being collected.","In another aspect, another configuration switching trigger may be utilized instead of a timer element. For example, an interrupt or an external interrupt from another device may be set up to periodically or at a predetermined time or event, to trigger the hardware performance counter reconfiguration or switching.","In one embodiment, a register or memory location specifies the number of times to perform the configuration switch. In another embodiment, rather than a count, an on\/off binary value may indicate whether hardware should continue switching configurations or not.","Yet in another embodiment, the user may set a register or memory location to indicate that when the hardware switches groups, it should clear performance counters. In still yet another embodiment, a mask register or memory location may be used to indicate which counters should be cleared.","FIG. -- shows a hardware device  that supports performance counter reconfiguration in one embodiment of the present disclosure. The device  may be built into a microprocessor and includes a plurality of hardware performance counters , which are registers or the like used to store the counts of hardware-related activities within a computer. Examples of activities of which the counters  may store counts may include, but are not limited to, cache misses, translation lookaside buffer (TLB) misses, the number of instructions completed, number of floating point instructions executed, processor cycles, input\/output (I\/O) requests, and other hardware-related activities and events.","A plurality of configuration registers ,  may each include a set of configurations that specify what activities and\/or events the counters  should count. For example, configuration 1 register  may specify counter events related to the network activity, like the number of packets sent or received in each of networks links, the errors when sending or receiving the packets to the network ports, or the errors in the network protocol. Similarly, configuration 2 register  may specify a different set of configurations, for example, counter events related to the memory activity, for instance, the number of cache misses for any or all cache level L1, L2, L3, or the like, or the number of memory requests issued to each of the memory banks for on-chip memory, or the number of cache invalidates, or any memory coherency related events. Yet another counter configuration can include counter events related to one particular processor's activity in a chip multiprocessor systems, for example, instructions issued or instructions completed, integer and floating-point instructions, for the processor 0, or for any other processor. Yet another counter configuration may include the same type of counter events but belonging to different processors, for example, the number of integer instructions issued in all N processors. Any other counter configurations are possible. In one aspect, software may set up those configuration registers to include desired set of configurations by writing to those registers.","Initially, the state machine may be set to select a configuration (e.g.,  or ), for example, using a multiplexer or the like at . A multiplexer or the like at  then selects from the activities and\/or events , , , , , etc., the activities and\/or events specified in the selected configuration (e.g.,  or ) received from the multiplexer . Those selected activities and\/or events are then sent to the counters . The counters  accumulate the counts for the selected activities and\/or events.","A time interval component  may be a register or the like that stores a data value. In another aspect, the time interval component  may be a memory location or the like. Software such as an operating system or another program may set the data value in the time interval . A timer  may be another register that counts down from the value specified in the time interval register . In response to the count down value reaching zero, the timer  notifies a control state machine . For instance, when the timer reaches zero, this condition is recognized, and a control signal connected to the state machine  becomes active. Then the timer  may be reset to the time interval value to start a new period for collecting data associated with the next configuration of hardware performance counters.","In response to receiving a notification from the timer , the control state machine  selects the next configuration register, e.g., configuration 1 register  or configuration 2 register  to reconfigure activities tracked by the performance counters . The selection may be done using a multiplexer , for example, that selects between the configuration registers  and . It should be noted that while two configuration registers are shown in this example, any number of configuration registers may be implemented in the present disclosure. Activities and\/or events (e.g., as shown at , , , , , etc.) are selected by the multiplexer  based on the configuration selected at the multiplexer . Each counter at  accumulates counts for the selected activities and\/or events.","In another embodiment, there may be a register or memory location labeled \u201cswitch\u201d  for indicating the number of times to perform the configuration switch. In yet another embodiment, the indication to switch may be provided by an on\/off binary value. In the embodiment with a number of possible switching between the configurations, the initial value may be specified by software. Each time the state machine  initiates state switching, the value of the remaining switching is decremented. Once the number of the allowed configuration switching reaches zero, all further configuration change conditions are ignored. Further switching between the configurations may be re-established after intervention by software, for instance, if the software re-initializes the switch value.","In addition, a register or memory location \u201cclear\u201d  may be provided to indicate whether to clear the counters when the configuration switch occurs. In one embodiment, this register has only one bit, to indicate if all counter values have to be cleared when the configuration is switched. In another embodiment, this counter has a number of bits M+1, where M is the number of performance counters . These register or memory values may be a mask register or memory location for indicating which of M counters should be cleared. In this embodiment, when configuration switching condition is identified, the state machine  clears the counters and selects different counter events by setting appropriate control signals for the multiplexer . If the clear mask is used, only the selected counters will be cleared. This may be implemented, for example, by AND-ing the clear mask register bits  and \u201cclear registers\u201d signal generated by the state machine  and feeding them to the performance counters .","In addition, or instead of using the time interval register  and timer , an external signal  generated outside of the performance monitoring unit may be used to start reconfiguration. For example, this signal may be an interrupt signal generated by a processor, or by some other component in the system. In response to receiving this external signal, the state machine  may start reconfiguration in the same way as described above.","FIG. -- is a flow diagram illustrating a hardware support method that supports software controlled reconfiguration of performance counters in one embodiment of the present disclosure. At , a timer element reads a value from a time interval register or the like. The software, for example, may have set or written the value into the time interval register. Examples of the software may include, but are not limited to, an operating system, another system program, or an application program, or the like. The value indicates the time interval for switching performance counter configuration. The value may be in units of clock cycles, milliseconds, seconds, or others. At , the timer element detects the expiration of the time specified by the value. For instance, the timer element may have counted down from the value and when the value reaches zero, the timer elements detects that the value has expired. Any other methods may be utilized by the timer element to detect the expiration of the time interval, e.g., the timer element may count up from zero until it reaches the value.","At , in response to detecting that the time interval set in the time interval register has passed, the timer element signals or otherwise notifies the state machine controlling the configuration register selection. At , the state machine selects the next configuration, for example, stored in a register. For example, the performance counters may have been providing counts for activities specified in configuration register A. After the state machine  selects the next configuration, for example, configuration register B, the performance counters start counting the activities specified in configuration register B, thus reconfiguring the performance counters. Once the state machine switches configuration, the timer elements again starts counting the time. For example, the timer element may again read the value from the timer interval register and for instance, start counting down from that number until it reaches zero. In the present disclosure, any number of configurations, for example, each stored in a register can be supported.","As described above, the desired time intervals for multiplexing (i.e., reconfiguring) are programmable. Further, the counter configurations are also programmable. For example, the software may set the desired configurations in the configuration registers. FIG. -- is a flow diagram illustrating the software programming the registers. At , the software may set the time interval value in a register, for example, from which register the time may read the value to start counting down. At , the software may set the configurations for performance counters, for instance, in different configuration registers. At , the software may set a register value that indicates whether the state machine should be switching configurations. The value may be an on\/off bit value, which the timer element reads to determine whether to signal the state machine. In another aspect, this value may be a number which indicates how many times the switching of the reconfiguration should occur. In addition, the software may set or program other parameters such as whether to clear the performance counters when switching or a select counter to clear. The steps shown in FIG. -- may be performed at any time and in any order.","There is further provided, in one aspect, hardware support to facilitate the efficient counter reconfiguration, OS switching and storing of hardware performance counters. Particularly, in one aspect, the hardware support of the present disclosure allows specification of a set of groups of hardware performance counters, and the ability to switch between those groups without software intervention. Hardware switching may be performed, for example, for reconfiguring the performance counters, for instance, to be able to collect information related to different sets of events and activities occurring on a processor or system. Hardware switching also may be performed, for example, as a result of operating system context switching that occurs between the processes or threads. The hardware performance counter data may be stored directly to memory and\/or restored directly from memory, for example, without software intervention, for instance, upon reconfiguration of the performance counters, operating system context switching, and\/or at a predetermined interval or time.","The description of the embodiments herein uses the term \u201chardware\u201d interchangeably with the state machine and associated registers used for controlling the automatic copying of the performance counter data to memory. Further, the term \u201csoftware\u201d may refer to the hypervisor, operating system, or another tool that either of those layers has provided direct access of the hardware to. For example, the operating system could set up a mapping, allowing a tool with the correct permission to interact directly with the hardware state machine.","In one aspect, hardware and software may be combined to allow for the ability to set up a series of different configurations of hardware performance counter groups. The hardware then may automatically switch between the different configurations. For the hardware to automatically switch between the different configurations, the software may set an interval timer that counts down, which upon reaching zero, switches to the next configuration in the stored set of configurations. For example, the software may set up a set of configurations that it wants the hardware to switch between and also set a count of the number of hardware configurations it has set up. In response to the interval timer reaching zero, the hardware may change the currently collected set of hardware performance counter data automatically without involving the software and set up a new group of hardware performance counters to start being collected. The hardware may automatically copy the current value in the counters to the pre-determined area in the memory. In another aspect, the hardware may switch between configurations in response to receiving a signal from another device, or receiving an external interrupt or others. In addition, the hardware may store the performance counter data directly in memory automatically.","In one embodiment, a register or memory location specifies the number of times to perform the configuration switch. In another embodiment, rather than a count, an on\/off binary value may indicate whether hardware should continue switching configurations or not. Yet in another embodiment, the user may set a register or memory location to indicate that when the hardware switches groups, it should clear performance counters. In still yet another embodiment, a mask register or memory location may be used to indicate which counters should be cleared.","FIG. -- shows a hardware device  that supports performance counter switching in one embodiment of the present disclosure. The device  may be built into a microprocessor and includes a plurality of hardware performance counters , which are registers or the like used to store the counts of hardware-related activities within a computer. Examples of activities of which the counters  may store counts may include, but are not limited to, cache misses, translation lookaside buffer (TLB) misses, the number of instructions completed, number of floating point instructions executed, processor cycles, input\/output (I\/O) requests, and network related activities, other hardware-related activities and events.","A plurality of configuration registers , ,  may each include a set of configurations that specify what activities and\/or events the counters  should count. For example, configuration 1 register  may specify counter events related to the network activity, like the number of packets sent or received in each of networks links, the errors when sending or receiving the packets to the network ports, or the errors in the network protocol. Similarly, configuration 2 register  may specify a different set of configurations, for example, counter events related to the memory activity, for instance, the number of cache misses for any or all cache level L1, L2, L3, or the like, or the number of memory requests issued to each of the memory banks for on-chip memory, or the number of cache invalidates, or any memory coherency related events. Yet another counter configuration can include counter events related to one particular process activity in a chip multiprocessor systems, for example, instructions issued or instructions completed, integer and floating-point instructions, for the process 0, or for any other process. Yet another counter configuration may include the same type of counter events but belonging to different processes, for example, the number of integer instructions issued in all N processes. Any other counter configurations are possible. In one aspect, software may set up those configuration registers to include desired set of configurations by writing to those registers.","Initially, the state machine  may be set to select a configuration (e.g., , , . . . , or ), for example, using a multiplexer or the like at . A multiplexer or the like at  then selects from the activities and\/or events , , , , , etc., the activities and\/or events specified in the selected configuration (e.g.,  or ) received from the multiplexer . Those selected activities and\/or events are then sent to the counters . The counters  accumulate the counts for the selected activities and\/or events.","A time interval component  may be a register or the like that stores a data value. In another aspect, the time interval component  may be a memory location or the like. Software such as an operating system or another program may set the data value in the time interval . A timer  may be another register that counts down from the value specified in the time interval register . In response to the count down value reaching zero, the timer  notifies a control state machine . For instance, when the timer reaches zero, this condition is recognized, and a control signal connected to the state machine  becomes active. Then the timer  may be reset to the time interval value to start a new period for collecting data associated with the next configuration of hardware performance counters.","In another aspect, an external interrupt or another signal  may trigger the state machine  to begin reconfiguring the hardware performance counters .","In response to receiving a notification from the timer  or another signal, the control state machine  selects the next configuration register, e.g., configuration 1 register  or configuration 2 register  to reconfigure activities tracked by the performance counters . The selection may be done using a multiplexer , for example, that selects between the configuration registers , , . It should be noted that while three configuration registers are shown in this example, any number of configuration registers may be implemented in the present disclosure. Activities and\/or events (e.g., as shown at , , , , , etc.) are selected by the multiplexer  based on the configuration selected at the multiplexer . Each counter at  accumulates counts for the selected activities and\/or events.","In another embodiment, there may be a register or memory location labeled \u201cswitch\u201d  for indicating the number of times to perform the configuration switch. In yet another embodiment, the indication to switch may be provided by an on\/off binary value. In the embodiment with a number of possible switching between the configurations, the initial value may be specified by software. Each time the state machine  initiates state switching, the value of the remaining switching is decremented. Once the number of the allowed configuration switching reaches zero, all further configuration change conditions are ignored. Further switching between the configurations may be re-established after intervention by software, for instance, if the software re-initializes the switch value.","In addition, a register or memory location \u201cclear\u201d  may be provided to indicate whether to clear the counters when the configuration switch occurs. In one embodiment, this register has only one bit, to indicate if all counter values have to be cleared when the configuration is switched. In another embodiment, this counter has a number of bits M+1, where M is the number of performance counters . These register or memory values may be a mask register or memory location for indicating which of M counters should be cleared. In this embodiment, when configuration switching condition is identified, the state machine  clears the counters and selects different counter events by setting appropriate control signals for the multiplexer . If the clear mask is used, only the selected counters may be cleared. This may be implemented, for example, by AND-ing the clear mask register bits  and \u201cclear registers\u201d signal generated by the state machine  and feeding them to the performance counters .","In addition, or instead of using the time interval register  and timer , an external signal  generated outside of the performance monitoring unit may be used to start reconfiguration. For example, this signal may be an interrupt signal generated by a processor, or by some other component in the system. In response to receiving this external signal, the state machine  may start reconfiguration in the same way as described above.","In addition, the software may specify a memory location  and have the hardware engine copy the counters without the software getting involved. In another aspect, the software may specify a sequence of memory locations and have the hardware perform a sequence of copies from the hardware performance counter registers to the sequence of memory locations specified by software.","The hardware may be used to copy the values of performance monitoring counters  from the performance monitoring unit  directly to the memory area  without intervention of software. The software may specify the starting address  of the memory where the counters are to be copied, and a number of counters to be copied.","In hardware, events are monitored and counted, and an element such as a timer  keeps track of time. After a time interval expires, or another triggering event, the hardware may start copying counter values to the predetermined memory locations. For each performance counter, the destination memory address  may be calculated, and a set of signals for writing the counter value into the memory may be generated. After the specified counters are copied to memory, the timer (or another triggering event or element) may be reset.","Referring to FIG. --, a register or a memory location  may specify how many times the hardware state machine should copy the hardware performance counter registers  to memory. Software, such as the operating system, or a performance tool the operating system enabled to directly access the hardware state machine control registers, may set this register to frequency at which it wants the hardware performance counter registers  sampled.","In another aspect, instead of a separate register or memory location , the register at  that specifies the number of configuration switches may be also used for specifying the number of memory copies. In this case, the number of reconfigurations and copying to memory may coincide.","Another register or memory location  may provide the start memory location of the first memory address . For example, the software program running in address space A, may have allocated memory to provide space to write the data. A segmentation fault may be generated if the specific memory location is not mapped writable into the user address space A that interacted with the hardware state machine  to set up the automatic copying.","Yet another register or memory location  may indicate the length of the memory region to be written to. For each counter to be copied, hardware calculates the destination address, which is saved in the register .","For the hardware to automatically and directly perform copy of data from the performance counters  to store in the memory area , the software may set a time interval in the register . The time interval value may be copied into the timer  that counts down, which upon reaching zero, triggers a state machine  to invoke copying of the data to the address of memory specified in register . For each new value to be stored, the current address in register  is calculated. When the interval timer reaches zero, the hardware may perform the copying automatically without involving the software. The time interval register  and the timer  may be utilized by the performance counter unit for both counter reconfiguration and counter copy to memory, or there may be two sets of time interval registers and timers, one used for directly copying the performance counter data to memory, the other used for counter reconfiguration. In this manner, the reconfiguration of the hardware performance counters and copying of hardware performance counter data may occur independently or asynchronously.","In addition, or instead of using the time interval register  and timer , an external signal  generated outside of the performance monitoring unit may be used to start direct copying. For example, this signal may be an interrupt signal generated by a processor or by some other component in the system.","Optionally, a register or memory location  may contain a bit mask indicating which of the hardware performance counter registers  should be copied to memory. This allows software to choose a subset of the registers. Copying and storing only a selected set of hardware performance counters may be more efficient in terms of the amount of the memory consumed to gather the desired data.","The software is responsible for pre-allocating a region of memory sufficiently large to hold the intended data. In one aspect, if the software does not pass a large enough buffer in, a segmentation fault will occur when the hardware attempts to write the first piece of data beyond the buffer provided by the user (assuming the addressed location is unmapped memory).","Another register or memory location  may store a value that specifies the number of times to write the above specified hardware performance counters to memory . This register may be decremented every time the hardware state machine starts copying all, or a subset of counters to the memory. Once this register reaches zero, the counters are no longer copied until the next re-programming by software. Alternatively or additionally, the value may include an on or off bit which indicates whether the hardware should collect data or not.","The memory location for writing and collecting the counter data may be a pre-allocated block  at the memory  such as L2 cache or another with a starting address (e.g., specified in ) and a predetermined length (e.g., specified in ). In one embodiment, the block  may be written once until the upper boundary is reached, after which an interrupt signal may be initialized, and further copying is stopped. In another embodiment, memory block  is arranged as a circular buffer, and it is continuously overwritten each time the block is filled. In this embodiment, another register  or memory location may be used to store an indication as to whether the hardware should wrap back to the beginning of the area, or stop when it reaches the end of the memory region or block specified by software. Memory device  that stores the performance counter data may be an L2 cache, L3 cache, or memory.","The memory location for writing and collecting the counter data may be a set of distinct memory blocks specified by set of addresses and lengths. For example, the element shown at  may be a set of registers or memory locations that specify the set of start memory locations of the memory blocks . Similarly, the element shown at  may be another set of registers or memory locations that indicate the lengths of the set of memory blocks to be written to. The starting addresses  and lengths  may be organized as a list of available memory locations. A hardware mechanism, such as a finite state machine  in the performance counter unit  may point from memory region to memory region as each one gets filled up. The state machine may use current pointer register or memory location  to indicate where in the multiple specified memory regions the hardware is currently copying to, or which of the pairs of start address  and length  it is currently using from the performance counter unit .","FIG. -- is a flow diagram illustrating a method for reconfiguring and data copying of hardware performance counters in one embodiment of the present disclosure. At , software sets up all or some configuration registers in the performance counter unit . Software, which may be a user-level application or an operating system, may set up several counter configurations, and one or more starting memory addresses and lengths where performance counter data will be copied. In one aspect, software also writes time interval value into a designated register, and at , hardware transfers the value into a timer register. In another aspect an interrupt triggers the transfer of data or reconfiguration.","At , the timer register counts down the time interval value, and when the timer count reaches zero, notifies a state machine. Any other method of detecting expiration of the timer value may be utilized. At , the state machine triggers copying of all or selected performance counter register values to specified address in memory. At , hardware copies performance counters to the memory.","At , hardware checks if the configuration of performance counters needs to be changed, by checking a value in another register. If the configuration does not need to be changed, the processing returns to . At , a state machine changes the configuration of the performance counter data.","FIG. -- shows a hardware device that supports performance counter reconfiguration and copying, and OS context switching in one embodiment of the present disclosure. The hardware device shown in FIG. -- may include all the elements shown and described with respect to FIG. --. Further, the device may include automatic hardware support capabilities for operating system context switching. Automatic refers to the fact that the hardware goes through each of the control registers and data values of the hardware performance counter information and stores them all into memory rather than requiring the operating system or other such software (for example, one skilled in the art would understand how to apply the mechanisms described herein to a hypervisor environment) to read out the values individually and store the values itself.","While there are many operations that need to occur as part of a context switch, this disclosure focuses the description on those that pertain to the hardware performance counter infrastructure. In preparation for performing a context switch, the operating system, which knows of the characteristics and capabilities of the computer, will have set aside memory associated with each process commensurate with the number of hardware performance control registers and data values.","One embodiment of the hardware implementation to perform the automatic saving and restoring of data may utilize two control registers associated with the infrastructure, i.e., the hardware performance counter unit. One register, R1 (for convenience of naming), , is designated to hold the memory address that data is to be copied to or from. Another register, for example, a second register R2, , indicates whether and how the hardware should perform the automatic copying process. The value of second register may be normally a zero. When the operating system wishes to initiate a copy of the hardware performance information to memory it writes a value in the register to indicate this mode. When the operating system wishes to initiate a copy of the hardware performance values from memory it writes another value in the register that indicates this mode. For example, when the operating system wishes to initiate a copy of the hardware performance information to memory it may write a \u201c1\u201d to the register, and when the operating system wishes to initiate a copy of the hardware performance values from memory it may write a \u201c2\u201d to the register. Any other values for such indications may be utilized. This may be an asynchronous operation, i.e., the hardware and the operating system may operate or function asynchronously. An asynchronous operation allows the operating system to continue performing other tasks associated with the context switch while the hardware automatically stores the data associated with the performance monitoring unit and sets an indication when finished that the operating system can check to ensure the process was complete. Alternatively, in another embodiment, the operation may be performed synchronously by setting a third register. For example, R3, , can be set to \u201c1\u201d indicating that the hardware should not return control to the operating system after the write to R2 until the copying operation has completed.","Referring to FIG. --, a performance counter unit  may be built into a microprocessor, or in a multiprocessor system, and includes a plurality of hardware performance counters , which are registers used to store the counts of hardware-related activities within a computer as described above.","A memory device , which may be an L2 cache or other memory, stores various data related to the running of the computer system and its applications. A register  stores an address location in memory  for storing the hardware performance counter information associated with the switched out process. For example, when the operating system determines it needs to switch out a given process A, it looks up in its data structures the previously allocated memory addresses (e.g., in ) for process A's hardware performance counter information and writes the beginning value of that address range into a register . A register  stores an address location in memory  for loading the hardware performance counter information associated with the switched in process. For example, when the operating system determines it needs to switch in a given process B, it looks up in its data structures the previously allocated memory addresses (e.g., in ) for process B's hardware performance counter information and writes the beginning value of that address range into a register .","Context switch register  stores a value that indicates the mode of copying, for example, whether the hardware should start copying, and if so, whether the copying should be from the performance counters  to memory , or from the memory  to the performance counters , for example, depending on whether the process is being context switched in or out. Table 1 for examples shows possible values that may be stored by or written into the context switch  as an indication for copying. Any other values may be used.",{"@attributes":{"id":"p-1151","num":"1277"},"tables":{"@attributes":{"id":"TABLE-US-00007","num":"00007"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"189pt","align":"left"}}],"thead":{"row":[{"entry":"TABLE 1"},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}},{"entry":["Value","Meaning of the value"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["0","No copying needed"]},{"entry":["1","Copy the current values from the performance counters to the "]},{"entry":[{},"memory location indicated in the context address current "]},{"entry":[{},"register, and then copy values from the memory location "]},{"entry":[{},"indicated in the context address new to the performance counters"]},{"entry":["2","Copy from the performance counters to the memory location "]},{"entry":[{},"indicated in the context address register"]},{"entry":["3","Copy from the memory location indicated in context address "]},{"entry":[{},"register to the performance counters"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},"The operating system for example writes those values into the register , according to which the hardware performs its copying.","A control state machine  starts the context switch operation of the performance counter information when the signal  is active, or when the timer  indicates that the hardware should start copying. If the value in the register  is 1 or 2, the circuitry of the performance counter unit  stores the current context (i.e., the information in the performance counters ) of the counters  to the memory area  specified in the current address register . All performance counters and their configurations are saved to the memory starting at the address specified in the register . The actual arrangement of counter values and configuration values in the memory addresses can be different for different implementations, and does not change the scope of this invention.","If the value in the register  is 3, or it is 1 and the copy-out step described above is completed, the copy-in step starts. The new context (i.e., hardware performance counter information associated with the process being switched in) is loaded from the memory area  indicated in the context address . In addition, the values of performance counters are copied from the memory back to the performance counters . The exact arrangement of counter values and configurations values does not change the scope of this invention.","When the copying is finished, the state machine  may set the context switch register to a value (e.g., \u201c0\u201d) that indicates that the copying is completed. In another embodiment, the performance counters may generate an interrupt to signal the completion of copying. The interrupt may be used to notify the operating system that the copying has completed. In one embodiment, the hardware clears the context switch register . In another embodiment, the operating system resets the context switch register value  (e.g., \u201c0\u201d) to indicate no copying.","The state machine  copies the memory address stored in the context address register  to the current address register . Thus, the new context address register  is free to be used for the next context switch.","In another embodiment of the implementation, the second context address register  may not be needed. That is, the operating system may use one context address register  for indicating the memory address to copy to or to copy from, for context switching out or context switching in, respectively. Thus, for example, register  may be also used for indicating a memory address from where to context switch in the hardware performance counter information associated with a process being context switched in, when the operating system is context switching back in a process that was context switched out previously.","Additional number of registers or the like, or different configurations for hardware performance counter unit may be used to accomplish the automatic saving of storing and restoring of contexts by the hardware, for example, while the operating system may be performing other operations or tasks, and\/or, so that the operating system or the software or the like need not individually read the counters and associated controls.","FIG. -- is a flow diagram illustrating a method for reconfiguring, data copying, and context switching of hardware performance counters in one embodiment of the present disclosure. While the method shown in FIG. -- illustrates specific steps for invoking the automatic copying mechanisms using several registers, it should be understood that other implementation of the method and any number of registers or the like may be used for the operating system or the like to invoke an automatic copying of the counters to memory and memory to counters by the hardware, for instance, so that the operating system or the like does not have to individually read the counters and associated controls.","At , software sets up all or some configuration registers in the performance counter unit or module . Software, which may be a user-level application or an operating system, may set up several counter configurations, and one or more starting memory addresses and lengths where performance counter data will be copied. Software also writes time interval value into a designated register, and the information needed for switching out a given process A, and switching in the process B: allocated memory addresses for process A's hardware performance counter information, and writes the beginning value of that range into a register, e.g., register R1.","At , condition is checked if operating system switch needs to be performed. This can be initiated by receiving an external signal to start operating system switch, or the operating system or the like may write in another register (e.g., register R2) to indicate that copying from and to performance counters to the memory should begin. For instance, the operating system or the like writes \u201c1\u201d to R2.","At , if no OS switch needs to be performed, hardware transfers the value into a timer register. At , the timer register counts down the time interval value, and when the timer count reaches zero, notifies a state machine. Any other method of detecting expiration of the timer value may be utilized. At , the state machine triggers copying of all or selected performance counter register values to specified address in memory. At , hardware copies performance counters to the memory.","At , hardware checks if the configuration of performance counters needs to be changed, by checking a value in another register. If the configuration does not need to be changed, the processing returns to . At , a state machine changes the configuration of the performance counter data, and loops back to .","Going back to , operating system may indicate, for example, by storing a value, to begin context switching of the performance counter data, and the control transfers to . At , a state machine begins context switching the performance counter data, and copies the current context\u2014all or some performance counter values, and all or some configuration registers into the memory. At , after values associated with process A are copied out, the values associated with process B are copied into the performance counters and configuration registers from the memory. For instance, the state machine copies data from another specified memory location into the performance counters. After the hardware finishes copying, the hardware resets the value at register R2, for example, to \u201c0\u201d to indicate that the copying is done, which indicates that the hardware has finished the copy. Finally, at , the new configuration consistent with the process B is performed.","At , the software may specify reconfiguring of the performance counters, for example, periodically or every time interval, and the hardware, for instance, the state machine, may switch configuration of the performance counters at the specified periods. The specifying of reconfiguring and the hardware reconfiguring may occur while the operating system thread is in one context in one aspect. In another aspect, the reconfiguration of the performance counters may occur asynchronously to the context switching mechanism.","At , the software may also specify copying of performance counters directly to memory, for instance, periodically or at every specified time interval. For example, the software may write a value in a register that automatically triggers the state machine (hardware) to automatically perform direct copying of the hardware performance counter data to memory without further software intervention. In one aspect, the specifying of copying the performance counter data directly to memory and the hardware automatically performing the copying may occur while an operating system thread is in context. In another aspect, this step may occur asynchronously to the context switching mechanism.","In one aspect, the storage needed for majority of performance count data is centralized, thereby achieving an area reduction. For instance, only a small number of least-significant bits are kept in the local units, thus saving area. This allows each processor to keep a large number of performance counters (e.g., 24 local counters per processor) at low resolution (e.g., 14 bits). To attain higher resolution counts, the local counter unit periodically transfer its counter values (counts) to a central unit. The central unit aggregates the counts into a higher resolution count (e.g., 64 bits). The local counters count a number of events, e.g., up to the local counter capacity. Before the local counter overflow occurs, it transfers its count to the central unit. Thus, no counts are lost in the local counters. The count values may be stored in a memory device such as a single central Static Random Access Memory (SRAM), which provides high bit density. Using this approach, it becomes possible to have multiples of performance counters supported per processor, while still providing for very large (e.g. 64 bit) counter values.","In another aspect, the memory or central SRAM may be used in multiple modes: a distributed mode, where each core or processor on a chip provides a relatively small number of counts (e.g., 24 per processor), as well as a detailed mode, where a single core or processor can provide a much larger number of counts (e.g., 7116).","In yet another aspect, multiple performance counter data counts from multiple performance counters residing in multiple processing modules (e.g., cores and cache modules) may be collected via a single daisy chain bus in a predetermined number of cycles. The predetermined number of cycles depends on the number of performance counters per processing module, the number of processing modules residing on the daisy chain bus, and the number of bits that can be transferred at one time on the daisy chain. In the description herein, the example configuration of the chip supports 24 local counters in each of its 17 cores, 16 local counters in each of its 16 L2 cache units or modules. The daisy chain bus supports 96 bits of data. Other configurations are possible, and the present invention is not limited only to that configuration.","In still yet another aspect, the performance counter modules and monitoring of performance data may be programmed by user software. Counters of the present disclosure may be configured through memory access bus. The hardware modules of the present disclosure are configured as not privileged such that user program may access the counter data and configure the modules. Thus, with the methodology and hardware set up of the present disclosure, it is not necessary to perform kernel-level operations such as system calls when configuring and gathering performance counts, which can be costly. Rather, the counters are under direct user control.","Still yet in another aspect, the performance counters and associated modules are physically placed near the cores or processing units to minimize overhead and data travel distance and to provide low-latency control and configuration of the counters by the unit to which the counters are associated.","FIG. -- is a high level diagram illustrating performance counter structure of the present disclosure in one embodiment. It depicts a single chip that includes several processor modules, as well as several L2 slice modules. The processor modules each have an associated counter logic unit, referred to as the UPC_P. The UPC_P gathers and aggregates event information from the processor to which it is attached. Similarly, the UPC_L2 module performs the equivalent function for the L2 Slice. In the figure, the UPC_P and UPC_L2 modules are all attached to a single daisy-chain bus structure. Each UPC_P\/L2 module periodically sends count information to the UPC_C unit via this bus.","A processing node may have multiple processors or cores and associated L1 cache units, L2 cache units, a messaging or network unit, and I\/O interfaces such as PCI Express. The performance counters of the present disclosure allow the gathering of performance data from such functions of a processing node and may present the performance data to software. A processing node  also referred to as a chip herein such as an application-specific integrated circuit (ASIC) may include (but not limited to) a plurality of cores (, , ) with associated L1 cache prefetchers (L1P). The processing node may also include (but not limited to) a plurality of L2 cache units (, , ), a messaging\/network unit , PCIe  and Devbus , connecting to a centralized counter unit referred to herein as UPC_C (). A core (e.g., , , ), also referred to herein as a PU (processing unit) may include a performance monitoring unit or a performance counter (, , ) referred to herein as UPC_P. UPC_P resides in the PU complex and gathers performance data from the associated core (e.g., , , ). Similarly, an L2 cache unit (e.g., , , ) may include a performance monitoring unit or a performance counter (e.g., , , ) referred to herein as UPC_L2. UPC_L2 resides in the L2 module and gathers performance data from it. The terminology UPC (universal performance counter) is used in this disclosure synonymously or interchangeable with general performance counter functions.","UPC_C  may be a single, centralized unit within the processing node , and may be responsible for coordinating and maintaining count data from the UPC_P (, , ) and UPC_L2 (, , ) units. The UPC_C unit  (also referred to as the UPC_C module) may be connected to the UPC_P (, , ) and UPC_L2 (, , ) via a daisy chain bus , with the start  and end  of the daisy chain beginning and terminating at the UPC_C . The performance counter modules (i.e., UPC_P, UPC_L2 and UPC_C) of the present disclosure may operate in different modes, and depending on the operating mode, the UPC_C  may inject packet framing information at the start of the daisy chain , enabling the UPC_P (, , ) and\/or UPC_L2 (, , ) modules or units to place data on the daisy chain bus  at the correct time slot. In a similar manner, messaging\/network unit , PCIe  and Devbus  may be connected via another daisy chain bus  to the UPC_C .","The performance counter functionality of the present disclosure may be divided into two types of units, a central unit (UPC_C), and a group of local units. Each of the local units performs a similar function, but may have slight differences to enable it to handle, for example, a different number of counters or different event multiplexing within the local unit. For gathering performance data from the core and associated L1, a processor-local UPC unit (UPC_P) is instantiated within each processor complex. That is, a UPC_P is added to the processing logic. Similarly, there may be a UPC unit associated with each L2 slice (UPC_L2). Each UPC_L2 and UPC_P unit may include a small number of counters. For example, the UPC_P may include 24 14 bit counters, while the UPC_L2 counters may instantiate 16 10 bit counters. The UPC ring (shown as solid line from  to ) may be connected such that each UPC_P (, , ) or UPC_L2 unit (, , ) may be connected to its nearest neighbor. In one aspect, the daisy chain may be implemented using only registers in the UPC units, without extra pipeline latches.","Although not shown or described, a person of ordinary skill in the art will appreciate that a processing node may include other units and\/or elements. The processing node  may be an application-specific integrated circuit (ASIC), or a general-purpose processing node.","The UPC of the present disclosure may operate in different modes, as described below. However, the UPC is not limited to only those modes of operation.","Mode 0 (Distributed Count Mode)","In this operating mode (also referred to as distributed count mode), counts from multiple performance counters residing in each core or processing unit and L2 unit may be captured. For example, in an example implementation of a chip that includes 17 cores each with 24 performance counters, and 16 L2 units each with 16 performance counters, 24 counts from 17 UPC_P units and 16 counts from 16 UPC_L2 units may be simultaneously captured. Local UPC_P and UPC_L2 counters are periodically transferred to a corresponding 64 bit counter residing in the central UPC unit (UPC_C), over a 96 bit daisy chain bus. Partitioning the performance counter logic into local and central units allows for logic reduction, but still maintains 64 bit fidelity of event counts. Each UPC_P or UPC_L2 module places its local counter data on the daisy chain (4 counters at a time), or passes 96 bit data from its neighbor. The design guarantees that all local counters will be transferred to the central unit before they can overflow locally (by guaranteeing a slot on the daisy chain at regular intervals). With a 14 bit local UPC_P counter, each counter is transferred to the central unit at least every 1024 cycles to prevent overflow of the local counters. In order to cover corner cases and minimize the latency of updating the UPC_C counters, each counter is transferred to the central unit every 400 cycles. For Network, DevBus and PCIe, a local UPC unit similar to UPC_L2 and UPC_P may be used for these modules.","Mode 1 (Detailed Count Mode)","In this mode, the UPC_C assists a single UPC_P or UPC_L2 unit in capturing performance data. More events can be captured in the mode from a single processor (or core) or L2 than can be captured in distributed count mode. However, only one UPC_P or UPC_L2 may be examined at a time.","The UPC_P and UPC_L2 modules may be connected to the UPC_C unit via a 96 bit daisy chain, using a packet based protocol. Each UPC operating mode may use a different protocol. For example, in Mode 0 or distributed mode, each UPC_P and\/or UPC_L2 places its data on the daisy chain bus at a specific time (e.g., cycle or cycles). In this mode, the UPC_C transmits framing information on the upper bits (bits 64:95) of the daisy chain. Each UPC_P and\/or UPC_L2 module uses this information to place its data on the daisy chain at the correct time. The UPC_P and UPC_L2 send their counter data in a packet on bits 0:63 of the performance daisy chain. Bits 64:95 are generated by the UPC_C module, and passed unchanged by the UPC_P and\/or UPC_L2 module. Table 1-2 defines example packets sent by UPC_P. Table 1-3 defines example packets sent by UPC_L2. Table 1-4 shows framing information injected by the UPC_C. The packet formats and framing information may be pre-programmed or hard-coded in the logic of the processing.",{"@attributes":{"id":"p-1181","num":"1307"},"tables":{"@attributes":{"id":"TABLE-US-00008","num":"00008"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 1-2"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"UPC_P Daisy Chain Packet Format"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"6"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Cycle","Bit 0:15","Bits 16:31","Bits 32:47","Bits 48:63","Bits 64:95"]},{"entry":{"@attributes":{"namest":"1","nameend":"6","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"6"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["0","Counter 0","Counter 1","Counter 2","Counter 3","Passed"]},{"entry":[{},{},{},{},{},"Unchanged"]},{"entry":["1","Don't Care","Don't Care","Don't Care ","Don't Care","Passed"]},{"entry":[{},{},{},{},{},"Unchanged"]},{"entry":["2","Counter 4","Counter 5","Counter 6","Counter 7","Passed"]},{"entry":[{},{},{},{},{},"Unchanged"]},{"entry":["3","Don't Care","Don't Care","Don't Care ","Don't Care","Passed"]},{"entry":[{},{},{},{},{},"Unchanged"]},{"entry":["4","Counter 8","Counter 9","Counter 10","Counter 11","Passed"]},{"entry":[{},{},{},{},{},"Unchanged"]},{"entry":["5","Don't Care","Don't Care","Don't Care ","Don't Care","Passed"]},{"entry":[{},{},{},{},{},"Unchanged"]},{"entry":["6","Counter 12","Counter 13 ","Counter 14","Counter 15","Passed"]},{"entry":[{},{},{},{},{},"Unchanged"]},{"entry":["7","Don't Care","Don't Care","Don't Care ","Don't Care","Passed"]},{"entry":[{},{},{},{},{},"Unchanged"]},{"entry":["8","Counter 16","Counter 17 ","Counter 18","Counter 19","Passed"]},{"entry":[{},{},{},{},{},"Unchanged"]},{"entry":["9","Don't Care","Don't Care","Don't Care ","Don't Care","Passed"]},{"entry":[{},{},{},{},{},"Unchanged"]},{"entry":["10","Counter 20","Counter 21 ","Counter 22","Counter 23","Passed"]},{"entry":[{},{},{},{},{},"Unchanged"]},{"entry":["11","Don't Care","Don't Care","Don't Care ","Don't Care","Passed"]},{"entry":[{},{},{},{},{},"Unchanged"]},{"entry":["12","Don't Care","Don't Care","Don't Care ","Don't Care","Passed"]},{"entry":[{},{},{},{},{},"Unchanged"]},{"entry":["13","Don't Care","Don't Care","Don't Care ","Don't Care","Passed"]},{"entry":[{},{},{},{},{},"Unchanged"]},{"entry":["14","Don't Care","Don't Care","Don't Care ","Don't Care","Passed"]},{"entry":[{},{},{},{},{},"Unchanged"]},{"entry":["15","Don't Care","Don't Care","Don't Care ","Don't Care","Passed"]},{"entry":[{},{},{},{},{},"Unchanged"]},{"entry":{"@attributes":{"namest":"1","nameend":"6","align":"center","rowsep":"1"}}}]}}]}}},"Table 1-2 defines example packets sent by an UPC_P. Each UPC_P may follow this format. Thus, the next UPC_P may send packets on the next 16 cycles, i.e., 16-31. The next UPC_P may send packets on the next 16 cycles, i.e., 32-47, and so forth. Table 1-5 shows an example of cycle to performance counter unit mappings.","Similar to UPC_P, the UPC_L2 may place data from its counters (e.g., 16 counters) on the daisy chain in an 8-flit packet, on daisy chain bits 0:63. This is shown in Table 1-3.","Table 1-3. UPC_L2 Daisy Chain Packet Format",{"@attributes":{"id":"p-1185","num":"1311"},"tables":{"@attributes":{"id":"TABLE-US-00009","num":"00009"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 1-3"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"UPC_L2 Daisy Chain Packet Format"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"6"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Cycle ","Bit 0:15","Bits 16:31","Bits 32:47 ","Bits 48:63","Bits 64:95"]},{"entry":{"@attributes":{"namest":"1","nameend":"6","align":"center","rowsep":"1"}}},{"entry":["0","Counter 0","Counter 1 ","Counter 2","Counter 3","Passed"]},{"entry":[{},{},{},{},{},"Unchanged"]},{"entry":["1","Don't Care","Don't Care","Don't Care","Don't Care","Passed"]},{"entry":[{},{},{},{},{},"Unchanged"]},{"entry":["2","Counter 4","Counter 5 ","Counter 6","Counter 7","Passed"]},{"entry":[{},{},{},{},{},"Unchanged"]},{"entry":["3","Don't Care","Don't Care","Don't Care","Don't Care","Passed"]},{"entry":[{},{},{},{},{},"Unchanged"]},{"entry":["4","Counter 8","Counter 9 ","Counter 10","Counter 11","Passed"]},{"entry":[{},{},{},{},{},"Unchanged"]},{"entry":["5","Don't Care","Don't Care","Don't Care","Don't Care","Passed"]},{"entry":[{},{},{},{},{},"Unchanged"]},{"entry":["6","Counter 12","Counter 13","Counter 14","Counter 15","Passed"]},{"entry":[{},{},{},{},{},"Unchanged"]},{"entry":["7","Don't Care","Don't Care","Don't Care","Don't Care","Passed"]},{"entry":[{},{},{},{},{},"Unchanged"]},{"entry":{"@attributes":{"namest":"1","nameend":"6","align":"center","rowsep":"1"}}}]}}]}}},"Table 1-4 shows the framing information transmitted by the UPC_C in Mode 0.",{"@attributes":{"id":"p-1187","num":"1313"},"tables":{"@attributes":{"id":"TABLE-US-00010","num":"00010"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 1-4"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"UPC_C Daisy Chain Packet Format, bits 64:95"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Bits","Function"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}},{"entry":["64:72 ","Daisy Chain Cycle Count (0-399)"]},{"entry":["73","\u20180\u2019 -- unused"]},{"entry":["74:81","counter_arm_q(0 to 7) \u2212"]},{"entry":[{},"counter address (four counters at a time) for overflow indication"]},{"entry":["82:85","counter_arm_q(8 to 11) \u2212"]},{"entry":[{},"mask bit for each adder slice, e.g. 4 counters per sram location"]},{"entry":["86:93","(others => \u20180\u2019)"]},{"entry":["94","upc_pu_ctl_q(0) \u2212 turns on run bit in upc_p"]},{"entry":["95","upc_pu_ctl_q(1) \u2212 clock gate for ring"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}]}}},"In this example format of both the UPC_P and UPC_L2 packet formats, every other flit contains no data. Flit refers to one cycle worth of information. The UPC_C uses these \u201cdead\u201d cycles to service memory-mapped I\/O (MMIO) requests to the Static Random Access Memory (SRAM) counters or the like.","The UPC_L2 and UPC_P modules monitor the framing information produced by the UPC_C. The UPC_C transmits a repeating cycle count, ranging from 0 to 399 decimal. Each UPC_P and UPC_L2 compares this count to a value based on its logical unit number, and injects its packet onto the daisy chain when the cycle count matches the value for the given unit. The values compared by each unit are shown in Table 1-5.",{"@attributes":{"id":"p-1190","num":"1316"},"tables":{"@attributes":{"id":"TABLE-US-00011","num":"00011"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 1-5"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Cycle each unit places data on daisy chain, Mode 0"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"6"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"5","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"28pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Cycle","Cycle",{},"Cycle","Cycle"]},{"entry":["UPC_P","Injected","Injected","UPC_L2","Injected","Injected"]},{"entry":["ID","(decimal)","(hex)","ID","(decimal)","(hex)"]},{"entry":{"@attributes":{"namest":"1","nameend":"6","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"6"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"5","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"28pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["PU_0","0","9'h000","L2_0","272","9'h110"]},{"entry":["PU_1","16","9'h010","L2_1","280","9'h118"]},{"entry":["PU_2","32","9'h020","L2_2","288","9'h120"]},{"entry":["PU_3","48","9'h030","L2_3","296","9'h128"]},{"entry":["PU_4","64","9'h040","L2_4","304","9'h130"]},{"entry":["PU_5","80","9'h050","L2_5","312","9'h138"]},{"entry":["PU_6","96","9'h060","L2_6","320","9'h140"]},{"entry":["PU_7","112","9'h070","L2_7","328","9'h148"]},{"entry":["PU_8","128","9'h080","L2_8","336","9'h150"]},{"entry":["PU_9","144","9'h090","L2_9","344","9'h158"]},{"entry":["PU_10","160","9'h0A0","L2_10","352","9'h160"]},{"entry":["PU_11","176","9'h0B0","L2_11","360","9'h168"]},{"entry":["PU_12","192","9'h0C0","L2_12","368","9'h170"]},{"entry":["PU_13","208","9'h0D0","L2_13","376","9'h178"]},{"entry":["PU_14","224","9'h0E0","L2_14","384","9'h180"]},{"entry":["PU_15","240","9'h0F0","L2_15","392","9'h188"]},{"entry":["PU_16","256","9'h100"]},{"entry":{"@attributes":{"namest":"1","nameend":"6","align":"center","rowsep":"1"}}}]}}]}},"br":{}},"In Mode 0 (also referred to as distributed count mode), each UPC_P and UPC_L2 may contribute counter data. It may be desirable to have the local units start and stop counting on the same cycle. To accommodate this, the UPC_C sends a counter start\/stop bit on the daisy chain. Each unit can be programmed to use this signal to enable or disable their local counters. Since each unit is on a different position on the daisy chain, each unit delays a different number of cycles, depending on their position in the daisy chain, before responding to the counter start\/stop command from the UPC_C. This delay value may be hard coded into each UPC_P\/UPC_L2 instantiation.","Mode 1 UPC_P, UPC_L2 Daisy Chain Protocol","As described above, Mode 1 (also referred to as detailed count mode) may be used to allow more counters per processor or L2 than what the local counters provide. In this mode, a given UPC_P or UPC_L2 is selected for ownership of the daisy chain. The selected UPC_P or UPC_L2 sends 92 bits of real time performance event data to the UPC_C for counting. In addition, the local counters are transferred to the UPC_C as in Mode 0. One daisy chain wire can be used to transmit information from all the performance counters in the processor, e.g., all 24 performance counters. The majority of the remaining wires can be used to transfer events to the UPC_C for counting. The local counters may be used in this mode to count any event presented to it. Also, all local counters may by used for instruction decoding. In Mode 1 92 events may be selected for counting by the UPC_C unit. 1 bit of the daisy chain is used to periodically transfer the local counters to the UPC_C, while 92 bits are used to transfer events. The three remaining bits are used to send control information and power gating signals to the local units. The UPC_C sends a rotating count from 0-399 on daisy chain bits 64:72, identically to Mode 0. The UPC_P or UPC_L2 that is selected for Mode 1 places it's local counters on bits 0:63 in a similar fashion as Mode 0, e,g. when the local unit decodes a certain value of the ring counter.","Examples of the data sent by the UPC_P are shown in Table 1-6. UPC_L2 may function similarly, for example, with 32 different types of events being supplied. The specified bits may be turned on to indicate the selected events for which the count is being transmitted. Daisy chain bus bits - specify control information such as the packet start signal on a given cycle.",{"@attributes":{"id":"p-1194","num":"1320"},"tables":{"@attributes":{"id":"TABLE-US-00012","num":"00012"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 1-6"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"UPC_P Mode 1 Daisy Chain Packet Definition"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"168pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Bit",{}]},{"entry":[{},"Field","Function"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"168pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"0:7","UPC_P Mode 1 Event Group 0 (8 events)"]},{"entry":[{},"\u20028:15 ","UPC_P Mode 1 Event Group 1 (8 events)"]},{"entry":[{},"16:23","UPC_P Mode 1 Event Group 2 (8 events)"]},{"entry":[{},"24:31","UPC_P Mode 1 Event Group 3 (8 events)"]},{"entry":[{},"32:39","UPC_P Mode 1 Event Group 4 (8 events)"]},{"entry":[{},"40:47","UPC_P Mode 1 Event Group 5 (8 events)"]},{"entry":[{},"48:55","UPC_P Mode 1 Event Group 6 (8 events)"]},{"entry":[{},"56:63","UPC_P Mode 1 Event Group 7 (8 events)"]},{"entry":[{},"64:70","UPC_P Mode 1 Event Group 8 (7 events)"]},{"entry":[{},"71:77","UPC_P Mode 1 Event Group 9 (7 events)"]},{"entry":[{},"78:84","UPC_P Mode 1 Event Group 10 (7 events)"]},{"entry":[{},"85:91","UPC_P Mode 1 Event Group 11 (7 events)"]},{"entry":[{},"92:95","Local Counter Data"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}]}}},"FIG. -- illustrates a structure of the UPC_P unit or module in one embodiment of the present disclosure. The UPC_P module may be tightly coupled to the core  which may also include L1 prefetcher module or functionality. It gathers performance and trace data from the core  and presents it to the UPC_C via the daisy chain bus  for further processing.","The UPC_P module may use the x1 and x2 clocks. It may expect the x1 and x2 clocks to be phase-aligned, removing the need for synchronization of x1 signals into the x2 domain.","UPC_P Modes","As described above, the UPC_P module  may operate in distributed count mode or detailed count mode. In distributed count mode (Mode 0), a UPC_P module  may monitor performance events, for example 24 performance events from its 24 performance counters. The daisy chain bus is time multiplexed so that each UPC_P module sends its information to the UPC_C in turn. In this mode, the user may count 24 events per core, for example.","In Mode 1 (detailed count mode), one UPC_P module may be selected for ownership of the daisy chain bus. Data may be combined from the various inputs (core performance bus, core trace bus, L1P events), formatted and sent to the UPC_C unit each cycle. As shown in FIG. -- the UPC_C unit  may decode the information provided on the daisy chain bus into as many as 116 (92 wires for raw events and 24 for local counters) separate events to be counted from the selected core or processor complex. For the raw events, the UPC_C module manages the low order bits of the count data, similar to the way that the UPC_P module manages its local counts.","Edge\/Level\/Polarity module  may convert level signals emanating from the core's Performance bus  into single cycle pulses suitable for counting. Each performance bit has a configurable polarity invert, and edge filter enable bit, available via a configuration register.","Widen module  converts signals from one clock domain into another. For example, the core's Performance , Trace , and Trigger  busses all may run at clkx1 rate, and are transitioned to the clkx2 domain before being processed by the UPC_P. Widen module  performs that conversion, translating each clkx1clock domain signal into 2 clkx2 signals (even and odd). This module is optional, and may be used if the rate at which events are output are different (e.g., faster or slower) than the rate at which events are accumulated at the performance counters.","QPU Decode module  and execution unit (XU) Decode module  take the incoming opcode stream from the trace bus, and decode it into groups of instructions. In one aspect, this module resides in the clkx2 domain, and there may be two opcodes (even and odd) of each type (XU and QPU) to be decoded per clkx2 cycle. To accomplish this, two QPU and two XU decode units may be instantiated. This applies to implementations where the core  operates at twice the speed, i.e., outputs 2 events, per operating cycle of the performance counters, as explained above. The 2 events saved by the widen module  may be processed at the two QPU and two XU decode units. The decoded instruction stream is then sent to the counter blocks for selection and counting.","Registers module  implements the interface to the MMIO bus. This module may include the global MMIO configuration registers and provide the support logic (readback muxes, partial address decode) for registers located in the UPC_P Counter units. User software may program the performance counter functions of the present disclosure via the MMIO bus.","Thread Combine module  may combine identical events from each thread, counts them, and present a value for accumulation by a single counter. Thread Combine module  may conserve counters when aggregate information across all threads is needed. Rather than using four counters (or number of counters for each thread), and summing in software, summing across all threads may be done in hardware using this module. Counters may be selected to support thread combining.","The Mode 1 Compress module  may combine event inputs from the core's event bus , the local counters . . . , and the L1 cache prefetch (L1P) event bus , , and place them on the appropriate daisy chain lines for transmission to the UPC_C, using a predetermined packet format, for example, shown in Table 1-6. This module  may divide the 96 bit bus into 12 Event groups, with Event Group 0-7 containing 8 events, and Event Groups 8-11 containing 7 events, for a total of 92 events. Some event group bits can be sourced by several events. Not all events may connect to all event groups. Each event group may have a single multiplexer (mux) control, spanning the bits in the event group.","There may be 24 UPC_P Counter units in each UPC_P module. To minimize muxing, not all counters are connected to all events. Similarly, all counters may be used to count opcodes, but this is not required. Counters may be used to capture a given core's performance event or L1P event.","Referring to FIG. --, a core or processor () may provide performance and trace data via busses. Performance (Event) Bus  may provide information about the internal operation of the core. The bus may be 24 bits wide. The data may include performance data from the core units such as execution unit (XU), instruction unit (IU), floating point unit (FPU), memory management unit (MMU). The core unit may multiplex (mux) the performance events for each unit internally before presenting the data on the 24 bit performance interface. Software may specify the desired performance event to monitor, i.e., program the multiplexing, for example, using a device control register (DCR) or the like. The core  may output the appropriate data on the performance bus  according to the software programmed multiplexing.","Trace (Debug) Bus  may be used to collect the opcode of all committed instructions.","MMIO interface  to allow configuration and interrogation of the UPC_P module by the local core unit ().","UPC_P Outputs","The UPC_P  may include two output interfaces. A UPC_P daisy chain bus , used for transfer of UPC_P data to the UPC_C, and a MMIO bus , used for reading\/writing of configuration and count information from the UPC_P.","UPC_L2 Module","FIG. -- illustrates an example structure of a UPC_L2 module in one embodiment. The UPC_L2 module  is coupled to the L2 slice ; the coupling may be tight. UPC_L2 module  gathers performance data from the L2 slice  and presents it to the UPC_C for further processing. Each UPC_L2  may have 16 dedicated counters (e.g., , , ), each capable of selecting one of two events from the L2 (). For L2 with 32 possible events that can be monitored, either L2 events 0-15 or L2 events - can be counted at any given time. There may be a single select bit that determines whether events 0:15 or events 16:31 are counted. The counters (e.g., , , ) may be configured through MMIO memory access bus to enable selecting of appropriate events for counting.","UPC_L2 Modes","The UPC_L2 module  may operate in distributed count mode (Mode 0) or detailed count mode (Mode 1). In Mode 0, each UPC_L2 module may monitor 16 performance events, on its 16 performance counters. The daisy chain bus is time multiplexed so that each UPC_L2 module sends its information to the UPC_C in turn. In this mode, the user may count 16 events per L2 slice. In Mode 1, one UPC_L2 module is selected for ownership of the daisy chain bus. In this mode, all 32 events supported by the L2 slice may be counted.","UPC_C Module","Referring back to FIG. --, a UPC_C module  may gather information from the PU, L2, and Network Units, and maintain 64 bit counts for each performance event. The UPC_C may contain, for example, a 256D\u00d7264W SRAM, used for storing count and trace information.","The UPC_C module may operate in different modes. In Mode 0, each UPC_P and UPC_L2 contribute 24 and 16 performance events, respectively. In this way, a coarse view of the entire ASIC may be provided. In this mode, the UPC_C Module  sends framing information to the UPC_P and UPC_L2 modules to the UPC_C. This information is used by the UPC_P and UPC_L2 to globally synchronize counter starting\/stopping, and to indicate when each UPC_P or UPC_L2 should place its data on the daisy chain.","In Mode 1, one UPC_L2 module or UPC_P unit is selected for ownership of the daisy chain bus. All 32 events supported by a selected L2 slice may be counted, and up to 116 events can be counted from a selected PU. A set of 92 counters local to the UPC_C, and organized into Central Counter Groups, is used to capture the additional data from the selected UPC_P or UPC_L2.","The UPC_P\/L2 Counter unit  gathers performance data from the UPC_P and UPC_L2 units, while the Network\/DMA\/IO Counter unit  gathers event data from the rest of the ASIC, e.g., input\/output (I\/O) events, network events, direct memory access (DMA) events, etc.","UPC_P\/L2 Counter Unit  is responsible for gathering data from each UPC_P and UPC_L2 unit, and accumulating in it in the appropriate SRAM location. The SRAM is divided into 32 counter groups of 16 counters each. In Mode 0, each counter group is assigned to a particular UPC_P or UPC_L2 unit. The UPC_P unit has 24 counters, and uses two counter groups per UPC_P unit. The last 8 entries in the second counter group is unused by the UPC_P.","The UPC_L2 unit has 16 counters, and fits within a single counter group. For every count data, there may exist an associated location in SRAM for storing the count data.","Software may read or write any counter from SRAM at any time. In one aspect, data is written in 64 bit quantities, and addresses a single counter from a single counter group.","In addition to reading and writing counters, software may cause selected counters of an arbitrary counter group to be added to a second counter group, with the results stored in a third counter group. This may be accomplished by writing to special registers in the UPC_P\/L2 Counter Unit .","FIG. -- illustrates an example structure of the UPC_C Central Unit  in one embodiment of the present disclosure. In Mode 0, the state machine  sends a rotating count on the daisy chain bus upper bits, as previously described. The state machine  fetches from SRAM  or the like, the first location from counter group 0, and waits for the count value associated with Counter 0 to appear on the incoming daisy chain. When the data arrives, it is passed through a 64 bit adder, and stored back to the location from which the SRAM was read. The state machine  then increments the expected count and fetches the next SRAM location. The fetching of data, receiving the current count, adding the current count to the fetched data and writing back to the memory from where the data was fetched is shown by the route drawn in bold line in FIG. --. This process repeats for each incoming packet on the daisy chain bus. Thus, previous count stored in the appropriate location in memory  is read, e.g., and held in holding registers , then added with the incoming count, and written back to the memory , e.g., SRAM. The current count data may be also accessed via registers , allowing software accessibility.","Concurrently with writing the result to memory, the result is checked for a near-overflow. If this condition has occurred, a packet is sent over the daisy chain bus, indicating the SRAM address at which the event occurred, as well as which of the 4 counters in the SRAM has reached near-overflow (each 256 bit SRAM location stores 4 64-bit counters). Note that any combination of the 4 counters in a single SRAM address can reach near-overflow on a given cycle. Because of this, the counter identifier is sent as separate bits (one bit for each counter in a single SRAM address) on the daisy chain. The UPC_P monitors the daisy chain for overflow packets coming from the UPC_C. If the UPC_P detects a near-overflow packet associated with one or more of its counters, it sets an interrupt arming bit for the identified counters. This enables the UPC_P to issue an interrupt to its local processor on the next overflow of the local counter. In this way, interrupts can be delivered to the local processor very quickly after the actual event that caused overflow, typically within a few cycles.","Upon startup the UPC_C sends an enable signal along the daisy chain. A UPC_P\/L2 unit  may use this signal to synchronize the starting and stopping of their local counters. It may also optionally send a reset signal to the UPC_P and UPC_L2, directing them to reset their local counts upon being enabled. The 96 bit daisy chain provides adequate bandwidth to support both detailed count mode and distributed count mode operation.","For operating in detailed count mode, the entire daisy chain bandwidth can be dedicated to a single processor or L2. This greatly increases the amount of information that can be sent from a single UPC_P or UPC_L2, allowing the counting of more events. The UPC_P module receives information from three sources: core unit opcodes received via the trace bus, performance events from the core unit, and events from the L1P. In Mode 1, the bandwidth of the daisy chain is allocated to a single UPC_P or UPC_L2, and used to send more information. Global resources in the UPC_C (The Mode 1 Counter unit) assist in counting performance events, providing a larger overall count capability.","The UPC_P module may contain decode units that provide roughly 50 groups of instructions that can be counted. These decode units may operate on 4 16 bit instructions simultaneously. In one aspect, instead of transferring raw opcode information, which may consume available bandwidth, the UPC_P local counters may be used to collect opcode information. The local counters are periodically transmitted to the UPC_C for aggregation with the SRAM counter, as in Mode 0. However, extra data may be sent to the UPC_C in the Mode 1 daisy chain packet. This information may include event information from the core unit and associated L1 prefetcher. Multiplexers in the UPC_P can select the events to be sent to the UPC_C. This approach may use 1 bit on the daisy chain.","The UPC_C may have 92 local counters, each associated with an event in the Mode 1 daisy chain packet. These counters are combined in SRAM with the local counters in the UPC_P or L2. They are organized into 8-counter central counter groups. In total there may be 116 counters in mode 1, (24 counters for instruction decoding, and 92 for event counting).","The daisy chain input feeds events from the UPC_P or UPC_L2 into the Mode 1 Counter Unit for accumulation, while UPC_P counter information is sent directly to SRAM for accumulation. The protocol for merging the low order bits into the SRAM may be similar to Mode 0.","Each counter in the Mode 1 Counter Unit may correspond to a given event transmitted in the Mode 1 daisy chain packet.","The UPC counters may be started and stopped with fairly low overhead. The UPC_P modules map the controls to start and stop counters into MMIO user space for low-latency access that does not require kernel intervention. In addition, a method to globally start and stop counters synchronously with a single command via the UPC_C may be provided. For local use, each UPC_P unit can act as a separate counter unit (with lower resolution), controlled via local MMIO transactions. For example, the UPC_P Counter Data Registers may provide MMIO access to the local counter values. The UPC_P Counter Control Register may provide local configuration and control of each UPC_P counter.","All events may increment the counter by a value of 1 or more.","Software may communicate with the UPC_C via local Devbus access. In addition, UPC_C Counter Data Registers may give software access to each counter on an individual basis. UPC_C Counter Control Registers may allow software to enable each local counter independently. The UPC units provide the ability to count and report various events via MMIO operations to registers residing in the UPC units, which software may utilize via Performance Application Programming Interface (PAPI) Application Program Interface (API).","A UPC_C Accumulate Control Register may allow software to add counter groups to each other, and place the result in a third counter group. This register may be useful for temporarily storing the added counts, for instance, in case the added counts should not count toward the performance data. An example of such counts would be when a processor executes instructions based on anticipated future execution flow, that is, the execution is speculative. If the anticipated future execution flow results in incorrect or unnecessary execution, the performance counts resulting from those executions should not be counted.","FIGS. --, -- and -- are flow high-level overview diagrams that illustrate a method for distributed performance counters in one embodiment of the present disclosure. Before the steps taken in those figures, a set up of the performance counters may take place. For instance, initial values of counters may be loaded, operating mode (e.g., distributed mode (Mode 0), detailed mode (Mode 1), or trace mode (Mode 2) may be programmed, and events may be selected for counting. Additionally, during the operations of the local and central performance counters of the present disclosure, one or more of those parameters may be reprogrammed, for instance, to change the mode of operation and others. The set up and reprogramming may have been performed by user software writing into appropriate registers as described above.","FIG. -- is a flow diagram illustrating central performance counter unit sending the data on the daisy chain bus. At , a central performance counter unit (e.g., UPC_C described above), for example, its UPC_C sender module or functionality is enabled to begin sending information, for example, framing and near-overflow information where applicable, for example, by software. At , the central performance counter unit sends framing information on a daisy chain connection. The framing information may be placed on upper bits of the connection, e.g., upper 32 bits of a 96 bit bus connection. The framing information may include clock cycle count for indicating to the local performance counter modules (e.g., UPC_P and UPC_L2 described above), which of the local performance counter modules should transfer their data. An example format of the framing information is shown in Table 1-4 above. Other format may be used for controlling the data transfer from the local performance counters. In addition, if it is determined that a near-overflow indication should be sent, the UPC_C also sends the indication. Determination of the near-overflow is made, for instance, by the UPC_C's receiving functionality that checks whether the overflow is about to occur in the SRAM location after aggregating the received data with the SRAM data as will be described below.","FIG. -- is a flow diagram illustrating functions of a local performance counter module (e.g., UPC_P and UPC_L2) receiving and sending data on the daisy chain bus. At , a local performance counter module (e.g., UPC_P or UPC_L2) monitors (or reads) the framing information produced by the central performance counter unit (e.g., UPC_C). At , the local performance counter module compares a value in the framing information to a predetermined value assigned or associated with the local performance counter module. If the values match at , the local performance counter module places its counter data onto the daisy chain . For example, as described above, the UPC_C may transmit a repeating cycle count, ranging from 0 to 399 decimal. Each UPC_P and UPC_L2 compares this count to a value based on its logical unit number, and injects its packet onto the daisy chain when the cycle count matches the value for the given unit. Example values compared by each unit are shown in Table 1-5. Other values may be used for this functionality. If, on the other hand, there is no match at , the module returns to . At , the local counter data is cleared. In one aspect, UPC_P may clear only the upper bit of the performance counter, leaving the lower bits intact.","At the same time or substantially the same time, the local performance counter module also monitors for near-overflow interrupt from the UPC_C at . If there is an interrupt, the local performance counter module may retrieve the information associated with the interrupt from the daisy chain bus and determine whether the interrupt is for any one of its performance counters. For example, the SRAM location specified on the daisy chain associated with the interrupt is checked to determine whether that location is where the data of its performance counters are stored. If the interrupt is for any one of its performance counters, the local performance counter module arms the counter to handle the near-overflow. If a subsequent overflow of the counter in UPC_P or UPC_L2 occurs, the UPC_P or UPC_L2 may optionally freeze the bits in the specified performance counter, as well as generate an interrupt.","FIG. -- is a flow diagram illustrating the UPC_C receiving the data on the daisy chain bus. At , the central performance counter module (e.g., UPC_C) reads the previously stored count data (e.g., in SRAM) associated with the performance counter whose count data is incoming on the daisy chain bus. At , the central performance counter module receives the incoming counter data (e.g., the data injected by the local performance counters), and at , adds the counter data to the corresponding appropriate count read from the SRAM. At , the aggregated count data is stored in its appropriate addressable memory, e.g., SRAM. At , the central performance counter module also may check whether an overflow is about to occur in the received counter data and notifies or flags to send a near-overflow interrupt and associated information on the daisy chain bus, specifying the appropriate performance counter module for example, by its storage location or address in the memory (SRAM). At , the central performance counter module updates the framing information, for example, increments the cycle count, and sends the updated framing information on the daisy chain to repeat the processing at . Interrupt handling is described, for example, in U.S. Patent Publication No. 2008\/0046700 filed Aug. 21, 2006 and entitled \u201cMethod and Apparatus for Efficient Performance Monitoring of a Large Number of Simultaneous Events\u201d, which is incorporate herein in its entirety by reference thereto.","Miscellaneous Memory-Mapped Devices","All other devices accessed by the core or requiring direct memory access are connected via the device bus unit (DEVBUS) to the crossbar switch. The PCI express interface unit uses this path to enable PCIe devices to DMA data into main memory via the L2-caches. The DEVBUS switches requests from its slave port also to the boot eDRAM, an on-chip memory used for boot, RAS messaging and control-system background communication. Other units accessible via DEVBUS include the universal performance counter unit (UPC), the interrupt controller (BIC), the test controller\/interface (TESTINT) as well as the global L2 state controller (L2-central).  illustrates in more detail memory mapped devices according to one embodiment.","Generally, hardware performance counters are extra logic added to the central processing unit (CPU) to track low-level operations or events within the processor. For example, there are counter events that are associated with the cache hierarchy that indicate how many misses have occurred at L1, L2, and the like. Other counter events indicate the number of instructions completed, number of floating point instructions executed, translation lookaside buffer (TLB) misses, and others. A typical computing system provides a small number of counters dedicated to collecting and\/or recording performance events for each processor in the system. These counters consume significant logic area, and cause high-power dissipation. As such, only a few counters are typically provided. Current computer architecture allows many processors or cores to be incorporated into a single chip. Having only a handful of performance counters per processor does not provide the ability to count several events simultaneously from each processor.","Thus, in a further embodiment, there is provided a distributed trace device, that, in one aspect, may include a plurality of processing cores, a central storage unit having at least memory, and a daisy chain connection connecting the central storage unit and the plurality of processing cores and forming a daisy chain ring layout. At least one of the plurality of processing cores places trace data on the daisy chain connection for transmitting the trace data to the central storage unit. The central storage unit detects the trace data and stores the trace data in the memory.","Further, there is provided a method for distributed trace using central memory, that, in one aspect, may include connecting a plurality of processing cores and a central storage unit having at least memory using a daisy chain connection, the plurality of processing cores and the central storage unit being formed in a daisy chain ring layout. The method also may include enabling at least one of the plurality of processing cores to place trace data on the daisy chain connection for transmitting the trace data to the central storage unit. The method further may include enabling the central storage unit to detect the trace data and store the trace data in the memory.","Further, a method for distributed trace using central performance counter memory, in one aspect, may include placing trace data on a daisy chain bus connecting the processing core and a plurality of second processing cores to a central storage unit on an integrated chip. The method further may include reading the trace data from the daisy chain bus and storing the trace data in memory.","A centralized memory is used to store trace information from a processing core, for instance, in an integrated chip having a plurality of cores. Briefly, trace refers to signals or information associated with activities or internal operations of a processing core. Trace may be analyzed to determine the behavior or operations of the processing core from which the trace was obtained. In addition to a plurality of cores, each of the cores also referred to as local core, the integrated chip may include a centralized storage for storing the trace data and\/or performance count data.","Each processor or core may keep a number of performance counters (e.g., 24 local counters per processor) at low resolution (e.g., 14 bits) local to it, and periodically transfer these counter values (counts) to a central unit. The central unit aggregates the counts into a higher resolution count (e.g., 64 bits). The local counters count a number of events, e.g., up to the local counter capacity, and before the counter overflow occurs, transfer the counts to the central unit. Thus, no counts are lost in the local counters.","The count values may be stored in a memory device such as a single central Static Random Access Memory (SRAM), which provides high bit density. The count values may be stored in a single central Static Random Access Memory (SRAM), which provides high bit density. Using this approach, it becomes possible to have multiples of performance counters supported per processor.","This local-central count storage device structure may be utilized to capture trace data from a single processing core (also interchangeably referred to herein as a processor or a core) residing in an integrated chip. In this way, for example, 1536 cycles of 44 bit trace information may be captured into an SRAM, for example, 256\u00d7256 bit SRAM. Capture may be controlled via trigger bits supplied by the processing core.","FIG. -- is a high level diagram illustrating performance counter structure of the present disclosure in one embodiment, which may be used to gather trace data. The structure illustrated in  is shown as an example only. Different structures are possible and the method and system disclosed herein is not only limited to the particular structural configuration shown. Generally, a processing node may have multiple processors or cores and associated L1 cache units, L2 cache units, a messaging or network unit, and PCIe\/Devbus. Performance counters allow the gathering of performance data from such functions of a processing node and may present the performance data to software. Referring to FIG. --, a processing node  also referred to as an integrated chip herein such as an application-specific integrated circuit (ASIC) may include (but not limited to) a plurality of cores (, , ). The plurality of cores (, , ) may also have associated L1 cache prefetchers (L1P). The processing node may also include (but not limited to) a plurality of L2 cache units (, , ), a messaging\/network unit , PCIe , and Devbus , connecting to a centralized counter unit referred to herein as UPC_C (). In the figure, the UPC_P and UPC_L2 modules are all attached to a single daisy-chain bus structure . Each UPC_P\/L2 module may sends information to the UPC_C unit via this bus . Although shown in FIG. --, not all components are needed or need to be utilized for performing the distributed trace functionality of the present disclosure. For example, L2 cache units (, , ) need not be involved in gathering the core trace information.","A core (e.g., , , ), which may be also referred to herein as a PU (processing unit) may include a performance monitoring unit or a performance counter (, , ) referred to herein as UPC_P. UPC_P resides in the PU complex (e.g., , , ) and gathers performance data of the associated core (e.g., , , ). The UPC_P may be configured to collect trace data from the associated PU.","Similarly, an L2 cache unit (e.g., , , ) may include a performance monitoring unit or a performance counter (e.g., , , ) referred to herein as UPC_L2. UPC_L2 resides in the L2 and gathers performance data from it. The terminology UPC (universal performance counter) is used in this disclosure synonymously or interchangeable with general performance counter functions.","UPC_C  may be a single, centralized unit within the processing node , and may be responsible for coordinating and maintaining count data from the UPC_P (, , ) and UPC_L2 (, , ) units. The UPC_C unit  (also referred to as the UPC_C module) may be connected to the UPC_P (, , ) and UPC_L2 (, , ) via a daisy chain bus , with the start  and end  of the daisy chain beginning and terminating at the UPC_C . In a similar manner, messaging\/network unit , PCIe  and Devbus  may be connected via another daisy chain bus  to the UPC_C .","The performance counter modules (i.e., UPC_P, UPC_L2 and UPC_C) of the present disclosure may operate in different modes, and depending on the operating mode, the UPC_C  may inject packet framing information at the start of the daisy chain , enabling the UPC_P (, , ) and\/or UPC_L2 (, , ) modules or units to place data on the daisy chain bus at the correct time slot. In distributed trace mode, UPC_C  functions as a central trace buffer.","As mentioned above, the performance counter functionality of the present disclosure may be divided into two types of units, a central unit (UPC_C), and a group of local units. Each of the local units performs a similar function, but may have slight differences to enable it to handle, for example, a different number of counters or different event multiplexing within the local unit. For gathering performance data from the core and associated L1, a processor-local UPC unit (UPC_P) is instantiated within each processor complex. That is, a UPC_P is added to the processing logic. Similarly, there may be a UPC unit associated with each L2 slice (UPC_L2). Each UPC_L2 and UPC_P unit may include a small number of counters. For example, the UPC_P may include 24 14 bit counters, while the UPC_L2 counters may instantiate 16 10 bit counters. The UPC ring (shown as solid line from  to ) may be connected such that each UPC_P (, , ) or UPC_L2 unit (, , ) may be connected to its nearest neighbor. In one aspect, the daisy chain may be implemented using only registers in the UPC units, without extra pipeline latches.","For collecting trace information from a single core (e.g., , , ), the UPC_C  may continuously record the data coming in on the connection, e.g., a daisy chain bus, shown at . In response to detecting one or more trigger bits on the daisy chain bus, the UPC_C  continues to read the data (trace information) on the connection (e.g., the daisy chain bus) and records the data for a programmed number of cycles to the SRAM . Thus, trace information before and after the detection of the trigger bits may be seen and recorded.","The UPC_P and UPC_L2 modules may be connected to the UPC_C unit via a 96 bit daisy chain, using a packet based protocol. In trace mode, the trace data from the core is captured into the central SRAM located in the UPC_C . Bit fields 0:87 may be used for the trace data (e.g., 44 bits per cycle), and bit fields 88:95 may be used for trigger data (e.g., 4 bits per cycle).","FIG. -- illustrates a structure of the UPC_P unit or module in one embodiment of the present disclosure. The UPC_P module  may be tightly coupled to the core  which may also include L1 prefetcher module or functionality. It may gather trace data from the core  and present it to the UPC_C via the daisy chain bus  for further processing.","The UPC_P module may use the x1 and x2 clocks. It may expect the x1 and x2 clocks to be phase-aligned, removing the need for synchronization of x1 signals into the x2 domain. In one aspect, x1 clock may operate twice as fast as x2 clock.","Bits of trace information may be captured from the processing core  and sent across the connection connecting to the UPC_C, for example, the daisy chain bus shown at . For instance, one-half of the 88 bit trace bus from the core (44 bits) may be captured, replicated as the bits pass from different clock domains, and sent across the connection. In addition, 4 of the 16 trigger signals supplied by the core  may be selected at  for transmission to the UPC_C. The UPC_C then may store 1024 clock cycles of trace information into the UPC_C SRAM. The stored trace information may be used for post-processing by software.","Edge\/Level\/Polarity module  may convert level signals emanating from the core's Performance bus  into single cycle pulses suitable for counting. Each performance bit has a configurable polarity invert, and edge filter enable bit, available via a configuration register.","Widen module  converts clock signals. For example, the core's Performance , Trace , and Trigger  busses all may run at clkx1 rate, and are transitioned to the clkx2 domain before being processed. Widen module  performs that conversion, translating each clkx1clock domain signal into 2 clkx2 signals (even and odd). This module is optional, and may be used if the rate at which events are output are different (e.g., faster) than the rate at which events are accumulated at the performance counters.","QPU Decode module  and execution unit (XU) Decode module  take the incoming opcode stream from the trace bus, and decode it into groups of instructions. In one aspect, this module resides in the clkx2 domain, and there may be two opcodes (even and odd) of each type (XU and QPU) to be decoded per clkx2 cycle. To accomplish this, two QPU and two XU decode units may be instantiated. This applies to implementations where the core  operates at twice the speed, i.e., outputs 2 events, per operating cycle of the performance counters, as explained above. The 2 events saved by the widen module  may be processed at the two QPU and two XU decode units. The decoded instruction stream is then sent to the counter blocks for selection and counting.","Registers module  implements the interface to the MMIO bus. This module may include the global MMIO configuration registers and provide the support logic (readback muxes, partial address decode) for registers located in the UPC_P Counter units. User software may program the performance counter functions of the present disclosure via the MMIO bus.","Thread Combine module  may combine identical events from each thread, count them, and present a value for accumulation by a single counter. Thread Combine module  may conserve counters when aggregate information across all threads is needed. Rather than using four counters (or number of counters for each thread), and summing in software, summing across all threads may be done in hardware using this module. Counters may be selected to support thread combining.","The Compress module  may combine event inputs from the core's event bus , the local counters . . . , and the L1 cache prefetch (L1P) event bus , , and place them on the appropriate daisy chain lines for transmission to the UPC_C, using a predetermined packet format.","There may be 24 UPC_P Counter units in each UPC_P module. To minimize muxing, not all counters need be connected to all events. All counters can be used to count opcodes. One counter may be used to capture a given core's performance event or L1P event.","Referring to FIG. --, a core or processor () may provide performance and trace data via busses. Performance (Event) Bus  may provide information about the internal operation of the core. The bus may be 24 bits wide. The data may include performance data from the core units such as execution unit (XU), instruction unit (IU), floating point unit (FPU), memory management unit (MMU). The core unit may multiplex (mux) the performance events for each unit internally before presenting the data on the 24 bit performance interface. Software may specify the desired performance event to monitor, i.e., program the multiplexing, for example, using a device control register (DCR) or the like. The software may similarly program for distributed trace. The core  may output the appropriate data on the performance bus  according to the software programmed multiplexing.","Trace (Debug) bus  may be used to send data to the UPC_C for capture into SRAM. In this way, the SRAM is used as a trace buffer. In one aspect, the core whose trace information is being sent over the connection (e.g., the daisy chain bus) to the UPC_C may be configured to output trace data appropriate for the events being counted.","Trigger bus  from the core may be used to stop and start the capture of trace data in the UPC_C SRAM. The user may send, for example, 4 to 16 possible trigger events presented by the core to the UPC for SRAM start\/stop control.","MMIO interface  may allow configuration and interrogation of the UPC_P module by the local core unit ().","The UPC_P  may include two output interfaces. A UPC_P daisy chain bus , used for transfer of UPC_P data to the UPC_C, and a MMIO bus , used for reading\/writing of configuration and count information from the UPC_P.","Referring back to FIG. --, a UPC_C module  may gather information from the PU, L2, and Network Units, and maintain 64 bit counts for each performance event. The UPC_C may contain, for example, a 256D\u00d7264W SRAM, used for storing count and trace information.","The UPC_C module may operate in different modes. In trace mode, the UPC_C acts as a trace buffer, and can trace a predetermined number of cycles of a predetermined number of bit trace information from a core. For instance, the UPC_C may trace 1536 cycles of 44 bit trace information from a single core.","The UPC_P\/L2 Counter unit  gathers performance data from the UPC_P and\/or UPC_L2 units, while the Network\/DMA\/IO Counter unit  gathers event data from the rest of the ASIC, e.g., input\/output (I\/O) events, network events, direct memory access (DMA) events, etc.","UPC_P\/L2 Counter Unit  may accumulate the trace data received from a UPC_P in the appropriate SRAM location. The SRAM is divided into a predetermined number of counter groups of predetermined counters each, for example, 32 counter groups of 16 counters each. For every count data or trace data, there may exist an associated location in SRAM for storing the count data.","Software may read or write any counter from SRAM at any time. In one aspect, data is written in 64 bit quantities, and addresses a single counter from a single counter group.","FIG. -- illustrates an example structure of the UPC_C  in one embodiment of the present disclosure. The SRAM  is used to capture the trace data. For instance, 88 bits of trace data may be presented by the UPC_P\/L2 Counter units to the UPC_C each cycle. In one embodiment, the SRAM may hold 3 88 bit words per SRAM entry, for example, for a total of 256\u00d73\u00d72=1536 cycles of 44 bit data. The UPC_C may gather multiple cycles of data from the daisy chain, and store them in a single SRAM address. The data may be stored in consecutive locations in SRAM in ascending bit order. Other dimensions of the SRAM  and order of storage may be possible. Most of the data in the SRAM  may be accessed via the UPC_C counter data registers (e.g., 7608). The remaining data (e.g., 8 bits residue per SRAM address in the above example configuration) may be accessible through dedicated Devbus registers.","The following illustrates the functionality of UPC_C in capturing and centrally storing trace data from one or more of the processor connected on the daisy chain bus in one embodiment of the present disclosure.","1) UPC_C is programmed with the number of cycles to capture after a trigger is detected.","2) UPC_C is enabled to capture data from the ring (e.g., daisy chain bus  of FIG. --). It starts writing data from the ring into the SRAM. For example, each SRAM address may hold 3 cycles of daisy chain data (88\u00d73)=264. SRAM of the UPC_C may be 288 bits wide, so there may be a few bits to spare. In this example, 6 trigger bits (a predetermined number of bits) may be stored in the remaining 24 bits (6 bits of trigger per daisy chain cycle). That is 3 cycles of daisy chain per SRAM location.\n\n3) UPC_C receives a trigger signal from ring (sent by UPC_P). UPC_C stores the address that UPC_C was writing to when the trigger occurred. This for example allows software to know where in the circular SRAM buffer the trigger happened.\n\n4) UPC_C then continues to capture until the number of cycles in step  has expired. UPC_C then stops capture and may return to an idle state. Software may read a status register to see that capture is complete. The software may then reads out the SRAM contents to get the trace.\n","The following illustrates the functionality of UPC_P in distributed tracing of the present disclosure in one embodiment.","1) UPC_P is configured to send bits from a processor (or core), for example, either upper or lower 44 bits from processor, to UPC_C. (e.g., set mode 2, enable UPC_P, set up event muxes).","2) In an implementation where the processor operates at a faster (e.g., twice as fast) than the rest of the performance counter components, UPC_P takes two x1 cycles of 44 bit data and widens it to 88 bits at \u00bd processor rate.","3) UPC_P places this data, along with trigger data sourced from the processor, or from an MMIO store to a register residing in the UPC_P or UPC_L2, on the daisy chain. For example, 88 bits are used for data, and 6 bits of trigger are passed.","FIG. -- is a flow diagram illustrating an overview method for distributed trace in one embodiment of the present disclosure. At , the devices or units (for example, shown in FIG. --) are configured to perform the tracing. For instance, the devices may have been running in different operating capabilities, for example, collecting the performance data. The configuring to run in trace mode or such operating capability may be done by the software writing into one of the registers, for example, via the MMIO bus of a selected processing core whose trace data is to be acquired. Configuring at  starts the UPC_C to start capturing the trace data on the daisy chain bus.","At , the central counter unit detects the stop trigger on the daisy chain bus. Depending on programming, the central counter unit may operate differently. For example, in one embodiment, in response to detecting the stop trigger signal on the daisy chain bus, the central counter unit may continue to read and store the trace data from the daisy chain bus for predetermined number cycles after the detecting of the stop trigger signal. In another embodiment, the central counter unit may stop reading and storing the trace data in response to detecting the stop trigger signal. Thus, the behavior of the central counter unit may be programmable. The programming may be done by the software, for instance, writing on an appropriate register associated with the central counter unit. In another embodiment, the programming may be done by the software, for instance, writing on an appropriate register associated with the local processing core, and the local processing core may pass this information to the central unit via the daisy chain bus.","The store trace data on the SRAM may be read or otherwise accessible to the user, for example, via the user software. In one aspect, the hardware devices of the present disclosure allow the user software to directly access its data. No kernel system call may be needed to access the trace data, thus reducing the overhead needed to run the kernel or system calls.","The trigger may be sent by the processing cores or by software. For example, software or user program may write to an MMIO location to send the trigger bits on the daisy chain bus to the UPC_C. Trigger bits may also be pulled from the processing core bus and sent out on the daisy chain bus. The core sending out the trace information continues to place its trace data on the daisy chain bus and the central counter unit continuously reads the data on the daisy chain bus and stores the data in memory.","System Packaging","Each compute rack contains 2 midplanes, and each midplane contains 512 16-way PowerPC A2 compute processors, each on a compute ASIC Midplanes are arranged vertically in the rack, one above the other, and are accessed from the front and rear of the rack. Each midplane has its own bulk power supply and line cord. These same racks also house I\/O boards. Each passive compute midplane contains 16 node boards, each with 32 compute ASICs and 9 Blue Gene\/Q Link ASICs, and a service card that provides clocks, a control buss, and power management. An I\/O midplane may be formed with 16 I\/O boards replacing the 16 node boards. An I\/O board contains 8 compute ASICs, 8 link chips, and 8 PCI2 2.0 adapter card slots.","The midplane, the service card, the node (or I\/O) boards, as well as the compute, and direct current assembly (DCA) cards that plug into the I\/O and node boards are described here. The BQC chips are mounted singly, on small cards with up to 72 (36) associated SDRAM-DDR3 memory devices (in the preferred embodiment, 64 (32) chips of 2Gb SDRAM constitute a 16 (8) GB node, with the remaining 8 (4) SDRAM chips for chipkill implementation.) Each node board contains 32 of these cards connected in a 5 dimensional array of length 2 (2^5=32). The fifth dimension exists only on the node board, connecting pairs of processor chips. The other dimensions are used to electrically connect 16 node boards through a common midplane forming a 4 dimensional array of length 4; a midplane is thus 4^4\u00d72=512 nodes. Working together, 128 link chips in a midplane extend the 4 midplane dimensions via optical cables, allowing midplanes to be connected together. The link chips can also be used to space partition the machine into sub-tori partitions; a partition is associated with at least one I\/O node and only one user program is allowed to operate per partition. The 10 torus directions are referred to as the +\/\u2212a, +\/\u2212b, +\/\u2212c, +\/\u2212d, +\/\u2212e dimensions. The electrical signaling rate is 4Gb\/s and a torus port is 4 bits wide per direction, for an aggregate bandwidth of 2 GB\/s per port per direction. The 5-dimensional torus links are bidirectional. We have the raw aggregate link bandwidth of 2 GB\/s* 2*10=40 GB\/s. The raw hardware Bytes\/s:FLOP\/s is thus 40:204.8=0.195. The link chips double the electrical datarate to 8Gb\/s, add a layer of encoding (8b\/10b+parity), and drive directly the Tx and Rx optical modules at 10 GB\/s. Each port has 2 fibers for send and 2 for receive. The Tx+Rx modules handle 12+12 fibers, or 4 uni-directional ports, per pair, including spare fibers. Hardware and software work together to seamlessly change from a failed optical fiber link, to a spare optical fiber link, without application fail.","The BQC ASIC contains a PCIe 2.0 port of width 8 (8 lanes). This port, which cannot be subdivided, can send and receive data at 4 GB\/s (8\/10 encoded to 5 GB\/s). It shares pins with the fifth (+\/\u2212e) torus ports. Single node compute cards can become single node I\/O cards by enabling this adapter card port. Supported adapter cards include IB-QDR and dual 10 Gb Ethernet. Compute nodes communicate to I\/O nodes over an I\/O port, also 2+2 GB\/s. Two compute nodes, each with an I\/O link to an I\/O node, are needed to fully saturate the PCIe bus. The I\/O port is extended optically, through a 9link chip on a node board, which allows compute nodes to communicate to I\/O nodes on other racks. I\/O nodes in their own racks communicate through their own 3 dimensional tori. This allows for fault tolerance in I\/O nodes in that traffic may be re-directed to another I\/O node, and flexibility in traffic routing in that I\/O nodes associated with one partition may, software allowing, be used by compute nodes in a different partition.","A separate control host distributes at least a single 10Gb\/s Ethernet link (or equivalent bandwidth) to an Ethernet switch which in turn distributes 1 Gb\/s Ethernet to a service card on each midplane. The control systems on BG\/Q and BG\/P are similar. The midplane service card in turn distributes the system clock, provides other rack control function, and consolidates individual 1 Gb Ethernet connections to the node and I\/O boards. On each node board and I\/O board the service bus converts from 1Gb Ethernet to local busses (JTAG, I2C, SPI) through a pair of Field Programmable Gate Array (FPGA) function blocks codenamed iCon and Palimino. The local busses of iCon & Palimino connect to the Link and Compute ASICs, local power supplies, various sensors, for initialization, debug, monitoring, and other access functions.","Bulk power conversion is N+1 redundant. The input is 440V 3phase, with one power supply with one input line cord and thus one bulk power supply per midplane at 48V output. Following the 48V DC stage is a custom N+1 redundant regulator supplying up to 7 different voltages built directly into the node and I\/O boards. Power is brought from the bulk supplies to the node and I\/O boards via cables. Additionally DC-DC converters of modest power are present on the midplane service card, to maintain persistent power even in the event of a node card failure, and to centralize power sourcing of low current voltages. Each BG\/Q circuit card contains an EEPROM with Vital product data (VPD).","From a full system perspective, the supercomputer as a whole is controlled by a Service Node, which is the external computer that controls power-up of the machine, partitioning, boot-up, program load, monitoring, and debug. The Service Node runs the Control System software. The Service Node communicates with the supercomputer via a dedicated, private 1 Gb\/s Ethernet connection, which is distributed via an external Ethernet switch to the Service Cards that control each midplane (half rack). Via an Ethernet switch located on this Service Card, it is further distributed via the Midplane Card to each Node Card and Link Card. On each Service Card, Node Card and Link Card, a branch of this private Ethernet terminates on a programmable control device, implemented as an FPGA (or a connected set of FPGAs).https:\/\/watgsa.ibm.com\/%7Eswetz\/shared\/bgp\/docs\/Palomino.3.0\/Palomino.htmlThe FPGA(s) translate between the Ethernet packets and a variety of serial protocols to communicate with on-card devices: the SPI protocol for power supplies, the IC protocol for thermal sensors and the JTAG protocol for Compute and Link chips.","On each card, the FPGA is therefore the center hub of a star configuration of these serial interfaces. For example, on a Node Card the star configuration comprises 34 JTAG ports (one for each compute or IO node) and a multitude of power supplies and thermal sensors.","Thus, from the perspective of the Control System software and the Service Node, each sensor, power supply or ASIC in the supercomputer system is independently addressable via a standard 1 Gb Ethernet network and IP packets. This mechanism allows the Service Node to have direct access to any device in the system, and is thereby an extremely powerful tool for booting, monitoring and diagnostics. Moreover, the Control System can partition the supercomputer into independent partitions for multiple users. As these control functions flow over an independent, private network that is inaccessible to the users, security is maintained.","In one embodiment, the computer utilizes a 5D torus interconnect network for various types of inter-processor communication. PCIe-2 and low cost switches and RAID systems are used to support locally attached disk storage and host (login nodes). A private 1 Gb Ethernet (coupled locally on card to a variety of serial protocols) is used for control, diagnostics, debug, and some aspects of initialization. Two types of high bandwidth, low latency networks make up the system \u201cfabric\u201d.","System Interconnect\u2014Five Dimensional Torus","The Blue Gene compute ASIC incorporates an integrated 5-D torus network router. There are 11 bidirectional 2 GB\/s raw data rate links in the compute ASIC, 10 for the 5-D torus and 1 for the optional I\/O link. A network messaging unit (MU) implements the prior generation Blue Gene style network DMA functions to allow asynchronous data transfers over the 5-D torus interconnect. MU is logically separated into injection and reception units.","The injection side MU maintains injection FIFO pointers, as well as other hardware resources for putting messages into the 5-D torus network. Injection FIFOs are allocated in main memory and each FIFO contains a number of message descriptors. Each descriptor is 64 bytes in length and includes a network header for routing, the base address and length of the message data to be sent, and other fields like type of packets, etc., for the reception MU at the remote node. A processor core prepares the message descriptors in injection FIFOs and then updates the corresponding injection FIFO pointers in the MU. The injection MU reads the descriptors and message data packetizes messages into network packets and then injects them into the 5-D torus network.","Three types of network packets are supported: (1) Memory FIFO packets; the reception MU writes packets including both network headers and data payload into pre-allocated reception FIFOs in main memory. The MU maintains pointers to each reception FIFO. The received packets are further processed by the cores; (2) Put packets; the reception MU writes the data payload of the network packets into main memory directly, at addresses specified in network headers. The MU updates a message byte count after each packet is received. Processor cores are not involved in data movement, and only have to check that the expected numbers of bytes are received by reading message byte counts; (3) Get packets; the data payload contains descriptors for the remote nodes. The MU on a remote node receives each get packet into one of its injection FIFOs, then processes the descriptors and sends data back to the source node.","MU resources are in memory mapped I\/O address space and provide uniform access to all processor cores. In practice, the resources are likely grouped into smaller groups to give each core dedicated access. In one embodiment there is supported 544 injection FIFOs, or 32\/core, and 288 reception FIFOs, or 16\/core. The reception byte counts for put messages are implemented in L2 using the atomic counters described herein below. There is effectively unlimited number of counters subject to the limit of available memory for such atomic counters.","The MU interface is designed to deliver close to the peak 18 GB\/s (send)+18 GB\/s (receive) 5-D torus nearest neighbor data bandwidth, when the message data is fully contained in the 32 MB L2. This is basically 1.8 GB\/s+1.8 GB\/s maximum data payload bandwidth over 10 torus links. When the total message data size exceeds the 32 MB L2, the maximum network bandwidth is then limited by the sustainable external DDR memory bandwidth.","The Blue Gene\/P DMA drives the 3-D torus network, but not the collective network. On Blue Gene\/Q, because the collective and I\/O networks are embedded in the 5-D torus with a uniform network packet format, the MU will drive all regular torus, collective and I\/O network traffic with a unified programming interface.","There is provided an architecture of a distributed parallel messaging unit (\u201cMU\u201d) for high throughput networks, wherein a messaging unit at one or more nodes of a network includes a plurality of messaging elements (\u201cMEs\u201d). In one embodiment, each ME operates in parallel and includes a DMA element for handling message transmission (injection) or message reception operations.","The top level architecture of the Messaging Unit  interfacing with the Network Interface Unit  is shown in FIG. --. The Messaging Unit  functional blocks involved with packet injection control as shown in FIG. -- includes the following: an Injection control unit  implementing logic for queuing and arbitrating the processors' requests to the control areas of the injection MU; and, a plurality of iMEs (injection messaging engine units)  that read data from L2 cache or DDR memory and insert it in the network injection FIFOs . In one embodiment, there are 16 iMEs , one for each network injection FIFO . The Messaging Unit  functional blocks involved with packet reception control as shown in FIG. -- include a Reception control unit  implementing logic for queuing and arbitrating the requests to the control areas of the reception MU; and, a plurality of rMEs (reception messaging engine units)  that read data from the network reception FIFOs , and insert them into the associated memory system. In one embodiment, there are 16 rMEs , one for each network reception FIFO . A DCR control Unit  is provided that includes DCR (control) registers for the MU .","As shown in FIG. --, the herein referred to Messaging Unit, \u201cMU\u201d such as MU  implements plural direct memory access engines to offload the Network Interface Unit . In one embodiment, it transfers blocks via three Xbar interface masters  between the memory system and the network reception FIFOs  and network injection FIFOs  of the Network Interface Unit . Further, in one embodiment, L2 cache controller accepts requests from the Xbar interface masters  to access the memory system, and accesses either L2 cache  or the external memory  () to satisfy the requests. The MU is additionally controlled by the cores via memory mapped I\/O access through an additional switch slave port .","In one embodiment, one function of the messaging unit  is to ensure optimal data movement to, and from the network into the local memory system for the node by supporting injection and reception of message packets. As shown in FIG. --, in the Network Interface Unit  the network injection FIFOs  and network reception FIFOs  (sixteen for example) each comprise a network logic device for communicating signals used for controlling routing data packets, and a memory for storing multiple data arrays. Each network injection FIFOs  is associated with and coupled to a respective network sender device \u2014(where n=1 to 16 for example), each for sending message packets to a node, and each network reception FIFOs  is associated with and coupled to a respective network receiver device (where n=1 to 16 for example), each for receiving message packets from a node. A network DCR (device control register)  is provided that is coupled to the network injection FIFOs , network reception FIFOs , and respective network receivers , and network senders . A complete description of the DCR architecture is available in IBM's Device Control Register Bus 3.5 Architecture Specifications Jan. 27, 2006, which is incorporated by reference in its entirety. The network logic device controls the flow of data into and out of the network injection FIFO  and also functions to apply \u2018mask bits\u2019 supplied from the network DCR . In one embodiment, the rMEs communicate with the network FIFOs in the Network Interface Unit  and receives signals from the network reception FIFOs  to indicate, for example, receipt of a packet. It generates all signals needed to read the packet from the network reception FIFOs . This Network Interface Unit  further provides signals from the network device that indicate whether or not there is space in the network injection FIFOs  for transmitting a packet to the network and can also be configured to write data to the selected network injection FIFOs.","The MU  further supports data prefetching into the L2 cache . On the injection side, the MU splits and packages messages into network packets, and sends packets to the network respecting the network protocol. On packet injection, the messaging unit distinguishes between packet injection and memory prefetching packets based on certain control bits in the message descriptor, e.g., such as a least significant bit of a byte of a descriptor  shown in FIG. --. A memory prefetch mode is supported in which the MU fetches a message into L2, but does not send it. On the reception side, it receives packets from a network, and writes them into the appropriate location in memory system, depending on control information stored in the packet. On packet reception, the messaging unit  distinguishes between three different types of packets, and accordingly performs different operations. The types of packets supported are: memory FIFO packets, direct put packets, and remote get packets.","With respect to on-chip local memory copy operation, the MU copies content of an area in the associated memory system to another area in the memory system. For memory-to-memory on chip data transfer, a dedicated SRAM buffer, located in the network device, is used. Injection of remote get packets and the corresponding direct put packets, in one embodiment, can be \u201cpaced\u201d by software to reduce contention within the network. In this software-controlled paced mode, a remote get for a long message is broken up into multiple remote gets, each for a sub-message. The sub-message remote get is allowed to enter the network if the number of packets belonging to the paced remote get active in the network is less than an allowed threshold. To reduce contention in the network, software executing in the cores in the same nodechip can control the pacing.","The MU  further includes an interface to a crossbar switch (Xbar)  in additional implementations. The MU  includes three (3) Xbar interface masters  to sustain network traffic and one Xbar interface slave  for programming. The three (3) Xbar interface masters  may be fixedly mapped to the iMEs , such that for example, the iMEs are evenly distributed amongst the three ports to avoid congestion. A DCR slave interface unit  providing control signals is also provided.","The handover between network device  and MU  is performed via buffer memory, e.g., 2-port SRAMs, for network injection\/reception FIFOs. The MU , in one embodiment, reads\/writes one port using, for example, an 800 MHz clock (operates at one-half the speed of a processor core clock, e.g., at 1.6 GHz, for example), and the network reads\/writes the second port with a 500 MHz clock, for example. The handovers are handled using the network injection\/reception FIFOs and FIFOs' pointers (which are implemented using latches, for example).","As shown in FIG. -- illustrating a more detailed schematic of the Messaging Unit  of FIG. --, multiple parallel operating DMA engines are employed for network packet injection, the Xbar interface masters  run at a predetermined clock speed, and, in one embodiment, all signals are latch bound. The Xbar write width is 16 bytes, or about 12.8 GB\/s peak write bandwidth per Xbar interface master in the example embodiment. In this embodiment, to sustain a 2*10 GB\/s=20 GB\/s 5-D torus nearest neighbor bandwidth, three (3) Xbar interface masters  are provided. Further, in this embodiment, these three Xbar interface masters are coupled with iMEs via ports , , . . . , . To program MU internal registers for the reception and injection sides, one Xbar interface slave  is used.","As further shown in FIG. --, there are multiple iMEs (injection messaging engine units) , , . . . , in correspondence with the number of network injection FIFOs, however, other implementations are possible. In the embodiment of the MU injection side A depicted, there are sixteen iMEs  for each network injection FIFO. Each of the iMEs ,, . . . , includes a DMA element including an injection control state machine , and injection control registers . Each iMEs ,, . . . , initiates reads from the message control SRAM (MCSRAM)  to obtain the packet header and other information, initiates data transfer from the memory system and, write back updated packet header into the message control SRAM . The control registers  each holds packet header information, e.g., a subset of packet header content, and other information about the packet currently being moved. The DMA injection control state machine  initiates reads from the message control SRAM  to obtain the packet header and other information, and then it initiates data transfer from the memory system to a network injection FIFO.","In an alternate embodiment, to reduce size of each control register  at each node, only a small portion of packet information is stored in each iME that is necessary to generate requests to switch . Without holding a full packet header, an iME may require less than 100 bits of storage. Namely, each iME  holds pointer to the location in the memory system that holds message data, packet size, and miscellaneous attributes.","Header data is sent from the message control SRAM  to the network injection FIFO directly; thus the iME alternatively does not hold packet headers in registers. The Network Interface Unit  provides signals from the network device to indicate whether or not there is space available in the paired network injection FIFO. It also writes data to the selected network injection FIFOs.","As shown in FIG. --A, the Xbar interface masters  generate external connection to Xbar for reading data from the memory system and transfer received data to the correct iME\/network interface. To reduce the size of the hardware implementation, in one embodiment, iMEs  are grouped into clusters, e.g., clusters of four, and then it pairs (assigns) one or more clusters of iMEs to a single Xbar interface master. At most one iME per Xbar interface master can issue a read request on any cycle for up to three (3) simultaneous requests (in correspondence to the number of Xbar interface masters, e.g., three (3) Xbar interface masters). On the read data return side, one iME can receive return data on each master port. In this embodiment of MU injection side A, it is understood that more than three iMEs can be actively processing at the same time, but on any given clock cycle three can be requesting or reading data from the Xbar , in the embodiment depicted. The injection control SRAM  is also paired with one of the three master ports, so that it can fetch message descriptors from memory system, i.e., Injection memory FIFOs. In one embodiment, each iME has its own request and acknowledgement signal lines connected to the corresponding Xbar interface master. The request signal is from iME to Xbar interface master, and the acknowledgement signal is from Xbar interface master to iME. When an iME wants to read data from the memory system, it asserts the request signal. The Xbar interface master selects one of iMEs requesting to access the memory system (if any). When Xbar interface master accepts a request, it asserts the acknowledgement signal to the requesting iME. In this way iME knows when the request is accepted. The injection control SRAM has similar signals connected to a Xbar interface master (i.e. request and acknowledgement signals). The Xbar interface master treats the injection control SRAM in the same way as an iME.","FIG. -- further shows internal injection control status registers  implemented at each iME of the MU device that receive control status data from message control SRAM. These injection control status registers include, but are not limited to, registers for storing the following: control status data including pointer to a location in the associated memory system that holds message data, packet size, and miscellaneous attributes. Based on the control status data, iME will read message data via the Xbar interface master and store it in the network injection FIFO.","FIG. --A depicts in greater detail those elements of the MU injection side A for handling the transmission (packet injection) for the MU . Messaging support including packet injection involves packaging messages into network packets and, sending packets respecting network protocol. The network protocol includes point-to-point and collective. In the point-to-point protocol, the packet is sent directly to a particular destination node. On the other hand, in the collective protocol, some operations (e.g. floating point addition) are performed on payload data across multiple packets, and then the resulting data is sent to a receiver node.","For packet injection, the Xbar interface slave  programs injection control by accepting write and read request signals from processors to program SRAM, e.g., an injection control SRAM (ICSRAM)  of the MU  that is mapped to the processor memory space. In one embodiment, Xbar interface slave processes all requests from the processor in-order of arrival. The Xbar interface masters generate connection to the Xbar  for reading data from the memory system, and transfers received data to the selected iME element for injection, e.g., transmission into a network.","The ICSRAM  particularly receives information about a buffer in the associated memory system that holds message descriptors, from a processor desirous of sending a message. The processor first writes a message descriptor to a buffer location in the associated memory system, referred to herein as injection memory FIFO (imFIFO) shown in FIG. --A as imFIFO . The imFIFO(s) , implemented at the memory system in one embodiment shown in FIG. --A, are implemented as circular buffers having slots  for receiving message descriptors and having a start address  (indicating the first address that this imFIFO  can hold a descriptor), imFIFO size (from which the end address  can be calculated), and including associated head and tail pointers to be specified to the MU. The head pointer points to the first descriptor stored in the FIFO, and the tail pointer points to the next free slot just after the last descriptor stored in the FIFO. In other words, the tail pointer points to the location where the next descriptor will be appended. FIG. --A shows an example empty imFIFO , where a tail pointer is the same as the head pointer (i.e., pointing to a same address); and FIG. --B shows that a processor has written a message descriptor  into the empty slot in an injection memory FIFO  pointed to by the tail pointer. After storing the descriptor, the processor increments the tail pointer by the size of the descriptor so that the stored descriptor is included in the imFIFO, as shown in FIG. --C. When the head and tail pointers reach the FIFO end address (=start pointer plus the FIFO size), they wrap around to the FIFO start address. Software accounts for this wrap condition when updating the head and tail pointers. In one embodiment, at each compute node, there are 17 \u201cgroups\u201d of imFIFOs, for example, with 32 imFIFOs per group for a total of 544, in an example embodiment. In addition, these groups may be sub-grouped, e.g., 4 subgroups per group. This allows software to assign processors and threads to groups or subgroups. For example, in one embodiment, there are 544 imFIFOs to enable each thread on each core to have its own set of imFIFOs. Some imFIFOs may be used for remote gets and for local copy. It is noted that any processor can be assigned to any group.","Returning to FIG. --, the message descriptor associated with the message to be injected is requested by the injection control state machine  via one of the Xbar interface masters . Once retrieved from memory system, the requested descriptor returns via the Xbar interface master and is sent to the message control SRAM  for local storage. FIG. -- depicts an example layout of a message descriptor . Each message descriptor describes a single complete packet, or it can describe a large message via a message length (one or more packets) and may be 64 bytes in length, aligned on a 64 byte boundary. The first 32 bytes of the message descriptor includes, in one embodiment, information relevant to the message upon injection, such as the message length , where its payload starts in the memory system (injection payload starting address ), and a bit-mask  (e.g., 16 bits for the 16 network injection FIFO's in the embodiment described) indicating into which network injection FIFOs the message may be injected. That is, each imFIFO can use any of the network injection FIFOs, subject to a mask setting in the message descriptor such as specified in \u201cTorus Injection FIFO Map\u201d field  specifying the mask, for example, as 16 least significant bits in this field that specifies a bitmap to decide which of the 16 network injection FIFOs can be used for sending the message. The second 32 bytes include the packet header  whose content will be described in greater detail herein.","As further shown in FIG. --, the message descriptor further includes a message interrupt bit  to instruct the message unit to send an interrupt to the processor when the last (and only last) packet of the message has been received. For example, when the MU injection side sends the last packet of a message, it sets the interrupt bit (bit  in FIG. --A, field ). When an rME receives a packet and sees this bit set in the header, it will raise an interrupt. Further, one bit e.g., a least significant bit, of Prefetch Only bits , FIG. --, when set, will cause the MU to fetch the data into L2 only. No message is sent if this bit is set. This capability to prefetch data is from the external memory into the L2. A bit in the descriptor indicates the message as prefetch only and the message is assigned to one of iMEs (any) for local copy. The message may be broken into packets, modified packet headers and byte count. Data is not written to any FIFO.","In a methodology  implemented by the MU for sending message packets, ICSRAM holds information including the start address, size of the imFIFO buffer, a head address, a tail address, count of fetched descriptors, and free space remaining in the injection memory FIFO (i.e., start, size, head, tail, descriptor count and free space).","As shown in step  of FIG. --, the injection control state machine  detects the state when an injection memory FIFO  is non-empty, and initiates copying of the message specific information of the message descriptor  to the message control SRAM block . That is, the state machine logic  monitors all write accesses to the injection control SRAM. When it is written, the logic reads out start, size, head, and tail pointers from the SRAM and check if the imFIFO is non-empty. Specifically, an imFIFO is non-empty if the tail pointer is not equal to the head pointer. The message control SRAM block  includes information (received from the imFIFO) used for injecting a message to the network including, for example, a message start address, message size in bytes, and first packet header. This message control SRAM block  is not memory-mapped (it is used only by the MU itself).","The Message selection arbiter unit  receives the message specific information from each of the message control SRAM , and receives respective signals  from each of the iME engines , , . . . , . Based on the status of each respective iME, Message selection arbiter unit  determines if there is any message waiting to be sent, and pairs it to an available iME engine , , . . . , , for example, by issuing an iME engine selection control signal . If there are multiple messages which could be sent, messages may be selected for processing in accordance with a pre-determined priority as specified, for example, in Bits - in virtual channel in field  specified in the packet header of FIG. --A. The priority is decided based on the virtual channel. Thus, for example, a system message may be selected first, then a message with high-priority, then a normal priority message is selected. If there are multiple messages that have the highest priority among the candidate messages, a message may be selected randomly, and assigned to the selected iME engine. In every clock cycle, one message can be selected and assigned.","Injection Operation","Returning to FIG. --A, in operation, as indicated at , a processor core  writes to the memory system message data  that is to be sent via the network. The message data can be large, and can require multiple network packets. The partition of a message into packets, and generation of correct headers for these packets is performed by the MU device A.","Then, as indicated at , once an imFIFO  is updated with the message descriptor, the processor, via the Xbar interface slave  in the messaging unit, updates the pointer located in the injection control SRAM (ICSRAM)  to point to a new tail (address) of the next descriptor slot  in the imFIFO . That is, after a new descriptor is written to an empty imFIFO by a processor, e.g., imFIFO , software executing on the cores of the same chip writes the descriptor to the location in the memory system pointed to by the tail pointer, and then the tail pointer is incremented for that imFIFO to point to the new tail address for receiving a next descriptor, and the \u201cnew tail\u201d pointer address is written to ICSRAM  as depicted in FIG. -- showing ICSRAM contents . Subsequently, the MU will recognize the new tail pointer and fetch the new descriptor. The start pointer address  in FIG. --A may be held in ICSRAM along with the size of the buffer. That is, in one embodiment, the end address  is NOT stored in ICSRAM. ICSRAM does hold a \u201csize minus 1\u201d value of the imFIFO. MU logic calculates end addresses using the \u201csize minus 1\u201d value. In one embodiment, each descriptor is 64 bytes, for example, and the pointers in ICSRAM are managed in 64-byte units. It is understood that, in view of FIGS. --D and --E a new descriptor may be added to a non-empty imFIFO, e.g., imFIFO \u2032. The procedure is similar as the case shown in FIG. --B and FIG. --C, where, in the non-empty imFIFO depicted, a new message descriptor  is being added to the tail address, and the tail pointer is incremented, and the new tail pointer address written to ICSRAM .","As shown in the method depicting the processing at the injection side MU, as indicated at  in FIG. --, the injection control FSM  waits for indication of receipt of a message descriptor for processing. Upon detecting that a new message descriptor is available in the injection control SRAM , the FSM  at will initiate fetching of the descriptor at the head of the imFIFO. At , the MU copies the message descriptor from the imFIFO  to the message control SRAM  via the Xbar interface master, e.g., port 0. This state machine , in one embodiment, also calculates the remaining free space in that imFIFO whenever size, head, or tail pointers are changed, and updates the correct fields in the SRAM. If the available space in that imFIFO crosses an imFIFO threshold, the MU may generate an interrupt, if this interrupt is enabled. That is, when the available space (number of free slots to hold a new descriptors) in an imFIFO exceeds the threshold, the MU raises an interrupt. This threshold is specified by software on the cores via a register in DCR Unit. For example, suppose the threshold is 10, and an imFIFO is filled with the descriptors (i.e., no free slot to store a new descriptor). The MU will process the descriptors. Each time a descriptor has been processed, imFIFO will get one free slot to store a new descriptor. After 11 descriptors have been processed, for example, the imFIFO will have 11 free slots, exceeds the threshold of 10. As a result, MU will raise an interrupt for this imFIFO.","Next, the arbitration logic implemented in the message selection arbiter  receives inputs from the message control SRAM  and particularly, issues a request to process the available message descriptor, as indicated at , FIG. --. The message selection arbiter  additionally receives inputs  from the iMEs , . . . , to apprise the arbiter of the availability of iMEs. The message control SRAM  requests of the arbiter  an iME to process the available message descriptor. From pending messages and available iMEs, the arbiter logic implemented pairs an iME, e.g., iME , and a message at .","FIG. -- depicts a flowchart showing message selection arbiter logic  implemented according to an example embodiment. A first step  depicts the message selection arbiter  waiting until at least one descriptor becomes available in message control SRAM. Then, at , for each descriptor, the arbiter checks the Torus Injection FIFO Map field  (FIG. --) to find out which iME can be used for this descriptor. Then, at , the arbiter checks availability of each iME and selects only the descriptors that specify at least one idle (available) iME in their FIFO map . If there is no descriptor, then the method returns to  to wait for a descriptor. Otherwise, at , one descriptor is selected from among the selected ones. It is understood that various selection algorithms can be used (e.g., random, round-robin, etc.). Then, at , for the selected descriptor, select one of the available iMEs specified in the FIFO map . At , the selected iME processes the selected descriptor.","In one embodiment, each imFIFO  has assigned a priority bit, thus making it possible to assign a high priority to that user FIFO. The arbitration logic assigns available iMEs to the active messages with high priority first (system FIFOs have the highest priority, then user high priority FIFOs, then normal priority user FIFOs). From the message control SRAM , the packet header (e.g., 32B), number of bytes, and data address are read out by the selected iME, as indicated at step , FIG. --. On the injection side, one iME can work on a given message at any time. However, multiple iMEs can work in parallel on different messages. Once a message and an iME are matched, only one packet of that message is processed by the iME. An active status bit for that message is set to zero during this time, to exclude this imFIFO from the arbitration process. To submit the next packet to the network, the arbitration steps are repeated. Thus, other messages wanting the same iME (and network injection FIFO) are enabled to be transmitted.","In one embodiment, as the message descriptor contains a bitmap indicating into which network injection FIFOs packets from the message may be injected (Torus injection FIFO map bits  shown in FIG. --), the iME first checks the network injection FIFO status so that it knows not to arbitrate for a packet if its paired network injection FIFO is full. If there is space available in the network injection FIFO, and that message can be paired to that particular iME, the message to inject is assigned to the iME.","Messages from injection memory FIFOs can be assigned to and processed by any iME and its paired network injection FIFO. One of the iMEs is selected for operation on a packet-per-packet basis for each message, and an iME copies a packet from the memory system to a network injection FIFO, when space in the network injection FIFO is available. At step , the iME first requests the message control SRAM to read out the header and send it directly to the network injection FIFO paired to the particular iME, e.g., network injection FIFO , in the example provided. Then, as shown at , FIGS. --A and --, the iME initiates data transfer of the appropriate number of bytes of the message from the memory system to the iME, e.g., iME , via an Xbar interface master. In one aspect, the iME issues read requests to copy the data in 32B, 64B, or 128B at a time. More particularly, as a message may be divided into one or more packets, each iME loads a portion of message corresponding to the packet it is sending. The packet size is determined by \u201cBit -, Size\u201d in field , FIG. --B. This 5-bit field specifies packet payload size in 32-byte units (e.g. 1=>32B, 2=>64B, . . . 16=>512B). The maximum allowed payload size is 512B. For example, the length of a message is 129 bytes, and the specified packet size is 64 bytes. In this case this message is sent using two 64B packets and one 32B packet (only 1B in the 32B payload is used). The first packet sends 1st to 64th bytes of the message, the second one sends 65to 128bytes, and the third one sends 129byte. Therefore, when an iME is assigned to send the second packet, it will request the master port to load 65to 128byte of the message. The iME may load unused bytes and discard them, due to some alignment requirements for accessing the memory system.","Data reads are issued as fast as the Xbar interface master allows. For each read, the iME calculates the new data address. In one embodiment, the iME uses a start address (e.g., specified as address  in FIG. --) and the payload size ( in FIG. --B) to decide data address. Specifically, iME reads data block starting from the start address () whose size is equal to payload size (). Each time a packet is processed, the start address () is incremented by payload size () so that the next iME gets the correct address to read payload data. After the last data read request is issued, the next address points to the first data \u201cchunk\u201d of the next packet. Each iME selects whether to issue a 32B, 64B, or 128B read to the Xbar interface master.","The selection of read request size is performed as follows: In the following examples, a \u201cchunk\u201d refers to a 32B block that starts from 32B-aligned address. Thus, for example, for a read request of 128B, the iME requests 128B block starting from address 128N (N: integer), when it needs at least the 2nd and 3rd chunks in the 128B block (i.e., It needs at least 2 consecutive chunks starting from address 128N+32. This also includes the cases that it needs first 3 chunks, last 3 chunks, or all the 4 chunks in the 128B block, for example.) For a read request of 64B, the iME requests 64B block starting from address 64N, e.g., when it needs both chunks included in the 64B block. For read request of 32B: the iME requests 32B block. For example, when the iME is to read 8 data chunks from addresses 32 to 271, it generates requests as follows:","1. iME requests 128B starting from address 0, and uses only the last 96B;","2. iME requests 128B starting from address 128, and uses all 128B;","3. iME requests 32B starting from address 256.","It is understood that read data can arrive out of order, but returns via the Xbar interface master that issued the read, e.g., the read data will be returned to the same master port requesting the read. However, the order between read data return may be different from the request order. For example, suppose a master port requested to read address 1, and then requested to read address 2. In this case the read data for address 2 can arrive earlier than that for address 1.","iMEs are mapped to use one of the three Xbar interface masters in one implementation. When data arrives at the Xbar interface master, the iME which initiated that read request updates its byte counter of data received, and also generates the correct address bits (write pointer) for the paired network injection FIFO, e.g., network injection FIFO . Once all data initiated by that iME are received and stored to the paired network injection FIFO, the iME informs the network injection FIFO that the packet is ready in the FIFO, as indicated at . The message control SRAM  updates several fields in the packet header each time it is read by an iME. It updates the byte count of the message (how many bytes from that message are left to be sent) and the new data offset for the next packet.","Thus, as further shown in FIG. --, at step , a decision is made by the iME control logic whether the whole message has been injected. If the whole message has not been sent, then the process resumes at step  where the arbiter logic implemented pairs an iME to send the next one packet for the message descriptor being processed, and steps - are repeated, until such time the whole message is sent. The arbitration step is repeated for each packet.","Each time an iME  starts injecting a new packet, the message descriptor information at the message control SRAM is updated. Once all packets from a message have been sent, the iME removes its entry from the message control SRAM (MCSRAM), advances its head pointer in the injection control SRAM . Particularly, once the whole message is sent, as indicated at , the iME accesses the injection control SRAM  to increment the head pointer, which then triggers a recalculation of the free space in the imFIFO . That is, as the pointers to injection memory FIFOs work from the head address, thus, when the message is finished, the head pointer is updated to the next slot in the FIFO. When the FIFO end address is reached, the head pointer will wrap around to the FIFO start address. If the updated head address pointer is not equal to the tail of the injection memory FIFO then there is a further message descriptor in that FIFO that could be processed, i.e., the imFIFO is not empty and one or more message descriptors remain to be fetched. Then, the ICSRAM will request the next descriptor read via the Xbar interface master, and the process returns to . Otherwise, if the head pointer is equal to the tail, the FIFO is empty.","As mentioned, the injection side A of the Messaging Unit supports any byte alignment for data reads. The correct data alignment is performed when data are read out of the network reception FIFOs, i.e., alignment logic for injection MU is located in the network device. The packet size will be the value specified in the descriptor, except for the last packet of a message. MU adjusts the size of the last packet of a message to the smallest size to hold the remaining part of the message data. For example, when user injects a 1025B message descriptor whose packet size is 16 chunks=512B, the MU will send this message using two 512B packets and one 32B packet. The 32B packet is the last packet and only 1B in the 32B payload is valid.","As additional examples: for a 10B message with a specified packet size=16 (512B), the MU will send one 32B packet, only 10B in the 32B data is valid. For a 0B message with a specified packet size=anything, the MU will send one 0B packet. For a 260B message with a specified packet size=8 (256B), the MU will send one 256B packet and one 32B packet. Only 4B in the last 32B packet data are valid.","In operation, the iMEs\/rMEs further decide priority for payload read\/write from\/to the memory system based on the virtual channel (VC) of the message. Certain system VCs (e.g., \u201csystem\u201d and \u201csystem collective\u201d) will receive the highest priority. Other VCs (e.g., high priority and usercommworld) will receive the next highest priority. Other VCs will receive the lower priority. Software executing at the processors sets a VC correctly to get desired priority.","It is further understood that each iME can be selectively enabled or disabled using a DCR register. An iME  is enabled when the corresponding DCR (control signal), e.g., bit, is set to 1, and disabled when the DCR bit is set to 0, for example. If this DCR bit is 0, the iME will stay in the idle state until the bit is changed to 1. If this bit is cleared while the corresponding iME is processing a packet, the iME will continue to operate until it finishes processing the current packet. Then it will return to the idle state until the enable bit is set again. When an iME is disabled, messages are not processed by it. Therefore, if a message specifies only this iME in the FIFO map, this message will not be processed and the imFIFO will be blocked until the iME is enabled again.","Reception","FIG. -- depicts a high level diagram of the MU reception side B for handling the packet reception in the MU . Reception operation includes receiving packets from the network and writing them into the memory system. Packets are received at network reception FIFOs , , . . . , . In one embodiment, the network reception FIFOs are associated with torus network, collective, and local copy operations. In one implementation, n=16, however, other implementations are possible. The memory system includes a set of reception memory FIFOs (rmFIFOs), such as rmFIFO  shown in FIG. --A, which are circular buffers used for storing packets received from the network. In one embodiment, there are sixteen (16) rmFIFOs assigned to each processor core, however, other implementations are possible.","As shown in FIG. --, reception side MU device B includes multiple rMEs (reception messaging engine units) ,, . . . , . In one embodiment, n=16, however, other implementations are possible. Generally, at the MU reception side B, there is an rME for each network reception FIFO. Each of the rMEs contains a DMA reception control state machine , byte alignment logic , and control\/status registers (not shown). In the rMEs ,, . . . , , the DMA reception control state machine  detects that a paired network reception FIFO is non-empty, and if it is idle, it obtains the packet header, initiates reads to an SRAM, controls data transfer to the memory system, including an update of counter data located in the memory system, and it generates an interrupt, if selected. The Byte alignment logic  ensures that the data to be written to the memory system are aligned, in one embodiment, on a 32B boundary for memory FIFO packets, or on any byte alignment specified, e.g., for put packets.","In one embodiment, storing of data to Xbar interface master is via 16-byte unit and must be 16-byte aligned. The requestor rME can mask some bytes, i.e., it can specify which bytes in the 16-byte data are actually stored. The role of alignment logic is to place received data in the appropriate position in a 16-byte data line. For example: an rME needs to write 20-byte received data to memory system address 35 to 54. In this case 2 write requests are necessary: 1) The alignment logic builds the first 16-byte write data. The 1to 13received bytes are placed in byte 3 to 15 in the first 16-byte data. Then the rME tells the Xbar interface master to store the 16-byte data to address 32, but not to store the byte 0, 1, and 2 in the 16-byte data. As a result, byte 3 to 15 in the 16-byte data (i.e. 1to 13received bytes) will be written to address 35 to 47 correctly. Then the alignment logic builds the second 16-byte write data. The 14to 20received bytes are placed in byte 0 to 6 in the second 16-byte data. Then the rME tell the Xbar interface master to store the 16-byte data to address 48, but not to store byte 7 to 15 in the 16-byte data. As a result, the 14to 20received bytes will be written to address 48 to 54 correctly.","Although not shown, control registers and SRAMs are provided that store part of control information when needed for packet reception. These status registers and SRAMs may include, but are not limited to, the following registers and SRAMs: Reception control SRAM (Memory mapped); Status registers (Memory mapped); and remote put control SRAM (Memory mapped).","In operation, when one of the network reception FIFOs receives a packet, the network device generates a signal  for receipt at the paired rME  to inform the paired rME that a packet is available. In one aspect, the rME reads the packet header from the network reception FIFO, and parses the header to identify the type of the packet received. There are three different types of packets: memory FIFO packets, direct put packets, and remote get packets. The type of packet is specified by bits in the packet header, as described below, and determines how the packets are processed.","In one aspect, for direct put packets, data from direct put packets processed by the reception side MU device B are put in specified locations in memory system. Information is provided in the packet to inform the rME of where in memory system the packet data is to be written. Upon receiving a remote get packet, the MU device B initiates sending of data from the receiving node to some other node.","Other elements of the reception side MU device B include the Xbar interface slave  for management. It accepts write and read requests from a processor and updates SRAM values such as reception control SRAM (RCSRAM)  or remote put control SRAM (R-put SRAM)  values. Further, the Xbar interface slave  reads SRAM and returns read data to the Xbar. In one embodiment, Xbar interface slave  processes all requests in-order of arrival. More particularly, the Xbar interface master  generates a connection to the Xbar  to write data to the memory system. Xbar interface master  also includes an arbiter unit  for arbitrating between multiple rMEs (reception messaging engine units) , , . . . to access the Xbar interface master. In one aspect, as multiple rMEs compete for a Xbar interface master to store data, the Xbar interface master decides which rME to select. Various algorithm can be used for selecting an rME. In one embodiment, the Xbar interface master selects an rME based on the priority. The priority is decided based on the virtual channel of the packet the rME is receiving. (e.g., \u201csystem\u201d and \u201csystem collective\u201d have the highest priority, \u201chigh priority\u201d and \u201cusercommworld\u201d have the next highest priority, and the others have the lowest priority). If there are multiple rMEs that have the same priority, one of them may be selected randomly.","As in the MU injection side of FIG. --, the MU reception side also uses the three Xbar interface masters. In one embodiment, a cluster of five or six rMEs may be paired to a single Xbar interface master (there can be two or more clusters of five or six rMEs). In this embodiment, at most one rME per Xbar interface master may write on any given cycle for up to three simultaneous write operations. Note that more than three rMEs can be active processing packets at the same time, but on any given cycle only three can be writing to the switch.","The reception control SRAM  is written to include pointers (start, size, head and tail) for rmFIFOs, and further, is mapped in the processor's memory address space. The start pointer points to the FIFO start address. The size defines the FIFO end address (i.e. FIFO end=start+size). The head pointer points to the first valid data in the FIFO, and the tail pointer points to the location just after the last valid data in the FIFO. The tail pointer is incremented as new data is appended to the FIFO, and the head pointer is incremented as new data is consumed from the FIFO. The head and tail pointers need to be wrapped around to the FIFO start address when they reach the FIFO end address. A reception control state machine  arbitrates access to reception control SRAM (RCSRAM) between multiple rMEs and processor requests, and it updates reception memory FIFO pointers stored at the RCSRAM. As will be described in further detail below, R-Put SRAM  includes control information for put packets (base address for data, or for a counter). This R-Put SRAM is mapped in the memory address space. R-Put control FSM  arbitrates access to R-put SRAM between multiple rMEs and processor requests. In one embodiment, the arbiter mechanism employed alternately grants an rME and the processor an access to the R-put SRAM. If there are multiple rMEs requesting for access, the arbiter selects one of them randomly. There is no priority difference among rMEs for this arbitration.","FIG. -- depicts a methodology  for describing the operation of an rME , , . . . . As shown in FIG. --, at , the rME is idle waiting for reception of a new packet in a network reception FIFO , , . . . , . Then, at , having received a packet, the header is read and parsed by the respective rME to determine where the packet is to be stored. At , the type of packet is determined so subsequent packet processing can proceed accordingly. Thus, for example, in the case of memory FIFO packets, processing proceeds at the rME at step  et seq.; in the case of direct put packets, processing proceeds at the rME at step  et seq.; and, for the case of remote get packets, processing proceeds at the rME at step  et seq.","In the case of memory FIFO packet processing, in one embodiment, memory FIFO packets include a reception memory FIFO ID field in the packet header that specifies the destination rmFIFO in memory system. The rME of the MU device B parses the received packet header to obtain the location of the destination rmFIFO. As shown in FIG. --A depicting operation of the MU device B- for processing received memory FIFO packets, these memory FIFO packets are to be copied into the rmFIFOs  identified by the memory FIFO ID. Messages processed by an rME can be moved to any rmFIFO. Particularly, as shown in FIG. --A and FIG. -- at step , the rME initiates a read of the reception control SRAM  for that identified memory FIFO ID, and, based on that ID, a pointer to the tail of the corresponding rmFIFO in memory system (rmFIFO tail) is read from the reception control SRAM at . Then, the rME writes the received packet, via one of the Xbar interface masters , to the rmFIFO, e.g., in 16B write chunks. In one embodiment, the rME moves both the received packet header and the payload into the memory system location starting at the tail pointer. For example, as shown at , the packet header of the received memory FIFO packet is written, via the Xbar interface master, to the location after the tail in the rmFIFO  and, at , the packet payload is read and stored in the rmFIFO after the header. Upon completing the copy of the packet to the memory system, the rME updates the tail pointer and can optionally raise an interrupt, if the interrupt is enabled for that rmFIFO and an interrupt bit in the packet header is set. In one embodiment, the tail is updated for number of bytes in the packets atomically. That is, as shown at , the tail pointer of the rmFIFO is increased to include the new packet, and the new tail pointer is written to the RCSRAM . When the tail pointer reaches the end of FIFO as a result of the increment, it will be wrapped around to the FIFO start. Thus, for memory FIFO packets, the rmFIFOs can be thought of as a simple producer-consumer queue: rMEs are the producers who move packets from network reception FIFOs into the memory system, and the processor cores are the consumers who use them. The consumer (processor core) advances a header pointer, and the producer (rME) advances a tail pointer.","In one embodiment, as described in greater detail herein, to allow simultaneous usage of the same rmFIFO by multiple rMEs, each rmFIFO has advance tail, committed tail, and two counters for advance tail ID and committed tail ID. The rME copies packets to the memory system location starting at the advance tail, and gets advance tail ID. After the packet is copied to the memory system, the rME checks the committed tail ID to determine if all previously received data for that rmFIFO are copied. If this is the case, the rME updates committed tail, and committed tail ID, otherwise it waits. An rME implements logic to ensure that all store requests for header and payload have been accepted by the Xbar before updating committed tail (and optionally issuing interrupt).","In the case of direct put packet processing, in one embodiment, the MU device B further initiates putting data in specified location in the memory system. Direct put packets include in their headers a data ID field and a counter ID field\u2014both used to index the R-put SRAM ; however, the header includes other information such as, for example, a number of valid bytes, a data offset value, and counter offset value. The rME of the MU device B parses the header of the received direct put packet to obtain the data ID field and a counter ID field values. Particularly, as shown in FIG. --B depicting operation of the MU device B- for processing received direct put packets and, the method of FIG. -- at step , the rME initiates a read of the R-put SRAM  and, based on data ID field and a counter ID field values, indexes and reads out a respective data base address and a counter base address. Thus, for example, a data base address is read from the R-put SRAM , in one embodiment, and the rME calculates an address in the memory system where the packet data is to be stored. In one embodiment, the address for packet storage is calculated according to the following:\n\nBase address+data offset=address for the packet\n","In one embodiment, the data offset is stored in the packet header field \u201cPut Offset\u201d  as shown in FIG. --. This is done on the injection side at the sender node. The offset value for the first packet is specified in the header field \u201cPut Offset\u201d  in the descriptor. MU automatically updates this offset value during injection. For example, suppose offset value  is specified in a message descriptor, and three 512-byte packets are sent for this message. The first packet header will have offset=10000, and the next packet header will have offset=10512, and the last packet header will have offset=11024. In this way each packet is given a correct displacement from the starting address of the message. Thus each packet is stored to the correct location.","Likewise, a counter base address is read from the R-put SRAM , in one embodiment, and the rME calculates another address in the memory system where a counter is located. The value of the counter is to be updated by the rME. In one embodiment, the address for counter storage is calculated according to the following:\n\nBase address+counter offset=address for the counter\n","In one embodiment, the counter offset value is stored in header field \u201cCounter Offset\u201d , FIG. --. This value is directly copied from the packet header field in the descriptor at the sender node. Unlike the data offset, all the packets from the same message will have the same counter offset. This means all the packets will correctly access the same counter address.","In one embodiment, the rME moves the packet payload from a network reception FIFO  into the memory system location calculated for the packet. For example, as shown at , the rME reads the packet payload and, via the Xbar interface master, writes the payload contents to the memory system specified at the calculated address, e.g., in 16B chunks or other byte sizes. Additionally, as shown at , the rME atomically updates a byte counter in the memory system.","The alignment logic implemented at each rME supports any alignment of data for direct put packets. FIG. -- depicts a flow chart of a method for performing data alignment for put packets. The alignment logic is necessary because of processing restrictions when rME stores data via Xbar interface master: 1) rME can store data in 16-byte unit and the destination is to be 16-byte aligned; 2) If rME wants to write a subset of a 16-byte chunk, it needs to set Byte Enable (BE) signals correctly. There are 16 bits of byte enable signals to control whether each byte in a 16-byte write data line is stored to the memory system. When rME wants to store all 16 bytes, it needs to assert all the 16 byte enable (BE) bits. Because of this, rME needs to place each received byte at a particular position in a 16-byte line. Thus, in one embodiment, a write data bus provides multiple bytes, and byte enable signals control which bytes on the bus are actually written to the memory system.","As shown in FIG. -- depicting a flowchart showing byte alignment method  according to an example embodiment, a first step  includes an rME waiting for a new packet to be received and, upon arrival, rME provides number of valid bytes in the payload and destination address in the memory system. Then, the following variables are initialized including: N=number of valid bytes, A=destination address, and, R=A mod 16 (i.e. position in a 16B chunk), BUF(0 to 15): buffer to hold 16B write data line, each element is a byte, and BE(0 to 15): buffer to hold byte enable, (each element is a bit). Then, at , a determination is made as to whether the whole payload data fits in one 16B write data line, e.g., by performing a check of whether R+N<16. If determined that the payload data could fit, then the process proceeds to  where the rME performs storing the one 16B line; and, copying the N bytes payload data to BUF(R to R+N\u22121). Letting (Byte Enable) BE(R to R+N\u22121)=1 and others=0, the rME requests the Xbar interface master to store BUF to address A-R, with byte enable BE. Then the process returns to step  to wait for the next packet. Otherwise, if it is determined at step  that the payload data could not fit in one 16B write data line, then the process proceeds to  to perform storing the first 16B line and copying a first 16-R payload bytes to BUF (R to 15) and letting BE (R to 15)=1 and others=0. Then, the rME requests Xbar interface master to store BUF to address A\u2212R, with byte enable BE and letting A=A\u2212R+16, and N=N+R\u221216. Then the process proceeds to step  where a check is made to determine whether the next 16B line is the last line (i.e., N<16). If at , it is determined that the next 16B line is the last line, then the rME performs storing the last 16B line and copying the last N bytes to BUF (0 to N\u22121); and letting BE(0 to N\u22121)=1 and others=0 prior to requesting Xbar interface master to store BUF to address A, with byte enable BE. Then the process returns to step  to wait for the next packet arrived. Otherwise, if it is determined at step  that the next 16B line is not the last line, then the process proceeds to  where the rME performs: storing the next 16B line and copying the next 16 payload bytes to BUF (0 to 15) and letting BE(0 to 15)=1 (i.e. all bytes valid) before requesting the Xbar interface master to store BUF to address A, with byte enable BE, Let A=A+16, N=N\u221216. The process then returns to  to make the check of whether the remaining data of the received packet payload does fit in the last line and perform the processing of  if the last line is not being written. Only until the last line of the received packet payload is written to 16B line are steps  and  repeated.","Utilizing notation in FIG. --, a packet payload storage alignment example is provided with respect to FIGS. --A---E. As shown in FIG. --A, twenty (20) bytes of valid payload at network reception FIFO  are to be stored by the rME device to address 30. A goal is thus to store bytes D0, . . . , D19 to address 30, . . . , 49. The rME logic implemented thus initializes variables N=number of valid bytes=20, A=destination address=30 and R=A mod 16=14. Given these values, it is judged whether the data can fit in one 16B line, i.e., is R+N<16. As the valid bytes will not fit in one line, the first 16B line is stored by copying the first 16\u2212R=2 bytes (i.e. D0, D1) to BUF (R to 15), i.e., BUF (14 to 15) then assigning BE (14 to 15)=1 and others=0 as depicted in FIG. --B.","Then, the rME requests the Xbar interface master to store BUF to address A-R=16 (16B-aligned) resulting in byte enable (BE)=000000000000011. As a result, D0 and D1 is stored to correct address 30 and 31 and the variables are re-calculated as: A=A-R+16=32, N=N+R\u221216=18. Then, a further check is performed to determine if the next 16B line is the last N<16 and in this example, the determination would be that the next line is not the last line. Thus, the next line is stored, e.g., by copying the next 16 bytes (D2, . . . , D17) to BUF(0 to 15) and letting BE(0 to 15)=1 as depicted in FIG. --C. Then, the rME requests the Xbar interface master to store BUF to address 32, and byte enable (BE)=1111111111111111. As a result, D2, . . . , D17 are stored to correct address 32 to 47, and the variables are re-calculated as: A=A+16=48, N=N\u221216=2 resulting in N=2, A=48 and R=14. Then, continuing, a determination is made as to whether the next 16B line is the last, i.e., N<16. In this example, the next line is the last line. Thus, the rME initiates storing the last line and copying the last N=2 bytes (i.e. D18, D19) to BUF (0 to N\u22121) i.e. BUF (0 to 1) then letting BE(0 to 1)=1 and others=0 as depicted in FIG. --D. Then, the rME requests the Xbar interface master to store BUF to address A=48 resulting in byte enable (BE)=1100000000000000. Thus, as a result, payload bytes D18 and D19 are stored to address 48 and 49. Now all valid data D0, . . . , D19 have been correctly stored to address 30 . . . 49.","Furthermore, an error correcting code (ECC) capability is provided and an ECC is calculated for each 16B data sent to the Xbar interface master and on byte enables.","In a further aspect of direct put packets, multiple rMEs can receive and process packets belonging to the same message in parallel. Multiple rMEs can also receive and process packets belonging to different messages in parallel.","Further, it is understood that a processor core at the compute node has previously performed operations including: the writing of data into the remote put control SRAM ; and, a polling of the specified byte counter in the memory system until it is updated to a value that indicates message completion.","In the case of remote get packet processing, in one embodiment, the MU device B receives remote get packets that include, in their headers, an injection memory FIFO ID. The imFIFO ID is used to index the ICSRAM . As shown in the MU reception side B- of FIG. --C and the flow method of FIG. --, at  the imFIFO ID indexes ICSRAM to read a tail pointer (address) to the corresponding imFIFO location. This tail pointer is the destination address for that packet. Payload of remote get packet includes one or more descriptors, and these descriptors are appended to the imFIFO by the MU. Then the appended descriptors are processed by the MU injection side. In operation, if multiple reception rMEs try to access the same imFIFO simultaneously, the MU detects conflict between rMEs. Each rME informs the ICSRAM which imFIFO (if any) it is working on. Based on this information, ICSRAM rejects rMEs requesting an imFIFO on which another rME is working.","Further, at , via the Xbar interface master, the rME writes descriptors from the packet payload to the memory system location in the imFIFO pointed to by the corresponding tail pointer read from the ICSRAM. In one example, payload data at the network reception FIFO  is written in 16B chunks or other byte denominations. Then, at , the rME updates the imFIFO tail pointer in the injection control SRAM  so that the imFIFO includes the stored descriptors. The Byte alignment logic  implemented at the rME ensures that the data to be written to the memory system are aligned, in one embodiment, on a 32B boundary for memory FIFO packets. Further in one embodiment, error correction code is calculated for each 16B data sent to the Xbar and on byte enables.","Each rME can be selectively enabled or disabled using a DCR register. For example, an rME is enabled when the corresponding DCR bit is 1 at the DCR register, and disabled when it is 0. If this DCR bit is 0, the rME will stay in the idle state or another wait state until the bit is changed to 1. The software executing on a processor at the node sets a DCR bit. The DCR bits are physically connected to the rMEs via a \u201cbackdoor\u201d access mechanism (not shown). Thus, the register value propagates to rME immediately when it is updated.","If this DCR bit is cleared while the corresponding rME is processing a packet, the rME will continue to operate until it reaches either the idle state or a wait state. Then it will stay in the idle or wait state until the enable bit is set again. When an rME is disabled, even if there are some available packets in the network reception FIFO, the rME will not receive packets from the network reception FIFO. Therefore, all messages received by the network reception FIFO will be blocked until the corresponding rME is enabled again.","When an rME can not store a received packet because the target imFIFO or rmFIFO is full, the rME will poll the FIFO until it has enough free space. More particularly, the rME accesses ICSRAM and when it finds the imFIFO is full, ICSRAM communicates to rME that it is full and can't accept the request. Then rME waits for a while to access the ICSRAM again. This process is repeated until the imFIFO becomes not-full and the rME's request is accepted by ICSRAM. The process is similar when rME accesses reception control SRAM but the rmFIFO is full.","In one aspect, a DCR interrupt will be issued to report the FIFO full condition to the processors on the chip. Upon receiving this interrupt, the software takes action to make free space for the imFIFO\/rmFIFO. (e.g. increasing size, draining packets from rmFIFO, etc.). Software running on the processor on the chip manages the FIFO and makes enough space so that the rME can store the pending packet. Software can freeze rMEs by writing DCR bits to enable\/disable rMEs so that it can safely update FIFO pointers.","Packet Header and Routing","In one embodiment, a packet size may range from 32 to 544 bytes, in increments of 32 bytes. In one example, the first 32 bytes constitute a packet header for an example network packet. As shown in FIG. --, the packet header  includes a first network header portion  (e.g., 12 bytes) as shown in the example network header packet depicted as shown in FIG. --A or a second network header portion \u2032 as shown in the example network header packet depicted as shown in FIG. --B. This header portion may be followed by a message unit header  (e.g., 20 bytes) as shown in FIG. --. The header is then followed by 0 to 16 payload \u201cchunks\u201d, where each chunk contains 32B (bytes) for example. There are two types of network headers: point-to-point and collective. Many of the fields in these two headers are common as will be described herein below.","The first network header portion  as shown in FIG. --A, depicts a first field  identifying the type of packet (e.g., point-to-point and collective packet) which is normally a value set by the software executing at a node. A second field  provides a series of hint bits, e.g., 8 bits, with 1 bit representing a particular direction in which the packet is to be routed (2 bits\/dimension), e.g., directions A\u2212, A+, B\u2212, B+, C\u2212, C+, D\u2212, D+ for a 4-D torus. The next field  includes two further hint bits identifying the \u201cE\u201d dimension for packet routing in a 5-D Torus implementation. Packet header field  further includes a bit indicating whether an interrupt bit has been set by the message unit, depending on a bit in the descriptor. In one embodiment, this bit is set for the last packet of a message (otherwise, it is set to 0, for example). Other bits indicated in Packet header field  may include: a route to I\/O node bit, return from I\/O node, a \u201cuse torus\u201d port bit(s), use I\/O port bit(s), a dynamic bit, and, a deposit bit.","A further field  includes class routes must be defined so that the packet could travel along appropriate links. For example, bits indicated in Packet header field  may include: virtual channel bit (e.g., which bit may have a value to indicate one of the following classes: dynamic, deterministic (escape); high priority; system; user commworld; subcommincator, or, system collective); zone routing id bit(s); and, \u201cstay on bubble\u201d bit.","A further field  includes destination addresses associated with the particular dimension A-E, for example. A further field  includes a value indicating the number (e.g., 0 to 16) of 32 byte data payload chunks added to header, i.e., payload sizes, for each of the memory FIFO packets, put, get or paced-get packets. Other packet header fields indicated as header field  include data bits to indicate the packet alignment (set by MU), a number of valid bytes in payload (e.g., the MU informs the network which is the valid data of those bytes, as set by MU), and, a number of 4B words, for example, that indicate amount of words to skip for injection checksum (set by software). That is, while message payload requests can be issued for 32B, 64B and 128B chunks, data comes back as 32B units via the Xbar interface master, and a message may start at a middle of one of those 32B units. The iME keeps track of this and writes, in the packet header, the alignment that is off-set within the first 32B chunk at which the message starts. Thus, this offset will indicate the portion of the chunk that is to be ignored, and the network device will only parse out the useful portion of the chunk for processing. In this manner, the logic implemented at the network logic can figure out which bytes out of the 32B are the correct ones for the new message. The MU knows how long the packet is (message size or length), and from the alignment and the valid bytes, instructs the Network Interface Unit where to start and end the data injection, i.e., from the 32 Byte payload chunk being transferred to network device for injection. For data reads, the alignment logic located in the network device supports any byte alignment.","As shown in FIG. --B, a network header portion \u2032 depicts a first field  identifying a collective packet, which is normally a value set by the software executing at a node. A second field  provides a series of bits including the collective Opcode indicating the collective operation to be performed. Such collective operations include, for example: and, or, xor, unsigned add, unsigned min, unsigned max, signed add, signed min, signed max, floating point add, floating point minimum, and floating point maximum. It is understood that, in one embodiment, a word length is 8 bytes for floating point operations. A collective word length, in one embodiment, is computed according to B=4*2^n bytes where n is the collective word length exponent. Thus additional bits indicate the collective word length exponent. For example, for floating point operations n=1 (B=8). In one embodiment, the Opcode and word length are ignored for broadcast operation. The next field  includes further bits including an interrupt bit that set by the message unit, depending on a bit in the descriptor. It is only set for the last packet of a message (else 0). Packet header field  further indicates class routes defined so that the packet could travel along appropriate links. These class routes specified, include, for example, virtual channel (VC) (having values indicating dynamic, deterministic (escape), high priority, system, user commworld, user subcommunicator, and, system collective. Further bits indicate collective type routes including (broadcast, reduce, all-reduce, and reserved\/possible point-point over collective route). As in the network packet header a field  includes destination addresses associated with the particular dimension A-E, for example, in a 5-D torus network configuration. In one embodiment, for collective operations, a destination address is used for reduction. A further payload size field  includes a value indicating the number of 32 byte chunks added to header, e.g., payload sizes range from 0B to 512B (32B*16), for example, for each of the memory FIFO packets, put, get or paced-get packets. Another packet header fields indicated as header field  include data bits to indicate the packet alignment (set by MU), a number of valid bytes in payload (e.g., 0 means 512, as set by MU), and, a number of 4 byte words, for example, that indicate amount of words to skip for injection checksum (set by software).","The payload size field specifies number of 32 bytes chunks. Thus payload size is 0B to 512B (32B*16).","Remaining bytes of the each network packet or collective packet header of FIGS. --A, --B are depicted in FIG. -- for each of the memory FIFO, direct put and remote get packets. For the memory FIFO packet header , there is provided a reception memory FIFO ID processed by the MU B- as described herein in connection with FIG. --A. In addition to rmFIFO ID, there is specified the Put Offset value. The Initial value of Put Offset is specified, in one embodiment, by software and updated for each packet by the hardware.","For the case of direct put packets, the direct put packet header  includes bits specifying: a Rec. Payload Base Address ID, Put Offset and a reception Counter ID (e.g., set by software), a number of Valid Bytes in Packet Payload (specifying how many bytes in the payload are actually valid\u2014for example, when the packet has 2 chunks (=32B*2=64B) payload but the number of valid bytes is 35, the first 35 bytes out of 64 bytes payload data is valid; thus, MU reception logic will store only first 35 bytes to the memory system.); and Counter Offset value (e.g., set by software), each such as processed by MU B- as described herein in connection with FIG. --B.","For the case of remote get packets, the remote get packet header  includes the Remote Get Injection FIFO ID such as processed by the MU B- as described herein in connection with FIG. --C.","Interrupt Control","Interrupts and, in one embodiment, interrupt masking for the MU  provide additional functional flexibility. In one embodiment, interrupts may be grouped to target a particular processor on the chip, so that each processor can handle its own interrupt. Alternately, all interrupts can be configured to be directed to a single processor which acts as a \u201cmonitor\u201d of the processors on the chip. The exact configuration can be programmed by software at the node in the way that it writes values into the configuration registers.","In one example, there are multiple interrupt signals  that can be generated from the MU for receipt at the 17 processor cores shown in the compute node embodiment depicted in FIG. --. In one embodiment, there are four interrupts being directed to each processor core , with one interrupt corresponding to each thread, making for a total of 68 interrupts directed from the MU  to the cores. A few aggregated interrupts are targeted to an interrupt controller (Global Event Aggregator or GEA) . The signal interrupts are raised based on three conditions including, but not limited to: an interrupt signaling a packet arrival to a reception memory FIFO, a reception memory FIFO fullness crossing a threshold, or an injection memory FIFO free space crossing a threshold, e.g., injection memory FIFO threshold. In any of these cases, software at the processor core handles the situation appropriately.","For example, MU generated interrupts include: packet arrival interrupts that are raised by MU reception logic when a packet has been received. Using this interrupt, the software being run at the node can know when a message has been received. This interrupt is raised when the interrupt bit in the packet header is set to 1. The application software on the sender node can set this bit as follows: if the interrupt bit in the header in a message descriptor is 1, the MU will set the interrupt bit of the last packet of the message. As a result, this interrupt will be raised when the last packet of the message has been received.","MU generated interrupts further include: imFIFO threshold crossed interrupt that is raised when the free space of an imFIFO exceeds a threshold. The threshold can be specified by a control register in DCR. Using this interrupt, application software can know that an MU has processed descriptors in an imFIFO and there is space to inject new descriptors. This interrupt is not used for an imFIFO that is configured to receive remote get packets.","MU generated interrupts further include: remote get imFIFO threshold crossed interrupt. This interrupt may be raised when the free space of an imFIFO falls below the threshold (specified in DCR). Using this interrupt, the software can notice that MU is running out of free space in the FIFO. Software at the node might take some action to avoid FIFO full (e.g. increasing FIFO size). This interrupt is used only for an imFIFO that is configured to receive remote get packets.","MU generated interrupts further include an rmFIFO threshold crossed interrupt which is similar to the remote get FIFO threshold crossed interrupt; this interrupt to be raised when the free space of an rmFIFO fall below the threshold.","MU generated interrupts further include a remote get imFIFO insufficient space interrupt that is raised when the MU receives a remote get packet but there is no more room in the target imFIFO to store this packet. Software responds by taking some action to clear the FIFO.","MU generated interrupts further include an rmFIFO insufficient space interrupt which may be raised when the MU receives a memory FIFO packet but there is no room in the target rmFIFO to store this packet. Software running at the node may respond by taking some action to make free space. MU generated interrupts further include error interrupts that reports various errors and are not raised under normal operations.","In one example embodiment shown in FIG. --, the interrupts may be coalesced, as follows: within the MU, there is provided, for example, 17 MU groups with each group divided into 4 subgroups. A subgroup consists of 4 reception memory FIFOs (16 FIFOs per group divided by 4) and 8 injection memory FIFOs (32 FIFOs per group divided by 4). Each of the 68 subgroups can generate one interrupt, i.e., the interrupt is raised if any of the three conditions above occurs for any FIFO in the subgroup. The group of four interrupt lines for the same processor core has paired an interrupt status register (not shown) located in the MU's memory mapped I\/O space, thus, providing a total of 17 interrupt status registers, in the embodiment described herein. Each interrupt status register has 64 bits with the following assignments: 16 bits for packet arrived including one bit per reception memory FIFO coupled to that processor core; 16 bits for reception memory FIFO fullness crossed threshold with one bit per reception memory FIFO coupled to that processor core; and, 32 bits for injection memory FIFO free space crossed threshold with one bit per injection memory FIFO coupled to that processor core. For the 16 bits for packet arrival, these bits are set if a packet with interrupt enable bit set is received in the paired reception memory FIFO; for the 16 bits for reception memory FIFO fullness crossed threshold, these bits are used to signal if free space in a FIFO is less than some threshold, which is specified in a DCR register. There is one threshold register for all reception memory FIFOs. This check is performed before a packet is actually stored to FIFO. If the current available space minus the size of the new packet is less than the threshold, this interrupt will be issued. Therefore, if the software reads FIFO pointers just after an interrupt, the observed available FIFO space may not necessarily be less than the threshold. For the 32 bits for injection memory FIFO free space crossed threshold, the bits are used to signal if the free space in the FIFO is larger than the threshold which is specified in the injection threshold register mapped in the DCR address space. There is one threshold register for all injection memory FIFOs. If a paired imFIFO is configured to receive remote get packets, then these bits are used to indicate if the free space in the FIFO is smaller than the \u201cremote get\u201d threshold which is specified in a remote get threshold register mapped in the DCR address space (note that this is a separate threshold register, and this threshold value can be different from both thresholds used for the injection memory FIFOs not configured to receive remote get packets and reception memory FIFOs.)","In addition to these 68 direct interrupts , there may be provided 5 more interrupt lines  with the interrupt: groups 0 to 3 are connected to the first interrupt line, groups 4 to 7 to the second line, groups 8 to 11 to the third interrupt, groups 12 to 15 to the fourth interrupt, and the group 16 is connected to the fifth interrupt line. These five interrupts  are sent to a global event aggregator (GEA)  where they can then be forwarded to any thread on any core.","The MU additionally, may include three DCR mask registers to control which of these 68 direct interrupts participate in raising the five interrupt lines connected to the GEA unit. The three (3) DCR registers, in one embodiment, may have 68 mask bits, and are organized as follows: 32 bits in the first mask register for cores 0 to 7, 32 bits in the second mask register for cores 8 to 15, and 4 mask bits for the 17th core in the third mask register.","In addition to these interrupts, there are additional more interrupt lines  for fatal and nonfatal interrupts signaling more serious errors such as a reception memory FIFO becoming full, fatal errors (e.g., an ECC uncorrectable error), correctable error counts exceeding a threshold, or protection errors. All interrupts are level-based and are not pulsed.","Additionally, software can \u201cmask\u201d interrupts, i.e., program mask registers to raise an interrupt only for particular events, and to ignore other events. Thus, each interrupt can be masked in MU, i.e., software can control whether MU propagates a given interrupt to the processor core, or not. The MU can remember that an interrupt happened even when it is masked. Therefore, if the interrupt is unmasked afterward, the processor core will receive the interrupt.","As for packet arrival and threshold crossed interrupts, they can be masked on a per-FIFO basis. For example, software can mask a threshold crossed interrupt for imFIFO 0, 1, 2, but enable this interrupt for imFIFO 3, et seq.","In one embodiment, direct interrupts  and shared interrupt lines  are available for propagating interrupts from MU to the processor core. Using direct interrupts , each processor core can directly receive packet arrival and threshold crossed interrupts generated at a subset of imFIFOs\/rmFIFOs. For this purpose, there are logic paths directly connect between MU and cores.","For example, a processor core 0 can receive interrupts that happened on imFIFO 0-31 and rmFIFO 0-15. Similarly, core 1 can receive interrupts that happened on imFIFO 32-63 and rmFIFO 16-31. In this example scheme, a processor core N (N=0, . . . , 16) can receive interrupts that happened on imFIFO 32*N to 32*N+31 and rmFIFO 16*N to 16*N+15. Using this mechanism each core can monitor its own subset of imFIFOs\/rmFIFOs which is useful when software manages imFIFOs\/rmFIFOs using 17 cores in parallel. Since no central interrupt control mechanism is involved, direct interrupts are faster than GEA aggregated interrupts as these interrupt lines are dedicated for MU.","Software can identify the source of the interrupt quickly, speeding up interrupt handling. A processor core can ignore interrupts reported via this direct path, i.e., a direct interrupt can be masked using a control register.","As shown in FIG. --, there is a central interrupt controller logic GEA  outside of the MU device. In general GEA interrupts  are delivered to the cores via this controller. Besides the above direct interrupt path, all the MU interrupts share connection to this interrupt controller. This controller delivers MU interrupts to the cores. Software is able to program how to deliver a given interrupt.","Using this controller, a processor core can receive arbitrary interrupts issued by the MU. For example, a core can listen to threshold crossed interrupts on all the imFIFOs and rmFIFOs. It is understood that a core can ignore interrupts coming from this interrupt controller.","As shown in FIG. --A, in one embodiment, to allow simultaneous usage of the same rmFIFO by multiple rMEs, each rmFIFO  further has an associated advance tail , committed tail , and two counters: one advance tail ID counter  associated with advance tail ; and, one committed tail ID counter  associated with the committed tail . An rME includes a DMA engine that copies packets to the memory buffer (e.g., FIFO)  starting at a slot pointed to by an advance tail pointer  in an SRAM memory, e.g., the RCSRAM  (e.g. --A) and obtains an advance tail ID. After the packet is copied to the memory, the rME  checks the committed tail ID to determine if all previously received data for that rmFIFO have been copied. If determined that all previously received data for that rmFIFO have been copied, the rME atomically updates both committed tail and committed tail ID, otherwise it waits. A control logic device  shown in FIG. --A implements logic to manage the memory usage, e.g., manage respective FIFO pointers, to ensure that all store requests for header and payload have been accepted by the interconnect  before atomically updating committed tail (and optionally issuing interrupt). For example, in one embodiment, each rME , . . . , , ensures that all store requests for header and payload have been accepted by the interconnect  before updating commit tail (and, optionally issuing an interrupt). In one embodiment, there are interconnect interface signals issued by the control logic device that tell MU that a store request has been accepted by the interconnect, i.e., an acknowledgement signal. This information is propagated to the respective rMEs. Thus, each rME is able to ensure that all interesting store requests have been accepted by the interconnect. An \u201coptional\u201d interrupt may be used by the software on the cores to track the FIFO free space and may be raised when the available space in an rmFIFO falls below a threshold (such as may be specified in a DCR register). For this interrupting, the control logic  asserts some interrupt lines that are connected to cores (directly or via a GEA (Global Event Aggregator) engine).","In one embodiment, the control logic device  processing may be external to both the L2 cache and MU . Further, in one embodiment, the Reception control SRAM includes associated status and control registers that maintain and atomically update these advance tail ID counter, advance tail, committed tail ID counter, committed tail pointer values in addition to fields maintaining packet \u201cstart\u201d address, \u201csize minus one\u201d and \u201chead\u201d fields.","When a MU wants to read from or write to main memory, it accesses L2 memory controller via the xbar master ports. If the access hits L2, the transaction completes within the L2 and hence no actual memory access is necessary. On the other hand, if it doesn't hit, L2 has to request the memory controller (e.g., DDR-3 Controller , ) to read or write main memory.","FIG. -- illustrates conceptually a reception memory FIFO  or like memory storage area showing a plurality of slots including some completely filled packets  and after the most recent slot pointed to by a commit tail address (commit tail)  and further showing multiple DMA engines (e.g., each from respective rMEs) having placed or placing packets received after the last packet pointed to by the commit tail pointer (last committed packet) in respective locations. The advance tail address (advance tail)  points to the address the next new packet will be stored.","When a DMA engine implemented in a rME wants to store a packet, it obtains from the RCSRAM  the advance tail  which points to the next memory area in that reception memory FIFO  to store a packet (Advance tail address). Then, the advance tail is then moved (incremented) for next packet. The read of advance tail and the increment of advance tail both occur at the same time and cannot be intervened, i.e. they happen atomically. After the DMA at the rME has stored the packet, it requests an atomic update of the Commit tail pointer to indicate that the last address packets have been completely stored. The Commit tail may be referred to by software to know up to where there are completely stored packets in the memory area (e.g., software checks commit tail and the processor may read packets in the main memory up to the commit tail for further processing.) DMAs write commit tail in the same order as they get advance tail. Thus, the commit tail will have the last address correctly. To manage and guarantee this ordering between DMAs, advance ID and commit ID are used.","FIGS. --A to --N depict example scenario for parallel DMA handling of received packets belonging to the same rmFIFO. In an example operation, as shown in FIG. --A, in an initial state, commit tail=advance tail (address 100000), and commit ID=advance ID. The following steps are performed for each rME DMA, I=0, 1, . . . , n), in each MU at a multiprocessor node or system any processing system having more than one DMA engine. The advance tail, advance ID, commit tail, and commit ID are shared among all DMAs.","As exemplified in FIG. --B, DMA0 first requests of the control logic  managing the memory area, e.g., rmFIFO, to stores a 512B packet FIG. --B, and in FIG. --C, the control logic  replies to the rME (DMA 0), to store the packet at the advance tail address, e.g., 100000. Further, the DMA0 is assigned an advance tail ID of \u201c0\u201d, for example. As further shown in FIG. --D, the control logic  managing the memory area atomically updates the advance tail by the amount of bytes of the packet to be stored by DMA) (i.e., (100000+512=100512) and, as part of the same atomic operation, increments the advance tail ID (e.g. now assigned a value of \u201c1\u201d). FIG. --E depicts the DMA0 initiating storing of the packet at address 100000.","As exemplified in FIG. --F, a second DMA element, DMA1, then requests of the control logic  managing the memory area, e.g., rmFIFO, to store a 160B packet FIG. --G, and the control logic  replies to the rME (DMA 0), to store the packet at the advance tail address, e.g., 100512. Further, the DMA1 is assigned an advance tail ID of \u201c1\u201d, for example. As further shown in FIG. --H, the control logic  managing the memory area atomically updates the advance tail by the amount of bytes of the packet to be stored by DMA) (i.e., (100512+160=100672) and, as part of the same atomic operation, increments the advance tail ID (e.g. now assigned a value of \u201c2\u201d). As shown in FIG. --I, DMA1 starts storing the example 160B packet, with both the DMAs operating in parallel. The DMA1 completes storing the 160B packet before DMA0 and tries to update the commit tail before DMA0 by requesting the control logic to update the commit tail address to 100512+160=100672 and informing the control logic  that the DMA1 ID is 1. The control logic  detects that there is a pending DMA write before DMA1 (i.e., DMA0) and replies to DMA1 that commit ID is still 0 and that commit tail cannot be updated and has to wait and attempt subsequently as shown in FIG. --J. Thus, as exemplified, the advance ID and commit ID for the DMAs are used by the control logic to detect this ordering violation. That is, in this detection, the control logic compares the current commit ID with the advance ID the requestor DMA has, i.e., a DMA (rME) obtains the advance ID when it gets advance tail. If there is a pending DMA before the requestor DMA, the commit ID does not match the requestor DMA's advance ID.","Continuing to FIG. --K, it is shown that DMA0 has finished storing the packet and initiates atomic updating the commit tail address, e.g., to 100000+512=100512, for DMA) having ID is 0. FIG. --L shows the updating of the commit tail and incrementing commit ID value. Then, as shown in FIG. --M, the DMA1 tries to update the commit tail again. In this example, the request from DMA1, having a commit ID assigned a value of 1, is to update the commit tail to 100672. This time DMA 1's request is accepted because there is no preceding DMA. Thus, the memory control logic  replies to DMA1 that as the commit ID is 1 that DMA1 can now turn to update commit tail as shown in FIG. --N. Finally commit tail points to the correct location (i.e., next to the area DMA1's packet was stored).","It should be understood that the foregoing described algorithm holds for multiple DMA engine writes in any multiprocessing architecture. It holds even when all DMAs (e.g., DMA0 . . . 15) in respective rMEs configured to operate in parallel. In one embodiment, commit ID and advanced ID are 5 bit counters that roll-over to zero when they overflow. Further, in one embodiment, memory FIFOs are implemented as circular buffers with pointers (e.g. head and tail) that, when updated, must account for circular wrap conditions by using modular arithmetic, for example, to calculate the wrapped pointer address.","FIGS. --A and --B provide a flow chart describing the method  that every DMA (rME) performs in parallel for a general case (i.e. this flow chart holds for any number of DMAs). In a first step , there is performed setting of the \u201ccommit tail\u201d address to the \u201cadvance tail\u201d address and the setting of the \u201ccommit ID\u201d equal to the \u201cadvance ID.\u201d Then, as indicated at and , each ME in MU performs a wait operation, or idle, until a new packet belonging to a message arrives at a reception FIFO to be transferred to the memory.","Once a packet of a particular byte length has arrived at a particular DMA engine (e.g., at an rME), then in , the globally maintained advance tail and advance ID are locally recorded by the DMA engine. Then, as indicated at , the advance tail is set equal to the advance tail+size of the packet being stored in memory, and, at the same time (atomically) advance ID is incremented, i.e., advance ID=advance ID+1, in the embodiment described. The packet is then stored to the memory area pointed to by the locally recorded advance tail in the manner as described herein at . At this point, an attempt is made to update the commit tail and commit tail ID at . Proceeding next to , FIG. --B, a determination is made as to whether the commit ID is equal to the locally recorded advance ID from step  as detected by the control memory logic . If not, the DMA engine having just stored the packet in memory waits at  until the control memory logic has determined that prior stores to that rmFIFO of other DMAs have completed such that the memory control logic has updated commit ID to become equal to the advance ID of the waiting DMA. Then, after the commit ID becomes equal to the advance ID, the commit tail for that DMA engine is atomically updated and set equal to the locally recorded advance tail recorded plus the size of the stored packet, and the commit ID is incremented (atomically with the tail update), i.e., set equal to commit ID+1. Then, the process proceeds back to step , FIG. --A, where the reception FIFO waits for a new packet to arrive.","Thus, in a multiprocessing system comprising parallel operating distributed messaging units (MUs), each with multiple DMAs engines (messaging elements, MEs), packets destined for the same rmFIFO, or packets targeted to the same processor in a multiprocessor system could be received at different DMAs. To achieve high throughput, the packets can be processed in parallel on different DMAs.","FIG. -- is an example of an asymmetrical torus. The shown example is a two-dimensional torus that is longer along one axis, e.g., the y-axis (+\/\u2212y-dimension) and shorter along another axis, e.g., the x-axis (+\/\u2212x-dimension). The size of the torus is defined as (Nx, Ny), where Nx is the number of nodes along the x-axis and Ny is the number of nodes along the y-axis; the total number of nodes in the torus is calculated as Nx*Ny. In the given example, there are six nodes along the x-axis and seven nodes along the y-axis, for a total of 42 nodes in the entire torus. The torus is asymmetrical because the number of nodes along the y-axis is greater than the number of nodes along the x-axis. It is understood that an asymmetrical torus is also possible within a three-dimensional torus having x, y, and z-dimensions, as well as within a five-dimensional torus having a, b, c, d, and e-dimensions.","The asymmetrical torus comprises nodes to . These nodes are also known as \u2018compute nodes\u2019. Each node  occupies a particular point within the torus and is interconnected, directly or indirectly, by a physical wire to every other node within the torus. For example, node is directly connected to node and indirectly connected to node . Multiple connecting paths between nodes  are often possible. A feature of the present invention is a system and method for selecting the \u2018best\u2019 or most efficient path between nodes . In one embodiment, the best path is the path that reduces communication bottlenecks along the links between nodes . A communication bottleneck occurs when a reception FIFO at a receiving node is full and unable to receive a data packet from a sending node. In another embodiment, the best path is the quickest path between nodes  in terms of computational time. Often, the quickest path is also the same path that reduces communication bottlenecks along the links between nodes .","As an example, assume node is a sending node and node is a receiving node. Nodes and are indirectly connected. There exists between these nodes a \u2018best\u2019 path for communicating data packets. In an asymmetrical torus, experiments conducted on the IBM BLUEGENE\u2122 parallel computer system have revealed that the \u2018best\u2019 path is generally found by routing the data packets along the longest dimension first, then continually routing the data across the next longest path, until the data is finally routed across the shortest path to the destination node. In this example, the longest path between node and node is along the y-axis and the shortest path is along the x-axis. Therefore, in this example the \u2018best\u2019 path is found by communicating data along the y-axis from node to node to node to node and then along the x-axis from node node and finally to receiving node . Traversing the torus in this manner, i.e., by moving along the longest available path first, has been shown in experiments to increase the efficiency of communication between nodes in an asymmetrical torus by as much as 40%. These experiments are further discussed in \u201cOptimization of All-to-all Communication on the Blue Gene\/L Supercomputer\u201d 37International Conference on Parallel Processing, IEEE 2008, the contents of which are incorporated by reference in their entirety. In those experiments, packets were first injected into the network and sent to an intermediate node along the longest dimension, where it was received into the memory of the intermediate node. It was then re-injected into the network to the final destination. This requires additional software overhead and requires additional memory bandwidth on the intermediate nodes. The present invention is much more general than this, and requires no receiving and re-injecting of packets at intermediate nodes.","As shown in FIG. --A, the injection FIFO , (where i=1 to 16 for example, see FIG. --) comprises a network logic device  for routing data packets, a hint bit calculator , and data arrays . While only one data array  is shown, it is understood that the injection FIFO  contains a memory for storing multiple data arrays. The data array  further includes data packets  and . The injection FIFO  is coupled to the network DCR . The network DCR is also coupled to the reception FIFO , the receiver , and the sender . A complete description of the DCR architecture is available in IBM's Device Control Register Bus 3.5 Architecture Specifications Jan. 27, 2006, which is incorporated by reference in its entirety. The network logic device  controls the flow of data into and out of the injection FIFO . The network logic device  also functions to apply \u2018mask bits\u2019 supplied from the network DCR  (FIG. --) to hint bits stored in the data packet  as described in further detail below. The hint bit calculator functions to calculate the \u2018hint bits\u2019 that are stored in a data packet  to be injected into the torus network.","The MU  (FIG. --) further includes an Interface to a cross-bar switch (XBAR) switch, or in additional implementations SerDes switches. In one embodiment, the MU  operates at half the clock of the processor core, i.e., 800 MHz. In one embodiment, the Network Device  operates at 500 MHz (e.g., 2 GB\/s network). The MU  includes three (3) XBAR masters  to sustain network traffic and two (2) XBAR slaves  for programming. A DCR slave interface unit  for connecting the DMA DCR unit  to one or more DCR slave registers (not shown) is also provided.","The handover between network device  and MU  is performed via 2-port SRAMs for network injection\/reception FIFOs. The MU  reads\/writes one port using, for example, an 800 MHz clock, and the network reads\/writes the second port with a 500 MHz clock. The only handovers are through the FIFOs and FIFOs' pointers (which are implemented using latches).","FIG. -- is an example of a data packet . There are 2 hint bits per dimension that specify the direction of a of a packet route in that dimension in the data packet header. A data packet routed over a 2-dimensional torus utilizes 4 hint bits. One hint bit represents the \u2018+x\u2019 dimension and another hint bit represents the \u2018\u2212x\u2019 dimension; one hint bit represents the \u2018+y\u2019 dimension and another hint bit represents the \u2018\u2212y\u2019 dimension. A data packet routed over a 3-dimensional torus utilizes 6 hint bits. One hint bit each represents the +\/\u2212x, +\/\u2212y and +\/\u2212z dimensions. A data packet routed over a 5-dimensional torus utilizes 10 hint bits. One hint bit each represents the +\/\u2212a, +\/\u2212b, +\/\u2212c, +\/\u2212d and +\/\u2212e dimensions.","The size of the data packet  may range from 32 to 544 bytes, in increments of 32 bytes. The first 32 bytes of the data packet  form the packet header. The first 12 bytes of the packet header form a network header (bytes 0 to 11); the next 20 bytes form a message unit header (bytes 12 to 31). The remaining bytes (bytes 32 to 543) in the data packet  are the payload \u2018chunks\u2019. In one embodiment, there are up to 16 payload \u2018chunks\u2019, each chunk containing 32 bytes.","Several bytes within the data packet , i.e., byte , byte  and byte  are shown in further detail in FIG. --. In one embodiment of the invention, bytes  and  comprise hint bits for the +\/\u2212a, +\/\u2212b, +\/\u2212c, +\/\u2212d and +\/\u2212e dimensions. In addition, byte  comprises additional routing bits. Byte  comprises bits for selecting a virtual channel (an escape route), i.e., bits , ,  for example, and zone identifier bits. In one embodiment, the zone identifier bits are set by the processor. Zone identifier bits are also known as \u2018selection bits\u2019. The virtual channels prevent communication deadlocks. To prevent deadlocks, the network logic device  may route the data packet on a link in direction of an escape link and an escape virtual channel when movement in the one or more allowable routing directions for the data packet within the network is unavailable. Once a data packet is routed onto the escape virtual channel, if the \u2018stay on bubble\u2019 bit  is set to 1 to keep the data packet on the escape virtual channel towards its final destination. If the \u2018stay on bubble\u2019 bit  is 0, the packet may change back to the dynamic virtual channel and continue to follow the dynamic routing rules as described in this patent application. Details of the escape virtual channel are further discussed in U.S. Pat. No. 7,305,487.","Referring now to FIG. --, bytes ,  and  are described in greater detail. The data packet  includes a virtual channel (VC), a destination address, \u2018hint\u2019 bits and other routing control information. In one embodiment utilizing a five-dimensional torus, the data packet  has 10 hint bits stored in bytes  and , 1 hint bit for each direction (2 bits\/dimension) indicating whether the network device is to route the data packet in that direction. Hint bit  for the \u2018\u2212a\u2019 direction, hint bit  for the \u2018+a\u2019 direction, hint bit  for the \u2018\u2212b\u2019 direction, hint bit  for the \u2018+b\u2019 direction, hint bit  for the \u2018\u2212c\u2019 direction, hint bit  for the \u2018+c\u2019 direction, hint bit  for the \u2018\u2212d\u2019 direction, hint bit  for the \u2018+d\u2019 direction, hint bit  for the \u2018\u2212e\u2019 direction and hint bit  for the \u2018+e\u2019 direction. When the hint bits for a direction are set to 1, in one embodiment the data packet  is allowed to be routed in that direction. For example, if hint bit  is set to 1, then the data packet is allowed to move in the \u2018\u2212a\u2019 direction. It is illegal to set both the plus and minus hint bits for the same dimension. For example, if hint bit  is set to 1 for the \u2018\u2212a\u2019 dimension, then hint bit  for the \u2018+a\u2019 dimension must be set to 0.","A point-to-point packet flows along the directions specified by the hint bits at each node until reaching its final destination. As described in U.S. Pat. No. 7,305,487 the hint bits get modified as the packet flows through the network. When a node reaches its destination in a dimension, the network logic device  changes the hint bits for that dimension to 0, indicating that the packet has reached its destination in that dimension. When all the hint bits are 0, the packet has reached its final destination. An optimization of this permits the hint bit for a dimension to be set to 0 on the node just before it reaches its destination in that dimension. This is accomplished by having a DCR register containing the node's neighbor coordinate in each direction. As the packet is leaving the node on a link, if the data packet's destination in that direction's dimension equals the neighbor coordinate in that direction, the hint bit for that direction is set to 0.","The Injection FIFO  stores data packets that are to be injected into the network interface by the network logic device . The network logic device  parses the data packet to determine in which direction the data packet should move towards its destination, i.e., in a five-dimensional torus the network logic device  determines if the data packet should move along links in the \u2018a\u2019 \u2018b\u2019 \u2018c\u2019 \u2018d\u2019 or \u2018e\u2019 dimensions first by using the hint bits. With dynamic routing, a packet can move in any direction provided the hint bit for direction is set and the usual flow control tokens are available and the link is not otherwise busy. For example, if the \u2018+a\u2019 and \u2018+b\u2019 hint bits are set, then a packet could move in either the \u2018+a\u2019 or \u2018+b\u2019 directions provided tokens and links are available.","Dynamic routing, where the proper routing path is determined at every node, is enabled by setting the \u2018dynamic routing\u2019 bit in the data packet header  to 1. To improve performance on asymmetric tori, \u2018zone\u2019 routing can be used to force dynamic packets down certain dimensions before others. In one embodiment, the data packet  contains 2 zone identifier bits  and , which point to registers in the network DCR unit  (FIG. --) containing the zone masks. These masks are only used when dynamic routing is enabled. The mask bits are programmed into the network DCR  registers by software. The zone identifier set by \u2018zone identifier\u2019 bits  and  are used to select an appropriate mask from the network DCR . In one embodiment, there are five sets of masks for each zone identifier. In one embodiment, there is one corresponding mask bit for each hint bit. In another embodiment, there is half the number of mask bits as there are hint bits, but the mask bits are logically expanded so there is a one-to-one correlation between the mask bits and the hint bits. For example, in a five-dimensional torus if the mask bits are set to 10100, where 1 represents the \u2018a\u2019 dimension, 0 represents the \u2018b\u2019 dimension, 1 represents the \u2018c\u2019 dimension, 0 represents the \u2018d\u2019 dimension, and 0 represents the \u2018e\u2019 dimension, the bits for each dimension are duplicated so that 11 represents the \u2018a\u2019 dimension, 00 represents the \u2018b\u2019 dimension, 11 represents the \u2018c\u2019 dimension, 00 represents the \u2018d\u2019 dimension, and 00 represents the \u2018e\u2019 dimension. The duplication of bits logically expands 10100 to 1100110000 so there are ten corresponding mask bits for each of the ten hint bits.","In one embodiment, the mask also breaks down the torus into \u2018zones\u2019. A zone includes all the allowable directions in which the data packet may move. For example, in a five dimensional torus, if the mask reveals that the data packet is only allowed to move along in the \u2018+a\u2019 and \u2018+e\u2019 dimensions, then the zone includes only the \u2018+a\u2019 and \u2018+e\u2019 dimensions and excludes all the other dimensions.","For selecting a direction or a dimension, the packet's hint bits are AND-ed with the appropriate zone mask to restrict the set of directions that may be chosen. For a given set of zone masks, the first mask is used until the destination in the first dimension is reached. For example, in a 2N\u00d7N\u00d7N\u00d7N\u00d72 torus, where N is an integer such as 16, the masks may be selected in a manner that routes the packets along the \u2018a\u2019 dimension first, then either the \u2018b\u2019 \u2018c\u2019 or \u2018d\u2019 dimensions, and then the \u2018e\u2019 dimension. For random traffic patterns this tends to have packets moving from more busy links onto less busy links. If all the mask bits are set to 1, there is no ordering of dynamic directions. Regardless of the zone bits, a dynamic packet may move to the \u2018bubble\u2019 VC to prevent deadlocks between nodes. In addition, a \u2018stay on bubble\u2019 bit  may be set; if a dynamic packet enters the bubble VC, this bit causes the packet to stay on the bubble VC until reaching its destination.","As an example, in a five-dimensional torus, there are two zone identifier bits and ten hint bits stored in a data packet. The zone identifier bits are used to select a mask from the network DCR . As an example, assume the zone identifier bits  and  are set to \u201800\u2019. In one embodiment, there are up to five masks associated with the zone identifier bits set to \u201800\u2019. A mask is selected by identifying an \u2018operative zone\u2019, i.e., the smallest zone for which both the hint bits and the zone mask are non-zero. The operative zone can be found using equation 1 where in this example m=\u201800\u2019, the set of zone masks corresponding to zone identifier bits \u201800\u2019;\n\nzone =min{&()!=0\u2003\u2003(1)\n","Where j is a variable representing the zone masks for each of the dimensions in the torus, i.e., in a five-dimensional torus k=0 to 4, j varies between 0 and 4 h represents the hint bits and ze_m(j) represents the mask bits, and the \u2018&\u2019 represents a bitwise \u2018AND\u2019 operation.","The following example illustrates how a network logic device  implements equation 1 is used to select an appropriate mask from the network DCR registers. As an example, assume the hint bits are set as \u2018h\u2019=1000100000 corresponding to moves along the \u2018\u2212a\u2019 and the \u2018\u2212c\u2019 dimensions. Assume that three possible masks associated with the zone identifiers bits  and  are stored in the network DCR unit as follows: ze_m(0)=0011001111 (b, d or e moves allowed); ze_m(1)=1100000000 (a moves allowed); and ze_m(2)=0000110000 (c moves allowed).","Network logic device  further applies equation 1 to the hint bits and each individual zone, i.e., ze_m(0), ze_m(1), ze_m(2), reveals the operative zone is found when k=1 because h & ze_m(0)=0, but h& ze_m(1) !=0, i.e., when the hint bits and the mask are \u2018AND\u2019ed together the result is the minimum value that does not equal zero. When j=0, h & ze_m(0)=0, i.e., 1000100000 & 0011001111=0. When j=1, h & ze_m(1)=1000100000 & 1100000000=1000000000. Thus in equation 1, the min j such that h & ze_m(j) !=0 is 1 and so k=1.","After all the moves along the links interconnecting nodes in the \u2018a\u2019 dimension are made, at the last node of the \u2018a\u2019 dimension, as described earlier the logic sets the hint bits for the \u2018a\u2019 dimension to \u201800\u2019 and the hint bits \u2018h\u2019=0000100000, corresponding to moves along the \u2018c\u2019 dimension in the example described. The operative zone is found according to equation 1 when k=2 because \u2018h & ze_m(0)=0\u2019, and \u2018h & ze_m(1)=0\u2019, and \u2018h & ze_m(2) !=0\u2019.","The network logic device  then applies the selected mask to the hint bits to determine which direction to forward the data packet. In one embodiment, the mask bits are \u2018AND\u2019ed with the hint bits to determine the direction of the data packet. Using the example where the mask bits are 1, 0, 1, 0, 0, indicating that moves in the dimensions \u2018a\u2019 or \u2018c\u2019 are allowed. Assume the hint bits are set as follows: hint bit  is set to 1, hint bit  is set to 0, hint bit  is set to 0, hint bit  is set to 0, hint bit  is set to 1, hint bit  is set to 0, hint bit  is set to 0, hint bit  is set to 0, hint bit  is set to 0, and hint bit  is set to 0. The first hint bit , a 1 is \u2018AND\u2019ed with the corresponding mask bit, also a 1 and the output is a 1. The second hint bit , a 0 is \u2018AND\u2019ed with the corresponding mask bit, a 1 and the output is a 0. Application of the mask bits to the hint bits reveals that movement is enabled along \u2018\u2212a\u2019. The remaining hint bits are \u2018AND\u2019ed together with their corresponding mask bits to reveal that movement is enabled along the \u2018\u2212c\u2019 dimension. In this example, the data packet will move along either the \u2018\u2212a\u2019 dimension or the \u2018\u2212c\u2019 dimension towards its final destination. If the data packet first reaches a destination along the \u2018\u2212a\u2019 dimension, then the data packet will continue along the \u2018\u2212c\u2019 dimension towards its destination on the \u2018\u2212c\u2019 dimension. Likewise, if the data packet reaches a destination along the \u2018\u2212c\u2019 dimension then the data packet will continue along the \u2018\u2212a\u2019 dimension towards its destination on the \u2018\u2212a\u2019 dimension.","As a data packet  moves along towards its destination, the hint bits may change. A hint bit is set to 0 when there are no more moves left along a particular dimension. For example, if hint bit  is set to 1, indicating the data packet is allowed to move along the \u2018\u2212a\u2019 direction, then hint bit  is set to 0 once the data packet moves the maximum amount along the \u2018\u2212a\u2019 direction. During the process of routing, it is understood that the data packet may move from a sending node to one or more intermediate nodes before each arriving at the destination node. Each intermediate node that forwards the data packet towards the destination node also functions as a sending node.","In some embodiments, there are multiple longest dimensions and a node chooses between the multiple longest dimensions to selecting a routing direction for the data packet . For example, in a five dimensional torus, dimensions \u2018+a\u2019 and \u2018+e\u2019 may be equally long. Initially, the sending node chooses to between routing the data packet  in a direction along the \u2018+a\u2019 dimension or the \u2018+e\u2019 dimension. A redetermination of which direction the data packet  should travel is made at each intermediate node. At an intermediate node, if \u2018+a\u2019 and \u2018+e\u2019 are still the longest dimensions, then the intermediate node will decide whether to route the data packet  in direction of the \u2018+a\u2019 or \u2018+e\u201d dimensions. The data packet  may continue in direction of the dimension initially chosen, or in direction of any of the other longest dimensions. Once the data packet  has exhausted travel along all of the longest dimensions, a network logic device at an intermediate node sends the data packet in direction of the next longest dimension.","The hint bits are adjusted at each compute node  (FIG. --) as the data packet  moves towards its final destination. In one embodiment, the hint bit is only set to 0 at the next to last node along a particular dimension. For example, if there are 32 nodes along the \u2018+a\u2019 direction, and the data packet  is travelling to its destination on the \u2018+a\u2019 direction, then the hint bit for the \u2018+a\u2019 direction is set to 0 at the 31st node. When the 32nd node is reached, the hint bit for the \u2018+a\u2019 direction is already set to 0 and the data packet  is routed along another dimension as determined by the hint bits, or received at that node if all the hint bits are zero.","In an alternative embodiment, the hint bits need not be explicitly stored in the packet, but the logical equivalence to the hint bits, or \u201cimplied\u201d hint bits can be calculated by the network logic on each node as the packet moves through the network. For example, suppose the packet header contains not the hint bits and destination, but rather the number of remaining hops to make in each dimension and whether the plus or minus direction should be used in each direction (a direction indicator). Then, when a packet reaches a node, the implied hint for a direction is 1 if the number of remaining hops in that dimension is non-zero, and the direction indicator for that dimension is set. Each time the packet makes a move in a dimension, the remaining hop count is decremented is decremented by the network logic device . When the remaining hop count is zero, the packet has reached its destination in that dimension, at which point the implied hint bit is zero.","Referring now to FIG. --, a method for calculating the hint bits is described. The method may be employed by the hardware bit calculator or by a computer readable medium (software running on a processor device at a node). The method is implemented when the data packet  is written to an Injection FIFO buffer  and the hint bits have not yet been set within the data packet, i.e., all the hint bits are zero. This occurs when a new data packet originating from a sending node is placed into the Injection FIFO buffer . A hint bit calculator in the network logic device  reads the network DCR registers , determines the shortest path to the receiving node and sets the hint bits accordingly. In one embodiment, the hint bit calculator calculates the shortest distance to the receiving node in accordance with the method described in the following pseudocode, which is also shown in further detail in FIG. --:",{"@attributes":{"id":"p-1434","num":"1560"},"tables":{"@attributes":{"id":"TABLE-US-00013","num":"00013"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"7pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"210pt","align":"left"}}],"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["\u2003","If src[d] == dest[d] hint bits in dimension d are 0"]},{"entry":[{},"if (dest[d] > src[d] )"]},{"entry":[{},"{"]},{"entry":[{},"if ( dest[d] <= cutoff_plus[d]) hint bits in dimension d is set to plus"]},{"entry":[{},"else hint bits in dimension d = minus"]},{"entry":[{},"}"]},{"entry":[{},"if (dest[d] < src[d] )"]},{"entry":[{},"{"]},{"entry":[{},"if ( dest[d] >= cutoff_minus[d]) hint bits in dimension d is set to minus"]},{"entry":[{},"else hint bits in dimension d = plus}"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},"Where d is a selected dimension, e.g., \u2018+\/\u2212x\u2019, \u2018+\/\u2212y\u2019, \u2018+\/\u2212z\u2019 or \u2018+\/\u2212a\u2019, \u2018+\/\u2212b\u2019, \u2018+\/\u2212c\u2019, \u2018+\/\u2212d\u2019, \u2018+\/\u2212e\u2019; and cutoff_plus[d] and cutoff_minus[d] are software controlled programmable cutoff registers that store values that represent the endpoints of the selected dimension. The hint bits are recalculated and rewritten to the data packet  by the network logic device  as the data packet  moves towards its destination. Once the data packet  reaches the receiving node, i.e., the final destination address, all the hint bits are set to 0, indicating that the data packet  should not be forwarded.","The method starts at block . At block , if a node along the source dimension is equal to a node along the dimension, then the data packet has already reached its destination on that particular dimension and the data packet does not need to be forwarded any further along that one dimension. If this situation is true, then at block  all of the hint bits for that dimension are set to zero by the hint bit calculator and the method ends. If the node along the source dimension is not equal to the node along the destination dimension, then the method proceeds to step . At step , if the node along the destination dimension is greater than the node along the source dimension, e.g., the destination node is in a positive direction from the source node, then method moves to block . If the node along the destination dimension is not greater than the source node, e.g., the destination node is in a negative direction from the source node, then method proceeds to block .","At block , a determination is made as to whether the destination dimension is greater than or equal to a value stored in the cutoff_minus register. The plus and minus cutoff registers are programmed in such a way that a packet will take the smallest number of hops in each dimension If the destination dimension is greater than or equal to the value stored in the cutoff_minus register, then the method proceeds to block  and the hint bits are set so that the data packet  is routed in a negative direction for that particular dimension. If the destination dimension is not greater than or equal to the value stored in the cutoff_plus register, then the method proceeds to block  and the hint bits are set so the data packet  is routed in a positive dimension for that particular dimension.","At block , a determination is made as to whether the destination dimension is less than or equal to a value stored in the cutoff_plus register. If the destination dimension is less than or equal to the value stored in the cutoff_plus register, then the method proceeds to block  and the hint bits are set so that the data packet is routed in a positive direction for that particular dimension. If the destination dimension is not less than or equal to the value stored in the cutoff_plus register, then the method proceeds to block  and the hint are set so that the data packet  is routed in a negative direction for that particular dimension.","The above method is repeated for each dimension to set the hint bits for that particular dimension, i.e., in a five-dimensional torus the method is implemented once for each of the \u2018a\u2019, \u2018b\u2019, \u2018c\u2019, \u2018d\u2019, and \u2018e\u2019 dimensions.","Network Support for System Initiated Checkpoint","In parallel computing system, such as BlueGene\u00ae (a trademark of International Business Machines Corporation, Armonk N.Y.), system messages are initiated by the operating system of a compute node. They could be messages communicated between the Operating System (OS) kernel on two different compute nodes, or they could be file I\/O messages, e.g., such as when a compute node performs a \u201cprintf\u201d function, which gets translated into one or more messages between the OS on a compute node OS and the OS on (one or more) I\/O nodes of the parallel computing system. In highly parallel computing systems, a plurality of processing nodes may be interconnected to form a network, such as a Torus; or, alternately, may interface with an external communications network for transmitting or receiving messages, e.g., in the form of packets.","As known, a checkpoint refers to a designated place in a program at which normal processing is interrupted specifically to preserve the status information, e.g., to allow resumption of processing at a later time. Checkpointing, is the process of saving the status information. While checkpointing in high performance parallel computing systems is available, generally, in such parallel computing systems, checkpoints are initiated by a user application or program running on a compute node that implements an explicit start checkpointing command, typically when there is no on-going user messaging activity. That is, in prior art user-initiated checkpointing, user code is engineered to take checkpoints at proper times, e.g., when network is empty, no user packets in transit, or MPI call is finished.","In one aspect t is desirable to have the computing system initiate checkpoints, even in the presence of on-going messaging activity. Further, it must be ensured that all incomplete user messages at the time of the checkpoint be delivered in the correct order after the checkpoint. To further complicate matters, the system may need to use the same network as is used for transferring system messages.","In one aspect, a system and method for checkpointing in parallel, or distributed or multiprocessor-based computer systems is provided that enables system initiation of checkpointing, even in the presence of messaging, at arbitrary times and in a manner invisible to any running user program.","In this aspect, it is ensured that all incomplete user messages at the time of the checkpoint be delivered in the correct order after the checkpoint. Moreover, in some instances, the system may need to use the same network as is used for transferring system messages.","The system, method and computer program product supports checkpointing in a parallel computing system having multiple nodes configured as a network, and, wherein the system, method and computer program product in particular, obtains system initiated checkpoints, even in the presence of on-going user message activity in a network.","As there is provided a separation of network resources and DMA hardware resources used for sending the system messages and user messages, in one embodiment, all user and system messaging be stopped just prior to the start of the checkpoint. In another embodiment, only user messaging be stopped prior to the start of the checkpoint.","Thus, there is provided a system for checkpointing data in a parallel computing system having a plurality of computing nodes, each node having one or more processors and network interface devices for communicating over a network, the checkpointing system comprising: one or more network elements interconnecting the network interface devices of computing nodes via links to form a network; a control device to communicate control signals to each the computing node of the network for stopping receiving and sending message packets at a node, and to communicate further control signals to each the one or more network elements for stopping flow of message packets within the formed network; and, a control unit, at each computing node and at one or more the network elements, responsive to a first control signal to stop each of the network interface devices involved with processing of packets in the formed network, and, to stop a flow of packets communicated on links between nodes of the network; and, the control unit, at each node and the one or more network elements, responsive to second control signal to obtain, from each the plurality of network interface devices, data included in the packets currently being processed, and to obtain from the one or more network elements, current network state information, and, a memory storage device adapted to temporarily store the obtained packet data and the obtained network state information.","As described herein with respect to FIG. --, the herein referred to Messaging Unit  implements plural direct memory access engines to offload the network interface . In one embodiment, it transfers blocks via three switch master ports  between the L2-caches  () and the reception FIFOs  and transmission FIFOs  of the network interface unit . The MU is additionally controlled by the cores via memory mapped I\/O access through an additional switch slave port .","One function of the messaging unit  is to ensure optimal data movement to, and from, the network into the local memory system for the node by supporting injection and reception of message packets. As shown in FIG. --, in the network interface  the injection FIFOs  and reception FIFOs  (sixteen for example) each comprise a network logic device for communicating signals used for controlling routing data packets, and a memory for storing multiple data arrays. Each injection FIFOs  is associated with and coupled to a respective network sender device (where n=1 to 16 for example), each for sending message packets to a node, and each network reception FIFOs  is associated with and coupled to a respective network receiver device (where n=1 to 16 for example), each for receiving message packets from a node. Each sender  also accepts packets routing through the node from receivers . A network DCR (device control register)  is provided that is coupled to the injection FIFOs , reception FIFOs , and respective network receivers , and network senders . A complete description of the DCR architecture is available in IBM's Device Control Register Bus 3.5 Architecture Specifications Jan. 27, 2006, which is incorporated by reference in its entirety. The network logic device controls the flow of data into and out of the injection FIFO  and also functions to apply \u2018mask bits\u2019, e.g., as supplied from the network DCR . In one embodiment, the iME elements communicate with the network FIFOs in the Network interface unit  and receives signals from the network reception FIFOs  to indicate, for example, receipt of a packet. It generates all signals needed to read the packet from the network reception FIFOs . This network interface unit  further provides signals from the network device that indicate whether or not there is space in the network injection FIFOs  for transmitting a packet to the network and can be configured to also write data to the selected network injection FIFOs.","The MU  further supports data prefetching into the memory, and on-chip memory copy. On the injection side, the MU splits and packages messages into network packets, and sends packets to the network respecting the network protocol. On packet injection, the messaging unit distinguishes between packet injection, and memory prefetching packets based on certain control bits in its memory descriptor, e.g., such as a least significant bit of a byte of a descriptor  shown in FIG. --. A memory prefetch mode is supported in which the MU fetches a message into L2, but does not send it. On the reception side, it receives packets from a network, and writes them into the appropriate location in memory, depending on the network protocol. On packet reception, the messaging unit  distinguishes between three different types of packets, and accordingly performs different operations. The types of packets supported are: memory FIFO packets, direct put packets, and remote get packets.","With respect to on-chip local memory copy operation, the MU copies content of an area in the local memory to another area in the memory. For memory-to-memory on chip data transfer, a dedicated SRAM buffer, located in the network device, is used.","FIG. -- particularly, depicts the system elements involved for checkpointing at one node  of a multi processor system, such as shown in . While the processing described herein is with respect to a single node, it is understood that the description is applicable to each node of a multiprocessor system and may be implemented in parallel, at many nodes simultaneously. For example, FIG. -- illustrates a detailed description of a DCR control Unit  that includes DCR (control and status) registers for the MU , and that may be distributed to include (control and status) registers  for the network device (ND)  shown in FIG. --. In one embodiment, there may be several different DCR units including logic for controlling\/describing different logic components (i.e., sub-units). In one implementation, the DCR units  may be connected in a ring, i.e., processor read\/write DCR commands are communicated along the ring\u2014if the address of the command is within the range of this DCR unit, it performs the operation, otherwise it just passes through.","As shown in FIG. --, DCR control Unit  includes a DCR interface control device  that interfaces with a DCR processor interface bus , . In operation, a processor at that node issues read\/write commands over the DCR Processor Interface Bus which commands are received and decoded by DCR Interface Control logic implemented in the DCR interface control device  that reads\/writes the correct register, i.e., address within the DCR Unit . In the embodiment depicted, the DCR unit  includes control registers  and corresponding logic, status registers  and corresponding logic, and, further implements DCR Array \u201cbackdoor\u201d access logic . The DCR control device  communicates with each of these elements via Interface Bus . Although these elements are shown in a single unit, as mentioned herein above, these DCR unit elements can be distributed throughout the node. The Control registers  affect the various subunits in the MU  or ND . For example, Control registers may be programmed and used to issue respective stop\/start signals , . . . N over respective conductor lines, for initiating starting or stopping of corresponding particular subunit(s) i, e.g., subunit , . . . , (where N is an integer number) in the MU  or ND . Likewise, DCR Status registers  receive signals , . . . , over respective conductor lines that reflect the status of each of the subunits, e.g., , . . . , , from each subunit's state machine , . . . , , respectively. Moreover, the array backdoor access logic  of the DCR unit  permits processors to read\/write the internal arrays within each subunit, e.g., arrays , . . . , corresponding to subunits , . . . , . Normally, these internal arrays , . . . , within each subunit are modified by corresponding state machine control logic , . . . , implemented at each respective subunit. Data from the internal arrays , . . . , are provided to the array backdoor access logic  unit along respective conductor lines , . . . , . For example, in one embodiment, if a processor issued command is a write, the \u201cvalue to write\u201d is written into the subunit id's \u201caddress in subunit\u201d, and, similarly, if the command is a read, the contents of \u201caddress in subunit\u201d from the subunit id is returned in the value to read.","In one embodiment of a multiprocessor system node, such as described herein, there may be a clean separation of network and Messaging Unit (DMA) hardware resources used by system and user messages. In one example, users and systems are provided to have different virtual channels assigned, and different messaging sub-units such as network and MU injection memory FIFOs, reception FIFOs, and internal network FIFOs. FIG. -- shows a receiver block in the network logic unit  in FIG. --. In one embodiment of the BlueGene\/Q network design, each receiver has 6 virtual channels (VCs), each with 4 KB of buffer space to hold network packets. There are 3 user VCs (dynamic, deterministic, high-priority) and a system VC for point-to-point network packets. In addition, there are 2 collective VCs, one can be used for user or system collective packets, the other for user collective packets. In one embodiment of the checkpointing scheme of the present invention, when the network system VCs share resources with user VCs, for example, as shown in FIG. --, both user and system packets share a single 8 KB retransmission FIFO  for retransmitting packets when there are link errors. It is then desirable that all system messaging has stopped just prior to the start of the checkpoint. In one embodiment, the present invention supports a method for system initiated checkpoint as now described with respect to FIGS. --A to --B.","FIGS. --A to --B depict an example flow diagram depicting a method  for checkpoint support in a multiprocessor system, such as shown in . As shown in FIG. --A, a first step  is a step for a host computing system e.g., a designated processor core at a node in the host control system, or a dedicated controlling node(s), to issue a broadcast signal to each node's O\/S to initiate taking of the checkpoint amongst the nodes. The user program executing at the node is suspended. Then, as shown in FIG. --A, at , in response to receipt of the broadcast signal to the relevant system compute nodes, the O\/S operating at each node will initiate stopping of all unit(s) involved with message passing operations, e.g., at the MU and network device and various sub-units thereof.","Thus, for example, at each node(s), the DCR control unit for the MU  and network device  is configured to issue respective stop\/start signals , . . . N over respective conductor lines, for initiating starting or stopping of corresponding particular subunit(s), e.g., subunit , . . . , . In an embodiment described herein, for checkpointing, the sub-units to be stopped may include all injection and reception sub-units of the MU (DMA) and network device. For example, in one example embodiment, there is a Start\/stop DCR control signal, e.g., a set bit, associated with each of the iMEs , rMEs  (FIG. --), injection control FSM (finite state machine), Input Control FSM, and all the state machines that control injection and reception of packets. Once stopped, new packets cannot be injected into the network or received from the network.","For example, each iME and rME can be selectively enabled or disabled using a DCR register. For example, an iME\/rME is enabled when the corresponding DCR bit is 1 at the DCR register, and disabled when it is 0. If this DCR bit is 0, the rME will stay in the idle state or another wait state until the bit is changed to 1. The software executing on a processor at the node sets a DCR bit. The DCR bits are physically connected to the iME\/rMEs via a \u201cbackdoor\u201d access mechanism including separate read\/write access ports to buffers arrays, registers, and state machines, etc. within the MU and Network Device. Thus, the register value propagates to iME\/rME registers immediately when it is updated.","The control or DCR unit may thus be programmed to set a Start\/stop DCR control bit provided as a respective stop\/start signal , . . . , N corresponding to the network injection FIFOs to enable stop of all network injection FIFOs. As there is a DCR control bit for each subunit, these bits get fed to the appropriate iME FSM logic which will, in one embodiment, complete any packet in progress and then prevent work on subsequent packets. Once stopped, new packets will not be injected into the network. Each network injection FIFO can be started\/stopped independently.","As shown in FIG. -- illustrating the referred to backdoor access mechanism, a network DCR register  is shown coupled over conductor or data bus  with one injection FIFO , (where i=1 to 16 for example) that includes a network logic device  used for the routing of data packets stored in data arrays , and including controlling the flow of data into and out of the injection FIFO , and, for accessing data within the register array for purposes of checkpointing via an internal DCR bus. While only one data array  is shown, it is understood that each injection FIFO may contain multiple memory arrays for storing multiple network packets, e.g., for injecting packets  and .","Further, the control or DCR unit sets a Start\/stop DCR control bit provided as a respective stop\/start signal , . . . N corresponding to network reception FIFOs to enable stop of all network reception FIFOs. Once stopped, new packets cannot be removed from the network reception FIFOs. Each FIFO can be started\/stopped independently. That is, as there is a DCR control bit for each subunit, these bits get fed to the appropriate FSM logic which will, in one embodiment, complete any packet in progress and then prevent work on subsequent packets. It is understood that a network DCR register  shown in FIG. -- is likewise coupled to each reception FIFO for controlling the flow of data into and out of the reception FIFO (FIG. --), and, for accessing data within the register array for purposes of checkpointing.","In an example embodiment, for the case of packet reception, if this DCR stop bit is set to logic 1, for example, while the corresponding rME is processing a packet, the rME will continue to operate until it reaches either the idle state or a wait state. Then it will stay in the state until the stop bit is removed, or set to logic 0, for example. When an rME is disabled (e.g., stop bit set to 1), even if there are some available packets in the network device's reception FIFO, the rME will not receive packets from the network FIFO. Therefore, all messages received by the network FIFO will be blocked until the corresponding rME is enabled again.","Further, the control or DCR unit sets a Start\/stop DCR control bit provided as a respective stop\/start signal , . . . N corresponding to all network sender and receiver units such as sender units -and receiver units -shown in FIG. --. FIG. --A, particularly depicts DCR control registers  at predetermined addresses, some associated for user and system use, having a bit set to stop operation of Sender Units, Receiver Units, Injection FIFOs, Rejection FIFOs. That is, a stop\/start signal may be issued for stop\/starting all network sender and receiver units. Each sender and receiver can be started\/stopped independently. FIG. --A and FIG. --B depicts example (DCR) control registers  that support Injection\/\/Reception FIFO control at the network device (FIG. --A) used in stopping packet processing, and, example control registers  that support resetting Injection\/\/Reception FIFOs at the network device (FIG. --B). FIG. --C depicts example (DCR) control registers  that are used to stop\/start state machines and arrays associated with each link's send (Network Sender units) and receive logic (Receiver units) at the network device  for checkpointing.","In the system shown in , there may be employed a separate external host control network  that may include Ethernet and\/or JTAG [(Joint Test Action Group) IEEE Std 1149.1-1990)] control network interfaces, that permits communication between the control host and computing nodes to implement a separate control host barrier. Alternately, a single node or designated processor at one of the nodes may be designated as a host for purposes of taking checkpoints.","That is, the system of the invention may have a separate control network, wherein each compute node signals a \u201cbarrier entered\u201d message to the control network, and it waits until receiving a \u201cbarrier completed\u201d message from the control system. The control system implemented may send such messages after receiving respective barrier entered messages from all participating nodes.","Thus, continuing in FIG. --A, after initiating checkpoint at , the control system then polls each node to determine whether they entered the first barrier. At each computing node, when all appropriate sub-units in that node have been stopped, and when all packets can no longer move in the network (message packet operations at each node cease), e.g., by checking state machines, at , FIG. --A, the node will enter the first barrier. When all nodes entered the barrier, the control system then broadcasts a barrier done message through the control network to each node. At , the node determines whether all process nodes of the network subject to the checkpoint have entered the first barrier. If all process nodes subject to the checkpoint have not entered the first barrier, then, in one embodiment, the checkpoint process waits at  until each of the remaining nodes being processed have reached the first barrier. For example, if there are retransmission FIFOs for link-level retries, it is determined when the retransmission FIFOs are empty. That is, as a packet is sent from one node to another, a copy is put into a retransmission FIFO. According to a protocol, a packet is removed from retransmission FIFO when acknowledgement comes back. If no acks come back for a predetermined timeout period, packets from the retransmission FIFO are retransmitted in the same order to the next node.","As mentioned, each node includes \u201cstate machine\u201d registers (not shown) at the network and MU devices. These state machine registers include unit status information such as, but not limited to, FIFO active, FIFO currently in use (e.g., for remote get operation), and whether a message is being processed or not. These status registers can further be read (and written to) by system software at the host or controller node.","Thus, when it has been determined at the computer nodes forming a network (e.g., a Torus or collective) to be checkpointed that all user programs have been halted, and all packets have stopped moving according to the embodiment described herein, then, as shown at step , FIG. --A, each node of the network is commanded to store and read out the internal state of the network and MU, including all, packets in transit. This may be performed at each node using a \u201cbackdoor\u201d read mechanism. That is, the \u201cbackdoor\u201d access devices perform read\/write to all internal MU and network registers and buffers for reading out from register\/SRAM buffer contents\/state machines\/link level sequence numbers at known backdoor access address locations within the node, when performing the checkpoint and, eventually write the checkpoint data to external storage devices such as hard disks, tapes, and\/or non-volatile memory. The backdoor read further provides access to all the FSM registers and the contents of all internal SRAMS, buffer contents and\/or register arrays.","In one embodiment, these registers may include packets ECC or parity data, as well as network link level sequence numbers, VC tokens, state machine states (e.g., status of packets in network), etc., that can be read and written. In one embodiment, the checkpoint reads\/writes are read by operating system software running on each node. Access to devices is performed over a DCR bus that permits access to internal SRAM or state machine registers and register arrays, and state machine logic, in the MU and network device, etc. as shown in FIGS. -- and --. In this manner, a snapshot of the entire network including MU and networked devices, is generated for storage.","Returning to FIG. --A, at , it is determined whether all checkpoint data and internal node state and system packet data for each node, has been read out and stored to the appropriate memory storage, e.g., external storage. For example, via the control network if implemented, or a supervising host node within the configured network, e.g., Torus, each compute node signals a \u201cbarrier entered\u201d message (called the 2barrier) once all checkpoint data has been read out and stored. If all process nodes subject to the checkpoint have not entered the 2barrier, then, in one embodiment, the checkpoint process waits at  until each of the remaining nodes being processed have entered the second barrier, upon which time checkpointing proceeds to step  FIG. --B.","Proceeding to step , FIG. --B, it is determined by the compute node architecture whether the computer nodes forming a network (e.g., a Torus or collective) to be checkpointed permits selective restarting of system only units as both system and users may employ separate dedicated resources (e.g., separate FIFOs, separate Virtual Channels). For example, FIG. -- shows an implementation of a retransmission FIFO  in the network sender  logic where the retransmission network packet buffers are shared between user and system packets. In this architecture, it is not possible to reset the network resources related to user packets separately from system packets, and therefore the result of step  is a \u201cno\u201d and the process proceeds to step .","In another implementation of the network sender \u2032 illustrated in FIG. --, user packets and system packets have respective separated retransmission FIFOs ,  respectively, that can be reset independently. There are also separate link level packet sequence numbers for user and system traffic. In this latter case, thus, it is possible to reset the logic related to user packets without disturbing the flow of system packets, thus the result of step  is \u201cyes\u201d. Then the logic is allowed to continue processing system only packets via backdoor DCR access to enable network logic to process system network packets. With a configuration of hardware, i.e., logic and supporting registers that support selective re-starting, then at , the system may release all pending system packets and start sending the network\/MU state for checkpointing over the network to an external system for storing to disk, for example, while the network continues running, obviating the need for a network reset. This is due to additional hardware engineered logic forming an independent system channel which means the checkpointed data of the user application as well as the network status for the user channels can be sent through the system channel over the same high speed torus or collective network without needing a reset of the network itself.","For restarting, there is performed setting the unit stop DCR bits to logic \u201c0\u201d, for example, bits in DCR control register  (e.g., FIG. --A) and permitting the network logic to continue working on the next packet, if any. To perform the checkpoint may require sending messages over the network. Thus, in one embodiment, there is permitted only system packets, those involved in the checkpointing, to proceed. The user resources, still remain halted in the embodiment employing selective restarting.","Returning to FIG. --B, if, at step , it is determined that such a selective restart is not feasible, the network and MU are reset in a coordinated fashion at  to remove all packets in network.","Thus, if selective re-start can not be performed, then the entire network is Reset which effectively rids the network of all packets (e.g., user and system packets) in network. After the network reset, only system packets will be utilized by the OS running on the compute node. Subsequently, the system using the network would send out information about the user code and program and MU\/network status and writes that to disk, i.e., the necessary network, MU and user information is checkpointed (written out to external memory storage, e.g., disk) using the freshly reset network. The user code information including the network and MU status information is additionally checkpointed.","Then, all other user state, such as user program, main memory used by the user program, processor register contents and program control information, and other checkpointing items defining the state of the user program, are checkpointed. For example, as memory is the content of all user program memory, i.e., all the variables, stacks, heap is checkpointed. Registers include, for example, the core's fixed and floating point registers and program counter. The checkpoint data is written to stable storage such as disk or a flash memory, possibly by sending system packets to other compute or I\/O nodes. This is so the user application is later restarted at the exactly same state it was in.","In one aspect, these contents and other checkpointing data are written to a checkpoint file, for example, at a memory buffer on the node, and subsequently written out in system packets to, for example, additional I\/O nodes or control host computer, where they could be written to disk, attached hard-drive optical, magnetic, volatile or non-volatile memory storage devices, for example. In one embodiment the checkpointing may be performed in a non-volatile memory (e.g., flash memory, phase-change memory, etc) based system, i.e., with checkpoint data and internal node state data expediently stored in a non-volatile memory implemented on the computer node, e.g., before and\/or in addition to being written out to I\/O. The checkpointing data at a node could further be written to possibly other nodes where stored in local memory\/flash memory.","Continuing, after user data is checkpointed, at , FIG. --B, the backdoor access devices are utilized, at each node, to restore the network and MU to their exact user states at the time of the start of the checkpoint. This entails writing all of the checkpointed data back to the proper registers in the units\/sub-units using the read\/write access. Then the user program, network and MU are restarted from the checkpoint. If an error occurs between checkpoints (e.g., ECC shows uncorrectable error, or a crash occurs), such that the application must be restarted from a previous checkpoint, the system can reload user memory and reset the network and MU state to be identical to that at the time of the checkpoint, and the units can be restarted.","After restoring the network state at each node, a call is made to a third barrier. The system thus ensures that all nodes have entered the barrier after each node's state has restored from a checkpoint (i.e., have read from stable storage and restored user application and network data and state. The system will wait until each node has entered the third data barrier such as shown at steps ,  before resuming processing.","From the foregoing, the system and methodology can re-start the user application at exactly the same state in which it was in at time of entering the checkpoint. With the addition of system checkpoints, in the manner as described herein checkpointing can be performed anytime while a user application is still running.","In an alternate embodiment, two external barriers could be implemented, for example, in a scenario where system checkpoint is taken and the hardware logic is engineered so as not to have to perform a network reset, i.e., system is unaffected while checkpointing user. That is, after first global barrier is entered upon halting all activity, the nodes may perform checkpoint read step using backdoor access feature, and write checkpoint data to storage array or remote disk via the hardware channel. Then, these nodes will not need to enter or call the second barrier after taking checkpoint due to the use of separate built in communication channel (such as a Virtual Channel). These nodes will then enter a next barrier (the third barrier as shown in FIG. --B) after writing the checkpoint data.","The present invention can be embodied in a system in which there are compute nodes and separate networking hardware (switches or routers) that may be on different physical chips. For example, network configuration shown in FIG. --A in greater detail, show an inter-connection of separate network chips, e.g., router and\/or switch devices , , . . . , , i.e., separate physical chips interconnected via communication links . Each of the nodes (), . . . , () connect with the separate network of network chips and links forming network, such as a multi-level switch \u2032, e.g., a fat-tree. Such network chips may or may not include a processor that can be used to read and write the necessary network control state and packet data. If such a processor is not included on the network chip, then the necessary steps normally performed by a processor can instead be performed by the control system using appropriate control access such as over a separate JTAG or Ethernet network  as shown in FIG. --A. For example, control signals  for conducting network checkpointing of such network elements (e.g., router and switches , , . . . , ) and nodes (), . . . , () are communicated via control network . Although a single control network connection is shown in FIG. --A, it is understood that control signals  are communicated with each network element in the network \u2032. In such an alternative network topology, the network \u2032 shown in FIG. --A, may comprise or include a cross-bar switch network, where there are both compute nodes (), . . . , () and separate switch chips , , . . . , \u2014the switch chip including only network receivers, senders and associate routing logic, for example. There may additionally be some different control processors in the switch chip also. In this implementation, the system and method stop packets in both the compute node and the switch chips.","In the further embodiment of a network configuration \u2033 shown in FIG. --B, a 2D Torus configuration is shown, where a compute node (), . . . , () comprises a processor(s), memory, network interface such as shown in . However, in the network configuration \u2032, the compute node may further include a router device, e.g., on the same physical chip, or, the router (and\/or switch) may reside physically on another chip. In the embodiment where the router (and\/or switch) resides physically on another chip, the network includes an inter-connection of separate network elements, e.g., router and\/or switch devices , , . . . , , shown connecting one or more compute nodes (), . . . , (), on separate chips interconnected via communication links  to form an example 2D Torus. Control signals  from control network may be communicated to each of the nodes and network elements, with one signal being shown interfacing control network  with one compute node () for illustrative purposes. These signals enable packets in both the compute node and the switch chips to be stopped\/started and checkpoint data read according to logic implemented in the system and method. It is understood that control signals  may be communicated to each network element in the network \u2033. Thus, in one embodiment, the information about packets and state is sent over the control network  for storage over the control network by the control system. When the information about packets and state needs to be restored, it is sent back over the control network and put in the appropriate registers\/SRAMS included in the network chip(s).","Further, the entire machine may be partitioned into subpartitions each running different user applications. If such subpartitions share network hardware resources in such a way that each subpartition has different, independent network input (receiver) and output (sender) ports, then the present invention can be embodied in a system in which the checkpointing of one subpartition only involves the physical ports corresponding to that subpartition. If such subpartitions do share network input and output ports, then the present invention may be embodied in a system in which the network can be stopped, checkpointed and restored, but only the user application running in the subpartition to be checkpointed is checkpointed while the applications in the other subpartitions continue to run.","Programs running on large parallel computer systems often save the state of long running calculations at predetermined intervals. This saved data is called a checkpoint. This process enables restarting the calculation from a saved checkpoint after a program interruption due to soft errors, hardware or software failures, machine maintenance or reconfiguration. Large parallel computers are often reconfigured, for example to allow multiple jobs on smaller partitions for software development, or larger partitions for extended production runs.","A typical checkpoint requires saving data from a relatively large fraction of the memory available on each processor. Writing these checkpoints can be a slow process for a highly parallel machine with limited I\/O bandwidth to file servers. The optimum checkpoint interval for reliability and utilization depends on the problem data size, expected failure rate, and the time required to write the checkpoint to storage. Reducing the time required to write a checkpoint improves system performance and availability.","Thus, it is desired to provide a system and method for increasing the speed and efficiency of a checkpoint process performed at a computing node of a computing system, such as a massively parallel computing system.","In one aspect, there is provided a system and method for increasing the speed and efficiency of a checkpoint process performed at a computing node of a computing system by integrating a non-volatile memory device, e.g., flash memory cards, with a direct interface to the processor and memory that make up each parallel computing node.","This flash memory provides a local storage for checkpoints thus relieving the bottleneck due to I\/O bandwidth limitations. Simple available interfaces from the processor such as ATA or UDMA that are supported by commodity flash cards provide sufficient bandwidth to the flash memory for writing checkpoints. For example, a multiple GB checkpoint can be written to local flash at 20 MB\/s to 40 MB\/s in a few minutes. All processors writing the same data through normal I\/O channels could take more than 10\u00d7 as long. An example implementation is shown in FIG. -- that shows a compute card with a processor ASIC, DRAM memory and a flash memory card.","The flash memory size associated with each processor is ideally 2\u00d7 to 4\u00d7 the required checkpointmemory size to allow for multiple backups so that recovery is possible from any failures that occur during the checkpoint write itself. Also, the system is tolerant of a limited number of hard failures in the local flash storage, since checkpoint data from those few nodes can simply be written to the file system through the normal I\/O channels using only a fraction of the total I\/O bandwidth.","FIG. -- shows an example physical layout of a compute card  implemented in the multiprocessor system such as a BluGene\u00ae parallel computing system in which the nodechip  () and an additional compact non-volatile memory card  for storing checkpoint data resulting from checkpoint operation is implemented. In one embodiment, the non-volatile memory size associated with each processor is ideally at least two (2) times the required checkpoint memory size to allow for multiple backups so that recovery is possible from any failures that occur during a checkpoint write itself. FIG. -- particularly shows a front side  of compute card  having the large processor ASIC, i.e., nodechip , surrounded by the smaller size memory (DRAM) chips . The blocks  at the bottom of the compute card, represent connectors that attach this card to the next level of the packaging, i.e., a node board, that includes 32 of these compute cards. The node compute card  in one embodiment shown in FIG. -- further illustrates a back side  of the card with additional memory chips , and including a centrally located non-volatile memory device, e.g., a phase change memory device, a flash memory storage device such as a CompactFlash\u00ae card  (CompactFlash\u00ae a registered trademark of SANDISK, Inc. California), directly below the nodechip  disposed on the top side  of the card. The flash signal interface (ATA\/UDMA) is connected between the CompactFlash\u00ae connector (toward the top of the card) and the pins on the compute ASIC by wiring in the printed circuit board. A CompactFlash standard (CF+ and CompactFlash Specification Revision 4.1 dated Feb. 16, 2007) defined by a CompactFlash Association including a consortium of companies such as Sandisk, Lexar, Kingston Memory, etc., that includes a specification for conforming devices and interfaces to the CompactFlash\u00ae card ) is incorporated by reference as if fully set forth herein. It should be understood that other types of flash memory cards, such as SDHC (Secure Digital High Capacity) may also be implemented depending on capacity, bandwidth and physical space requirements.","In one embodiment, there is no cabling used in these interfaces. Network interfaces are wired through the compute card connectors to the node board, and some of these, including the I\/O network connections are carried from the node board to other parts of the system, e.g., via optical fiber cables.","In one aspect, checkpointing data are written to a checkpoint file, for example, at a compact non-volatile memory buffer on the node, and subsequently written out in system packets to the I\/O nodes where they could be written to disk, attached hard-drive optical, magnetic, volatile or non-volatile memory storage devices, for example.","As shown in FIG. --, the checkpointing is performed in a non-volatile based system, i.e., the system-on-chip (SOC) compute nodechip, DRAM memory and a flash memory such as a pluggable CompactFlash (CF) memory card, with checkpoint data and internal node state data expediently stored in the flash memory  implemented on the computer nodechip, e.g., before and\/or in addition to being written out to I\/O. The checkpointing data at a node could further be written to possibly other nodes and stored in local memory\/flash memory at those nodes.","Data transferred to\/from the flash memory may be further effected by interfaces to a processor such as ATA or UDMA (\u201cUltra DMA\u201d) that are supported by commodity flash cards that provide sufficient bandwidth to the flash memory for writing checkpoints. For example, the ATA\/ATAPI-4 transfer modes support speeds at least from 16 MByte\/s to 33 MByte\/second. In the faster Ultra DMA modes and Parallel ATA up to 133 MByte\/s transfer rate is supported.","From the foregoing, the system and methodology can re-start the user application at exactly the same state in which it was in at time of entering the checkpoint. With the addition of system checkpoints, in the manner as described herein checkpointing can be performed anytime while a user application is still running.","In one example embodiment, a large parallel supercomputer system, that provides 5 gigabyte\/s I\/O bandwidth from a rack, where a rack includes 1024 compute nodes in an example embodiment, each with 16 gigabyte of memory, would require about 43 minutes to checkpoint 80% of memory. If this checkpoint instead were written locally at 40 megabyte\/s to a non-volatile memory such as flash memory  shown in FIG. --, it would require under 5.5 minutes for about an 8\u00d7 speedup. To minimize total processing time, the optimum interval between checkpoints varies as the square root of the product of checkpoint time and job run time.","Thus, for a 200 hour compute job the system without flash memory might use 12-16 checkpoints, depending on expected failure rate, adding a total time of 8.5 to 11.5 hours for backup. Using the same assumptions, the system with local flash memory could perform 35-47 checkpoints, adding only 3.1 to 4.2 hours. With no fails or restarts during the job, the improvement in throughput is modest, about 3%. However, for one or two fails and restarts, the throughput improvement increases to over 10%.","As mentioned, in one embodiment, the size of the flash memory associated with each processor core is, in one embodiment, two time (or greater) the required checkpoint memory size to allow for multiple backups so that recovery is possible from any failures that occur during the checkpoint write itself. Larger flash memory size is preferred to allow additional space for wear leveling and redundancy. Also, the system design is tolerant of a limited number of hard failures in the local flash storage, since checkpoint data from those few nodes can simply be written to the file system through the normal I\/O network using only a small fraction of the total available I\/O bandwidth. In addition, redundancy through data striping techniques similar to those used in RAID storage can be used to spread checkpoint data across multiple flash memory devices on nearby processor nodes via the internal networks, or on disk via the I\/O network, to enable recovery from data loss on individual flash memory cards.","Thus a checkpoint storage medium provided with only modest reliability can be employed to improve the reliability and availability of a large parallel computing system. Furthermore, the flash memory cards is a more cost effective way of increasing system availability and throughput than increasing in IO bandwidth.","In sum, the incorporation of the flash memory device  at the multiprocessor node provides a local storage for checkpoints thus relieving the bottleneck due to I\/O bandwidth limitations associated with some memory access operations. Simple available interfaces to the processor such as ATA or UDMA (\u201cUltra DMA\u201d) that are supported by commodity flash cards provide sufficient bandwidth to the flash memory for writing checkpoints. For example, the ATA\/ATAPI-4 transfer modes support speeds at least from 16 MByte\/s to 33 MByte\/second. In the faster Ultra DMA modes and Parallel ATA up to 133 MByte\/s transfer rate is supported.","For example, a multiple gigabyte checkpoint can be written to local flash card at 5020 megabyte\/s to 40 megabyte\/s in only a few minutes. Writing the same data to disk storage from all processors using the normal I\/O network could take more than ten (10) times as long.","Highly parallel computing systems, with tens to hundreds of thousands of nodes, are potentially subject to a reduced mean-time-to-failure (MTTF) due to a soft error on one of the nodes. This is particularly true in HPC (High Performance Computing) environments running scientific jobs. Such jobs are typically written in such a way that they query how many nodes (or processes) N are available at the beginning of the job and the job then assumes that there are N nodes available for the duration of the run. A failure on one node causes the job to crash. To improve availability such jobs typically perform periodic checkpoints by writing out the state of each node to a stable storage medium such as a disk drive. The state may include the memory contents of the job (or a subset thereof from which the entire memory image may be reconstructed) as well as program counters. If a failure occurs, the application can be rolled-back (restarted) from the previous checkpoint on a potentially different set of hardware with N nodes.","However, on machines with a large number of nodes and a large amount of memory per node, the time to perform such a checkpoint to disk may be large, due to limited I\/O bandwidth from the HPC machine to disk drives. Furthermore, the soft error rate is expected to increase due to the large number of transistors on a chip and the shrinking size of such transistors as technology advances.","To cope with such software, processor cores and systems increasingly rely on mechanisms such as Error Corrrecting Codes (ECC) and instruction retry to turn otherwise non-recoverable soft errors into recoverable soft errors. However, not all soft errors can be recovered in such a manner, especially on very small, simple cores that are increasingly being used in large HPC systems such as BlueGene\/Q (BG\/Q).","Thus, in one aspect, there is provided an approach to recover from a large fraction of soft errors without resorting to complete checkpoints. If this can be accomplished effectively, the frequency of checkpoints can be reduced without sacrificing availability.","There is thus provided a technique for performing \u201clocal rollbacks\u201d by utilizing a multi-versioned memory system such as that on BlueGene\/Q. On BG\/Q, the level 2 cache memory (L2) is multi-versioned to support both speculative running, a transactional memory model, as well as a rollback mode. Data in the L2 may thus be speculative. On BG\/Q, the L2 is partitioned into multiple L2 slices, each of which acts independently. In speculative or transactional mode, data in the main memory is always valid, \u201ccommitted\u201d data and speculative data is not written back to the main memory. In rollback mode, speculative data may be written back to the main memory, at which point it cannot be distinguished from committed data. In this invention, we focus on the hardware capabilities of the L2 to support local rollbacks. That capability is somewhat different than the capability to support speculative running and transactional memory. This multi-versioned cache is used to improve reliability. Briefly, in addition to supporting common caching functionality, the L2 on BG\/Q includes the following features for running in rollback mode. The same line (128 bytes) of data may exist multiple times in the cache. Each such line has a generation id tag and there is an ordering mechanism such that tags can be ordered from oldest to newest. There is a mechanism for requesting and managing new tags, and for \u201cscrubbing\u201d the L2 to clean it of old tags.","FIG. -- illustrates a transactional memory mode in one embodiment. A user defines parallel work to be done. A user explicitly defines a start and end of transactions within parallel work that are to be treated as atomic. A compiler performs, without limitation, one or more of: Interpreting user program annotations to spawn multiple threads; Interpreting user program annotation for start of transaction and save state to memory on entry to transaction to enable rollback; At the end of transactional program annotation, testing for successful completion and optionally branch back to rollback pointer. A transactional memory  supports detecting transaction failure and rollback. An L1 (Level 1) cache visibility for L1 cache hits as well as misses allowing for ultra low overhead to enter a transaction.","Local Rollback\u2014the Case when there is No I\/O","There is first described an embodiment in which there is no I\/O into and out of the node, including messaging between nodes. Checkpoints to disk or stable storage are still taken periodically, but at a reduced frequency. There is a local rollback interval. If the end of the interval is reached without a soft error, the interval is successful and a new interval can be started. Under certain conditions to be described, if a soft error occurs during the local rollback interval, the application can be restarted from the beginning of the local interval and re-executed. This can be done without restoring the data from the previous complete checkpoint, which typically reads in data from disk. If the end of the interval is then reached, the interval is successful and the next interval can be started. If such conditions are met, we term the interval \u201crollbackable\u201d. If the conditions are not met, a restart from the previous complete checkpoint is performed. The efficiency of the method thus depends upon the overhead to set up the local rollback intervals, the soft error rate, and the fraction of intervals that are rollbackable.","In this approach, certain types of soft errors cannot be recovered via local rollback under any conditions. Examples of such errors are an uncorrectable ECC error in the main memory, as this error corrupts state that is not backed up by multi-versioning, or an unrecoverable soft error in the network logic, as this corrupts state that can not be reinstated by rerunning. If such a soft error occurs, the interval is not rollbackable. We categorize soft errors into two classes: potentially rollbackable, and unconditionally not rollbackable. In the description that follows, we assume the soft error is potentially rollbackable. Examples of such errors include a detected parity error on a register inside the processor core.","At the start of each interval, each thread on each core saves it's register state (including the program counter). Certain memory mapped registers outside the core, that do not support speculation and need to be restored on checkpoint restore, are also saved. A new speculation generation id tag T is allocated and associated with all memory requests run by the cores from hereon. This ID is recognized by the L2-cache to treat all data written with this ID to take precedence, i.e., to maintain semantics of these accesses overwriting all previously written data. At the start of the interval, the L2 does not contain any data with tag T and all the data in the L2 has tags less than T, or has no tag associated (T) and is considered nonspeculative. Reads and writes to the L2 by threads contain a tag, which will be T for this next interval.","When a thread reads a line that is not in the L2, that line is brought into the L2 and given the non-speculative tag T. Data from this version is returned to the thread. If the line is in the L2, the data returned to the thread is the version with the newest tag.","When a line is written to the L2, if a version of that line with tag T does not exist in the L2, a version with tag T is established. If some version of the line exists in the L2, this is done by copying the newest version of that line into a version with tag T. If a version does not exist in the L2, it is brought in from memory and given tag T. The write from the thread includes byte enables that indicate which bytes in the current write command are to be written. Those bytes with the byte enable high are then written to the version with tag T. If a version of the line with tag T already exists in the L2, that line is changed according to the byte enables.","At the end of an interval, if no soft error occurred, the data associated with the current tag T is committed by changing the state of the tag from speculative to committed. The L2 runs a continuous background scrub process that converts all occurrences of lines written with a tag that has committed status. It merges all committed version of the same address into a single version based on tag ordering and removes the versions it merged.","The L2 is managed as a set-associative cache with a certain number of lines per set. All versions of a line belong to the same set. When a new line, or new version of a line, is established in the L2, some line in that set may have to be written back to memory. In speculative mode, non-committed, or speculative, versions are never allowed to be written to the memory, In rollback mode, non-committed versions can be written to the memory, but an \u201coverflow\u201d bit in a control register in the L2 is set to 1 indicating that such a write has been done. At the start of an interval all the overflow bits are set to 0.","Now consider the running during a local rollback interval. If a detected soft error occurs, this will trigger an interrupt that is delivered to at least one thread on the node. Upon receiving such an interrupt, the thread issues a core-to-core interrupt to all the other threads in the system which instructs them to stop runninig the current interval. If at this time, all the L2 overflow bits are 0, then the main memory contents have not been corrupted by data generated during this interval and the interval is rollbackable. If one of the overflow bits is 1, then main memory has been corrupted by data in this interval, the interval is not rollbackable and running is restarted from the most previous complete checkpoint.","If the interval is rollbackable, the cores are properly re-initialized, all the lines in the L2 associated with tag T are invalidated, all of the memory mapped registers and thread registers are restored to their values at the start of the interval, and the running of the interval restarts. The L2 invalidates the lines associated with tag T by changing the state of the tag to invalid. The L2 background invalidation process removes occurrences of lines with invalid tags from the cache.","This can be done in such a way that is completely transparent to the application being run. In particular, at the beginning of the interval, the kernel running on the threads can, in coordinated fashion, set a timer interrupt to fire indicating the end of the next interval. Since interrupt handlers are run in kernel, not user mode, this is invisible to the application. When this interrupt fires, and no detectable soft-error has occurred during the interval, preparations for the next interval are made, and the interval timer is reset. Note that this can be done even if an interval contained an overflow event (since there was no soft error). The length of the interval should be set so that an L2 overflow is unlikely to occur during the interval. This depends on the size of the L2 and the characteristics of the application workload being run.","Local Rollback\u2014the Case with I\/O","An embodiment is now described in the more complicated case of when there is I\/O, specifically messaging traffic between nodes. If all nodes participate in a barrier synchronization at the start of an interval, and if there is no messaging activity at all during the interval (either data injected into the network or received from the network) on every node, then if a rollbackable software error occurs during the interval on one or more nodes, then those nodes can re-run the interval and if successful, enter the barrier for the next interval. In such a case, the other nodes in the system are unaware that a rollback is being done somewhere else. If one such node has a soft error that is non-rollbackable, then all nodes may begin running from the previous full checkpoint. There are three problems with this approach:\n\n","We therefore seek alternative conditions that do not require barriers and relax the assumption that no messaging activity occurs during the interval. This will reduce the overhead and increase the fraction of rollbackable intervals. In particular, an interval will be rollbackable if no data that was generated during the current interval is injected into the network (in addition to some other conditions to be described later). Thus an interval is rollbackable if the data injected into the network in the current interval were generated during previous intervals. Thus packets arriving during an interval can be considered valid. Furthermore, if a node does do a local rollback, it will never inject the same messages (packets) twice, (once during the failed interval and again during the re-running). In addition note that the local rollback intervals can proceed independently on each node, without coordination from other nodes, unless there is a non rollbackable interval, in which case the entire application may be restarted from the previous checkpoint.","We assume that network traffic is handled by a hardware Message Unit (MU), specifically the MU is responsible for putting messages, that are packetized, into the network and for receiving packets from the network and placing them in memory. Dong Chen, et al., \u201cDISTRIBUTED PARALLEL MESSAGING UNIT FOR MULTIPROCESSOR SYSTEMS\u201d, U.S. Pat. No. 8,458,267, wholly incorporated by reference as if set forth herein, describes the MU in detail. Dong Chen, et al., \u201cSUPPORT FOR NON-LOCKING PARALLEL RECEPTION OF PACKETS BELONGING TO THE SAME FIFO\u201d, U.S. Pat. No. 8,086,766, wholly incorporated by reference as if set forth herein, also describes the MU in detail. Specifically, there are message descriptors that are placed in Injection FIFOs. An Injection Fifo is a circular buffer in main memory. The MU maintains memory mapped registers that, among other things contain pointers to the start, head, tail and end of the FIFO. Cores inject messages by placing the descriptor in the memory location pointed to by the tail, and then updating the tail to the next slot in the FIFO. The MU recognizes non-empty Fifos, pulls the descriptor at the head of the FIFO, and injects packets into the network as indicated in the descriptor, which includes the length of the message, its starting address, its destination and other information having to do with what should be done with the message's packets upon reception at the destination. When all the packets from a message have been injected, the MU advances the head of the FIFO. Upon reception, if the message is a \u201cdirect put\u201d, the payload bytes of the packet are placed into memory starting at an address indicated in the packet. If the packets belong to a \u201cmemory FIFO\u201d message, the packet is placed at the tail of a reception FIFO and then the MU updates the tail. Reception FIFOS are also circular buffers in memory and the MU again has memory mapped registers pointing to the start, head, tail and end of the FIFO. Threads read packets at the head of the FIFO (if non-empty) and then advance the head appropriately. The MU may also support \u201cremote get\u201d messages. The payload of such messages are message descriptors that are put into an injection FIFO. In such a way, one node can instruct another node to send data back to it, or to another node.","When the MU issues a read to an L2, it tags the read with a non-speculative tag. In rollback mode, the L2 still returns the most recent version of the data read. However, if that version was generated in the current interval, as determined by the tag, then a \u201crollback read conflict\u201d bit is set in the L2. (These bits are initialized to 0 at the start of an interval.) If subsections (sublines) of an L2 line can be read, and if the L2 tracks writes on a subline basis, then the rollback read conflict bit is set when the MU reads the subline that a thread wrote in the current interval. For example, if the line is 128 bytes, there may be 8 subsections (sublines) each of length 16 bytes. When a line is written speculatively, it notes in the L2 directory for that line which sublines are changed. If a soft error occurs during the interval, if any rollback read conflict bit is set, then the interval cannot be rolled back.","When the MU issues a write to the L2, it tags the write with a non-speculative id. In rollback mode, both a non-speculative version of the line is written and if there are any speculative versions of the line, all such speculative versions are updated. During this update, the L2 has the ability to track which subsections of the line were speculatively modified. When a line is written speculatively, it notes which sublines are changed. If the non-speculative write modifies a subline that has been speclatively written, a \u201cwrite conflict\u201d bit in the L2 is set, and that interval is not rollbackable. This permits threads to see the latest MU effects on the memory system, so that if no soft error occurs in the interval, the speculative data can be promoted to non-speculative for the next interval. In addition, if a soft error occurs, it permits rollback to non-speculative state.","On BG\/Q, the MU may issue atomic read-modify-write commands. For example, message byte counters, that are initialized by software, are kept in memory. After the payload of a direct put packet is written to memory, the MU issues an atomic read-modify-write command to the byte counter address to decrement the byte counter by the number of payload bytes in the packet. The L2 treats this as both a read and a write command, checking for both read and write conflicts, and updating versions.","In order for the interval to be rollbackable, certain other conditions may be satisfied. The MU cannot have started processing any descriptors that were injected into an injection FIFO during the interval. Violations of this \u201cnew descriptor injected\u201d condition are easy to check in software by comparing the current MU injection FIFO head pointers with those at the beginning of the interval, and by tracking how many descriptors are injected during the interval. (On BG\/Q, for each injection FIFO the MU maintains a count of the number of descriptors injected, which can assist in this calculation.)","In addition, during the interval, a thread may have received packets from a memory reception FIFO and advanced the FIFO's head pointer. Those packets will not be resent by another node, so in order for the rollback to be successful, it may be able to reset the FIFO's head pointer to what it was at the beginning of the interval so that packets in the FIFO can be \u201cre-played\u201d. Since the FIFO is a circular buffer, and since the head may have been advanced during the interval, it is possible that a newly arrived packet has overwritten a packet in the FIFO that may be re-played during the local rollback. In such a case, the interval is not rollbackable. It is easy to design messaging software that identifies when such an over-write occurs. For example, if the head is changed by an \u201cadvance_head\u201d macro\/inline or function, advance_head can increment a counter representing the number of bytes in the FIFO between the old head and the new head. If that counter exceeds a \u201csafe\u201d value that was determined at the start of the interval, then a write to an appropriate memory location system that notes the FIFO overwrite condition occurred. Such a write may be invoked via a system call. The safe value could be calculated by reading the FIFOs head and tail pointers at the beginning of the interval and, knowing the size of the FIFO, determining how many bytes of packets can be processed before reaching the head.","On BG\/Q barriers or global interrupts may be initiated by injecting descriptors into FIFOs, but via writing a memory mapped register that triggers barrier\/interrupt logic inside the network. If during an interval, a thread initiates a barrier and a soft error occurs on that node, then the interval is not rollbackable. Software can easily track such new barrier\/interrupt initiated occurrences, in a manner similar to the FIFO overwrite condition. Or, the hardware (with software cooperation) can set a special bit in the memory mapped barrier register whenever a write occurs; if that bit is initialized to 0 at the beginning of the interval, then if the bit is high, the interval cannot be rolled back.","We assume that the application uses a messaging software library that is consistent with with local rollbacks. Specifically, hooks in the messaging software support monitoring the reception FIFO overwrite condition, the injection FIFO new descriptor injected condition, and the new global interrupt\/barrier initiated condition. In addition, if certain memory mapped I\/O registers are written during an interval, such as when a FIFO is reconfigured by moving it, or resizing it, an interval cannot be rolled back. Software can be instrumented to track writes to such memory mapped I\/O registers and to record appropriate change bits if the conditions to rollback an interval are violated. These have to be cleared at the start of an interval, and checked when soft errors occur.","Putting this together, at the beginning of an interval:\n\n","If there is no detected soft error at the end of the interval, running of the next interval is initiated. If an unconditionally not rollbackable soft error occurs during the interval, running is re-started from the previous complete checkpoint. If a potentially rollbackable soft error occurs:\n\n","The above discussion assumes that no real-time interrupts such as messages from the control system, or MU interrupts occur. ON BG\/Q, a MU interrupt may occur if a packet with an interrupt bit set is placed in a memory FIFO, the amount of free space in a reception FIFO decreases below a threshold, or the amount of free space in an injection FIFO crosses a threshold. For normal injection FIFOS, the interrupt occurs if the amount of free space in the FIFO increases above a threshold, but for remote get injection FIFOs the interrupt occurs if the amount of free space in the FIFO decreases below a threshold.","A conservative approach would be to classify an interval as non rollbackable if any of these interrupts occurs, but we seek to increase the fraction of rollbackable intervals by appropriately handling these interrupts. First, external control system interrupts or remote get threshold interrupts are rare and may trigger very complicated software that is not easily rolled back. So if such an interrupt occurs, the interval will be marked not rollbackable.","For the other interrupts, we assume that the interrupt causes the messaging software to run some routine, e.g., called \u201cadvance\u201d, that handles the condition.","For the reception FIFO interrupts, advance may pull packets from the FIFO and for an injection FIFO interrupt, advance may inject new descriptors into a previously full injection FIFO. Note that advance can also be called when such interrupts do not occur, e.g., it may be called when an MPI application calls MPI_Wait. Since the messaging software may correctly deal with asynchronous arrival of messages, it may be capable of processing messages whenever they arrive. In particular, suppose such an interrupt occurs during an interval, and software notes that it has occurred, and an otherwise rollbackable soft error occurs during the interval. Note that when the interval is restarted, there are at least as many packets in the reception FIFO as when the interrupt originally fired. If when the interval is restarted, the software sets the hardware interrupt registers to re-trigger the interrupt, this will cause advance to be called on one or more threads at, or near the beginning of the interval (if the interrupt is masked at the time). In either case, the packets in the reception FIFO will be processed and the condition causing the interrupt will eventually be cleared. If when the interval starts, advance is already in progress, having the interrupt bit high may simply cause advance to be run a second time.","Mode Changes","As alluded to above, the L2 can be configured to run in different modes, including speculative, transactional, rollback and normal. If there is a mode change during an interval, the interval is not rollbackable.","Multiple Tag Domains","In the above description, it assumes that there is a single \u201cdomain\u201d of tags. Local rollback can be extended to the case when the L2 supports multiple domain tags. For example, suppose there are 128 tags that can be divided into up to 8 tag domains with 16 tags\/domain. Reads and writes in different tag domains do not affect one another. For example, suppose there are 16 (application) cores per node with 4 different processes each running on a set of 4 cores. Each set of cores could comprise a different tag domain. If there is a shared memory region between the 4 processers, that could comprise a fifth tag domain. Reads and writes by the MU are non-speculative and may be seen by every domain. The checks for local rollback may be satisfied by each tag domain. In particular, if the overflow, read and write conflict bits are on a per domain basis, then an interval cannot be rolled back if any of the domains indicate a violation.","FIG. -- illustrates a cache memory, e.g., L2 cache memory device (\u201cL2 cache\u201d) , and a control logic device  for controlling the L2 cache  according to one embodiment. Under software control, a local rollback is performed, e.g., by the control logic device . Local rollback refers to resetting processors, reinstating states of the processors as of the start of a last computation interval, and using the control logic device  to invalidate all or some memory state changes performed since the start of the last interval in the L2, and restarting the last computational interval. A computational interval (e.g., an interval 1 () in FIG. --) includes certain number of instructions. The length of the computational interval is set so that an L2 cache overflow is unlikely to occur during the interval. The length of the interval depends on a size of the L2 cache and characteristics of an application workload being run.","The L2 cache  is multi-versioned to support both speculative running mode, a transactional memory mode, and a rollback mode. A speculative running mode computes instruction calculations ahead of their time as defined in a sequential program order. In such a speculative mode, data in the L2 cache  may be speculative (i.e., assumed ahead or computed ahead and may subsequently be validated (approved), updated or invalidated). A transactional memory mode controls a concurrency or sharing of the L2 cache , e.g., by enabling read and write operations to occur at simultaneously, and by allowing that intermediate state of the read and write operations are not visible to other threads or processes. A rollback mode refers to performing a local rollback.","In one embodiment, the L2 cache  is partitioned into multiple slices, each of which acts independently. In the speculative or transactional mode, data in a main memory (not shown) is always valid. Speculative data held in the L2 cache  are not written back to the main memory. In the rollback mode, speculative data may be written back to the main memory, at which point the speculative data cannot be distinguished from committed data and the interval can not be rolled back if an error occurs. In addition to supporting a common caching functionality, the L2 cache  is operatively controlled or programmed for running in the rollback mode. In one embodiment, operating features include, but are not limited to: an ability to store a same cache line (e.g., 128 bytes) of data multiple times in the cache (i.e., multi-versioned); Each such cache line having or provided with a generation ID tag (e.g., tag 1 () and a tag T () in FIG. -- for identifying a version of data); Provide an ordering mechanism such that tags can be ordered from an oldest data to a newest data; Provide a mechanism for requesting and managing new tags and for \u201cscrubbing\u201d (i.e., filtering) the L2 cache  to clean old tags. For example, the L2 cache  includes multiple version of data (e.g., a first version (oldest version)  of data tagged with \u201c1\u201d (), a newest version  of data tagged with \u201cT\u201d ()) indicating an order, e.g., an ascending order, of the tags attached to the data. How to request and manage new tags are described below in detail.","FIG. -- illustrates exemplary local rollback intervals  and  defined as instruction sequences according to one exemplary embodiment. In this exemplary embodiment, the sequences include various instructions including, but not limited to: an ADD instruction , a LOAD instruction , a STORE instruction , a MULT instruction , a DIV instruction  and a SUB instruction . A local rollback interval refers to a set of instructions that may be restarted upon detecting a soft error and for which the initial state at the sequence start can be recovered. Software (e.g., Operating System, etc.) or hardware (e.g., the control logic device , a processor, etc.) determines a local rollback interval 1 () to include instructions from the ADD instruction  to the MULT instruction . How to determine a local rollback interval is described below in detail. If no soft error occurs during the interval 1 (), the software or hardware decides that the interval 1 () is successful and starts a new interval (e.g., an interval 2 ()). If a rollbackable soft error (i.e., soft error that allows instructions in the interval 1 () to restart and\/or rerun) occur, the software or hardware restarts and reruns instructions in the interval 1 () from the beginning of the interval 1 (), e.g., the ADD instruction , by using the control logic device . If a non-rollbackable soft error (i.e., soft error that does not allow instructions in the interval 1 () to restart and\/or rerun), a processor core (e.g., CPU  in FIG. --) or the control logic device  restarts and\/or rerun instructions from a prior checkpoint.","In one embodiment, the software or hardware sets a length of the current interval so that an overflow of the L2 cache  is unlikely to occur during the current interval. The length of the current interval depends on a size of the L2 cache  and\/or characteristics of an application workload being run.","In one embodiment, the control logic device  communicates with the cache memory, e.g., the L2 cache. In a further embodiment, the control logic device  is a memory management unit of the cache memory. In a further embodiment, the control logic device  is implemented in a processor core. In an alternative embodiment, the control logic device  is implemented is a separate hardware or software unit.","The following describes situations in which there is no I\/O operation into and out of a node, including no exchange of messages between nodes. Checkpoints to disk or a stable storage device are still taken periodically, but at a reduced frequency. If the end of a current local rollback interval (e.g., an interval 1 () in FIG. --) is reached without a soft error, the current local rollback interval is successful and a new interval can be started. If a rollbackable soft error occurs during the current local rollback interval, an application or operation can be restarted from the beginning of that local interval and rerun. This restarting and rerunning can be performed without retrieving and\/or restoring data from a previous checkpoint, which typically reads in data from a disk drive. If a non-rollbackable soft error (i.e., soft error not recoverable by local rollback) occurs during the local rollback interval, a restart from the previous checkpoint occurs, e.g., by bringing in data from a disk drive. An efficiency of the method steps described in FIG. -- thus depends upon an overhead to set up the local rollback interval, a soft error rate, and a fraction of intervals that are rollbackable.","In one embodiment, certain types of soft errors cannot be recovered via local rollback under any conditions (i.e., are not rollbackable). Examples of such errors include one or more of: an uncorrectable ECC error in a main memory, as this uncorrectable ECC error may corrupt a state that is not backed up by the multi-versioning scheme; an unrecoverable soft error in a network, as this unrecoverable error may corrupt a state that can not be reinstated by rerunning. If such a non-rollbackable soft error occurs, the interval is not rollbackable. Therefore, according to one embodiment of the present invention, there are two classes of soft errors: potentially rollbackable and unconditionally not rollbackable. For purposes of description that follow, it is assumed that a soft error is potentially rollbackable.","At the start of each local rollback interval, each thread on each processor core stores its register state (including its program counter), e.g., in a buffer. Certain memory mapped registers (i.e., registers that have their specific addresses stored in known memory locations) outside the core that do not support the speculation (i.e., computing ahead or assuming future values) and need to be restored on a checkpoint are also saved, e.g., in a buffer. A new (speculation) generation ID tag \u201cT\u201d (e.g., a tag \u201cT\u201d bit or flag  in FIG. --) is allocated and associated with some or all of memory requests run by the core. This ID tag is recognized by the L2 cache to treat all or some of the data written with this ID tag to take precedence, e.g., to maintain semantics for overwriting all or some of previously written data. At the start of the interval, the L2 cache  does not include any data with the tag \u201cT\u201d () and all the data in the L2 cache have tags less than \u201cT\u201d (e.g., tag T\u22121, et seq.) (), as shown in FIG. --, or has no tag \u201cT\u201d () which a newest non-speculative tag (i.e., tag attached data created or requested in a normal cache mode (e.g., read and\/or write)). Reads and writes to the L2 cache  by a thread include a tag which will be \u201cT\u201d for a following interval. When a thread reads a cache line that is not in the L2 cache , that line is brought into the L2 cache and given the non-speculative tag \u201cT\u201d (). This version of data (i.e., data tagged with \u201cT\u201d ()) is returned to the thread. If the line is in the L2 cache , the data returned to the thread is a version with the newest tag, e.g., the tag \u201cT\u201d (). In one embodiment, the control logic device  includes a counter that automatically increment a tag bit or flag, e.g., 0, 1, . . . , T\u22121, T, T+1.","When a cache line is written to the L2 cache, if a version of that line with the tag \u201cT\u201d () does not exist in the L2 cache, a version with the tag \u201cT\u201d () is created. If some version of the line exists in the L2 cache, the control logic device  copies the newest version of that line into a version with the tag \u201cT\u201d (). If a version of the line does not exist in the L2 cache, the line is brought in from a main memory and given the tag \u201cT\u201d (). A write from a thread includes, without limitation, byte enables that indicate which bytes in a current write command are to be written. Those bytes with the byte enable set to a predetermined logic level (e.g., high or logic \u20181\u2019) are then written to a version with the tag \u201cT\u201d (). If a version of the line with the tag \u201cT\u201d () already exists in the L2 cache , that line is changed according to the byte enables.","At the end of a local rollback interval, if no soft error occurred, data associated with a current tag \u201cT\u201d () is committed by changing a state of the tag from speculative to committed (i.e., finalized, approved and\/or determined by a processor core). The L2 cache  runs a continuous background scrub process that converts all occurrences of cache lines written with a tag that has committed status to non-speculative. The scrub process merges all or some of a committed version of a same cache memory address into a single version based on tag ordering and removes the versions it merged.","In one embodiment, the L2 cache  is a set-associative cache with a certain number of cache lines per set. All versions of a cache line belong to a same set. When a new cache line, or new version of a cache line, is created in the L2 cache, some line(s) in that set may have to be written back to a main memory. In the speculative mode, non-committed, or speculative, versions are may not be allowed to be written to the main memory. In the rollback mode, non-committed versions can be written to the main memory, but an \u201coverflow\u201d bit in a control register in the L2 cache is set to 1 indicating that such a write has been done. At the start of a local rollback interval, all the overflow bits are set to 0.","In another embodiment, the overflow condition may cause a state change of a speculation generation ID (i.e., an ID of a cache line used in the speculative mode in which speculation the line was changed) in to a committed state in addition to or as an alternative to setting an overflow flag.","If a soft error occurs during a local rollback interval, this soft error triggers an interrupt that is delivered to at least one thread running on a node associated with the L2 cache . Upon receiving such an interrupt, the thread issues a core-to-core interrupt (i.e., an interrupt that allow threads on arbitrary processor cores of an arbitrary computing node to be notified within a deterministic low latency (e.g., 10 clock cycles)) to all the other threads which instructs them to stop running the current interval. If at this time, all the overflow bits of the L2 cache are 0, then contents in the main memory have not been corrupted by data generated during this interval and the interval is rollbackable. If one of the overflow bits is 1, then the main memory has been corrupted by data in this interval, the interval is not rollbackable and rerunning is restarted from the most previous checkpoint.","If the interval is rollbackable, processor cores are re-initialized, all or some of the cache lines in the L2 associated with the tag \u201cT\u201d () are invalidated, all or some of the memory mapped registers and thread registers are restored to their values at the start of the interval, and a running of the interval restarts. The control logic device  invalidates cache lines associated with the tag \u201cT\u201d () by changing a state of the tag \u201cT\u201d () to invalid. The L2 cache background invalidation process initiates removal of occurrences of lines with invalid tags from the L2 cache  in the rollbackable interval.","Recovering rollbackable soft errors can be performed in a way that is transparent to an application being run. At the beginning of a current interval, a kernel running on a thread can, in a coordinated fashion (i.e., synchronized with the control logic device ), set a timer interrupt (i.e., an interrupt associated with a particular timing) to occur at the end of the current interval. Since interrupt handlers are run in kernel, this timer interrupt is invisible to the application. When this interrupt occurs and no detectable soft error has occurred during the interval, preparations for the next interval are made, and the timer interrupt is reset. These preparations can be done even if a local rollback interval included an overflow event (since there was no soft error).","The following describes situation in which there is at least one I\/O operation, for example, messaging traffic between nodes. If all nodes participate in a barrier synchronization at the start of a current interval, if there is no messaging activity at all during the interval (no data injected into a network or received from the network) on every node, if a rollbackable software error occurs during the interval on one or more nodes, then those nodes can rerun the interval and, if successful, enter the barrier (synchronization) for a next interval.","In one embodiment, nodes are unaware that a local rollback is being performed on another node somewhere else. If a node has a soft error that is non-rollbackable, then all other nodes may begin an operation from the previous checkpoint.","In another embodiment, software or the control logic device  checks the at least one condition or state, which does not require barriers and that relaxes an assumption that no messaging activity occurs during a current interval. This checking of the at least one condition reduces an overhead and increases a fraction of rollbackable intervals. For example, a current interval will be rollbackable if no data that was generated during the current interval is injected into the network. Thus the current interval is rollbackable if the data injected into the network in the current interval were generated during previous intervals. Thus, packets arriving during a local rollback interval can be considered valid. Furthermore, if a node performs a local rollback within the L2 cache , it will not inject the same messages (packets) twice, (i.e., once during a failed interval and again during a rerunning). Local rollback intervals can proceed independently on each node, without coordination from other nodes, unless there is a non-rollbackable interval, in which case an entire application may be restarted from a previous checkpoint.","In one embodiment, network traffic is handled by a hardware Message Unit (MU). The MU is responsible for putting messages, which are packetized, into the network and for receiving packets from the network and placing them in a main memory device. In one embodiment, the MU is similar to a DMA engine on IBM\u00ae Blue Gene\u00ae\/P supercomputer described in detail in \u201cOverview of the IBM Blue Gene\/P project\u201d, IBM\u00ae Blue Gene\u00ae team, IBM J. RES. & DEV., Vol. 52, No. 1\/2 January\/March 2008, wholly incorporated by reference as if set forth herein. There may be message descriptors that are placed in an injection FIFO (i.e., a buffer or queue storing messages to be sent by the MU). In one embodiment, an injection FIFO is implemented as a circular buffer in a main memory.","The MU maintains memory mapped registers that include, without limitation, pointers to a start, head, tail and end of the injection FIFO. Processor cores inject messages by placing the descriptor in a main memory location pointed to by the tail, and then updating the tail to a next slot in the injection FIFO. The MU recognizes non-empty slots in the injection FIFO, pulls the descriptor at the head of the injection FIFO, and injects a packet or message into the network as indicated in the descriptor, which includes a length of the message, its starting address, its destination and other information indicating what further processing is to be performed with the message's packets upon a reception at a destination node. When all or some of the packets from a message have been injected, the MU advances the head pointer of the injection FIFO. Upon a reception, if the message is a \u201cdirect put\u201d, payload bytes of the packet are placed into a receiving node's main memory starting at an address indicated in the packet. (A \u201cdirect put\u201d is a packet type that goes through the network and writes payload data into a receiving node's main memory.) If a packet belongs to a \u201cmemory FIFO\u201d message (i.e., a message associated with a queue or circular buffer in a main memory of a receiving node), the packet is placed at the tail of a reception FIFO and then the MU updates the tail. In one embodiment, a reception FIFO is also implemented as a circular buffer in a main memory and the MU again has memory mapped registers pointing to the start, head, tail and end of the reception FIFO. Threads read packets at the head of the reception FIFO (if non-empty) and then advance the head pointer of the reception FIFO appropriately. The MU may also support \u201cremote get\u201d messages. (A \u201cremote get\u201d is a packet type that goes through the network and is deposited into the injection FIFO on a node A. Then, the MU causes the \u201cremote get\u201d message to be sent from the node A to some other node.) A payload of such \u201cremote get\u201d message is message descriptors that are put into the injection FIFO. Through the \u201cremote get\u201d message, one node can instruct another node to send data back to it, or to another node.","When the MU issues a read to the L2 cache , it tags the read with a non-speculative tag (e.g., a tag \u201cT\u201d () in FIG. --). In the rollback mode, the L2 cache  still returns the most recent version of data read. However, if that version was created in the current interval, as determined by a tag (e.g., the tag \u201cT\u201d () in FIG. --), then a \u201crollback read conflict\u201d bit is set to high in the L2 cache . (This \u201crollback read conflict\u201d bit is initialized to 0 at the start of a local rollback interval.) The \u201crollback read conflict\u201d bit indicates that data generated in the current interval is being read and\/or indicates that the current interval is not rollbackable. If subsections (sublines) of an L2 cache line can be read, and if the L2 cache  tracks writes on a subline basis, then the rollback read conflict bit is set when the MU reads the subline that a thread wrote to in the current interval. For example, if a cache line is 128 bytes, there may be 8 subsections (sublines) each of length 16 bytes. When a cache line is written speculatively, the control logic device  marks that line having changed sublines, e.g., by using a flag or dirty bit. If a soft error occurs during the interval and\/or if any rollback read conflict bit is set, then the interval cannot be rolled back (i.e., cannot be restarted).","In another embodiment, the conflict condition may cause a state change of the speculation ID to the committed state in addition to or as an alternative to setting a read conflict bit.","When the MU issues a write to the L2 cache , it tags the write with a non-speculative ID (e.g., a tag \u201cT\u201d () in FIG. --). In the rollback mode, a non-speculative version of a cache line is written to the L2 cache  and if there are any speculative versions of the cache line, all such speculative versions are updated. During this update, the L2 cache has an ability to track which subsections of the line were speculatively modified. When a cache line is written speculatively, the control logic device  or the L2 cache  marks which sublines are changed, e.g., by using a flag or dirty bit. If the non-speculative write (i.e., normal write) modifies a subline that has been speculatively written during a local rollback interval, a \u201cwrite conflict\u201d bit in the L2 cache  is set to, for example, high or logic \u201c1\u201d, and that interval is not rollbackable. A \u201cwrite conflict\u201d bit indicates that a normal write modifies speculative data (i.e., assumed data or data computed ahead) and\/or that the current interval is not rollbackable. This \u201cwrite conflict\u201d bit also permits threads to see the latest effects or operations by the MU on a memory system. If no soft error occurs in the current interval, the speculative data can be promoted to non-speculative for a next interval. In addition, although a rollbackable soft error occurs, the control logic device  promotes the speculative data to be non-speculative.","In another embodiment, the write conflict condition may cause a state change of the speculation ID to the committed state in addition to or as an alternative to setting a write conflict bit.","In one embodiment, the MU issues an atomic read-modify-write command. When a processor core accesses a main memory location with the read-modify-write command, the L2 cache  is read and then modified and the modified contents are stored in the L2 cache. For example, message byte counters (i.e., counters that store the number of bytes in messages in a FIFO), which are initialized by an application, are stored in a main memory. After a payload of a \u201cdirect put\u201d packet is written to the main memory, the MU issues the atomic read-modify-write command to an address of the byte counter to decrement the byte counter by the number of payload bytes in the packet. The L2 cache  treats this command as both a read and a write command, checking for both read and write conflicts and updating versions.","In one embodiment, in order for the current interval to be rollbackable, certain conditions should be satisfied. One condition is that the MU cannot have started processing any descriptors that were injected into an injection FIFO during the interval. Violations of this \u201cnew descriptor injected\u201d condition (i.e., a condition that a new message descriptor was injected into the injection FIFO during the current interval) can be checked by comparing current injection FIFO head pointers with those at the beginning of the interval and\/or by tracking how many descriptors are injected during the interval. In a further embodiment, for each injection FIFO, the MU may count the number of descriptors injected.","In a further embodiment, during the current interval, a thread may have received packets from the reception FIFO and advanced the reception FIFO head pointer. Those packets will not be resent by another node, so in order for a local rollback to be successful, the thread should be able to reset the reception FIFO head pointer to what it was at the beginning of the interval so that packets in the reception FIFO can be \u201cre-played\u201d. Since the reception FIFO is a circular buffer, and since the head pointer may have been advanced during the interval, it is possible that a newly arrived packet has overwritten a packet in the reception FIFO that should be re-played during the local rollback. In such a situation where an overwriting occurred during a current interval, the interval is not rollbackable. In one embodiment, there is provided messaging software that identifies when such an overwriting occurs. For example, if the head pointer is changed by an \u201cadvance_head\u201d macro\/inline or function (i.e., a function or code for advancing the head pointer), the \u201cadvance_head\u201d function can increment a counter representing the number of bytes in the reception FIFO between an old head pointer (i.e., a head pointer at the beginning of the current interval) and a new head pointer (i.e., a head pointer at the present time). If that counter exceeds a \u201csafe\u201d value (i.e., a threshold value) that was determined at the start of the interval, then a write to a main memory location that invokes the reception FIFO overwriting condition occurs. Such a write may also be invoked via a system call (e.g., a call to a function handled by an Operating System (e.g., Linux\u2122 of a computing node). The safe value can be calculated by reading the reception FIFO head and tail pointers at the beginning of the interval, by knowing a size of the FIFO, and\/or by determining how many bytes of packets can be processed before reaching the reception FIFO head pointer.","The barrier(s) or interrupt(s) may be initiated by writing a memory mapped register (not shown) that triggers the barrier or interrupt handler inside a network (i.e., a network connecting processing cores, a main memory, and\/or cache memory(s), etc.). If during a local rollback interval, a thread initiates a barrier and a soft error occurs on a node, then the interval is not rollbackable. In one embodiment, there is provided a mechanism that can track such barrier or interrupt, e.g., in a manner similar to the reception FIFO overwriting condition. In an alternative embodiment, hardware (with software cooperation) can set a flag bit in a memory mapped barrier register  whenever a write occurs. This flag bit is initialized to 0 at the beginning of the interval. If the special bit is high, the interval cannot be rolled back. A memory mapped barrier register  is a register outside a processor core but accessible by the processor core. When values in the memory mapped barrier register changes, the control logic device  may cause a barrier or interrupt packet (i.e., packet indicating a barrier or interrupt occurrence) to be injected to the network. There may also be control registers that define how this barrier or interrupt packet is routed and what inputs triggers or creates this packet.","In one embodiment, an application being run uses a messaging software library (i.e., library functions described in the messaging software that is consistent with local rollbacks. The messaging software may monitor the reception FIFO overwriting condition (i.e., a state or condition indicating that an overwriting occurred in the reception FIFO during the current interval), the injection FIFO new descriptor injected condition (i.e., a state or condition that a new message descriptor was injected into the injection FIFO during the current interval), and the initiated interrupt\/barrier condition (i.e., a state or condition that the barrier or interrupt is initiated by writing a memory mapped register). In addition, if a memory mapped I\/O register  (i.e., a register describing status of I\/O device(s) or being used to control such device(s)) is written during a local rollback interval, for example, when a FIFO is reconfigured by moving that FIFO, or resizing that FIFO, the interval cannot be rolled back. In a further embodiment, there is provided a mechanism that tracks a write to such memory mapped I\/O register(s) and records change bits if condition(s) for local rollback is(are) violated. These change bits have to be cleared at the start of a local rollback interval and checked when soft errors occur.","Thus, at the beginning of a local rollback interval:","1. Threads, run by processing cores of a computing node, set the read and write conflict and overflow bits to 0.","2. Threads store the injection FIFO tail pointers and reception FIFO head pointers, compute and store the safe value and set the reception FIFO overwrite bit (i.e., a bit indicating an overwrite occurred in the reception FIFO during the interval) to 0, set the barrier\/intrrupt bit (i.e., a bit indicating a barrier or interrupr is initated, e.g., by writing a memory mapped register, during the interval) to 0, and set the change bits (i.e., bits indicating something has been changed during the interval) to 0.","3. Threads initiate storing of states of their internal and\/or external registers.","4. A new speculative ID tag (e.g., a tag \u201cT\u201d () in FIG. --) is generated and used for duration of the interval; and,","5. Threads begin running code in the interval.","If there is no detected soft error at the end of a current interval, the control logic device  runs a next interval. If an unconditionally not rollbackable soft error (i.e., non-rollbackable soft error) occurs during the interval, the control logic device  or a processor core restarts an operation from a previous checkpoint. If a potentially rollbackable soft error occurs:","1. If the MU is not already stopped, the MU is stopped, thereby preventing new packets from entering a network (i.e., a network to which the MU is connected to) or being received from the network. (Typically, when the MU is stopped, it continues processing any packets currently in progress and then stops.)","2. Rollbackable conditions are checked: the rollback read and write conflict bits, or if the speculation ID is already in committed state, the injection FIFO new descriptor injected condition, the reception FIFO overwrite bits, the barrier\/interrupt bit, and the change bits. If the interval is not rollbackable, the control logic device  or a processor core restarts an operation from a previous checkpoint. If the interval is rollbackable, proceeding to the next step 3.","3. Processor cores are reinitialized, all or some of the cache lines in the L2 cache  are invalidated (without writing back speculative data in the L2 cache  to a main memory), and, all or some of the memory mapped registers and thread registers are restored to their values at the start of the current interval. The injection FIFO tail pointers are restored to their original values at the start of the current interval. The reception FIFO head pointers are restored to their original values at the start of the current interval. If the MU was already stopped, the MU is restarted; and,","4. Running of the current interval restarts.","In one embodiment, real-time interrupts such as messages from a control system (e.g., a unit controlling the HPC system), or interrupts initiated by the MU (\u201cMU interrupt\u201d) occur. An MU interrupt may occur if a packet with an interrupt bit set high is placed in an injection or reception FIFO, if an amount of free space in a reception FIFO decreases below a threshold, or if an amount of free space in an injection FIFO increases above a threshold. For a (normal) injection FIFO, an interrupt occurs if the amount of free space in the injection FIFO increases above a threshold. For a remote get injection FIFO (i.e., a buffer or queue storing \u201cremote get\u201d message placed by the MU), an interrupt occurs if an amount of free space in the reception FIFO decreases below a threshold.","In one embodiment, the control logic device  classifies an interval as non-rollbackable if any of these interrupts occurs. In an alternative embodiment, the control logic device  increases a fraction of rollbackable intervals by appropriately handling these interrupts as described below. Control system interrupts or remote get threshold interrupts (i.e., interrupts initiated by the remote get injection FIFO due to an amount of free space lower than a threshold) may trigger software that is not easily rolled back. So if such an interrupt (e.g., control system interrupts and\/or remote get threshold interrupt) occurs, the interval is not rollbackable.","All the other interrupts cause the messaging software to run a software routine, e.g., called \u201cadvance\u201d, that handles all the other interrupts. For example, for the reception FIFO interrupts (i.e., interrupts initiated by the reception FIFO because an amount of free space is below a threshold), the advance may pull packets from the reception FIFO. For the injection FIFO interrupt (i.e., an interrupt occurred because an amount of free space is above a threshold), the advance may inject new message descriptors into a previously full injection FIFO (i.e., a FIFO which was full at some earlier point in time; when the injection FIFO interrupt occurred, the FIFO was no longer full and a message descriptor may be injected). The advance can also be called when such interrupts do not occur, e.g., the advance may be called when an MPI (Messaging Passing Interface) application calls MPI_Wait. MPI refers to a language-independent communication protocol used to program parallel computers and is described in detail in http:\/\/www.mpi-forum.org\/ or http:\/\/www.mcs.anl.gov\/research\/projects\/mpi\/. MPI_Wait refers to a function that waits for an MPI application to send or receive to complete its request.","Since the messaging software can correctly deal with asynchronous arrival of messages, the messaging software can process messages whenever they arrive. In a non-limiting example, suppose that an interrupt occurs during a local rollback interval and that the control logic device  detects that the interrupt has occurred, e.g., by checking whether the barrier or interrupt bit is set to high (\u201c1\u201d), and that a rollbackable soft error occurs during the interval. In this example, when the interval is restarted, there may be at least as many packets in the reception FIFO as when the interrupt originally occurred. If the control logic device  sets hardware interrupt registers (i.e., registers indicating interrupt occurrences) to re-trigger the interrupt, when the interval is restarted, this re-triggering will cause the advance to be called on one or more threads at, or near the beginning of the interval (if the interrupt is masked at the time). In either case, the packets in the reception FIFO will be processed and a condition causing the interrupt will eventually be cleared. If the advance is already in progress, when the interval starts, having interrupt bits set high (i.e., setting the hardware interrupt registers to a logic \u201c1\u201d for example) may cause the advance to be run a second time.","The L2 cache  can be configured to run in different modes, including, without limitation, speculative, transactional, rollback and normal (i.e., normal caching function). If there is a mode change during an interval, the interval is not rollbackable.","In one embodiment, there is a single \u201cdomain\u201d of tags in the L2 cache . In this embodiment, a domain refers to a set of tags. In one embodiment, the software (e.g., Operating System, etc.) or the hardware (e.g., the control logic device, processors, etc.) performs the local rollback when the L2 cache supports a single domain of tags or multiple domains of tags. In the multiple domains of tags, tags are partitioned into different domains. For example, suppose that there are 128 tags that can be divided into up to 8 tag domains with 16 tags per domain. Reads and writes in different tag domains do not affect one another. For example, suppose that there are 16 (application) processor cores per node with 4 different processes each running on a set of 4 processor cores. Each set of cores could comprise a different tag domain. If there is a shared memory region between the 4 processes, which could comprise a fifth tag domain. Reads and writes by the MU are non-speculative (i.e., normal) and may be seen by every domain. Evaluations for local rollback may be satisfied by each tag domain. In particular, if the overflow, read and write conflict bits are set to high in a domain during a local rollback interval, then interval cannot be rolled back if any of the domains indicate non-rollbackable situation (e.g., the overflow bits are high).","FIG. -- illustrates a flow chart including method steps for performing a local rollback (i.e., restart) in a parallel computing system including a plurality of computing nodes according to one embodiment of the present invention. A computing node includes at least one cache memory device and at least one processor. At step , the software or hardware starts a current computational interval (e.g., an interval 1 () in FIG. --). At step , processors (e.g., CPU  in FIG. --) run(s) at least one instruction in the interval. At step , while running the at least one instructions in the interval, the control logic device  evaluates whether at least one unrecoverable condition occurs. The at least one unrecoverable condition includes, without limitation, the conflict bit set to high (logic \u201c1\u201d)\u2014an occurrence of a read or write conflict during the interval, the overflow bit being set to high\u2014an occurrence of an overflow in the cache memory device during the interval, the barrier or interrupt bit being set to high\u2014an occurrence of a barrier of interrupt during the interval, the reception FIFO overwrite bit being set to high\u2014an occurrence of overwriting a FIFO, the injection FIFO new descriptor injected condition\u2014an occurrence of an injection of data modified during the interval into a FIFO. If the at least one unrecoverable condition does not occur, at step , an interrupt handler evaluates whether an error occurs during the local rollback and\/or the interval. The error that can be detected in the step  may be a rollbackable error (i.e., an error that can be recovered by performing local rollback in the L2 cache ) because the unrecoverable condition has not occurred during the current interval. A non-rollbackable error is detected, e.g., by utilizing the uncorrectable error detecting capability of a parity bit scheme or ECC (Error Correcting Code). If the rollbackable error occurs, at steps  and , the control logic device  restarts the running of the current interval. Otherwise, at step , the software or hardware completes the running of the current interval and instructs the control logic device  to commit changes occurred during the current interval. Then, the control goes to the step  to run a next local rollback interval in the L2 cache .","If, at step , an unrecoverable condition occurs during the current interval, at step , the control logic device  commits changes made before the occurrence of the unrecoverable condition. At step , the control logic device  evaluates whether a minimum interval length is reached. The minimum interval length refers to the least number of instructions or the least amount of time that the control logic device  spends to run a local rollback interval. If the minimum interval length is reached, at step , the software or hardware ends the running of the current interval and instructs the control logic device  to commit changes (in states of the processor) occurred during the minimum interval length. Then, the control returns to the step  to run a next local rollback interval in the L2 cache . Otherwise, if the minimum interval length is not reached, at step , the software or hardware continues the running of the current interval until the minimum interval length is reached.","Continuing to step , while running the current interval before reaching the minimum interval length, whether an error occurred or not can be detected. The error that can be detected in step  may be non-recoverable soft error because an unrecoverable condition has been occurred during the current interval. If a non-recoverable error (i.e., an error that cannot be recovered by restarting the current interval) has not occurred until the minimum interval length is reached, at step , the software or hardware ends the running of the current interval upon reaching the minimum interval length and commits changes occurred during the minimum interval length. Then, the control returns to the step  to run a next local rollback interval. Otherwise, if a non-recoverable error occurs before reaching the minimum interval length, at step , the software or hardware stops running the current interval even though the minimum interval length is not reached and the control is aborted .","FIG. -- illustrates a flow chart detailing the step  described in FIG. -- according to a further embodiment of the present invention. At step , at the start of the current interval, the software or hardware stores states (e.g., register contents, program counter values, etc.) of a computing node's processor cores, e.g., in a buffer. At steps -, the control logic device  allocates and uses the newest generation ID tag (e.g., the tag \u201cT\u201d () in FIG. --) to versions of data created or accessed during the current interval.","FIG. -- illustrates a method step supplementing the steps  and\/or  described in FIG. -- according to a further embodiment of the present invention. After the control logic device  runs the step  or step  in FIG. --, the software or hardware may run a step  in FIG. --. At the step , the software or the processor(s) instructs the control logic device  to declare all or some of changes associated with the newest generation ID tag as permanent changes. In other words, at step , the control logic device  makes tentative changes in the state of the memory that occur in the current interval as permanent changes.","FIG. -- illustrates a flow chart detailing the step  described in FIG. -- according to a further embodiment of the present invention. At step , the software or processor(s) instructs the control logic device  to declare all or some of changes associated with the newest generation ID tag as invalid. Consequently, the control logic device  discards and\/or invalidates all or some of changes associated with the newest generation ID tag. Then, at step , the control logic device  restores the stored states of the process cores from the buffer.","In one embodiment, at least one processor core performs method steps described in FIGS. -----. In another embodiment, the control logic device  performs method steps described in FIGS. -----. In one embodiment, the method steps in FIGS. ----- and\/or the control logic device  are implemented in hardware or reconfigurable hardware, e.g., FPGA (Field Programmable Gate Array) or CPLD (Complex Programmable Control logic device Device), using a hardware description language (Verilog, VHDL, Handel-C, System C, etc.). In another embodiment, the method steps in FIGS. ----- and\/or the control logic device  are implemented in a semiconductor chip, e.g., ASIC (Application-Specific Integrated Circuit), using a semi-custom design methodology, i.e., designing a semiconductor chip using standard cells and a hardware description language. Thus, the hardware, reconfigurable hardware or the semiconductor chip operates the method steps described in FIGS. -----.","IEEE 754 describes floating point number arithmetic. Kahan, \u201cIEEE Standard 754 for Binary Floating-Point Arithmetic,\u201d May 31, 1996, UC Berkeley Lecture Notes on the Status of IEEE 754, wholly incorporated by reference as if set forth herein, describes IEEE Standard 754 in detail.","According to IEEE Standard 754, to perform floating point number arithmetic, some or all floating point numbers are converted to binary numbers. However, the floating point number arithmetic does not need to follow IEEE or any particular standard. Table 1 illustrates IEEE single precision floating point format.",{"@attributes":{"id":"p-1592","num":"1730"},"tables":{"@attributes":{"id":"TABLE-US-00014","num":"00014"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 1"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"IEEE single precision floating point number format"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"98pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"56pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"63pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Signed (S)","Exponent (E)","Mantissa (M)"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"98pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"21pt","align":"right"}},{"@attributes":{"colname":"4","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"5","colwidth":"42pt","align":"right"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["0","1","8","9","31"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}}]}}},"\u201cSigned\u201d bit indicates whether a floating point number is a positive (S=0) or negative (S=1) floating point number. For example, if the signed bit is 0, the floating point number is a positive floating point number. \u201cExponent\u201d field (E) is represented by a power of two. For example, if a binary number is 10001.001001=1.0001001001\u00d72, then E becomes 127+4=131=10000011. \u201cMantissa\u201d field (M) represents fractional part of a floating point number.","For example, to add 2.5and 4.75, 2.5is converted to 0x40200000 (in hexadecimal format) as follows:\n\n","Convert the result to a decimal number. In this example, the exponent field of the result is 10000001=129. By subtracting 127from 129, the user obtains 2. Thus, the result is represented by 1.1101\u00d72=111.01. 111is equal to 7. 0.01is equal to 0.25. Thus, the user obtains 7.25.","Although this example is based on single precision floating point numbers, the mechanism used in this example can be extended to double precision floating point numbers. A double precision floating number is represented by 64 bits, i.e., 1 bit for the signed bit, 11 bits for the exponent field and 52 bits for the mantissa field.","Traditionally, in a parallel computing system, floating point number additions in multiple computing node operations, e.g., via messaging, are done in part, e.g., by software. The additions require at per network hop a processor to first receive multiple network packets associated with multiple messages involved in a reduction operation. Then, the processor adds up floating point numbers included in the packets, and finally puts the results back into the network for processing at the next network hop. An example of the reduction operations is to find a summation of a plurality of floating point numbers contributed (i.e., provided) from a plurality of computing nodes. This software had large overhead, and could not utilize a high network bandwidth (e.g., 2 GB\/s) of the parallel computing system.","Therefore, it is desirable to perform the floating point number additions in a collective logic device to reduce the overhead and\/or to fully utilize the network bandwidth.","In one embodiment, the present disclosure illustrates performing floating point number additions in hardware, for example, to reduce the overhead and\/or to fully utilize the network bandwidth.","FIG. -- illustrates a collective logic device  for adding a plurality of floating point numbers in a parallel computing system (e.g., IBM\u00ae Blue Gene\u00ae Q). As shown in FIG. --, the collective logic device  comprises, without restriction, a front-end floating point logic device , an integer ALU (Arithmetic Logic Unit) tree , a back-end floating point logic device . The front-end floating point logic device  comprises, without limitation, a plurality of floating point number (\u201cFP\u201d) shifters (e.g., FP shifter ) and at least one FP exponent max unit . In one embodiment, the FP shifters  are implemented by shift registers performing a left shift(s) and\/or right shift(s). The at least one FP exponent max unit  finds the largest exponent value among inputs  which are a plurality of floating point numbers. In one embodiment, the FP exponent max unit  includes a comparator to compare exponent fields of the inputs . In one embodiment, the collective logic device  receives the inputs  from network links, computing nodes and\/or I\/O links. In one embodiment, the FP shifters  and the FP exponent max unit  receive the inputs  in parallel from network links, computing nodes and\/or I\/O links. In another embodiment, the FP shifters  and the FP exponent max unit  receive the inputs  sequentially, e.g., the FP shifters  receives the inputs  and forwards the inputs  to the FP exponent max unit . The ALU tree  performs integer arithmetic and includes, without limitations, adders (e.g., an adder ). The adders may be known adders including, without limitation, carry look-ahead adders, full adders, half adders, carry-save adders, etc. This ALU tree  is used for floating point arithmetic as well as integer arithmetic. In one embodiment, the ALU tree  is divided by a plurality of layers. Multiple layers of the ALU tree  are instantiated to do integer operations over (intermediate) inputs. These integer operations include, but are not limited to: integer signed and unsigned addition, max (i.e., finding a maximum integer number among a plurality of integer numbers), min (i.e., finding a minimum integer number among a plurality of integer numbers), etc.","In one embodiment, the back-end floating point logic device  includes, without limitation, at least one shift register for performing normalization and\/or shifting operation (e.g., a left shift, a right shift, etc.). In embodiment, the collective logic device  further includes an arbiter device . The arbiter device is described in detail below in conjunction with FIG. --. In one embodiment, the collective logic device  is fully pipelined. In other words, the collective logic device  is divided by stages, and each stage concurrently operates according to at least one clock cycle.","In a further embodiment, the collective logic device  is embedded and\/or implemented in a 5-Dimensional torus network. FIG. -- illustrates a 5-Dimensional torus network . A torus network is a grid network where a node is connected to at least two neighbors along one or more dimensions. The network  includes, without limitation, a plurality of computing nodes (e.g., a computing node ). The network  may have at least 2 GB\/s bandwidth. In a further embodiment, some or all of the computing nodes in the network  includes at least one collective logic device . The collective logic device  can operate at a peak bandwidth of the network .","FIG. -- illustrates a flow chart for adding a plurality of floating point numbers in a parallel computing system. The parallel computing system may include a plurality of computing nodes. A computing node may include, without limitation, at least one processor and\/or at least one memory device. At step  in FIG. --, the collective logic device  receives the inputs  which include a plurality of floating point numbers (\u201cfirst floating point numbers\u201d) from computing nodes or network links. At step , the FP exponent max unit  finds a maximum exponent (i.e., the largest exponent) of the first floating point numbers, e.g., by comparing exponents of the first floating point numbers. The FP exponent max unit  broadcast the maximum exponent to the computing nodes. At step , the front-end floating point logic device  converts the first floating point numbers to integer numbers, e.g., by performing left shifting and\/or right shifting the first floating point numbers according to differences between exponents of the first floating point numbers and the maximum exponent. Then, the front-end floating point logic device  sends the integer numbers to the ALU tree  which includes integer adders (e.g., an adder ). When sending the integer numbers, the front-end floating point logic device  may also send extra bits representing plus(+) infinity, minus(\u2212) infinity and\/or a not-a-number (NAN). NAN indicates an invalid operation and may cause an exception.","At step , the ALU tree  adds the integer numbers and generates a summation of the integer values. Then, the ALU tree  provides the summation to the back-end floating point logic device . At step , the back-end logic device  converts the summation to a floating point number (\u201csecond floating point number\u201d), e.g., by performing left shifting and\/or right shifting according to the maximum exponent and\/or the summation. The second floating point number is an output of adding the inputs . This second floating point numbers is reproducible. In other words, upon receiving same inputs, the collective logic device  produces same output(s). The outputs do not depend on an order of the inputs. Since an addition of integer numbers (converted from the floating point numbers) does not generate a different output based on an order of the addition, the collective logic device  generates the same output(s) upon receiving same inputs regardless of an order of the received inputs.","In one embodiment, the collective logic device  performs the method steps - in one pass. One pass refers that the computing nodes sends the inputs  only once to the collective logic device  and\/or receives the output(s) only once from the collective logic device .","In a further embodiment, in each computing node, besides at least 10 bidirectional links for the 5D torus network , there is also at least one dedicated I\/O link that is connected to at least one I\/O node. Both the I\/O link and the bidirectional links are inputs to the collective logic device . In one embodiment, the collective logic device  has at least 12 inputs. One or more of the inputs may come from a local computing node(s). In another embodiment, the collective logic device  has at most 12 inputs. One or more of the inputs may come from a local computing node(s).","In a further embodiment, at least one computing node defines a plurality of collective class maps to select a set of inputs for a class. A class map defines a set of input and output links for a class. A class represents an index into the class map on at least one computing node and is specified, e.g., by at least one packet.","In another embodiment, the collective logic device  performs the method steps - in at least two passes, i.e., the computing nodes sends (intermediate) inputs at least twice to the collective logic device  and\/or receives (intermediate) outputs at least twice from the collective logic device . For example, in the first pass, the collective logic device  obtains the maximum exponent of the first floating point numbers. Then, the collective logic device normalizes the first floating point numbers and converts them to integer numbers. In the second pass, the collective logic device  adds the integer numbers and generates a summation of the integer numbers. Then, the collective logic device  converts the summation to a floating point number called the second floating point number. When the collective logic device  operates based on at least two passes, its latency may be at least twice larger than a latency based on one pass described above.","In one embodiment, the collective logic device  performing method steps in FIG. -- is implemented in hardware or reconfigurable hardware, e.g., FPGA (Field Programmable Gate Array) or CPLD (Complex Programmable Logic deviceDevice), using a hardware description language (Verilog, VHDL, Handel-C, or System C). In another embodiment, the collective logic device  is implemented in a semiconductor chip, e.g., ASIC (Application-Specific Integrated Circuit), using a semi-custom design methodology, i.e., designing a chip using standard cells and a hardware description language. Thus, the hardware, reconfigurable hardware or the semiconductor chip may operate the method steps described in FIG. --. In one embodiment, the collective logic device  is implemented in a processor (e.g., IBM\u00ae PowerPC\u00ae processor, etc.) as a hardware unit.","Following describes an exemplary floating point number addition according to one exemplary embodiment. Suppose that the collective logic device  receives two floating point numbers A=2*1.5=3and B=2*1.25=10as inputs. The collective logic device  adds the number A and the number B as follows:","I. (corresponding to Step  in FIG. --) The collective logic device  obtains the maximum exponent, e.g., by comparing exponent fields of each input. In this example, the maximum exponent is 3.","II. (corresponding to Step  in FIG. --) A floating point representation for the number A is 0x0018000000000000 (in hexadecimal notation)=1.1\u00d72. A floating point representation for the number B is 0x0034000000000000=1.01\u00d72. The collective logic device  converts the floating point representations to integer representations as follows:\n\n","Thus, when the number A is converted to an integer number, it becomes 0x0180000000000000. When the number B is converted, it becomes 0x0500000000000000. Note that the integer numbers comprise only the mantissa field. Also note that the most significant bit of the number B is two binary digits to the left (larger) than the most significant bit of the number A. This is exactly the difference between the two exponents (1 and 3). III. (corresponding to Step  in FIG. --) The two integer numbers are added. In this example, the result is 0x0680000000000000=0x0180000000000000+0x0500000000000000. IV. (corresponding to Step  in FIG. --) This result is then converted back to a floating point representation, taking into account the maximum exponent which has been passed through the collective logic device  in parallel with the addition as follows:\n\n","In this example, after steps 1-3, 0x0680000000000000 is converted to 0x003a000000000000=2*1.625=13, which is expected by adding 10and 3.","In one embodiment, the collective logic device  performs logical operations including, without limitation, logical AND, logical OR, logical XOR, etc. The collective logic device  also performs integer operations including, without limitation, an unsigned and signed integer addition, min and max with an operand size from 32 bits to 4096 bits in units of (32*2) bits, where n is a positive integer number. The collective logic device  further performs floating point operations including, without limitation, a 64-bit floating point addition, min (i.e., finding a minimum floating point number among inputs) and max (finding a maximum floating point number among inputs). In one embodiment, the collective logic device  performs floating point operations at a peak network link bandwidth of the network.","In one embodiment, the collective logic device  performs a floating point addition as follows: First, some or all inputs are compared and the maximum exponent is obtained. Then, the mantissa field of each input is shifted according to the difference of its exponent and the maximum exponent. This shifting of each input results in a 64-bit integer number which is then passed through the integer ALU tree  for doing an integer addition. A result of this integer addition is then converted back to a floating point number, e.g., by the back-end logic device .","FIG. -- illustrates an arbiter device  in one embodiment. The arbiter device  controls and manages the collective logic device , e.g., by setting configuration bits for the collective logic device . The configuration bits define, without limitation, how many FP shifters (e.g., an FP shifter ) are used to convert the inputs  to integer numbers, how many adders (e.g., an adder ) are used to perform an addition of the integer numbers, etc. In this embodiment, an arbitration is done in two stages: first, three types of traffic (user \/system \/subcomm ) arbitrate among themselves; second, a main arbiter  chooses between these three types (depending on which have data ready). The \u201cuser\u201d type  refers to a reduction of network traffic over all or some computing nodes. The \u201csystem\u201d type  refers to a reduction of network traffic over all or some computing nodes while providing security and\/or reliability on the collective logic device. The \u201csubcomm\u201d type  refers to a rectangular subset of all the computing nodes. However, the number of traffic types is not limited to these three traffic types. The first level of arbitration includes a tree of 2-to-1 arbitrations. Each 2-to-1 arbitration is round-robin, so that if there is only one input request, it will pass through to a next level of the tree , but if multiple inputs are requesting, then one will be chosen which was not chosen last time. The second level of the arbitration is a single 3-to-1 arbiter, and also operates a round-robin fashion.","Once input requests has been chosen by an arbiter, those input requests are sent to appropriate senders (and\/or the reception FIFO)  and\/or . Once some or all of the senders grant permission, the main arbiter  relays this grant to a particular sub-arbiter which has won and to each receiver (e.g., an injection FIFO  and\/or ). The main arbiter  also drives correct configuration bits to the collective logic device . The receivers will then provide their input data through the collective logic device  and an output of the collective logic device  is forwarded to appropriate sender(s).","Integer Operations","In one embodiment, the ALU tree  is built with multiple levels of combining blocks. A combining block performs, at least, an unsigned 32-bit addition and\/or 32-bit comparison. In a further embodiment, the ALU tree  receives control signals for a sign (i.e., plus or minus), an overflow, and\/or a floating point operation control. In one embodiment, the ADD tree  receives at least two 32-bit integer inputs and at least one carry-in bit, and generates a 32-bit output and a carry-out bit. A block performing a comparison and\/or selection receives at least two 32-bit integer inputs, and then selects one input depending on the control signals. In another embodiment, the ALU tree  operates with 64-bit integer inputs\/outputs, 128-bit integer inputs\/outputs, 256-bit integer inputs\/outputs, etc.","Floating Point Operations","In one embodiment, the collective logic device  performs 64-bit double precision floating point operations. In one embodiment, at most 12 (e.g., 10 network links+1 I\/O link+1 local computing node) floating point numbers can be combined, i.e., added. In an alternative embodiment, at least 12 floating point number are added.","A 64-bit floating point number format is illustrated in Table 2.",{"@attributes":{"id":"p-1621","num":"1779"},"tables":{"@attributes":{"id":"TABLE-US-00015","num":"00015"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"IEEE double precision floating point number format"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"91pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"56pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"70pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Signed (S)","Exponent (E)","Mantissa (M)"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"91pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"right"}},{"@attributes":{"colname":"4","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"5","colwidth":"42pt","align":"right"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["0","1","11","12","63"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}}]}}},"In IEEE double precision floating point number format, there is a signed bit indicating whether a floating point number is an unsigned or signed number. The exponent field is 11 bits. The mantissa field is 52 bits.","In one embodiment, Table 3 illustrates a numerical value of a floating point number according to an exponent field value and a mantissa field value:",{"@attributes":{"id":"p-1624","num":"1782"},"tables":{"@attributes":{"id":"TABLE-US-00016","num":"00016"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 3"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Numerical Values of Floating Point Numbers"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"1","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"56pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"77pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Exponent ","Exponent","Exponent",{}]},{"entry":["field binary","field value","(E)","Value"]},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"1","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"56pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"77pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["11 . . . 11","2047",{},"If M = 0, +\/\u2212 Infinity"]},{"entry":[{},{},{},"If M ! = 0, NaN"]},{"entry":["Non zero","1 to 2046","\u22121022 to ","(\u22121){circumflex over (\u2009)}S * 1.M * 2{circumflex over (\u2009)}E"]},{"entry":[{},{},"1023",{}]},{"entry":["00 . . . 00","0","zero or","+\/\u2212 0, when x = 0;"]},{"entry":[{},{},"denormalized ","(\u22121){circumflex over (\u2009)}S * 0.M * 2{circumflex over (\u2009)}(\u22121022)"]},{"entry":[{},{},"numbers"]},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}}]}}]}}},"If the exponent field is 2047 and the mantissa field is 0, a corresponding floating point number is plus or minus Infinity. If the exponent field is 2047 and the mantissa field is not 0, a corresponding floating point number is NaN (Not a Number). If the exponent field is between 1 and 2046, a corresponding floating point number is (\u22121)\u00d70.M\u00d72. If the exponent field is 0 and the mantissa field is 0, a corresponding floating point number is 0. If the exponent field is 0 and the mantissa field is not 0, a corresponding floating point number is (\u22121)\u00d70.M\u00d72. In one embodiment, the collective logic device  normalizes a floating point number according to Table. 3. For example, if S is 0, E is 2=10and M is 1000000000000000000000000000000000000000000000000000, a corresponding floating number is normalized to 1.1000 . . . 000\u00d72.","In one embodiment, an addition of (+) infinity and (+) infinity generates (+) infinity, i.e., (+) Infinity+(+) Infinity=(+) Infinity. An addition of (\u2212) infinity and (\u2212) infinity generates (\u2212) infinity, i.e., (\u2212) Infinity+(\u2212) Infinity=(\u2212) Infinity. An addition of (+) infinity and (\u2212) infinity generates NaN, i.e., (+) Infinity+(\u2212) Infinity=NaN. Min or Max operation for (+) infinity and (+) infinity generates (+) infinity, i.e., MIN\/MAX (+Infinity, +Infinity)=(+) infinity. Min or Max operation for (\u2212) infinity and (\u2212) infinity generates (\u2212) infinity, i.e., MIN\/MAX (\u2212Infinity, \u2212Infinity)=(\u2212) infinity.","In one embodiment, the collective logic device  does not distinguish between different NaNs. An NaN newly generated from the collective logic device  may have the most significant fraction bit (the most significant mantissa bit) set, to indicate NaN.","Floating Point (FP) Min and Max","In one embodiment, an operand size in FP Min and Max operations is 64 bits. In another embodiment, an operand size in FP Min and Max operations is larger than 64 bits. The operand passes through the collective logic device  without any shifting and\/or normalization and thus reduces an overhead (e.g., the number of clock cycles to perform the FP Min and\/or Max operations). Following describes the FP Min and Max operations according to one embodiment. Suppose that \u201cI\u201d be an integer representation (i.e., integer number) of bit patterns for 63 bits other than the sign bit. Given two floating point numbers A and B,","if (Sign(A)=0 and Sign(B)=0, or both positive) then","if (I(A)>I(B)), then A>B.","(If both A and B are positive numbers and if A's integer representation is larger than B's integer representation, A is larger than B.)","if (Sign(A)=0, and Sign(B)=1), then A>B.","(If A is a positive number and B is a negative number, A is larger than B.)","if (Sign(A)=1 and Sign(B)=1, both negative) then","if (I(A)>I(B)), then A<B.","(If both A and B are negative numbers and if A's integer representation is larger than B's integer representation (i.e., |A|>|B|), A is smaller than B.)","Floating Point ADD","In one embodiment, operands are 64-bit double precision Floating point numbers. In one embodiment, the operands are 32 bits floating point numbers, 128 bits floating point numbers, 256 bits floating point numbers, 256 bits floating point numbers, etc. There is no reordering on injection FIFOs - and\/or reception FIFOs -.","In one embodiment, when a first half of the 64-bit floating point number is received, the exponent field of the floating point number is sent to the FP exponent max unit  to get the maximum exponent for some or all the floating point numbers contributing to an addition of these floating point numbers. The maximum exponent is then used to convert each 64-bit floating point numbers to 64-bit integer numbers. The mantissa field of each floating point numbers has a precision of 53 bits, in the form of 1.x for regular numbers, and 0.x for denormalized numbers. The converted integer numbers reserve 5 most significant bits, i.e., 1 bit for a sign bit and 4 bits for guarding against overflow with up to 12 numbers being added together. The 53-bits mantissa field is converted into a 64-bit number in the following way. The left most 5 bits are zeros. The next bit is one if the floating point number is normalized and it is zero if the floating point number is denormalized. Next, the 53-bit mantissa field is appended and then 6 zeroes are appended. Finally, the 64-bit number is right-shifted by Emax\u2212E, where Emax is the maximum exponent and E is a current exponent value of the 59-bit number. E is never greater than Emax, and so Emax\u2212E is zero or positive. After this conversion, if the sign bit retained from the 64-bit floating point number, then the shifted number (\u201cN\u201d) is converted to 2's complementary format (\u201cN_new\u201d), e.g., by N_new=(not N)+1, where \u201cnot N\u201d may be implemented by a bitwise inverter. A resulting number (e.g., N_new or N) is then sent to the ALU tree  with a least significant 32-bit word first. In a further embodiment, there are additional extra control bits to identify special conditions. In one embodiment, each control bit is binary. For example, if the NaN bit is 0, then it is not a NaN, and if it is 1, then it is a NaN. There are control bits for +Infinity and \u2212Infinity as well.","The resulting numbers are added as signed integers with operand sizes of 64 bits, with a consideration to control bits for Infinity and NaN. A result of the addition is renormalized to a regular floating point format: (1) if a sign bit is set (i.e., negative sum), covert the result back from 2's complementary format using, e.g., K_new=not (K\u22121), where K_new is the converted result and K is the result before the converting; (2) Then, right or left shift K or K_new until the left-most bit of the final integer sum (i.e., an integer output of the ALU ) which is a \u20181\u2019 is in the 12bit position from the left of the integer sum. This \u20181\u2019 will be a \u201chidden\u201d bit in the second floating point number (i.e., a final output of adding of floating point numbers). If the second floating point number is a denormalized number, shift right the second floating point number until the left-most \u20181\u2019 is in the 13position, and then shift to the right again, e.g., by the value of the maximum exponent. The resultant exponent is calculated as Emax+the amount it was right-shifted\u22126, for normalized floating point results. For denormalized floating point results, the exponent is set to the value according to the IEEE specification. A result of this renormalization is then sent on with most significant 64-bit word to computing nodes as a final result of the floating point addition.","Global Clock","There are a wide variety of inter-chip and intra-chip clock frequencies required for BG\/Q. The processor frequency is 1.6 GHz and portions of the chip run at fractions of this speed, e.g., \/2, \/4, \/8, or \/16 of this clock. The high speed communication in BG\/Q is accomplished by sending and receiving data between ASICs at 4Gb\/s, or 2.5 times the target processor frequency of 1.6 GHz. All signaling between BG\/Q ASICs is based on IBM Micro Electronic Division (IMD) High Speed I\/O which accepts an input clock at \u215b the datarate, or 500 MHz. The optical communication is at 8Gb\/s but due to the need for DC balancing of the currents, this interface is 8b-10b encoded and runs at 10 Gb\/s with an interface of 1 GBs\/. The memory system is based on SDRAM-DDR3 at 1.333 Gb\/s (667 MHz address frequency).","These frequencies are generated on the BQC chip through Phase Locked Loops. The PLLs are driven from a single global 100 MHz clock.","The BG\/P clock network uses over 10,000 1-10 PECL clock redrive buffers to distribute the signal derived from a single source to the up to 36 racks or beyond. There are 7 layers to the clock tree. The first 3 layers exist on the 1->10 clock fanout cards on each rack, connected with max differential cables. The next 4 layers exist on the service and node or I\/O boards themselves. For a 96-rack BG\/Q system, IBM has designed an 8-layer LVPECL clock redrive tree with slightly longer rack-to-rack cables. The service card contains circuitry to drop a clock pulse, with the number of clocks to be dropped and the spacing between dropped clocks variable. Glitch detection circuitry in BQC detects these clock glitches and uses them for tight synchronization.  shows an intra-rack clock fanout designed for the BG\/Q 96 rack system with racks in a row on 5 foot pitch, and optional I\/O racks at the end of each row.","While modern processing systems have clock frequencies in a multi-GHz range, this may result in communications paths between processors necessarily involving multiple clock cycles. Additionally, the clock frequencies in modern multiprocessor systems are not all exactly equal, as they are typically derived from multiple local oscillators that are each directly used by only a small fraction of the processors in the multiprocessor systems. Having all processors utilize the same clock may require that all modules in the system receive a single global clock signal, thereby requiring a global clock network. Both the lack of a global clock signal and the complexities of synchronization of chips when communication distances between chips are many cycles may result in an inability of modern systems to exactly synchronize.","Thus, in a further aspect, there is provided a system, method and computer program product for synchronizing a plurality of processors in a parallel computing system.","That is, in one aspect, there is a method, a system and a computer program product by which a global clock network can be enhanced along with innovative circuits inside receiving devices to enable global clock synchronization. By achieving the global clock synchronization, the multiprocessor system may enable exact reproducibility of processing of instructions. Thus, this global clock synchronization may assist to accurately reproduce processing results in a system-wide debugging mechanism.","This disclosure describes a method, system and a computer program product to generate and\/or detect a global clock signal having a pulse width modification in one or more selected clock period(s). In the present disclosure, a global clock signal can be used as an absolute phase reference signal (i.e., a reference signal for a phase correction of a clock signal) as well as a clock signal to synchronize processors in the parallel computing system. A global clock signal can be used for a synchronized system with a resetting capability, network synchronization, pacing of parallel calculations and power management in a parallel computing system. This disclosure describes a clock signal with modulated clock pulse width used for a global synchronization signal. This disclosure also describes a method, system and a computer program product for generating a global synchronization signal (e.g., a signal  in FIG. --) based on the global clock signal with the pulse width modification. A global synchronization signal refers to a signal that can be used to notify a plurality of processors to synchronize, for example, to perform instructions, operations and others. In other words, the global synchronization signal can cause an interrupt signal to one or more of processors in a parallel computing system. A pulse width modulation refers to a technique for modifying one or more clock pulses in a clock signal. The parallel computer system may derive their processor clocks from the global clock signal having the pulse width modification. This disclosure also describes how a single clock signal can be used to enable processor synchronization in a parallel computing system.","FIG. -- illustrates a system diagram for generating a global clock signal in which one or more clock pulse(s) has been modified in one embodiment. In FIG. --, a clock generation circuit  generates a global clock signal with pulse modification(s). The clock generation circuit  includes, but is not limited to: an oscillator , a clock synthesizer , a clock divider and splitter , a hardware module , a flip flop  and a clock splitter . FIG. -- illustrates a flow chart describing method steps that clock generation circuit  operates. For clarity of explanation, the functional components of FIG. -- are described with reference to method steps in FIG. --. At step  in FIG. --, an oscillator (e.g., an oscillator  in FIG. --, a spread-spectrum VSS4 oscillator from Vectron\u2122 International, Inc., and\/or others) generates a stable fixed frequency signal (e.g., 25 MHz oscillating signal). At step  in FIG. --, a clock synthesizer (e.g., a clock synthesizer  in FIG. --, a CDCE62005 from Texas Instruments\u00ae Incorporated., hereinafter \u201cTI\u201d, and\/or others) generates a first clock signal based on the stable fixed frequency signal. For example, if the oscillator  generates a 25 MHz oscillating signal, the clock synthesizer  produces 400 MHz clock signal, e.g., by multiplying the 25 MHz oscillating signal. CDCE949 and CDCEL949 from TI are commercial products that perform clock signal synthesis (i.e., clock signal generation), clock signal multiplication (e.g., generating a 400 MHz clock signal from a 100 MHz clock signal), and clock signal division (e.g., generating a 200 MHz clock signal from a 400 MHz clock signal).","At step  in FIG. --, a clock divider\/splitter (e.g., clock divider and splitter  in FIG. --, CDCE949 and CDCEL949 from TI, and\/or others) divides a clock frequency of the first clock signal to generate a second clock signal, e.g., by dividing by \u201cN\u201d, and splits the first clock signal and the second clock signal. Vakil, et al., \u201cLow skew minimized clock splitter,\u201d U.S. Pat. No. 6,466,074, wholly incorporated by reference as if set forth herein, describes a clock splitter in detail. For example, as shown in FIG. --, the clock divider and splitter  receives a 400 MHz first clock signal from the clock synthesizer  and outputs a 200 MHz second clock signal to a hardware module (e.g., an FPGA (Field Programmable Gate Array) or CPLD (Complex Programmable Logic Device)  in FIG. --) and outputs the 400 MHz first clock signal to a flip flop (e.g., D flip flop  in FIG. --).","At step  in FIG. --, the hardware module  divides a clock frequency of the second clock signal to generate a third clock signal and performs a pulse width modulation on the third clock signal. The pulse width modulation changes a pulse width within a clock period in the third clock signal. In one embodiment, the hardware module is reconfigurable, i.e., the hardware module can be modified or updated by loading different code.","In one embodiment, a user configures the hardware module, e.g., through a hardware console (e.g., JTAG) by loading code written by a hardware description language (e.g., VHDL, Verilog, etc.). The hardware module  may include, but is not limited to: a logical exclusive OR gate for narrowing a pulse width within a clock period in the third clock signal, a logical OR gate for widening a pulse width within a clock period in the third clock signal, and\/or another logical exclusive OR gate for removing a pulse within a clock period within the second clock signal. The hardware module  may also include a counter device to divide clock signal frequency and to determine a specific clock cycle to perform a pulse width modification.","FIG. --illustrates an example of removing a pulse within a clock period in a clock signal. In this example, the clock divider and splitter  receives a 200 MHz first clock signal () from the clock synthesizer  and outputs a 100 MHz second clock signal () to the hardware module . The hardware module  generates a pulse (), e.g., by counting the number of rising edges in the 100 MHz second clock signal () and generating a pulse when the counting reaches a certain number (e.g., a determined number two). The pulse shown at , also referred to as a gate pulse is used to determine which clock period in the 100 MHz second clock signal () is going to be modified. In this example, there is a pulse () at a location () corresponding to the second pulse () in the 100 MHz second clock signal (). The location () of this pulse () corresponds to the second pulse () in the 100 MHz second clock signal (). Thus, it is determined that the second pulse () is to be modified as shown at FIG. --. To remove the second pulse in the 100 MHz second clock signal (), the hardware module  performs a logical exclusive OR operation between the 100 MHz second clock signal () and the pulse () and generates a pulse width modified clock signal ().","FIG. --illustrates an example of narrowing a pulse width within a clock period in the third clock signal. In this example, the clock divider and splitter  receives a 400 MHZ first clock signal () from the clock synthesizer  and outputs a 200 MHz second clock signal () to the hardware module . The hardware module  generates a pulse (), e.g., by counting the number of rising edges in the 200 MHz second clock signal () and generating a pulse when the counting reaches a certain number (e.g., a determined number 2). The hardware module  also divides the clock frequency of the 200 MHz second clock signal () to generate a 100 MHz third clock signal (). The pulse shown at , also referred to as a gate pulse, is used to determine which clock period in the 100 MHz third clock signal () is going to be modified. In this example, there is a pulse () at a location () corresponding to the second pulse () in the 100 MHz third clock signal (). The location () of this pulse () corresponds to the second pulse () in the 100 MHz third clock signal (). Thus, it is determined that the second pulse () is to be modified as shown at FIG. --. To narrow the second pulse in the 100 MHz third clock signal (), the hardware module  performs a logical exclusive OR operation between the 100 MHz third clock signal () and the pulse () and generates a pulse width modified clock signal ().","To widen a clock pulse in a clock signal, after generating the pulse (), the hardware module  may shift the pulse (), e.g., shift left or right the pulse () by a fraction of a clock cycle such as a quarter or half cycle of the 100 MHz third clock signal () and perform a logical OR operation between the shifted pulse and the 100 MHz third clock signal () to generate a pulse width modified clock signal.","FIG. --illustrates an example of widening a pulse width within a clock period in the third clock signal. In this example, the clock divider and splitter  receives a 400 MHZ first clock signal () from the clock synthesizer  and outputs a 200 MHz second clock signal () to the hardware module . The hardware module  generates a pulse (), e.g., by counting the number of rising edges in the 200 MHz second clock signal () and generating a pulse when the counting reaches a certain number (e.g., a determined number 2). The hardware module  also divides the clock frequency of the 200 MHz second clock signal () to generate a 100 MHz third clock signal (). The pulse shown at , also referred to as a gate pulse, is used to determine which clock period in the 100 MHz third clock signal () is going to be modified. In this example, there is a pulse () at a location () corresponding to the second pulse () in the 100 MHz third clock signal (). The location () of this pulse () corresponds to the second pulse () in the 100 MHz third clock signal (). Thus, it is determined that the second pulse () is to be modified as shown at FIG. --. To widen the second pulse in the 100 MHz third clock signal (), the hardware module  performs a logical OR operation between the 100 MHz third clock signal () and the pulse () and generates a pulse width modified clock signal ().","Referring again to FIG. --, at step , a flip flop (e.g., a D flip flop  in FIG. --) receives a pulse width modified clock signal (e.g., a signal  or signal  in FIGS. -----) and filters the pulse width modified clock signal, e.g., by removing jitters in the pulse width modified clock signal. At step , a clock splitter (e.g., a clock splitter  in FIG. --) receives the filtered clock signal from the flip flop , an optional external clock signal from other sources , and a selection signal for selecting the filtered clock signal or the external clock signal from the hardware module . Then, the clock splitter outputs a selected signal (i.e., the filtered clock signal or the external clock signal) to a plurality of processors in a
