---
title: Method and apparatus for providing refractive transparency in selected areas of video displays
abstract: In a selected area of a display defined by a polygon, magnifying is simulated. The selected area may be, for example, a circle simulating a magnifying glass. Textures are represented by texel coordinates U and V, which specify the location of color components within a set of image data. Within the area selected to appear magnified, the present invention perturbs the texel location selection to simulate an angle of refraction in the selected area and offset texel coordinates.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=06894696&OS=06894696&RS=06894696
owner: Sony Electronics Inc.
number: 06894696
owner_city: Park Ridge
owner_country: US
publication_date: 20011221
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD OF INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION"],"p":["The present invention relates to computer graphics and more particularly to efficiently simulate refraction through a magnifier in a computer display.","There is a continuing quest for visual realism in computer entertainment. The two-dimensional surface of a computer display is utilized to display three-dimensional effects. Pixels are made to portray real objects and are used to produce special effects. The image is often composed from textured polygons. A wallpaper, pattern or other effect is applied to the polygons being rendered in the scene. The pixels are derived from rendering three-dimensional data.","In computer graphics, existing texture-rendering techniques map a pixel on a screen (typically using screen coordinates (x, y)) to a polygon, such as a triangle, on a surface in a viewing plane (typically using geometric or surface coordinates (s, t)). The polygon is rasterized into a plurality of smaller pieces called fragments. Each polygon may have information, such as color and\/or a normal vector, associated with each vertex of the polygon. To assign a texture (i.e., a color pattern or image, either digitized or synthesized) to a fragment, the fragment is mapped onto a texture map (typically using texture coordinates (u, v)). A texture map represents a type of image, such as stripes, checkerboards, or complex patterns that may characterize natural materials. Texture maps are stored in a texture memory. A texture map comprises a plurality of texels. A texel is the smallest graphical element in a 2-D texture map used to render a 3-D object. A texel represents a single color combination at a specific position in the texture map.","Each texture map has a plurality of associated MIP (multi in parvum) maps, which are abbreviated versions of a full texture map. One of the MIP maps may be selected to provide a suitable resolution for the fragment of the polygon being rasterized. Several techniques exist to interpolate the desired color information from one or more MIP levels. These texel selection techniques are known technology. The final texture color derived from the selected MIP map is applied onto the fragment. The applied texture may be blended with a color already associated with the fragment or polygon.","In computer entertainment, a two-dimensional surface of a computer display is used to display three-dimensional graphics. This is done by supplying information to individual pixels comprising the display. Real objects are portrayed and special effects are used. In order to portray a realistic view of three-dimensional objects, it is necessary to provide a geometrical description of those objects. In order to simplify the description of an object surface, the three-dimensional object is resolved into a representation by polygons. The smooth surface of a curved object is resolved into a faceted surface of polygons. Generally, the polygon used is the triangle. Triangles provide a number of advantages, particularly having vertices which are of necessity co-planar.","Data is collected characterizing points defining vertices of the triangles for rendering a two-dimensional screen image of three-dimensional objects. Each vertex has a local normal, a line projecting perpendicularly from the three-dimensional surface. An eye point is selected relative to a plane of projection. A line from the eye point to the vertex forms an \u201ceye point \u03b4 angle\u201d with the local normal, which is resolved into x-z and y-z angles. In effect, the intersection of the line from the eye point through the plane of projection defines the pixels(s) at which the vertex appears in the two-dimensional display.","One prior art special effect is establishing a selected area and creating the effect that is a transparent panel through which the image is seen. One prior art technique blends colors of a selected area with colors whatever is to appear behind them. One special effect that has not been provided in consumer products is 3-D rendering to create a magnifying (or magnifying by a factor of less than one) through an area of interest. An area of interest may be a circle representing a simulated magnifying glass. Some prior art ray tracing systems have provided this effect. However, ray tracing systems are complex and are not usable practically as a viable consumer product due to their expense.","In accordance with the present invention, a method and apparatus are provided in which polygons in a selected area of a display is provided with the special effect of distortion as produced by thick glass or lens. The selected area may be, for example, a circle simulating a magnifying glass. Textures are represented by texel coordinates U and V, which specify the location of color components within a set of image data. Within the area selected to appear magnified, the present invention perturbs the texel location selection to simulate an angle of refraction in the selected area. This is most conveniently and efficiently done during the texturing operation. The scene may also be animated through a rendering of a real-time sequence of images appearing to be shown through a magnifying glass by re-mapping a different portion of the texture in each frame. Optionally, prior to rendering the polygons within the simulated magnifying glass area, the content of the frame buffer could be captured and used as a texture and the refraction effect could then be applied to the entire capture frame.",{"@attributes":{"id":"p-0019","num":"0018"},"figref":["FIG. 1","FIG. 1"],"b":["2","5","2","7","8","9","2","12","2","2","2","2","14","5","14","2","2","2","16","5"]},{"@attributes":{"id":"p-0020","num":"0019"},"figref":["FIG. 2","FIG. 2"],"b":["5","1","16"]},{"@attributes":{"id":"p-0021","num":"0020"},"figref":["FIGS. 3","FIG. 3","FIGS. 4 and 5"],"b":["4","5","40","5","1","2","40","40","40","42","43","44","45","40","46","46","40","40","50","50"]},"In order to illustrate orientation of an eye point  with respect to the local surface normal , a line segment  is illustrated connecting the eye point  to the origin of the surface normal  at the surface of the triangle . In this manner, an eye point \u03b4 angle  is defined in the \u00be view of FIG. . The angle  has a projection  in the X-Z plane in  projection  in the Y-Z plane of FIG. .","For a given scene, the eye point \u03b4 angle  with the surface normal is preferably established for each vertex ,  and  during image rendering and then stored.","The present invention is utilized in conjunction with texture mapping. Texture mapping is the process of projecting a position on one plane onto another plane. Texture mapping is one way of providing a bit map image onto a rendered surface. This is the prior art application of texture mapping. A separate rendering process is to produce the textured image. In the present embodiment, a first plane corresponds to that of the polygon  and the second plane is the viewing surface of display . The planes may be in any position or orientation relative to each other in three dimensions. The projection calculations incorporate offset, rotation and perspective transformations.","Texture maps are bit maps composed themselves by pixels. Textures are, in general, generated as Multi in Parvam (MIP) maps. The color components within texture maps are referred to as texels to avoid confusion with screen pixels. While in the prior art and in the area of the display  outside of the circle , all textures on the polygon  are projected at the same in accordance with their stored eye point \u03b4 angle. However, for points within the same circle  to simulate the magnifying effect, the eye point \u03b4 angle  is perturbed. In other words, as seen in , no change in the positions of the letters A, F and G occurs. However, in , the angle of projection of the letters B, C, D and E is changed. The letters C and D are made to appear larger, and the letters B and E are refracted so greatly they are not within the field of view of the magnified area, i.e. the selected area .",{"@attributes":{"id":"p-0026","num":"0025"},"figref":["FIG. 6","FIG. 7"],"b":["60","62","64","66","68","68","62","74","68","76","74","76","62","68","80","82","68","86","88","68","68","62","66","68","68"]},"Rendering is achieved through what is referred to as the \u201cgraphics pipeline\u201d illustrated in , the first step is to project geometry that describes the polygonal data onto a plane. At block , image data is accessed from the host memory  for rasterization of a polygon. Operation proceeds to block  where a determination is made as to whether refractive transparency, i.e., the magnification effect, will be applied to that polygon. If not, operation proceeds in accordance with existing rendering methodology at block . Polygon lighting calculations are also performed during this step. This process may be performed in the host CPU .","The accessed data comprises a particular number of geometric descriptions of a 3-dimensional polygon projected onto a two-dimensional plane. The two-dimensional plane represents the display screen  (interspersed with this data will be control through overall system settings such as ambient light value and other global attributes). The ASICs on the graphics processor  convert the transformed and lit geometric data into a set of fragment data through a process called rasterization. The fragment is a portion of a polygon that covers the area assigned to correspond to one pixel on the display. Subdividing polygon data into a set of fragment data where each fragment indicates the contribution of a polygon to a particular pixel, i.e. a display location on the display, is called rasterization.","If, at block  the magnification effect is selected to be provided, operation proceeds at block . All of the data associated with each vertex ,  and  is interpolated across the fragment data as it is processed to provide the values at block . The eye point angle is angle is included as part of the data interpolated on a per fragment basis. The eye point angle is represented by eye point \u03b4x and eye point \u03b4y. These values are projected into the X-Z and Y-Z planes to produce the scalar eye point values in X and Y that are stored with the vertex data.","At block , multiplying is done using values obtained at block . This multiplication is performed for each polygon fragment. Many different particular mechanisms for multiplying are well known in the art. The eye point \u03b4 angle stored at each vertex can be pre-scaled by a constant factor N to modify the magnitude of the refractive effect. Offsetting the U and V values by the scale eye point \u03b4 angles is defined as:\n\n(*(eye point \u03b4x angle)) \n\n\n\n(*(eye point \u03b4y angle \n\nN represents a means of controlling the effect to achieve the desired result. N<1 represents magnification. N>1 represents demagnification. The new U and V prime values are used to specify the texel location in the texture image to be used with the fragment. In performing calculations, actual angles values may be used, but it is most convenient to normalize and angular value to pre-selected linear displacement. As seen at block , texturing is next performed. For achieving the refracting effect, U and V coordinates associated with each fragment are modified by eye points values generated for that fragment. This step cannot be pre-processed to simply modify the U and V coordinates at each vertex by eye point values. The per fragment modification involves a non-linear relationship that cannot be interpolated between vertices. The non-linearity relates to the use of divide by w (or multiple by 1\/w), which is a standard prior art function used in automatic perspective correction. At block , z buffering is provided. Z buffering comprises eliminating fragments that are occluded by those closer to the plane of projection. The contents of the z buffer are continuously scanned out to the video display or computer monitor.\n",{"@attributes":{"id":"p-0031","num":"0030"},"figref":["FIG. 8","FIGS. 8","FIGS. 3-5","FIGS. 8","FIG. 8"],"i":["a ","b","a ","b","b"],"b":["8","160","164","5","160","164","8","160","164","160","164","5","180","170","5"]},"The specification has been written with a view toward enabling those skilled in the art to make modifications in the particular example illustrated in order to provide the magnification special effects in accordance with the present invention."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The invention, both as to its organization and manner of operation, may be further understood by the following description taken in connection with the following drawings.","Of the drawings:",{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":["FIG. 2","FIG. 1"]},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":["FIGS. 4 and 5","FIG. 3"]},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":["FIG. 8","FIGS. 1 and 2"]}]},"DETDESC":[{},{}]}
