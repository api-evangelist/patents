---
title: System and method for measuring clarity of images used in an iris recognition system
abstract: An image quality measuring method enables a biometric image to be evaluated to determine whether the biometric image data are adequate for identification processing. The method includes converting a biometric image to dimensionless image data, filtering the dimensionless image data with a band pass filter, identifying a plurality of portions in the filtered data as containing identification features, each portion in the plurality having an information measurement that indicates feature content greater than portions in the filtered data that are excluded from the plurality, and measuring clarity for the biometric image from the identified plurality of portions in the filtered data.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08260009&OS=08260009&RS=08260009
owner: Indiana University Research and Technology Corp.
number: 08260009
owner_city: Indianapolis
owner_country: US
publication_date: 20080815
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"p":["This application claims priority from international application PCT\/US2008\/073298, which was filed on Aug. 15, 2008 and which claims priority from U.S. Provisional Application Ser. No. 60\/964,797, which was filed on Aug. 15, 2007.","This invention was made with government support under grant N00014-07-1-0788, awarded by the Office of Naval Research. The government has certain rights to this invention.","The system and method disclosed below relates to biometric identification, and, more particularly, to identification systems that use images of a person's iris to identify the person.","Biometric identification systems are known. In these systems, an image is typically taken of some aspect of a person's physiology and information from the image is compared to stored data corresponding to that physiological aspect. The degree of correlation between the acquired image and the stored data determines whether the person corresponding to the acquired image is the person from which the stored data has been obtained. The stored data may correspond to a person's fingerprint, face, and\/or voice. Each type of biometric possesses advantages and disadvantages. For example, fingerprints require contact with a person to obtain the image of the fingerprint for comparison to the stored data. Because contact with a person to be identified is not always possible, this form of identification may be problematic.","One reliable way of identifying persons at a distance has been identification of a person through an image of a human eye iris. The iris of a human eye possesses a pattern of high complexity that changes very little over the life of a person. Iris patterns are so unique that the iris patterns of the left and right eyes of the same person are different. Additionally, the iris patterns can be obtained at a distance using a near infrared (NIR) camera with an appropriate lens. The iris is protected by the cornea of an eye. The uniqueness and relatively minor changes in the iris under different environmental conditions makes the iris a good candidate for automated and highly reliable personal identification.","In previously known iris identification systems, such as the one disclosed in U.S. Pat. No. 5,291,560 to Daugman, an image of a person's eye is obtained and then processed to identify the portion of the eye that corresponds to the iris. Data from the iris that are not occluded by the eyelids may be used to generate a raw data signal. This signal may then be filtered using a pair of two-dimensional Gabor filters to extract pattern information from the raw data signal. The resulting data signal may be compared to stored data for identification purposes. In the Daugman reference, Hamming distances are selected to vary the criteria for evaluating an identification match.","The quality of the iris image that is used for identification evaluation considerably affects the accuracy of the system. Failures to detect imposters and false identification of imposters are more likely to occur with blurred iris images. Many factors affect the quality of an iris image. These factors include blurriness, resolution, image contrast, iris occlusion, and iris deformation. Blurriness, however, remains one of the most significant problems for iris image acquisition. Methods that have been used to evaluate the quality of an iris image have been adversely affected by occlusion of the iris that occurs from the eyelids covering a portion of the iris. Being able to assess the quality of an iris image would help ensure that an iris identification system is obtaining an iris image containing sufficient information for identifying a person.","An image quality measuring method enables an evaluation of an iris image to determine whether the iris image is adequate for identification processing. The method may also be used to evaluate other types of biometric data. The method includes converting a biometric image to dimensionless image data, filtering the dimensionless image data with a band pass filter, identifying a plurality of portions in the filtered data as containing identification features, each portion in the plurality having an information measurement that indicates feature content greater than portions in the filtered data that are excluded from the plurality, and measuring clarity for the biometric image from the identified plurality of portions in the filtered data.","The method may be implemented by a system that evaluates biometric image data before a biometric recognition system processes the image for identification purposes. The system includes a preprocessor configured to convert biometric image data to dimensionless image data, a feature extractor configured to filter the dimensionless data with a band pass filter, a region of interest selector configured to identify a plurality of portions of the filtered image data, each portion in the plurality having an information distance that is indicative of feature content, and a feature information measurement generator configured to measure clarity of the biometric image from the portions in the plurality of portions.","A system  for measuring quality of an iris image is shown in . The system is comprised of an image preprocessor , a clarity measurement generator , an occlusion measurement generator , a dilation measurement generator , and a quality score generator . The clarity measurement generator  includes a feature extractor , a region of interest selector , and a feature information measurement generator . In greater detail, the image preprocessor  receives an image of a human iris from a video camera or other image generating device. The preprocessor  extracts features from the iris image so appropriate regions of interest can be selected and information distances for the selected regions measured. The occlusion measurement generator  and the dilation measurement generator  also receive the iris image for generation of occlusion and dilation in the image. The image score generator  weights the feature information measurement, the occlusion measurement, and dilation measurement to generate a quality score for the image. This score indicates whether the iris image contains sufficient information for a reliable identification.","The system  may be implemented on a computer system having a processor and associated input\/output circuitry and memory. For example, the system  may be a computer system having a CPU processor operating at 1 GHz. The system may also include a 1 GB hard drive and 256 MB of volatile memory, such as static RAM. The received image may be a 200\u00d7300 image. Alternatively, the clarity measurement generator, the dilation measurement generator and\/or the occlusion measurement generator may be implemented with one or more application specific integrated circuits (ASIC) for performing the methods set forth in more detail below. In another embodiment, the clarity measurement generator, the dilation measurement generator, and the occlusion measurement generator may be implemented with programmed instructions stored in program memory for the general purpose processor. These instructions may be executed to extract and transform a portion of the iris image, measure the clarity, occlusion, and dilation of the image, and generate a quality score for the image. In addition, the system  may be integrated with an existing iris recognition system for quality measurement. When integrated with existing iris recognition systems, the preprocessor  may be replaced with the image data preprocessor of the existing system, if it performs the appropriate data transformations, to enable more computationally efficient processing.","In the preprocessor , the portion of the eye image corresponding to the iris is extracted from the image by detection of the pupil and limbic boundaries, the eyelids, and the eyelashes. The limbic boundary is a boundary between the outer circumference of the iris and the sclera or white portion of the eye. The pupil boundary is the meeting of the outer circumference of the pupil and the inner circumference of the iris. Eyelids and eyelashes typically occlude portions of the top and bottom regions of an iris image. To select the most accessible areas of an iris image, the image data are processed with a Canny edge detection method. Two thresholds are used for this method. A higher threshold is used to detect the pupil boundary and a lower threshold is used for detecting the limbic boundary. These physical boundaries and their corresponding edges in the image are depicted in .","The area that lies between the pupil and limbic boundaries that corresponds to iris data is segmented into multiple arcs. These arcs are located at various radii for a polar axis centered in the pupil. The radii may range, for example, from the pupil boundary to the quarter, half, three-quarters, and full length of the limbic radii. The segmented portions of the iris image are transformed from rectangular coordinates to polar coordinates to form rectangular image areas for processing. Because the iris pattern resolution and the actual distance between the pupil boundary and the limbic boundaries may be changed for the same iris by changes in image resolution, variances in camera-to-face distance, and pupil contraction\/dilation, normalization of the segmented iris region is required when the transformation to polar coordinates is computed. For each pixel in the original iris image located at rectangular coordinates (x, y), the polar coordinates (r, \u03b8) may be computed as:",{"@attributes":{"id":"p-0028","num":"0027"},"maths":[{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":["r","i"]},"mo":"=","mrow":{"mfrac":{"mover":{"mi":"L","mo":"~"},"mi":"L"},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"msqrt":{"mrow":{"msup":[{"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["x","i"]},{"mi":"x","mn":"0"}],"mo":"-"}},"mn":"2"},{"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["y","i"]},{"mi":"y","mn":"0"}],"mo":"-"}},"mn":"2"}],"mo":"+"}},"mo":"-","msub":{"mi":"r","mn":"0"}}}}}}},{"@attributes":{"id":"MATH-US-00001-2","num":"00001.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":["\u03b8","i"]},"mo":"=","mrow":{"mo":"{","mtable":{"mtr":[{"mtd":{"mrow":{"mstyle":{"mspace":{"@attributes":{"width":"2.2em","height":"2.2ex"}}},"mo":"\u2062","mrow":{"mrow":{"mfrac":{"mn":"1","mi":"\u0394\u0398"},"mo":["\u2062","\u2062","\u2062"],"mrow":{"mi":"arcsin","mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mrow":[{"msub":[{"mi":["y","i"]},{"mi":"y","mn":"0"}],"mo":"-"},{"msub":[{"mi":["x","i"]},{"mi":"x","mn":"0"}],"mo":"-"}]}}},"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"msub":{"mi":["y","i"]}},"mo":"\u2265","msub":{"mi":"y","mn":"0"}}}}},{"mtd":{"mrow":{"mrow":[{"mrow":{"mfrac":{"mn":"1","mi":"\u0394\u03b8"},"mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mrow":{"mi":"arcsin","mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mrow":[{"msub":[{"mi":["y","i"]},{"mi":"y","mn":"0"}],"mo":"-"},{"msub":[{"mi":["x","i"]},{"mi":"x","mn":"0"}],"mo":"-"}]}}},"mo":"+","mi":"\u03c0"}}},"mo":"\u2062","msub":{"mi":["y","i"]}},{"msub":{"mi":"y","mn":"0"},"mo":"."}],"mo":"<"}}}]}}}}}],"br":{},"sub":["0","0","0 "]},{"@attributes":{"id":"p-0029","num":"0028"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mfrac":{"mrow":{"mn":"2","mo":"\u2062","mi":"\u03c0"},"mi":"\u0394\u0398"},"mo":"."}}},"br":{}},{"@attributes":{"id":"p-0030","num":"0029"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mfrac":{"mrow":{"mn":"2","mo":"\u2062","mi":"\u03c0"},"mi":"\u0394\u0398"},"mo":"."}}},"br":{}},"Log-polar coordinates (r, \u0398) may also be calculated for the normalization as",{"@attributes":{"id":"p-0032","num":"0031"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"msubsup":{"mi":["r","i","log"]},"mo":"=","mfrac":{"mrow":[{"mover":{"mi":"L","mo":"~"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"log","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["r","i"]},{"mi":"r","mn":"0"}],"mo":"\/"}}}},{"mi":"log","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["r","lim"]},{"mi":"r","mn":"0"}],"mo":"\/"}}}]}},"mo":","}}},"br":{},"sub":["lim ","i","i "],"sup":"log "},{"@attributes":{"id":"p-0033","num":"0032"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mfrac":{"mrow":{"mn":"2","mo":"\u2062","mi":"\u03c0"},"mi":"\u0394\u03b8"},"mo":"."}}},"br":{},"figref":["FIG. 3A","FIG. 3B","FIG. 3D","FIG. 3D","FIG. 3B","FIG. 3C","FIG. 3E"],"b":["14","28","30"]},"The image normalization achieved through the described transformations help maximize the rows of data in image data for further processing. Although the increments from the pupil boundary to the limbic boundary are the same for each image, the angle \u03b8 varies in accordance with the amount of occlusion. By varying the angle \u03b8, the length of the rows in a transformed arc are maximized to provide the most information for analysis in each iris image.","After the preprocessor  has extracted and transformed the arcs for the iris image, the transformed arcs are provided to the feature extractor  for clarity analysis. The feature extractor  performs a frequency transform on each row of each polar image and then the transformed rows are filtered with a one-dimensional (1D) Gabor transform filter. The filtered row data are returned to the polar coordinate space for further analysis.","The processing of the feature selector  is shown in . In more detail, a one dimensional fast Fourier transform (FFT) is applied to a row of data within one of the polar or log-polar images (block ). The FFT transforms the dimensionless data to the spatial domain so the frequency characteristics from \u2212\u03c0 to \u03c0 radians can be analyzed. The highest and lowest frequencies are removed using a Log-Gabor band pass filter (block ). The highest frequencies are removed to diminish the effects of noise and artifacts that are not intrinsic to an iris pattern. The lowest frequencies are removed to diminish the effects of any DC component in the signal. This processing enables the frequencies that represent the iris pattern in an arc to be isolated.","The Log-Gabor transform applied to the transformed rows has the mathematical form:",{"@attributes":{"id":"p-0038","num":"0037"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mi":"G","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03c9"}},"mo":"=","msup":{"mi":"\u2147","mfrac":{"mrow":[{"mo":"-","msup":{"mrow":{"mi":"log","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"\u03c9","mo":"\/","msub":{"mi":"\u03c9","mn":"0"}}}},"mn":"2"}},{"mn":"2","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mrow":{"mi":["log","\u03c3"],"mo":["(",")"]},"mn":"2"}}]}}}}},"br":{},"sub":"0 "},{"@attributes":{"id":"p-0039","num":"0038"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"\u03c3","mo":"=","mn":"0.2"},{"msub":{"mi":"\u03c9","mn":"0"},"mo":"=","mfrac":{"mn":"1","mi":"\u03bb"}},{"mi":"s","mo":"=","mrow":{"mi":["number","of","pixels","in","a","row"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]}},{"mrow":[{"mi":["and","\u03bb"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},{"mfrac":{"mi":"s","mn":"40"},"mo":"."}],"mo":"="}],"mo":[",","\u2062",",","\u2062",","],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}}},"br":{},"b":"208","figref":["FIG. 4B","FIG. 3D"]},"A blurry iris image is much more homogenous than a clear image. Consequently, the distribution of patterns in a blurry image is closer to a uniform distribution than the distribution in a clear image. Therefore, the information distance between the selected features and the uniform distribution provides a measure of the image's clarity. The information distance not only describes the randomness of the features, but it also may be used to generate high-order statistics of an iris image based on its features. If the magnitude of a selected feature is , the probability mass function is",{"@attributes":{"id":"p-0041","num":"0040"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mover":{"mi":"p","mo":"-"},"mo":"=","mfrac":{"mover":{"mi":"r","mo":"-"},"msub":{"mrow":{"mo":["\uf605","\uf606"],"mover":{"mi":"r","mo":"-"}},"mn":"2"}}},"mo":","}}},"br":{}},{"@attributes":{"id":"p-0042","num":"0041"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mover":{"mi":"q","mo":"-"},"mo":"=","mrow":{"mfrac":{"mn":"1","mi":"L"},"mo":"."}}}},"br":{}},{"@attributes":{"id":"p-0043","num":"0042"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mrow":[{"mi":"J","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mover":[{"mi":"p","mo":"-"},{"mi":"q","mo":"-"}],"mo":","}}},{"mi":"D","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mover":{"mi":"p","mo":"-"},"mo":"\u2062","mrow":{"mo":["\uf605",")"],"mover":{"mi":"q","mo":"-"}}},{"mrow":{"mi":"D","mo":["(","\uf606"],"mover":{"mi":"q","mo":"-"}},"mo":"\u2062","mover":{"mi":"p","mo":"-"}}],"mo":"+"}}}],"mo":"="},{"mrow":[{"mi":"where","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"mi":"D","mo":["(",")"],"mrow":{"mrow":[{"mover":{"mi":"p","mo":"-"},"mo":"\u2062","mrow":{"mo":["\uf605",")"],"mover":{"mi":"q","mo":"-"}}},{"mo":"\u2211","mrow":{"msub":[{"mover":{"mi":"p","mo":"-"},"mi":"i"},{"mi":"log","mn":"2"}],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mfrac":{"msub":[{"mover":{"mi":"p","mo":"-"},"mi":"i"},{"mover":{"mi":"q","mo":"-"},"mi":"i"}]},"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"and","mrow":{"mi":"D","mo":["(","\uf606"],"mover":{"mi":"q","mo":"-"}},"mover":{"mi":"p","mo":"-"}}}],"mo":"="}}},{"mo":"\u2211","mrow":{"msub":[{"mover":{"mi":"q","mo":"-"},"mi":"i"},{"mi":"log","mn":"2"}],"mo":["\u2062","\u2062"],"mrow":{"mfrac":{"msub":[{"mover":{"mi":"q","mo":"-"},"mi":"i"},{"mover":{"mi":"p","mo":"-"},"mi":"i"}]},"mo":"."}}}],"mo":"="}],"mo":[",","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}}},"br":{},"o":["p","q","q","p"]},"As shown in , some iris images contain distinguishable patterns throughout the iris region, while others, as shown in , only contain distinguishable patterns in the inner rings closest to the pupil. Thus, if the entire iris region is evaluated, images containing pattern data only in areas close to the pupil may be difficult to distinguish from blurry images. Consequently, region of interest selector  uses a sliding window to select non-occluded portions of iris features having the highest information distances as calculated above. If a window length L has a start location at (u,v), the filtered magnitude values of the L pixels in the window form a vector . The probability mass functions  of this selected portion and the uniform distribution calculated above may be used to identify the information distance J( , ) for the portion within the window. The information distance indicates whether the portion within the window contains identification feature content. The portion of the row having the largest information distance is stored as the representative information distance for a row. If the total number of consecutive non-occluded pixels in a row is smaller than L, the information distance for that row is set to zero.","Because the inner circles for an iris image contain more distinguishable patterns, their distance measurements are more heavily weighted. The weights may be determine in the following manner: WJ=(1\u2212ke)*J, where k and l are constants. By empirical determinations, k=1\/33 and l=0.1. Jis the representative information distance for the ith row. The N rows having the largest weighted representative information distances, which indicate feature content, are then selected as regions of interest.","To prevent severely blurred images from producing large information distances, the distributions are normalized. Large information distances may arise from the random perturbations in the data, which is essentially noise. Therefore, a threshold value Tmay be used to preclude this condition. The threshold is used by either comparing the distance computed for row portion having the highest score or a mean score for a row to the threshold. If the score is less than the threshold, the representative information for the row is set to zero. The selected regions of interest are provided to the feature information measurement score generator . This generator calculates the measurement as",{"@attributes":{"id":"p-0047","num":"0046"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mi":"FIM","mo":"=","mrow":{"mfrac":{"mn":"1","mi":"N"},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mi":"i"},"mo":"\u2062","msub":{"mi":["WJ","i"]}}}},"mo":","}}},"br":{}},"The occlusion measurement generator  also receives the dimensionless iris image data from the preprocessor . Generator  uses the dimensionless image and dimensionless mask data to measure how much of the iris area contains insufficient data for recognition. The occlusion measurement O is expressed as a percentage according to the following formula:",{"@attributes":{"id":"p-0049","num":"0048"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"O","mo":"=","mrow":{"mrow":[{"mfrac":{"mrow":[{"mi":["Invalid","area","in","the","segmentation","mask"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]},{"mi":["Segmentation","mask","size"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]}]},"mo":"\u2a2f","mn":"100"},{"mi":"%","mo":["\u2062","."],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}],"mo":"\u2062"}}}},"br":{},"b":"30"},{"@attributes":{"id":"p-0050","num":"0049"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"D","mo":"=","mrow":{"mrow":[{"mfrac":{"mrow":[{"mi":["Pupil","radius"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},{"mi":["Iris","radius"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}]},"mo":"\u2a2f","mn":"100"},{"mi":"%","mo":["\u2062","."],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}],"mo":"\u2062"}}}}},"The measurements obtained by the feature information measurement generator , the occlusion measurement generator , and the dilation measurement generator  are provided to the quality score generator . These measurements are combined to produce a single quality score for the iris image. The quality score Q is determined as Q=f(FIM)\u00b7g(O)\u00b7h(D), where f( ), g( ), and h( ) are normalization functions. The function f normalizes the FIM score from 0 to 1 as follows:",{"@attributes":{"id":"p-0052","num":"0051"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"FIM"}},{"mo":"{","mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"mi":["\u03b1","FIM"],"mo":"\u00b7"},"mo":","}},{"mrow":{"mn":"0","mo":["\u2264","\u2264"],"mi":["FIM","\u03b2"]}}]},{"mtd":[{"mrow":{"mn":"1","mo":","}},{"mrow":{"mi":["FIM","\u03b2"],"mo":">"}}]}]}}],"mo":"="}}},"br":{},"sup":["\u03bbO","\u2212\u03b3D","\u22126 "],"figref":"FIG. 6"},"Those skilled in the art will recognize that numerous modifications can be made to the specific implementations described above. While the embodiments above have been described with reference to specific applications, embodiments addressing other applications may be developed without departing from the principles of the invention described above. For example, while the image quality evaluation system has been described as being used with an iris recognition system, it may also be used with other identification systems that use biometric image data. Therefore, the following claims are not to be limited to the specific embodiments illustrated and described above. The claims, as originally presented and as they may be amended, encompass variations, alternatives, modifications, improvements, equivalents, and substantial equivalents of the embodiments and teachings disclosed herein, including those that are presently unforeseen or unappreciated, and that, for example, may arise from applicants\/patentees and others."],"GOVINT":[{},{}],"heading":["GOVERNMENT INTEREST","TECHNICAL FIELD","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 3A"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":["FIG. 3B","FIG. 3A"]},{"@attributes":{"id":"p-0015","num":"0014"},"figref":["FIG. 3C","FIG. 3A"]},{"@attributes":{"id":"p-0016","num":"0015"},"figref":["FIG. 3D","FIG. 3A"]},{"@attributes":{"id":"p-0017","num":"0016"},"figref":["FIG. 3E","FIG. 3A"]},{"@attributes":{"id":"p-0018","num":"0017"},"figref":["FIG. 4A","FIG. 1"]},{"@attributes":{"id":"p-0019","num":"0018"},"figref":["FIG. 4B","FIG. 3D","FIG. 4A"]},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 5A"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 5B"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":["FIG. 6","FIG. 1"]},{"@attributes":{"id":"p-0023","num":"0022"},"figref":["FIG. 7","FIG. 1"]}]},"DETDESC":[{},{}]}
