---
title: Information technology process workflow for data centers
abstract: A method, system, computer system, and computer program product that use application requirements, business priorities, and compatibility and dependency among applications to allocate resources among those applications in a clustering environment. A workload policy engine is aware of the resources available within each cluster, as well as the capacities of those resources. Multiple instances of the workload policy engine can be run in different clusters. The workload policy engine can be used in conjunction with disaster recovery products as well as with provisioning software so that new machines can be provisioned in and out of a cluster dynamically, such as in a blade environment. Furthermore, the workload policy engine can be used in conjunction with dynamic repartitioning capabilities provided by different hardware platforms for large computer systems, as well as with performance monitoring software.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07900206&OS=07900206&RS=07900206
owner: Symantec Operating Corporation
number: 07900206
owner_city: Mountain View
owner_country: US
publication_date: 20040331
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"p":["Portions of this patent application contain materials that are subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document, or the patent disclosure, as it appears in the Patent and Trademark Office file or records, but otherwise reserves all copyright rights whatsoever.","As the use of open systems grows, managing data centers that may have hundreds or thousands of computer systems becomes an increasingly difficult task. Many data centers support large numbers of heterogeneous computer systems, running different operating systems and connected to a variety of networks, such as Storage Area Networks (SANs) and Internet Protocol (IP) networks. Many information technology (IT) managers are working to move from large numbers of small open systems, many running well below their capacities, to a much smaller number of large-scale enterprise servers running at or near their capacities. This trend in the IT industry is called \u201cserver consolidation.\u201d","Computer systems in a data center may include large mainframe computers and\/or very large servers, such as Hewlett-Packard's Superdome and Sun's Enterprise 10,000 (E10K), providing mainframe-like power using various physical and logical partitioning schemes. Such powerful machines enable server consolidation to be \u201cscaled up\u201d to a small number of powerful servers.","Data centers may also include servers that are symmetric multi-processor (SMP) systems, uniprocessor (UP) systems, and\/or blade servers, which include a large number of blades (thin computer cards with one or more microprocessors and memory) that typically share a housing and a common bus. Blade servers enable server consolidation to be \u201cscaled out,\u201d so that the blade server becomes a \u201ccompute node\u201d to which blade microprocessors can be allocated upon demand. Similarly, \u201cvirtual machines\u201d enable computing power or memory to be provided by a number of processors which are called upon when needed.","Furthermore, the computer systems in a data center may support hundreds of application programs, also referred to as applications. These applications typically have different hardware resource requirements and business priorities, and one application may depend upon other applications. Each of these applications can have respective performance requirements, availability requirements, and disaster recovery requirements. Some application programs may run as batch jobs and have timing constraints (e.g., a batch job computing the price of bonds at a financial firm may need to end an hour before the next trading day begins). Other applications may operate best when resources are allocated as needed, such as stateless web servers and shared disk database applications. Single instance applications may run best on a single large machine with dynamic reconfiguration capabilities.","One early answer to the demand for increased application availability was to provide one-to-one backups for each server running a critical application. When the critical application failed at the primary server, the application was \u201cfailed over\u201d (restarted) on the backup server. However, this solution was very expensive and wasted resources, as the backup servers sat idle. Furthermore, the solution could not handle cascading failure of both the primary and backup servers.","Enterprises require the ability to withstand multiple cascading failures, as well as the ability to take some servers offline for maintenance while maintaining adequate redundancy in the server cluster. Clusters of servers became commonplace, with either one server or multiple servers serving as potential failover nodes. Examples of commercially available cluster management applications include, VERITAS\u00ae Cluster Server, Hewlett-Packard\u00ae MC\/Service Guard, and Microsoft\u00ae Cluster Server (MSCS).","N+1 clustering refers to multiple servers, each typically running one application, plus one additional server acting as a \u201cspare.\u201d When a server fails, the application restarts on the \u201cspare\u201d server. When the original server is repaired, the original server becomes the spare server. In this configuration, there is no longer a need for a second application outage to put the service group back on the \u201cprimary node\u201d. Any server can provide redundancy for any other server. Such a configuration allows for clusters having eight or more nodes with one spare server.","N-to-N clustering refers to multiple application groups running on multiple servers, with each application group being capable of failing over to different servers in the cluster. For example, a four-node cluster of servers could support three critical database instances. Upon failure of any of the four nodes, each of the three instances can run on a respective server of the three remaining servers, without overloading one of the three remaining servers. N-to-N clustering expands the concept of a cluster having one backup server to a requirement for \u201cbackup capacity\u201d within the servers forming the cluster.","N+1 and N-to-N clustering, however, provide only limited support should multiple servers fail, as there is no generally available method to determine which applications should be allowed to continue to run, and which applications should be shut down to preserve performance of more critical applications. This problem is exacerbated in a disaster recovery (DR) situation. If an entire cluster or site fails, high priority applications from the failed cluster or site can be started on the DR site, co-existing with applications already running at the DR site. What is needed is a process for managing information technology that enables enterprise applications to survive multiple failures in accordance with business priorities. An enterprise administrator should be able to define resources, machine characteristics, application requirements, application dependencies, business priorities, load requirements, and other such variables once, rather than several times in different systems that are not integrated. Preferably, resource management software should operate to ensure that high priority applications are continuously available.","The present invention relates to a method, system and computer program product that manages information technology resources in accordance with business priorities. Application needs for availability, disaster recovery, and performance are taken into account when allocating resources in an environment having one or more clusters. When an application is started, restarted upon failure, or moved due to an overload situation, resources best fulfilling the requirements for running the application are allocated to the application. Respective priorities of applications can be used to determine whether a lower-priority application can be moved or even halted to free resources for running a higher-priority application.","Resources in a cluster are allocated in response to failure of an application, starting the application, and\/or identifying a problem with performance of the application. A particular application may be selected to receive allocated resources in accordance with the application's business priority. If existing resources in the cluster cannot be allocated to provide the quantity of resources needed by the application, the cluster can be reconfigured to enable the cluster to provide the quantity of the resources to the application. Alternatively, if existing resources in the cluster are insufficient, the application can be restarted in another cluster having sufficient resources for the application. This other cluster can be located remotely from the cluster in which the resources are needed. Reconfiguring the cluster can include adding a resource to the cluster or partitioning a resource within the cluster. In one embodiment, performance of applications running in the cluster may be monitored. If performance of one of the applications fails to satisfy a criterion, an additional quantity of the resources for the one application can be requested to enable the performance of the one application to satisfy the criterion.","The foregoing is a summary and thus contains, by necessity, simplifications, generalizations and omissions of detail; consequently, those skilled in the art will appreciate that the summary is illustrative only and is not intended to be in any way limiting. Other aspects, inventive features, and advantages of the present invention, as defined solely by the claims, will become apparent in the non-limiting detailed description set forth below.","The use of the same reference symbols in different drawings indicates similar or identical items.","The present invention provides a comprehensive cluster workload policy engine. Resources are allocated among applications in a clustering environment using application requirements, business priorities, and compatibility and dependency among applications. The workload policy engine is aware of the resources available within each cluster, as well as the capacities of those resources. Multiple instances of the workload policy engine can run on multiple clusters, and applications can be failed over from one cluster to another. The workload policy engine can be used in conjunction with disaster recovery products as well as with provisioning software so that new machines can be provisioned in and out of a cluster dynamically, such as in a blade environment. Furthermore, the workload policy engine can be used in conjunction with dynamic repartitioning capabilities provided by different hardware platforms for large computer systems such as Hewlett-Packard's Superdome and Sun's Enterprise 10,000 (E10K).","The workload policy engine can also be integrated with resource managers, such as Sun Solaris Resource Manager or Aurema Resource Manager, to enforce resource usage rules. In addition, the workload policy engine can be used with performance management systems to solve problems that can be corrected by providing more capacity to an application. The workload policy engine can be used in a heterogeneous environment and allows multiple hardware types within a single cluster. A tight environment that connects each node in a cluster via a private network is supported, as is a loose environment that includes nodes connected only by a TCP\/IP connection to the cluster.",{"@attributes":{"id":"p-0041","num":"0040"},"figref":"FIG. 1","b":["110","110","110","110","111","110","110","111"]},"MV site A and UK site B are shown as connected via network , which typically corresponds to a private wide area network or a public distribution network such as the Internet. MV site A includes cluster A containing nodes A through D, which are connected via redundant cluster connections AB- and AB-, BC-, BC-, CD-, and CD-. All four nodes A, B, C, and D share common storage A, with respective interconnections A, B, C, and D to storage A. Node B is shown as the host for workload policy engine A, which controls resource allocation and workload for running applications on nodes A through D in cluster A.","Similarly, cluster B at UK site B includes nodes E and F, which are connected via redundant cluster connections EF- and EF-. Node E shares common storage B with node F. Node E is interconnected with storage B via interconnection E and node F is interconnected with storage B via interconnection F. Node F is shown as the host for workload policy engine B, which controls resource allocation and workload for running applications on nodes E and F in cluster B.","Factors included in the determination of the \u201cbest\u201d server to initially start or to re-start an application include server capacity and finite resource availability. One technique for allocating resources in a server consolidation environment is described in U.S. patent application Ser. No. 10\/609,363, entitled \u201cBusiness Continuation Policy for Server Consolidation Environment,\u201d filed May 31, 2002, and naming as inventors Darshan B. Joshi, Kaushal R. Dalal, and James A. Senicka, the application being incorporated by reference herein in its entirety for all purposes.",{"@attributes":{"id":"p-0045","num":"0044"},"figref":["FIG. 2","FIG. 1"],"b":["130","130","210"]},"At \u201cSufficient Resources Available in Cluster\u201d decision point , a determination is made whether sufficient resources are available within the cluster to host the application. Part of the determination of whether sufficient resources are available involves taking into account the compatibility of the application needing resources with other applications running on a given node. If the applications are compatible, then the capacity of the resources available is considered.","If sufficient resources are not available, control proceeds to \u201cCan Sufficient Resources in Cluster be Freed\u201d decision point , where a determination is made whether resources in the cluster can be freed to enable the application to obtain the resources needed to run. One technique for freeing resources using relative business priorities assigned to applications is described in U.S. patent application Ser. No. 10\/609,363 referenced above.","At \u201cSufficient Resources Available in Cluster\u201d decision point , if sufficient resources are available within the cluster to start the application, control proceeds to \u201cAllocate Resources in Accordance with Policy\u201d step . The resources are allocated to the application in accordance with business policies, application priorities, and application requirements. As noted above, the compatibility of applications is taken into account in allocating resources. In some instances, lower-priority applications may be shut down to provide resources to higher-priority applications.","From \u201cAllocate Resources in Accordance with Policy\u201d step , control proceeds to \u201cMonitor Resource Usage\u201d step . The application's use of resources is monitored to ensure that the application continues to conform to policy and meet performance requirements as long as the application is running. Control cycles through \u201cProblem\u201d decision point  and \u201cTerminated\u201d decision point  until the application is terminated or a problem is detected. If the application is terminated, monitoring of resources used by the application ceases. Resource monitoring is discussed in further detail below.","If a problem is detected at \u201cProblem\u201d decision point , control proceeds to \u201cIdentify Resources Needed to Correct Problem\u201d step . The resources needed to correct the problem are identified. Control then returns to \u201cSufficient Resources Available in Cluster\u201d decision point  to determine whether the resources needed to correct the performance problem are available within the cluster.","At \u201cCan Sufficient Resources in Cluster be Freed\u201d decision point , if sufficient resources in the cluster cannot be freed, control proceeds to \u201cCan Sufficient Resources be Added to Cluster\u201d decision point . If sufficient resources can be added to the cluster, control proceeds to \u201cAdd Resources\u201d step , where additional resources are added to the cluster. For example, a node may be added by reconfiguring the cluster. A new computer system to serve as a new node may be added to the cluster, or an additional instance of an operating system may be started on a processor that can support multiple instances of operating systems to provide another node. Control proceeds from \u201cAdd Resources\u201d step  to \u201cAllocate Resources in Accordance with Policy\u201d step .","At \u201cCan Sufficient Resources be Added to Cluster\u201d decision point , if sufficient resources cannot be added to the cluster, control proceeds to \u201cCan Resource be Partitioned to Provide Sufficient Resources\u201d step . If a resource can be dynamically partitioned so that the application has sufficient resources to run, control proceeds to \u201cPartition Resource\u201d step . For example, a 64 CPU machine may be partitioned initially into 4 machines, each with 16 CPUs. If an application needs additional CPUs, it may be possible to re-partition the 64 CPUs into two 32 CPU machines. After partitioning, the cluster size remains the same, but the organization of processors may vary. From \u201cPartition Resource\u201d step , control proceeds to \u201cAllocate Resources in Accordance with Policy\u201d step . If a resource cannot be partitioned to meet the needs of the application, control proceeds to \u201cFailover to Another Cluster Possible\u201d decision point .","At \u201cFailover to Another Cluster Possible\u201d decision point , if the application can be failed over to another cluster, control proceeds to \u201cFailover to Other Cluster\u201d step . The application begins at step \u201cResources Needed for Application\u201d step  on the other cluster and follows the steps described herein.","At \u201cFailover to Another Cluster Possible\u201d decision point , if the application cannot be failed over to another cluster, control proceeds to \u201cProvide Notification that Application Cannot Run\u201d step . An administrator can be notified to intervene and restart the application.","One of skill in the art will recognize that the order of the decisions made at \u201cCan Sufficient Resources in Cluster be Freed\u201d decision point , \u201cCan Sufficient Resources be Added to Cluster\u201d decision point , \u201cCan Resource be Partitioned to Provide Sufficient Resources\u201d step , and \u201cFailover to Another Cluster Possible\u201d decision point  can be varied without departing from the scope of the invention. Furthermore, not all options may be needed by a given organization and thus all of these steps are not necessary to practice the invention.",{"@attributes":{"id":"p-0056","num":"0055"},"figref":"FIG. 3","b":["300","310","312","320","310","330","330","310","330","330"]},"Local workload policy engine L operates in conjunction with application monitor , policy enforcer , and performance monitor  to manage resources within the cluster. Application monitor  monitors application  to ensure that application  remains running and notifies local workload policy engine L of application, job, node, or site failure events . Policy enforcer  monitors the use of resources by application  to ensure that application  continues to follow policy. If a policy violation  is detected, policy enforcer  notifies local workload policy engine L. Application monitor  is an example of a monitoring module, means, and\/or instructions to monitor performance of a plurality of applications running in a cluster. If performance of one application fails to satisfy a criterion, application monitor  can notify local workload policy engine L, which can request to allocate a second quantity of the resource for the one application to enable the performance of the one application to satisfy the criterion.","Local workload policy engine L also operates in conjunction with external systems, such as performance monitor , provisioning component , and partitioning component . One of skill in the art will recognize that the organization of system  into local components, such as local workload policy engine L, as distinguished from external systems such as performance monitor , provisioning component , and partitioning component , is but one embodiment of the system. Part or all of the functionality provided by these external systems may be performed by local workload policy engine L, and some of the functionality of local workload policy engine L, application monitor , and policy enforcer  may be provided by external systems in alternative embodiments.","Performance monitor  notifies local workload policy engine L of performance failure events . If local workload policy engine L finds sufficient resources for application  within a cluster, local workload policy engine L allocates resources  directly to application .","If sufficient resources are not available within the cluster, local workload policy engine L can request resources from other components, as shown with request resources event . These other components can be external to the cluster as shown, such as remote workload policy engine R, provisioning component , and partitioning component . Each of remote workload policy engine R, provisioning component , and partitioning component  can provide resources  to application . Alternatively, provisioning component  and partitioning component  can notify local workload policy engine L of the resources available to allocate, and local workload policy engine L can provide resources  to application . Provisioning component  and partitioning component  are examples of an enabling module, means, and\/or instructions that enable a cluster to provide a quantity of the resource to the application by reconfiguring the cluster. Provisioning component  is an example of an adding module, means, or instructions to add a quantity of the resource to reconfigure the cluster. Partitioning component  is an example of a partitioning module, means, and\/or instructions because partitioning component  reconfigures the cluster by partitioning a resource within the cluster. Remote workload policy engine R is an example of a restarting module, means, or instructions that restart the application in a second cluster having a sufficient amount of the resource to provide the needed quantity of the resource to the application.","The workload policy engine of the present invention can be integrated with other systems or components to provide an enterprise-wide view of resource availability. For example, most major operating systems have a corresponding resource manager, such as Solaris resource manager, HP Process Resource Manager and AIX Resource manager. These resource managers, collectively called xRM herein, allow an administrator to control CPU and memory utilization. However, typically xRM packages are only aware of the system on which the xRM package is running, and not of other systems within the cluster. Preferably, the workload policy engine is integrated with xRM packages and controls resource utilization, and therefore load, on all systems in the cluster.",{"@attributes":{"id":"p-0062","num":"0061"},"figref":["FIG. 4","FIG. 3","FIG. 3"],"b":["400","410","410","410","415","1","415","2","415","1","415","2","410","410","410","425","410","430","400","430","400","410","410","432","432","430","434","440","440","440","410","410","410","340","350","360"]},{"@attributes":{"id":"p-0063","num":"0062"},"figref":"FIG. 5","b":["500","560","560","560","560","510","510","560","560","560","530","560","510","515","1","515","2","510","550","510"]},"Each of nodes B and C hosts a respective cluster client B and C. Workload policy engine (master) A can receive requests from clients , which may include a graphical user interface, a command line interface used by an administrator, a command line interface used by an administrator, or another application via an application programming interface (API) call. Agents A, B, and C running on nodes A, B, and C, respectively, may perform the functionality of application monitor , policy enforcer , and\/or performance monitor  of .",{"@attributes":{"id":"p-0065","num":"0064"},"figref":"FIG. 6","b":["1","9"]},"Five applications are running within the cluster, named App through App. Some applications include multiple service groups; for example, App includes service groups OraDB and OraApp. Each service group has an assigned business priority and load. Service groups of an application may be dependent upon one another; for example, it is possible to have applications that run only if all service groups can be run on a node, although no such service group dependencies are shown in . Some applications are incompatible with each other. For example, OraDB and OraDB are incompatible and cannot be run on the same node at the same time.","While the total capacity of nodes N through N is 2800, the application load currently running in the cluster is only 2100, leaving excess capacity of 700. The workload policy engine of the present invention attempts to use excess capacity before shutting down any application. For this scenario, assume that all application service groups can run on all machines, although it is possible to configure some applications to run on only some machines. An objective of the workload policy engine is to ensure that high priority applications are continuously available.",{"@attributes":{"id":"p-0068","num":"0067"},"figref":["FIGS. 7A through 7D","FIG. 6","FIG. 7A","FIG. 6","FIG. 6"],"b":"9"},{"@attributes":{"id":"p-0069","num":"0068"},"figref":["FIG. 7B","FIG. 7A","FIG. 7A"],"b":["1","1","2","2","2","1","1","2","2","2","3","6","1","3","6","2","2","6","2","2","3","3","6","1"]},"No service group dependency was defined for App OraDB and App OraApp in the preceding example. The workload policy engine of the present invention is designed to handle application service group dependencies. Had a service group dependency been defined, App OraDB and OraApp may have been required to run on the same node and could not be separated. Terminating another application would have been necessary to provide the capacity of 400 required to run both applications on a single node.",{"@attributes":{"id":"p-0071","num":"0070"},"figref":["FIG. 7C","FIG. 7B"],"b":["2","1","1","1","1","1","1"]},"As shown in , the lowest priority applications are application App service group OraTest, having a priority of 4 and a load of 300, and application App service group ReportTest, having a priority of 4 and a load of 200. Application App service group OraTest provides the largest quantity of resources and is selected for termination. Terminating application App service group OraTest frees a capacity of 300 on node N. Application App service group OraDB, having a load of 200, and application App service group OraApp, having a load of 100, can then be failed over to node N. The total load of all applications has been reduced from 2100 to 1800 by terminating application App service group OraTest. The failure of node N reduces the remaining capacity in the cluster to only 100.",{"@attributes":{"id":"p-0073","num":"0072"},"figref":["FIG. 7D","FIG. 7C"],"b":["10","10","4"]},{"@attributes":{"id":"p-0074","num":"0073"},"figref":["FIGS. 8A through 8D","FIG. 6","FIG. 8A","FIG. 6","FIG. 6"],"b":"9"},{"@attributes":{"id":"p-0075","num":"0074"},"figref":["FIG. 8B","FIG. 8A"],"b":["1","1","2","3","3","2"]},"It is possible, in some embodiments, to increase the load of an application when it is migrated for performance reasons to ensure that the application is allocated additional resources. Alternatively, load of an application can be increased in response to performance failure or policy violations if necessary after the application is migrated.",{"@attributes":{"id":"p-0077","num":"0076"},"figref":["FIG. 8C","FIG. 8B"],"b":["1","1","2","1","1","2"]},{"@attributes":{"id":"p-0078","num":"0077"},"figref":["FIG. 8D","FIG. 8C"],"b":["2","10","2","1","1","1","10","1","3"]},{"@attributes":{"id":"p-0079","num":"0078"},"figref":["FIG. 9A","FIG. 6"],"b":["910","910","910","910"]},{"@attributes":{"id":"p-0080","num":"0079"},"figref":["FIG. 9B","FIG. 9A"],"b":["3","1","3","910","6","910","3","910","400","3","6","910","6","400","910","3","910"]},{"@attributes":{"id":"p-0081","num":"0080"},"figref":"FIG. 10A","b":["1","2","1","2","2","2","2","2","2","2"]},{"@attributes":{"id":"p-0082","num":"0081"},"figref":["FIG. 10B","FIG. 10A"],"b":["2","1","1","2","2","2","2","2","2","2","2","2","2"]},{"@attributes":{"id":"p-0083","num":"0082"},"figref":"FIG. 11A","b":["2","2","1","1","2","2","2","2"]},{"@attributes":{"id":"p-0084","num":"0083"},"figref":["FIG. 11B","FIG. 11A"],"b":["2","1","1","2","2","2","2","2","2","2","2","2","2"]},"The present invention provides many advantages. Information technology resources are managed in accordance with business priorities. Application needs for availability, disaster recovery, and performance are taken into account when allocating resources. When an application is started, restarted upon failure, or moved due to an overload situation, resources best fulfilling the requirements for running the application are allocated to the application. Respective priorities of applications can be used to determine whether a lower-priority application can be moved to free resources for running a higher-priority application. Resources can be dynamically added to a cluster or partitioned to meet changing needs of the application environment. An application can be failed over from one cluster to another in order to maintain continuous operation of high priority applications.","The present invention is well adapted to attain the advantages mentioned as well as others inherent therein. While the present invention has been depicted, described, and is defined by reference to particular embodiments of the invention, such references do not imply a limitation on the invention, and no such limitation is to be inferred. The invention is capable of considerable modification, alteration, and equivalents in form and function, as will occur to those ordinarily skilled in the pertinent arts. The depicted and described embodiments are examples only, and are not exhaustive of the scope of the invention.","The foregoing described embodiments include components contained within other components. It is to be understood that such architectures are merely examples, and that in fact many other architectures can be implemented which achieve the same functionality. In an abstract but still definite sense, any arrangement of components to achieve the same functionality is effectively \u201cassociated\u201d such that the desired functionality is achieved. Hence, any two components herein combined to achieve a particular functionality can be seen as \u201cassociated with\u201d each other such that the desired functionality is achieved, irrespective of architectures or intermediate components. Likewise, any two components so associated can also be viewed as being \u201coperably connected,\u201d or \u201coperably coupled,\u201d to each other to achieve the desired functionality.","The foregoing detailed description has set forth various embodiments of the present invention via the use of block diagrams, flowcharts, and examples. It will be understood by those within the art that each block diagram component, flowchart step, operation and\/or component illustrated by the use of examples can be implemented, individually and\/or collectively, by a wide range of hardware, software, firmware, or any combination thereof.","The present invention has been described in the context of fully functional computer systems; however, those skilled in the art will appreciate that the present invention is capable of being distributed as a program product in a variety of forms, and that the present invention applies equally regardless of the particular type of signal bearing media used to actually carry out the distribution. Examples of signal bearing media include recordable media such as floppy disks and CD-ROM, transmission type media such as digital and analog communications links, as well as media storage and distribution systems developed in the future.","The above-discussed embodiments may be implemented by software modules that perform certain tasks. The software modules discussed herein may include script, batch, or other executable files. The software modules may be stored on a machine-readable or computer-readable storage medium such as a disk drive. Storage devices used for storing software modules in accordance with an embodiment of the invention may be computer-readable storage media, such as magnetic floppy disks, hard disks, or optical discs such as CD-ROMs or CD-Rs, for example. A storage device used for storing firmware or hardware modules in accordance with an embodiment of the invention may also include a semiconductor-based memory, which may be permanently, removably or remotely coupled to a microprocessor\/memory system. Thus, the modules may be stored within a computer system memory to configure the computer system to perform the functions of the module. Other new and various types of computer-readable storage media may be used to store the modules discussed herein.","The above description is intended to be illustrative of the invention and should not be taken to be limiting. Other embodiments within the scope of the present invention are possible. Those skilled in the art will readily implement the steps necessary to provide the structures and the methods disclosed herein, and will understand that the process parameters and sequence of steps are given by way of example only and can be varied to achieve the desired structure as well as modifications that are within the scope of the invention. Variations and modifications of the embodiments disclosed herein can be made based on the description set forth herein, without departing from the scope of the invention. Consequently, the invention is intended to be limited only by the scope of the appended claims, giving full cognizance to equivalents in all respects."],"heading":["BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION","Other Embodiments"],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The present invention may be better understood, and its numerous features and advantages made apparent to those skilled in the art by referencing the accompanying drawings.",{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":["FIG. 4","FIG. 3"]},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":["FIGS. 7A through 7D","FIG. 6"]},{"@attributes":{"id":"p-0023","num":"0022"},"figref":["FIG. 7A","FIG. 6"]},{"@attributes":{"id":"p-0024","num":"0023"},"figref":["FIG. 7B","FIG. 7A"]},{"@attributes":{"id":"p-0025","num":"0024"},"figref":["FIG. 7C","FIG. 7B"]},{"@attributes":{"id":"p-0026","num":"0025"},"figref":["FIG. 7D","FIG. 7C"]},{"@attributes":{"id":"p-0027","num":"0026"},"figref":["FIGS. 8A through 8D","FIG. 6"]},{"@attributes":{"id":"p-0028","num":"0027"},"figref":["FIG. 8A","FIG. 6"]},{"@attributes":{"id":"p-0029","num":"0028"},"figref":["FIG. 8B","FIG. 8A"]},{"@attributes":{"id":"p-0030","num":"0029"},"figref":["FIG. 8C","FIG. 8B"]},{"@attributes":{"id":"p-0031","num":"0030"},"figref":["FIG. 8D","FIGS. 8B and 8C","FIG. 8C"]},{"@attributes":{"id":"p-0032","num":"0031"},"figref":["FIG. 9A","FIG. 6"]},{"@attributes":{"id":"p-0033","num":"0032"},"figref":["FIG. 9B","FIG. 9A"]},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 10A"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":["FIG. 10B","FIG. 10A"]},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 11A"},{"@attributes":{"id":"p-0037","num":"0036"},"figref":["FIG. 11B","FIG. 11A"]}]},"DETDESC":[{},{}]}
