---
title: Efficient 2-D and 3-D graphics processing
abstract: Techniques for supporting both 2-D and 3-D graphics are described. A graphics processing unit (GPU) may perform 3-D graphics processing in accordance with a 3-D graphics pipeline to render 3-D images and may also perform 2-D graphics processing in accordance with a 2-D graphics pipeline to render 2-D images. Each stage of the 2-D graphics pipeline may be mapped to at least one stage of the 3-D graphics pipeline. For example, a clipping, masking and scissoring stage in 2-D graphics may be mapped to a depth test stage in 3-D graphics. Coverage values for pixels within paths in 2-D graphics may be determined using rasterization and depth test stages in 3-D graphics. A paint generation stage and an image interpolation stage in 2-D graphics may be mapped to a fragment shader stage in 3-D graphics. A blending stage in 2-D graphics may be mapped to a blending stage in 3-D graphics.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08203564&OS=08203564&RS=08203564
owner: QUALCOMM Incorporated
number: 08203564
owner_city: San Diego
owner_country: US
publication_date: 20070216
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["I. Field","The present disclosure relates generally to electronics, and more specifically to techniques for performing graphics processing.","II. Background","Graphics processing units (GPUs) are specialized hardware units used to render 2-dimensional (2-D) or 3-dimensional (3-D) images for various applications such as video games, graphics, computer-aided design (CAD), simulation and visualization tools, imaging, etc. The process to render a 2-D drawing typically includes a series of stages that is specific to 2-D graphics. The processing for these 2-D graphics stages may be computationally intensive. A 2-D GPU may be designed and used to perform computationally intensive 2-D processing in hardware. The use of the 2-D GPU may shorten rendering time for 2-D drawings and improve overall performance.","Similarly, the process to render a 3-D image typically includes a series of stages that is specific to 3-D graphics. The processing for these 3-D graphics stages may also be computationally intensive. A 3-D GPU may be designed and used to perform computationally intensive 3-D processing in hardware. The use of the 3-D GPU may shorten rendering time for 3-D images and improve overall performance.","An electronics device may support only 2-D graphics and may utilize a 2-D GPU for 2-D graphics acceleration. An electronics device may also support only 3-D graphics and may utilize a 3-D GPU for 3-D graphics acceleration. For an electronics device that supports both 2-D and 3-D graphics, it is desirable to achieve good performance for both 2-D and 3-D graphics with as little hardware as possible in order to reduce cost and power consumption.","Techniques for efficiently supporting both 2-D and 3-D graphics are described herein. A GPU may be designed to perform 3-D graphics processing in accordance with a 3-D graphics pipeline to render 3-D images. The GPU may also perform 2-D graphics processing in accordance with a 2-D graphics pipeline to render 2-D images or drawings. Each of a plurality of stages of the 2-D graphics pipeline may be mapped to at least one of a plurality of stages of the 3-D graphics pipeline. Processing units within the GPU may be efficiently used to perform processing for both 2-D and 3-D graphics.","In general, the 2-D graphics pipeline and 3-D graphics pipeline may each include any number of stages. The 2-D graphics pipeline stages may be mapped to the 3-D graphics pipeline stages based on various mapping schemes. For example, a clipping, masking and scissoring stage in 2-D graphics may be mapped to a depth test stage in 3-D graphics. Coverage values for pixels within paths in 2-D graphics may be determined using a rasterization stage, the depth test stage, etc., in 3-D graphics. A paint generation stage and an image interpolation stage in 2-D graphics may be mapped to a fragment shader stage in 3-D graphics. A blending stage in 2-D graphics may be mapped to a blending stage in 3-D graphics, and both blending stages may be supported with a plurality of blending modes. Other stages in 2-D graphics may be mapped to other stages in 3-D graphics as described below.","Various aspects and features of the disclosure are described in further detail below.","A GPU may support 2-D and\/or 3-D graphics. 2-D graphics stores 2-D representation of geometric data that is processed to generate 2-D images or drawings. 3-D graphics stores 3-D representation of geometric data that is processed to generate 2-D images. 2-D graphics may be compared to painting whereas 3-D graphics may be compared to sculpting. 2-D and 3-D graphics may utilize different pipelines with different stages and graphics operations to generate final images for display.","A 2-D or 3-D image may be represented with primitives, which are basic units of geometry. For 3-D graphics, primitives may include polygons (typically triangles), lines, points, etc. Each triangle is defined by three vertices, and each line is defined by two vertices. Each vertex may be associated with various attributes such as space coordinates, color values, texture coordinates, etc. Each attribute may have up to four components. For example, space coordinates may be given by either three components x, y and z or four components x, y, z and w, where x and y are horizontal and vertical coordinates, z is depth, and w is a homogeneous coordinate. Color values may be given by three components r, g and b or four components r, g, b and a, where r is red, g is green, b is blue, and a is a transparency factor that determines the transparency of a picture element (pixel). Texture coordinates are typically given by horizontal and vertical coordinates, u and v. A vertex may also be associated with other attributes. For 2-D graphics, primitives may include points, lines, curved lines, rectangles, etc. A path may be composed of a combination of different primitives. A 2-D or 3-D image may be rendered by computing attribute component values for pixels to be displayed.",{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 1","b":["100","100","100"]},"Graphics applications  may run concurrently and may be for video games, graphics, videoconference, etc. Graphics applications  generate high-level commands to perform graphics operations on graphics data. The high-level commands may be relatively complex but the graphics data may be fairly compact. The graphics data may include geometry information (e.g., information for vertices of primitives in an image), information describing what the image looks like, etc. Graphics applications  interface with a GPU driver  via application programming interfaces (APIs) (not shown in ).","GPU driver  may be software and\/or firmware executing on a processor, or a hardware unit, or a combination of both software\/firmware and hardware. GPU driver  converts the high-level commands from graphics applications  to low-level commands, which may be machine dependent and tailored for processing units within a GPU . GPU driver  may also indicate where data is located, e.g., which buffers store the data. GPU  may split the processing of each graphics application into a series of threads, e.g., automatically and transparent to the graphics applications. A thread (or thread of execution) indicates a specific task that may be performed with a set of one or more instructions. For example, a thread may perform blending for a set of pixels. Threads allow a graphics application to have multiple tasks performed simultaneously by different processing units and further allow different graphics applications to share resources. GPU driver  provides the low-level commands for threads and data location indicators to GPU .","GPU  includes processing units that perform various functions to render 3-D images. The terms \u201cprocessing unit\u201d, \u201cengine\u201d, \u201ccore\u201d, \u201cmachine\u201d, \u201cprocessor\u201d, \u201cmodule\u201d, etc., are often used interchangeably. Within GPU , a command engine  receives the low-level commands from GPU driver  and\/or fetches the commands from memory. Command engine  decodes the low-level commands and provides instructions to processing units affected by these commands.","A vertex shader  performs vertex processing on geometry data, which may comprise primitives such as triangles and lines. For example, vertex shader  may transform geometry from one coordinate system to another coordinate system, calculate light values for vertices of geometry, perform blending for geometry, etc. A vertex buffer  stores vertex information for vertex shader . A primitive processor  computes parameters for subsequent rasterization process. For example, primitive processor  may compute coefficients of linear equations for three edges of each triangle, depth (z) gradient for depth interpolation for pixels, etc. A scan converter  decomposes each primitive (e.g., triangle or line) into pixels and determines a screen coordinate for each pixel.","A depth test engine  may perform depth test (which is also called a z test) and\/or stencil test for pixels, if enabled, to determine whether the pixels should be rendered or discarded. A z buffer  stores a depth or z value for each pixel location in the image being rendered. For depth test, engine  compares a z value for a pixel (the current z value) against a corresponding z value in z buffer  (the stored z value), passes the pixel and updates z buffer  and possibly stencil buffer  if the current z value is closer\/nearer than the stored z value, and discards the pixel if the current z value is further back than the stored z value. A stencil buffer  stores a stencil value for each pixel location in an image being rendered. For stencil test, engine  compares a stored stencil value for a pixel against a reference value and either passes or discards the pixel based on an outcome of the comparison.","A fragment shader  (which may also be called a pixel shader) may perform various graphics operations on pixels and fragments. A fragment is a pixel and its associated information. Fragment shader  may compute parameters for interpolation of pixel attributes, e.g., coefficients of linear equations for attribute interpolation. Fragment shader  may then compute attribute component values for each pixel within each triangle based on the pixel's screen coordinate and using the interpolation parameters. Fragment shader  may also perform texture mapping, if enabled, to apply textures to each triangle. A texture image may be stored in a texture buffer . The three vertices of a triangle may be associated with three (u, v) coordinates in the texture image, and each pixel in the triangle may then be associated with specific texture coordinates in the texture image. Texturing is achieved by modifying the color of each pixel in the triangle with the color of the texture image at the location indicated by that pixel's texture coordinates. Fragment shader  may also perform texture filtering and\/or other graphics operations.","A pixel blender  may perform graphics operations such as alpha blending, alpha test, fog blending, logic operation, dithering operation, etc., on fragments. Pixel blender  may provide results for a final image to a color buffer  for presentation on a display device (shown in ).","As shown in , GPU  implements a 3-D graphics pipeline composed of multiple stages designed for 3-D graphics. GPU  may also implement other 3-D graphics pipelines with different stages and\/or with the stages arranged in different manners. Buffers  through  may be implemented with memories that are external to GPU , as shown in . GPU  may include caches, which are high-speed memories, to store instructions and data for the processing units within the GPU.",{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 2","b":"200"},"In block  for stage , a graphics application defines a path to be drawn and sets transformation, stroke, and paint parameters, if any, to be used by subsequent stages for the path. In block  for stage , the path is transformed from a user coordinate system to a screen coordinate system. The user coordinate system is a coordinate system used to represent geometry. The screen coordinate system is a coordinate system for a display screen. If the path is to be stroked, then the stroked parameters are applied in the user coordinate system to generate a new path that describes the stroked geometry in the screen coordinate system. This new path is referred to as the current path and is operated on by subsequent stages in the 2-D graphics pipeline. In block  for stage , path-to-surface transformation is applied to the geometry of the current path to generate screen coordinates. In block  for stage , a coverage value is computed for each pixel affected by the current path. A coverage value for a pixel indicates what fraction of that pixel's area is covered by the current path. The coverage value is used in subsequent stages to compute a color value for the pixel. Stage  maps the current path to pixels on the drawing surface.","In block  for stage , pixels that are not within the bounds of the drawing surface and, if scissoring is enabled, not within one or more scissoring regions are assigned coverage values of zero. The remaining processing is skipped for pixels with coverage values of zero. Stage  clips the drawing to the bounds of the drawing surface and any scissoring regions. The portions of the drawing that are clipped are discarded.","In block  for stage , a color value and an alpha value are defined for each pixel on the drawing surface based on the current relevant paint, which may depend on whether the path is being filled or stroked. Stage  applies a paint image to pixels in the current path. In block  for stage , if an image is being drawn, then a color value and an alpha value are computed for each pixel by interpolating image values using the inverse of an image-to-surface transformation. The results are combined with paint color and alpha values in accordance with the current image drawing mode. In block  for stage , source color and alpha values for each pixel from stage  are converted to destination color space and blended with the corresponding destination color and alpha values in accordance with a blending mode.",{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 2"},"Device  in  may support 2-D graphics using units  through . These units may be designed for 3-D graphics, but their capabilities may be extended and used for 2-D graphics.",{"@attributes":{"id":"p-0039","num":"0038"},"figref":["FIG. 3","FIG. 1","FIG. 2"],"b":["300","100","112","1","2","200","124","3","126","128","4","130","5","132","6","7","134","8","100"]},{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 3","b":["200","100","120","100","200","100","100","1","2"]},"A 2-D drawing or image may be rendered in multiple passes. A first pass may generate values in a buffer for scissoring and masking for the image being rendered. A second pass may perform tessellation for paths in the image and may determine coverage values for these paths. A third pass may apply paint and input images to the paths and generate a final image for a screen. Each pass may utilize some or all of the units within device . The processing by various units within device  for different stages of the OpenVG 2-D graphics pipeline is described below.","For stage , GPU driver  may perform tessellation of curved and stroked paths and provide triangle fans. A curved path may be approximated with a triangle fan having a series of edges defined by vertices. These vertices may be determined based on the curvature of the path and image quality requirements.",{"@attributes":{"id":"p-0043","num":"0042"},"figref":"FIG. 4","b":["0","1","2","3","4","412","0","1","414","1","2","416","2","3","418","3","4","420","4","0","0"]},"For stage , vertex shader  may perform transformation of triangle fans, if necessary, from user coordinate system to screen coordinate system. The transformation for 2-D graphics may be performed in similar manner as transformation for 3-D graphics.","For stage , units ,  and  may generate a coverage mask for the 2-D image based on the triangle fans after transformation, if any. These units determine pixels that are inside and outside of shapes formed by the triangle fans. A path may be closed and its shape may divide a plane into an inside region and an outside region. Pixels within the inside region may be treated differently than pixels within the outside region by subsequent stages.","The inside and outside regions of a shape may be determined based on either a non-zero fill rule or an odd\/even fill rule. For both rules, to determine whether a given point is inside or outside of the shape, a line is drawn from that point toward infinity in any direction. In the example shown in , a line  is drawn from a point  to the right. A counter is reset to zero. Starting from point  and walking along line  toward infinity, the counter is (a) incremented by one if line  is crossed by an edge going from left to right and (b) decremented by one if line  is crossed by an edge going from right to left. In , starting from point  and walking along line , the counter is incremented by one due to edge  crossing the line from left to right and is again incremented by one due to edge  crossing the line from left to right. Point  thus has a fill value of 2. Fill values of 0 and 1 for other regions are indicated in .","For the non-zero fill rule, a point is inside the shape if its fill value is not equal to zero. For the odd\/even fill rule, a point is inside the shape if its fill value is odd, regardless of the sign (e.g., \u22127 is odd and 0 is even). The two fill rules may provide different definitions of the inside and outside regions. In the example shown in , for the non-zero fill rule, the regions with fill values of 1 and 2 are inside the shape and all other regions are outside the shape. For the odd\/even fill rule, the regions with fill values of 1 are inside the shape and all other regions are outside the shape.","Units ,  and  may determine coverage values for pixels in the image. Depth test engine  may be configured for 2-D instead of 3-D processing by disabling back face culling, disabling color, setting the drawing surface, etc. Engine  may also be configured to perform stencil test and to disable depth test. Stencil buffer  may be cleared in the drawing surface.","Primitive processor  may decompose a triangle fan into one or more triangles based on a predetermined rule. For example, the triangle fan in  may be decomposed into three triangles\u2014a first triangle with vertices V, V and V, a second triangle with vertices V, V and V, and a third triangle with vertices V, V and V. Each triangle includes vertex V, the last vertex of a prior triangle (if any), and a new vertex. Primitive processor  may also generate parameters used by scan converter  to rasterize each triangle. Scan converter  may determine pixels within each triangle.","Depth test engine  may update the stencil values for pixels received from scan converter . Engine  may increment stencil values for pixels within triangles having clockwise orientation and decrement stencil values for pixels within triangles having counter-clockwise orientation, or vice verse. For the example shown in , engine  may increment the stencil values for pixels within the first triangle with vertices V, V and V having clockwise orientation. Engine  may decrement the stencil values for pixels within the second triangle with vertices V, V and V having counter-clockwise orientation. Engine  may increment the stencil values for pixels within the third triangle with vertices V, V and V having clockwise orientation.  gives the stencil values for pixels in different regions after all triangles of the triangle fan are processed.","The stencil value for each pixel may be converted to a coverage value based on the non-zero or odd\/even fill rule. For the non-zero rule, a pixel is inside the shape if its stencil value is non-zero and is outside the shape otherwise. For the odd\/even rule, a pixel is inside the shape if its stencil value is odd and is outside the shape otherwise.","Pixels may be represented with normal sampling or super sampling antialiasing. Normal sampling refers to the use of one sample for each pixel. A stencil value and a coverage value for each pixel may be determined as described above. Super sampling antialiasing (or simply, antialiasing) refers to the use of multiple samples for each pixel and is a technique to improve image quality. For example, each pixel may be represented with:","2\u00d72 samples for a scale factor of 2 in both x and y directions,","4\u00d72 samples for a scale factor of 4 in the x direction and 2 in the y direction, or","4\u00d74 samples for a scale factor of 4 in both x and y directions.","Other scale factors may also be used for the x and y directions. Different sampling patterns may be used as well, e.g., a pattern sparsely populated with samples and rotated in different directions. With antialiasing, each sample represents a sub-pixel.","With antialiasing, stencil values may be determined for samples (or sub-pixels) instead of pixels. Scan converter  may determine the samples within each triangle, and the stencil value for each sample may be updated as described above. After all triangles are processed, a coverage value of \u20181\u2019 or \u20180\u2019 may be determined for each sample based on the stencil value for that sample. A coverage value may then be determined for each pixel by (a) summing the coverage values for all samples of that pixel and (b) normalizing the sum value. For example, an 8-bit coverage value for a pixel may be within a range of 0 to 255, where 0 indicates that the pixel is completely outside the shape and 255 indicates that the pixel is completely inside the shape. An 8-bit coverage value for a pixel may be set equal to the floor of ((Sum\/NumSamples)*255+0.5), where Sum is the sum of the coverage values for all samples of that pixel, and NumSamples is the number of samples for the pixel. NumSamples is equal to 4 for 2\u00d72 super sampling, 8 for 4\u00d72 super sampling, 16 for 4\u00d74 super sampling, etc.","For both normal sampling and antialiasing, the coverage values for pixels may be stored in z buffer  in  and used to determine color values for the pixels. Antialiasing may also be performed in other manners. For example, coverage values of \u20181\u2019 and \u20180\u2019 may be stored for sub-pixels, color values may be determined for sub-pixels, and the color values for sub-pixels may be combined based on their coverage values to determine final color values for pixels.","For stage , depth test engine  may perform clipping and masking to clip paths and final image to the bounds of the drawing surface and\/or the interior of one or more scissoring regions. Each scissoring region describes a specific section of the drawing surface to retain. Engine  may perform scissoring by determining whether each pixel on the drawing surface falls within any one of the scissoring regions. Each pixel may be associated with a scissor value that may indicate whether that pixel is within any scissoring region.","Stencil buffer  may be used to store scissor values for pixels and may be cleared (e.g., to all zeros) prior to performing scissoring. Scissoring may then be performed for one scissoring region at a time. For each scissoring region, scan converter  may determine the pixels within the scissoring region. Engine  may update the scissor value for each pixel from scan converter . For example, engine  may increment the scissor value for each pixel by one if the scissoring region is being added and may decrement the scissor value by one if the scissoring region is being removed. A scissoring mask is obtained after all scissoring regions have been drawn. The scissor value for a pixel is zero if the pixel is not within any scissoring region and is non-zero if the pixel is within at least one scissoring region. Scissoring may be performed before or after drawing paths.","The scissor values in stencil buffer  and the coverage values in z buffer  may be used to determine whether or not to render pixels. A bounding box may be drawn for a path and rasterized to determine the pixels within the box. Each pixel in the box may be (a) discarded or dropped if its coverage value is zero or its scissor value is zero or (b) passed to the next stage otherwise. The coverage values for the passed pixels may also be provided to the next stage and used to determine color values for these pixels.","Masking may also be performed based on the coverage values and mask values from a mask buffer. A coverage value for a pixel may be multiplied with a mask value for the pixel. The pixel may be dropped if the resultant value is zero and passed otherwise. Masking may also be performed by fragment shader  or some other unit instead of depth test engine .","For stage , fragment shader  may determine color and alpha values for each pixel in the final image based on paint or an input image. Fragment shader  may fill the interior of a path with one type of paint and\/or stroke the outline of the path with the same or different type of paint. Paint defines a color value and an alpha value for each pixel being drawn. There are three types of paint\u2014color paint, gradient paint, and pattern paint. Color paint defines a constant color for all pixels. Gradient paint defines a linear or radial pattern of smoothly varying colors. Pattern paint defines a possibly repeating rectangular pattern of colors based on a source image. Paint is defined in a paint coordinate system and may be transformed to user coordinate system. After transformation, a paint value nearest a given (x,y) pixel may be used for that pixel, or paint values from multiple pixels surrounding a central pixel may be combined to produce an interpolated paint value. Fragment shader  may fill and\/or stroke a path with paint in an analogous manner as applying texture to a 3-D image. Fragment shader  may also filter pattern paint with a kernel. The paint and color information may be defined by GPU driver  and sent to the appropriate processing units.","Color information may be stored or processed in several color formats. Human eyes are more sensitive to low luminance\/intensity than high luminance. A color value may be compressed via a process commonly referred to as gamma compression to obtain a non-linear color value. Gamma compression maps the color value such that finer resolution is achieved for low luminance and coarser resolution is achieved for high luminance. Gamma compression may be performed, e.g., as c=c where cis a linear color value, cis a non-linear color value, and \u03b3 is a gamma value that determines the amount of compression. Gamma expansion is a complementary process that expands a non-linear color value to obtain a linear color value. Gamma expansion is based on a function that is an inverse of the function used for gamma compression.","Fragment shader  may perform color conversion for stages  and . Color values for pixels may be represented in any one of the following formats:","sRGBA\u2014non-premultiplied non-linear color format,","sRGBA_PRE\u2014premultiplied non-linear color format,","lRGBA\u2014non-premultiplied linear color format, and","lRGBA_PRE\u2014premultiplied linear color format,","where R, G, B and A denote red, green, blue and alpha, respectively.","Paint and images may be stored in memory as non-linear color values and may be processed as linear color values. Color format conversion may be performed to convert non-linear color values to linear color values, and vice versa, as necessary. A color value c may be multiplied with an alpha value \u03b1 to obtain a pre-multiplied color value c\u2032=\u03b1*c. Certain operations (e.g., blending) may be more conveniently performed on pre-multiplied color values.","The conversion between various color formats may be achieved with a reciprocal (RCP) look-up table (LUT), a gamma LUT, and a degamma LUT. The RCP LUT receives an input value and provides a reciprocal of the input value. The gamma LUT stores a gamma compression function, receives a linear color value, and provides a non-linear color value. The degamma LUT stores a complementary gamma expansion function, receives a non-linear color value, and provides a linear color value.","The conversion from pre-multiplied format to non pre-multiplied format may be achieved as follows:",{"@attributes":{"id":"p-0072","num":"0071"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"161pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"oneOverAlpha = RCP_LUT (alpha)"]},{"entry":[{},"RGB = RGB*oneOverAlpha"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}},"br":{}},"The conversion between linear and non-linear color formats may be achieved as follows:","RGB=DeGamma_LUT (RGB)\/\/non-linear to linear format conversion","RGB=Gamma_LUT (RGB)\/\/linear to non-linear format conversion","RGBA=LUT (RGBA)\/\/general format conversion","General format conversion may be used to transform one color space to another color space, where the transformation may be based on any function.","For stage , fragment shader  may perform convolution filtering on input images. Convolution filtering may be used to obtain effects such as edge sharpening, blurring, noise reduction, scaling, rotation, texture mapping, etc.",{"@attributes":{"id":"p-0078","num":"0077"},"figref":"FIG. 5","b":["510","500","510","520"],"sub":["x\u22121,y\u22121 ","x+1,y+1 ","0,0 ","2,2 "]},"Convolution filtering may be expressed as:",{"@attributes":{"id":"p-0080","num":"0079"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msubsup":{"mi":["p","\u2032"],"mrow":{"mi":["x","y"],"mo":","}},"mo":"=","mrow":{"mrow":{"mi":"S","mo":"\u00b7","mrow":{"mo":["(",")"],"mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"i","mo":"=","mn":"0"},{"mi":"W","mo":"-","mn":"1"}]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"j","mo":"=","mn":"0"},{"mi":"H","mo":"-","mn":"1"}]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":[{"mi":"k","mrow":{"mi":["i","j"],"mo":","}},{"mi":"p","mrow":{"mrow":[{"mi":["x","i","shiftX"],"mo":["+","-"]},{"mi":["y","i","shiftY"],"mo":["+","-"]}],"mo":","}}],"mo":"\u00b7"}}}}},"mo":"+","mi":"b"}},"mo":","}},{"mrow":{"mi":"Eq","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"mo":["(",")"],"mn":"1"}}}]}}}},"br":{}},"W is the width of the grid\/kernel and H is the height of the grid,","kis a kernel coefficient at position (i,j) in the grid,","S is a scaling factor and b is a bias value,","pis an original pixel value at position (x, y) in the image,","p\u2032is a new pixel value that replaces the original pixel value at position (x, y),","shiftX is an offset from position (x, y) to the left of the grid, and","shiftY is an offset from position (x, y) to the bottom of the grid.","S may be a normalization factor of",{"@attributes":{"id":"p-0088","num":"0087"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"S","mo":"=","msup":{"mrow":[{"mo":["(",")"],"mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"i","mo":"=","mn":"0"},{"mi":"W","mo":"-","mn":"1"}]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"j","mo":"=","mn":"0"},{"mi":"H","mo":"-","mn":"1"}]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":"k","mrow":{"mi":["i","j"],"mo":","}}}}},{"mo":"-","mn":"1"}]}}}},"br":{}},"If the kernel coefficients are separable in the x and y direction, then convolution filtering may be expressed as:",{"@attributes":{"id":"p-0090","num":"0089"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msubsup":{"mi":["p","\u2032"],"mrow":{"mi":["x","y"],"mo":","}},"mo":"=","mrow":{"mrow":{"mi":"S","mo":"\u00b7","mrow":{"mo":["(",")"],"mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"i","mo":"=","mn":"0"},{"mi":"W","mo":"-","mn":"1"}]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"j","mo":"=","mn":"0"},{"mi":"H","mo":"-","mn":"1"}]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":[{"mi":["k","i"]},{"mi":["k","j"]},{"mi":"p","mrow":{"mrow":[{"mi":["x","i","shiftX"],"mo":["+","-"]},{"mi":["y","i","shiftY"],"mo":["+","-"]}],"mo":","}}],"mo":["\u00b7","\u00b7"]}}}}},"mo":"+","mi":"b"}},"mo":","}},{"mrow":{"mi":"Eq","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"mo":["(",")"],"mn":"2"}}}]}}}},"br":{}},"kis a kernel coefficient at horizontal position i in the grid, and","kis a kernel coefficient at vertical position j in the grid.","The kernel coefficient at position (i, j) may be derived as: k=k\u00b7k.","In the example shown in , W=3, H=3, shiftX=1, and shiftY=1. In general, shiftX is approximately half of the width, and shiftY is approximately half of the height. Different types of convolution filtering and different kernels may produce different effects in the filtered image.","Convolution filtering of an arbitrary H\u00d7W pixel grid may be achieved by partitioning the grid into smaller sections, performing computation for each section, and combining the intermediate results for all sections to obtain a final result. The size of the section may be selected based on the processing capabilities of an arithmetic logic unit (ALU) used for computation. In general, any section size may be used. In one design, the section size is 2\u00d72 or smaller. In this design, a 3\u00d73 pixel grid may be decomposed into one 2\u00d72 section, one 1\u00d72 section, one 2\u00d71 section, and one 1\u00d71 section.","The convolution computation for a 2\u00d72 section may be expressed as:\n\n,\u2003\u2003Eq. (3)\n\nwhere (x, y) denotes the position of the lower left pixel in the 2\u00d72 section, and\n","ris an intermediate result for the 2\u00d72 section.","The computation in equation (3) may be efficiently performed by an ALU with four units, e.g., a quad ALU that can compute an inner product of two 4\u00d71 vectors or four scalar ALUs that can multiply and accumulate four pairs of scalars. The convolution computation for a 1\u00d71, 1\u00d72 or 2\u00d71 section includes a subset of the coefficients and a subset of the pixels shown in equation (3).","The coordinate system for the pixels may or may not match the coordinate system for the kernel coefficients. Furthermore, the x and y values for a given pixel may be integer or non-integer values. For example, convolution filtering may be performed on texture pixels (texels) given in (u, v) coordinate system that may not match the (x, y) coordinate system of the image. If the coordinates for a pixel are real values, then the coefficient to apply to the pixel may be determined in several manners. For a snap-to-nearest mode, the coordinates for the pixel are rounded to the nearest integer values, and the kernel coefficient at the rounded coordinates is applied to the pixel. For an interpolated mode, four kernel coefficients closest to the pixel are used to generate an interpolated coefficient, which is then applied to the pixel.","Two modes of convolution filtering may be supported\u2014mode 0 and mode 1. For mode 0, convolution computation may be performed on sections of up to four pixels using snap-to-nearest coefficients. The four pixel coordinates may be rounded to the nearest integer values. Up to four pixel values at the rounded coordinates may be retrieved from memory and applied with up to four kernel coefficients to obtain a new pixel value. For mode 1, convolution computation may be performed on each pixel using an interpolated coefficient generated from four kernel coefficients that are closest to the pixel coordinates.",{"@attributes":{"id":"p-0100","num":"0099"},"figref":"FIG. 6","b":["600","132","610","612","614","620","622"]},"Fragment shader  may also perform interpolation between input image and background color in order to determine color and alpha values for each pixel. Fragment shader  may provide interpolation results to pixel blender  for blending with other color and alpha values.","For stage , pixel blender  may perform blending in accordance with one of a number of blending modes supported by OpenVG. A blending mode is associated with the following:","1. an alpha blending equation denoted as \u03b1(\u03b1, \u03b1) and","2. a color blending equation denoted as c (c, c, \u03b1, \u03b1),","where \u03b1is a source alpha value, \u03b1is a destination alpha value, cis a source color value, and cis a destination color value. A color value c may be for red (R), green (G), or blue (B).","Blending combines a source color and alpha tuple denoted as (R, G, B, \u03b1) with a destination color and alpha tuple denoted as (R, G, B, \u03b1) and provides a blended tuple composed of c (R, R, \u03b1, \u03b1), c(G, G, \u03b1, \u03b1), c(B, B, \u03b1, \u03b1) and \u03b1(\u03b1, \u03b1). The combining is performed in accordance with the alpha and color blending equations. In particular, the source and destination color values for each color component is combined in accordance with the color blending equation, and the source and destination alpha values are combined in accordance with the alpha blending equation. Different results may be obtained with different equations and\/or different alpha values. The blended tuple typically replaces the destination tuple.","OpenVG supports five Porter-Duff blending modes that use the following alpha and color blending equations:",{"@attributes":{"id":"p-0107","num":"0106"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"mrow":[{"mi":"\u03b1","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["\u03b1","src"]},{"mi":["\u03b1","dst"]}],"mo":","}}},{"mrow":[{"msub":[{"mi":["\u03b1","src"]},{"mi":["F","src"]}],"mo":"*"},{"msub":[{"mi":["\u03b1","dst"]},{"mi":["F","dst"]}],"mo":"*"}],"mo":"+"}],"mo":"="},"mo":",","mi":"and"}},{"mrow":{"mi":"Eq","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"mo":["(",")"],"mn":"4"}}}]},{"mtd":[{"mrow":{"mrow":{"mrow":{"mi":"c","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["c","src"]},{"mi":["c","dst"]},{"mi":["\u03b1","src"]},{"mi":["\u03b1","dst"]}],"mo":[",",",",","]}}},"mo":"=","mfrac":{"mrow":[{"mrow":[{"msub":[{"mi":["\u03b1","src"]},{"mi":["c","src"]},{"mi":["F","src"]}],"mo":["*","*"]},{"msub":[{"mi":["\u03b1","dst"]},{"mi":["c","dst"]},{"mi":["F","dst"]}],"mo":["*","*"]}],"mo":"+"},{"mi":"\u03b1","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["\u03b1","src"]},{"mi":["\u03b1","dst"]}],"mo":","}}}]}},"mo":","}},{"mrow":{"mi":"Eq","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"mo":["(",")"],"mn":"5"}}}]}]}}},"br":{},"sub":["src ","dst ","src ","dst "]},"A color blending equation for pre-multiplied color values may be given as:\n\n(,\u03b1,\u03b1)=\u2003\u2003Eq (6)\n\nEquation (6) is equivalent to equation (4). Equation (6) may be used for pre-multiplied color values c\u2032and c\u2032whereas equation (4) may be used for non pre-multiplied color values cand c.\n","OpenVG supports four additional blending modes. Table 1 lists the four additional blending modes and gives the color blending equation for each mode. Each additional blending mode combines color values and provides a blended pre-multiplied color value denoted as c\u2032(c, c, \u03b1, \u03b1).",{"@attributes":{"id":"p-0110","num":"0109"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"84pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"133pt","align":"left"}}],"thead":{"row":[{"entry":"TABLE 1"},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}},{"entry":[{},"Color Blending"]},{"entry":["Blending Mode","Equation c\u2032 (c, c, \u03b1, \u03b1)"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["VG_BLEND_MULTIPLY","\u03b1* c* (1 \u2212 \u03b1) + \u03b1* c*"]},{"entry":[{},"(1 \u2212 \u03b1) + \u03b1* c* \u03b1* c"]},{"entry":["VG_BLEND_SCREEN","\u03b1* c+ \u03b1* c\u2212 \u03b1* c* \u03b1* c"]},{"entry":["VG_BLEND_DARK","min {(\u03b1* c+ \u03b1* c* (1 \u2212 \u03b1),"]},{"entry":[{},"(\u03b1* c+ \u03b1* c* (1 \u2212 \u03b1)}"]},{"entry":["VG_BLEND_LIGHTEN","max {(\u03b1* c+ \u03b1* c* (1 \u2212 \u03b1),"]},{"entry":[{},"(\u03b1* c+ \u03b1* c* (1 \u2212 \u03b1)}"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},"OpenVG further supports an additive blending mode that may be used when the source and destination do not overlap. The alpha and color blending equations for the additive blending mode are given as:",{"@attributes":{"id":"p-0112","num":"0111"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"mrow":[{"mi":"\u03b1","mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["\u03b1","src"]},{"mi":["\u03b1","dst"]}],"mo":","}}},{"mi":"min","mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["\u03b1","src"]},{"mi":["\u03b1","dst"]}],"mo":"+"}},"mo":",","mn":"1"}}}],"mo":"="},"mo":",","mi":"and"}},{"mrow":{"mi":"Eq","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"mo":["(",")"],"mn":"7"}}}]},{"mtd":[{"mrow":{"mrow":[{"mi":"c","mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["c","src"]},{"mi":["c","dst"]},{"mi":["\u03b1","src"]},{"mi":["\u03b1","dst"]}],"mo":[",",",",","]}}},{"mfrac":{"mrow":[{"mrow":[{"msub":[{"mi":["\u03b1","src"]},{"mi":["c","src"]}],"mo":"*"},{"msub":[{"mi":["\u03b1","dst"]},{"mi":["c","dst"]}],"mo":"*"}],"mo":"+"},{"mi":"min","mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["\u03b1","src"]},{"mi":["\u03b1","dst"]}],"mo":"+"}},"mo":",","mn":"1"}}}]},"mo":"."}],"mo":"="}},{"mrow":{"mi":"Eq","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"mo":["(",")"],"mn":"8"}}}]}]}}}},"Other blending modes may also be supported for 2-D graphics. In general, GPU  may support any set of blending modes for any set of blending equations for 2-D graphics. GPU  may also support stencil interpolation, which uses the following set of equations:\n\n\u03b1=\u03b1(\u03b1*\u03b1,\u03b1),\u2003\u2003Eq (9)\n\n(*\u03b1*\u03b1,\u03b1)\/\u03b1,\u2003\u2003Eq (10)\n\n\u03b1\u2190\u03b1,\u2003\u2003Eq (11)\n\nwhere \u03b1is a first source alpha value, \u03b1is a second source alpha value, \u03b1=\u03b1*\u03b1for equation (9), and \u03b1=\u03b1*\u03b1*cfor equation (10).\n","Alpha blending equation (9) and color blending equation (10) may be dependent on blending mode. For example, if a \u201cSrc over Dst\u201d Porter-Duff blending mode is selected, then the stencil interpolation may be expressed as:\n\n\u03b1=\u03b1*\u03b1+\u03b1*(1\u2212\u03b1*\u03b1),\n\n\u2190(\u03b1*\u03b1+\u03b1*(1\u2212\u03b1*\u03b1))\/\u03b1,\n\n\u03b1\u2190\u03b1.\n","Blending equations for various blending modes (e.g., the blending modes in OpenVG and\/or other blending modes) may be efficiently implemented with a base set of operations. Each blending equation may be decomposed into a sequence of operations, with each operation taken from the base set. Different blending equations may be implemented with different sequences of operations. The base set of operations may simplify hardware implementation and may provide flexibility to support various blending equations as well as other graphics functions and features.",{"@attributes":{"id":"p-0116","num":"0115"},"figref":"FIG. 7","b":["700","714","720","718","724","722","716","712"]},"Table 2 shows an example of the base set of operations and an Op code for each operation. Operations with Op codes 0 through 7 are referred to as operations 0 through 7, respectively.",{"@attributes":{"id":"p-0118","num":"0117"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"91pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"126pt","align":"left"}}],"thead":{"row":[{"entry":"TABLE 2"},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}},{"entry":["Op Code","Operation"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["0","Res = Src * Srcf + Dst * Dstf"]},{"entry":["1","Res = Src * Srcf \u2212 Dst * Dstf"]},{"entry":["2","Res = Min {Src, Dst}"]},{"entry":["3","Res = Max {Src, Dst}"]},{"entry":["4","Src = Src * Srcf, Dst = Dst * Dstf"]},{"entry":["5","Res = RCP {Src}"]},{"entry":["6","Res = DegammaLUT {Src}"]},{"entry":["7","Res = GammaLUT {Src}"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},"For operations 0 and 1, the source and destination values are scaled by the source and destination blending factors, respectively, and the scaled values are combined. Operation 2 provides the smaller of the source and destination values whereas operation 3 provides the larger of the two values. Operation 4 scales the source and destination values with the source and destination blending factors, respectively, and stores the two scaled values. Operation 5 performs a reciprocal operation on a source value and provides a result value. Operations 6 and 7 are used for gamma expansion and gamma compression, respectively. Table 2 shows a specific design for the base set of operations. Other sets of operations may also be used for blending.","Source color values may be received from fragment shader  and stored in a source register. Destination color values may be retrieved from memory (e.g., color buffer ) and stored in a destination register. The source and destination color values may be combined, and the blended color values may be stored in the destination register. To flexibly support different operations, multiple source registers may be used to store source color and alpha values as well as intermediate values. Multiple destination registers may also be used to store destination color and alpha values as well as intermediate values.","Src field  and Dst field  may each indicate any register in a set of available registers. This set may include a source register, a second source register, a destination register, a temporary destination register, etc. Res field  may indicate any one of multiple output options such as, e.g., the source register, second source register, destination register, temporary destination register, both source and destination registers, final output, etc.","The Srcf and Dstf blending factors are used to scale the Src and Dst values, respectively, for operations 0, 1 and 4 in Table 2. The Srcf and Dstf blending factors may each be selected from a set of blending factors that can support all blending modes. This set may include (a) zero, (b) alpha and color values in the source register, second source register, destination register, a constant register, etc., (c) one minus these alpha and color values, (d) one over these alpha and color values, (e) the smaller of source alpha and one minus source alpha, and\/or (f) other values.","Each blending mode may be implemented with a set of blending instructions. For example, each of the five Porter-Duff blending modes shown in equations (4) through (6) may be implemented with two instructions. The first instruction may multiply the source and destination color values with the source and destination alpha values, respectively. The second instruction may multiply the pre-multiplied source and destination color values with Fsrc and Fdst blending factors, respectively, and combine the two scaled values, as shown in equation (6). The two instructions may be given as:","Src=Src*Srcf, Dst=Dst*Dstf","Res=Src*Srcf+Dst*Dstf","where the source blending factor Srcf and the destination blending factor Dstf are dependent on the Porter-Duff blending mode being implemented.","Pixel blender  may also perform color conversion between different color formats. Color conversion may be performed for color values read from memory and\/or color values written to memory.","Various blending modes may be supported as follows:",{"@attributes":{"id":"p-0128","num":"0127"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"105pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"84pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"10","Call RCP to compute 1\/a.dst","\/\/Dst pre-format"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"20","Call MUL to divide a.dst out of Dst"]},{"entry":[{},"30","Call DeGammaLUT to convert non-linear color to linear color"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"105pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"84pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"40","Op0: Src=a.src*Src, Dst=a.dst*Dst","\/\/do blending"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"41","Op1: Instruction for mode-dependent blending equation"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"189pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\u2002.","."]},{"entry":[{},"\u2002.","."]},{"entry":[{},"\u2002.","."]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"4n","Opn: Instruction for mode-dependent blending equation"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"105pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"84pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"50","Call RCP to compute 1\/a.res","\/\/Dst post-format"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"60","Call MUL to divide a.res out of Dst"]},{"entry":[{},"70","Call GammaLUT to convert linear color to non-linear color"]},{"entry":[{},"80","Call MUL to multiply a.res with Dst and store Dst"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}]}}},"The pseudo-code above assumes that color components are stored as pre-multiplied non-linear color values. Instructions 10, 20 and 30 are for pre-formatting of the destination color value. Instructions 10 and 20 divide out the destination alpha (a.dst) from the destination color value. Instruction 30 converts non-linear color to linear color. Instructions 40 to 4n are for operations that are dependent on the selected blending mode and may include multiplication of the source and destination color values with the source alpha (a.src) and destination alpha, respectively. Instructions 50 to 80 are for post-formatting of the result color value. Instructions 50 and 60 divide out the result alpha (a.res) to obtain a non pre-multiplied color value. Instruction 70 converts linear color to non-linear color. Instruction 80 multiplies the non-linear color value with the result alpha and stores the final result back to memory. Instructions 10, 20 and 80 may be omitted if the color values are stored in non pre-multiplied format. Instructions 30 and 70 may be omitted if color components are stored as linear color values instead of non-linear color values.",{"@attributes":{"id":"p-0130","num":"0129"},"figref":["FIG. 8","FIG. 3","FIG. 3"],"b":["800","134","812","814","132","810","820","148","820","822","824","820","810"]},"Blending execution unit  receives blending instructions (e.g., from command engine  in ) and decodes the received instructions. Unit  also reads source and destination values and blending factors according to the decoded instructions and sends these values and factors to a computation unit  for processing. Unit  receives results from unit  and stores these results in the appropriate registers. A post-formatting unit  performs post-formatting on the results and may divide out alpha for pre-multiplied color values, perform conversion from linear color to non-linear color, multiply by alpha if the result color values are to be stored in pre-multiplied format, etc.","Unit  includes a reciprocal (RCP) unit , a gamma LUT , a degamma LUT , and a blending ALU . Blender ALU  may operate on operands received from unit ,  and  and provide the results back to these units. Unit  receives input operands and provides the reciprocal of the operands. Gamma LUT  receives linear color values and provides non-linear color values. Degamma LUT  receives non-linear color values and provides linear color values. Unit  may be part of blending unit  and used just for pixel blender . Alternatively, unit  may be external to pixel blender  and shared by pixel blender  and fragment shader  for color conversion.","A control register  stores control bits that indicate the color format of the data being processed. For example, the control bits may indicate whether color values are stored in (a) pre-multiplied or non pre-multiplied format and (b) non-linear or linear format. Unit  may perform pre-formatting on inbound destination color values in accordance with the control bits. Similarly, unit  may perform post-formatting on outbound color values in accordance with the control bits. Control register  may be set by GPU driver  or some other unit.",{"@attributes":{"id":"p-0134","num":"0133"},"figref":["FIG. 9","FIG. 1"],"b":["900","100","122","128","134"]},"Primitives for basic 2-D graphics may include lines, rectangle, and triangle fans. Other primitives may be tessellated into rectangles, triangles, or lines, e.g., by GPU driver . Attributes for basic 2-D graphics may include, for each pixel, one color value and one pair of coordinates for a source buffer for a bit block transfer (bitblt), a stretch bitblt, or a stipple pattern.","For a stippled line, GPU driver  may treat a line stipple pattern as 1-D texture and compute its coordinates for two end points of the line and may then provide coordinate information to scan converter  via command engine . Scan converter  may compute the coordinates of every pixel on the line and generate pixel addresses. The coordinates may be used to look up the stipple pattern in a control register without using fragment shader . A mask may be obtained from the stipple pattern. The rasterization direction may be changed for overlapping bitblt. To avoid use of fragment shader  and attribute setup and interpolation for power saving, GPU driver  may perform attribute setup and compute gradients, e.g., for up to four components. Scan converter  may include four adders to perform interpolation for up to four components. For bitblt and stretched bitblt, GPU driver  may provide initial coordinates and gradients for a source image\/buffer. Scan converter  may then compute coordinates for every pixel inside a primitive, e.g., a line, a triangle, or a rectangle primitive. The coordinates may be used to look up pixel values in the source image\/buffer via a color cache. For gradient fill, GPU driver  may provide initial color and gradients (e.g., for up to 4 components). Scan converter  may use the same adders (used to compute pixel coordinates) to compute color values for every pixel inside a rectangle primitive.","Pixel information and color information may be provided directly to pixel blender . A multiplexer  may receive source color (Src) from an external source via command engine , source color for gradient fill from scan converter , or source color from a color cache\/buffer . Multiplexer  may provide the source color from one of its three inputs to pixel blender . Color cache  may be located between pixel blender  and color buffer  in . Color cache  may receive a source address or stretched bitblt address from scan converter  and may provide a source color to multiplexer . Color cache  may also receive a destination read address from scan converter  and provide a destination color to pixel blender . Pixel blender  may support various 2-D graphics functions such as bit block transfer, 2-D alpha blending, font rendering, ROP3, source and destination color key, palette table (e.g., 256\u00d732 or 16\u00d716\u00d732), etc. ROP3 is a raster operation of three operands (e.g., source color, destination color, and pattern\/brush color). Other 2-D graphics functions such as convolution filtering may be performed by fragment shader  or some other unit.",{"@attributes":{"id":"p-0138","num":"0137"},"figref":["FIG. 10","FIG. 3"],"b":["1000","1012","1014"]},"In general, the 2-D graphics pipeline and 3-D graphics pipeline may each include any number of stages. The 2-D graphics pipeline stages may be mapped to the 3-D graphics pipeline stages based on various mapping schemes. For example, a clipping, masking and scissoring stage in 2-D graphics may be mapped to a depth test stage in 3-D graphics. Coverage values for pixels within paths in 2-D graphics may be determined using a rasterization stage, the depth test stage, etc., in 3-D graphics. A paint generation stage and an image interpolation stage in 2-D graphics may be mapped to a fragment shader stage in 3-D graphics. A blending stage in 2-D graphics may be mapped to a blending stage in 3-D graphics, and both blending stages may be supported with a plurality of blending modes. Each blending mode may be associated with a respective sequence of instructions. Other stages in 2-D graphics may be mapped to other stages in 3-D graphics, e.g., as described above.","The GPU described herein may be used for wireless communication devices, handheld devices, gaming devices, computing devices, computers, consumer electronics devices, etc. An exemplary use of the GPU for a wireless communication device is described below.",{"@attributes":{"id":"p-0141","num":"0140"},"figref":"FIG. 11","b":["1100","1100"]},"Wireless device  is capable of providing bi-directional communication via a receive path and a transmit path. On the receive path, signals transmitted by base stations are received by an antenna  and provided to a receiver (RCVR) . Receiver  conditions and digitizes the received signal and provides samples to a digital section  for further processing. On the transmit path, a transmitter (TMTR)  receives data to be transmitted from digital section , processes and conditions the data, and generates a modulated signal, which is transmitted via antenna  to the base stations.","Digital section  includes various processing, interface and memory units such as, for example, a modem processor , a video processor , a controller\/processor , a display processor , a reduced instruction set computer\/digital signal processor (RISC\/DSP) , a GPU , an internal memory , and an external bus interface (EBI) . Modem processor  performs processing for data transmission and reception (e.g., encoding, modulation, demodulation, and decoding). Video processor  performs processing on video content (e.g., still images, moving videos, and moving texts) for video applications such as camcorder, video playback, and video conferencing. Controller\/processor  may direct the operation of various processing and interface units within digital section . Display processor  performs processing to facilitate the display of videos, graphics, and texts on a display unit . RISC\/DSP  may perform various types of processing for wireless device . For example, GPU  in  may be executed in RISC\/DSP . GPU  performs graphics processing and may implemented GPU  in , all or part of 2-D graphics pipeline  in , all or part of filtering unit  in , all or part of blending unit  in , process  in , etc. Internal memory  stores data and\/or instructions for various units within digital section . EBI  facilitates transfer of data between digital section  (e.g., internal memory ) and a main memory . Memory  and\/or memory  may implement buffers  through  in .","Digital section  may be implemented with one or more DSPs, micro-processors, RISCs, etc. Digital section  may also be fabricated on one or more application specific integrated circuits (ASICs) and\/or some other type of integrated circuits (ICs).","The GPU described herein may be implemented in various hardware units. For example, the GPU may be implemented within ASICs, DSPs, digital signal processing devices (DSPDs), programmable logic devices (PLDs), field programmable gate arrays (FPGAs), processors, controllers, micro-controllers, microprocessors, electronic devices, other electronic units. The GPU may or may not include integrated\/embedded memory.","The GPU described herein may be a stand-alone unit or may be part of a device. The device may be (i) a stand-alone IC such as a graphics IC, (ii) a set of one or more ICs that may include memory ICs for storing data and\/or instructions, (iii) an ASIC, such as a mobile station modem (MSM), with integrated graphics processing functions, (iv) a module that may be embedded within other devices, (v) a cellular phone, wireless device, handset, or mobile unit, (vi) etc.","The previous description of the disclosure is provided to enable any person skilled in the art to make or use the disclosure. Various modifications to the disclosure will be readily apparent to those skilled in the art, and the generic principles defined herein may be applied to other variations without departing from the spirit or scope of the disclosure. Thus, the disclosure is not intended to be limited to the examples described herein but is to be accorded the widest scope consistent with the principles and novel features disclosed herein."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":["FIG. 3","FIG. 1"]},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":["FIG. 9","FIG. 1"]},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 11"}]},"DETDESC":[{},{}]}
