---
title: Failure detection and recovery of host computers in a cluster
abstract: In one or more embodiments of the invention, communication among host agents providing high availability in a computer cluster is implemented by reading and writing to files on a shared data store. Each host agent holds a lock on a file on the shared data store corresponding to a liveness indicator for the host agent and a coordinator host agent periodically monitors the liveness indicators for host failures.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08549364&OS=08549364&RS=08549364
owner: VMware, Inc.
number: 08549364
owner_city: Palo Alto
owner_country: US
publication_date: 20090218
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION"],"p":["A computer \u201ccluster\u201d typically refers to a group of linked computers (also referred to herein as \u201chosts\u201d) that are deployed in an aggregate, and a so-called \u201chigh availability\u201d cluster is one in which redundant computing resources are provided in case of hardware failure.","In a virtual machine environment, each host in a cluster can support multiple virtual machines. In a high availability cluster in such a virtual machine environment, when a host fails, each of the virtual machines running on the host is re-instantiated on another host in the cluster that has sufficient resources to support such virtual machine (such re-instantiation being referred to as \u201cfailover\u201d). Current methods of detecting host failure and performing \u201cfailover\u201d depend upon a software agent running on each host in the cluster. These agents communicate with each other through a common network (typically, a private network that differs from a network utilized by the virtual machines to provide services) to coordinate activity, such communication including selecting one or more \u201cprimary\u201d agents having the responsibility of: (a) synchronizing cluster state and configuration information across the cluster, (b) monitoring the condition of hosts in the cluster (e.g., by receiving TCP messages from the hosts that indicate \u201cliveness\u201d), and (c) directing the initiation of failover upon detecting a failure.","In a computer system having virtual machines instantiated on a cluster of host computers networked to a shared storage system, a method of monitoring failures in the host computers according to an embodiment of the present invention includes the steps of periodically examining, for each host computer in the cluster, a liveness indicator associated with a file in the shared storage system corresponding to the host computer, and identifying one or more host computers whose liveness indicator has not been updated within a predetermined time interval as a failed host computer.","In one embodiment, the liveness indicator is a heartbeat entry of the corresponding host computer and a lock associated with the file includes a reference to the heartbeat entry. In such an embodiment, the corresponding host computer maintains possession of the lock and the examining step involves inspecting the lock associated with the file.","In yet another embodiment, one of the host computers in the cluster serves as a coordinator that performs the foregoing method while each of the other host computers in the cluster periodically examine a liveness indicator associated with a coordinator file in the shared storage system to assess a liveness of the coordinator and assume responsibilities of the coordinator if the liveness indicator associated with the coordinator file has not been updated within a predetermined time period.",{"@attributes":{"id":"p-0013","num":"0012"},"figref":["FIG. 1","FIG. 1","FIG. 1"],"b":["100","100","102","104","106","108","108","110","112","102","118","120","120","112","114","102","104","106","108","110","120","120","120","120","122","122","124","126","128","130","120","122","132","136","120","122","122","138","138","112","120","120","122","122","138","138","138","138","112","112"],"sub":["1","N","1","N ","1","N ","1","N","1","1 ","1","1","N ","A","N ","1","N","1","N ","A","N","A","N "]},{"@attributes":{"id":"p-0014","num":"0013"},"figref":["FIG. 2","FIG. 2"],"b":["200","205","210","215","200","220","225","205","230","235","210","240","245","215","220","225","245","215","250","200","205","210","200","205","210","250","215"],"sub":["VHD","VHD","VHD","HB","HB","HB"]},"Each of hosts ,  and  also runs a software \u201chost agent\u201d (e.g., , , , respectively) that manages the host's activities relating to providing high availability. In one embodiment, such host agents run as processes in user space relative to the hypervisor and interact with the kernel of the hypervisor (e.g., via file system application programming interfaces) to access shared data store . Each such host agent creates and maintains the following files in its own directory in shared data store  for its respective host: (1) a \u201cmonitoring\u201d file (e.g., ), (2) a virtual machine data file (e.g., ), (3) a host data file (e.g., ), and (4) a mailbox directory (e.g., ). The directory structure and naming conventions of these files are well-known such that other host agents of other hosts can find and access these files in shared data store  if desired. For example, a host agent serving as a \u201ccoordinator\u201d for shared data store , as further described below, may have such a desire to access the foregoing directory and files.","The monitoring file (e.g., ) is an empty file created by a host agent to provide a mechanism to indicate its host's liveness to other host agents. In the particular embodiment of , a host agent maintains exclusive and continual access to its monitoring file by requesting that its host's hypervisor accesses and acquires the \u201cexclusive lock\u201d of the monitoring file. Once the hypervisor has acquired the exclusive lock, the host agent has exclusive access to the monitoring file and the host's hypervisor will automatically periodically update its heartbeat entry in heartbeat region  to indicate its liveness. The exclusive lock of the monitoring file is, itself, a data structure that is stored in shared data store  and is locatable by any process desiring to access the monitoring file. The exclusive lock contains a liveness field that is edited by a host's hypervisor that has successfully acquired the lock to point to the host's heartbeat entry in heartbeat region . As such, any process desiring to access the monitoring file can locate the exclusive lock and extract the location of the host's heartbeat entry to check the liveness of the host. Further details of one embodiment of the interaction between exclusive locks and a heartbeat region is described in the '109 Application. It should be recognized that various other techniques to implement exclusive locks for files on a shared data store may be utilized, including, for example, having a heartbeat entry or timestamp entry within the exclusive lock itself.","The virtual machine data file (e.g., ) comprises a list of virtual machines (e.g., location of the virtual hard drive and virtual machine configuration file, etc. stored on data store ) running on the host, as well as each such virtual machine's characteristics, such as its CPU, memory reservation and current memory overhead. The host data file (e.g., ) contains characteristics about the host, itself, such as the host's DNS name, MAC and IP addresses and other host resource information, total CPU and memory, and unreserved CPU and memory. The mailbox directory (e.g., ) is a messaging mechanism whereby other hosts can submit message files to the host by adding the message files to the mailbox directory.","In addition to managing the foregoing files in its own directory, one of the host agents also serves as a \u201ccoordinator\u201d for data store  to monitor the liveness of hosts that are running virtual machines whose virtual hard disks (e.g., , etc.) are stored on data store . To do this, the coordinator maintains a coordinator's directory that includes: (1) coordinator lock file , and (2) coordinator mailbox directory . The directory structure and naming conventions of these files are also well-known such that host agents of other hosts can find and access these files in shared data store  if desired (as further described below). In the example of , host agent serves as the coordinator for data store .","Similar to the monitoring file of a host agent's own directory, host agent , as the coordinator, maintains exclusive and continual access to coordinator lock file  by requesting that its host's hypervisor accesses and acquires the exclusive lock for coordinator lock file . Upon successfully acquiring the exclusive lock, the host's hypervisor inserts its heartbeat entry location into the liveness field of the exclusive lock. Because host agent has exclusive access to coordinator lock file , the host's hypervisor automatically periodically updates its heartbeat entry in heartbeat region  to indicate its liveness. As further discussed in the context of , the host agents of each host networked to data store  continually monitor the status of coordinator lock file  by locating the exclusive lock of the coordinator lock file , extracting the location of the heartbeat entry in the liveness field of the exclusive lock, and checking whether the heartbeat entry has been timely updated by the hypervisor of the host of host agent . If the hypervisor of the host of the current coordinator has not timely updated its heartbeat entry in the heartbeat region (i.e., indicating a possible failure of the current coordinator), a host agent that is currently checking the liveness field will recognize this, and steal the lock (e.g., by instructing its hypervisor to acquire the lock by embedding its heartbeat entry in the liveness field) to become the new coordinator. Coordinator mailbox directory  is similar to host agent 's own mailbox directory (i.e., ), and it provides a file based messaging mechanism for other host agents to communicate with the coordinator (i.e., whichever host agent may be serving as such at any particular time). The coordinator has the responsibility of detecting failures in the cluster and initiating failover recovery, including in accordance with any priorities set forth in cluster files  (as further discussed below). As such, the coordinator obviates a need for each host in a cluster to continually monitor the liveness of every other host in the cluster for failover purposes.","Virtual machine management center  is utilized by an IT function of an enterprise to create physical clusters and provision virtual machines for the computer systems in the cluster. Virtual machine management center  may be run on a separate computer system networked to the other computer systems in a cluster, as in , or it may be run inside a virtual machine on any particular computer system. One example of a virtual machine management center is VMware's VirtualCenter. Such virtual machine management centers create a cluster directory in a data store for each cluster that utilizes the data store such that each data store used by a cluster has a cluster directory for that particular cluster. As shown in , virtual machine management center  has created a cluster directory containing cluster files , such as a configuration file and virtual-machine-to-host compatibility file, that are accessible by each agent on a host belonging to the cluster. The configuration file details a restart order and priorities for virtual machines running in the cluster, as well as any specific hosts that are designated as failover hosts for other hosts. The virtual-machine-to-host compatibility file details compatibility information between virtual machines and hosts in the event certain virtual machines cannot be supported by the hardware and software configurations of certain hosts in the cluster. It should be recognized that any number of files may be used to store any number of cluster-wide settings and that the foregoing compatibility file and virtual machine-to-host file are merely exemplary. These cluster files, combined with the various files stored for each host agent (monitoring file, virtual machine data file, host data file, and mailbox) and the coordinator for the data store (coordinator lock file and mailbox), provide each data store with metadata needed to failover virtual machines located on such data store.",{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 3A","b":["300","270","250","302","300","304"]},"Simultaneous with the activities of each host agent above, in step , the host agent currently serving as the coordinator, through its corresponding hypervisor, checks the liveness field of the locks associated with each of the other host agents' monitoring files, each which points to the heartbeat entry of the hypervisor of each such host agent's host, to confirm that such host agents are alive. If the coordinator discovers in step  that any particular host's heartbeat entry has not been timely updated, the coordinator concludes in step  that the host has failed and begins to take failover recovery measures to re-instantiate the virtual machines that were running on the failed host.",{"@attributes":{"id":"p-0023","num":"0022"},"figref":["FIG. 3B","FIG. 3A"],"b":["315","320","315","325","330","335"]},"In step , the host agent currently serving as the coordinator for the data store, through its corresponding hypervisor, also updates the timestamp of the coordinator lock file to indicate liveness to the other host agents. In step , if the pre-determined time interval has elapsed, the coordinator will return to step  to update the coordinator lock file again. In step , the coordinator monitors all timestamps of the monitoring files of all the other host agents. If the coordinator discovers in step  that any particular host agent's monitoring file has not been timely updated within a predetermined interval, the coordinator concludes in step  that the host of such host agent has failed and begins to take failover recovery measures to re-instantiate the virtual machines that were running on the failed host.",{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 4","b":["400","405","285","410","285","415","420","425","405"]},"Simultaneous with the foregoing activities of the coordinator, in step , the host agent continuously polls its mailbox directory for new messages. When it receives the coordinator's message from step , the host agent reads the message in step  and begins the process of interacting with the host's hypervisor to perform recovery of the specified virtual machines in step . Once the virtual machines are successfully instantiated on the host, in step , the host agent transmits a message to the coordinator's mailbox directory confirming the successful recovery of the virtual machines. In step , the host agent then updates its own virtual machine data file and host data file to incorporate information regarding the newly instantiated virtual machines and the resources utilized by them.","Once the coordinator receives the confirmation message in step , the coordinator removes those virtual machines from the failed host's virtual machine data file in step . In step , if there are any more remaining virtual machines in the failed host's virtual machine data file, the coordinator returns to step  to find another host with available resources to fail over the remaining virtual machines.","Since it is possible for the hosts of a cluster to make use of different data stores, the different coordinators of each data store used by the cluster can communicate among each other to coordinate failover virtual machine priority across the different data stores.  depicts an example cluster configuration using different data stores. Each of hosts ,  and  have access to both of shared data stores  and . Host agent serves as the coordinator of data store  and host agent serves as the coordinator of data store . Host  runs four instantiated virtual machines, two of them,  and , use data store  and the other two,  and , use data store . Because the cluster configuration utilizes both shared data stores  and , as shown in , virtual machine management center  stores cluster files  in a cluster directory in both data stores. Cluster files  indicate that virtual machines  and  are high priority virtual machines while virtual machines  and  are low priority machines. This means that in the event that host  suffers a failure, virtual machine  on data store  and virtual machine  on data store  should be failed over before virtual machines  and . However, as the coordinator for data store , host agent will only recognize that virtual machines  and  have failed and will begin to failover these two virtual machines. Similarly, host agent as coordinator for data store  only recognizes that virtual machines  and  have failed and will begin to failover these two virtual machines. If, for example, host  and host  each have only enough resources to each failover one virtual machine, in the event of a failure of host , host agent , as coordinator for data store , may first failover high priority virtual machine  on itself and then instruct host agent to failover low priority virtual machine . Similarly, host agent , as coordinator for data store , may first failover high priority virtual machine  and then instruct host agent to failover low priority virtual machine . Without coordination between the two coordinators, it is possible that low priority virtual machines may be failed over before high priority machines. For example, if host agent , as coordinator for data store , successfully fails over virtual machine  before host agent , as coordinate for data store , even recognizes that host  has failed, host agent will instruct host agent to failover low priority virtual machine  on data store  before host agent , as coordinator for data store , even recognizes that high priority virtual machine  (as well as low priority virtual machine ) has failed and needs to be restored.","In one embodiment, a \u201cmaster\u201d coordinator is selected among the various coordinators to ensure that virtual machine failover priorities are respected. For example, in the embodiment of , host agent may be elected to be the master coordinator (e.g., automatically and arbitrarily or manually, by user configuration). Other coordinators consult the master coordinator to make placement decisions for failover to ensure that high priority virtual machines are restored before low priority virtual machines. For example, host agent will consult host agent (e.g., by sending host agent a list of the virtual machines it needs to fail over) prior to restoring virtual machines  (on host  on data store ) and  (on host  on data store ). Host agent , as the master coordinator, may inform host agent that it should only restore virtual machine  on data store  and allow low priority virtual machine  to lapse, thereby allowing host agent (as coordinator for data store ) to restore high priority virtual machine  on data store , given the priorities.","Persons skilled in the art will understand that various modifications and changes may be made to the specific embodiments described herein without departing from the broader spirit and scope of the invention as set forth in the appended claims. The foregoing description and drawings are, accordingly, to be regarded in an illustrative rather than a restrictive sense. For example, while the foregoing description has described host agents as separate from hypervisors, it should be recognized that this is a functional distinction and that alternative embodiments may have the functionality of such host agents embedded into hypervisors. Furthermore, in alternative embodiments, network based heartbeating may be utilized as a primary liveness check, whereby the coordinator transmits heartbeat signals through the network to the other host agents to indicate its liveness and each of the other host agents transmit heartbeat signals to the coordinator to indicate their respective liveness. In such alternative embodiments, storage-based heartbeating, such as those utilized in  may be utilized as a secondary liveness check. Similarly, host-to-host communication utilizing the network may be used in alternative embodiments as a primary communication medium, with the file-based mailbox communication mechanisms described in  as a backup mechanism. Alternative embodiments may also not have coordinator mailbox directory  as in . Instead, during step  of , a failover host agent submits a confirmation message to the mailbox of the host that serves as coordinator (i.e., rather than a special coordinator mailbox). As discussed, while embodiments utilizing VMFS data stores may have a heartbeat region in each data store, embodiments utilizing NFS data stores do not have a heartbeat region. In such embodiments, the hypervisor may create a special lock file associated with an opened file (e.g., coordinator lock file or monitoring file, etc.) which includes a logical timestamp that is periodically updated by the hypervisor (in a similar manner as the heartbeat region). It should be further recognized that the techniques disclosed herein can be applied to environments where some subset of the hosts in the cluster have access to some data stores while other hosts have access to different data stores. In such embodiments, virtual machines can be failed over to other hosts in their \u201cstorage island.\u201d It should also be recognized that the techniques disclosed herein can be further applied to environments where virtual machines are not running on a shared data store. For example, mechanisms such as data store mirroring can keep a copy of the virtual machine files in sync on a different data store.","The various embodiments described herein may employ various computer-implemented operations involving data stored in computer systems. For example, these operations may require physical manipulation of physical quantities usually, though not necessarily, these quantities may take the form of electrical or magnetic signals where they, or representations of them, are capable of being stored, transferred, combined, compared, or otherwise manipulated. Further, such manipulations are often referred to in terms, such as producing, identifying, determining, or comparing. Any operations described herein that form part of one or more embodiments of the invention may be useful machine operations. In addition, one or more embodiments of the invention also relate to a device or an apparatus for performing these operations. The apparatus may be specially constructed for specific required purposes, or it may be a general purpose computer selectively activated or configured by a computer program stored in the computer. In particular, various general purpose machines may be used with computer programs written in accordance with the teachings herein, or it may be more convenient to construct a more specialized apparatus to perform the required operations.","The various embodiments described herein may be practiced with other computer system configurations including hand-held devices, microprocessor systems, microprocessor-based or programmable consumer electronics, minicomputers, mainframe computers, and the like.","One or more embodiments of the present invention may be implemented as one or more computer programs or as one or more computer program modules embodied in one or more computer readable media. The term computer readable medium refers to any data storage device that can store data which can thereafter be input to a computer system computer readable media may be based on any existing or subsequently developed technology for embodying computer programs in a manner that enables them to be read by a computer. Examples of a computer readable medium include a hard drive, network attached storage (NAS), read-only memory, random-access memory (e.g., a flash memory device), a CD (Compact Discs) CD-ROM, a CD-R, or a CD-RW, a DVD (Digital Versatile Disc), a magnetic tape, and other optical and non-optical data storage devices. The computer readable medium can also be distributed over a network coupled computer system so that the computer readable code is stored and executed in a distributed fashion.","Although one or more embodiments of the present invention have been described in some detail for clarity of understanding, it will be apparent that certain changes and modifications may be made within the scope of the claims. Accordingly, the described embodiments are to be considered as illustrative and not restrictive, and the scope of the claims is not to be limited to details given herein, but may be modified within the scope and equivalents of the claims. In the claims, elements and\/or steps do not imply any particular order of operation, unless explicitly stated in the claims.","In addition, while described virtualization methods have generally assumed that virtual machines present interfaces consistent with a particular hardware system, persons of ordinary skill in the art will recognize that the methods described may be used in conjunction with virtualizations that do not correspond directly to any particular hardware system. Virtualization systems in accordance with the various embodiments, implemented as hosted embodiments, non-hosted embodiments, or as embodiments that tend to blur distinctions between the two, are all envisioned. Furthermore, various virtualization operations may be wholly or partially implemented in hardware. For example, a hardware implementation may employ a look-up table for modification of storage access requests to secure non-disk data.","Many variations, modifications, additions, and improvements are possible, regardless of the degree of virtualization. The virtualization software can therefore include components of a host, console, or guest operating system that performs virtualization functions. Plural instances may be provided for components, operations or structures described herein as a single instance. Finally, boundaries between various components, operations and data stores are somewhat arbitrary, and particular operations are illustrated in the context of specific illustrative configurations. Other allocations of functionality are envisioned and may fall within the scope of the invention(s). In general, structures and functionality presented as separate components in exemplary configurations may be implemented as a combined structure or component. Similarly, structures and functionality presented as a single component may be implemented as separate components. These and other variations, modifications, additions, and improvements may fall within the scope of the appended claims(s)."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0007","num":"0006"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 3A"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 3B"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 5"}]},"DETDESC":[{},{}]}
