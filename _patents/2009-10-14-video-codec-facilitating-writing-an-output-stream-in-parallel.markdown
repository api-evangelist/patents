---
title: Video codec facilitating writing an output stream in parallel
abstract: Methods, apparatuses and systems directed to a video codec that facilitates parallel processing operations is implemented on a graphics processing unit or other hardware device. The codec encodes video frames by dividing a frame into macroblocks and processing the macroblocks in parallel either via plane fitting operations or by motion estimation adjustments. A gathering operation writes the data of each macroblock onto an output stream in a parallel processing operation to accelerate writing the output from the memory of the graphics processing unit to a main memory of a hardware system.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08396122&OS=08396122&RS=08396122
owner: Otoy, Inc.
number: 08396122
owner_city: Sherman Oaks
owner_country: US
publication_date: 20091014
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["TECHNICAL FIELD","BACKGROUND","SUMMARY","DESCRIPTION OF EXAMPLE EMBODIMENT(S)"],"p":["The present disclosure generally relates to video codecs.","Video compression refers to reducing the quantity of data used to represent video images and often combines image compression and motion compensation. A video codec is a device, or process implemented in software executed by a general purpose computing system, that enables video compression and\/or decompression for digital video. Traditionally, video codecs apply various spatial and temporal transforms (such as discrete cosine transforms and the like) on the two-dimensional frames that make up a video sequence in order to reduce the raw data that must be stored on a storage medium or transmitted across a network.","A graphics processing unit or GPU (also occasionally called visual processing unit or VPU) is a specialized processor that offloads graphics rendering from the microprocessor. It is used in embedded systems, mobile phones, personal computers, workstations, and game consoles. Modern GPUs are very efficient at manipulating computer graphics, and their highly parallel structure makes them quite effective compared to general-purpose CPUs for a range of complex algorithms. Additionally, a somewhat recent concept is to use a modified form of a stream processor to allow a general purpose graphics processing unit. This concept turns the massive floating-point computational power of a modern graphics accelerator's shader pipeline into general-purpose computing power, as opposed to being hard wired solely to do graphical operations. In certain applications requiring massive vector operations, this can yield several orders of magnitude higher performance than a conventional CPU. For example, Nvidia(r) Corporation has begun releasing GPU cards that support an application programming interface (API) extension to the C programming language CUDA (\u201cCompute Unified Device Architecture\u201d), which allows specified functions from a normal C program to run on the GPU's stream processors. This makes C programs capable of taking advantage of a GPU's ability to operate on large matrices in parallel, while still making use of the CPU where appropriate.","The present invention provides methods, apparatuses and systems directed to a video codec that facilitates parallel processing operations. In some implementations, the video encoding and\/or decoding processes can be implemented on a graphics processing unit or other hardware device that allows for massively parallel processing operations to increase throughput and reduce processing latency, while still achieving a high compression ratios. Other implementations of the invention are directed to a novel gather operation that facilitates the use of massively parallel processing operations for entropy coding to increase throughput and\/or reduce latency.","The following example embodiments are described and illustrated in conjunction with apparatuses, methods, and systems which are meant to be illustrative, not limiting in scope.","A. Overview","A.1. Network Environment",{"@attributes":{"id":"p-0014","num":"0013"},"figref":["FIG. 1","FIG. 1"],"b":["20","60","60","60","60","60","82","84","82","84"]},"Video transmission system  is a network addressable system that hosts one or more applications that stream video data to one or more users over a computer network. The video transmission system  may include web site and server functionality where users may request and receive identified web pages, video and other content over the computer network. The video data may incorporate non-interactive multimedia content, such a movies, animation, television programming, live broadcasts and the like, or may be video data rendered in connection with interactive games or other network-based applications in a server-side rendering system.","In particular implementations, video transmission system  comprises one or more physical servers  and one or more data stores . The one or more physical servers  are operably connected to computer network  via a router . The one or more physical servers , in one particular implementation, host functionality that allows users to browse available content, such as receiving requests from, and transmitting responsive data to, client nodes  and . In one implementation, the functionality hosted by the one or more physical servers may include web or HyperText Transport Protocol (HTTP) servers, Real Time Streaming Protocol (RTSP) servers, and the like.","Physical servers , as discussed above, host functionality directed to supporting and implementing video transmission system . In a particular implementation, the physical servers  may host one or more instances of a video streaming (e.g., RTSP) server and one or more instances of a remote rendering device server. In one implementation, a data store  may store video content such as digital content data objects, user information, and other media assets. A content data object or a content object, in particular implementations, is an individual item of digital information typically stored or embodied in a data file, binary large object (BLOB) or record. Content objects may take many forms, including: text (e.g., ASCII, SGML, HTML), images (e.g., jpeg, tif and gif), graphics (vector-based or bitmap), audio, video (e.g., mpeg), or other multimedia, and combinations thereof. Content object data may also include executable code objects, object or asset definitions, and the like. Structurally, content data store  connotes a large class of data storage and management systems. In particular implementations, content data store  may be implemented by any suitable physical system including components, such as database servers, mass storage media, media library systems, and the like. This data may be accessed to render video frames in substantial real-time for transmission to one or more client nodes , . The video codec described below can be used to compress the video data stream for transmission over network cloud .","A.2. Example Computing System Architecture","The server and client host systems described herein may be implemented in a wide array of computing systems and architectures. The following describes example computing architectures for didactic, rather than limiting, purposes.",{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 2","b":["200","202","204","200","206","208","210","202","206","212","206","208","214","216","206","200","224","206","208","224","224"]},"Mass storage , and I\/O ports  couple to bus . Hardware system  may optionally include a keyboard and pointing device, and a display device (not shown) coupled to bus . Collectively, these elements are intended to represent a broad category of computer hardware systems, including but not limited to general purpose computer systems based on the x86-compatible processors manufactured by Intel Corporation of Santa Clara, Calif., and the x86-compatible processors manufactured by Advanced Micro Devices (AMD), Inc., of Sunnyvale, Calif., as well as any other suitable processor.","The elements of hardware system  are described in greater detail below. In particular, network interface  provides communication between hardware system  and any of a wide range of networks, such as an Ethernet (e.g., IEEE 802.3) network, etc. Mass storage  provides permanent storage for the data and programming instructions to perform the above described functions implemented in the location server , whereas system memory  (e.g., DRAM) provides temporary storage for the data and programming instructions when executed by processor . I\/O ports  are one or more serial and\/or parallel communication ports that provide communication between additional peripheral devices, which may be coupled to hardware system .","Hardware system  may include a variety of system architectures; and various components of hardware system  may be rearranged. For example, cache  may be on-chip with processor . Alternatively, cache  and processor  may be packed together as a \u201cprocessor module,\u201d with processor  being referred to as the \u201cprocessor core.\u201d Furthermore, certain embodiments of the present invention may not require nor include all of the above components. For example, the peripheral devices shown coupled to standard I\/O bus  may couple to high performance I\/O bus . In addition, in some embodiments, only a single bus may exist, with the components of hardware system  being coupled to the single bus. Furthermore, hardware system  may include additional components, such as additional processors, storage devices, or memories.","Graphics processing unit , in one implementation, comprises one or more integrated circuits and\/or processing cores that are directed to mathematical operations commonly used in graphics rendering. In some implementations, the GPU  may use a special graphics unit instruction set, while in other implementations, the GPU may use a CPU-like (e.g. a modified x86) instruction set. Graphics processing unit  can implement a number of graphics primitive operations, such as Hitting, texture mapping, pixel shading, frame buffering, and the like. In addition to the 3D hardware, graphics processing unit  may include basic 2D acceleration and framebuffer capabilities. In addition, graphics processing unit  may support the YUV color space and hardware overlays, as well as MPEG primitives (such as motion compensation and iDCT). Graphics processing unit  may be a graphics accelerator, a GPGPU (General Purpose GPU), or any other suitable processing unit.","As discussed below, in one implementation, the operations of one or more of the physical servers described herein are implemented as a series of software routines run by hardware system . These software routines comprise a plurality or series of instructions to be executed by a processor in a hardware system, such as processor . Initially, the series of instructions may be stored on a storage device or other computer readable medium, such as mass storage . However, the series of instructions can be stored on any suitable storage medium, such as a diskette, CD-ROM, ROM, EEPROM, etc. Furthermore, the series of instructions need not be stored locally, and could be received from a remote storage device, such as a server on a network, via network\/communication interface . The instructions are copied from the storage device, such as mass storage , into memory  and then accessed and executed by processor . The software routines can cause certain operations to be performed by the graphics processing unit  and\/or the processor .","An operating system manages and controls the operation of hardware system , including the input and output of data to and from software applications (not shown). The operating system provides an interface between the software applications being executed on the system and the hardware components of the system. According to one embodiment of the present invention, the operating system is the Windows\u00ae 95\/98\/NT\/XP\/Vista\/7 operating system, available from Microsoft Corporation of Redmond, Wash. However, the present invention may be used with other suitable operating systems, such as the Apple Macintosh Operating System, available from Apple Computer Inc. of Cupertino, Calif., UNIX operating systems, LINUX operating systems, and the like. Of course, other implementations are possible. For example, the server functionalities described herein may be implemented by a plurality of server blades communicating over a backplane.","Additionally, video decoding clients may be hosted on the same or similar hardware architectures. For example, client computer  may include a GPU which loads encoded video into GPU memory, and decodes the encoded video data to render one or more frames of a video sequence. In other implementations, some or all of the video compression and de-compression operations may also be performed in system memory using a general purpose computing architecture as well.","The example video codec described herein processes video frames buffered in a memory. The memory may be main CPU memory or, in some implementations, the memory buffers available on a GPU. For example, a separate rendering process executing on a GPU (or a CPU) may render a video frame. The example codec may process the frame as described below for output to a video client for decoding and display. The following describes both an I frame encoding process and a P frame encoding process. The codec described herein can be used in an I-frame only mode or a P-frame mode.","I Frame Rendering Process",{"@attributes":{"id":"p-0029","num":"0028"},"figref":["FIG. 3","FIG. 3"]},"The transformation into the YCbCr color model enables downsampling, which is to reduce the spatial resolution of the Cb and Cr components (called \u201cdownsampling\u201d or \u201cchroma subsampling\u201d). The ratios at which the codec applies downsampling, in one implementation, is 4:2:0 (resulting in a reduction by a factor of 2 in horizontal and vertical directions). For the rest of the compression process, the Y, Cb and Cr components are processed separately and in a very similar manner.  illustrates the process steps applies to the luma or Y component of the frame data. The operations applied to the chroma components of the image differ slightly as described in more detail below.","Referring now to , the video codec accesses the frame stored in memory and divides the frame into macroblocks (). In one implementation, the size of each macroblock is 16 pixels by 16 pixels. As described below, DCT operations are performed, in some implementations, on 8\u00d78 partitions. Accordingly, each macroblock, due to 4:2:0 downsampling includes 4 8\u00d78 luma partitions, 4 4\u00d74 Cb partitions and 4 4\u00d74 Cr partitions. However, other macroblock sizes can be used. The video codec then performs certain transform and quantization operations on the luma and chroma partitions of each macroblock () as follows. These operations can be done serially or in parallel. If the encoding operations are performed on a GPU, the processing for the macroblocks can be accomplished in parallel resulting in reduced processing time and greater through-put.","As  illustrates, the codec fits a 2-dimensional plane to each macroblock that best models the luma values of the pixels of the macroblock. The plane is defined by the equation z=ax+by, where a and b define the gradient parameters of the plane. The codec then adjusts the luma values for the pixels of the macroblock based on the plane gradient parameters. In one implementation, the codec, for each pixel, subtracts the corresponding value of the plane equation from the pixel value at that location. Conceptually, this is defined by the following equation: adjusted pixel(x,y)=pixel(x,y)\u2212z. The codec, in one implementation, does not adjust the values of the chroma partitions.","The codec then applies a Discrete Cosine Transformation (DCT) to the macroblock (). In one implementation, the codec applies the DCT separately to the 4 luma partitions of 8\u00d78 pixels and the 8 4\u00d74 chroma partitions. The codec then applies a 2\u00d72 Hadamard transform on the DC components of each DCT coefficient set corresponding to each luma partition, and to each chroma partition, to find correlation between the DC coefficient values. In an alternative implementation, the codec dynamically determines the size of the partitions. For example, the codec may dynamically choose between 4\u00d74, 8\u00d78 or 16\u00d716 partitions based on rate and distortion for each partition choice.","The codec reorders the DCT coefficients for a macroblock into a 1-dimensional array (). In one implementation, DCT coefficients of the 4 luma partitions are scanned in a zigzag order and then the DCT coefficients are interlaced to group the DC component values and other corresponding DCT component values together. In one implementation, the luma partitions are separately encoded from the chroma partitions. In one implementation, the chroma partitions are interlaced together in a manner similar to the luma channel. In other implementations, alternatives to zigzag re-ordering can be used.","The codec also quantizes the DCT coefficients in the 1-D array () using, for example, rate distortion optimization, which is a quantization process that evaluates whether to round up or down to the nearest integer, or to round to zero, based on the resulting cost (the number of bits required to encode) for each choice and the error or distortion associated with each choice. Rate-distortion optimization, in one implementation, essentially applies a video quality metric, measuring both the deviation from the source and the bit cost for each possible decision outcome. The bits are mathematically measured by multiplying the bit cost by the Lagrangian, a value representing the relationship between bit cost and quality for a particular quality level. The deviation from the source can be measured as the mean squared error, in order to maximize the Peak signal-to-noise ratio (PSNR) video quality metric. Calculating the bit cost can be accomplished by passing each block of video to be tested to the entropy coder to measure its actual bit cost.","The codec then computes the DC prediction values for each macroblock () and adjusts the DC coefficient for each macroblock based on the DC prediction value (), in one implementation, by subtracting the DC prediction value from the DC coefficient for the macroblock. In one implementation, the DC prediction value, X, is based on the DC coefficients of neighboring blocks, as shown in the diagram and equation set forth in . In other words, the DC prediction value for block X is based on the sum of the DC coefficients for blocks a and b, less the DC coefficient value of block c. In one implementation, the DC prediction value adjustments are performed on both the luma and chroma channels.","The codec then encodes the quantized coefficients into a reserved memory space for each block (). In one implementation, the codec uses a form of variable length coding to compress the quantized coefficients for each block. In one implementation, the codec uses run-length encoding (RLE) followed by Huffman coding. After encoding the quantized DCT coefficients for each macroblock, the result is the encoded bits for the macroblock and a bit-length. With knowledge of the bit length, the codec can perform a parallel prefix sum to obtain the starting position of each macroblock in the final output bit stream.","The codec then causes the GPU to writes the encoded data onto an output stream to be buffered in main system memory, in one implementation, by writing the data of each macroblock in a raster-like order. In one implementation, the data can be written into CPU memory by the GPU using parallel processing operations. For example, the codec may cause the data to be written to an output stream in a gather operation. For example, a process or function tasked with writing a data word (such as 1 or 2 bytes) can use the starting bit positions for each macroblock and the position of the data word in the output stream for the frame to determine from which reserved memory locations to access the data bit to construct the data word and write it to memory. The gather operation allows for writing the output in a parallel processing operation utilizing the resources of the GPU thereby further increasing throughput. For example, in one implementation, a function can be defined that receives as inputs the position of the data or machine word in the memory to which the output stream is written. The function is operative to assemble the machine word from selected portions of the bits of one or more macroblocks based on the position of the data or machine word and the starting bit values of the macroblocks, and write the data of machine word to the memory. This function can be implemented in parallel to accelerate writing this output from a memory of a GPU to main memory for transmission over a network. In other implementations, the memory of the GPU can be directly accessed to write the data for transmission to a socket or other connection abstraction.","P Frame Encoding","As discussed above, the process illustrated in  can be used to naively encode each individual video frame at some desired frame rate. In some implementations, the process illustrated in  can be used in predictive coding schemes to encode so-called I frames, while the process described below in connection with  can be used to encode so-called P (predictive) frames.","As  illustrates, the codec accesses a video frame and divides the frame into macroblocks (). Similar to the processing illustrated in , the codec processes each macroblock (), which processing can be done in parallel using the resources of a GPU or other parallel processing hardware. In the implementation shown, the codec estimates the motion between the current macroblock and the reference frame (). The codec then processes the macroblock according to a plurality of modes (I, C, and P) and selects the best result for output. For the I mode, in one implementation, the codec computes the quantized DCT coefficients as discussed above in connection with . In the P mode, the code computes the quantized DCT coefficients in a manner similar to  with the differences being that 1) the plane fitting operations are substituted by motion estimation adjustments, and 2) adjustments to the DC coefficients based on DC prediction values are omitted. The C mode is essentially a copy of the corresponding macroblock of the reference frame, which is usually the previously decoded frame.","The codec then selects from among the results of the I, C and P modes (). In one implementation, the codec uses rate distortion optimization to select the mode for a given macroblock. To find the error or distortion of the I, C and P modes, the codec can compare the true image to the decoded result of the I, C and P modes. In another implementation, the codec can compute the sum of the changes to the DCT coefficients resulting from quantization for the I and P modes. To compute the error for the C mode, the codec can take the difference between the current and the reference block and apply a DCT operation to that difference to yield the error. The cost for the C mode, in one implementation, is 1 bit to indicate an instruction to copy a macroblock. The cost for the I mode are the number of bits of the compressed DCT coefficients, while the cost for the P mode includes the number of bits for the compressed DCT coefficients and the motion estimation parameters. Various factor weighting schemes can be used on the error and\/or costs terms to bias selection in favor of a given mode or for perceptual choices. For example, the codec may be configured to bias selection away from the C mode if the error in the DC coefficient is beyond some threshold.","The codec encodes the macroblocks () and writes the data of each macroblock onto an output stream in a manner the same as that described above in connection with . The data for each macroblock, in one implementation, includes a mode selection indicator, as well as the data discussed above. The data for each frame may also include a table indicating the starting bit positions for each macroblock. Additional frame metadata can also be added. The resulting data can be written to a socket for transmission across a network. In another implementation, the encoded data can be written to a data storage medium, such as an optical or magnetic disk.","Decoding the Video Stream","Decoding the compressed video data essentially involves the inverse of the operations discussed above.  illustrates an example decoding process according to one implementation of the present invention. In one implementation, the video decoder accesses the encoded data and decompresses the video data stream (). In implementations, where RLE and Huffman coding is used, the decoder decompresses the data stream using known algorithms. The decoder then identifies the correspondence between macroblocks of a frame in the uncompressed video data (). In one implementation, the video data also includes a table that identifies the starting bit positions for each macroblock. In some implementations involving P frames, the decoder can identify a mode (I, C or P) for each macroblock.","For I mode macroblocks, the decoder solves for the DC prediction values and adjusts the DC coefficients based on the determined DC prediction values (). In one implementation, the decoder processes the macroblocks in a raster-like order. Accordingly, the DC prediction values can be computed by accessing the neighboring macroblocks, the DC coefficients of which have already been computed. In some implementations, the DC prediction values can be solved in parallel processing operations by performing the 2D equivalent of a prefix sum.","As  illustrates, the decoder constructs a DCT coefficient matrix for each partition of each macroblock () based on the inverse of the re-ordering process described above. The decoder then de-quantizes the matrix for each partition of the macroblocks () and applies an inverse DCT operation to the matrices (). For I Frames and I-mode macroblocks, the decoder also adjusts the pixel values based on the plane gradient parameters (). For P-mode macroblocks, the decoder adjusts the pixel values based on the motion estimation parameters. For C-mode macroblocks, the decoder avoids all of the following operations and merely copies the data of the corresponding reference frame macroblock. Although not illustrated in , the decoder may also upsample the chroma components of the video data and remap the data from the YCbCr color model to the RGB color model. The resulting data can be written to on-screen memory for display by a display device.","The decoder operations described above can be implemented on a general purpose computer or a mobile device using the resources of the central processing unit. In other implementations, the resources of a GPU can be used to facilitate some or all of the decoding operations discussed above. Furthermore, many of the operations can be done in parallel or in serial. For example, the processing at the macroblock level can be processed in parallel.","Lastly, the above-described elements and operations can be comprised of instructions that are stored on storage media. The instructions can be retrieved and executed by a processing system. Some examples of instructions are software, program code, and firmware. Some examples of storage media are memory devices, tape, disks, integrated circuits, and servers. The instructions are operational when executed by a processing system to direct the processing system to operate in accord with the invention. The term \u201cprocessing system\u201d refers to a single processing device or a group of inter-operational processing devices. Some examples of processing devices are integrated circuits and logic circuitry. Those skilled in the art are familiar with instructions, computers, and storage media.","The present invention has been explained with reference to specific embodiments. For example, although the embodiments described above operate in connection with a client-server model, the video codec described above can also operate in peer-to-peer architectures as well, such as in connection with Video over IP or teleconferencing systems. Still further, the codec described above can be used to encode video data for storage on a data storage medium, such as a magnetic or optical disk. Other embodiments will be evident to those of ordinary skill in the art. It is therefore not intended that the present invention be limited, except as indicated by the appended claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0006","num":"0005"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0007","num":"0006"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 6"}]},"DETDESC":[{},{}]}
