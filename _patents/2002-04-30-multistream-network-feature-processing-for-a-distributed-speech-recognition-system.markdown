---
title: Multistream network feature processing for a distributed speech recognition system
abstract: A distributed voice recognition system and method for obtaining acoustic features and speech activity at multiple frequencies by extracting high frequency components thereof on a device, such as a subscriber station and transmitting them to a network server having multiple stream processing capability, including cepstral feature processing, MLP nonlinear transformation processing, and multiband temporal pattern architecture processing. The features received at the network server are processed using all three streams, wherein each of the three streams provide benefits not available in the other two, thereby enhancing feature interpretation. Feature extraction and feature interpretation may operate at multiple frequencies, including but not limited to 8 kHz, 11 kHz, and 16 kHz.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07089178&OS=07089178&RS=07089178
owner: Qualcomm Inc.
number: 07089178
owner_city: San Diego
owner_country: US
publication_date: 20020430
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD","BACKGROUND","SUMMARY","DETAILED DESCRIPTION","Definitions","DESCRIPTION"],"p":["The present invention relates generally to the field of communications and more specifically to a distributed voice recognition system employing feature computation, transmission, and extraction utilizing multistream network server processing.","Voice recognition (VR), nowadays more precisely called speech recognition, refers to a technique enabling a device to recover linguistic information from user-voiced speech. Once the device recognizes the linguistic information, the device may act on the information or cause another device to act on the information, thus facilitating a human interface with a device. Systems employing techniques to recover a linguistic message from an acoustic speech signal are called voice recognizers, or VR systems.","Recently, communication systems facilitating multiple-access, i.e., simultaneous transmission and\/or reception, of several signals over a common communication channel have been developed and achieved widespread usage. Multiple-access communication systems often include a plurality of remote subscriber units requiring intermittent service of relatively short duration rather than continuous access to the common communication channel. Several multiple-access techniques are known in the art, such as time division multiple-access (TDMA) and a frequency division multiple-access (FDMA). Another type of a multiple-access technique is a code division multiple-access (CDMA) spread spectrum system that conforms to the \u201cTIA\/EIA\/IS-95 Mobile Station-Base Station Compatibility Standard for Dual-Mode Wide-Band Spread Spectrum Cellular System,\u201d hereinafter referred to as the IS-95 standard. The use of CDMA techniques in a multiple-access communication system is disclosed in U.S. Pat. No. 4,901,307, entitled \u201cSPREAD SPECTRUM MULTIPLE-ACCESS COMMUNICATION SYSTEM USING SATELLITE OR TERRESTRIAL REPEATERS,\u201d and U.S. Pat. No. 5,103,459, entitled \u201cSYSTEM AND METHOD FOR GENERATING WAVEFORMS IN A CDMA CELLULAR TELEPHONE SYSTEM,\u201d both assigned to the assignee of the present invention.","A multiple-access communication system may be a wireless or wire-line and may carry voice and\/or data. An example of a communication system carrying both voice and data is a system in accordance with the IS-95 standard, which specifies transmitting voice and data over the communication channel. A method for transmitting data in code channel frames of fixed size is described in detail in U.S. Pat. No. 5,504,773, entitled \u201cMETHOD AND APPARATUS FOR THE FORMATTING OF DATA FOR TRANSMISSION\u201d, assigned to the assignee of the present invention. In accordance with the IS-95 standard, the data or voice is partitioned into code channel frames that are 20 milliseconds wide with data rates as high as 14.4 Kbps. Additional examples of a communication systems carrying both voice and data comprise communication systems conforming to the \u201c3rd Generation Partnership Project\u201d (3GPP), embodied in a set of documents including Document Nos. 3G TS 25.211, 3G TS 25.212, 3G TS 25.213, and 3G TS 25.214 (the W-CDMA standard), or \u201cTR-45.5 Physical Layer Standard for cdma2000Spread Spectrum Systems\u201d (the IS-2000 standard).","In a multiple-access communication system, communications between users are conducted through one or more base stations. A first user on one subscriber station communicates to a second user on a second subscriber station by transmitting data on a reverse link to a base station. The base station receives the data and can route the data to another base station. The data is transmitted on a forward link of the same base station, or the other base station, to the second subscriber station. The forward link refers to transmission from a base station to a subscriber station and the reverse link refers to transmission from a subscriber station to a base station. Likewise, the communication can be conducted between a first user on one mobile subscriber station and a second user on a landline station. A base station receives the data from the user on a reverse link, and routes the data through a public switched telephone network (PSTN) to the second user. In many communication systems, e.g., IS-95, W-CDMA, IS-2000, the forward link and the reverse link are allocated separate frequencies.","A user usually interfaces with a subscriber station via a keypad and a display. Such an interface imposes certain limits on its operation. For example, when the user is engaged in another activity requiring visual and physical attention to the activity to operate the subscriber station, e.g., driving an automobile, the user must remove his or her hand from the steering wheel and look at the telephone keypad while pushing buttons on the keyboard. Such actions tend divert attention from driving. Even if a full concentration on the interface is assured, certain actions, e.g., entry of short messages in a short message system (SMS) enabled subscriber station, can be cumbersome.","As a result of these user interface problems, there is an interest in implementing a VR system into a subscriber station. In general, a VR system comprises an acoustic processor, also called the front end of the VR system, and a word decoder, also called the back end of the VR system. The acoustic processor performs feature extraction, i.e., extracting a sequence of information bearing features from a speech signal. Feature extraction is necessary for enabling recognition of the speech signal linguistic information. Extracted features are transmitted from the front end to the back end of the VR system. The word decoder decodes the sequence of features received to provide a meaningful and desired output, representing the linguistic information contained in the speech signal.","For complex voice recognition tasks, the computational requirement of the processing associated with VR is significant. In a typical DVR system, the word decoder has relatively high computational and memory requirements as measured against the front end of the voice recognizer. Consequently, it is often desirable to place the feature\/word decoding task on a subsystem having the ability to appropriately manage computational and memory requirement, such as a network server, while keeping the acoustic processor physically as close to the speech source as possible to reduce adverse effects associated with vocoders. A vocoder is a device for processing the speech signal prior to transmission. Such a VR system implementation, using distributed system architecture, is known as a Distributed Voice Recognition (DVR) system. Thus, in a DVR system, feature extraction is performed at a device, such as a subscriber station comprising a front end, and the features subscriber station sends the features to the network, comprising a back end. The network decodes the features and provides a desired linguistic output. Examples of DVR systems are disclosed in U.S. Pat. No. 5,956,683, entitled \u201cDistributed Voice Recognition System,\u201d assigned to the assignee of the present invention.","Certain DVR systems and designs have been employed with varying results. Certain previous systems have operated at low frequency levels, such as in the range of 4 kHz, and have ignored or omitted certain high frequency components of speech, both on the subscriber station side and the network server side. Performance of such systems tend to favor low frequency components received at the expense of high frequency components, particularly those in excess of about 4 kHz. Failure to properly decode, implement, and pass these high frequency components has a tendency to miss certain aspects of the received analog speech signal and create an improper representation of the speech at the network server. Further, interpretation of received features at the network server has tended to use cepstral features exclusively. Cepstral features provide certain information on the features, but use of cepstral processing alone tends to omit certain aspects of speech, or fail to identify certain properties in the speech that are transferred over as features. Previous systems have also operated at a single or limited frequency, thus potentially again adversely affecting either the quality of the speech transmitted, the quality of features derived, or both.","As follows from the above description, there is a need in the art to extract acoustic features, including the high frequency components thereof, and transmit the features with minimal delay over the network such that the back end may process and employ high frequency components in to provide an enhanced acoustic representation of the received speech signal.","The aspects described herein are directed to a DVR system and method for obtaining acoustic features and speech activity at multiple frequencies by extracting high frequency components thereof on a device, such as a subscriber station, and transmitting them to a network server having multiple stream processing capability, including cepstral feature processing, MLP nonlinear transformation processing, and multiband temporal pattern architecture processing. Multiple stream processing may occur at a network server. Either or both sides of the DVR may operate at various frequencies, including but not limited to 8 kHz, 11 kHz, and 16 kHz.","According to one aspect of the present invention, there is provided a method of processing, transmitting, receiving, and decoding speech information, comprising receiving signals representing speech, decomposing the signals representing speech into higher frequency components and lower frequency components, processing the higher frequency components and lower frequency components separately and combining processed higher frequency components and lower frequency components into a plurality of features. The method further comprises transmitting the features to a network, receiving the features and processing said received features using a plurality of streams. The plurality of streams includes a cepstral stream, a nonlinear neural network stream, and a multiband temporal pattern architecture stream. The method further includes concatenating all received features processed by the plurality of streams into a concatenated feature vector.","According to a second aspect of the present invention, there is provided a system for processing speech into a plurality of features, comprising an analog to digital converter able to convert analog signals representing speech into a digital speech representation, a fast fourier transform element for computing a magnitude spectrum for the digital speech representation, a power spectrum splitter for splitting the magnitude spectrum into higher and lower frequency components, a noise power spectrum estimator and a noise reducer for estimating the power spectrum and reducing noise of the higher and lower frequency components to noise reduced higher frequency components and noise reduced lower frequency components, a mel filter for mel filtering the noise reduced lower frequency components, and a plurality of linear discriminant analysis filters for filtering the mel filtered noise reduced lower frequency components and the noise reduced lower frequency components. The system further includes a combiner for combining the output of the linear discriminant analysis filters with a voice activity detector representation of the mel filtered noise reduced lower frequency components and a feature compressor for compressing combined data received from the combiner.","According to a third aspect of the present invention, there is provided a system for incorporating information from a relatively long time span of feature vectors. The system includes a multiple stream processing arrangement. The multiple stream processing arrangement includes a cepstral stream processing arrangement for computing mean and variance normalized cepstral coefficients and at least one derivative thereof, a nonlinear transformation of the cepstral stream comprising a multi layer perceptron to discriminate between phoneme classes in said features, and a multiband temporal pattern architecture stream comprising mel spectra reconstruction and at least one multi layer perceptron to discriminate between manner of articulation classes in each mel spectral band. The system further includes a combiner to concatenate features received from the cepstral stream processing arrangement, the nonlinear transformation of the cepstral stream, and the multiband temporal pattern architecture stream.","The word \u201cexemplary\u201d is used exclusively herein to mean \u201cserving as an example, instance, or illustration.\u201d Any embodiment or aspect described herein as \u201cexemplary\u201d is not necessarily to be construed as preferred or advantageous over other embodiments or aspects.","The term \u201caccess network\u201d or \u201cnetwork server\u201d is used herein to mean a collection of access points (AP) or collection of base stations and one or more controllers, including processing hardware coupled thereto by any means. The access network or network server transports data packets between multiple access terminals (AT). The access network or network server may be further connected to additional networks outside the access network, such as a corporate intranet or the Internet, and may transport data packets between each access terminal and such outside networks.","The term \u201cbase station,\u201d referred to herein as an AP in the case of an HDR communication system, is used exclusively herein to mean the hardware with which subscriber stations communicate. Cell refers to the hardware or a geographic coverage area, depending on the context in which the term is used. A sector is a partition of a cell. Because a sector has the attributes of a cell, the teachings described in terms of cells are readily extended to sectors.","The term \u201csubscriber station,\u201d referred to herein as an AT in the case of an HDR communication system, is used exclusively herein to mean the hardware with which an access network communicates. An AT may be mobile or stationary. An AT may be any data device that communicates through a wireless channel or through a wired channel, for example using fiber optic or coaxial cables. An AT may further be any of a number of types of devices including but not limited to PC card, compact flash, external or internal modem, or wireless or wireline phone. An AT that is in the process of establishing an active traffic channel connection with an AP is said to be in a connection setup state. An AT that has established an active traffic channel connection with an AP is called an active AT, and is said to be in a traffic state.","The term \u201ccommunication channel\/link\u201d is used exclusively herein to mean a single route over which a signal is transmitted described in terms of modulation characteristics and coding, or a single route within the protocol layers of either the AP or the AT.","The term \u201creverse channel\/link\u201d is used exclusively herein to mean a communication channel\/link through which the AT sends signals to the AP.","The term \u201cforward channel\/link\u201d is used exclusively herein to mean a communication channel\/link through which an AP sends signals to an AT.",{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 1","b":["2","4","6","6","8","10","10","4","8","6","8","10"]},"The acoustic processor  extracts features from an input speech signal and provides those features to word decoder . In general, the word decoder  translates the acoustic features received from the acoustic processor  into an estimate of the speaker's original word string. The estimate is created via acoustic pattern matching and language modeling. Language modeling may be omitted in certain situations, such as applications of isolated word recognition. The acoustic pattern matching element  detects and classifies possible acoustic patterns, such as phonemes, syllables, words, and so forth. The acoustic pattern matching element  provides candidate patterns to language modeling element , which models syntactic constraint rules to determine gramatically well formed and meaningful word sequences. Syntactic information can be employed in VR when acoustic information alone is ambiguous. The VR system  sequentially interprets acoustic feature matching results and provides the estimated word string based on language modeling.","Both the acoustic pattern matching and language modeling in the word decoder  employ deterministic or stochastic modeling to describe the speaker's phonological and acoustic-phonetic variations. VR system performance is related to the quality of pattern matching and language modeling. Two commonly used models for acoustic pattern matching known by those skilled in the art are template-based dynamic time warping (DTW) and stochastic hidden Markov modeling (HMM).","The acoustic processor  represents a front end speech analysis subsystem of the VR system . In response to an input speech signal, the acoustic processor  provides an appropriate representation to characterize the time varying speech signal. The acoustic processor  may discard irrelevant information such as background noise, channel distortion, speaker characteristics, and manner of speaking.","Combining multiple VR systems, or VR engines, provides enhanced accuracy and uses a greater amount of information from the input speech signal than a single VR system. One system for combining VR engines is described in U.S. patent application Ser. No. 09\/618,177, entitled \u201cCombined Engine System and Method for Voice Recognition,\u201d filed Jul. 18, 2000, and U.S. patent application Ser. No. 09\/657,760, entitled \u201cSystem and Method for Automatic Voice Recognition Using Mapping,\u201d filed Sep. 8, 2000, assigned to the assignee of the present application.","In one aspect of the present system, multiple VR engines may be combined into a distributed VR system. The multiple VR engines provide a VR engine at both a subscriber station and the network server. The VR engine on the subscriber station is called the local VR engine, while the VR engine on the network server is called the network VR engine. The local VR engine comprises a processor for executing the local VR engine and a memory for storing speech information. The network VR engine comprises a processor for executing the network VR engine and a memory for storing speech information.","One example of a distributed VR system is disclosed in U.S. patent application Ser. No. 09\/755,651, entitled \u201cSystem and Method for Improving Voice Recognition in a Distributed Voice Recognition System,\u201d filed Jan. 5, 2001, assigned to the assignee of the present invention.",{"@attributes":{"id":"p-0045","num":"0044"},"figref":["FIG. 2","FIG. 2"],"b":["40","42","22","40","48"]},"Speech is provided to microphone , which converts the speech signal into electrical signals and provided to feature extraction element . Signals from microphone  may be analog or digital. If analog, an A\/D converter (not shown in this view) may be interposed between microphone  and feature extraction element . Speech signals are provided to feature extraction element , which extracts relevant characteristics of the input speech used to decode the linguistic interpretation of the input speech. One example of characteristics used to estimate speech is the frequency characteristics of an input speech frame. Input speech frame characteristics are frequently employed as linear predictive coding parameters of the input speech frame. The extracted speech features are then provided to transmitter  which codes, modulates, and amplifies the extracted feature signal and provides the features through duplexer  to antenna , where the speech features are transmitted to base station . Various types of digital coding, modulation, and transmission schemes known in the art may be employed by the transmitter .","At base station , the transmitted features are received at antenna  and provided to receiver . Receiver  may perform the functions of demodulating and decoding received transmitted features, and receiver  provides these features to word decoder . Word decoder  determines a linguistic estimate of the speech from the speech features and provides an action signal to transmitter . Transmitter  amplifies, modulates, and codes the action signal, and provides the amplified signal to antenna . Antenna  transmits the estimated words or a command signal to subscriber station . Transmitter  may also employ digital coding, modulation, or transmission techniques known in the art.","At subscriber station , the estimated words or command signals are received at antenna , which provides the received signal through duplexer  to receiver  which demodulates and decodes the signal and provides command signal or estimated words to control element . In response to the received command signal or estimated words, control element  provides the intended response, such as dialing a phone number, providing information to a display screen on the portable phone, and so forth.","The information sent from base station  need not be an interpretation of the transmitted speech, but may instead be a response to the decoded message sent by the portable phone. For example, one may inquire about messages on a remote answering machine coupled via a communications network to base station , in which case the signal transmitted from the base station  to subscriber unit  may be the messages from the answering machine. A second control element for controlling the data, such as the answering machine messages, may also be located in the central communications center.","Transmission time between the subscriber station  and the base station  is partitioned into time units. In one aspect of the present system, the transmission time may be partitioned into frames. In another aspect, the transmission time may be partitioned into time slots. In accordance with one aspect, the system, specifically the subscriber station  or the base station , partitions data into data packets and transmits each data packet over one or more time units. At each time unit, the base station  can direct data transmission to any subscriber unit  within the cell that is in communication with the base station . In one aspect, frames may be further partitioned into a plurality of time slots. In yet another aspect, time slots may be further partitioned, such as into half-slots and quarter-slots.","Subscriber Station.  illustrates a subscriber station or terminal side VR system  in accordance with one aspect of the present invention. Speech is provided to a microphone (not shown), that converts speech into an analog electrical signal. The analog electrical signal is provided to an analog-to-digital converter (ADC) . The ADC  provides a digital signal at sampling rates of 8 kHz, 11 kHz, or 16 kHz. One of ordinary skill in the art understands that other sampling rates may be employed, but these rates generally provide good performance for most speech signals encountered. To remove direct current (DC) offset, the digital signal is provided to an offset compensator (OC) . In one aspect, the OC  comprises a high-pass filter that provides a signal free of DC offset, x, where:\n\n()=()\u2212(\u22121)+\u03b1(\u22121)\u2003\u2003(1)\n\nwhere: x(n) is a sample of digital signal at frame n, of \u03b1=1.0\u22120.05*(8000\/f), and fis the sampling frequency of the ADC 302, namely 8, 11, or 16 kHz in the aspect shown.\n","The OC 304 provides the offset-free signal xto a framing block . Framing block  divides the offset-free signal xinto multiple overlapping frames, each frame comprising N samples. The frame shift interval M is the number of samples between the starting points of consecutive frames. M determines number of frames per unit of time. For all sampling rates, the frame length may be 25 ms. In the case of 11 kHz sampling rate the signal is linearly interpolated to 16 kHz after framing. For the 8, 11, and 16 kHz sampling rates, the values of parameters N and Mare given in Table 1.",{"@attributes":{"id":"p-0053","num":"0052"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"56pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"63pt","align":"center"}}],"thead":{"row":[{"entry":[{},"TABLE 1"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]},{"entry":[{},"SamplingRate",{},{},{}]},{"entry":[{},"(kHz)","f = 16","f = 11","f = 8"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"56pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"63pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Frame Length N","400","400","200"]},{"entry":[{},"(samples)"]},{"entry":[{},"Shift Interval M","160","160","80"]},{"entry":[{},"(samples)"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]}]}}]}}},"In operation, for a sampling rate of 16 kHz, the received signal is divided into overlapping frames each containing 400 samples. A first frame comprises samples 0 through 399, while the second frame is offset by 160 samples, and thus comprises samples 160 through 559, and so forth. Each 400 sample frame is 25 ms long.","Windowing block  windows the data received from the framing block . In one aspect, windowing block  employs a Hamming window function according to the following equation:",{"@attributes":{"id":"p-0056","num":"0055"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["x","w"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},{"mrow":{"mrow":[{"mo":["{","}"],"mrow":{"mn":"0.54","mo":"-","mrow":{"mn":"0.46","mo":"\u00b7","mrow":{"mi":"cos","mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mrow":[{"mn":"2","mo":"\u2062","mrow":{"mi":"\u03c0","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"n","mo":"-","mn":"1"}}}},{"mi":"N","mo":"-","mn":"1"}]}}}}}},{"msub":{"mi":["x","of"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}}],"mo":["*",",","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},"mn":"1"},"mo":["\u2264","\u2264"],"mi":["n","N"]}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"2"}}]}}}},"br":{},"sub":"w","b":"308"},"The windowed frame is then provided to a fast Fourier transform (FFT) module , which computes a magnitude spectrum for each of the frames. First, a FFT is performed in accordance with the following equation:",{"@attributes":{"id":"p-0058","num":"0057"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":[{"mo":["|","|"],"msub":{"mi":["X","k"]}},{"mrow":[{"mo":["|","|"],"mrow":[{"munderover":{"mo":"\u2211","mrow":[{"mi":"n","mo":"=","mn":"0"},{"mi":"FFTL","mo":"-","mn":"1"}]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":{"msub":{"mi":["x","w"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},"mo":"\u2062","msup":{"mi":"\u2147","mrow":{"mrow":{"mo":"-","mi":"j"},"mo":["\u2062","\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"nk","mfrac":{"mrow":{"mn":"2","mo":"\u2062","mi":"\u03c0"},"mi":"FFTL"}}}}},{"mo":[",","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mi":"k"}]},{"mrow":{"mn":"0","mo":[",","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}],"mi":["\u2026","FFTL"],"mrow":{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mo":","}},"mo":"-","mn":"1."}],"mo":"="}],"mo":"="},"mo":"\u2062","mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}},{"mrow":{"mo":["(",")"],"mn":"3"}}]}}}},"br":{},"sub":["k","w ","k","w ","k"],"b":"310"},"Lower frequency power spectrum computation block  and higher frequency power spectrum computation block  compute a power spectrum, |X|, by taking the square of the magnitude spectrum |X|. In order to efficiently utilize the processing blocks, the first 129 values of the magnitude spectrum |X| for all sampling rates, representing a frequency range 0 to 4 kHz are provided to the lower frequency power spectrum computation block . If the subscriber station or terminal employs a sampling rate higher than 8 kHz, the high frequency information in the frequency range above 4 kHz is added as follows.","For the 11 kHz sampling rate, the magnitude values |X| through |X| comprise the information for the frequency range 4 kHz to 5.5 kHz. The magnitude values |X| through |X| are provided to the higher frequency power spectrum computation block  that calculates a single power value for this frequency range according to the following equation:",{"@attributes":{"id":"p-0061","num":"0060"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":"E","mn":"0"},"mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"129"},"mn":"176"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mo":["|","\u2062"],"msub":{"mi":["X","k"]},"msup":{"mo":"|","mn":"2"}}}}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}}}}},"For the 16 kHz sampling rate, the magnitude values |X| through |X| comprise the information for the frequency range 5.5 kHz to 8 kHz. The magnitude values |X| through |X| are provided to the higher frequency power spectrum computation block , and the higher frequency power spectrum computation block  calculates a power value for the frequency range 4 kHz to 5.5 kHz in accordance with Equation (3) and another power value for the frequency range 5.5 kHz to 8 kHz as follows:",{"@attributes":{"id":"p-0063","num":"0062"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":"E","mn":"1"},"mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"177"},"mn":"256"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mo":["|","\u2062"],"msub":{"mi":["X","k"]},"msup":{"mo":"|","mn":"2"}}}}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}}}}},"The Eand Eresults of equations 3 and 4 are scalar values that are concatenated onto the 129 point vector from the 0\u20134 kHz part of the signal. The size of the spectral vector provided to noise compensation block  is 129, 130, and 131 for sampling rates 8, 11, and 16 kHz.","Noise compensation block  performs a noise power spectrum estimate based on the signal received using a first order recursion scheme and applies noise reduction to the signal. For the noise power spectrum estimate, k is the frequency bin index, n is the frame number, |X(k,n)|is the power spectrum of the noisy speech, and |{circumflex over (N)}(k,n)|is the estimated power spectrum of the noise. When the frame index n is equal to zero, the noise power estimate |{circumflex over (N)}(k,n)|is:\n\n(|()|)=(1()|)\u2003\u2003(5)\n\nWhen n is greater than zero, the noise power estimate |{circumflex over (N)}(k,n)|is\n\n(|()|)=(|(\u22121) |)+\u03b1*((1()|)\u2212(|(\u22121)|))\u2003\u2003(6)\n\nwhere:\n",{"@attributes":{"id":"p-0066","num":"0065"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"\u03b1","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["k","n"],"mo":[",","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}}},{"mrow":{"mfrac":{"mn":"1","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":[",","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}}],"mi":["for","n"]},"mo":"<","mn":"15"}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"7"}}]}}}},"br":[{},{}],"in-line-formulae":[{},{},{},{}],"i":["k,n","k,n"],"sub":["n","n\u22121"]},{"@attributes":{"id":"p-0067","num":"0066"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":[{"mi":"where","mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mtext":":"},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"msub":{"mi":["FrameEnergy","n"]}},{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"0"},"mn":"128"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mo":["|","\u2062"],"mrow":{"mi":"X","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["k","n"],"mo":[",","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}}},"msup":{"mo":"|","mn":"2"}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"9"}}]},{"mtd":[{"mrow":{"mrow":[{"mi":"and","mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mtext":":"},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"msub":{"mi":["NoiseEnergy","n"]}},{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"0"},"mn":"128"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mo":["|","\u2062"],"mrow":{"mi":"N","mo":"\u2061","mrow":{"mo":["(",")"],"mover":{"mrow":{"mi":["k","n"],"mo":[",","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}},"mo":"\u22c0"}}},"msup":{"mo":"|","mn":"2"}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"10"}}]}]}}}},"Once noise compensation block  has estimated the noise power spectrum |{circumflex over (N)}(k,n)|, noise compensation block  filters the noise power spectrum in the power spectral domain using the following filter, which is a generalized form of Wiener filtering:",{"@attributes":{"id":"p-0069","num":"0068"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mo":["|","\u2062"],"mrow":{"msub":{"mi":["H","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["k","n"],"mo":[",","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}}},"msup":{"mo":"|","mn":"2"}},"mo":"=","msup":{"mrow":{"mi":"max","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mfrac":{"mrow":[{"mrow":{"mo":["|","\u2062","\u2062","|","\u2062"],"mrow":[{"mi":"X","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["k","n"],"mo":[",","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}}},{"mrow":{"mo":"-","mi":"\u03b3"},"mo":"*"},{"mi":"N","mo":"\u2061","mrow":{"mo":["(",")"],"mover":{"mrow":{"mi":["k","n"],"mo":[",","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}},"mo":"\u22c0"}}}],"msup":[{"mo":"|","mn":"2"},{"mo":"|","mn":"2"}]},"mo":"\u2062","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mo":["|","\u2062"],"mrow":{"mi":"X","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["k","n"],"mo":[",","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}}},"msup":{"mo":"|","mn":"2"}}]},"mo":[",","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},"mi":"\u03b2"}}},"mn":"2"}}},{"mrow":{"mo":["(",")"],"mn":"11"}}]}}}}},"In Equation (11), \u03b2 is a spectral floor parameter, used to avoid negative or very small transfer function components. In one embodiment, \u03b2=0.01. \u03b3 is a noise overestimation factor, enabling subtraction from the noisy speech spectrum of more than the estimated noise power spectrum |{circumflex over (N)}(k,n)|. The overestimation is determined in accordance with a posterior signal-to-noise ratio (Posterior SNR). Posterior SNR may be computed as follows:",{"@attributes":{"id":"p-0071","num":"0070"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["PosteriorSNR","n"]},"mo":"=","mrow":{"mn":["10","10"],"mo":["*","\u2062","\u2062","*"],"mi":"log","mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},"mrow":{"mo":["(",")"],"mfrac":{"msub":[{"mi":["FrameEnergy","n"]},{"mi":["NoiseEnergy","n"]}]}}}}},{"mrow":{"mo":["(",")"],"mn":"12"}}]}}}},"br":{}},{"@attributes":{"id":"p-0072","num":"0071"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"\u03b3","mo":"=","mrow":{"mrow":{"mfrac":{"mrow":{"mo":"-","mn":"1.875"},"mn":"20"},"mo":"*","msub":{"mi":["PosteriorSNR","n"]}},"mo":"+","mn":"3.125"}}},{"mrow":{"mo":["(",")"],"mn":"13"}}]}}}}},{"@attributes":{"id":"p-0073","num":"0072"},"figref":["FIG. 4","FIG. 4"]},"To reduce undesirable effects associated with power spectrum estimates, the noise compensation block  smoothes the raw filter transfer function |H(k,n)|in both the time and frequency domains. Smoothing in both time and frequency domains is required because no true estimates of the noise in the time and frequency bands are available at this point. In one aspect, time domain smoothing initially uses a first-order recursion of |H(k,n)|to produce smoothed filter |H(k,n)|according to the following equation:\n\n|()|=\u03b1(\u22121)|+(1\u03b1)*|H()(14)\n\nwhere \u03b1is a filter smoothing constant. In one aspect \u03b1.is 0.9. The final time-domain smoothed filter transfer function is obtained in one aspect as an average of the raw and the smoothed filter transfer function as follows:\n\n|()|()|()|()|(1()|\u2003\u2003(15)\n\nwhere lat is latency compensation. The initial smoothing given by Equation (14) introduced a group delay; consequently, the smoothed filter transfer function |H(k,n)|is not synchronized with the raw filter transfer function |H(k, n)|. To reestablish the synchronization, the latency compensation lat is used. In one aspect the lat is 2, indicating a two frame buffer is employed in association with Equation (15).\n","The time domain filtered transfer function is used for filtering the spectral content of the frequency range higher than 4 kHz. The final filter transfer function for filtering the spectral content of the frequency range 0 to 4 kHz is obtained by filtering the time domain filtered transfer function in the frequency domain with a sliding window rectangular filter as follows:",{"@attributes":{"id":"p-0076","num":"0075"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mo":["|","\u2062"],"mrow":{"mi":"H","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["k","n"],"mo":[",","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}}},"msup":{"mo":"|","mn":"2"}},{"mrow":[{"mfrac":{"mn":"1","mrow":{"mrow":{"mn":"2","mo":"*","mi":"L"},"mo":"+","mn":"1"}},"mo":"*","munderover":{"mo":"\u2211","mrow":{"mi":"l","mo":"=","mrow":{"mo":"-","mi":"L"}},"mi":"L"}},{"msub":{"mi":["H","s2"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["j","n"],"mo":[",","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}}}],"mo":["\u2062","|","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mo":"|","mn":"2"}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"16"}}]}}}},"br":{}},"|H(k,n)|is then applied to filter the noisy power spectra as follows:\n\n|()|(|()|()|, \u03b2()|)\u2003\u2003(17)\n\nwhere \u03b2is a noise floor, comprising a fraction of the estimated noise spectrum |{circumflex over (N)}(k,n)|. In one aspect \u03b2=0.001. The resultant vector, |\u0176(k,n)|, is an estimate of the clean speech power spectra. Hence noise compensation block  receives the 129, 130, or 131 bin power spectra |X(k,n)|, creates a noise power estimate |{circumflex over (N)}(k,n)|, and filters the noise from the power spectra |X(k,n)|to create a clean speech power spectra |\u0176(k,n)|.\n","The clean speech power spectra |\u0176(k,n)|is filtered using Mel filtering for the 0 to 4 kHz components or using Upper Band (UB) filtering for the 4 to 8 kHz components of the 11 and 16 kHz signals. Mel filtering block  uses a specific frequency range to compute a Mel-warped spectrum. In one aspect, Mel filtering block  uses the 64 to 4000 Hz frequency range to compute the Mel-warped spectrum. This range may be divided into 23 channels equidistant in the Mel frequency scale, with each channel having a triangular-shaped frequency window. In this arrangement, consecutive channels are half-overlapping. For x having a range from 64 Hz to 4000 Hz, the Mel-warped spectrum is:\n\n()=2595(1700)\u2003\u2003(18)\n","The center frequency fcof filter i and the FFT bin cbincorresponding to this center frequency are given by:",{"@attributes":{"id":"p-0080","num":"0079"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":[{"mi":"f","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["c","i"]}},{"mi":["M","e"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mrow":{"msup":{"mi":"l","mrow":{"mo":"-","mn":"1"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":["M","e"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mrow":{"mi":"l","mo":"\u2061","mrow":{"mo":["(",")"],"mn":"64"}}},{"mi":"i","mo":"*","mfrac":{"mrow":[{"mrow":[{"mi":["M","e"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mrow":{"mi":"l","mo":"\u2061","mrow":{"mo":["(",")"],"mn":"4000"}}},{"mi":["M","e"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mrow":{"mi":"l","mo":"\u2061","mrow":{"mo":["(",")"],"mn":"64"}}}],"mo":"-"},{"mn":["23","1"],"mo":"+"}]}}],"mo":"+"}}}}],"mo":"="},{"mi":"i","mo":"=","mn":"1"}],"mo":[",",",","\u2062","\u2062",","],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}],"mi":"\u2026","mn":"23"}},{"mrow":{"mo":["(",")"],"mn":"19"}}]}}}},"br":{},"in-line-formulae":[{},{}],"i":["cbin","fc"],"sub":["i","i"]},"The output of the Mel filter is the weighted sum of the power spectrum values (|\u0176(k,n)|) in each band. Mel filtering block  uses triangular, half-overlapped windowing based on the received clean speech power spectra |\u0176(k,n)|according to the following equation:",{"@attributes":{"id":"p-0082","num":"0081"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"mi":["f","b","a","n"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msub":{"mi":["k","i"]}},{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","msub":{"mi":"cbin","mrow":{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mo":"\u2062","mrow":{"mi":"i","mo":"-","mn":"1"}}}},"msub":{"mi":"cbin","mrow":{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mo":"\u2062","mi":"i"}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mfrac":{"mrow":[{"mi":"k","mo":"-","msub":{"mi":"cbin","mrow":{"mi":"i","mo":"-","mn":"1"}}},{"msub":[{"mi":["cbin","i"]},{"mi":"cbin","mrow":{"mi":"i","mo":"-","mn":"1"}}],"mo":"-"}]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mover":{"mi":"Y","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"k"}}},"mn":"2"}}},"mo":"+"}],"mo":["=","\u2062"],"mi":{}}}},{"mtd":{"mrow":{"mi":{},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mrow":{"msub":{"mi":"cbin","mrow":{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mo":"\u2062","mi":"i"}},"mo":"+","mn":"1"}},"msub":{"mi":"cbin","mrow":{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mo":"\u2062","mrow":{"mi":"i","mo":"+","mn":"1"}}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mfrac":{"mrow":[{"msub":{"mi":"cbin","mrow":{"mi":"i","mo":"+","mn":"1"}},"mo":"-","mi":"k"},{"msub":[{"mi":"cbin","mrow":{"mi":"i","mo":"+","mn":"1"}},{"mi":["cbin","i"]}],"mo":"-"}]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mover":{"mi":"Y","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"k"}}},"mn":"2"}}}}}}]}},{"mrow":{"mo":["(",")"],"mn":"21"}}]}}}},"br":[{},{},{}],"sub":["0 ","24 ","0 ","24 ","0","24"],"in-line-formulae":[{},{},{},{}],"i":"cbin"},"For the 11 kHz and 16 kHz sampling rates, Mel filtering block  only uses the 64 Hz to 4000 Hz range for Mel filtering. Use of this limited range facilitates using subsequent storage intensive components of the front-end algorithm designed for the 8 kHz sampling rate, such as VAD and Vector Quantization table. Further, the Mel filtering block's use of the 64 Hz to 4000 Hz range allows using server post-processing techniques designed for the 11 kHz and 16 kHz sampling rate.","Upper band frequency block  selects the two power values Eand Efrom the clean speech power spectra |\u0176(k,n)for further processing. Lower frequency natural logarithm block  applies a natural logarithm function to the outputs fbankof the Mel filterbank, while higher frequency natural logarithm block  applies a natural logarithm function to the power values Eand E. For lower frequency natural logarithm block ,\n\n=ln(),i=1, . . . , 23\u2003\u2003(24)\n\nFor higher frequency natural logarithm block ,\n\n=ln(),i=0,1\u2003\u2003(25)\n\nwhere Ecleanare the upper band energies Eand Eafter noise compensation.\n","Time trajectories for each of the 23 channels, as well as the two upper band energies are filtered using RASTA-LDA filters  and . The RASTA-LDA filters  and  may be FIR filters where filter coefficients are derived using the linear discriminant analysis (LDA) technique on a phonetically labeled database. One such phonetically labeled database that may be employed is the phonetically labeled OGI-Stories database known in the art. The OGI-Stories database may be augmented with certain acoustic aspects, such as automobile noise or other commonly encountered noises. Automobile noise at, for example, 10 dB SNR may be artificially added to the OGI-Stories database for the RASTA-LDA filters  and . Noisy OGI-Stories files are cleaned using the noise compensation technique outlined above for noise compensation block . RASTA-LDA filters  and  compute twenty three logarithmic Mel filter-bank energies from the cleaned speech files every 10 ms using a 25 ms speech segment. RASTA-LDA filters  and  use a 101-point feature vector from the seventh Mel band. Each 101-point feature vector contains 50 points in the past and 50 points in the future and is labeled by the phone label of the center point of the vector. Each of the forty one phones is represented uniformly by three states for a total of 123 states and each state is used as a class. The leading discriminant vector from the seventh band is used as a temporal RASTA filter. One example of a RASTA filter that may be used in this instance in accordance with RASTA-LDA filters  and  is described in U.S. Pat. No. 5,450,522 entitled, \u201cAuditory Model for Parameterization of Speech\u201d filed Sep. 12, 1995. The filter is truncated to a 51 point causal filter to reduce latency. Finally, the filter is convolved with a low-pass filter with a cut-off modulation frequency of 25 Hz and further truncated to 30 points. The resultant LDA filter used for both RASTA-LDA filter  and  has a 20 ms delay at 5 Hz modulation frequency and its response is illustrated in . The resultant filtered two element vector for higher frequencies from RASTA-LDA filter  is e, while the resultant filtered 23 element vector for lower frequencies from RASTA-LDA filter  is f.","DCT block  computes 15 cepstral coefficients from the RASTA-LDA filtered log energies of the 23 channels transmitted from LDA filter . Cepstral coefficients care computed according to the following equation:",{"@attributes":{"id":"p-0087","num":"0086"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["c","i"]},"mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mn":"23"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":["f","j"]},"mo":"*","mrow":{"mi":"cos","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mfrac":{"mrow":{"mi":["\u03c0","i"],"mo":"*"},"mn":"23"},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mi":"j","mo":"-","mn":"0.5"}}}}}}}},{"mn":["0","14"],"mo":["\u2264","\u2264"],"mi":"i"}],"mo":","}},{"mrow":{"mo":["(",")"],"mn":"26"}}]}}}}},"The VAD  is a single hidden layer feed forward multiplayer perceptron, or neural network. The VAD  discriminates between speech and nonspeech frames. The VAD  performs training offline using a backpropagation algorithm and a noisy database. Further information regarding backpropagation algorithms may be found in Bourlard and Morgan, \u201cConnectionist Speech Recognition,\u201d Kluwer Academic Publishers (1994), at Chapter 4. The VAD  may use six cepstral coefficients computed from low-pass filtered log-energies of the 23 Mel filters. The low-pass filter employed here by the VAD  used here is:\n\n()=0.5*c()+0.5*c\u2032(1)\u2003\u2003(27)\n\nwhere c\u2032is the output of the low-pass filter and cthe input to the low-pass filter. The VAD  is a neural network that may employ 54 input units, 50 hidden units and one output. The inputs to the neural network may include nine frames of the six cepstral coefficients. The VAD  may be trained using two outputs, one for the speech class and one for the silence class. The output of the trained VAD  can provide an estimate of the posterior probability of the current frame being speech or silence. During testing only the output corresponding to the silence class is used. VAD  computes silence and nonsilence coefficients y(sil) and y(nosil) as follows:\n",{"@attributes":{"id":"p-0089","num":"0088"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["y","n"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["s","i","l"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"0"},"mn":"49"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msubsup":{"mi":"w","mrow":{"mi":"k","mo":",","mrow":{"mi":["s","il"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},"mn":"2"},"mo":"\u2062","mrow":{"mi":"sigm","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"0"},"mn":"5"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mrow":{"mo":"-","mn":"4"}},"mn":"4"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msubsup":{"mi":"w","mrow":{"mi":["i","j","k"],"mo":[",",","]},"mn":"1"},"mo":"\u2062","mrow":{"msub":{"mi":["c","j"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"n","mo":"+","mn":"1"}}}}}}}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"28"}}]},{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["y","n"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["nos","i","l"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"0"},"mn":"49"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msubsup":{"mi":"w","mrow":{"mi":"k","mo":",","mrow":{"mi":["nos","il"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},"mn":"2"},"mo":"\u2062","mrow":{"mi":"sigm","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"0"},"mn":"5"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mrow":{"mo":"-","mn":"4"}},"mn":"4"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msubsup":{"mi":"w","mrow":{"mi":["i","j","k"],"mo":[",",","]},"mn":"1"},"mo":"\u2062","mrow":{"msub":{"mi":["c","j"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"n","mo":"+","mn":"1"}}}}}}}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"29"}}]}]}}},"br":{},"sub":"j","sup":"th "},{"@attributes":{"id":"p-0090","num":"0089"},"maths":{"@attributes":{"id":"MATH-US-00015","num":"00015"},"math":{"@attributes":{"overflow":"scroll"},"msubsup":{"mi":"w","mrow":{"mi":["i","j","k"],"mo":[",",","]},"mn":"1"}}},"br":{}},{"@attributes":{"id":"p-0091","num":"0090"},"maths":{"@attributes":{"id":"MATH-US-00016","num":"00016"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msubsup":[{"mi":"w","mrow":{"mi":"k","mo":",","mrow":{"mi":["s","il"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},"mn":"2"},{"mi":"w","mrow":{"mi":"k","mo":",","mrow":{"mi":["nos","il"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},"mn":"2"}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"and"}}},"br":{}},{"@attributes":{"id":"p-0092","num":"0091"},"maths":{"@attributes":{"id":"MATH-US-00017","num":"00017"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"sigm","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}},"mo":"=","mfrac":{"mn":"1","mrow":{"mn":"1","mo":"+","msup":{"mi":"\u2147","mrow":{"mo":"-","mi":"x"}}}}}},{"mrow":{"mo":["(",")"],"mn":"30"}}]}}}}},"The probability that the frame n is silence is given by:",{"@attributes":{"id":"p-0094","num":"0093"},"maths":{"@attributes":{"id":"MATH-US-00018","num":"00018"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msub":{"mi":["p","n"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["s","i","l"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}}},"mo":"=","mfrac":{"msup":{"mi":"\u2147","mrow":{"msub":{"mi":["y","n"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["s","i","l"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}}}},"mrow":{"msup":[{"mi":"\u2147","mrow":{"msub":{"mi":["y","n"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["s","i","l"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}}}},{"mi":"\u2147","mrow":{"msub":{"mi":["y","n"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["nos","i","l"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}}}}],"mo":"+"}}}},{"mrow":{"mo":["(",")"],"mn":"31"}}]}}}},"br":{},"sub":"n"},"Downsampler  combines the single probability of silence, p(Sil) from VAD  with the 15 cepstral coefficients from DCT block  and the two upper band values from RASTA-LDA filter . Downsampler  downsamples the cepstral coefficients, the upper band values, and the probability of silence by a factor of two in time, i.e. every alternate frame in time. The final feature vector transmitted by the downsampler  thus includes 18 parameters, including the 15 cepstral coefficients, the silence probability, and the two upper band energy based features. In the case of the 8 kHz sampling rate, the two energy based features are set to zero and ignored during server processing, described in detail below. For the 11 kHz sampling rate, the second energy based feature is set to zero.","Feature compressor  receives the 18 element vector and compresses the vector using a split Vector Quantization (VQ) algorithm. VQ uses a Linde-Buzo-Gray algorithm, or LBG algorithm, known in the art to train the codebook. The feature compressor  initializes the codebook with the mean value of all training data. At each training step, the feature compressor  may divide each centroid into two parts and may re-estimate the centroid values. The feature compressor  performs splitting in the positive and negative direction of standard deviation divided by 0.2. The form of the 17 element codebook employed is shown in Table 2.",{"@attributes":{"id":"p-0097","num":"0096"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 2"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Split Vector Quantization Feature Pairings and Bit Allocation"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"49pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Codebook","No. of bits","Size","Element 1","Element 2"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"49pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Q","7","128","P(sil)","\u2014"]},{"entry":["Q","7","128","C","\u2014"]},{"entry":["Q","7","128","C","\u2014"]},{"entry":["Q","6","64","C","\u2014"]},{"entry":["Q","5","32","C","\u2014"]},{"entry":["Q","5","32","C","\u2014"]},{"entry":["Q","5","64","C","C"]},{"entry":["Q","5","64","C","C"]},{"entry":["Q","5","64","C","C"]},{"entry":["Q","5","64","C","C"]},{"entry":["Q","5","64","C","C"]},{"entry":["Q","7","128","E","\u2014"]},{"entry":["Q","7","128","E","\u2014"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}}]}}},"From Table 2, Element 1 and Element 2 represent the values received from the downsampler . Codebook element has 128 levels. If the value P(sil) received is 0.5, for example, the feature compressor  computes the closest entry of the 128 entries to that value, for example entry 17. The binary equivalent of this entry 17 is transmitted by the feature compressor , using the seven bits available, such as 0010001 for 17. The feature compressor  finds the closest VQ centroid using the Euclidean distance, with the weight matrix set to the identity matrix. 76 bits describe one frame after packing the aforementioned indices.","The 76 bits are packed and transmitted from the subscriber station to the network server according to a standard frame structure. One common example of frame structure is defined in ETSI ES 201 108 v1.1.2, \u201cDistributed Speech Recognition; Front-end Feature Extraction Algorithm; Compression Algorithm\u201d, April 2000 (\u201cthe ETSI document\u201d). The ETSI document discusses the multiframe format, the synchronization sequence, and the header field.","Previous systems used cyclic redundancy code to detect errors in a pair of adjacent frames of transmitted features. Such systems also compared the two sets of received vectors to determine whether the recieved frames are in error. One common example of error detection is defined in ETSI ES 201 108 v1.1.2, \u201cDistributed Speech Recognition; Front-end Feature Extraction Algorithm; Compression Algorithm\u201d, April 2000 (\u201cthe ETSI document\u201d). The ETSI document discusses the error detection using CRC and a heuristic algorithm to compare the parameters of recived pair of vectors. Such an implementation prohibits the use of CRC for both error detection and correction because a large number of bits are required for error correction, making the bandwidth requirement large. Downsampler  allows the packing of information corresponding to two frames of features into the available bits with more bits for error detection and correction. The present design employs a cyclic redundancy check mechanism in the bitstream formatter, framer, and error protector (BFFEP) 338 to enable single frame transmission and checking of received information.","Each of the foregoing 18 codebook elements form a frame, and each 20 ms frame from the subscriber station is transmitted using the 18 codebook elements. The indices for a single frame are formatted according to . From , five bit vector element Qis transmitted first, followed by five bit vector element Q, and so forth, through seven bit vector element Q. The 76 bits of data shown in are protected with 16 bits of cyclic forward error correction code using the following equation:\n\n()=1.\u2003\u2003(32)\n",{"@attributes":{"id":"p-0102","num":"0101"},"figref":"FIG. 12"},"This 16 bit CRC together with the aforementioned indices of form a 92 point vector that is split into 11.5 octets, or 11.5 eight element packets. Twelve 11.5 octet packets are combined to form a 138 octet stream having the format shown in . The BFFEP  combines the feature stream with the overhead, specifically the synchronization sequence and the header. All trailing frames within a final multiframe that contain no valid speech data, such as the end of a conversation, are set to zero and transmitted by the BFFEP . The resulting format may be transmitted by the BFFEP at a data rate of 4800 bits per second.","Network Server. The bitstream is received by the network server and decoded to generate speech vectors. One aspect of a network server design operating at 8 kHz that may be employed is presented in .  illustrates a network server system according to the present design operating at 16 kHz. Bitstream decoding at the network server in the 8 kHz case is performed by the decoder  and includes synchronization sequence detection, header decoding and feature decompression. An example of bitstream decoding, synchronization sequence detection, header decoding, and feature decompression may be found in the ETSI document.","From , three streams of features are generated at the network server from the decompressed packet received from the subscriber station. The first stream includes cepstral features, which are upsampled, mean and variance normalized and augmented with first and second delta (derivative) coefficients. The result of cepstral feature processing is 45 features. The second stream includes cepstral features computed from 0\u20134 kHz frequency range that are transformed at the network server to estimates of phoneme posterior probabilities using a multi layer perceptron. The network server retains nine features after the dimensionality reduction. The final stream includes the log Mel filter bank energies obtained by an Inverse DCT, which are nonlinearly transformed to estimates of posterior probabilities of manner of articulation (five classes plus one silence class) using a multiband temporal patterns (TRAPs) architecture. Six features are generated using this stream. The final feature vector sent to the back-end recognizer is a concatenation of these three streams. The dimensionality of the final feature vector is 60. Use of these three streams provides information complementary to the cepstral features exclusively employed in previous systems. In particular, the second and third streams, phoneme posterior probabilities, also known as the nonlinear transformation of the cepstral stream, and multiband temporal patterns, can correct some of the errors made by cepstral features, and the cepstral features can correct some of the errors made by the other two streams. The phoneme posterior probabilities and multiband temporal patterns incorporate information from longer time spans than the cepstral features and thus provide enhanced information at the network server.","The following description is directed primarily to the 8 kHz case of , as many of the elements in  are similar. Those features in  that differ from  are called out as appropriate.","From , the network server employs error mitigation to reduce transmission errors. Decoder  performs error mitigation using Fire code. Fire code is a known error correction strategy that appends a small number of bits to a block of data during encoding so that a single small burst of errors in each data block can be corrected at the receiver. In the present design, the Fire code is employed at the network server, specifically at decoder , to repair error bursts of up to five bits and to detect six bits long error bursts.","Decoder  computes a second polynomial f(X) from the polynomial g(X) for purposes of error correction:\n\n()=+\u2003\u2003(33)\n\n()=1\u2003\u2003(34)\n\ng(x) is the same as on the subscriber station side. F(x) is computed according to the design shown in , and further detail may be found in Peterson et al., Error-Correcting Codes.\u201d The received bitstream consists of 76 bits of data and 16 bits of received CRC, or CRC. The decoder  may perform error detection and correction on the received bitstream in two steps.\n","The decoder  computes the CRC again from the received bitstream based on both data and CRC. The CRC computed by the detector  is CRC. The detector  divides the received bitstream by polynomial g(X) when the highest significant bit (HSB) of CRCis 1. The detector divides the received bitstream by polynomial f(X) when the last received bit of the incoming bitstream is 1.","If CRCis zero, then there are no errors in the frame packet. If CRCis nonzero, a transmission error has occurred. Detector  then uses the computed CRCfor error detection and correction. Only polynomial g(X) is used in this step. Detector  uses g(X) to cyclically divide CRC92 times (the length of the frame packet). At any point during the division, if the highest 11 bit positions of CRCare zeros, last five positions give the error burst. The position of the error is given by the division count, such as if the detector  has divided CRC. 52 times, and the highest 11 bit positions are zero, the error is in bit position 52. Detector  adds the five bits dictating the error burst modulo 2 with the corresponding part of the packet to correct the error. If the highest 11 bit positions are nonzero even after 92 divisions, the detector  is unable to correct the errors and the packet is marked as bad.","If the detector  finds 2B consecutive bad frames, detector  may replace the first B speech vectors by a copy of the last good speech vector before the error. The last B speech vectors may be replaced by a copy of the first good speech vector received after the error. In the case of an odd number of bad frames, one more may be copied from the past than from the future.","Detector  provides the decompressed packet to upsampler  and lower frequency upsampler . Upsampler  upsamples the probability p(sil) to 100 Hz by linearly interpolating the probability across adjacent values. Threshold application block  applies a threshold of 0.5 to the upsampled probability p(sil) to convert the upsampled probability p(sil) to a binary feature called the silence bit:",{"@attributes":{"id":"p-0113","num":"0112"},"maths":{"@attributes":{"id":"MATH-US-00019","num":"00019"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"mrow":{"mi":"SilenceBit","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},"mo":"=","mn":"0"},"mo":","}},{"mi":"if"},{"mrow":{"mrow":{"msub":{"mi":["p","n"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["s","i","l"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}}},"mo":">","mn":"0.5"}}]},{"mtd":[{"mrow":{"mstyle":{"mspace":{"@attributes":{"width":"7.8em","height":"7.8ex"}}},"mo":"\u2062","mrow":{"mrow":{"mo":"=","mn":"1"},"mo":","}}},{"mi":"if"},{"mrow":{"mrow":{"msub":{"mi":["p","n"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["s","i","l"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}}},"mo":"<","mn":"0.5"}}]}]}},{"mrow":{"mo":["(",")"],"mn":"35"}}]}}}}},"Lower frequency upsampler  upsamples the features by two using linear interpolation. The cepstral coefficients and energy features are then filtered using LPF block  using a low-pass filter with a cutoff frequency of 25 Hz.",{"@attributes":{"id":"p-0115","num":"0114"},"maths":{"@attributes":{"id":"MATH-US-00020","num":"00020"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"mi":"out","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},"mrow":{"mo":["(",")"],"mi":"n"}},{"mrow":[{"mn":"0.08895249559651","mo":["\u2062","*","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mi":"in","mrow":{"mo":["(",")"],"mi":"n"}},{"mn":"0.27489720439596","mo":["\u2062","*"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"in","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"n","mo":"-","mn":"1"}}}}],"mo":["+","+"]}],"mo":["=","\u2062"],"mi":{}}}},{"mtd":{"mrow":{"mi":{},"mo":"\u2062","mrow":{"mrow":[{"mn":"0.45087105994837","mo":["\u2062","*"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"in","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"n","mo":"-","mn":"2"}}}},{"mn":"0.45087105994837","mo":["\u2062","*","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mi":"in","mrow":{"mo":["(",")"],"mrow":{"mi":"n","mo":"-","mn":"3"}}}],"mo":["+","+"]}}}},{"mtd":{"mrow":{"mi":{},"mo":"\u2062","mrow":{"mrow":[{"mn":"0.27489720439596","mo":["\u2062","*"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"in","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"n","mo":"-","mn":"4"}}}},{"mn":"0.08895249559651","mo":["\u2062","*","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mi":"in","mrow":{"mo":["(",")"],"mrow":{"mi":"n","mo":"-","mn":"5"}}}],"mo":["+","+"]}}}},{"mtd":{"mrow":{"mi":{},"mo":"\u2062","mrow":{"mrow":[{"mn":"0.20466646013718","mo":["\u2062","*"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"out","mo":["\u2062","(",")"],"mrow":[{"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mo":"\u2062"},{"mi":"n","mo":"-","mn":"1"}]}},{"mn":"0.75433478131523","mo":["\u2062","*","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}],"mi":"out","mrow":{"mo":["(",")"],"mrow":{"mi":"n","mo":"-","mn":"2"}}}],"mo":["-","+"]}}}},{"mtd":{"mrow":{"mi":{},"mo":"\u2062","mrow":{"mrow":[{"mn":"0.01516974960443","mo":["\u2062","*","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}],"mi":"out","mrow":{"mo":["(",")"],"mrow":{"mi":"n","mo":"-","mn":"3"}}},{"mn":"0.09118094189891","mo":["\u2062","*"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"out","mo":["\u2062","(",")"],"mrow":[{"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mo":"\u2062"},{"mi":"n","mo":"-","mn":"4"}]}}],"mo":["-","-"]}}}},{"mtd":{"mrow":{"mi":{},"mo":"\u2062","mrow":{"mn":"0.00376200640914","mo":["\u2062","*"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"out","mo":["\u2062","(",")"],"mrow":[{"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mo":"\u2062"},{"mi":"n","mo":"-","mn":"4"}]}}}}}]}},{"mrow":{"mo":["(",")"],"mn":"36"}}]}}}}},"From , for sampling rates of 11 and 16 kHz, the 15 cepstral coefficients are linearly transformed to 23 point Mel scale spectrum using Inverse Discrete Cosine Transform (IDCT) block . Inverse Discrete Cosine Transform (IDCT) block  pads the cepstral coefficients with zeros before the transformation. The two energy features from the 4\u20138 kHz frequency region received from upper band LPF block  are appended to the 23 point Mel spectrum. DCT block  applies a 25 point DCT to the 25 element vector to decorrelate the 25 element vector. DCT block  retains 15 dimensions from the transformation.","OLN block  applies online mean and variance normalization to the LPF filtered cepstral coefficients. In the case of the 8 kHz sampling frequency of , these are the cepstral coefficients representing the 0\u20134 kHz range. For the 11 and 16 kHz sampling frequencies of , these cepstral coefficients also represent the full frequency range and are obtained using IDCT block . Cepstral coefficients are denoted c(n) whether received by OLN block  or OLN block , even though transformation has been applied to the high frequency components in . OLN block  updates the estimates of the local mean and variance for each frame marked as speech by the binary silence bit VAD feature. OLN block  adds a bias 0 to the variance estimates before normalizing the features. The bias \u03b8 tends to eliminate the effects of small noisy estimates of the variance in the long silence regions.","If SilenceBit(n) is equal to zero,\n\n()=(1)\u2212\u03b1*((1)\u2212())\n\n\u03c3()=\u03c3(1)\u2212\u03b1*(\u03c3(1)\u2212(()\u2212()))\u2003\u2003(37)\n","If SilenceBit(n) is not equal to zero,\n\n()=(1)\n\n\u03c3()=\u03c3(1)\u2003\u2003(38)\n",{"@attributes":{"id":"p-0120","num":"0119"},"maths":{"@attributes":{"id":"MATH-US-00021","num":"00021"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msubsup":{"mi":["c","i","\u2032"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},"mo":"=","mfrac":{"mrow":[{"mrow":[{"msub":{"mi":["c","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},{"msub":{"mi":["m","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}}],"mo":"-"},{"mrow":{"msub":{"mi":["\u03c3","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},"mo":"+","mi":"\u03b8"}]}}},{"mrow":{"mo":["(",")"],"mn":"39"}}]}}}}},"where c(n) is the icepstral coefficient at frame n, m(n) and \u03c3(n)are the mean and the variance of the icepstral coefficient estimated at frame n, and c\u2032(n) is the normalized cepstral coefficient at frame n. In one aspect, the value of \u03b1 is 0.01 and the bias \u03c3 is 1.0.","The means mand variances \u03c3may be initialized using global means and variances estimated on a noisy speech database. One aspect of the global means and variances are as follows:",{"@attributes":{"id":"p-0123","num":"0122"},"maths":{"@attributes":{"id":"MATH-US-00022","num":"00022"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":{"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"mi":"m","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mo":"-","mn":"1"}}},{"mo":"[","mrow":{"mn":["23.815484","1.037240"],"mo":["\u2062",",","\u2062",",","\u2062",",","\u2062",",","\u2062",","],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mrow":[{"mo":"-","mn":"0.382422"},{"mo":"-","mn":"0.203596"},{"mo":"-","mn":"0.557713"}]}}],"mo":["=","\u2062"],"mi":{}}}},{"mtd":{"mrow":{"mi":{},"mo":"\u2062","mrow":{"mrow":[{"mo":"-","mn":"0.051042"},{"mo":"-","mn":"0.208684"},{"mo":"-","mn":"0.100447"}],"mo":["\u2062",",","\u2062",",","\u2062",",","\u2062",",","\u2062",","],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mn":["0.073762","0.007481"]}}}},{"mtd":{"mrow":{"mi":{},"mo":["\u2062","]"],"mrow":{"mrow":[{"mo":"-","mn":"0.062511"},{"mo":"-","mn":"0.001553"},{"mo":"-","mn":"0.123591"},{"mo":"-","mn":"0.006837"},{"mo":"-","mn":"140246"}],"mo":["\u2062",",","\u2062",",","\u2062",",","\u2062",","],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}}}}]}}},{"mtd":{"mtable":{"mtr":[{"mtd":{"mrow":{"msup":{"mrow":{"mi":"\u03c3","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mo":"-","mn":"1"}}},"mn":"2"},"mo":["=","\u2062"],"mi":{},"mrow":{"mo":"[","mrow":{"mn":["5.957326","1.350153","0.992368","0.685526","0.834512"],"mo":["\u2062",",","\u2062",",","\u2062",",","\u2062",",","\u2062",","],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}}}}},{"mtd":{"mrow":{"mi":{},"mo":"\u2062","mrow":{"mn":["0.545422","0.478728","0.476498","0.422000","0.417962"],"mo":["\u2062",",","\u2062",",","\u2062",",","\u2062",",","\u2062",","],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}}}},{"mtd":{"mrow":{"mi":{},"mo":["\u2062","]"],"mrow":{"mn":["0.351819","0.361830","0.323899","0.322991","0.287901"],"mo":["\u2062",",","\u2062",",","\u2062",",","\u2062",","],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}}}}]}}}]}}}},"Delta block  derives dynamic features from the normalized static features c(n). The derivatives are calculated according to the following regression equation:\n\n\u0394()=(1.0\/28)*(\u22123.0(3)\u22122.0(2) \u22121.0(1)+1.0(1)+2.0(2)+3.0(3))\u2003\u2003(40)\n\nDelta block  also computes the second order derivatives according to the following equation:\n\n\u0394\u0394()=1.0(n\u22123)\u22120.6(1) \u22120.8()\u22120.6(1) +1.0(3)\u2003\u2003(41)\n","The delta block  then appends the first derivative and second derivative to the original 15-dimensional features, yielding a 45-dimensional feature vector. In calculating both the first and second derivatives, delta block  may employ a window of a predetermined number of frames. In one aspect, the delta block uses a seven frame window to calculate first and second derivatives, providing a latency of 30 ms coming out of the delta block .","From the delta block , the 45-dimensional feature vector passes to a downsampling block , which downsamples the 45-dimensional feature vector element by two in time. Downsampling reduces the complexity of the multi layer perceptron (MLP) , or neural net, without significant degradation in performance. The frame context window  stacks five consecutive 45-dimensional feature vector frames together to yield a 225 dimensional input vector to the MLP , including a present frame, two previous frames, and two subsequent frames. Stacking creates a delay of 40 ms. Normalization block  then normalizes this 225 dimensional input vector by subtracting and dividing the global mean and the standard deviation calculated on features from a training corpus. The global mean is subtracted from the input vector and the result is divided by the standard deviation. The MLP  has two layers excluding the input layer; the hidden layer consists of 500 units equipped with sigmoid activation function, while the output layer consists of 56 output units with softmax activation function. The MLP  is trained on phonetic targets from labeled TIMIT database, a database commonly available and known to those skilled in the art, with added noise. One aspect of the phonetic target that may be employed is 56 monophones of English. During recognition at the MLP , the softmax function in the output units is not employed, and so the output of this block corresponds to \u201clinear outputs\u201d of the MLP hidden layer. The MLP  is typically trained using features extracted from speech sampled at 8 kHz. The MLP  only uses the cepstral features extracted from 0\u20134 kHz frequency range for all sampling rates.","Because the subsequent Hidden Markov Models (HMM) used for decoding the linguistic information in the features use diagonal covariance matrices for the gaussian mixtures, the system applies Principal Component Analysis (PCA) to the 56-dimensional \u201clinear output\u201d of the MLP  at PCA block . The PCA block  computes eigenvectors of the total covariance matrix (56\u00d756) of the features from the same database (TIMIT with additive noise) used for training the MLP . The PCA block  retains the first nine eigenvectors corresponding to the nine largest eigenvalues. The within the PCA block  is a 56 by 9 transformation matrix. The PCA block  then projects these features onto the nine eigenvectors by multiplying a 1 by 56 vector with a 56 by 9 matrix. The nine resultant features are transmitted by the PCA block . A further explanation of the PCA process used in PCA block  and the remaining PCA blocks employed herein may be found in \u201cPrincipal Component Analysis,\u201d by I. T. Jolliffe, Springer-Verlag, New York, 1986.","For the high frequency case of , the blocks \u2013 from  are included as blocks \u2013. The online normalization block  and delta block  described above are incorporated as higher frequency OLN block  and higher frequency delta block  to employ the 15 element vector from LPF block  and perform the normalization and delta processes described above on the 15 element vector.","The third branch receives data from LPF block , which is a 15 feature vector, and performs an inverse DCT on the 15 feature vector in IDCT block . IDCT block  reconstructs a 15 dimensional mel spectrum from the 15 cepstral values received at the network server. IDCT block  performs reconstruction using an inverse DCT transformation, without using the two upper band energies. The output of IDCT block  is 15 bands.","In the 11 kHz and 16 kHz cases of , the reconstruction takes place based on LPF block  outputs rather than based on other points in the arrangement shown in . Temporal pattern features are based on multi-band and multi-stream neural network processing. For each mel-band, Band classifier MLP  trains a feed-forward Multi Layer Perceptron (MLP) to classify speech frames into target classes, called band classifiers. Band classifier MLP  has one hidden layer with a sigmoid activation function and are trained using a softmax activation function at their outputs. The softmax activation function is known to those of skill in the art. The input to Band classifier MLP  is a temporal trajectory of reconstructed mel-band energies. Each temporal trajectory covers a context of 50 frames in the past and 9 frames in the future. The temporal trajectories are mean subtracted, variance normalized and a Hamming window is applied at MVN block . The mean and variance are re-estimated for each frame at MVN block  using the 60 frames-long temporal trajectory. The number of hidden units is kept at 101 for Band classifier MLP . The output units of Band classifier MLP  includes manner-based articulatory-acoustic categories (Vowel, Flap, Stop, Fricative, Nasal) as well as the silence. The linear outputs from Band classifier MLP  (6 outputs before the final softmax non-linearity) are concatenated and used as input to Merging classifier MLP . Merging classifier  is trained to classify the same six manner based articulatory targets and has 200 hidden units. The Band classifier  and Merging classifier  are trained using a noisy speech database, such as TIMIT with artificially added noise. Band classifier  and Merging classifier  are trained using the phonetic labeling of the database and a canonical mapping between phoneme classes and manner-based classes. During recognition, the Band classifier  and Merging classifier  do not use the softmax function in the output units, and so the output of Band classifier  corresponds to \u201clinear outputs\u201d of the Merging classifier  hidden layer. PCA block  applies principal component analysis (6 by 6) to decorrelate the outputs. PCA block  computes eigenvectors from the total covariance matrix as in the second branch above using the noisy TIMIT database. The PCA retains all the six dimensions, thereby obtaining a 6 by 6 transformation matrix.","The six features from PCA block  are appended to the forty-five features from delta block  and nine features from PCA block  to yield a sixty dimensional feature vector. Finally, PCA block  applies principal component analysis to the sixty dimensional feature vector to decorrelate the features. PCA block  is computed using the noisy TIMIT database. The principal component analysis of PCA block  retains all sixty dimensions.","Smoothing block  smooths the silence bit using a \u201crank ordering\u201d filter:",{"@attributes":{"id":"p-0133","num":"0132"},"maths":{"@attributes":{"id":"MATH-US-00023","num":"00023"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":{"mi":"SmoothSilenceBit","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},"mo":"=","mn":"1"},{"mrow":{"mi":"if","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"i","mo":"=","mrow":{"mi":"max","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"0","mo":",","mrow":{"mi":"n","mo":"-","mn":"10"}}}}},{"mi":"min","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"n","mo":"+","mn":"10"},{"msub":{"mi":["N","fr"]},"mo":"-","mn":"1"}],"mo":","}}}]},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","mrow":{"mi":"SilenceBit","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"i"}}}}}},"mo":"<","mn":"5"}],"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}}}},{"mrow":{"mo":["(",")"],"mn":"42"}}]}}}},"br":{},"b":"744"},{"@attributes":{"id":"p-0134","num":"0133"},"figref":["FIGS. 9\u201311","FIG. 9","FIG. 3","FIG. 10","FIG. 7","FIG. 11","FIG. 8","FIGS. 9\u201311"],"b":["934","936","708","742","932","938","930","926","938","940","940","336"]},"Thus, a novel and improved method and apparatus for distributed voice recognition utilizing high frequency components on the subscriber unit side and three branches for processing cepstral coefficients, nonlinear transformations, and multiband temporal patterns on the network server side has been described. Those of skill in the art will understand that the various illustrative logical blocks, modules, and mapping described in connection with the aspects disclosed herein may be implemented as electronic hardware, computer software, or combinations of both. The various illustrative components, blocks, modules, circuits, and steps have been described generally in terms of their functionality. Whether the functionality is implemented as hardware or software depends upon the particular application and design constraints imposed on the overall system. Skilled artisans recognize the interchangeability of hardware and software under these circumstances, and how best to implement the described functionality for each particular application.","As examples, the various illustrative logical blocks, modules, and mapping described in connection with the aspects disclosed herein may be implemented or performed with a processor executing a set of firmware instructions, an application specific integrated circuit (ASIC), a field programmable gate array (FPGA) or other programmable logic device, discrete gate or transistor logic, discrete hardware components such as, e.g., registers, any conventional programmable software module and a processor, or any combination thereof designed to perform the functions described herein. Functionality denoted by the various components presented may advantageously be executed in a microprocessor, but in the alternative, may be executed in any conventional processor, controller, microcontroller, or state machine. Data and information could reside in RAM memory, flash memory, ROM memory, EPROM memory, EEPROM memory, registers, hard disk, a removable disk, a CD-ROM, or any other form of storage medium known in the art. The memory (not shown) may be integral to any aforementioned processor (not shown). A processor (not shown) and memory (not shown) may reside in an ASIC (not shown). The ASIC may reside in a subscriber station.","The previous description of the embodiments of the invention is provided to enable any person skilled in the art to make or use the present invention. The various modifications to these embodiments will be readily apparent to those skilled in the art, and the generic principles defined herein may be applied to other embodiments without the use of the inventive faculty. Thus, the present invention is not intended to be limited to the embodiments shown herein but is to be accorded the widest scope consistent with the and novel features disclosed herein."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The features, nature, and advantages of the present invention will become more apparent from the detailed description set forth below when taken in conjunction with the drawings in which like reference characters identify correspondingly throughout and wherein:",{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 6","i":"a "},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 6","i":"b "},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 13"}]},"DETDESC":[{},{}]}
