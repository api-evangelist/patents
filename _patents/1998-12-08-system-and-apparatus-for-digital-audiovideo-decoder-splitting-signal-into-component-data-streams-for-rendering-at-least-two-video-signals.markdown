---
title: System and apparatus for digital audio/video decoder splitting signal into component data streams for rendering at least two video signals
abstract: The present invention provides a system and an apparatus for a digital audio/video decoder comprising a file reader capable of obtaining an encoded audio/video data stream from a data source, a navigator that instructs the file reader to obtain the encoded audio/video data stream, a splitter that separates the encoded audio/video data stream obtained by the file reader into one or more component data streams, and a reprogrammable proxy filter that decodes and converts the one or more component data streams into three or more renderable signals including at least one renderable audio signal and at least two renderable video signals.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07844167&OS=07844167&RS=07844167
owner: STMicroelectronics, Inc.
number: 07844167
owner_city: Carrollton
owner_country: US
publication_date: 19981208
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD OF THE INVENTION","BACKGROUND","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION"],"p":["The present invention relates in general to the field of audio and video decoding devices, and more particularly, to a system and apparatus for a digital audio\/video decoder.","The storage and\/or transmission of digital audio-visual data, which typically includes not only video data and audio data, but also other data for menus, sub-pictures, graphics, control\/navigation, etc., is made possible through the use of compression or encoding techniques. For example, the amount of data required to represent the video images and audio signal of a movie in an uncompressed digital format would be enormous and could not fit entirely onto a conventional recording medium, such as a compact disk (\u201cCD\u201d). Similarly, transmitting a movie in uncompressed digital form over a communication link (for real-time video) would be prohibitively expensive due to the large quantity of data to be transmitted and the large bandwidth required to transmit the data.","The video compression techniques typically used for storing audio-visual data on a digital video disc (\u201cDVD\u201d), which can hold up to 18 gigabytes of data, have been formulated by the International Standard Organization's (\u201cISO\u201d) Motion Picture Experts Group (\u201cMPEG\u201d). The MPEG standards use a discrete cosine transform (\u201cDCT\u201d) algorithm to encode, or compress, the large amount of audio-visual digital data into a much smaller amount of audio-visual digital data that can be stored on a conventional recording medium. In general terms, this is accomplished by eliminating any repetitive video data, reducing the video data needed to depict movement, and eliminating any audio data that is not discernable by the human ear.","MPEG-1, which is defined in ISO\/IEC 11172 and is hereby incorporated by reference, sets forth a standard format for storing and distributing motion video and audio. This standard has been used as a basis for video CDs and video games. MPEG-1 was designed for the playback of digital audio and video at a bit rate of 1.416 megabits per second (\u201cMbps\u201d) (1.15 Mbps is designated for video) from data stored on a standard CD.","MPEG-2, which is defined in ISO\/IEC 13818 and is hereby incorporated by reference, enhances or expands MPEG-1 to cover a wider range of applications. MPEG-2 was originally designed for the transmission of all-digital broadcast-quality video and audio at bit rates between 4 and 9 Mbps. MPEG-2, however, has become useful for may other applications, such as high definition television, and supports applications having bit rates between 1.5 and 60 Mbps.","Although the MPEG standards are typically used only for one-way communication, the H.261 and H.263 standards, which are also based on the DCT algorithm, are typically used for two-way communication, such as video telephony.","Video and\/or audio compression devices, typically referred to as encoders, are used to encode a video and\/or audio sequence before the sequence is transmitted or stored. The resulting encoded bitstream may then be decoded by a video and\/or audio decompression device, typically referred to as a decoder, before the video and\/or audio sequence is output. An encoded bitstream can only be decoded by a decoder if the encoded bitstream complies with the standard used by the decoder. Therefore, to facilitate compatibility for products produced among several manufacturers in the consumer electronics industry, the MPEG standards are being utilized for the digital video and audio decompression.","In simple terms, the DVD stores video images to be retrieved and displayed on a video display, as well as audio data to be retrieved and heard. A DVD player reads the audio-visual data stored on the DVD, decompresses and decodes the data, and generates video and audio signals for output to a video display system and audio system (i.e., to be played). In addition, DVD players typically include the capability to read, decompress and decode audio data using a variety of audio decompression techniques, such as MPEG-1, MPEG-2, PCM, Dolby AC-3 (commonly referred to as Dolby Digital), etc. Accordingly, DVD players are well-suited for playing audio-visual works, such as movies, video games, etc.","Generally, the video and audio signals are output from a DVD player to a video display (e.g. television) and a sound system (e.g. stereo system). In other words, when playing an audio-visual work, such as a movie, the DVD player reads an audio-visual stream of data from the DVD and displays the video portion of the stream (including a sub-picture portion) on the video display (television) and plays the audio portion of the stream on one or more audio speakers (stereo system).","Once the audio-visual data has been read, decompressed and decoded, the audio data must be synchronized with the video data. To facilitate the synchronized playing of the video and audio portions of the audio-visual data stream, the data stream is stored on the DVD using time stamps from a referenced frequency. The referenced frequency is defined as an integer multiple of a 27 megahertz (\u201cMHZ\u201d) clock. The time stamps indicate when a particular portion of the data stream is to be played, and are also used to synchronize the display of the video portion with the playing of the audio portion. As a result, the DVD player requires an integer multiple of a 27 MHZ clock to ensure that portions of the data stream are played at the appropriate time and that both the video portion and audio portion of the data stream are synchronized.","The present invention can provide an apparatus for a digital audio\/video decoder including a file reader capable of obtaining an encoded audio\/video data stream from a data source, a navigator that instructs the file reader to obtain the encoded audio\/video data stream, a splitter that separates the encoded audio\/video data stream obtained by the file reader into one or more component data streams, and a reprogrammable proxy filter that decodes and converts the one or more component data streams into three or more renderable signals including at least one renderable audio signal and at least two renderable video signals.","The present invention can also provide a digital audio\/video decoder including a file reader capable of obtaining an encoded audio\/video data stream from a data source, a navigator that instructs the file reader to obtain the encoded audio\/video data stream, a user interface connected to the navigator and having one or more predefined functions for selecting the encoded audio\/video data stream to be obtained, a splitter that separates the encoded audio\/video data stream obtained by the file reader into an audio data stream, a video data stream, a subpicture data stream and a navigation data stream. The navigator being coupled to the splitter such that the navigator can use the navigation data stream to select the encoded audio\/video data stream to be obtained. The present invention also including an audio filter that decodes and converts the audio data stream into a renderable audio signal, a video filter that decodes and converts the video data stream into a renderable video signal, a subpicture filter that decodes and converts the subpicture data stream into a renderable subpicture signal, a mixer for combining the renderable subpicture signal with the renderable video signal and producing a combined video signal, a synchronizing filter for synchronizing the renderable audio signal and the combined video signal, an audio renderer coupled to the audio decoder and an audio application program interface, and a video renderer coupled to the mixer and a video application program interface. The audio renderer controlling the manipulation and rendering of an audio signal from the renderable audio signal. The video renderer controlling the manipulation and rendering of a video signal from the combined video signal.","In addition, the present invention can provide a digital audio\/video logic chip having one or more interfaces, a multi-standard audio\/video decoder coupled to the one or more interfaces, a multi-standard audio\/video encoder coupled to the one or more interfaces, and a core logic device for controlling the one or more interfaces, the multi-standard audio\/video decoder and the multi-standard audio\/video encoder.","The present invention can also provide a multimedia accelerator system which includes one or more interfaces, a 2D accelerator, a 3D accelerator, a multi-standard audio\/video decoder coupled to the one or more interfaces, the 2D accelerator and the 3D accelerator, and a multi-standard audio\/video encoder coupled to the one or more interfaces.","This implementation allows the motion compensation pipeline to be specified and implemented either separately or together with the block decode section with minimal coordination. The calculation of memory addresses and generation of pipeline control information may proceed in parallel with the data manipulation. In addition, the parsing and data filtering functions are decoupled from the motion compensation functions. And finally, the motion compensation pipeline may be fabricated in hardware while the parsing and motion compensation calculations are implemented in software.","The present invention is related to the following U.S. patent applications that are owned by STMicroelectronics, Inc. and which are hereby incorporated by reference: \u201cSystem, Method and Apparatus for an Instruction Driven Digital Video Processor\u201d (ST File No. 97-S-152); \u201cMethod and Apparatus for a Motion Compensation Instruction Generator\u201d (ST File No. 97-S-153); and \u201cSystem, Method and Apparatus for a Variable Output Video Decoder\u201d (ST File No. 97-S-155). While the making and using of various embodiments of the present invention are discussed in detail below, it should be appreciated that the present invention provides many applicable inventive concepts which can be embodied in a wide variety of specific contexts. The specific embodiments discussed herein are merely illustrative of specific ways to make and use the invention and do not delimit the scope of the invention.","Now referring to , a functional block diagram of a typical digital video disc (\u201cDVD\u201d) system is illustrated and generally denoted by the numeral . The DVD system  includes a DVD drive  for reading tracks of data stored on a DVD (not shown) and converting the stored data into a bitstream, which may be in compressed or partially-compressed form. The data stored on the DVD generally includes video data, audio data, control and navigation data, and other data, such as data for menus, sub-pictures, graphics, presentation control information, highlight information, etc.","The bitstream is then input to a track buffer  (i.e. memory), which outputs the bitstream to a demultiplexer  and a navigation manager . The demultiplexer  divides the bitstream into a number of divided data portions, one for each type of data within the bitstream. The DVD system  illustrated in  divides the bitstream into five divided data portions: digital audio data, digital vertical blanking interval (\u201cVBI\u201d) data, digital video data, digital sub-picture data, and digital presentation control information (\u201cPCI\u201d) data. The demultiplexer  outputs each of these divided data portions to their respective buffers: an audio buffer  capable of storing 4 kilobytes (\u201ckB\u201d) of Dolby Digital data or 8 kB of MPEG data; a VBI buffer  capable of storing 2 kB of data; a video buffer  capable of storing 232 kB of data; a sub-picture buffer  capable of storing 52 kB of data; and a PCI buffer  capable of storing 3 kB of data.","The digital audio data that is divided out from the bitstream and stored in the audio buffer  may be encoded in a number of ways, such as MPEG-1, MPEG-2, PCM, AC-3, etc., and may include digital audio data for one or more audio channels, such as mono, stereo, five channel-surround sound, etc. The digital audio data stored in the audio buffer  is decoded by an audio decoder  using the appropriate decoding process to recover the original audio data and a memory buffer  (typically RAM). The decoded digital audio data is then converted to analog form using a digital-to-analog (\u201cDIA\u201d) converter . The analog audio data  is output to a sound system (not shown) for presentation to the user.","The digital VBI data that is divided out from the bitstream and stored in the VBI buffer  may be encoded in a number of ways, such as MPEG-1, MPEG-2, etc. The digital VBI data stored in the VBI buffer  is decoded by a VBI decoder  using the appropriate decoding process to recover the original VBI data and a memory buffer  (typically RAM). VBI data includes data that has been inserted during the vertical blanking interval between video frames. The insertion of decoded VBI data  is optional and may be used to provide additional functionality for the DVD system .","The digital video data that is divided out from the bitstream and stored in the video buffer  may be encoded in a number of ways, such as MPEG-1, MPEG-2, etc. The digital video data stored in the video buffer  is decoded by a video decoder  using the appropriate decoding process to recover the original video data (i.e. video frames) and a memory buffer  (typically RAM). The decoded digital video data is then input to a mixer  for mixing with decoded digital sub-picture data from a sub-picture decoder . The combined digital video data output from the mixer  is scaled, frame rate adjusted and color space converted by a converter  into the red-green-blue (\u201cRGB\u201d) color video format, which may be either in digital or analog form (MPEG-2 uses the YCbCr color space, supporting 4:2:0, 4:2:2, and 4:4:4 sampling). A color space is a theoretical model describing how to separate color into different components. If RGB video data  is in digital form, another processing step of converting the digital RGB video data into analog RGB video data may be necessary (not shown) depending on the type of video display (analog or digital). The analog RGB video is then input to a video display, such as a computer monitor or a television (not shown).","The digital sub-picture data that is divided out from the bitstream and stored in the sub-picture buffer  may be encoded in a number of ways, such as MPEG-1, MPEG-2, etc. The digital sub-picture data stored in the sub-picture buffer  is decoded by a sub-picture decoder  using the appropriate decoding process to recover the original sub-picture data and a memory buffer  (typically RAM). Sub-picture data includes data representing a secondary video element that is desired to be combined with the primary video (output from the video decoder ). Examples of a sub-picture include picture-in-picture (\u201cPIP\u201d), on-screen text and menus, close captioning, or any other type of video element added to, combined with, or overlaid on, the primary video. As previously described, the decoded digital sub-picture data is input to the mixer  for mixing with the decoded digital video data from the video decoder .","The digital PCI data that is divided out from the bitstream and stored in the PCI buffer  may be encoded in a number of ways, such as MPEG-1, MPEG-2, etc. The digital PCI data stored in the PCI buffer  is decoded by a PCI decoder  using the appropriate decoding process to recover the original PCI data and a memory buffer  (typically RAM). The decoded digital PCI data is input to a highlight information (\u201cHLI\u201d) buffer  capable of storing 1 kB of data. The HLI buffer  outputs the decoded digital PCI data to a HLI decoder  for highlight information decoding. The decoded digital HLI data is mixed or combined with the digital sub-picture data (output from the sub-picture decoder ) and functions to perform on-screen highlighting. The decoded digital PCI data is also input to a presentation engine , which controls and synchronizes the audio decoder , the VBI decoder , the video decoder , the sub-picture decoder  and the HLI decoder .","The DVD system  further includes a navigation manager  (including a processor, not shown) that controls the playing of the program(s) stored on the DVD (and retrieval of stored information). A user inputs commands to the navigation manager  via inputs  (e.g. buttons, remote control, etc.). Examples of such commands include forward, fast forward, reverse, rewind, play, pause, frame freeze, program selection, and the like. These commands drive the DVD drive  and\/or the presentation engine  to perform the requested functions. The presentation engine  generates video, audio, VBI and sub-picture decoder control and synchronization signals .",{"@attributes":{"id":"p-0046","num":"0045"},"figref":"FIG. 2","b":["100","100","102","104","124","106","108","126","128","110","130","132","112","114","116","118"]},"The DVD drive , audio decoder  and video decoder  comprise DVD system  which utilizes the computer system  having multimedia capabilities and software, such as Microsoft's DirectShow, to decode and render compressed audio-visual data, such as MPEG-2. DVD system  utilizes the computer's data bus  and the computer's existing hardware and software components to render the decoded video data to the video display subsystem  and the decoded audio data to the sound subsystem .","Now referring both to  and , the memory  contains an operating system , such as the MICROSOFT WINDOWS' \u00ae 95 or NT operating system available from Microsoft Corporation of Redmond, Wash., and a DVD player program . The DVD player program  is responsible for reading a DVD stream  from the DVD drive ; separating the stream into separate audio, video, sub-picture, navigation, etc. streams; decoding the DVD stream  using the audio decoder  and the video decoder ; and rendering both the audio portion and the video portion of the DVD stream  on the sound subsystem  and the video display subsystem , respectively, at the appropriate time and in synchronization. Both the audio decoder  and the video decoder  are implemented as components that may exist of either hardware components or software components or both for decoding the DVD stream .","As previously stated, the DVD player program  reads the DVD stream  from the DVD drive  and renders the DVD stream  using the video display subsystem  and the sound subsystem . The DVD player program  operates as an application under the control of the operating system  and utilizes the operating system  to access the DVD drive . As such, the DVD player program  reads the DVD stream  by requesting the operating system  to open a file-on the DVD drive  that contains the DVD stream . The DVD stream  is read from the DVD drive  using normal file system calls of the operating system .","When receiving the DVD stream  from the DVD drive  via the operating system , the DVD stream  comprises a number of frames , , , ,  and . One skilled in the art will appreciate that a stream usually has many more frames. Each frame stores either audio data or video data and has a universal system clock reference (\u201cSCR\u201d) , ,  which may be a derivative of a 27 MHZ time base. All rendering of video and audio data may be performed with respect to the universal system clock reference to ensure a proper performance of the audio-visual work, and to prevent problems with lip synchronization and other audio-visual data. In addition to the SCR, ,  each frame has either an audio presentation time stamp (\u201cAPTS\u201d) , ,  or a video presentation time stamp (\u201cVPTS\u201d) , , . These audio and video presentation time stamps APTS, ,  and VPTS, ,  contain a value that, when reached by a clock initialized to the SCR , ,  and running at a defined rate, indicates that the corresponding audio data (\u201cADATA\u201d) , ,  or video data (\u201cVDATA\u201d) , ,  or sub-picture data (\u201cSPDATA\u201d) , ,  should be rendered.","Referring now to , which shows a general hierarchy of data structures for MPEG-2 video data. MPEG-2 can represent interlaced or progressive video sequences . Video sequence  may be divided into a group of pictures , which may be further divided into pictures . Each picture  may be further subdivided into frames or fields . Frame or field  may be further subdivided into slices . Slice  may be subdivided into macroblocks , which may be further subdivided into blocks . Blocks  may be further subdivided into pixels or pels . The data structures in  may be more readily understood with reference to .","As shown in , slice  of picture  may be divided into units of macroblocks , which are divided into blocks . All macroblock rows must start and end with at least one slice . Block  is the basic unit for DCT-based transform coding and is a data structure encoding an 8\u00d78 sub-array of pixels .","Referring now to , macroblock  has a 16\u00d716 array of luminance (Y) data and two 8\u00d78 arrays of associated chrominance (Cr, Cb) data. Thus, macroblock  represents four luminance blocks , , , and , and two chrominance blocks  and . In video broadcasting, for example, in lieu of using RGB information, chrominance and luminance data are used, which in digital form is YCbCr. Y is the luminance component; Cb (Blue-yellow) and Cr (Red-yellow) are the two chrominance components.","Since the human eye is very insensitive to color and very sensitive to intensity, a lower bandwidth is possible for the chrominance, which is sub-sampled in two dimensions. Thus, there is twice as much luminance information as there is chrominance information.","The chrominance samples are typically sampled at half the sampling rate of the luminance samples in both vertical and horizontal directions, producing a sampling mode of 4:2:0 (luminance:chrominance:chrominance). The chrominance, however, may also be sampled at other frequencies, such as one-half the sampling rate of the luminance in the vertical direction and the same sampling rate as luminance in the horizontal direction, producing a sampling mode of 4:2:2.","Referring now to , which depicts a typical frame storage and decode order for MPEG-2 video. MPEG-1 and MPEG-2 support multiple types of coded frames: intra (I) frames , forward predicted (P) frames , and bidirectionally predicted (B) frames  and . This order is different from the display order () because both an I frame  and a P frame  have to be decoded before the B frames  and  can be decoded.","Referring now to , which depicts a typical frame display order for MPEG-2 video. Displaying an I frame  every twelfth frame, IBBPBBPBBPBBIBBP, is based on the practical desire to have a new starting point at least every 0.4 seconds. An I frame  contains intrapicture coding; it is a frame coded as a still image without using any previous or future frames. I frames  and P frames  are used as reference frames for interpicture coding. Intrapicture coding for 1 frames  involves the reduction of redundancy between the original pixels and the macroblocks using block-based Discrete Cosine Transform (\u201cDCT\u201d) techniques, although other coding techniques can be used.","P frames  and B frames  and  may contain both intrapicture and interpicture coding. P frames  are predicted from the most recently reconstructed I frames  or P frames . B frames  and  are predicted from the two closest I or P frames  or , one in the past and one in the future. For P frames  and B frames  and , intrapicture coding involves using the same DCT-based techniques to remove redundancy between interpicture prediction error pixels.","In interpicture coding, the redundancy between two pictures is eliminated as much as possible and the residual difference, i.e., interpicture prediction errors, between the two pictures are transmitted. In scenes where objects are stationary, the pixel values in adjacent picture will be approximately equal. In scenes with moving objects, block-based motion compensation prediction, based on macroblocks, is utilized.","For each macroblock  () in a P frame , the best matching 16\u00d716 block in the previous picture, i.e., the prediction block, is found, and the resultant macroblock prediction error is then encoded. The match is determined by searching in a previous picture over a neighborhood of the pixel origin of the current macroblock. The motion vectors between the current macroblock and the prediction block are also transmitted in interpicture coding that uses motion compensation. The motion vectors describe how far and in what direction the macroblock has moved compared to the prediction block.","Turning now to , one of the preferred embodiments of the present invention is depicted as a computer multimedia architecture utilizing a DVD system and is denoted generally as . A preferred embodiment of the invention uses a multi-media Application Programming Interface (API), such as Microsoft's DirectShow\u00ae or other suitable multi-media API, to decode and render AC-3 audio, MPEG-2 video, PCM audio and MPEG-2 audio. The multi-media API enables the capture and playback of multimedia streams, which may contain video and audio data compressed in a wide variety of formats, including MPEG, Apple QuickTime, Audio-Video interleaved (\u201cAVI\u201d), and WAV files.","A preferred embodiment of the invention uses separate producer\/consumer components or threads of execution to separate input, output and decoding. Using existing software and hardware components that typically are part of a multimedia computer and incorporating new systems and methods provide enhanced functionality and implementations for DVD technology and computers.","The DVD player  is a playback application that provides a Graphical User Interface (\u201cGUI\u201d). DVD player  may be displayed as a bitmap image drawn inside an Microsoft Foundation Class (\u201cMFC\u201d) generated dialog box. The DVD player  may load an ActiveMovie graph and the DVD player may translate user button events into graph events and send the events to the DVD splitter and navigator . Performance operations may include such operations as video playback, unbroken AC-3 audio playback, audio-video sync, on screen sub-picture decoding and many others.","The DVD drive  may hold a DVD that holds large amounts of computer data and may be a single sided or double sided disc, single-layered or double-layered disc, which can hold up to 18 gigabytes of compressed data. For example, a DVD may provide up to 8 language versions of sound tracks, up to 32 subtitle tracks, up to 8 different ratings or angles and support for interactive branching. The DVD driver  provides the kernel mode software capabilities of reading data sectors from DVD drive . The CD File System-Small Computer Serial Interface (CDFS-SCSI)  and the DVD driver  are examples of SCSI interfaces and drivers that may be used to access the DVD drive  and may be part of the overall operating system  () or may be purchased separately and installed with the purchase of a DVD drive .","The DVD file reader  reads the DVD stream  from the DVD drive . The DVD splitter and navigator  instructs the DVD file reader  as to which file to read from the DVD drive . The DVD stream  is then split into multiple streams for audio , sub-picture  and video . As was described in reference to , the DVD stream  comprises frames , , ,  . . . , . Each audio frame ,  . . .  comprises a SCR,  . . . , an APTS,  . . .  and a ADATA,  . . . . Each video frame ,  . . .  comprises a SCR,  . . . , a VPTS,  . . . , a VDATA,  . . . , and SPDATA,  . . . . The video data may comprise compressed data using standard compression techniques such as MPEG-1, MPEG-2 and MPEG-4. The audio stream  may comprise compressed data such as ISO standard AC-3, MPEG or PCM standard format. Navigation information is filtered out of the DVD stream  and used to instruct the DVD splitter and navigator  how and when to render the audio, sub-picture and video streams ,  and .","The audio, sub-picture and video streams ,  and  are read into the proxy filter . The proxy filter  feeds the streams ,  and  to one of three decoders which may include but are not limited to: AC-3 or MPEG audio decoder , sub-picture decoder , and MPEG-2 decoder . Proxy filter  provides an interface to the hardware components within architecture  and synchronizes the audio, sub-picture and video streams ,  and .","Now also referring to , the proxy filter  reads the first occurring SCR from the audio, sub-picture and video streams ,  and . After reading the SCR, the proxy filter  stores the SCR into the time-stamp counter of the CPU  and starts the time counter , which typically runs at 27 MHZ. The proxy filter  starts a separate thread for executing the time counter  using a well known create thread system call of the operating system . After starting the time counter , all audio and video data is rendered with respect to the value of the time counter . The proxy filter  reads presentation time stamp APTS from the first audio frame encountered . After reading the APTS, the proxy filter  invokes the audio decoder  to decode the audio data ADATA corresponding to the APTS. The proxy filter  then reads the video presentation time stamp VPTS from the first video frame encountered  in the DVD stream  and invokes the MPEG-2 video decoder  to decode the video data VDATA and the sub-picture decoder  to decode the sub-picture data SPDATA. The proxy filter  uses existing synchronization technology, a further description of which may be found in United States patent application: Reading an Audio-Visual Stream Synchronized by a Software Clock in a Personal Computer, Ser. No. 08\/762,616, which is owned by STMicroelectronics and is hereby incorporated by reference. Moreover the proxy filter  is programmable in the field with software, which may update the proxy filter , add new features, and add, delete or replace decoders.","Video decoding and sub-picture decoding may be partitioned into a hardware section  and software section  comprising sub-picture decoder  and MPEG-2 video decoder . The preferred embodiment of the invention uses software decoders that conform to multi-media APIs, such as Direct Show API. Sub-picture decoder  acts as a filter and passes sub-picture data  to the MPEG-2 and sub-picture hardware  for decoding. The outputs of the MPEG-2 video decoder  and the sub-picture decoder  are fed to mixer . The mixer  is a hardware device used to combine the output of the MPEG video decoder,  and the sub-picture decoder  so that a low bandwidth video sequence, such as a closed captioning or picture in a picture may be over-layered with the original video content. The combined decoded video data , which is the output of the mixed MPEG-2 video decoder  and the sub-picture decoder , may be placed in a memory buffer in a YCrCb color conversion format.","The video renderer  outputs the combined decoded video data  to a video API , such as Direct Draw with VPE. The video renderer  reads and writes data to and from the video API . The video renderer  provides the intelligence on how to manipulate the combined decoded video data , i.e. when to render the data, what is the output format for the video data, what color space conversions to use, whether sub-picture gets included with the video output, etc. The video renderer  also communicates to the graphics adapter  through a series of layers provided with the operating system . The video API  and a DD hardware abstraction layer (\u201cHAL\u201d) with VPE  provides a communications layer between the hardware and software components.","Audio decoding, which may include AC-3, MPEG audio and PCM audio, is exclusively decompressed by software. AC-3 audio decoder  takes the compressed audio stream  and decodes and decompresses the audio stream  and outputs a Pulse Code Modulated (\u201cPCM\u201d) sample audio. The AC-3 audio decoder output  is made available to the audio renderer . The audio renderer  communicates with the sound card  through multiple layers of software drivers, such as a WDM audio minidriver . The audio output and the video output to the respected adapter card must be synchronized to produce desired results. The present invention can include copy protection , STVXD\/MP  and ST-HAL (Device Drive API) .","Turning now to , an architecture of the DVD splitter and navigator is depicted and denoted generally as . The DVD splitter and navigator  provides the intelligence for rendering video and audio data. The DVD stream  read from the DVD drive  also comprises navigation information. The navigation information comprises parametric values defining how the DVD drive  is read and how and when data is presented.","The interpreter  is a pre-programmed function which transforms the input X  into a value Y  defined by the transfer function h(X) . The interpreters  transfer function h(X) is a parametric function defined by the user input command or information contained in the navigation packet. For example, if the user selects a track from the DVD disc the interpreter function acts to extract the desired sequence from the video and audio stream or requests that information on the DVD drive  is retrieved. The interpreter places the desired output into memory . The splitting function  separates the DVD stream , which may include such compression standards as MPEG-2 video, AC-3 audio, sub-picture video and MPEG audio. The parsed outputs ,  and  are fed into the proxy filter  where the streams ,  and  are synchronized and partially decoded.",{"@attributes":{"id":"p-0073","num":"0072"},"figref":"FIG. 10","b":["54","54","561","566","44","44","326"]},"The encoded video data stream  contains compressed frames. A frame is a data structure representing the encoded data for one displayable image in the video sequence. This data structure consists of one two-dimensional array of luminance pixels, and two two-dimensional arrays of chrominance samples, i.e., color difference samples.","The compressed frame is parsed into smaller subunits by a bit unpack . Bit unpack  parses the information into macroblocks, and then parses the macroblocks and sends the header portion of each macroblock to a motion compensation pipeline . Using prediction mode determination block , the motion compensation pipeline  determines the frame type being processed (I, P or B) and determines which prediction frames must be accessed from picture memory . Using the motion vector information from motion vector decode , motion compensation pipeline  also determines the address in picture memory  where the prediction frame, and the prediction macroblock within the frame are located. This information is needed to decode the motion compensated prediction for the given macroblock to be decoded.","The prediction macroblock is obtained from picture memory  and is input into a prediction block fetch  and then into half pixel filter . Half pixel filter  performs vertical and horizontal half-pixel interpolation on the fetched prediction macroblock as dictated by the motion vectors. The prediction macroblocks are generated in prediction generation circuit .","Bit unpack  sends the encoded block data structures to a variable length decoder , which decodes variable length codes representing the encoded blocks and converts them into fixed length pulse code modulation (\u201cPCM\u201d) codes. These codes represent the DCT coefficients of the encoded blocks. The PCM codes are a serial representation of the 8\u00d78 block array obtained in a zig-zag format. An inverse zig-zag scanner , which is connected to the variable length decoder , converts the serial representation of the 8\u00d78 block array obtained in a zig-zag format to a rectangular 8\u00d78 block array. The coefficients are ordered in a rectangular array format, with the largest value in the top left of the array and typically decreasing in value to the bottom right of the array.","The rectangular array is passed to an inverse quantizer , which performs the inverse quantization based on the appropriate quantization tables. The data is then passed to an inverse DCT (\u201cIDCT\u201d) circuit , which performs an inverse DCT on its input block and produces a decompressed 8\u00d78 block. The decompressed 8\u00d78 block is then passed to the prediction error assembly , which generates the interpicture prediction errors.","The prediction macroblock from prediction generation  and the interpicture prediction errors from the prediction error assembly  are summed in a macroblock sum unit  and then passed to picture assembly unit . In MPEG-2 and other decompression protocols that use interpicture compression, the frames are encoded based on past and future frames; therefore in order to decode the frames properly the frames are not sent in order and need to be stored until they are to be displayed. A typical MPEG-2 video decoder  requires 16 Mbits of memory to operate in the main profile at main level mode (MP at ML). Therefore, MPEG-2 video decoder  may require a 2 Mbyte memory.","Assembly unit  ensures that the information is placed in the correct place in picture memory  to correspond to the frame being decompressed. The resulting decoded macroblock is then stored in picture memory  in the place designated for it by assembly unit . All frames should be stored in picture memory  because the decoded macroblock may not be the next macroblock that is to be sent to display generation  due to the storing and transmission format of the decompression protocol. Display generation  sends the decoded video data  in a color space format to be displayed.","MPEG-2 video decoder  may be designed to decode a bitstream formatted according to any one or a combination of standards. To decode a bitstream formatted according to a combination of standards, video decoder  needs to include circuitry in order to encode a bitstream to comply to a particular decompression protocol. Video decoder  may be a combination of decoders, and possibly encoders, for each desired decompression protocol. For example, video decoder  may decompress a bitstream encoded to comply to either the MPEG-2 standard or the H.261 standard which contains two sets of decoding circuitry with each set containing its own motion compensation circuits, its own block decoding circuits, one for each of the standards and specific to that particular standard.","Turning now to , a block diagram for a MPEG-2 video decoder with an instruction driven motion compensation engine is illustrated and is denoted generally as . The video input buffer , bit unpack , block decode section , variable length decoder , inverse zig-zag scanner , inverse quantizer , IDCT , prediction error assembly , prediction mode determination  and motion vector decode  provide the same functions as described in reference to .","The error terms , which are derived from the series of DCT coefficients by the prediction error assembly , are placed in the error buffer (FIFO) . The instructions , which are generated by instruction generator , are placed in the instruction buffer (FIFO) . The instructions  are then passed to the instruction queue , which is communicably coupled to the execution unit . The execution unit  is communicably coupled to the error memory (RAM)  and the motion compensation state machine . The motion compensation state machine  is communicably coupled to and provides functional operation commands to the half pixel filter  and the merge memory (RAM) .","As was the case with , picture data is stored in the picture memory  and is fetched for use in the instruction driven compensation engine . The picture data is placed in the reference buffer before it is moved to the half pixel filter . After the error terms from the error memory  are summed with the prediction macroblock from the merge memory  in sum unit , the resulting data is stored in the display buffer  before it is moved to picture memory  and later displayed.","The execution unit  synchronizes the operation of the instruction driven motion compensation pipeline  and the block decode section  when the data stored in the error memory (RAM)  and the merge memory (RAM)  are to be summed in sum unit . Otherwise, these two processing sections are decoupled and operate independently. This implementation allows the instruction driven motion compensation pipeline  to be specified and implemented separately from the block decode section  with minimal coordination. The calculation of memory addresses and generation of pipeline control information may proceed in parallel with the data manipulation. In addition, parsing and data filtering become non-critical functions.","The instruction driven motion compensation pipeline  may be used independently and may be implemented in hardware or software. Although both the block decoder  and the instruction driven motion compensation engine  may be implemented in software, the processing load of the CPU  () may be substantially reduced by implementing the instruction driven motion compensation pipeline  in hardware.","The instruction transferred to the instruction driven motion compensation pipeline  from the motion compensation state machine  comprises instruction descriptors and data descriptors. Each instruction is a group of descriptors or instruction sequences and data descriptors. The instruction descriptors and data descriptors provide instructions for processing the predictions blocks and the DCT coefficients. The instructions also comprise memory locations for each prediction block. For example, a load instruction reads and filters a macroblock and loads the result into the merge memory . A merge instruction reads and filters a macroblock and then sums the result with the contents of the merge memory . A store or write instruction sums the contents of the merge memory  and error memory  and puts the result back into picture memory .","The prediction block is obtained from data memory and input into the half-pixel filter , which is coupled to the motion compensation state machine . The motion compensation state machine  controls the interfaces to the memory. The half-pixel filter  performs vertical and horizontal half-pixel interpolation on the fetched prediction block as dictated by the motion vector. The merge memory  allows multiple reference macroblocks to be averaged together to form the prediction blocks. The prediction errors and the prediction blocks are summed in the sum unit  and placed in the output buffer (FIFO) , which may be any conventional memory such as Dynamic RAM (\u201cDRAM\u201d).",{"@attributes":{"id":"p-0089","num":"0088"},"figref":"FIG. 12","b":"710","sup":["st ","nd "]},"Turning now to , an instruction structure for prediction blocks is depicted and denoted generally as . The instruction structure details functional operation commands such as loading a prediction block in merge memory and merging a prediction block with existing data in merge memory and writing data resulting from the summation of data in merger memory and error memory to an output memory. Portions of the instruction structure also control interlacing the data and enabling half pixel prediction. The instruction descriptor comprises a function code, a stripe count (the number of data descriptors following the instruction descriptor), a vertical half pixel prediction, a horizontal half pixel prediction, a byte offset, and an interlace code. Each data descriptor comprises a word count and a starting memory address.","Turning now to , a flowchart depicting the steps of a video decoder in accordance with the prior art will be described. Video decoding begins in block  and memory is allocated for display buffers, I frame reference buffer and P frame reference buffer in block . If a terminate signal has been received, as determined in decision block , decoding ends in block . If, however, a terminate signal has not been received and encoded video data is ready in the video input buffer, as determined in decision block , the encoded macroblock is read from the video input buffer in block . If, however, encoded video data is not ready in the video input buffer, processing continues to loop until a terminate signal is received, as determined in decision block  or the encoded data is ready in the video input buffer, as determined in decision block .","After the encoded macroblock is read in block , the macroblock is bit unpacked in block . If the macroblock is part of a new frame, as determined in decision block , the current display buffer is released for display in block . Thereafter, processing will wait until the next display buffer is available, as determined in decision block . Once the next display buffer is available, it is held for use by the decoder and is identified as the current display buffer in block . Once the next display buffer is held and identified, or if the macroblock is not part of a new frame, as determined in decision block , processing of the macroblock is split in block .","The prediction information is processed using a motion compensation pipeline in block , which will be described in detail in reference to . The DCT information from block  is processed using a block decode process in block , which will be described in detail in reference to . The resulting predicted macroblock from block  and the error terms from block  are summed together in block  to produce a decoded macroblock. Next, the decoded macroblock is written to the current display buffer in block . Thereafter, processing loops back to decision block  where a terminate signal is checked for.","Now referring to , the motion compensation pipeline of block  will be described. Motion compensation begins in block  and blocks ,  and  preform prediction mode determination. If a P frame is currently being decoded, as determined in decision block  and the macroblock to be decoded is a P macroblock, as determined in decision block , motion compensation is performed on the macroblock in block , which will be described in detail below in reference to . Once the motion compensation of block  is complete or the macroblock to be decoded is an I macroblock, as determined in decision block , the macroblock is written to the P frame reference buffer in block  and the predicted macroblock is returned in block . If, however, an I frame is currently being decoded, as determined in decision block , the macroblock is written to the I frame reference buffer in block  and the macroblock is returned as a predicted macroblock in block . If, however, a B frame is currently being decoded, as determined in decision block  and the macroblock to be decoded is a B macroblock or a P macroblock, as determined in decision block , motion compensation is performed on the macroblock in block , which will be described in detail below in reference to . Once the motion compensation of block  is complete or the macroblock to be decoded is an I macroblock, as determined in decision block , the predicted macroblock is returned in block .","Now referring to , the block decode process  will be described. Block decoding begins in block  and variable length decoding is performed in block . An inverse zig-zag function is performed in block  followed by an inverse quantization in block  and an inverse DCT function in block . The error terms are then generated in block  and are returned in block .","Now referring to , the motion compensation process of blocks  and  will be described. Motion compensation begins in block  and the motion vectors are decoded in block . Next, the required macroblock to perform the motion compensation is retrieved from the I frame and\/or P frame reference buffer as required in block . The retrieved macroblock is then passed through a half pixel filter in block  and the predicted macroblock is generated in block . Thereafter, the predicted macroblock is returned in block .","Turning now to , a flowchart depicting the steps of a video decoder in accordance with one embodiment of the present invention will be described. Video decoding begins in block . If an instruction driven motion compensation engine is detected by the system or the decoded output is selected to be in an instruction format, as determined in decision block , memory is allocated for an error buffer, instruction buffer, I frame reference buffer and P frame reference buffer in block . If, however, an instruction driven motion compensation engine is not detected or the decoded output is to be in a color space format, as determined in decision block , memory is allocated for display buffers, I frame reference buffer and P frame reference buffer in block .","Although the format of the decoded output could be selected manually, an automatic evaluation of the computer system at the start of the decoding process can select an optimal decoding output based on various performance parameters. For example, utilizing a separate chip to perform the motion compensation functions greatly reduces the demand or \u201cchurn\u201d on the system processor.","After sufficient memory has been allocated in block  or , decision block  determines whether or not a terminate signal has been received. If a terminate signal has been received, decoding ends in block . If, however, a terminate signal has not been received and the encoded video data is ready in the video input buffer, as determined in decision block , the encoded macroblock is read from the video input buffer in block . If, however, encoded video data is not ready in the video input buffer, processing continues to loop until a terminate signal is received, as determined in decision block  or the encoded data is ready in the video input buffer, as determined in decision block .","After the encoded macroblock is read in block , the macroblock is bit unpacked in block . If the macroblock is part of a new frame, as determined in decision block , the current display buffer is released for display in block . Thereafter, processing will wait until the next display buffer is available, as determined in decision block . Once the next display buffer is available, it is held for use by the decoder and is identified as the current display buffer in block . Once the next display buffer is held and identified, or if the macroblock is not part of a new frame, as determined in decision block , processing of the macroblock is split in block .","If an instruction driven motion compensation engine is detected or the decoded output is selected to be in an instruction format, as determined in decision block , the prediction information is processed using a generate instruction process in block , which will be described in detail in reference to . If, however, an instruction driven motion compensation engine is not detected or the decoded output is to be in a color space format, as determined in decision block , the prediction information is processed using a motion compensation engine in block , which will be described in detail in reference to . The DCT information from block  is processed using a block decode process in block , which will be described in detail in reference to . If an instruction driven motion compensation engine is not found or the decoded output is to be in a color space format, as determined in decision block , the resulting predicted macroblock from block  and the error terms from block  are summed together in block  to produce a decoded macroblock. Next, the decoded macroblock is written to the current display buffer in block . If, however, an instruction driven motion compensation engine is detected or the decoded output is selected to be in a instruction format, as determined in decision block , the error terms from block  are written to the error buffer in block . After blocks ,  or  are complete, processing loops back to decision block  where a terminate signal is checked for.","Now referring to , the instruction generation process of block  will be described. Instruction generation begins in block  and blocks ,  and  preform prediction mode determination. If a P frame is currently being decoded, as determined in decision block  and the macroblock to be decoded is a P macroblock, as determined in decision block , the motion vectors are decoded in block . Once the motion vectors are decoded or if the macroblock to be decoded is an I macroblock, as determined in decision block , the instructions for the macroblock are generated in block  and the instructions and associated data is written to the instruction buffer in block . Thereafter, the macroblock is written to the P frame reference buffer in block  and processing returns in block . If, however, an I frame is currently being decoded, as determined in decision block , the instructions for the macroblock are generated in block  and the instructions and associated data is written to the instruction buffer in block . Thereafter, the macroblock is written to the I frame reference buffer in block  and processing returns in block . If, however, a B frame is currently being decoded, as determined in decision block  and the macroblock to be decoded is a B macroblock or a P macroblock, as determined in decision block , the motion vectors are decoded in block . Once the motion vectors are decoded or if the macroblock to be decoded is an I macroblock, as determined in decision block , the instructions for the macroblock are generated in block  and the instructions and associated data is written to the instruction buffer in block . Thereafter, processing returns in block .","Now referring to , the motion compensation pipeline of block  will be described. Motion compensation begins in block  and blocks ,  and  preform prediction mode determination. If a P frame is currently being decoded, as determined in decision block  and the macroblock to be decoded is a P macroblock, as determined in decision block , motion compensation is performed on the macroblock in block , which will be described in detail below in reference to . Once the motion compensation of block  is complete or the macroblock to be decoded is an I macroblock, as determined in decision block , the macroblock is written to the P frame reference buffer in block  and the predicted macroblock is returned in block . If, however, an I frame is currently being decoded, as determined in decision block , the macroblock is written to the I frame reference buffer in block  and the macroblock is returned as a predicted macroblock in block . If, however, a B frame is currently being decoded, as determined in decision block  and the macroblock to be decoded is a B macroblock or a P macroblock, as determined in decision block , motion compensation is performed on the macroblock in block , which will be described in detail below in reference to . Once the motion compensation of block  is complete or the macroblock to be decoded is an I macroblock, as determined in decision block , the predicted macroblock is returned in block .","Now referring to , the block decode process  will be described. Block decoding begins in block  and variable length decoding is performed in block . An inverse zig-zag function is performed in block  followed by an inverse quantization in block  and an inverse DCT function in block . The error terms are then generated in block  and are returned in block .","Now referring to , the motion compensation process of blocks  and  will be described. Motion compensation begins in block  and the motion vectors are decoded in block . Next, the required macroblock to perform the motion compensation is retrieved from the I frame and\/or P frame reference buffer as required in block . The retrieved macroblock is then passed through a half pixel filter in block  and the predicted macroblock is generated in block . Thereafter, the predicted macroblock is returned in block .","Turning now to ,  and A, the steps for driving a motion compensation engine in accordance with one embodiment of the present invention will be described. Processing begins in block  and decision block  determines whether there is an instruction in the instruction buffer and the instruction queue  is not full. If there is an instruction in the instruction buffer and the instruction queue  is not full, the next instruction is read from the instruction buffer in block  and the instruction is written to the instruction queue  in block . If, however, there is not an instruction in the instruction buffer or the instruction queue  is full, as determined in decision block , or the instruction has just been written to the instruction queue in block , decision block  determines whether there is an instruction in the instruction queue  and the execution unit  is empty. If there is an instruction in the instruction queue  and the execution unit  is empty, the next instruction is read from the instruction buffer in block  and the instruction is written to the execution unit  in block . If, however, there is not an instruction in the instruction queue  or the execution unit  is not empty, as determined in decision block , or the instruction has just been written to the execution unit in block , decision block decision block  determines whether there is an error term in the error buffer and the error memory  is not full. If there is an error term in the error buffer and the error memory  is not full, the next error term is read from the error buffer in block  and the error term is written to the error memory  in block . If, however, there is not an error term in the error buffer or the error memory  is full, as determined in decision block , or the error term has just been written to the error memory  in block , decision block  determines whether there is a prediction block in data buffer and the instruction in the execution unit  is a load instruction.","If there is a prediction block in data buffer and the instruction in the execution unit  is a load instruction, the load instruction is moved to the motion compensation state machine  in block  and the load instruction is executed in block . The processing steps associated with the load instruction will be described in detail below in reference to . Decision block  determines whether the process should be terminated. If the process should be terminated, processing ends in block . If, however, the process should not be terminated, as determined in decision block , processing loops back to block  where the instruction buffer and the instruction queue  are checked. Decision block  determines whether there is a prediction block in data buffer and the instruction in the execution unit  is a merge instruction.","If there is a prediction block in data buffer and the instruction in the execution unit  is a merge instruction, the merge instruction is moved to the motion compensation state machine  in block  and the merge instruction is executed in block . The processing steps associated with the merge instruction will be described in detail below in reference to . Processing then proceeds as previously described to decision block  to determine whether the process should be terminated. Decision block  determines whether the instruction in the execution unit  is a write instruction.","If the instruction in the execution unit  is not a write instruction, an error has occurred because the instruction was not recognized and an error handler is called in block . If, however, the instruction in the execution unit  is a write instruction, as determined in block , decision block  determines whether the error memory  is full. If the error memory  is not full, processing is suspended until the error memory  is full. When the error memory  is full, as determined in decision block , the write instruction is moved to the motion compensation state machine  in block  and the write instruction is executed in block . The processing steps associated with the write instruction will be described in detail below in reference to . Processing then proceeds as previously described to decision block  to determine whether the process should be terminated.","Turning now to ,  and B, the steps for executing a load instruction in accordance with one embodiment of the present invention will be described. Load instruction execution begins in block . The next prediction block is read from data buffer in block  and is filtered using the half pixel filter  in step . The filtered prediction block is then written to memory in block  and processing returns to the main process in block .","Turning now to ,  and C, the steps for executing a merge instruction in accordance with one embodiment of the present invention will be described. Merge instruction execution begins in block . The next prediction block is read from data buffer in block  and is filtered using the half pixel filter  in step . The filtered prediction block is then averaged with the previously filtered prediction blocks in merge memory  in block  and processing returns to the main process in block .","Turning now to ,  and D, the steps for executing a write instruction in accordance with one embodiment of the present invention will be described. Write instruction execution begins in block . In block , a decoded macroblock is produced in the sum unit  by algebraic summing all the error terms stored in the error memory  with the previously filtered and merged prediction blocks in merge memory . The decoded macroblock is then written to the video buffer  and processing returns to the main process in block .","Turning now to , a computer is depicted where the video decoder, sub-picture decoder and audio decoder are sharing a frame buffer  with a graphics accelerator . The graphics accelerator  can be any graphics accelerator known in the art. The graphics accelerator  contains a 2D accelerator , a 3D accelerator , a digital to analog converter , a memory interface , and bus interfaces  for any system busses  to which it is coupled. The graphics accelerator  can also contain an audio compressor\/decompressor . The graphics accelerator  is coupled to a display , and a frame buffer .","In this embodiment, the frame buffer  is the memory to which the memory interface  is coupled. The frame buffer  is coupled to the memory interfaces  through a memory bus. In current technology the memory bus , for coupling a graphics accelerator to a memory, is capable of having a bandwidth of up to 400 Mbytes\/s. This bandwidth is more than twice the bandwidth required for an optimized decoder\/encoder  and . This allows the decoder\/encoder  and  to operate in real time."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["For a more complete understanding of the features and advantages of the present invention, reference is now made to the detailed description of the invention along with the accompanying figures in which corresponding numerals in the different figures refer to corresponding parts and in which:",{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 7A"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 7B"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIGS. 14A","b":["14","14","14"]},{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIGS. 15A","b":["15","15","15","15"]},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 16A"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIGS. 16B","b":["16","16"]},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 17"}]},"DETDESC":[{},{}]}
