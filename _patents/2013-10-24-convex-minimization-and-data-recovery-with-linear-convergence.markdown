---
title: Convex minimization and data recovery with linear convergence
abstract: A convex minimization is formulated to robustly recover a subspace from a contaminated data set, partially sampled around it, and propose a fast iterative algorithm to achieve the corresponding minimum. This disclosure establishes exact recovery by this minimizer, quantifies the effect of noise and regularization, and explains how to take advantage of a known intrinsic dimension and establish linear convergence of the iterative algorithm. The minimizer is an M-estimator. The disclosure demonstrates its significance by adapting it to formulate a convex minimization equivalent to the non-convex total least squares (which is solved by PCA). The technique is compared with many other algorithms for robust PCA on synthetic and real data sets and state-of-the-art speed and accuracy is demonstrated.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09251438&OS=09251438&RS=09251438
owner: Regents of the University of Minnesota
number: 09251438
owner_city: Minneapolis
owner_country: US
publication_date: 20131024
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"p":["This application claims the benefit of U.S. Provisional Application 61\/717,994, filed Oct. 24, 2012, the entire content of which is incorporated herein by reference.","This invention was made with government support under DMS-0915064 and DMS-0956072 awarded by the National Science Foundation. The government has certain rights in the invention.","The invention relates to sensing systems and, more specifically, data analysis for sensing system and machine learning.","The most useful paradigm in data analysis and machine learning is arguably the modeling of data by a low-dimensional subspace. The well-known total least squares solves this modeling problem by finding the subspace minimizing the sum of squared errors of data points. This is practically done via principal components analysis (PCA) of the data matrix. Nevertheless, this procedure is highly sensitive to outliers.","In general, the invention techniques are described in which an iterative algorithm is applied to formulate a convex minimization to robustly recover a subspace from a contaminated data set, partially sampled around it. This disclosure establishes exact recovery by this minimizer, quantifies the effect of noise and regularization, and explains how to take advantage of a known intrinsic dimension and establish linear convergence of the iterative algorithm. The minimizer is an M-estimator. The disclosure demonstrates its significance by adapting it to formulate a convex minimization equivalent to the non-convex total least squares (which is solved by PCA). The technique is compared with many other algorithms for robust PCA on synthetic and real data sets and state-of-the-art speed and accuracy is demonstrated.","In one embodiment, a method comprises receiving, with a device, multidimensional data comprising a set of data points, wherein the set of data points conforms to a plurality of dimensions (D) and includes outlier data points, and iteratively processing the set of data points with the device to compute a reduced data set representative of the set of data points, wherein the reduced data set conforms to a subspace having a reduced number of dimensions (d) less than the plurality of dimensions (D) of the set of data points. According to the method, iteratively processing the set of data points to compute the reduced data set comprises determining, for each iteration, a scaled version of the set of data points by re-computing a corresponding coefficient for each of the data points as a function of a proximity of the data point to a current estimate of the subspace, and computing, for each iteration, an updated estimate of the subspace based on a minimization of a sum of least squares of the scaled version of the set of data points. The iteratively processing of the multidimensional data to compute the reduced data set has a processing complexity with linear convergence with respect to the number of iterations.","In another embodiment, a device comprises a memory to store multidimensional data having a set of outliers. A processor is configured to execute program code to process the multidimensional data with the device to compute a reduced data set that is representative of the multidimensional data without the outliers. The program code iteratively processes the set of data points with the device to compute a reduced data set representative of the set of data points, wherein the reduced data set conforms to a subspace having a reduced number of dimensions (d) less than the plurality of dimensions (D) of the set of data points. During this transformation of the data, the program code determines, for each iteration, a scaled version of the set of data points by re-computing a corresponding coefficient for each of the data points as a function of a proximity of the data point to a current estimate of the subspace and computes, for each iteration, an updated estimate of the subspace based on a minimization of a sum of least squares of the scaled version of the set of data points.","The details of one or more embodiments of the invention are set forth in the accompanying drawings and the description below. Other features, objects, and advantages of the invention will be apparent from the description and drawings, and from the claims.",{"@attributes":{"id":"p-0019","num":"0018"},"figref":["FIG. 1","FIG. 1"],"b":["10","12","15","14","12","14","12","14"]},"Estimator  of processing unit  process image data  to compute low-dimensional data model  representative of image data . Low dimensional data model  includes a reduced set of data points that conforms to a subspace having a reduced number of dimensions (d) less than the plurality of dimensions (D) of the set of data points. In this way, estimator  computes data model  of image data  by determining a low-dimensional subspace representative of the image data. Moreover, as described herein, estimator  computes data model  as an estimate of the underlying subspace in a manner that is resistance to any outliers within image data .","As further described herein, estimator  provides an iterative approach in which a convex minimization of image data  is formulated to robustly recover the subspace of image data , where the image data is contaminated with outlier data points in the form of noise. In each iteration, estimator  applies a scaling procedure to scale each data point computes an updated estimate for the subspace based on the scaled version of the data points. Estimator  determines, for each iteration, the scaled version of the set of data points by re-computing coefficients for the data points as a function of a proximity of each of the data points to the current estimate of the subspace. During each iteration, estimator  computes the updated estimate of the subspace based on a summation of weighted least absolute squares of the scaled version of the set of data points.","In general, image data  may be represented as a data set X={x}\u2282, for which the geometric median is the minimizer of the following function of \u03b5:",{"@attributes":{"id":"p-0023","num":"0022"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mrow":{"mi":"y","mo":"-","msub":{"mi":["x","i"]}}}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"1"}}]}}}},"br":{},"sub":["i=1","i","i ","i","i","i"],"sup":["N","D"],"img":{"@attributes":{"id":"CUSTOM-CHARACTER-00003","he":"3.13mm","wi":"2.46mm","file":"US09251438-20160202-P00001.TIF","alt":"custom character","img-content":"character","img-format":"tif"}}},"There are several obstacles in developing robust and effective estimators for subspaces. For purposes of example, estimators of linear subspaces are discussed and it is assumed that the data is centered at the origin. A main obstacle is due to the fact that the set of d-dimensional linear subspaces in , i.e., the Grassmannian G(D, d), is not convex. Therefore, a direct optimization on G(D, d) (or a union of G(D, d) over different d's) will not be convex (even not geodesically convex) and may result in several (or many) local minima. Another problem is that extensions of simple robust estimators of vectors to subspaces (e.g., using 11-type averages) can fail by a single far away outlier. For example, one may extend the d-dimensional geometric median minimizing (1) to the minimizer over L\u2208G(D, d) of the function",{"@attributes":{"id":"p-0025","num":"0024"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mrow":{"msub":{"mi":["x","i"]},"mo":"-","mrow":{"msub":[{"mi":["P","L"]},{"mi":["x","i"]}],"mo":"\u2062"}}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mrow":{"msub":[{"mi":["P","L"]},{"mi":["x","i"]}],"mo":"\u22a5"}}}],"mo":"\u2261"},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"2"}}]}}}},"br":{},"sup":["\u22a5 ","\u22a5 "],"sub":["L ","L"]},"A different convex relaxation is described herein that does not introduce arbitrary parameters and its implementation is significantly faster.","Described herein is a convex relaxation of the minimization of (2). The original minimization is over all subspaces L or equivalently all orthogonal projectors P\u2261P\u22a5. P with a D\u00d7D matrix satisfying P=P and P=P (where \u2022denotes the transpose) can be identified. Since the latter set is not convex, it is relaxed to include all symmetric matrices, but avoid singularities by enforcing unit trace. That is, the set follow set may be minimized over:",{"@attributes":{"id":"p-0028","num":"0027"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"H","mo":":=","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mrow":{"mi":"Q","mo":["\u2208","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msup":{"mi":"R","mrow":{"mi":["D","D"],"mo":"\u00d7"}},"mo":":","mi":"Q"}},"mo":"=","msup":{"mi":["Q","T"]}},{"mrow":{"mo":["(",")"],"mi":"Q"},"mo":"=","mn":"1"}],"mo":","}}}},{"mrow":{"mo":["(",")"],"mn":"3"}}]}}}},"br":{}},{"@attributes":{"id":"p-0029","num":"0028"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mover":{"mi":"Q","mo":"^"},"mo":"=","mrow":{"mtable":{"mtr":[{"mtd":{"mi":"arg"}},{"mtd":{"mrow":{"mi":["Q","\u025b","\u210d"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}}}]},"mo":["\u2062","\u2062","\u2062"],"mi":"min","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"F","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"Q"}}}},{"mrow":[{"mi":"where","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"mi":"F","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"Q"}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"Q","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["x","i"]}}}}],"mo":":="}],"mo":","}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}}}},"br":[{},{}],"in-line-formulae":[{},{}],"i":["{circumflex over (L)}:ker","{circumflex over (Q)}"]},"If the intrinsic dimension d is known (or can be estimate from the data), the subspace can be estimated by the span of the bottom d eigenvectors of {circumflex over (Q)}. This procedure is robust to sufficiently small levels of noise. This is referred to herein as the Geometric Median Subspace (GMS) algorithm and summarize in Algorithm 1.",{"@attributes":{"id":"p-0031","num":"0030"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Algorithm 1 "},{"entry":"The Geometric Median Subspace Algorithm"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"Input: \u03c7 = {x}   : data, d: dimension of L*,"},{"entry":"an algorithm for minimizing (4)"},{"entry":"Output: {circumflex over (L)}: a d-dimensional linear subspace in  ."},{"entry":"\u2003Steps:"},{"entry":"\u2003\u2003{v} = the bottom d eigenvectors of {circumflex over (Q)} (see (4))"},{"entry":"\u2003\u2003L = Sp({v})"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"We remark that {circumflex over (Q)} is semi-definite positive (we verify this later in Lemma 14). We can thus restrict  to contain only semi-definite positive matrices and thus make it even closer to a set of orthogonal projectors. Theoretically, it makes sense to require that the trace of the matrices in  is D\u2212d (since they are relaxed versions of projectors onto the orthogonal complement of a d-dimensional subspace). However, scaling of the trace in (3) results in scaling the minimizer of (4) by a constant, which does not effect the subspace recovery procedure.","Equation (4) is an M-estimator with residuals r=|Qx|, 1\u2266i\u2266N, and \u03c1(x)=|x|. Unlike (2), which can also be seen as a formal M-estimator, the estimator {circumflex over (Q)} is unique under a weak condition stated later.","The techniques described herein results in the following fundamental contributions to robust recovery of subspaces:\n\n","Exact Recovery and Conditions for Exact Recovery by GMS","In order to study the robustness to outliers of our estimator for the underlying subspace, formulate the exact subspace recovery problem is formulated. This problem assumes a fixed d-dimensional linear subspace L*, inliers sampled from L* and outliers sampled from its complement; it asks to recover L* as well as identify correctly inliers and outliers.","In the case of point estimators, like the geometric median minimizing (1), robustness is commonly measured by the breakdown point of the estimator. Roughly speaking, the breakdown point measures the proportion of arbitrarily large observations (that is, the proportion of \u201coutliers\u201d) an estimator can handle before giving an arbitrarily large result. For example, the breakdown point of the geometric median is 50%.","In the case of estimating subspaces, this definition cannot be directly extended since the set of subspaces, i.e., the Grassmannian (or unions of it), is compact, so \u201can arbitrarily large result\u201d, that is, a subspace with arbitrarily large distance from all other subspaces, cannot be described. Furthermore, given an arbitrarily large data point, a subspace containing it can always be formed; that is, this point is not arbitrarily large with respect to this subspace. Instead, the outliers are identified as the ones in the compliment of L* and the techniques focus on the largest fraction of outliers (or smallest fraction of inliers per outliers) allowing exact recovery of L*. Whenever an estimator can exactly recover a subspace under a given sampling scenario, it is viewed herein as robust and its effectiveness can be measured by the largest fraction of outliers it can tolerate. However, when an estimator cannot exactly recover a subspace, one needs to bound from below the distance between the recovered subspace and the underlying subspace of the model. Alternatively, one would need to point out at interesting scenarios where exact recovery cannot even occur in the limit when the number of points approaches infinity.","In one example, in order to guarantee exact recovery of our estimator, three kinds of restrictions are applied to the underlying data, which are explained herein. First of all, the inliers need to permeate through the whole underlying subspace L*, in particular, they cannot concentrate on a lower dimensional subspace of L*. Second of all, outliers need to permeate throughout the whole complement of L*. Some practical methods are also suggested to avoid this failure mode when d is unknown. Third of all, the \u201cmagnitude\u2019 of outliers needs to be restricted. All data points may be initially scaled to the unit sphere in order to avoid extremely large outliers. However, outliers concentrating along lines, which may have an equivalent effect of a single arbitrarily large outlier, still need to be avoided.",{"@attributes":{"id":"p-0040","num":"0046"},"figref":"FIGS. 2A-2D"},"In the case where d is known, a tight convex relaxation of the minimization of (31) over all projectors Pof rank d may be used. An optimizer, refered to as the REAPER (of the needle-in-haystack problem) minimize the same function F(Q) (see (4)) over the set",{"@attributes":{"id":"p-0042","num":"0048"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msup":{"mi":["H","\u2032"]},"mo":"=","mrow":{"mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mrow":{"mi":"Q","mo":"\u2208","mrow":{"msup":{"mi":"R","mrow":{"mi":["D","D"],"mo":"\u00d7"}},"mo":":","mi":"Q"}},"mo":"=","msup":{"mi":["Q","T"]}},{"mrow":{"mo":["(",")"],"mi":"Q"},"mo":"=","mn":"1"},{"mrow":{"mo":["\uf603","\uf604"],"mi":"Q"},"mo":"\u2264","mfrac":{"mn":"1","mrow":{"mi":["D","d"],"mo":"-"}}}],"mo":[",",","]}},"mo":"."}}}},"br":{},"i":"Robust computation of linear models, or How to find a needle in a haystack"},"The underlying subspace may be estimated by the bottom d eigenvectors of the REAPER. The new constraints in H\u2032 result elegant conditions for exact recovery and tighter probabilistic theory (due to the tighter relaxation). Since d is known, any failure mode of GMS is avoided. The REAPER algorithm for computing the REAPER is based on the IRLS procedure described herein with additional constraints, which complicate its analysis.","While the REAPER framework applies a tighter relaxation, the GMS framework still has several advantages over the REAPER framework. First of all, in various practical situations the dimension of the data is unknown and thus REAPER is inapplicable. On the other hand, GMS can be used for dimension estimation, demonstrated below. Second of all, the GMS algorithm described herein is faster than REAPER (the REAPER requires additional eigenvalue decomposition of a D\u00d7D matrix at each iteration of the IRLS algorithm). Third of all, when the failure mode of GMS is avoided, the empirical performances of REAPER and GMS are usually comparable (while GMS is faster). At last, GMS and REAPER have different objectives with different consequences. REAPER aims to find a projector onto the underlying subspace. On the other hand, GMS aims to find a \u201cgeneralized inverse covariance\u201d and is formally similar to other common M-estimators. Therefore, the eigenvalues and eigenvectors of the GMS estimator (i.e., the \u201cgeneralized inverse covariance\u201d) can be interpreted as robust eigenvalues and eigenvectors of the empirical covariance.","Exact and Near Subspace Recovery by the Minimization (4)","Problem Formulation","The formulation of the exact subspace recovery problem, which is a robust measure for the performance of the estimator, is repeated below. Image data  conforms to, in this example, a linear subspace L*\u2208G(D, d) and comprises a data set \u03c7={x}, which contains inliers sampled from L* and outliers sampled from \\L*. Given the data set \u03c7 and no other information, the objective of the exact subspace recovery problem is to exactly recover the underlying subspace L*.","In the noisy case (where inliers do not lie on L*, but perturbed by noise), the techniques investigate near subspace recovery, i.e., recovery up to an error depending on the underlying noise level. In this case, additional information on the model may be needed. In some example implementations, dimensionality d of the subspace may be know estimated from the data.","Common Difficulties with Subspace Recovery",{"@attributes":{"id":"p-0048","num":"0054"},"figref":"FIGS. 2A-2D"},{"@attributes":{"id":"p-0049","num":"0055"},"figref":["FIG. 2A","FIG. 2A"],"b":["30","32","30","30","32","30"],"sup":"3"},{"@attributes":{"id":"p-0050","num":"0056"},"figref":["FIG. 2B","FIG. 2B"],"b":["30","32","32","32"],"sup":"3"},{"@attributes":{"id":"p-0051","num":"0057"},"figref":["FIG. 2C","FIG. 2D","FIG. 2C","FIG. 2D"],"b":["30","32","30","32","32","32"]},"Another issue of exact subspace recovery is whether the subspace obtained by a proposed algorithm is unique. Many of the convex algorithms depend on convex l-type methods that may not be strictly convex. But it may still happen that in the setting of pure inliers and outliers and under some conditions avoiding the three types of enemies, the recovered subspace is unique (even though it may be obtained by several non-unique minimizers). Nevertheless, uniqueness of our minimizer (and not the recovered subspace) is important for analyzing the numerical algorithm approximating it and for perturbation analysis (e.g., when considering near recovery with noisy data).","Conditions for Handling the Three Enemies","Additional assumptions are introduced on the data to address the three types of enemies. The sets of exact inliers and outliers are denoted by \u03c7and \u03c7respectively, i.e., \u03c7=\u03c7\u2229L* and \u03c7=\u03c7\\L*. The following two conditions simultaneously address both type 1 and type 3 enemies:",{"@attributes":{"id":"p-0054","num":"0060"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":[{"msub":{"mi":"min","mrow":{"mrow":[{"mi":["Q","H"],"mo":"\u2208"},{"msub":{"mi":"QP","msup":{"mi":"L","mrow":{"mo":["*","\u22a5"]}}},"mo":"=","mn":"0"}],"mo":","}},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"mi":"x","mo":"\u2208","msub":{"mi":"\u03c7","mn":"1"}}},"mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mrow":{"mi":["Q","x"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}}},{"msqrt":{"mn":"2"},"mo":"\u2062","mrow":{"msub":{"mi":"min","mrow":{"mrow":[{"mi":"v","mo":["\u2208","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mi":"L","mrow":{"mo":["*","\u22a5"]}}},{"mrow":{"mo":["\uf603","\uf604"],"mi":"v"},"mo":"=","mn":"1"}],"mo":","}},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"mi":"x","mo":"\u2208","msub":{"mi":"\u03c7","mn":"0"}}},"mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mrow":{"msup":{"mi":["v","T"]},"mo":"\u2062","mi":"x"}}}}}],"mo":">"},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"6"}}]}}}}},{"@attributes":{"id":"p-0055","num":"0061"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":"min","mrow":{"mrow":[{"mi":["Q","H"],"mo":"\u2208"},{"msub":{"mi":"QP","msup":{"mi":"L","mrow":{"mo":["*","\u22a5"]}}},"mo":"=","mn":"0"}],"mo":","}},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"mi":"x","mo":"\u2208","msub":{"mi":"\u03c7","mn":"1"}}},"mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mrow":{"mi":["Q","x"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}}},{"msqrt":{"mn":"2"},"mo":"\u2062","mrow":{"msub":{"mi":"max","mrow":{"mrow":[{"mi":"v","mo":["\u2208","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mi":"L","mo":"*"}},{"mrow":{"mo":["\uf603","\uf604"],"mi":"v"},"mo":"=","mn":"1"}],"mo":","}},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"mi":"x","mo":"\u2208","msub":{"mi":"\u03c7","mn":"0"}}},"mo":"\u2062","mrow":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"msup":{"mi":["v","T"]},"mo":"\u2062","mi":"x"}},"mo":"."}}}}],"mo":">"}},{"mrow":{"mo":["(",")"],"mn":"7"}}]}}}}},"A lower bound on the common LHS of both (6) and (7) is designed to avoid type 1 enemies. This common LHS is a weak version of the permeance statistics, which can be defined as follows:",{"@attributes":{"id":"p-0057","num":"0063"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":"L","mo":"*"}}},{"msub":{"mi":"min","mrow":{"mrow":{"mrow":[{"mn":"0","mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mi":["p","t","u"]},{"msup":{"mi":"L","mo":"*"},"mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mi":"u"}}],"mo":["\u2062","\u2208","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},"mo":"=","mn":"1"}},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"mi":"x","mo":"\u2208","msub":{"mi":"\u03c7","mn":"1"}}},"mo":"\u2062","mrow":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"msup":{"mi":["u","T"]},"mo":"\u2062","mi":"x"}},"mo":"."}}}],"mo":":="}}}},"Similarly to the permeance statistics, it is zero if and only if all inliers are contained in a proper subspace of L*. Indeed, if all inliers lie in a subspace L\u2032\u2282L*, then this common LHS is zero with the minimizer Q=P\/(P). Similarly, if it is zero, then Qx=0 for any x\u2208\u03c7and for some Q with kernel containing L*. This is only possible when \u03c7is contained in a proper subspace of L*. Similarly to the permeance statistics, if the inliers nicely permeate through L*, then this common LHS clearly obtain large values.","The upper bounds on the RHS's of (6) and (7) address two complementing type 3 enemies. If \u03c7contains few data points of large magnitude, which are orthogonal to L*, then the RHS of (6) may be too large and (6) may not hold. If on the other hand \u03c7contains few data points with large magnitude and a small angle with L*, then the RHS of (7) will be large so that (7) may not hold. Conditions (6) and (7) thus complete each other.","The RHS of condition (7) is similar to the linear structure statistics (for L*), which was defined in (3.3) of LMTZ2012. The linear structure statistics uses an laverage of dot products instead of the laverage used here and was applied in this context to R(instead of L*) in Lerman et al. Similarly to the linear structure statistics, the RHS of (7) is large when outliers either have large magnitude or they lie close to a line (so that their combined contribution is similar to an outlier with a very large magnitude as exemplified in . The RHS of condition (7) is a very weak analog of the linear structure statics of L*since it uses a minimum instead of a maximum. There are some significant outliers within L*that will not be avoided by requiring (7). For example, if the codimension of L* is larger than 1 and there is a single outlier with an arbitrary large magnitude orthogonal to L*, then the RHS of (7) is zero.","The next condition avoids type 2 enemies and also significant outliers within L*(i.e., type 3 enemies) that were not avoided by condition (7). This condition requires that any minimizer of the following oracle problem",{"@attributes":{"id":"p-0062","num":"0068"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mover":{"mi":"Q","mo":"^"},"mn":"0"},"mo":":=","mrow":{"munder":{"mrow":[{"mi":["arg","min"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}},{"mrow":[{"mi":["Q","\u210d"],"mo":"\u2208"},{"mrow":{"mi":"Q","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":"P","mrow":{"mi":"L","mo":"*"}}},"mo":"=","mn":"0"}],"mo":","}]},"mo":"\u2062","mrow":{"mi":"F","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"Q"}}}}},{"mrow":{"mo":["(",")"],"mn":"8"}}]}}}}},"satisfies\n\nrank()=\u2003\u2003(9)\n","The requirement that QP=0 is equivalent to the condition ker(Q)L* and therefore the rank of the minimizer is at most D\u2212d. Enforcing the rank of the minimizer to be exactly D\u2212d restricts the distribution of the projection of \u03c7 onto L*. In particular, it avoids its concentration on lower dimensional subspaces and is thus suitable to avoid type 2 enemies. Indeed, if all outliers are sampled from{tilde over ( )}L\u2282L*, then any Q\u2208H with ker(Q){tilde over (\u2283)}L+L* satisfies F(Q)=0 and therefore it is a minimizer of the oracle problem (4), but it contradicts (9).","This condition also avoids some type 3 enemies, which were not handled by conditions (6) and (7). For example, any D\u2212d\u22121 outliers with large magnitude orthogonal to L* will not be excluded by requiring (6) or (7), but will be avoided by (9).","This condition is restrictive though, especially in very high ambient dimensions. Indeed, it does not hold when the number of outliers is smaller than D\u2212d (since then the outliers are sampled from some{tilde over ( )}L with dim{tilde over (()}L\u2295L*)<D). How to avoid this condition when knowing the dimension is described below. In addition, some practical solutions to overcome the corresponding restrictive lower bound on the number of outliers when the dimension is unknown is also described below.","This example demonstrates the violation of the conditions above for the examples depicted in . The actual calculations rely on the concepts described herein. For example, in  which represents a type 1 enemy, both conditions (6) and (7) are violated. Indeed, the common LHS of (6) and (7) is 5.69, whereas the RHS of (6) is 8.57 and the RHS of (7) is larger than 10.02 (this lower bound is obtained by substituting v=[0,1,0} in the RHS of (7); note that v is a unit vector in L*).","In  which represents a type 2 enemy, condition (9) is violated. Indeed, a solution {circumflex over (Q)}with rank ({circumflex over (Q)})=1\u2260D\u2212d=2 is obtained numerically. It can be proved in this case that {circumflex over (Q)}is the projector onto the orthogonal complement of the plane represented by the dashed rectangle.","In , which represents a type 3 enemy, both conditions (6) and (7) are violated. Indeed, the common LHS of (6) and (7) is 1.56 and the RHS's of (6) and (7) are 5.66 and 4.24 respectively. However, all points are normalized to lie on the unit circle, this enemy can be overcome. Indeed, for the normalized data, the common LHS of (6) and (7) is 6 and the RHS's of (6) and (7) are 1.13 and 0.85 respectively.","In the example of , which also represents a type 3 enemy, both conditions (6) and (7) are violated. Indeed, the LHS of (6) and (7) are 5.99 and the RHS's of (6) and (7) are 6.91 and 7.02 respectively.","Exact Recovery Under Combinatorial Conditions","This section discusses that that the minimizer of (4) solves the exact recovery problem under the above combinatorial conditions.","Theorem 1: Assume that d, D\u2208N, d<D, X is a data set in and L*\u2208G(D, d). If conditions (6), (7) and (9) hold (w.r.t. X and L*), then any minimizer of (4) {circumflex over (Q)}, recovers the subspace L* in the following way: ker{circumflex over (Q)}=L*. If only (6) and (7) hold, then ker{circumflex over (Q)}L*.","Uniqueness of the Minimizer","Theorem 1 implies that if (6), (7) and (9) hold, then ker {circumflex over (Q)} is unique. The uniqueness of is is guaranteed independently of the exact subspace recovery problem.","Theorem 2: If the following condition holds:","{\u03c7\u2229L}\u222a{X\u2229L}\u2260X for all (D\u22121)-dimensional subspace L, L\u2208, then F(Q) is a strictly convex function on .","Weaker Alternatives of Conditions (6) and (7)","It is sufficient to guarantee exact recovery by requiring (9) and that for an arbitrarily chosen solution of (8), {circumflex over (Q)}, the following two conditions are satisfied (as long as they are well-defined):",{"@attributes":{"id":"p-0077","num":"0083"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":"min","mrow":{"mrow":[{"mi":["Q","H"],"mo":["\u2062","\u2208","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},{"msub":{"mi":"QP","msup":{"mi":"L","mrow":{"mo":["*","\u22a5"]}}},"mo":"=","mn":"0"}],"mo":","}},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"mi":"x","mo":"\u2208","msub":{"mi":"\u03c7","mn":"1"}}},"mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mrow":{"mi":["Q","x"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}}},{"msqrt":{"mn":"2"},"mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mrow":{"munder":{"mo":"\u2211","mrow":{"mi":"x","mo":"\u2208","msub":{"mi":"\u03c7","mn":"0"}}},"mo":"\u2062","mrow":{"msub":{"mover":{"mi":"Q","mo":"^"},"mn":"0"},"mo":["\u2062","\u2062"],"msup":{"mi":["xx","T"]},"mrow":{"msub":{"mi":"P","msup":{"mi":"L","mrow":{"mo":["*","\u22a5"]}}},"mo":"\/","mrow":{"mo":["\uf603","\uf604"],"mrow":{"msub":{"mover":{"mi":"Q","mo":"^"},"mn":"0"},"mo":"\u2062","mi":"x"}}}}}}}],"mo":">"}},{"mrow":{"mo":["(",")"],"mn":"10"}}]}}}}},"and",{"@attributes":{"id":"p-0079","num":"0085"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":"min","mrow":{"mrow":[{"mi":["Q","H"],"mo":"\u2208"},{"mrow":{"mi":"Q","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":"P","msup":{"mi":"L","mrow":{"mo":["*","\u22a5"]}}}},"mo":"=","mn":"0"}],"mo":","}},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"mi":"x","mo":"\u2208","msub":{"mi":"\u03c7","mn":"1"}}},"mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mi":"Qx"}}},{"msqrt":{"mn":"2"},"mo":"\u2062","mrow":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"munder":{"mo":"\u2211","mrow":{"mi":"x","mo":"\u2208","msub":{"mi":"\u03c7","mn":"0"}}},"mo":"\u2062","mrow":{"msub":{"mover":{"mi":"Q","mo":"^"},"mn":"0"},"mo":["\u2062","\u2062"],"msup":{"mi":["xx","T"]},"mrow":{"msub":{"mi":"P","msup":{"mi":"L","mo":"*"}},"mo":"\/","mrow":{"mo":["\uf603","\uf604"],"mrow":{"msub":{"mover":{"mi":"Q","mo":"^"},"mn":"0"},"mo":"\u2062","mi":"x"}}}}}},"mo":"."}}],"mo":">"}},{"mrow":{"mo":["(",")"],"mn":"11"}}]}}}},"br":{},"sub":["0","0 "]},"Conditions (11) and (12) can be verified when \u03c7, \u03c7and L* are known (unlike (6) and (7)), where ^Qcan be found by Algorithm 2. Furthermore, (11) and (12) are weaker than (6) and (7), though they are more technically involved and harder to motivate.","In order to demonstrate the near-tightness of (11) and (12), we formulate the following necessary conditions for the recovery of L* as ker(^Q). For an arbitrarily chosen solution of (8), ^Q:",{"@attributes":{"id":"p-0082","num":"0088"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":"min","mrow":{"mrow":[{"mi":["Q","H"],"mo":"\u2208"},{"mrow":{"mi":"Q","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":"P","msup":{"mi":"L","mrow":{"mo":["*","\u22a5"]}}}},"mo":"=","mn":"0"}],"mo":","}},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"mi":"x","mo":"\u2208","msub":{"mi":"\u03c7","mn":"1"}}},"mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mi":"Qx"}}},{"mo":["\uf603","\uf604"],"mrow":{"munder":{"mo":"\u2211","mrow":{"mi":"x","mo":"\u2208","msub":{"mi":"\u03c7","mn":"0"}}},"mo":"\u2062","mrow":{"msub":{"mover":{"mi":"Q","mo":"^"},"mn":"0"},"mo":["\u2062","\u2062"],"msup":{"mi":["xx","T"]},"mrow":{"msub":{"mi":"P","msup":{"mi":"L","mrow":{"mo":["*","\u22a5"]}}},"mo":"\/","mrow":{"mo":["\uf603","\uf604"],"mrow":{"msub":{"mover":{"mi":"Q","mo":"^"},"mn":"0"},"mo":"\u2062","mi":"x"}}}}}}],"mo":">"}},{"mrow":{"mo":["(",")"],"mn":"12"}}]}}}}},"and",{"@attributes":{"id":"p-0084","num":"0090"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":{"mrow":[{"munder":{"mo":"\u2211","mrow":{"mi":"x","mo":"\u2208","msub":{"mi":"\u03c7","mn":"1"}}},"mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mrow":{"mover":{"mi":"Q","mo":"~"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"P","msup":{"mi":"L","mo":"*"}},"mo":"\u2062","mi":"x"}}}}},{"munder":{"mo":"\u2211","mrow":{"mi":"x","mo":"\u2208","msub":{"mi":"\u03c7","mn":"0"}}},"mo":"\u2062","mrow":{"mrow":[{"mo":["\u2329","\u2062"],"mrow":{"mover":{"mi":"Q","mo":"~"},"mo":",","mrow":{"msubsup":{"mi":["P","T"],"msup":{"mi":"L","mrow":{"mo":["*","\u22a5"]}}},"mo":["\u2062","\u2062","\u2062"],"msub":{"mover":{"mi":"Q","mo":"^"},"mn":"0"},"msup":{"mi":["xx","T"]},"mrow":{"msub":{"mi":"P","msup":{"mi":"L","mo":"*"}},"mo":"\/","mrow":{"mo":["\uf603","\uf604"],"mrow":{"msub":{"mover":{"mi":"Q","mo":"^"},"mn":"0"},"mo":"\u2062","mi":"x"}}}}},"munder":{"mo":"\u232a","mi":"F"}},{"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mo":"\u2062"}],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mi":["for","a","n","y","Q"]}}],"mo":"\u2265"},"mo":"\u2208","msup":{"mi":"R","mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":["D","d"],"mo":"-"}},"mo":"\u00d7","mi":"d"}}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"13"}}]}}}},"br":[{},{}],"sup":["k\u00d7l","T"],"img":[{"@attributes":{"id":"CUSTOM-CHARACTER-00013","he":"3.13mm","wi":"1.02mm","file":"US09251438-20160202-P00005.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00014","he":"3.13mm","wi":"1.02mm","file":"US09251438-20160202-P00006.TIF","alt":"custom character","img-content":"character","img-format":"tif"}}],"sub":"F"},"The conditions for exact recovery (or the main two of them) and the condition for uniqueness of the minimizer ^Q hold with high probability under basic probabilistic models. Such a probabilistic theory is cleaner when the outliers are sampled from a spherically symmetric distribution as demonstrate herein (with two different models). One issue is that when the outliers are spherically symmetric then various non-robust algorithms (such as PCA) can asymptotically approach exact recovery and nearly recover the underlying subspace with sufficiently large sample. It is shown herein how the theory presented can be slightly modified to establish exact recovery of the GMS algorithm in an asymmetric case, where PCA cannot even nearly recover the underlying subspace.","Cases with Spherically Symmetric Distributions of Outliers","First, we assume a more general probabilistic model. We say that \u03bc on is an Outliers-Inliers Mixture (OIM) measure (w.r.t. the fixed subspace L*\u2208G(D,d)) if \u03bc=\u03b1\u03bc+\u03b1\u03bc, where \u03b1, \u03b1>0, \u03b1+\u03b1=1, \u03bcis a sub-Gaussian probability measure and \u03bcis a sub-Gaussian probability measure on R(representing outliers) that can be decomposed to a product of two independent measures \u03bc=\u03bc\u00d7\u03bcsuch that the supports of \u03bcand \u03bcare L* and L*respectively, and \u03bcis spherically symmetric with respect to rotations within L*.","To provide cleaner probabilistic estimates, we also invoke a needle-haystack model that assumes that both \u03bcand \u03bcare the Gaussian distributions: \u03bc=N(0, \u03c3I\/D) and \u03bc=N(0, \u03c3PP\/d) (the factors 1\/D and 1\/d normalize the magnitude of outliers and inliers respectively so that their norms are comparable). While this assumes a fixed number of outliers and inliers independently sampled from \u03bcand \u03bcrespectively, here we independently sample from the mixture measure \u03bc=\u03b1\u03bc+\u03b1\u03bc; we refer tog as a needle-haystack mixture measure.","In order to prove exact recovery under any of these models, one needs to restrict the fraction of inliers per outliers (or equivalently, the ratio \u03b1\/\u03b1). We refer to this ratio as SNR (signal to noise ratio) since we may view the inliers as the pure signal and the outliers as some sort of \u201cnoise\u201d. For the needle-haystack model, we require the following SNR:",{"@attributes":{"id":"p-0089","num":"0095"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mfrac":{"msub":[{"mi":"\u03b1","mn":"1"},{"mi":"\u03b1","mn":"0"}]},"mo":">","mrow":{"mn":"4","mo":["\u2062","\u2062"],"mfrac":{"msub":[{"mi":"\u03c3","mn":"0"},{"mi":"\u03c3","mn":"1"}]},"mrow":{"mfrac":{"mi":"d","msqrt":{"mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":["D","d"],"mo":"-"}},"mo":"\u2062","mi":"D"}}},"mo":"."}}}},{"mrow":{"mo":["(",")"],"mn":"15"}}]}}}}},"We later explain how to get rid of the term \u03c3\/\u03c3. For the OIM model we assume the following more general condition:",{"@attributes":{"id":"p-0091","num":"0097"},"maths":{"@attributes":{"id":"MATH-US-00015","num":"00015"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":"\u03b1","mn":"1"},"mo":"\u2062","mrow":{"msub":{"mi":"min","mrow":{"mrow":[{"mi":["Q","H"],"mo":"\u2208"},{"mi":"Q","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":"P","msup":{"mi":"L","mrow":{"mrow":{"mo":["*","\u22a5"]},"mo":"=","mn":"0"}}}}],"mo":","}},"mo":"\u2062","mrow":{"mo":"\u222b","mrow":{"mrow":[{"mo":["\uf603","\uf604"],"mi":"Qx"},{"mo":"\u2146","mrow":{"msub":{"mi":"\u03bc","mn":"1"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}],"mo":"\u2062"}}}},{"mn":"2","mo":["\u2062","\u2062","\u2062"],"msqrt":{"mn":"2"},"mfrac":{"msub":{"mi":"\u03b1","mn":"0"},"mrow":{"mi":["D","d"],"mo":"-"}},"mrow":{"mo":"\u222b","mrow":{"mrow":[{"mo":["\uf603","\uf604"],"mrow":{"msub":{"mi":"P","msup":{"mi":"L","mrow":{"mo":["*","\u22a5"]}}},"mo":"\u2062","mi":"x"}},{"mo":"\u2146","mrow":{"msub":{"mi":"\u03bc","mn":"0"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}],"mo":"\u2062"}}}],"mo":">"}},{"mrow":{"mo":["(",")"],"mn":"16"}}]}}}},"br":{}},"For samples from an OIM measure satisfying (16), we can establish our modified conditions of unique exact recovery (i.e., (11), (12) and (9)) with overwhelming probability in the following way (we also guarantee the uniqueness of the minimizer ^Q).","Theorem 4 If X is an i.i.d. sample from an OIM measure \u03bc satisfying (16), then conditions (11), (12), and (9) hold with probability 1\u2212C exp(\u2212N\/C), where C is a constant depending on \u03bc and its parameters. Moreover, (10) hold with probability 1 if there are at least 2D\u22121 outliers (i.e., the number of points in X\/L* is at least 2D\u22121)","Under the needle-haystack model, the SNR established by Theorem 4 is comparable to the best SNR among other convex exact recovery algorithms. However, the probabilistic estimate under which this SNR holds is rather loose and thus its underlying constant C is not specified. Indeed, the proof of Theorem 4 uses \u03b5-nets and union-bounds arguments, which are often not useful for deriving tight probabilistic estimates. One may thus view Theorem 4 as a near-asymptotic statement.","The statement of Theorem 4 does not contradict our previous observation that the number of outliers should be larger than at least D\u2212d. Indeed, the constant C is sufficiently large so that the corresponding probability is negative when the number of outliers is smaller than D\u2212d.","In the next theorem, we assume only a needle-haystack model and thus we can provide a stronger probabilistic estimate based on the concentration of measure phenomenon. However, the SNR is worse than the one in Theorem 4 by a factor of order \u221a{square root over (D\u2212d)}. This is because we are unable to estimate ^Qof (8) by concentration of measure. Similarly, in this theorem we do not estimate the probability of (9) (which also involves ^Q). Nevertheless, we observed in experiments that (9) holds with high probability for N=2(D\u2212d) and the probability seems to go to 1 as N=2(D\u2212d) and D\u2212d\u2192\u221e. Moreover, one of the algorithms proposed below (EGMS) does not require condition (9).","Theorem 5 If X is an i.i.d. sample of size N from a needle-haystack mixture measure \u03bc and if",{"@attributes":{"id":"p-0098","num":"0104"},"maths":{"@attributes":{"id":"MATH-US-00016","num":"00016"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mfrac":{"msub":[{"mi":"\u03b1","mn":"1"},{"mi":"\u03b1","mn":"0"}]},"mo":">","mrow":{"mfrac":[{"msub":[{"mi":"\u03c3","mn":"0"},{"mi":"\u03c3","mn":"1"}]},{"mrow":[{"msqrt":{"mrow":{"mn":"2","mo":"\/","mi":"\u03c0"}},"mo":["-","-"],"mrow":[{"mn":["1","4"],"mo":"\/"},{"mn":["1","10"],"mo":"\/"}]},{"msqrt":{"mrow":{"mn":"2","mo":"\/","mi":"\u03c0"}},"mo":["+","+"],"mrow":[{"mn":["1","4"],"mo":"\/"},{"mn":["1","10"],"mo":"\/"}]}]}],"mo":["\u2062","\u2062"],"msqrt":{"mfrac":{"msup":{"mi":"d","mn":"2"},"mi":"D"}}}}},{"mrow":{"mo":["(",")"],"mn":"17"}}]}}}}},"And\n\n64 max(2, 2, 2()\/\u03b1),\u2003\u2003(18)\n\nThen (6) and (7) hold with probability 1\u2212e\u22122e\u2212e.\n\nA Special Case with Asymmetric Outliers\n","In the case of spherically symmetric outliers, PCA cannot exactly recover the underlying subspace, but it can asymptotically recover it. In particular, with sufficiently large sample with spherically symmetric outliers, PCA nearly recovers the underlying subspace. We thus slightly modify the two models described above so that the distribution of outliers is asymmetric and show that our combinatorial conditions for exact recovery still hold (with overwhelming probability). On the other hand, the subspace recovered by PCA, when sampling data from these models, is sufficiently far from the underlying subspace for any given sample size.","We first generalize Theorem 5 under a generalized needle-haystack model: Let \u03bc=\u03b1\u03bc+\u03b1\u03bc,\u03bc=N(0, \u03a3\/D), where \u03a3is an arbitrary positive definite matrix (not necessarily a scalar matrix as before), and as before \u03bc=N(0, \u03c3PP\/d). We claim that Theorem 5 still holds in this case if we replace \u03c3in the RHS of (17) with \u221a{square root over (\u03bb(\u03a3))}, where \u03bb(\u03a3) denotes the largest eigenvalue of \u03a3.","In order to generalize Theorem 4 for asymmetric outliers, we assume that the outlier component \u03bcof the OIM measure \u03bc is a sub-Gaussian distribution with an arbitrary positive definite covariance matrix \u03a3. Following Coudron and Lerman (2012), we define the expected version of F, Fand its oracle minimizer, ^Qwhich is analogous to (8) (the subscript I indicates integral):\n\n()=\u222b|()\u2003\u2003(19)\n","and\n\n^Q=().\u2003\u2003(20)\n","We assume that ^Qis the unique minimizer in (20) (we remark that the two-subspaces criterion in (25) for the projection of \u03bc onto L*implies this assumption). Under these assumptions Theorem 4 still holds if we multiply the RHS of (16) by the ratio between the largest eigenvalue of P^QPand the (D\u2212d)th eigenvalue of P^QP.","Near Subspace Recovery for Noisy Samples","We show that in the case of sufficiently small additive noise (i.e., the inliers do not lie exactly on the subspace L* but close to it), the GMS algorithm nearly recovers the underlying subspace.","We use the following notation: \u2225A\u2225and \u2225A\u2225 denote the Frobenius and spectral norms of A\u2208respectively. Furthermore, denotes the set of all positive semidefinite matrices in , that is, H={Q\u2208H:Q\u22670}. We also define the following two constants",{"@attributes":{"id":"p-0107","num":"0113"},"maths":{"@attributes":{"id":"MATH-US-00017","num":"00017"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msub":{"mi":"\u03b3","mn":"0"},"mo":"=","mrow":{"mfrac":{"mn":"1","mi":"N"},"mo":"\u2062","mrow":{"msub":{"mi":"min","mrow":{"mrow":[{"mi":"Q","mo":"\u2208","msub":{"mi":"H","mn":"1"}},{"msub":{"mrow":{"mo":["\uf603","\uf604"],"mi":"\u0394"},"mi":"F"},"mo":"=","mn":"1"},{"mrow":{"mo":["(",")"],"mi":"\u0394"},"mo":"=","mn":"0"}],"mo":[",",","]}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mfrac":{"mrow":{"mrow":{"msup":[{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mi":"\u0394","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["x","i"]}}},"mn":"2"},{"mrow":{"mo":["\uf603","\uf604"],"msub":{"mi":["Qx","i"]}},"mn":"2"}],"mo":"\u2062"},"mo":"-","msup":{"mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["x","i","T"]},"mo":["\u2062","\u2062","\u2062"],"mi":"\u0394","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["Qx","i"]}}},"mn":"2"}},"msup":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mi":"Q","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["x","i"]}}},"mn":"3"}}}}}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"21"}}]}}}}},"and",{"@attributes":{"id":"p-0109","num":"0115"},"maths":{"@attributes":{"id":"MATH-US-00018","num":"00018"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msubsup":{"mi":["\u03b3","\u2032"],"mn":"0"},"mo":"=","mrow":{"mfrac":{"mn":"1","mi":"N"},"mo":"\u2062","mrow":{"msub":{"mi":"min","mrow":{"mrow":[{"mi":"Q","mo":"\u2208","msub":{"mi":"H","mn":"1"}},{"mrow":{"mo":["\uf603","\uf604"],"mi":"\u0394"},"mo":"=","mn":"1"},{"mrow":{"mo":["(",")"],"mi":"\u0394"},"mo":"=","mn":"0"}],"mo":[",",","]}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mrow":{"mfrac":{"mrow":{"mrow":{"msup":[{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mi":"\u0394","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["x","i"]}}},"mn":"2"},{"mrow":{"mo":["\uf603","\uf604"],"msub":{"mi":["Qx","i"]}},"mn":"2"}],"mo":"\u2062"},"mo":"-","msup":{"mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["x","i","T"]},"mo":["\u2062","\u2062","\u2062"],"mi":"\u0394","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["Qx","i"]}}},"mn":"2"}},"msup":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mi":"Q","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["x","i"]}}},"mn":"3"}},"mo":"."}}}}}},{"mrow":{"mo":["(",")"],"mn":"22"}}]}}}}},"The sum in the RHS's of (21) and (22) is the following second directional derivative: d\/dtF(Q+t\u0394); when Qx=0, its ith term can be set to 0. It is interesting to note that both (21) and (22) express a Restricted Strong Convexity (RSC) parameter \u03b3. The difference between \u03b3and \u03b3\u2032 of (21) and (22) is due to the choice of either the Frobenius or the spectral norms respectively for measuring the size of \u0398\u2212\u0398\u2032.","Using this notation, we formulate our noise perturbation result as follows.","Theorem 6 Assume that {\u03b5}is a set of positive numbers, X={X}and Let F(Q) and F(Q) denote the corresponding versions of F(Q) w.r.t. the sets X and {tilde over (X)} and let {circumflex over (Q)} and {tilde over (Q)} denote their respective minimizers. Then we have",{"@attributes":{"id":"p-0113","num":"0119"},"maths":{"@attributes":{"id":"MATH-US-00019","num":"00019"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mover":[{"mi":"Q","mo":"~"},{"mi":"Q","mo":"^"}],"mo":"-"}},"mi":"F"},"mo":"<","mrow":{"msqrt":{"mrow":{"mrow":[{"mn":"2","mo":"\u2062","munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"}},{"mo":"\/","mrow":{"mo":["(",")"],"msub":{"mi":"N","mrow":{"mi":"\u03b3","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"0"}}}}],"mo":["\u2062","\u2062"],"msub":{"mo":"\u2208","mn":"1"}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},"mi":"and"}},{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mover":[{"mi":"Q","mo":"~"},{"mi":"Q","mo":"^"}],"mo":"-"}},"mo":"<","msqrt":{"mrow":{"mrow":[{"mn":"2","mo":"\u2062","munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"}},{"mo":"\/","mrow":{"mo":["(",")"],"msub":{"mi":"N","mrow":{"msup":{"mi":["\u03b3","\u2032"]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"0"}}}}],"mo":["\u2062","\u2062"],"msub":{"mo":"\u2208","mn":"1"}}}}],"mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mtext":{}}]}},{"mrow":{"mo":["(",")"],"mn":"23"}}]}}}}},"Moreover, if {tilde over (L)} and {circumflex over (L)} are the subspaces spanned by the bottom d eigenvectors of {circumflex over (Q)} and {tilde over (Q)} respectively and vis the (D\u2212d)th eigengap of {circumflex over (Q)}, then",{"@attributes":{"id":"p-0115","num":"0121"},"maths":{"@attributes":{"id":"MATH-US-00020","num":"00020"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":[{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msub":[{"mi":"P","mover":{"mi":"L","mo":"^"}},{"mi":"P","mover":{"mi":"L","mo":"~"}}],"mo":"-"}},"mo":"\u2062","mi":"F"},{"mfrac":{"mroot":{"mrow":{"mrow":[{"mn":"2","mo":"\u2062","munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"}},{"mo":"\/","mrow":{"mo":"(","msub":{"mi":["N","\u03b30"]}}}],"mo":["\u2062","\u2062"],"msub":{"mo":"\u2208","mi":"i"}},"mn":"2"},"msub":{"mi":"v","mrow":{"mi":["D","d"],"mo":"-"}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mi":"and"}],"mo":"\u2264"},{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msub":[{"mi":"P","mover":{"mi":"L","mo":"^"}},{"mi":"P","mover":{"mi":"L","mo":"~"}}],"mo":"-"}},"mo":"\u2264","mfrac":{"mroot":{"mrow":{"mrow":[{"mn":"2","mo":"\u2062","munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"}},{"mo":"\/","mrow":{"mo":["(",")"],"msub":{"mi":"N","mrow":{"msup":{"mi":["\u03b3","\u2032"]},"mo":"\u2062","mn":"0"}}}}],"mo":["\u2062","\u2062"],"msub":{"mo":"\u2208","mi":"i"}},"mn":"2"},"msub":{"mi":"v","mrow":{"mi":["D","d"],"mo":"-"}}}}],"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}}}},{"mrow":{"mo":["(",")"],"mn":"24"}}]}}}}},"We note that if \u03c7and {tilde over ( )}\u03c7 satisfy the conditions of Theorem 6, then given the perturbed data set {tilde over ( )}\u03c7 and the dimension d, Theorem 6 guarantees that GMS nearly recovers L*. More interestingly, the theorem also implies that we may properly estimate the dimension of the underlying subspace in this case. Such dimension estimation is demonstrated later in .","Theorem 6 is a perturbation result. We note that asymptotically the bounds on the recovery errors in (23) and (24) depend only on the empirical mean of {\u03b5}and do not grow with N. To clarify this point we formulate the following proposition.","Proposition 7 If X is i.i.d. sampled from a bounded distribution \u03bc and\n\n\u03bc()+\u03bc()<1 for any two D-1-dimensional subspaces and ,\u2003\u2003(25)\n\nThen there exists constants c(\u03bc)>0 and c\u2032(\u03bc)>0 depending on \u03bc such that\n",{"@attributes":{"id":"p-0119","num":"0125"},"maths":{"@attributes":{"id":"MATH-US-00021","num":"00021"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":[{"msubsup":{"mo":"\u2009","mrow":[{"mi":["N","\u221e"],"mo":"->"},{"mi":["lim","inf"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}]},"mo":"\u2062","mi":"\u03b3"},{"mo":["(",")"],"mi":"X"}],"mo":["\u2062","\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"0"},{"mrow":[{"msub":{"mi":"c","mn":"0"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03bc"}},{"mo":["(",")"],"mi":"X"}],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"and","mmultiscripts":{"mi":["\u03b3","\u2032"],"none":{},"mprescripts":{},"mrow":[{"mi":["N","\u221e"],"mo":"->"},{"mi":["lim","inf"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}]},"mn":"0"},{"mrow":[{"msubsup":{"mi":["c","\u2032"],"mn":"0"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03bc"}},{"mi":"surely","mo":"."}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"almost"}],"mo":["\u2265","\u2265"]}},{"mrow":{"mo":["(",")"],"mn":"26"}}]}}}}},"We first note that the bounds in Theorem 6 are generally not optimal. Indeed, if \u03b5=O(\u03b5) for all 1\u2266i\u2266N, then the error bounds in Theorem 6 are O(\u221a{square root over (\u03b5)}), whereas we empirically noticed that these error bounds are O(\u03b5).","The dependence of the error on D, which follows from the dependence of \u03b3and \u03b3\u2032 on D, is a difficult problem and strongly depends on the underlying distribution of \u03c7 and of the noise. For example, in the very special case where the set \u03c7 is sampled from a subspace L\u2208Rof dimension D<D, and the noise distribution is such that {tilde over ( )}\u03c7 also lies in L, then practically we are performing GMS over P(\u03c7) and P{tilde over ( )}(\u03c7), and the bound in (23) would depend on Dinstead of D.","Near Subspace Recovery for Regularized Minimization","For our practical algorithm it is advantageous to regularize the function F as follows (see Theorems 11 and 12 below):",{"@attributes":{"id":"p-0123","num":"0129"},"maths":{"@attributes":{"id":"MATH-US-00022","num":"00022"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["f","\u03b4"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"Q"}},{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mrow":[{"mi":"i","mo":"=","mn":"1"},{"mrow":{"mo":["\uf605","\uf606"],"msub":{"mi":["Qx","i"]}},"mo":"\u2265","mi":"\u03b4"}],"mo":","},"mi":"N"},"mo":"\u2062","mrow":{"mo":["\uf605","\uf606"],"msub":{"mi":["Qx","i"]}}},{"munderover":{"mo":"\u2211","mrow":{"mrow":[{"mi":"i","mo":"=","mn":"1"},{"mrow":{"mo":["\uf605","\uf606"],"msub":{"mi":["Qx","i"]}},"mo":"<","mi":"\u03b4"}],"mo":","},"mi":"N"},"mo":"\u2062","mrow":{"mrow":{"mo":["(",")"],"mrow":{"mfrac":[{"msup":{"mrow":{"mo":["\uf605","\uf606"],"msub":{"mi":["Qx","i"]}},"mn":"2"},"mrow":{"mn":"2","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"\u03b4"}},{"mi":"\u03b4","mn":"2"}],"mo":"+"}},"mo":"."}}],"mo":"+"}],"mo":":="}},{"mrow":{"mo":["(",")"],"mn":"27"}}]}}}}},"In order to address the regularization in our case and conclude that the GMS algorithm nearly recovers L* for the regularized objective function, we adopt a perturbation procedure. We denote by ^Qand ^Q the minimizers of F(Q) and F(Q) in H respectively. Furthermore, let ^Land ^L denote the subspaces recovered by the bottom d eigenvectors of ^Qand ^Q respectively. Using the constants vand \u03b3of Theorem 6, the difference between the two minimizers and subspaces can be controlled as follows.","Theorem 8 If X is a data set satisfying (10), then\n\n<\u221a{square root over (\u03b4\/2)}\u2003\u2003(28)\n\nand\n",{"@attributes":{"id":"p-0126","num":"0132"},"maths":{"@attributes":{"id":"MATH-US-00023","num":"00023"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msub":[{"mi":"P","msub":{"mover":{"mi":"L","mo":"^"},"mi":"\u03b4"}},{"mi":"P","mover":{"mi":"L","mo":"^"}}],"mo":"-"}},"mi":"F"},"mo":"\u2264","mrow":{"mfrac":{"mrow":{"mn":"2","mo":"\u2062","msqrt":{"mrow":{"mi":"\u03b4","mo":"\/","msub":{"mn":"2","mrow":{"mi":"\u03b3","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"0"}}}}},"msub":{"mi":"v","mrow":{"mi":["D","d"],"mo":"-"}}},"mo":"."}}},{"mrow":{"mo":["(",")"],"mn":"29"}}]}}}}},"Interpretation of Our M-Estimator","Formal Similarity with the Common M-estimator for Robust Covariance Estimation","A robust M-estimator for the 0-centered covariance matrix minimizes the following function over all D\u00d7D positive definite matrices (for some choices of a function \u03c1)",{"@attributes":{"id":"p-0129","num":"0135"},"maths":{"@attributes":{"id":"MATH-US-00024","num":"00024"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"A"}},{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mrow":{"mi":"\u03f1","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["x","i","T"]},"mo":["\u2062","\u2062"],"msup":{"mi":"A","mrow":{"mo":"-","mn":"1"}},"msub":{"mi":["x","i"]}}}}},{"mfrac":{"mi":"N","mn":"2"},"mo":"\u2062","mrow":{"mrow":{"mi":"log","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"det","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":"A","mrow":{"mo":"-","mn":"1"}}}}}},"mo":"."}}],"mo":"-"}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"30"}}]}}}},"br":{}},"If we set \u03c1(x)=\u221a{square root over (x)} and A=Qthen the objective function L(A) in (30) is \u03a3\u2225Q\u2225\u2212N log|Q|. This energy function is formally similar to our energy function. Indeed, using Lagrangian formulation, the minimizer ^Q in (4) is also the minimizer of \u03a3\u2225Q\u2225\u2212\u03bbE log|Q| among all D\u00d7D symmetric matrices (or equivalently nonnegative symmetric matrices) for some \u03bb>0 (the parameter \u03bb only scales the minimizer and does not effect the recovered subspace). Therefore, the two objective functions differ by their second terms. In the common M-estimator (with \u03c1(x)=\u221a{square root over (x)} and A=Q) it is log(det(Q)), or equivalently, (log(Q)), where in our M-estimator, it is (Q).","Problems with Exact Recovery for the Common M-estimator","The common M-estimator is designed for robust covariance estimation, however, we show here that in general it cannot exactly recover the underlying subspace. To make this statement more precise we recall the following uniqueness and existence conditions for the minimizer of (30): 1) \u03bc=2\u03c1\u2032 is positive, continuous and non-increasing. 2) Condition M: u(x)x is strictly increasing. 3) Condition D: For any linear subspace L:",{"@attributes":{"id":"p-0132","num":"0138"},"maths":{"@attributes":{"id":"MATH-US-00025","num":"00025"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mi":["\u03c7","L"],"mo":"\u22c2"}},"mo":"\/","mi":"N"},{"mn":"1","mo":"-","mrow":{"mrow":[{"mrow":[{"mo":["(",")"],"mrow":{"mi":"D","mo":"-","mrow":{"mi":"dim","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"L"}}}},{"msubsup":{"mo":"\u2009","mrow":{"mi":["x","\u221e"],"mo":"->"},"mi":"lim"},"mo":"\u2062","mi":"x"}],"mo":"\/"},{"mrow":{"mi":"u","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}},"mo":"."}],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}],"mo":"<"}}},"br":{}},"Theorem 9 Assume that d, D\u2208N, d<D, X is a data set in and L*\u2208G(D, d) and let \u00c2 be the minimizer of (30). If conditions M and Dhold, then Im(\u00c2)\u2260L*.","For symmetric outliers the common M-estimator can still asymptotically achieve exact recovery (similarly to PCA). However, for many scenarios of asymmetric outliers, in particular, the subspace recovered by the common M-estimator is sufficiently far from the underlying subspace for any given sample size.","Interpretation of ^Q as Robust Inverse Covariance Estimator","The total least squares subspace approximation is practically the minimization over L\u2208G(D,d) of the function",{"@attributes":{"id":"p-0136","num":"0142"},"maths":{"@attributes":{"id":"MATH-US-00026","num":"00026"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","msup":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"msub":{"mi":["x","i"]},"mo":"-","mrow":{"msub":[{"mi":["P","L"]},{"mi":["x","i"]}],"mo":"\u2062"}}},"mn":"2"}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mrow":{"msup":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"msub":[{"mi":"P","msup":{"mi":"L","mo":"\u22a5"}},{"mi":["x","i"]}],"mo":"\u2062"}},"mn":"2"},"mo":"."}}],"mo":"\u2261"}},{"mrow":{"mo":["(",")"],"mn":"31"}}]}}}},"br":{},"sup":"T"},{"@attributes":{"id":"p-0137","num":"0143"},"maths":{"@attributes":{"id":"MATH-US-00027","num":"00027"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":[{"mover":{"mi":"Q","mo":"^"},"mn":"2"},{"mo":":=","mrow":{"mi":["Q","H"],"mo":"\u2208"}}],"mo":["\u2062","\u2062"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mrow":{"msup":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mi":"Q","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["x","i"]}}},"mn":"2"},"mo":"."}}}},{"mrow":{"mo":["(",")"],"mn":"32"}}]}}}},"br":{}},"We show here that ^Qcoincides with a scaled version of the empirical inverse covariance matrix. This clearly implies that the \u201crelaxed\u201d total least squared subspace coincides with the original one (as the bottom eigenvectors of the inverse empirical covariance are the top eigenvectors of the empirical covariance). We require though that the data is of full rank so that the empirical inverse covariance is well-defined. This requirement does not hold if the data points are contained within a lower-dimensional subspace, in particular, if their number is smaller than the dimension. We can easily avoid this restriction by initial projection of the data points onto the span of eigenvectors of the covariance matrix with nonzero eigenvalues. That is, by projecting the data onto the lowest-dimensional subspace containing it without losing any information.","Theorem 10 If X is the data matrix,\n\n=()(())\u2003\u2003(33)\n","We view (4) as a robust version of (32). Since we verified robustness of the subspace recovered by (4) and also showed that (32) yields the inverse covariance matrix, we sometimes refer to the solution of (4) as a robust inverse covariance matrix (though we have only verified robustness to subspace recovery). This idea helps us interpret our numerical procedure for minimizing (4), which we present below.","IRLS Algorithms for Minimizing (4)","Heuristic Proposal for Two IRLS Algorithms","The procedure for minimizing ( )formally follows from the simple fact that the directional derivative of F at ^Q in any direction ^Q\u2212^Q, where {tilde over ( )}Q\u2208H, is 0, i.e.,",{"@attributes":{"id":"p-0143","num":"0149"},"maths":{"@attributes":{"id":"MATH-US-00028","num":"00028"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":{"mo":["\u2329","\u232a"],"mrow":{"mrow":[{"mi":"F","mo":"\u2061","mrow":{"mo":["(",")"],"mover":{"mi":"Q","mo":"^"}}},{"mo":",","mrow":{"mover":[{"mi":"Q","mo":"~"},{"mi":"Q","mo":"^"}],"mo":"-"}}],"mo":["\u2062","\u2062"],"msub":{"mo":"|","mrow":{"mi":"Q","mo":"=","mover":{"mi":"Q","mo":"^"}}}}},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"munder":{"mrow":{"mrow":{"mo":"=","mn":"0"},"mo":"\u2062","mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}},"mi":"F"},"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":["for","any"],"mover":{"mi":"Q","mo":"~"}},{"mi":"H","mo":"."}],"mo":"\u2208"}},{"mrow":{"mo":["(",")"],"mn":"34"}}]}}}},"br":{},"img":[{"@attributes":{"id":"CUSTOM-CHARACTER-00020","he":"3.13mm","wi":"2.79mm","file":"US09251438-20160202-P00004.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00021","he":"3.13mm","wi":"2.79mm","file":"US09251438-20160202-P00004.TIF","alt":"custom character","img-content":"character","img-format":"tif"}}]},"We formally differentiate (4) at ^Q as follows (see more details in (45), which appears later):",{"@attributes":{"id":"p-0145","num":"0151"},"maths":{"@attributes":{"id":"MATH-US-00029","num":"00029"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":{"mi":"F","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"Q"}},"mo":"\u2062","msub":{"mo":"|","mrow":{"mi":"Q","mo":"=","mover":{"mi":"Q","mo":"^"}}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mrow":{"mfrac":{"mrow":[{"mrow":[{"mover":{"mi":"Q","mo":"^"},"mo":["\u2062","\u2062"],"msub":{"mi":["x","i"]},"msubsup":{"mi":["x","i","T"]}},{"msub":{"mi":["x","i"]},"mo":["\u2062","\u2062"],"msubsup":{"mi":["x","i","T"]},"mover":{"mi":"Q","mo":"^"}}],"mo":"+"},{"mn":"2","mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mrow":{"mover":{"mi":"Q","mo":"^"},"mo":"\u2062","msub":{"mi":["x","i"]}}}}]},"mo":"."}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"35"}}]}}}},"br":{},"sub":"i"},"Since F(^Q) is symmetric and {tilde over ( )}Q\u2212^Q can be any symmetric matrix with trace 0, it is easy to note that (34) implies that F(^Q) is a scalar matrix (e.g., multiply it by a basis of symmetric matrices with trace 0 whose members have exactly 2 nonzero matrix elements). That is,",{"@attributes":{"id":"p-0147","num":"0153"},"maths":{"@attributes":{"id":"MATH-US-00030","num":"00030"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mfrac":{"mrow":[{"mrow":[{"mover":{"mi":"Q","mo":"^"},"mo":["\u2062","\u2062"],"msub":{"mi":["x","i"]},"msubsup":{"mi":["x","i","T"]}},{"msub":{"mi":["x","i"]},"mo":["\u2062","\u2062"],"msubsup":{"mi":["x","i","T"]},"mover":{"mi":"Q","mo":"^"}}],"mo":"+"},{"mn":"2","mo":"\u2062","mrow":{"mo":["\uf605","\uf606"],"mrow":{"mover":{"mi":"Q","mo":"^"},"mo":"\u2062","msub":{"mi":["x","i"]}}}}]}},{"mi":["c","I"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"36"}}]}}}},"br":{},"img":{"@attributes":{"id":"CUSTOM-CHARACTER-00022","he":"3.13mm","wi":"2.46mm","file":"US09251438-20160202-P00001.TIF","alt":"custom character","img-content":"character","img-format":"tif"}}},{"@attributes":{"id":"p-0148","num":"0154"},"maths":{"@attributes":{"id":"MATH-US-00031","num":"00031"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mover":{"mi":"Q","mo":"^"},"mo":"=","mrow":{"msup":{"mrow":[{"mi":"c","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mfrac":{"mrow":[{"msub":{"mi":["x","i"]},"mo":"\u2062","msubsup":{"mi":["x","i","T"]}},{"mo":["\uf603","\uf604"],"mrow":{"mover":{"mi":"Q","mo":"^"},"mo":"\u2062","msub":{"mi":["x","i"]}}}]}}}},{"mo":"-","mn":"1"}]},"mo":"."}}},{"mrow":{"mo":["(",")"],"mn":"37"}}]}}}}},"Indeed, we can easily verify that (37) solves (6), furthermore, (36) is a Lyapunov equation whose solution is unique. Since (^Q)=1, we obtain that",{"@attributes":{"id":"p-0150","num":"0156"},"maths":{"@attributes":{"id":"MATH-US-00032","num":"00032"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mover":{"mi":"Q","mo":"^"},"mo":"=","mrow":{"msup":{"mrow":[{"mo":["(",")"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mfrac":{"mrow":[{"msub":{"mi":["x","i"]},"mo":"\u2062","msubsup":{"mi":["x","i","T"]}},{"mo":["\uf603","\uf604"],"mrow":{"mover":{"mi":"Q","mo":"^"},"mo":"\u2062","msub":{"mi":["x","i"]}}}]}}},{"mo":"-","mn":"1"}]},"mo":"\/","mrow":{"mo":["(",")"],"msup":{"mrow":[{"mo":["(",")"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mfrac":{"mrow":[{"msub":{"mi":["x","i"]},"mo":"\u2062","msubsup":{"mi":["x","i","T"]}},{"mo":["\uf603","\uf604"],"mrow":{"mover":{"mi":"Q","mo":"^"},"mo":"\u2062","msub":{"mi":["x","i"]}}}]}}},{"mo":"-","mn":"1"}]}}}},"mo":","}}},"br":{}},{"@attributes":{"id":"p-0151","num":"0157"},"maths":{"@attributes":{"id":"MATH-US-00033","num":"00033"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":"Q","mrow":{"mi":"k","mo":"+","mn":"1"}},"mo":"=","mrow":{"msup":{"mrow":[{"mo":["(",")"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mfrac":{"mrow":[{"msub":{"mi":["x","i"]},"mo":"\u2062","msubsup":{"mi":["x","i","T"]}},{"mo":["\uf603","\uf604"],"mrow":{"msub":[{"mi":["Q","k"]},{"mi":["x","i"]}],"mo":"\u2062"}}]}}},{"mo":"-","mn":"1"}]},"mo":"\/","mrow":{"mrow":{"mo":["(",")"],"msup":{"mrow":[{"mo":["(",")"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mfrac":{"mrow":[{"msub":{"mi":["x","i"]},"mo":"\u2062","msubsup":{"mi":["x","i","T"]}},{"mo":["\uf603","\uf604"],"mrow":{"msub":[{"mi":["Q","k"]},{"mi":["x","i"]}],"mo":"\u2062"}}]}}},{"mo":"-","mn":"1"}]}},"mo":"."}}}},{"mrow":{"mo":["(",")"],"mn":"38"}}]}}}}},"Formula (38) is undefined whenever Qx=0 for some k\u2208N and 1\u2266i\u2266N. In theory, we address it as follows. Let I(Q)={1\u2266i\u2266N:Qx=0}, L(Q)={x}and",{"@attributes":{"id":"p-0153","num":"0159"},"maths":{"@attributes":{"id":"MATH-US-00034","num":"00034"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"T","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"Q"}},{"msup":{"mrow":[{"msub":{"mi":"P","msup":{"mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"Q"}},"mo":"\u22a5"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"munder":{"mo":"\u2211","mrow":{"mi":"i","mo":"\u2209","mrow":{"mi":"I","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"Q"}}}},"mo":"\u2062","mfrac":{"mrow":[{"msub":{"mi":["x","i"]},"mo":"\u2062","msubsup":{"mi":["x","i","T"]}},{"mo":["\uf603","\uf604"],"mrow":{"mi":"Q","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["x","i"]}}}]}}}},{"mo":"-","mn":"1"}]},"mo":"\u2062","mrow":{"msub":{"mi":"P","msup":{"mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"Q"}},"mo":"\u22a5"}},"mo":"\/","mrow":{"mrow":{"mo":["(",")"],"mrow":{"msup":{"mrow":[{"msub":{"mi":"P","msup":{"mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"Q"}},"mo":"\u22a5"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"munder":{"mo":"\u2211","mrow":{"mi":"i","mo":"\u2209","mrow":{"mi":"I","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"Q"}}}},"mo":"\u2062","mfrac":{"mrow":[{"msub":{"mi":["x","i"]},"mo":"\u2062","msubsup":{"mi":["x","i","T"]}},{"mo":["\uf603","\uf604"],"msub":{"mi":["Qx","i"]}}]}}}},{"mo":"-","mn":"1"}]},"mo":"\u2062","msub":{"mi":"P","msup":{"mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"Q"}},"mo":"\u22a5"}}}},"mo":"."}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"39"}}]}}}}},"Using this notation, the iterative formula can be corrected as follows\n\n().\u2003\u2003(40)\n\nIn practice, we can avoid data points satisfying |Qx|\u2266\u03b4 for a sufficiently small parameter \u03b4 (instead of |Qx|=0). We follow a similar idea by replacing F with the regularized function Ffor a regularized parameter \u03b4. In this case, (40) obtains the following form:\n",{"@attributes":{"id":"p-0155","num":"0161"},"maths":{"@attributes":{"id":"MATH-US-00035","num":"00035"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":"Q","mrow":{"mi":"k","mo":"+","mn":"1"}},"mo":"=","mrow":{"msup":{"mrow":[{"mo":["(",")"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mfrac":{"mrow":[{"msub":{"mi":["x","i"]},"mo":"\u2062","msubsup":{"mi":["x","i","T"]}},{"mi":"max","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"msub":[{"mi":["Q","k"]},{"mi":["x","i"]}],"mo":"\u2062"}},"mo":",","mi":"\u03b4"}}}]}}},{"mo":"-","mn":"1"}]},"mo":"\/","mrow":{"mo":["(",")"],"msup":{"mrow":[{"mo":["(",")"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mfrac":{"mrow":[{"msub":{"mi":["x","i"]},"mo":"\u2062","msubsup":{"mi":["x","i","T"]}},{"mi":"max","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"msub":[{"mi":["Q","k"]},{"mi":["x","i"]}],"mo":"\u2062"}},"mo":",","mi":"\u03b4"}}}]}}},{"mo":"-","mn":"1"}]}}}}},{"mrow":{"mo":["(",")"],"mn":"41"}}]}}}},"br":{}},"The two iterative formulas, i.e., (40) and (41), give rise to IRLS algorithms. For simplicity of notation, we exemplify this idea with the formal expression in (38). It iteratively finds the solution to the following weighted (with weight 1\/|Qx|) least squares problem:",{"@attributes":{"id":"p-0157","num":"0163"},"maths":{"@attributes":{"id":"MATH-US-00036","num":"00036"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"munder":{"mrow":[{"mi":["arg","min"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mi":["Q","H"],"mo":"\u2208"}]},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mrow":{"mfrac":{"mn":"1","mrow":{"mo":["\uf605","\uf606"],"mrow":{"msub":[{"mi":["Q","k"]},{"mi":["x","i"]}],"mo":"\u2062"}}},"mo":"\u2062","mrow":{"msup":{"mrow":{"mo":["\uf605","\uf606"],"msub":{"mi":["Qx","i"]}},"mn":"2"},"mo":"."}}}}},{"mrow":{"mo":["(",")"],"mn":"42"}}]}}}},"br":{},"sub":"k+i "},{"@attributes":{"id":"p-0158","num":"0164"},"maths":{"@attributes":{"id":"MATH-US-00037","num":"00037"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mrow":{"mfrac":{"mo":"\u2146","mrow":{"mo":"\u2146","mi":"Q"}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mrow":{"mfrac":{"mn":"1","mrow":{"mo":["\uf603","\uf604"],"mrow":{"msub":[{"mi":["Q","k"]},{"mi":["x","i"]}],"mo":"\u2062"}}},"mo":"\u2062","msup":{"mrow":{"mo":["\uf603","\uf604"],"msub":{"mi":["Qx","i"]}},"mn":"2"}}}},"mo":"\u2062","msub":{"mo":"|","mrow":{"mi":"Q","mo":"=","msub":{"mi":"Q","mrow":{"mi":"k","mo":"+","mn":"1"}}}}},{"mrow":[{"mrow":[{"msub":{"mi":"Q","mrow":{"mi":"k","mo":"+","mn":"1"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mfrac":{"mrow":[{"msub":{"mi":["x","i"]},"mo":"\u2062","msubsup":{"mi":["x","i","T"]}},{"mo":["\uf603","\uf604"],"mrow":{"msub":[{"mi":["Q","k"]},{"mi":["x","i"]}],"mo":"\u2062"}}]}}}},{"mrow":{"mo":["(",")"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mfrac":{"mrow":[{"msub":{"mi":["x","i"]},"mo":"\u2062","msubsup":{"mi":["x","i","T"]}},{"mo":["\uf603","\uf604"],"mrow":{"msub":[{"mi":["Q","k"]},{"mi":["x","i"]}],"mo":"\u2062"}}]}}},"mo":"\u2062","msub":{"mi":"Q","mrow":{"mi":"k","mo":"+","mn":"1"}}}],"mo":"+"},{"mi":["c","I"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}],"mo":"="}],"mo":"="}}},"br":{},"img":[{"@attributes":{"id":"CUSTOM-CHARACTER-00023","he":"3.13mm","wi":"2.46mm","file":"US09251438-20160202-P00001.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00024","he":"3.13mm","wi":"1.02mm","file":"US09251438-20160202-P00005.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00025","he":"3.13mm","wi":"1.02mm","file":"US09251438-20160202-P00006.TIF","alt":"custom character","img-content":"character","img-format":"tif"}}],"sub":["k+1 ","F","k+1 "]},"Formula (41) (as well as (40)) provides another interpretation for ^Q as robust inverse covariance. Indeed, we note for example that the RHS of (41) is the scaled inverse of a weighted covariance matrix; the scaling enforces the trace of the inverse to be 1 and the weights of xxare significantly larger when xis an inlier. In other words, the weights apply a shrinkage procedure for outliers. Indeed, since Qxapproaches ^Qxand the underlying subspace, which contain the inliers, is recovered by ker(^Q), for an inlier xthe coefficient of xxapproaches 1\/\u03b4, which is a very large number (in practice we use \u03b4=10). On the other hand, when xis sufficiently far from the underlying subspace, the coefficient of xxis significantly smaller.","As another example implementation, estimator  may re-computes the minimization matrix (A) as a normalized and weighted covariance (i.e., not an inverse covariance) having weighting coefficients that represent proximity to estimate of the subspace associated with minimization matrix (A). In this case, the minimization matrix A may be represented as follows:",{"@attributes":{"id":"p-0161","num":"0167"},"maths":{"@attributes":{"id":"MATH-US-00038","num":"00038"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msub":{"mi":"A","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mfrac":{"mrow":[{"msub":{"mi":["x","i"]},"mo":"\u2062","msubsup":{"mi":["x","i","T"]}},{"mo":["\uf605","\uf606"],"mrow":{"msubsup":{"mi":["A","n"],"mrow":{"mo":"-","mn":"1"}},"mo":"\u2062","msub":{"mi":["x","i"]}}}]}}},{"msub":{"mi":"A","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"mrow":[{"msub":{"mi":"A","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"\u00b7","mi":"t"},{"mi":"r","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":"A","mrow":[{"mi":"n","mo":"+","mn":"1"},{"mo":"-","mn":"1"}]}}}],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}],"mo":","}}},"br":[{},{}],"sub":["1","2","N","n","i"],"sup":"\u22121"},"The following theorem analyzes the convergence of the sequence proposed by (40) to the minimizer of (4).","Theorem 11 Let X={x}be a data set in satisfying (10), {circumflex over (Q)} the minimizer of (4), Qan arbitrary symmetric matrix with tr(Q)=1 and {Q}the sequence obtained by iteratively applying (40) (while initializing it with Q), then {Q}converges to a matrix {tilde over (Q)}\u2208. If {tilde over (Q)}\u22600 for all 1\u2266i\u2266N, then the sequence {Q}converges linearly to {tilde over (Q)} and {tilde over (Q)}={circumflex over (Q)}.","The condition for the linear convergence to ^Q in Theorem 11 (i.e., ^Qx\u22600 for all 1\u2266i\u2266N) usually does not occur for noiseless data. This phenomenon is common in IRLS algorithms whose objective functions are l-type and are not twice differentiable at 0. Regularized IRLS algorithms often converge linearly to the minimizer of the regularized function. We demonstrate this principle in our case as follows.","Theorem 12 Let X={x}be a data set in satisfying (10), Qan arbitrary symmetric matrix with tr(Q)=1 and {Q}the sequence obtained by iteratively applying (41) (while initializing it with Q). Then the sequence {Q}converges linearly to the unique minimizer of F(Q).","The linear convergence rate of the iterative application of (41) depends on \u03b4. This rate is at most:",{"@attributes":{"id":"p-0167","num":"0173"},"maths":{"@attributes":{"id":"MATH-US-00039","num":"00039"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"r","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03b4"}},{"msqrt":{"mrow":{"munder":{"mi":"max","mrow":{"mi":"\u0394","mo":"=","mrow":{"mrow":{"msup":{"mi":["\u0394","T"]},"mo":"\u00b7","mrow":{"mi":"tr","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u0394"}}},"mo":"=","mn":"0"}}},"mo":"\u2062","mfrac":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mrow":[{"mi":"i","mo":"=","mn":"1"},{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"Q","mo":"*","msub":{"mi":["x","i"]}}},"mo":">","mi":"\u03b4"}],"mo":","},"mi":"N"},"mo":"\u2062","mfrac":{"msup":[{"mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["x","i","T"]},"mo":["\u2062","*"],"mi":"\u0394Q","msub":{"mi":["x","i"]}}},"mn":"2"},{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"Q","mo":"*","msub":{"mi":["x","i"]}}},"mn":"3"}]}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mfrac":{"msup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"\u0394","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["x","i"]}}},"mn":"2"},"mrow":{"mi":"max","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mrow":{"mi":"Q","mo":"*","msub":{"mi":["x","i"]}},"mo":",","mi":"\u03b4"}}}}}}]}}},"mo":"."}],"mo":"="}}},"br":{},"sub":["k","i","i"],"sup":"k "},{"@attributes":{"id":"p-0168","num":"0174"},"maths":{"@attributes":{"id":"MATH-US-00040","num":"00040"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":[{"msubsup":{"mo":"\u2009","mrow":{"mi":"\u03b4","mo":"->","mn":"0"},"mi":"lim"},"mo":"\u2062","mi":"r"},{"mo":["(",")"],"mi":"\u03b4"}],"mo":"\u2061"},"mo":"<","mn":"1."}}},"br":{}},"Following the theoretical discussion, we prefer using the regularized version of the IRLS algorithm. We fix the regularization parameter to be smaller than the rounding error, i.e., \u03b4=10, so that the regularization is very close to the original problem (even without regularization the iterative process is stable, but may have few warnings on badly scaled or close to singular matrices). The idea of the algorithm is to iteratively apply (41) with an arbitrary initialization (symmetric with trace 1). We note that in theory {F(Q)}is non-increasing. However, empirically the sequence decreases when it is within the rounding error to the minimizer. Therefore, we check F(Q) every four iterations and stop our algorithm when we detect an increase (we noticed empirically that checking every four iterations, instead of every iteration, improves the accuracy of the algorithm). Algorithm 2 summarizes our practical procedure for minimizing (4).",{"@attributes":{"id":"p-0170","num":"0176"},"maths":{"@attributes":{"id":"MATH-US-00041","num":"00041"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":"Q","mrow":{"mi":"k","mo":"+","mn":"1"}},"mo":"=","mrow":{"msup":{"mrow":[{"mo":["(",")"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mfrac":{"mrow":[{"msub":{"mi":["x","i"]},"mo":"\u2062","msubsup":{"mi":["x","i","T"]}},{"mi":"max","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msub":[{"mi":["Q","k"]},{"mi":["x","i"]}],"mo":"\u2062"}},"mo":",","mi":"\u03b4"}}}]}}},{"mo":"-","mn":"1"}]},"mo":"\/","mrow":{"mrow":{"mi":"tr","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mrow":[{"mo":["(",")"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mfrac":{"mrow":[{"msub":{"mi":["x","i"]},"mo":"\u2062","msubsup":{"mi":["x","i","T"]}},{"mi":"max","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msub":[{"mi":["Q","k"]},{"mi":["x","i"]}],"mo":"\u2062"}},"mo":",","mi":"\u03b4"}}}]}}},{"mo":"-","mn":"1"}]}}},"mo":"."}}}}},"br":{}},"Each update of Algorithm 2 requires a complexity of order O(N\u00b7D), due to the sum of N D\u00d7D matrices. Therefore, for niterations the total running time of Algorithm 2 is of order O(n\u00b7N\u00b7D). In most of our numerical experiments nwas less than 40. The storage of this algorithm is O(N\u00d7D), which amounts to storing \u03c7. Thus, Algorithm 2 has the same order of storage and complexity as PCA. In practice, it might be a bit slower due to a larger constant for the actual complexity.","Subspace Recovery in Practice","We view the GMS algorithm as a prototype for various subspace recovery algorithms. We discuss here modifications and extensions of this procedure in order to make it even more practical.","Subspace Recovery without Knowledge of d","In theory, the subspace recovery described here can work without knowing the dimension d. In the noiseless case, one may use (5) to estimate the subspace as guaranteed by Theorem 1. In the case of small noise one can estimate d from the eigenvalues of ^Q and then apply the GMS algorithm. This strategy is theoretically justified by Theorems 1 and 6 as well as the discussion following (87). The problem is that condition (9) for guaranteeing exact recovery by GMS is restrictive; in particular, it requires the number of outliers to be larger than at least D\u2212d (according to our numerical experiments it is safe to use the lower bound 1.5 (D\u2212d)). For practitioners, this is a failure mode of GMS, especially when the dimension of the data set is large (for example, D>N).","While this seems to be a strong restriction, we remark that the problem of exact subspace recovery without knowledge of the intrinsic dimension is rather hard and some assumptions on data sets or some knowledge of data parameters would be expected. Other algorithms for this problem require estimates of unknown regularization parameters (which often depend on various properties of the data, in particular, the unknown intrinsic dimension) or strong assumptions on the underlying distribution of the outliers or corrupted elements.","We first note that if only conditions (6) and (7) hold, then Theorem 1 still guarantees that the GMS algorithm outputs a subspace containing the underlying subspace. Using some information on the data one may recover the underlying subspace from the outputted subspace containing it, even when dealing with the failure mode.","In the rest of this section we describe several practical solutions for dealing with the failure mode, in particular, with small number of outliers. We later demonstrate them numerically for artificial data and for real data.","Our first practical solution is to reduce the ambient dimension of the data. When the reduction is not too aggressive, it can be performed via PCA. We also propose a robust dimensionality reduction which can be used instead. There are two problems with this strategy. First of all, the reduced dimension is another parameter that requires tuning. Second of all, some information may be lost by the dimensionality reduction and thus exact recovery of the underlying subspace is generally impossible.","A second practical solution is to add artificial outliers. The number of added outliers should not be too large (otherwise (6) and (7) will be violated), but they should sufficiently permeate through so that (9) holds. In practice, the number of outliers can be 2D, since empirically (9) holds with high probability when N=2(D\u2212d). To overcome the possible impact of outliers with arbitrarily large magnitude, we project the data with artificial outliers onto the sphere. Furthermore, if the original data matrix does not have full rank (in particular if N<D) we reduce the dimension of the data (by PCA) to be the rank of the data matrix. This dimensionality reduction clearly does not result in any loss of information. We refer to the whole process of initial \u201clossless dimensionality reduction\u201d (if necessary), addition of 2D artificial Gaussian outliers, normalization onto the sphere and application of GMS (with optional estimation of d by the eigenvalues of ^Q) as the GMS2 algorithm. We believe that it is the best practical solution to avoid condition (9) when d is unknown.","A third solution is to regularize our M estimator, that is, to minimize the following objective function with the regularization parameter \u03bb:",{"@attributes":{"id":"p-0181","num":"0187"},"maths":{"@attributes":{"id":"MATH-US-00042","num":"00042"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mover":{"mi":"Q","mo":"^"},"mo":["\u2062","\u2062"],"msub":{"mo":"=","mrow":{"mrow":[{"mrow":{"mo":["(",")"],"mi":"Q"},"mo":"=","mn":"1"},{"mi":"Q","mo":"=","msup":{"mi":["Q","T"]}}],"mo":","}},"mrow":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mrow":{"mi":"Q","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["x","i"]}}}},{"mi":"\u03bb","mo":"\u2062","mrow":{"msubsup":{"mrow":{"mo":["\uf603","\uf604"],"mi":"Q"},"mi":"F","mn":"2"},"mo":"."}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"43"}}]}}}}},"The IRLS algorithm then becomes",{"@attributes":{"id":"p-0183","num":"0189"},"maths":{"@attributes":{"id":"MATH-US-00043","num":"00043"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":"Q","mrow":{"mi":"k","mo":"+","mn":"1"}},"mo":"=","mrow":{"msup":{"mrow":[{"mo":["(",")"],"mrow":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mfrac":{"mrow":[{"msub":{"mi":["x","i"]},"mo":"\u2062","msubsup":{"mi":["x","i","T"]}},{"mi":"max","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msub":[{"mi":["Q","k"]},{"mi":["x","i"]}],"mo":"\u2062"}},"mo":",","mi":"\u03b4"}}}]}},{"mn":"2","mo":["\u2062","\u2062","\u2062"],"mi":["\u03bb","I"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}],"mo":"+"}},{"mo":"-","mn":"1"}]},"mo":"\/","mrow":{"mrow":{"mo":["(",")"],"mrow":{"msup":{"mrow":[{"mo":["(",")"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mfrac":{"mrow":[{"msub":{"mi":["x","i"]},"mo":"\u2062","msubsup":{"mi":["x","i","T"]}},{"mi":"max","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"msub":[{"mi":["Q","k"]},{"mi":["x","i"]}],"mo":"\u2062"}},"mo":",","mi":"\u03b4"}}}]}}},{"mo":"-","mn":"1"}]},"mo":"+","mrow":{"mn":"2","mo":["\u2062","\u2062","\u2062"],"mi":["\u03bb","I"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}},"mo":"."}}}}},"br":[{},{}]},"Knowledge of the intrinsic dimension d can help improve the performance of GMS or suggest completely new variants (especially as GMS always finds a subspace containing the underlying subspace). For example, knowledge of d can be used to carefully estimate the parameter \u03bb of (43), e.g., by finding \u03bb yielding exactly a d-dimensional subspace via a bisection procedure.","The EGMS Algorithm","We formulate in Algorithm 3 the Extended Geometric Median Subspace (EGMS) algorithm for subspace recovery with known intrinsic dimension.","Algorithm 3 The Extended Geometric Median Subspace Algorithm","Input: X={x}: data, d:dimension of L*, an algorithm for minimizing (4)","Output: {circumflex over (L)}: a d-dimnsional linear subspace in R.","Steps:\n\n","We justify this basic procedure in the noiseless case without requiring (9) as follows. Theorem 13 Assume that d, D\u2208N, d<D, X is a data set in Rand L*\u2208G (D, d). If only conditions (6) and (7) hold, then the EGMS Algorithm exactly recovers L*.","The vectors obtained by EGMS at each iteration can be used to form robust principal components (in reverse order), even when ^Q is degenerate.","Computational Complexity of GMS and EGMS","The computational complexity of GMS is of the same order as that of Algorithm 2, i.e., O(n\u00b7N\u00b7D) (where nis the number of required iterations for Algorithm 2). Indeed, after obtaining ^Q, computing L* by its smallest d eigenvectors takes an order of O(d\u00b7D) operations.","EGMS on the other hand repeats Algorithm 2 D\u2212d times; therefore it adds an order of O((D\u2212d)\u00b7n\u00b7N\u00b7D) operations, where ndenotes the total number of iterations for Algorithm 2. In implementation, we can speed up the EGMS algorithm by excluding the span of some of the top eigenvectors of ^Q from ^L (instead of excluding only the top eigenvector in the third step of Algorithm 2). We demonstrate this modified procedure on artificial setting below.","Numerical Experiments","Model for Synthetic Data","Below, we generate data from the following model. We randomly choose L*\u2208G(D, d), sample Ninliers from the d-dimensional Multivariate Normal distribution N(0, I) on L* and add Noutliers sampled from a uniform distribution on [0,1]. The outliers are strongly asymmetric around the subspace to make the subspace recovery problem more difficult. In some experiments below, additional Gaussian noise is considered. When referring to this synthetic data we only need to specify its parameters N, N, D, d and possibly the standard deviation for the additive noise. For any subspace recovery algorithm (or heuristics), we denote by {tilde over ( )}L its output (i.e., the estimator for L*) and measure the corresponding recovery error by e=|P\u2212P|.","Demonstration of Practical Solutions","We present two different artificial cases, where in one of them condition (9) holds and in the other one it does not hold and test the practical solutions in the second case.","The two cases are the following instances of the synthetic model: (a) (N, N, D, d)=(100, 100, 100, 20) and (b) (N, N, D, d)=(100, 20, 100, 20). The GMS algorithm estimates the underlying subspace L* given d=20 with recovery errors 2.1\u00d710and 3.4 in cases (a) and (b) respectively. In case (a) there are sufficiently many outliers (with respect to D\u2212d) and the GMS algorithm is successful. We later show that the underlying dimension (d=20) can be easily estimated by the eigenvalues of ^Q. In case (b) N=0.25*(D\u2212d), therefore, condition (9) is violated and the GMS algorithm completely fails.","We demonstrate the success of the practical solutions in case (b). We assume that the dimension d is known, though we also estimate d correctly for the non-regularized solutions. Therefore, these solutions can be also applied without knowing the dimension. If we reduce the dimension of the data set in case (b) from D=100 to D=35 (via PCA; though one can also use EGMS), then GMS (with d=20) achieves a recovery error of 0.23, which indicates that GMS almost recovers the subspace correctly. We remark though that if we reduce the dimension to e.g., D=55, then the GMS algorithm will still fail. We also note that the recovery error is not as attractive as the ones below; this observation probably indicates that some information was lost during the dimension reduction.","The GMS2 algorithm with d=20 recovers the underlying subspace in case (b) with error 1.2\u00d710. This is the method we advocated for when possibly not knowing the intrinsic dimension.","The regularized minimization of (43) with \u03bb=100 works well for case (b). In fact, it recovers the subspace as ker^Q (without using its underlying dimension) with error 3.3\u00d710. The only issue is how to determine the value of \u03bb. We described above that if d is known, then \u03bb can be carefully estimated by the bisection method. This is true for this example, in fact, we initially chose \u03bb this way.","We remark that the REAPER algorithm did not perform well for this particular data, though in general it is a very successful solution. The recovery error of the direct REAPER algorithm was 3.725 (and 3.394 for S-REAPER) and the error for its modified version via bisection (relaxing the bound on the largest eigenvalue so that dim(ker(^Q))=20) was 3.734 (and 3.175 for S-REAPER).","At last we demonstrate the performance of EGMS and its faster heuristic with d=20. The recovery error of the original EGMS for case (b) is only 0.095. We suggested above a faster heuristic for EGMS, which can be reformulated as follows: In the third step of Algorithm 3, we replace u (the top eigenvector of ^Q) with U, the subspace spanned by several top eigenvectors. In the noiseless case, we could let U be the span of the nonzero eigenvectors of ^Q. This modification of EGMS (for the noiseless case) required only two repetitions of Algorithm 2 and its recovery error was 2.2\u00d710. In real data sets with noise we need to determine the number of top eigenvectors spanning U, which makes this modification of EGMS less automatic.","Demonstration of Dimension Estimation","We test dimension estimation by eigenvalues of ^Q for cases (a) and (b). The eigenvalues of ^Q obtained by Algorithm 2 for the two cases are shown in . That is, in , the starred points and the dotted point represent log-scaled eigenvalues of the output of Algorithm 2 for cases (a) and (b) respectively  corresponds to case (b) with dimension reduced to 35.","In , the largest logarithmic eigengap (i.e., the largest gap in logarithms of eigenvalues) occurs at 80, so we can correctly estimate that d=D\u221280=20 (the eigenvalues are not zero since Algorithm 2 uses the \u03b4-regularized objective function). However, in , the largest eigengap occurs at 60 and thus mistakenly predicts d=40.","As we discussed above, the dimension estimation fails here since condition (9) is not satisfied. However, we have verified that if we try any of the solutions proposed above then we can correctly recover that d=20 by the logarithmic eigengap. For example, in , we demonstrate the logarithms of eigenvalues of {circumflex over (Q)} in case (b) after dimensionality reduction (via PCA) onto dimension D=35 and it is clear that the largest gap is at d=20 (or D\u2212d=80). We obtained similar graphs when using 2D artificial outliers (more precisely, the GMS2 algorithm without the final application of the GMS algorithm) or the regularization of (43) with \u03bb=100.","Information Obtained from Eigenvectors","Throughout the paper we emphasized the subspace recovery problem, but did not discuss at all the information that can be inferred from the eigenvectors of our robust PCA strategy. Since in standard PCA these vectors have significant importance, we exemplify the information obtained from our robust PCA and compare it to that obtained from PCA and some other robust PCA algorithms.","We create a sample from a mixture of two Gaussian distributions with the same mean and same eigenvalues of the covariance matrices but different eigenvectors of the covariance matrices. The mixture percentages are 25% and 75%. We expect the eigenvectors of any good robust PCA algorithm (robust to outliers as perceived in this paper) to be close to that of the covariance of the mail component (with 75%).","More precisely, we sample 300 points from N(0, \u03a3), where \u03a3is a 10\u00d710 diagonal matrix with elements 1, 2, 2, . . . , 2and 100 points from N(0, \u03a3), where \u03a3=U\u03a3U, where U is randomly chosen from the set of all orthogonal matrices in . The goal is to estimate the eigenvectors of \u03a3(i.e., the standard basis vectors ) in the presence of 25% \u201coutlier\u201d. Unlike the subspace recovery problem, where we can expect to exactly recover a linear structure among many outliers, here the covariance structure is more complex and we cannot exactly recover it with 25% outliers.","We estimated the eigenvectors of \u03a3by the eigenvectors of {circumflex over (Q)} of Algorithm 2 in reverse order (recall that {circumflex over (Q)} is a scaled and robust version of the inverse covariance). We refer to this procedure as \u201cEVs (eigenvectors) of \u201c{circumflex over (Q)}\u201d. We also estimated these eigenvectors by standard PCA, LLD with \u03bb=0.8\u221a{square root over (D\/N)} and PCP with \u03bb=1\/\u221a{square root over (max(D, N))}. We repeated the random simulation (with different samples for the random orthogonal matrix U) 100 times reported in Table 2 the average angles between the estimated and actual top two eigenvectors of \u03a3according to the different methods. We note that the \u201cEVs of {circumflex over (Q)}\u201d outperforms PCS, LLD (or OP) and PCP in terms of estimation of the top two eigenvectors of \u03a3. We remark though that PCP does not suit for robust estimation of the empirical covariance and thus the comparison is unfair for PCP.",{"@attributes":{"id":"p-0207","num":"0219"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 2"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Angles (in degrees) between the estimated and actual "},{"entry":"top two eigenvectors of \u03a3"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"6"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"42pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},"EVs of {circumflex over (Q)}","LLD","PCP","PCA"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"5","align":"center","rowsep":"1"}}]},{"entry":[{},"Eigenvector 1","3.0\u00b0","5.5\u00b0","45.7\u00b0","14.8\u00b0"]},{"entry":[{},"Eigenvector 2","3.0\u00b0","5.5\u00b0","47.4\u00b0","40.3\u00b0"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"5","align":"center","rowsep":"1"}}]}]}}]}}},"When the covariance matrix \u03a3(and consequently also \u03a3) is degenerate, {circumflex over (Q)} might be singular and therefore {circumflex over (Q)} cannot be directly used to robustly estimate eigenvectors of the covariance matrix. For this case, EGMS (Algorithm 3 can be used where the vector u obtained in the ith iteration of Algorithm 3 can be considered as the (D\u2212i+1)st robust eigenvector (that is, we reverse the order again). To test the performance of this method, we modify \u03a3in the above model as follows: \u03a3=diag(1, 0.5, 0.25, 0, 0, . . . , 0). We repeated the random simulations of this modified model 100 times and reported in Table 2 the average angles between the estimated and actual top two eigenvectors of \u03a3according to the different methods. Here LLD did slightly better than EGMS and they both outperformed PCA (and PCP).",{"@attributes":{"id":"p-0209","num":"0221"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 3"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Angles (in degrees) between the estimated and actual"},{"entry":"top two eigenvectors of \u03a3"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"42pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"EGMS","LLD","PCP","PCA"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}},{"entry":["Eigenvector 1","5.2\u00b0","3.4\u00b0","42.6\u00b0","\u20028.2\u00b0"]},{"entry":["Eigenvector 2","5.2\u00b0","3.4\u00b0","47.3\u00b0","16.1\u00b0"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}}]}},"br":{}},"We assume a synthetic data set sampled according to the model above with (N, N, D, d)=(250, 250, 100, 10). We use the GMS algorithm with d=10 and different values of the regularization parameter \u03b4 and record the recovery error in .  is a graph of the recovery errors and the regularization parameters \u03b4. For 10\u2266\u03b4\u226610, log(error)\u2212log(\u03b4) is constant. We thus empirically obtain that the error is of order O(\u03b4) in this range. On the other hand, (29) only obtained an order of O(\u221a{square root over (\u03b4)}). It is possible that methods similar to those of Coudron and Lerman (2012) can obtain sharper error bounds. We also expect that for \u03b4 sufficiently small (here smaller than 10), the rounding error becomes dominant. On the other hand, perturbation results are often not valid for sufficiently large \u03b4 (here this is the case for \u03b4>10).","Detailed Comparison with Other Algorithms for Synthetic Data","Using the synthetic data, we compared the GMS algorithm with the following algorithms: MDR (Mean Absolute Deviation Roundin), LLD (Low-Leverage Decomposition), OP (Outlier Pursuit), PCP (Principal Component Pursuit), MKF (Median K-flats with K=1), HR-PCA (High-dimensional Robust PCA), a common M-estimator and R-PCA. We also record the output of standard PCA, where we recover the subspace by the span of the top d-eigenvectors. We ran the experiments on a computer with Intel Core 2 CPU at 2.66 GHz and 2 GB memory.","We remark that since the basic GMS algorithm already performed very well on these artificial instances.","For all of our experiments with synthetic data, we could correctly estimate d by the largest logarithmic eigengap of the output of Algorithm 2. Nevertheless, we used the knowledge of d for all algorithms for the sake of fair comparison.","For LLD. OP and PCP we estimated L* by the span of the top d eigenvectors of the low-rank matrix. Similarly, for the common M-estimator we used the span of the top d eigenvectors of the estimated covariance A. For the HR-PCA algorithm we also used the true percentage of outliers (50% in our experiments). For LLD, OP and PCP we set the mixture parameter \u03bb as 0.8\u221a{square root over (D\/N)},0.8\u221a{square root over (D\/n)},1\/\u221a{square root over (max (D,N))} respectively (following the suggestions of McCoy and Tropp (2011) for LLD\/OP and Candes et al., (2011) for PCP). These choices of parameters are also used in experiments with real data sets.","For the common M-estimator, we used u(x)=2 max(ln(x)\/x,10) and the algorithm discussed by Kent and Tyler (1991). Considering the conditions discussed above, we also tried other functions: u(x)=max(x,10) had a significantly larger recovery error and u(x)=max(x,10) resulted in a similar recovery error as max(ln(x)\/x, 10) but a double running time.","We used the syntectic data with different values of (N,ND,d). In some instances we also add noise from the Gaussian distribution N(0, \u03b7I) with \u03b7=0.1 or 0.01. We repeated each experiment 20 times (due to the random generation of data). We record in Table 4 the mean running time, the mean recovery error and their standard deviations.","We remark that PCP is designed for uniformly corrupted coordinates of data, instead of corrupted data points (i.e., outliers), therefore, the comparison with PCP is somewhat unfair for this kind of data. On the other hand, the applications are tailored for the PCP model (though the other algorithms still apply successfully to them).","From Table 4 we can see that GMS is the fastest robust algorithm. Indeed, its running time is comparable to that of PCA. We note that this is due to its linear convergence rate (usually it converges in less than 40 iterations). The common M-estimator is the closest algorithm in terms of running time to GMS, since it also has the linear convergence rate. In contrast, PCP, OP and LLD need a longer running time since their convergence rates are much slower. Overall, GMS performs best in terms of exact recovery. The PCP, OP and LLD algorithms cannot approach exact recovery even by tuning the parameter \u03bb. For example, in the case where , (N, N, D, d)=(125, 125, 10, 5) with \u03b7=0, we checked a geometric sequence of 101 \u03bb values from 0.01 to 1, and the smallest recovery errors for LLD, OP and PCP are 0.17, 0.16 and 0.22 respectively. The common M-estimator performed very well for many cases (sometimes slightly better than GMS), but its performance deteriorates as the density of outliers increases (e.g., poor performance for the case where (N, N, D, d)=(125, 125, 10, 5). Indeed, Theorem 9 indicates problems with the exact recovery of the common M-estimator.","At last, we note that the empirical recovery error of the GMS algorithm for noisy data sets is in the order of \u221a{square root over (\u03b7)}, where \u03b7 is the size of noise.","Face Data","We apply our algorithm to face images. It has been shown that face images from the same person lie in a low-dimensional linear subspace of dimension at most 9. However, cast shadows, specular reflections and saturations could possibly distort this low-rank modeling. Therefore, one can use a good robust PCA algorithm to remove these errors if one has many images from the same face.","We used the images of the first two persons in the extended Yale face database B, where each of them has 65 images of size 192\u00d7168 under different illumination conditions. Therefore we represent each person by 65 vectors of length 32256. We applied GMS, GMS2 and EGMS with d=9 and we also reduced the 65\u00d732256 matrix to 65\u00d765 (in fact, we only reduced the representation of the column space) by rejecting left vectors with zero singular values. We also applied the GMS algorithm after initial dimensionality reduction (via PCA) to D=20. The running times of EGMS and GMS (without dimensionality reduction) are 13 and 0.16 seconds respectively on average for each face (we used the same computer as above). On the other hand, the running times of PCP and LLD are 193 and 2.7 seconds respectively. Moreover, OP ran out of memory.",{"@attributes":{"id":"p-0222","num":"0234"},"figref":["FIG. 5","FIG. 5"]},{"@attributes":{"id":"p-0223","num":"0235"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"343pt","align":"center"}},"thead":{"row":{"entry":"TABLE 4"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Mean running times, recovery errors and their standard deviations for synthetic data."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"12"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"8","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"9","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"10","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"11","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"12","colwidth":"28pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["(N, N, D, d)",{},"GMS","MDR","LLD","OP","PCP","HR-PCA","MKF","PCA","M-est.","R-PCA"]},{"entry":{"@attributes":{"namest":"1","nameend":"12","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"12"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"6","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"8","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"9","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"10","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"11","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"12","colwidth":"28pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["(125, 125, 10, 5)","e","6e\u221211","0.275","1.277","0.880","0.605","0.210","0.054","0.193","0.102","0.121"]},{"entry":["\u03b7 = 0","std.e","4e\u221211","0.052","0.344","0.561","0.106","0.049","0.030","0.050","0.037","0.048"]},{"entry":[{},"t(s)","0.008","0.371","0.052","0.300","0.056","0.378","0.514","0.001","0.035","0.020"]},{"entry":[{},"std.t","0.002","0.120","0.005","0.054","0.002","0.001","0.262","8e\u221206","4e\u221204","0.014"]},{"entry":["(125, 125, 10, 5)","e","0.011","0.292","1.260","1.061","0.567","0.233","0.069","0.213","0.115","0.139"]},{"entry":["\u03b7 = 0.01","std.e","0.004","0.063","0.316","0.491","0.127","0.075","0.036","0.073","0.054","0.073"]},{"entry":[{},"t(s)","0.008","0.340","0.053","0.287","0.056","0.380","0.722","0.001","0.035","0.052"]},{"entry":[{},"std.t","0.001","0.075","0.007","0.033","0.001","0.009","0.364","1e\u221205","4e\u221204","0.069"]},{"entry":["(125, 125, 10, 5)","e","0.076","0.264","1.352","0.719","0.549","0.200","0.099","0.185","0.122","0.128"]},{"entry":["\u03b7 = 0.1","std.e","0.023","0.035","0.161","0.522","0.102","0.051","0.033","0.048","0.041","0.050"]},{"entry":[{},"t(s)","0.007","0.332","0.055","0.301","0.056","0.378","0.614","0.001","0.035","0.032"]},{"entry":[{},"std.t","0.001","0.083","0.004","0.044","0.001","0.001","0.349","7e\u221206","4e\u221204","0.037"]},{"entry":["(125, 125, 50, 5)","e","2e\u221211","0.652","0.258","0.256","0.261","0.350","0.175","0.350","1e\u221212","0.307"]},{"entry":["\u03b7 = 0","std.e","3e\u221211","0.042","0.030","0.032","0.033","0.023","0.028","0.025","5e\u221212","0.029"]},{"entry":[{},"t(s)","0.015","0.420","0.780","1.180","3.164","0.503","0.719","0.006","0.204","0.020"]},{"entry":[{},"std.t","0.001","0.128","0.978","0.047","0.008","0.055","0.356","9e\u221205","0.001","0.011"]},{"entry":["(125, 125, 50, 5)","e","0.061","0.655","0.274","0.271","0.273","0.355","0.196","0.359","0.007","0.321"]},{"entry":["\u03b7 = 0.01","std.e","0.009","0.027","0.039","0.038","0.040","0.038","0.038","0.033","0.001","0.038"]},{"entry":[{},"t(s)","0.023","0.401","4.155","1.506","0.499","0.653","0.656","0.006","0.191","0.028"]},{"entry":[{},"std.t","0.002","0.079","0.065","0.197","0.006","0.044","0.377","8e\u221205","0.001","0.022"]},{"entry":["(125, 125, 50, 5)","e","0.252","0.658","0.292","0.290","0.296","0.358","0.264","0.363","0.106","0.326"]},{"entry":["\u03b7 = 0.1","std.e","0.027","0.033","0.032","0.032","0.033","0.027","0.031","0.032","0.014","0.032"]},{"entry":[{},"t(s)","0.021","0.363","0.923","1.726","0.501","0.638","0.641","0.006","0.191","0.025"]},{"entry":[{},"std.t","0.001","0.063","0.033","0.470","0.009","0.051","0.240","1e\u221204","0.001","0.012"]},{"entry":["(250, 250, 100, 10) ","e","3e\u221212","0.880","0.214","0.214","0.215","0.332","0.161","0.330","2e\u221212","0.259"]},{"entry":["\u03b7 = 0","std.e ","2e\u221212","0.018","0.019","0.019","0.019","0.014","0.024","0.012","9e\u221212","0.016"]},{"entry":[{},"t(s)","0.062","1.902","3.143","7.740","2.882","1.780","1.509","0.039","0.819","1.344"]},{"entry":[{},"std.t","0.006","0.354","4.300","0.038","0.014","0.041","1.041","3e\u221204","0.023","0.708"]},{"entry":["(250, 250, 100, 10)","e","0.077","0.885","0.217","0.216","0.219","0.334","0.164","0.335","0.009","0.263"]},{"entry":["\u03b7 = 0.01","std.e ","0.006","0.031","0.019","0.018","0.020","0.019","0.019","0.017","3e\u221204","0.018"]},{"entry":[{},"t(s)","0.084","1.907","21.768","11.319","2.923","1.785","1.412","0.039","0.400","1.086"]},{"entry":[{},"std.t","0.010","0.266","0.261","0.291","0.014","0.041","0.988","3e\u221204","0.002","0.738"]},{"entry":["(250, 250, 100, 10)","e","0.225","0.888","0.238","0.237","0.262","0.342","0.231","0.345","0.136","0.276"]},{"entry":["\u03b7 = 0.1","std.e","0.016","0.020","0.019","0.019","0.019","0.019","0.018","0.015","0.010","0.019"]},{"entry":[{},"t(s)","0.076","1.917","4.430","16.649","2.876","1.781","1.555","0.039","0.413","1.135"]},{"entry":[{},"std.t","0.007","0.299","0.069","1.184","0.014","0.025","0.756","4e\u221204","0.011","0.817"]},{"entry":["(500, 500, 200, 20)","e","4e\u221211","1.246","0.162","0.164","0.167","0.381","0.136","0.381","3e\u221213","0.239"]},{"entry":["\u03b7 = 0","std.e","1e\u221210","0.018","0.011","0.011","0.011","0.010","0.009","0.008","6e\u221214","0.009"]},{"entry":[{},"t(s)","0.464","23.332\u2002","16.778","89.090","16.604\u2002","8.602","5.557","0.347","6.517","15.300\u2002"]},{"entry":[{},"std.t","0.024","2.991","0.878","1.836","0.100","0.216","4.810","0.009","0.126","3.509"]},{"entry":["(500, 500, 200, 20)","e","0.082","1.247","0.160","0.162","0.166","0.374","0.139","0.378","0.012","0.236"]},{"entry":["\u03b7 = 0.01","std.e ","0.003","0.018","0.007","0.007","0.008","0.011","0.010","0.006","2e\u221204","0.007"]},{"entry":[{},"t(s)","0.592","23.214\u2002","128.51","122.61","16.823\u2002","8.541","6.134","0.354","2.361","15.165\u2002"]},{"entry":[{},"std.t","0.060","3.679","1.155","6.500","0.036","0.219","4.318","0.019","0.064","3.485"]},{"entry":["(500, 500, 200, 20)","e","0.203","1.262","0.204","0.204","0.250","0.391","0.275","0.398","0.166","0.270"]},{"entry":["\u03b7 = 0.1","std.e","0.007","0.012","0.007","0.007","0.007","0.012","0.272","0.009","0.005","0.008"]},{"entry":[{},"t(s)","0.563","24.112\u2002","24.312","202.22","16.473\u2002","8.552","8.745","0.348","2.192","15.150\u2002"]},{"entry":[{},"std.t","0.061","2.362","0.226","8.362","0.050","0.155","3.408","0.010","0.064","3.420"]},{"entry":{"@attributes":{"namest":"1","nameend":"12","align":"center","rowsep":"1"}}}]}}]}},"br":{}},"For background subtraction in surveillance videos, we consider the following two videos: \u201cLobby in an office building with switching on\/off lights\u201d and \u201cShopping center\u201d from http:\/\/perception.i2r.a-star.edu.sg\/bk_model\/bk_index.html . In the first video, the resolution is 160\u00d7128 and we used 1546 frames from \u2018SwitchLight1000.bmp\u2019 to \u2018SwitchLight2545.bmp\u2019. In the second video, the resolution is 320\u00d7256 and we use 1000 frames from \u2018ShoppingMall1001.bmp\u2019 to \u2018ShoppingMall2000.bmp\u2019. Therefore, the data matrices are of size 1546\u00d720480 and 1001'81920. We used a computer with Intel Core 2 Quad Q6600 2.4 GHz and 8 GB memory due to the large size of these data.","We applied GMS, GMS2 and EGMS with d=3 and with initial dimensionality reduction to 200 to reduce running time. For this data we are unaware of a standard choice of d; though we noticed empirically that the outputs of our algorithms as well as other algorithms are very stable to changes in d within the range 2\u2266d\u22665. We obtain the foreground by the orthogonal projection to the recovered 3-dimensional subspace.",{"@attributes":{"id":"p-0226","num":"0238"},"figref":["FIG. 6","FIG. 6","FIG. 6","FIGS. 7A and 7B"],"b":["7","7"]},{"@attributes":{"id":"p-0227","num":"0239"},"figref":["FIG. 8","FIG. 8","FIG. 1"],"b":["22","10"]},"Initially, estimator  receives input data from a sensor, such as image source  (). The input data comprises a set of data points that conforms to a plurality of dimensions (D) and includes outlier data points, such as noise.","In some examples, estimator  pre-processes the data points (). For example, as explained above, estimator  may insert a plurality of artificial outlier data points into the set of data point prior to iteratively processing the set of data points to compute the reduced data set. As another example, as also explained herein, estimator  may normalize each of the data points to a unit sphere by dividing each of the data points to a corresponding Euclidean norm of the data point.","After optionally pre-processing the set of data points, estimator  initializes a minimization matrix (Q) for estimating a subspace of dimension (d) (). The minimization matrix (Q) (or A for a non-inverse covariance matrix) initialized for use with any of the techniques described herein, such as the GMS technique (Algorithm 2) or the Extended GMS technique (Algorithm 3).","Next, estimator  iteratively re-computes the minimization matrix (Q) as a normalized and weighted covariance having weighting coefficients that represent proximity to estimate of the subspace associated with minimization matrix (Q) (). As explained above, estimator  may, in some example implementations, compute the minimization matrix (Q) in inverse form, i.e., an inverse covariance matrix. In this way, estimator  determines, for each iteration, a scaled version of the set of data points by re-computing a corresponding coefficient for each of the data points as a function of a proximity of the data point to a current estimate of the subspace, and re-computes, for each iteration, the minimization matrix (A) representing an updated estimate of the subspace based on a summation of weighted least absolute squares of the scaled version of the set of data points.","Estimator  repeats this process until convergence or a sufficient number of iterations ().","Upon terminating the iterative process (YES of ), estimator  determines the estimate of the subspace from the minimization matrix (Q) (). For example, when using Qin the form of an inverse covariance matrix, estimator  extracts, as the subspace, a bottom set of d eigenvectors from the computed minimizer Q. As another example, when using Qin the form of a covariance matrix, estimator  extracts, as the subspace, a top set of d eigenvectors from the computed minimizer A.","Upon determining the reduced data set including the subspace, data analysis system  may take action. For example, data analysis system  may further process the reduced data set in view of the determined subspace to perform face recognition with respect to the image data received from the image source. As other examples, data analysis system  may further process the reduced data set in view of the determined subspace perform computer vision, machine learning or other actions.",{"@attributes":{"id":"p-0235","num":"0247"},"figref":["FIG. 9","FIG. 1"],"b":["500","10","22"]},"Here, a computer  includes a processor  that is operable to execute program instructions or software, causing the computer to perform various methods or tasks. Processor  is coupled via bus  to a memory , which is used to store information such as program instructions and other data while the computer is in operation. A storage device , such as a hard disk drive, nonvolatile memory, or other non-transient storage device stores information such as program instructions, data files of the multidimensional data and the reduced data set, and other information. The computer also includes various input-output elements , including parallel or serial ports, USB, Firewire or IEEE 1394, Ethernet, and other such ports to connect the computer to external device such a printer, video camera, surveillance equipment or the like. Other input-output elements include wireless communication interfaces such as Bluetooth, Wi-Fi, and cellular data networks.","The computer itself may be a traditional personal computer, a smart phone, a rack-mount or business computer or server as shown in , or any other type of computerized system such as an image capture or processing device. Other examples include appliances such as a set-top box, including a separate appliance or incorporated into another appliance such as a media player or television. The computer in a further example may include fewer than all elements listed above, such as a thin client or mobile device having only some of the shown elements. In another example, the computer is distributed among multiple computer systems, such as a distributed server that has many computers working together to provide various functions.","The techniques described herein may be implemented in hardware, software, firmware, or any combination thereof. Various features described as modules, units or components may be implemented together in an integrated logic device or separately as discrete but interoperable logic devices or other hardware devices. In some cases, various features of electronic circuitry may be implemented as one or more integrated circuit devices, such as an integrated circuit chip or chipset.","If implemented in hardware, this disclosure may be directed to an apparatus such a processor or an integrated circuit device, such as an integrated circuit chip or chipset. Alternatively or additionally, if implemented in software or firmware, the techniques may be realized at least in part by a computer readable data storage medium comprising instructions that, when executed, cause one or more processors to perform one or more of the methods described above. For example, the computer-readable data storage medium may store such instructions for execution by a processor. Any combination of one or more computer-readable medium(s) may be utilized.","A computer-readable medium may form part of a computer program product, which may include packaging materials. A computer-readable medium may comprise a computer data storage medium such as random access memory (RAM), read-only memory (ROM), non-volatile random access memory (NVRAM), electrically erasable programmable read-only memory (EEPROM), flash memory, magnetic or optical data storage media, and the like. In general, a computer-readable storage medium may be any tangible medium that can contain or store a program for use by or in connection with an instruction execution system, apparatus, or device. Additional examples of computer readable medium include computer-readable storage devices, computer-readable memory, and tangible computer-readable medium. In some examples, an article of manufacture may comprise one or more computer-readable storage media.","In some examples, the computer-readable storage media may comprise non-transitory media. The term \u201cnon-transitory\u201d may indicate that the storage medium is not embodied in a carrier wave or a propagated signal. In certain examples, a non-transitory storage medium may store data that can, over time, change (e.g., in RAM or cache).","The code or instructions may be software and\/or firmware executed by processing circuitry including one or more processors, such as one or more digital signal processors (DSPs), general purpose microprocessors, application-specific integrated circuits (ASICs), field-programmable gate arrays (FPGAs), or other equivalent integrated or discrete logic circuitry. Accordingly, the term \u201cprocessor,\u201d as used herein may refer to any of the foregoing structure or any other processing circuitry suitable for implementation of the techniques described herein. In addition, in some aspects, functionality described in this disclosure may be provided within software modules or hardware modules.","An M-estimator is described herein for the problems of exact and near subspace recovery. The recovery obtained by this estimator is quantified as well as its numerical approximation. Numerical experiments demonstrate state-of-the-art speed and accuracy for the implementation on both synthetic and real data sets.","Although specific embodiments have been illustrated and described herein, it will be appreciated by those of ordinary skill in the art that any arrangement that achieve the same purpose, structure, or function may be substituted for the specific embodiments shown. This application is intended to cover any adaptations or variations of the embodiments of the invention described herein. It is intended that this invention be limited only by the claims, and the full scope of equivalents thereof."],"GOVINT":[{},{}],"heading":["GOVERNMENT INTEREST","TECHNICAL FIELD","BACKGROUND","SUMMARY","DETAILED DESCRIPTION","Example 1"],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF DRAWINGS","p":[{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIGS. 2A-2D"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIGS. 3A-3B"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIGS. 7A and 7B","b":["7","7"]},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 9"}]},"DETDESC":[{},{}]}
