---
title: System and method for lossy source-channel coding at the application layer
abstract: A source channel encoder, source channel decoder and methods for implementing such devices are disclosed herein. The source channel encoder includes a linear transform encoder configured to generate a plurality of source components. A successive refinement quantizer is configured to generate a plurality of bit planes based on the source components. A systematic linear encoder configured to map the bit planes into channel-encoded symbols. The linear transform encoder may be configured to apply a Discrete Cosine Transform (DCT) or a Discrete Wavelet Transform (DWT). The linear transform encoder may be configured for differential encoding.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09131238&OS=09131238&RS=09131238
owner: University of Southern California
number: 09131238
owner_city: Los Angeles
owner_country: US
publication_date: 20110620
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO PRIOR FILED APPLICATIONS","UNITED STATES GOVERNMENT RIGHTS","FIELD OF INVENTION","BACKGROUND","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION","EXAMPLES"],"p":["This application claims priority to earlier filed provisional application 61\/416,986 filed on Jun. 18, 2010, which is incorporated herein in its entirety.","This invention was made with government support under CNS-06-25637 and NeTs-07-22073 awarded by U.S. National Science Foundation. The government has certain rights to this invention.","The invention relates to communications systems generally and more particularly relates to multicasting multimedia streaming systems.","Multimedia streaming over heterogeneous digital networks is one of the fastest growing applications both in terms of traffic demands and in terms of market potential. Systems can include a server, multicasting a multi-media source (audio\/video) to a number of clients. Such systems lack a mechanism to provide real-time multicasting where the decoding delay is the same for all users; but each user reconstructs the source at a possibly different distortion, level, depending on its own channel capacity.","A source channel encoder, source channel decoder and methods for implementing such devices are disclosed herein. The source channel encoder includes a linear transform encoder configured to generate a plurality of source components. A successive refinement quantizer is configured to generate a plurality of bit planes based on the source components. A systematic linear encoder is configured to map the bit planes into channel-encoded symbols. The linear transform encoder may be configured to apply a Discrete Cosine Transform (DCT) or a Discrete Wavelet Transform (DWT). The linear transform encoder may be configured for differential encoding. The quantizer may be configured to generate a top (most significant) bit plane and a plurality of lower significance bit planes configured with decreasing resolution. The systematic linear encoder may include a Raptor encoder. The systematic linear encoder may be configured to directly map the bits planes into the channel-encoded symbols, e.g., without an intervening compression function. The systematic linear encoder may be configured to generate a minimum number of channel coded symbols based on a pre-determined distortion level. The systematic linear encoder may be configured to minimize a weighted sum of distortion levels based on a pre-determined number of channel coded symbols. The systematic linear encoder may be configured to minimize a maximum gap with respect to individual optimal distortion across all users based on a pre-determined number of channel coded symbols.","The source channel decoder includes a plurality of Belief Propagation (BP) decoders configured for successive decoding. Each BP decoder is associated with a single bit plane and is configured to generate a soft estimate of the bit plane based on an a-posteriori Log-Likelihood Ratio (LLR). A reconstruction engine is configured to produce an estimate of a source sample based on the a-posteriori LRRs generated by the BP decoders. The BP decoders may be configured to generate a hard estimate based on a Maximum A-posteriori Probability (MAP). The hard estimate may be passed to a successive BP decoder stage. The reconstruction engine may be configured to produce an estimate of the source sample based on minimum mean-square error (MMSE) criterion.","The method of encoding a source includes transforming the source using a linear transform and generating a plurality of source components. Successive refinement quantization is performed and a plurality of bit planes are generated based on the source components. Systematic linear encoding is performed to map the bit planes into channel-encoded symbols. The linear transform may be a Discrete Cosine Transform (DCT) or a Discrete Wavelet Transform (DWT). The linear transform may generate differential encoding. Top (most significant) bit plane and a plurality of lower significance bit planes configured with decreasing resolution may be generated. Raptor encoding may be performed. The bits planes may be directly mapped into the channel-encoded symbols. A minimum number of channel coded symbols may be generated based on a pre-determined distortion level. A weighted sum of distortion levels may be minimized based on a pre-determined number of channel coded symbols. A maximum gap may be minimized with respect to individual optimal distortion across all users based on a pre-determined number of channel coded symbols.","The decoding method includes successively decoding a plurality of bit planes and generating a soft estimate of each bit plane based on an a-posteriori Log-Likelihood Ratio (LLR). An estimate of a source sample is generated based on the a-posteriori LRRs. A hard estimate may be generated based on a Maximum A-posteriori Probability (MAP). The hard estimate may be passed to a successive decoder stage. An estimate of the source sample may be generated based on minimum mean-square error (MMSE) criterion.","Multimedia streaming over heterogeneous digital networks is one of the fastest growing applications both in terms of traffic demands and in terms of market potential. Systems can include a server, multicasting a multi-media source (audio\/video) to a number of clients. In one embodiment, a system can include support at the application layer. The underlying network (from the server to the clients) can be modeled as an erasure broadcast channel. Erasures represent packets that are lost in the network, for any reason such packet dropped due to congested nodes, or packet dropped because corrupted by unrecoverable channel errors. The underlying network may encompass any sort of heterogeneous wired or wireless network without departing from the scope of this disclosure. For example, the system may be formed using an Internet backbone, connected to a wireless router, or to a wireless cellular base station, or even with a DSL line to the end users.","A multimedia streaming application is generally an overlay system, that is, it operates at the application layer by using a fixed transport mechanism already built-in to the network, e.g., the User Datagram Protocol (UDP) currently used as an alternative to TCP on top of the ubiquitous Internet Protocol (IP). A conventional approach consists of establishing individual streaming sessions from the server to each user. This may be very inefficient when many users wish to receive the same content (e.g., in mobile television or video on demand applications). As disclosed herein, it is possible to use an efficient scalable Joint Source-Channel Coding (JSCC) for the transmission of a source over an Erasure Broadcast Channel. This approach is referred to as JSCC at the application layer since the underlying physical channels is disregarded, in contrast to most common JSCC approaches.","A system may be formed as follows: at the encoder (server), a natural multimedia source is encoded using standard transform coding, with possible differential encoding in order to reduce the redundancy in the time domain. The subbands of the transform\/differential encoder are quantized by multiresolution (embedded) quantization, thus producing redundant bit-streams for successive refinement. Instead of using conventional entropy coding (e.g., arithmetic coding) in order to remove the redundancy in the bit-streams, the redundant bits are directly mapped onto channel-encoded symbols, using a family of linear encoders. The linear encoding of the bit-streams is performed independently, for each bit-stream, and may be implemented by any good family of systematic linear codes with tunable rate (for example, this can be obtained using a rateless code, a library of punctured turbo codes, a library of LDPC codes etc. . . . ). The allocation of the source layers to different users is optimized according to an optimal resource allocation algorithm that can incorporate suitable distortion criteria. At each decoder (client), the source layers are decoded successively, up to the layer allocated to the given user. Decoding may be performed using low-complexity Belief Propagation iterative decoding. The source may be reconstructed according to the minimum mean-square error (MMSE) criterion using softbit reconstruction. The system may be implemented using rateless codes and in particular Raptor Codes, since they offer a simple way to generate a very flexible adaptive rate encoding.","This disclosure encompasses systems used in real-time multicasting where the decoding delay is the same for all users, but each user reconstructs the source at a possibly different distortion level, depending on its own channel capacity. The embodiments disclosed herein can be naturally applied to static broadcasting (e.g., video on demand applications), if used jointly with well-known protocols such as harmonic broadcasting. Simulation results may be used to assess the effectiveness the proposed coding scheme showing that it can achieve end-to-end distortion performance very close to the theoretical limits with finite block length and low encoding\/decoding complexity.","Encoding",{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 1","sub":["1","2","L","l","l","l ","l"]},"A system may be formed with the following building blocks: an encoder (at the server), and decoders (at the clients). A description of the encoding and decoding processes is set forth below. For the sake of simplicity, the specific example of sending a digital picture is used. The disclosed techniques may be directly applied to video, with frame-by-frame encoding. As detailed below, the same encoding principle can be combined with standard differential encoding for video, in order to encode the residual frames after the elimination of the time-redundancy by motion compensation (as in the MPEG-2, MPEG-4 and H.264 standards). Also, the disclosed techniques may be combined with so-called harmonic broadcasting (also known as skyscraper architecture), in order to implement video on demand applications with low waiting time.",{"@attributes":{"id":"p-0027","num":"0026"},"figref":["FIG. 2","FIG. 2"],"i":["a ","b "],"b":["50","50","52","54","46","58","50","50"]},"1) Transform encoding, e.g., differential encoding in the case of video, followed by a linear transform of the resulting frames, e.g., Discrete Cosine Transform (DCT), Discrete Wavelet Transform (DWT). This example is directed to the encoding of a single image (single frame) and use of a DWT generally in accordance with the JPEG-2000 image compression standard. It should be understood that the techniques disclosed herein may be used with any suitable linear transform. In this example, S is an s\u00d7k array of transform coefficients. Each row of S, denoted by S, is referred to as a source component (encoded separately).","2) Quantization\u2014each source component Sis quantized by using a family of embedded nonuniform scalar quantizers. In this example, the quantization function is denoted as Q:\u2192{0,1}, where 2is the number of quantization regions for the highest level of refinement. D(p) denotes the distortion for the unit-variance source, with 0\u2266p\u2266P quantization bits, it follows that the distortion for the i-th source component is given by D(p)=\u03c3D(p), where \u03c3denotes the variance of S. The quantizer output for source Sis a binary array Uof dimension P\u00d7k, such that each column ucontains the binary representation of the quantization of the corresponding source symbol s. The rows of U, denoted by Ufor p=1, . . . , P, are referred to as bit-planes. Given the multiresolution embedded nature of the quantizer, the source is going to be reconstructed with distortion \u03c3D(p), provided that the top (most significant) p bit-planes are used by the decoder, while disregarding completely the bottom (least significant) P-p bit-planes. The number of bit-planes that must be used for reconstruction by decoders in each user class l is indicated by \u03c0. The determination of the values \u03c0for all source components i and user class l defines a bit-allocation. The determination of the optimal bit-allocation for a given source frame S and users' capacities C\u2266 . . . \u2266Cdepends on the desired objective function to be optimized. This forms the so-called resource allocation problem, is discussed below.","3) Linear encoding of the bit-planes into channel symbols\u2014 is a diagram showing possible bit-plane allocation to layers. For the given \u03c0,: \u03c0,:,\u03c0,:, \u03c0,: values, corresponding layer numbers are indicated for each bit plane. Let \u03c0denote the number of bit-planes of source component i that must be decoded by user l. Each bit-plane is encoded separately and the coding rate is allocated such that each user l can successively recover the bit-planes \u2266p\u2266\u03c0for each i=1, . . . , s. The bit-planes U, for \u03c0<p\u2266\u03c0, are mapped into codewords X=UG, where Gdenotes the k\u00d7ngenerator matrix of a suitable linear code. This approach uses a family of linear codes with block length and rate flexibility. In brief, we need an encoding Gfor dimension k\u00d7nwith any k and ratio 0\u2266k\/n\u22661. In order to accomplish this great block length and rate flexibility, a systematic Raptor Encoder may be used. This can be seen as a method for constructing such family of encoding matrices, such that each resulting linear code is an efficient code for the binary erasure channel. For completeness, Appendix A summarizes the main features of such a linear encoder. The overall channel-encoded block X is formed by the concatenation of all codewords",{"@attributes":{"id":"p-0031","num":"0030"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mo":["{","}"],"msubsup":{"mi":"X","mrow":[{"mo":["(",")"],"mrow":{"mi":"p","mo":[",",":"]}},{"mo":["(",")"],"mi":"i"}]}}}},"br":{}},{"@attributes":{"id":"p-0032","num":"0031"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"n","mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"s"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"l","mo":"=","mn":"1"},"mi":"L"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"p","mo":"=","mrow":{"msub":{"mi":"\u03c0","mrow":{"mi":"i","mo":",","mrow":{"mi":"l","mo":"-","mn":"1"}}},"mo":"+","mn":"1"}},"msub":{"mi":"\u03c0","mrow":{"mi":["i","l"],"mo":","}}},"mo":"\u2062","msub":{"mi":"n","mrow":{"mi":["p","i"],"mo":","}}}}}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"1"}}]}}}}},"where, by definition, \u03c0=0. Due to the degraded nature of the Erasure Broadcast Channel, the user l is able to decode all blocks (p,i) for 1\u2266p\u2266\u03c0and i=1, . . . , s. The bit-planes can be decoded in sequence, using the multi-stage decoder described below. The overall number of channel encoded binary symbols per source symbol is given by n\/(sk).","Decoding","The following disclosure focuses on the decoder of user class l. For each source component, the decoder wishes to reconstruct an approximation, in the MMSE sense, of the source block S. In order to do so, it decodes the bit-planes , , . . . , \u03c0for all i=1, . . . , s. The bit-planes of each source component can be decoded separately from the other source components. We focus on the decoder for the i-th source component. The same technique may be applied to all source components i=1, . . . , s and for all user classes l=1, . . . , L.",{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 4","b":["20","20","42","44","46","42","44","46","48"],"sub":"i,l"},"The input lines ,  and  indicate the flow of the received signal (from the l-th channel output), lines ,  and  indicate the flow of the hard decisions, propagated through the successive decoding stages, and lines , ,  and  indicate the flow of the decoders soft-output and reconstructed estimated source block. As discussed above, the decoder  may also include one or more processors . The decoder  can be implemented in hardware, software or a combination of both as is well known in the art with respect to such devices. It should also be understood that decoder  may be embedded within other devices such as a client device, e.g., a computing device, network appliance, mobile phone, television or other multimedia device. The decoder functionality is carried out by the following building blocks:","Successive Decoding: In order to decode the (p, i)-th bit-plane, the decoder must make use of the conditional a-priori probability (U|(U, . . . , U) of the p-th bit-plane, conditioned on the previous bit-planes , . . . , p\u22121. The conditional probability mass function (U|U, . . . , U) is encoded and sent as a header information directly to the decoders, and therefore it is known to the decoders.","The bit-planes may be decoded in sequence (successive decoding). Bit-plane (Umay be decoded first, using the a-priori probability mass function (U), known from the header information. This is decoder stage . Then, bit-plane (Umay be decoded using the probability mass function (U|\u00db), where \u00dbdenotes the decoded bit-plane decision made at stage . In decoder stage , bit-plane (Ube decoded using the probability mass function (U|\u00db, \u00db), where \u00dbthe decoded bit-plane decision made at stage . In decoder stage , the successive decoding process continues until the required bit-plane index \u03c0.","Iterative Message Passing Decoding with soft-output, for each stage: Let Ydenote the channel output received by user l corresponding to the transmission of X. The optimal decoding rule at each stage p of the multi-stage successive decoder operates according to the Maximum A-posteriori Probability (MAP) principle, according to the rule",{"@attributes":{"id":"p-0041","num":"0040"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msubsup":{"mover":{"mi":"U","mo":"^"},"mrow":[{"mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"p","mo":"."},"mo":":"}},"mo":",","mi":"l"},{"mo":["(",")"],"mi":"i"}]},"mo":"=","mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mi":"max","mrow":{"mi":"u","mo":"\u2208","msup":{"mrow":{"mo":["{","}"],"mrow":{"mn":["0","1"],"mo":","}},"mi":"k"}}},"mo":"\u2062","mrow":{"mrow":{"mi":"\u2119","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"uG","mrow":{"mi":["p","i"],"mo":","}},"mo":"|","mrow":{"mrow":{"msubsup":[{"mi":"Y","mrow":[{"mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"p","mo":"."},"mo":":"}},"mo":",","mi":"l"},{"mo":["(",")"],"mi":"i"}]},{"mover":{"mi":"U","mo":"^"},"mrow":[{"mrow":{"mo":["(",")"],"mrow":{"mn":"1.","mo":":"}},"mo":",","mi":"l"},{"mo":["(",")"],"mi":"i"}]}],"mo":"\u00b7"},"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"\u2026","msubsup":{"mover":{"mi":"U","mo":"^"},"mrow":[{"mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"p","mo":"-","mn":"1."},"mo":":"}},"mo":",","mi":"l"},{"mo":["(",")"],"mi":"i"}]}}}}},"mo":"."}}}}},{"mrow":{"mo":["(",")"],"mn":"2"}}]}}}}},"For sparse-graph codes, the MAP rule above would be computationally too complex. Instead, a low complexity iterative Belief Propagation algorithm (BP) may be used at each stage of the multi-stage decoder. Raptor Codes may be used for the linear encoding of the bit-planes. The BP decoder, at each stage p, produces a sequence of Extrinsic Log-Likelihood Ratios (Ext-LLRs) (see Appendix A) denoted by {\u03be: j=1, . . . , k}. The Ext-LLRs are combined with the source a-priori LLRs, given by \u03bb(u, . . . , u), as defined in Appendix B, in order to obtain the a-posteriori LLRs, given by \u039b(u, . . . , u)=\u03be+\u03bb(u, . . . , u). Then, for each quantization bit (p, j) a hard-decision is made according to the MAP symbol-by-symbol rule:",{"@attributes":{"id":"p-0043","num":"0042"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msubsup":{"mover":{"mi":"U","mo":"^"},"mrow":[{"mrow":{"mo":["(",")"],"mrow":{"mi":["p","j"],"mo":","}},"mo":",","mi":"l"},{"mo":["(",")"],"mi":"i"}]},"mo":"=","mrow":{"mo":"{","mtable":{"mtr":[{"mtd":[{"mn":"0"},{"mrow":{"mrow":{"mi":"if","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"msubsup":{"mi":"\u039b","mrow":[{"mrow":{"mo":["(",")"],"mrow":{"mi":["p","j"],"mo":","}},"mo":",","mi":"l"},{"mo":["(",")"],"mi":"i"}]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mover":{"mi":"U","mo":"^"},"mrow":[{"mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"p","mo":"-","mn":"1"},"mo":",","mi":"j"}},"mo":",","mi":"l"},{"mo":["(",")"],"mi":"i"}]},{"mover":{"mi":"U","mo":"^"},"mrow":[{"mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"p","mo":"-","mn":"1"},"mo":",","mi":"j"}},"mo":",","mi":"l"},{"mo":["(",")"],"mi":"i"}]}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"\u2026"}}}},"mo":"\u2265","mn":"0"}}]},{"mtd":[{"mn":"1"},{"mrow":{"mi":"otherwise","mo":"."}}]}]}}}},{"mrow":{"mo":["(",")"],"mn":"3"}}]}}}}},"These hard decisions are passed to the next stage p+1.","Soft-bit MMSE reconstruction: the collection of all a-posteriori LLRs {} for all p, j and i are used for the source reconstruction according to the minimum MSE (MMSE) criterion, according to the rule (see Appendix B):",{"@attributes":{"id":"p-0046","num":"0045"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msubsup":{"mover":{"mi":"S","mo":"^"},"mrow":[{"mi":["j","l"],"mo":","},{"mo":["(",")"],"mi":"i"}]},"mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"msub":{"mi":"u","mn":"1"},"mo":"=","mn":"0"},"mn":"1"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"mi":"\u2026","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"munderover":{"mo":"\u2211","mrow":{"msub":{"mi":"u","msub":{"mi":"\u03c0","mrow":{"mi":["i","l"],"mo":","}}},"mo":"=","mn":"0"},"mn":"1"},"mo":"\u2062","mrow":{"mrow":[{"msup":{"mi":"c","mrow":{"mo":["(",")"],"mi":"i"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"u","mn":"1"},{"mi":"u","msub":{"mi":"p","mrow":{"mi":["i","l"],"mo":","}}}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"\u2026"}}},{"munderover":{"mo":"\u220f","mrow":{"mi":"p","mo":"=","mn":"1"},"mi":"\u03c0"},"mo":"\u2062","mrow":{"mfrac":{"msup":{"mi":"\u2147","mrow":{"mrow":[{"mo":"-","msub":{"mi":["u","p"]}},{"msubsup":{"mi":"\u039b","mrow":[{"mrow":{"mo":["(",")"],"mrow":{"mi":["p","j"],"mo":","}},"mo":",","mi":"l"},{"mo":["(",")"],"mi":"i"}]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"u","mn":"1"},{"mi":"u","mrow":{"mi":"p","mo":"-","mn":"1"}}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"\u2026"}}}],"mo":"\u2062"}},"mrow":{"mn":"1","mo":"+","msup":{"mi":"\u2147","mrow":{"msubsup":{"mi":"\u039b","mrow":[{"mrow":{"mo":["(",")"],"mrow":{"mi":["p","j"],"mo":","}},"mo":",","mi":"l"},{"mo":["(",")"],"mi":"i"}]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"u","mn":"1"},{"mi":"u","mrow":{"mi":"p","mo":"-","mn":"1"}}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"\u2026"}}}}}},"mo":"."}}],"mo":"\u2062"}}}}}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}}}}},"where c(u, . . . , u) denotes the centroid of the quantization bin indexed by (u, . . . , u) for the i-th source component embedded quantizer, at the u-th resolution level. The rule above can be interpreted as the weighted sum of all centroids of the u-th resolution level of the quantizer, with weights given by the posterior probabilities of the quantization bits decoded by the multi-stage BP decoder.","Resource Allocation","The Binary Erasure Broadcast Channel (BEBC) with L receiver classes has input alphabet {0,1}, output alphabet {0,1,e} (\u201ce\u201d denoting erasure), and is defined by L Binary-Erasure Channels (BECs) with erasure probability \u03b5\u2266 . . . \u2266\u03b5and capacity C=1\u2212\u03b5. For simplicity, each class is referred to as a \u201cuser\u201d since, in a multicast scenario, all users belonging to the same class are statistically equivalent and achieve the same performance. The capacity region of the BEBC is given by:",{"@attributes":{"id":"p-0050","num":"0049"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"C","mo":"=","mrow":{"mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mrow":[{"mo":["(",")"],"mrow":{"msub":[{"mi":"R","mn":"1"},{"mi":["R","L"]}],"mo":[",","\u2062",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}},{"msub":{"mi":["R","l"]},"mo":["\u2265","\u2264"],"mrow":{"mn":"0","mo":"\u00b7","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"l","mo":"=","mn":"1"},"mi":"L"},"mo":"\u2062","mfrac":{"msub":[{"mi":["R","l"]},{"mi":["C","l"]}]}}},"mn":"1"}],"mo":":"},{"mi":"l","mo":"\u2208","mrow":{"mo":["{","}"],"mrow":{"mn":"1","mo":[",","\u2062",","],"mi":["\u2026","L"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}}}],"mo":","}},"mo":"."}}},{"mrow":{"mo":["(",")"],"mn":"5"}}]}}}}},"This region is achieved by time-sharing of the coding strategies that achieve the vertices of the region's dominant face, corresponding to the individual user capacities. A \u201cbroadcast code\u201d for the BEBC is obtained by multiplexing (time-sharing) single-user codewords for the different users. The BEBC belongs to the class of stochastically degraded broadcast channels. Whatever can be decoded by user l, can also be decoded by all users j>l.","An (s\u00d7k)-to-n source-channel multicast code for the BEBC is formed by an encoding function \u0192: \u2192{0,1}such that S is mapped into the channel input codeword X=\u0192(S), and by L decoding functions g: {0,1,e}\u2192, for l=1, . . . , L, such that at each l-th user receiver, the received channel output Yis mapped into the reconstructed source block \u015c=g(Y).","The details of the encoding function \u0192(\u00b7) and decoding functions g(\u00b7) for the proposed scheme are detailed above. The following disclosure is directed to optimizing system parameters in order to achieve optimal distortion according to certain specific criteria. A Weighted MSE (WMSE) distortion measure may be defined as follows. Let the MSE distortion for the l-th decoder and the i-th source component be given by",{"@attributes":{"id":"p-0054","num":"0053"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":"D","mrow":{"mi":["i","l"],"mo":","}},"mo":"=","mrow":{"mfrac":{"mn":"1","mi":"k"},"mo":"\u2062","mrow":{"mrow":{"mi":"\ud835\udd3c","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msup":{"mi":"S","mrow":{"mo":["(",")"],"mi":"i"}},"mo":"-","msubsup":{"mover":{"mi":"S","mo":"^"},"mi":"l","mrow":{"mo":["(",")"],"mi":"i"}}}},"mn":"2"}}},"mo":"."}}}}},"br":{}},{"@attributes":{"id":"p-0055","num":"0054"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msub":{"mi":["D","l"]},"mo":"=","mrow":{"mfrac":{"mn":"1","mi":"s"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"s"},"mo":"\u2062","mrow":{"msub":[{"mi":["v","i"]},{"mi":"D","mrow":{"mi":["i","l"],"mo":","}}],"mo":"\u2062"}}}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"6"}}]}}}}},"where {\u03c5\u22670: i=1, . . . , s}, are weights that depend on the specific application. Let r(\u00b7) denote the R-D function of the i-th source component with respect to the MSE distortion. Then the R-D function of S is given by:",{"@attributes":{"id":"p-0057","num":"0056"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":[{"mi":"\u211b","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"D"}},{"mi":"min","mo":["\u2062","\u2062"],"mfrac":{"mn":"1","mi":"s"},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"s"},"mo":"\u2062","mrow":{"msub":{"mi":["r","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["d","i"]}}}}}],"mo":"="},{"mrow":[{"mi":["subject","to"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mfrac":{"mn":"1","mi":"s"},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"s"},"mo":"\u2062","mrow":{"msub":[{"mi":["v","i"]},{"mi":["d","i"]}],"mo":"\u2062"}}},{"mi":"D","mo":"."}],"mo":"="}],"mo":","}},{"mrow":{"mo":["(",")"],"mn":"7"}}]}}}}},"Where {d: i=1, . . . , s} are dummy optimization variables, corresponding to the individual MSE distortions of the i-th source components.","The functions {r(d): i=1, . . . , s} depend the specific scalar embedded quantizer Q(\u00b7) as disclosed above. These functions are convex and non-increasing, and identically zero for",{"@attributes":{"id":"p-0060","num":"0059"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"d","mo":"\u2265","mrow":{"mfrac":{"mn":"1","mi":"k"},"mo":"\u2062","mrow":{"mrow":{"mi":"\ud835\udd3c","mo":"\u2061","mrow":{"mo":["[","]"],"msup":{"mrow":{"mo":["\uf605","\uf606"],"msup":{"mi":"S","mrow":{"mo":["(",")"],"mi":"i"}}},"mn":"2"}}},"mo":"."}}}}},"br":{},"sub":"l"},"Assume that b=n\/(ks) is the number of channel-coded bit per source sample (referred to as \u201cpixel\u201d in the following). The ratio b measures how many channel-encoded bits must be sent, on average, for each source pixel, and it is indicated in bit-per-pixel (bpp). Let R\u2032, for i=1, . . . , s and l=1, . . . , L, denote the source coding rate of source component i at successive refinement level l. This corresponds to kR\u2032information bits. Let Rdenote the channel coding rate of layer l. Then, we have",{"@attributes":{"id":"p-0062","num":"0061"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":{"mfrac":{"mn":"1","mi":"s"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"s"},"mo":"\u2062","msubsup":{"mi":["R","\u2032"],"mrow":{"mi":["i","l"],"mo":","}}}},"mo":"=","msub":{"mi":["bR","l"]}},"mo":","}}},"br":{},"sub":["1","L"]},{"@attributes":{"id":"p-0063","num":"0062"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["D","l"]},"mo":"=","mrow":{"mrow":[{"msup":{"mi":"\u211b","mrow":{"mo":"-","mn":"1"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mfrac":{"mn":"1","mi":"s"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"l"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"s"},"mo":"\u2062","msubsup":{"mi":["R","\u2032"],"mrow":{"mi":["i","l"],"mo":","}}}}}}},{"mrow":{"msup":{"mi":"\u211b","mrow":{"mo":"-","mn":"1"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"b","mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"l"},"mo":"\u2062","msub":{"mi":"R","mrow":{"mi":"j","mo":"\u2062","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}}}}},"mo":"."}],"mo":"="}}},{"mrow":{"mo":["(",")"],"mn":"8"}}]}}}}},"As disclosed in the Encoding section, sparse-graph codes (in particular, Raptor Codes) may be used to perform the linear encoding of the bit-plane bits into channel codewords. A common feature of these codes is that they exhibit a post-decoding erasure rate characterized by a very sharp \u201cwaterfall\u201d behavior: if R<C\/(1+\u03b8), for some code-dependent factor \u03b8>0, then the residual erasure probability is extremely small (in fact, it vanishes in the limit of infinite block length). Otherwise, for R>C\/(1+\u03b8), the residual erasure probability is \u2248\u03b5=1\u2212C, i.e., the code is ineffective to recover the channel erasures. The code operates \u201cbelow threshold\u201d if R<C\/(1+\u03b8). Otherwise, the code operates \u201cabove threshold\u201d. Since Raptor codes may be used, \u03b8 is referred to as the coding overhead.","This layered approach is defined by the family of successive refinement source codes described by their R-D functions {r(d): i=1, . . . , s}, and by a family of channel codes characterized by their overhead parameters {\u03b8:l=1, . . . , L}. Three system optimization problems, for three different performance criteria are set forth below:","Minimum Bandwidth (MB): For given user target distortions \u0394\u2267 . . . \u2267\u0394, we wish to minimize the number of coded bpp b. Using the equations disclosed above, we have the following problem:",{"@attributes":{"id":"p-0067","num":"0066"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":["Minimize","b"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},{"mrow":[{"mrow":[{"mi":["subject","to","b"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"l"},"mo":"\u2062","msub":{"mi":["R","j"]}}},{"mi":"R","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["\u0394","l"]}}}],"mo":"\u2265"},{"mo":"\u2200","mi":"l"}],"mo":","},{"mrow":[{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"l","mo":"=","mn":"1"},"mi":"L"},"mo":"\u2062","mfrac":{"mrow":{"msub":{"mi":["R","l"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"+","msub":{"mi":["\u03b8","l"]}}}},"msub":{"mi":["C","l"]}}},"mo":"\u2264","mn":"1"},{"msub":{"mi":["R","l"]},"mo":"\u2265","mrow":{"mn":"0","mo":"\u2062","mrow":{"mo":"\u2200","mrow":{"mi":"l","mo":"."}}}}],"mo":","}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mtext":{}},{"mtext":{}}]}},{"mrow":{"mo":["(",")"],"mn":"9"}}]}}}}},"This problem is immediately solved as follows: letting R(\u0394)=0, we find that a necessary condition for optimality is that the inequyities bR\u2267R(\u0394)\u2212R(\u0394) holds with equality for all l=1, . . . , L. Replacing these equalities into the capacity region constraint, we obtain the solution:",{"@attributes":{"id":"p-0069","num":"0068"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"b","mo":"*"},"mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"l","mo":"=","mn":"1"},"mi":"L"},"mo":"\u2062","mrow":{"mfrac":{"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"mrow":[{"mi":"\u211b","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["\u0394","l"]}}},{"mi":"R","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"\u0394","mrow":{"mi":"l","mo":"-","mn":"1"}}}}],"mo":"-"}},{"mo":["(",")"],"mrow":{"mn":"1","mo":"+","msub":{"mi":["\u03b8","l"]}}}],"mo":"\u2062"},"msub":{"mi":["C","l"]}},"mo":"."}}}},{"mrow":{"mo":["(",")"],"mn":"10"}}]}}}}},"Min Weighted Total Distortion (MWTD): For given b and non-negative weights {}, we wish to minimize \u03a3D. The problem can be formulated as:",{"@attributes":{"id":"p-0071","num":"0070"},"maths":{"@attributes":{"id":"MATH-US-00015","num":"00015"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"minimize","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"l","mo":"=","mn":"1"},"mi":"L"},"mo":"\u2062","mrow":{"msub":{"mi":["w","l"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mfrac":{"mn":"1","mi":"s"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"s"},"mo":"\u2062","mrow":{"msub":[{"mi":["v","i"]},{"mi":"D","mrow":{"mi":["i","l"],"mo":","}}],"mo":"\u2062"}}}}}}},{"mrow":{"mi":["subject","to"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"l","mo":"=","mn":"1"},"mi":"L"},"mo":"\u2062","mfrac":{"mrow":{"msub":{"mi":["R","l"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"+","msub":{"mi":["\u03b8","l"]}}}},"msub":{"mi":["C","l"]}}}},"mo":"\u2264","mn":"1"},{"msub":{"mi":["R","l"]},"mo":"\u2265","mrow":{"mn":"0","mo":"\u2062","mrow":{"mo":"\u2200","mi":"l"}}},{"mrow":[{"mi":"R","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mfrac":{"mn":"1","mi":"s"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"s"},"mo":"\u2062","mrow":{"msub":[{"mi":["v","i"]},{"mi":"D","mrow":{"mi":["i","l"],"mo":","}}],"mo":"\u2062"}}}}},{"mi":"b","mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"l"},"mo":"\u2062","msub":{"mi":["R","j"]}}}],"mo":"\u2264"},{"mo":"\u2200","mi":"l"},{"msubsup":{"mi":["\u03c3","i"],"mn":"2"},"mo":["\u2265","\u2265","\u2062","\u2062","\u2265","\u2265"],"msub":[{"mi":"D","mrow":{"mi":"i","mo":",","mn":"1"}},{"mi":"D","mrow":{"mi":["i","L"],"mo":","}}],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"\u2026","mn":"0"},{"mo":"\u2200","mi":"i"}],"mo":["\u2062","\u2062",",",",","\u2062",",",",",",",","],"mstyle":[{"mtext":{}},{"mtext":{}}]}},{"mrow":{"mo":["(",")"],"mn":"11"}}]}}}}},"where the optimization variables are the partial source-layer distortions {D} and the channel coding rates {R}, and where \u03c3denotes the variance (per symbol) of the i-th source component S.","Min-Max Distortion Penalty (MMDP): For given b, we wish to minimize maxD\/D, where D=R(bC) is the R-D bound for user l as if it was the only user in the system (single-user bound) and ideal (capacity achieving) channel coding was possible. The problem can be formulated as:",{"@attributes":{"id":"p-0074","num":"0073"},"maths":{"@attributes":{"id":"MATH-US-00016","num":"00016"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":["Minimize","\u03b1"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},{"mrow":[{"mrow":{"mi":["subject","to"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"l","mo":"=","mn":"1"},"mi":"L"},"mo":"\u2062","mfrac":{"mrow":{"msub":{"mi":["R","l"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"+","msub":{"mi":["\u03b8","l"]}}}},"msub":{"mi":["C","l"]}}}},"mo":"\u2264","mn":"1"},{"msub":{"mi":["R","l"]},"mo":"\u2265","mrow":{"mn":"0","mo":"\u2062","mrow":{"mo":"\u2200","mi":"l"}}},{"mrow":[{"mi":"R","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mfrac":{"mn":"1","mi":"s"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"s"},"mo":"\u2062","mrow":{"msub":[{"mi":["v","i"]},{"mi":"D","mrow":{"mi":["i","l"],"mo":","}}],"mo":"\u2062"}}}}},{"mi":"b","mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"l"},"mo":"\u2062","msub":{"mi":["R","j"]}}}],"mo":"\u2264"},{"mo":"\u2200","mi":"l"}],"mo":[",","\u2062",",","\u2062",","],"mstyle":[{"mtext":{}},{"mtext":{}}]},{"mrow":[{"mrow":[{"mfrac":{"mn":"1","mi":"s"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"s"},"mo":"\u2062","mrow":{"msub":[{"mi":["v","i"]},{"mi":"D","mrow":{"mi":["i","l"],"mo":","}}],"mo":"\u2062"}}},{"mi":"\u03b1","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":["D","l","opt"]}}],"mo":"\u2264"},{"mo":"\u2200","mi":"l"},{"msubsup":{"mi":["\u03c3","i"],"mn":"2"},"mo":["\u2265","\u2265","\u2062","\u2062","\u2265","\u2265"],"msub":[{"mi":"D","mrow":{"mi":"i","mo":",","mn":"1"}},{"mi":"D","mrow":{"mi":["i","L"],"mo":","}}],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"\u2026","mn":"0"},{"mo":"\u2200","mi":"i"}],"mo":[",",",",",",","]}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mtext":{}},{"mtext":{}}]}},{"mrow":{"mo":["(",")"],"mn":"12"}}]}}}}},"where the optimization is with respect to a, Dand {R}. The following sections particularize the above problems to some significant cases.","In order to apply the general optimization problems to this setting, we need to define the R-D functions r(d). Let U=Q(S) denote the sequence of binary quantization indices, formatted as an P\u00d7k binary array. The p-th row of U, denoted by U, is referred to as the p-th \u201cbit-plane\u201d. Without loss of generality, we let Udenote the sign bit-plane, and U, . . . , Udenote the magnitude bit-planes with decreasing order of significance. The quantizer output U(can be considered as a discrete memoryless source, with entropy rate",{"@attributes":{"id":"p-0077","num":"0076"},"maths":{"@attributes":{"id":"MATH-US-00017","num":"00017"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msup":{"mi":"H","mrow":{"mo":["(",")"],"mi":"i"}},"mo":"=","mrow":{"mfrac":{"mn":"1","mi":"k"},"mo":"\u2062","mrow":{"mi":"H","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":"U","mrow":{"mo":["(",")"],"mi":"i"}}}}}}}},"br":{},"sup":["(i)","p","(i)"],"sub":["p=1","p"]},{"@attributes":{"id":"p-0078","num":"0077"},"maths":{"@attributes":{"id":"MATH-US-00018","num":"00018"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msubsup":{"mi":["H","p"],"mrow":{"mo":["(",")"],"mi":"i"}},"mo":"=","mrow":{"mfrac":{"mn":"1","mi":"k"},"mo":"\u2062","mrow":{"mi":"H","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":"U","mrow":[{"mo":["(",")"],"mrow":{"mi":"p","mo":[",",":"]}},{"mo":["(",")"],"mi":"i"}]},"mo":"|","mrow":{"msubsup":[{"mi":"U","mrow":[{"mo":["(",")"],"mrow":{"mn":"1","mo":[",",":"]}},{"mo":["(",")"],"mi":"i"}]},{"mi":"U","mrow":[{"mo":["(",")"],"mrow":{"mrow":{"mi":"p","mo":"-","mn":"1"},"mo":[",",":"]}},{"mo":["(",")"],"mi":"i"}]}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"\u2026"}}}}}},{"mi":"p","mo":"=","mrow":{"mn":"1","mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"\u2026","mrow":{"mi":"P","mo":"."}}}],"mo":","}},{"mrow":{"mo":["(",")"],"mn":"13"}}]}}}}},"Then, the set of R-D points achievable by the embedded scalar quantizer using 0, 1, . . . , P quantization bits is given by:",{"@attributes":{"id":"p-0080","num":"0079"},"maths":{"@attributes":{"id":"MATH-US-00019","num":"00019"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"p"},"mo":"\u2062","msubsup":{"mi":["H","j"],"mrow":{"mo":["(",")"],"mi":"i"}}},{"msub":{"mi":"D","mrow":{"mi":["Q","i"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"p"}}],"mo":","}},{"mi":"p","mo":"=","mn":"0"}],"mo":[",",",","\u2062",",",","],"mi":["\u2026","P"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}},{"mrow":{"mo":["(",")"],"mn":"14"}}]}}}}},"where, by definition, D(0)=\u03c3. Using time-sharing, any point in the convex hull of the above achievable points is also achievable. Finally, the operational R-D curve r(d) of the scalar quantizer is given by the lower convex envelope of the convex hull of the points in the equation above. It is easy to see that since P is finite, then r(d) is a piecewise linear function. Therefore, the resulting function r(d) is convex and decreasing on the domain, D(P)\u2266d\u2266\u03c3.  shows, qualitatively, the typical shape of the functions r(d), a piecewise linear operational R-D function for the i-th source corresponding to a set of discrete R-D points.","As seen from , it is possible to represent r(d) as the pointwise maximum of a set of lines joining consecutive points in the set in the equation above. Hence, we can write:",{"@attributes":{"id":"p-0083","num":"0082"},"maths":{"@attributes":{"id":"MATH-US-00020","num":"00020"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":[{"msub":{"mi":["r","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"d"}},{"munder":{"mi":"max","mrow":{"mrow":[{"mi":"p","mo":"=","mn":"1"},{"mi":["\u2026","P"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}],"mo":[",","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":{"msub":{"mi":"a","mrow":{"mi":["i","p"],"mo":","}},"mo":"\u2062","mi":"d"},"mo":"+","msub":{"mi":"b","mrow":{"mi":["i","p"],"mo":","}}}}}],"mo":"="},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"15"}}]}}}}},"where the coefficients aand bare obtained from the prior equation (details are omitted for the sake of brevity). It is also possible to obtain the operational R-D function of the parallel source as the solution of a linear program. Introducing the auxiliary variables we have:",{"@attributes":{"id":"p-0085","num":"0084"},"maths":{"@attributes":{"id":"MATH-US-00021","num":"00021"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":[{"mi":"R","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"D"}},{"msub":{"mi":"min","mrow":{"mrow":[{"mo":["{","}"],"msub":{"mi":["d","i"]}},{"mo":["{","}"],"msub":{"mi":["\u03b3","i"]}}],"mo":","}},"mo":"\u2062","mrow":{"mfrac":{"mn":"1","mi":"s"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"s"},"mo":"\u2062","msub":{"mi":["\u03b3","i"]}}}}],"mo":"="},{"mrow":[{"mrow":{"mi":["subject","to"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mfrac":{"mn":"1","mi":"s"},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"s"},"mo":"\u2062","mrow":{"msub":[{"mi":["v","i"]},{"mi":["d","i"]}],"mo":"\u2062"}}},"mo":"\u2264","mi":"D"},{"mn":"0","mo":["\u2264","\u2264"],"msub":{"mi":["\u03b3","i"]},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"P"},"mo":"\u2062","mrow":{"msubsup":{"mi":["H","j"],"mrow":{"mo":["(",")"],"mi":"i"}},"mo":"\u2062","mrow":{"mo":"\u2200","mi":"i"}}}}],"mo":","},{"mrow":[{"mrow":[{"msub":{"mi":"D","mrow":{"mi":["Q","i"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"P"}},{"msubsup":{"mi":["\u03c3","i"],"mn":"2"},"mo":"\u2062","mrow":{"mo":"\u2200","mi":"i"}}],"mo":["\u2264","\u2264"],"msub":{"mi":["d","i"]}},{"msub":{"mi":["\u03b3","i"]},"mo":"\u2265","mrow":{"mrow":[{"msub":[{"mi":"a","mrow":{"mi":["i","p"],"mo":","}},{"mi":["d","i"]}],"mo":"\u2062"},{"msub":{"mi":"b","mrow":{"mi":["i","p"],"mo":","}},"mo":"\u2062","mrow":{"mo":"\u2200","mi":"i"}}],"mo":"+"}},{"mi":"p","mo":"."}],"mo":[",",","]}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mtext":{}},{"mtext":{}}]}},{"mrow":{"mo":["(",")"],"mn":"16"}}]}}}}},"The MWTD and MMDP problems defined before can be modified to include the case of a given entropycoded embedded quantization scheme, by using the above formulas directly into the optimization problems. For example, the MMDP problem can be written as follows:",{"@attributes":{"id":"p-0087","num":"0086"},"maths":{"@attributes":{"id":"MATH-US-00022","num":"00022"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":["Minimize","\u03b1"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},{"mrow":[{"mrow":{"mi":["subject","to"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"l","mo":"=","mn":"1"},"mi":"L"},"mo":"\u2062","mfrac":{"mrow":{"msub":{"mi":["R","l"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"+","msub":{"mi":["\u03b8","l"]}}}},"msub":{"mi":["C","l"]}}}},"mo":"\u2264","mn":"1"},{"msub":{"mi":["R","l"]},"mo":"\u2265","mrow":{"mn":"0","mo":"\u2062","mrow":{"mo":"\u2200","mi":"l"}}},{"mrow":[{"mfrac":{"mn":"1","mi":"s"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"s"},"mo":"\u2062","msub":{"mi":"\u03b3","mrow":{"mi":["i","l"],"mo":","}}}},{"mi":"b","mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"l"},"mo":"\u2062","mrow":{"msub":{"mi":["R","j"]},"mo":".","mrow":{"mo":"\u2200","mi":"l"}}}}],"mo":"\u2264"},{"msubsup":{"mi":["\u03c3","i"],"mn":"2"},"mo":["\u2265","\u2265","\u2062","\u2062","\u2265","\u2265"],"msub":[{"mi":"D","mrow":{"mi":"i","mo":",","mn":"1"}},{"mi":"D","mrow":{"mi":["i","L"],"mo":","}}],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"\u2026","mrow":{"msub":{"mi":"D","mrow":{"mi":["Q","i"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"P"}}},{"mo":"\u2200","mi":"i"},{"msub":{"mi":"\u03b3","mrow":{"mi":["i","l"],"mo":","}},"mo":"\u2265","mrow":{"mrow":{"msub":[{"mi":"a","mrow":{"mi":["i","p"],"mo":","}},{"mi":"D","mrow":{"mi":["i","l"],"mo":","}}],"mo":"\u2062"},"mo":"+","msub":{"mi":"b","mrow":{"mi":["i","p"],"mo":","}}}},{"mo":"\u2200","mrow":{"mi":["i","l"],"mo":"."}},{"mrow":[{"mi":"p","mo":[".","\u2062"],"mstyle":{"mtext":{}},"mn":"0"},{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"p"},"mo":"\u2062","mrow":{"msubsup":{"mi":["H","j"],"mrow":{"mo":["(",")"],"mi":"i"}},"mo":[".","\u2062"],"mstyle":{"mtext":{}},"mrow":{"mo":"\u2200","mrow":{"mrow":{"mrow":[{"mi":["i","l"],"mo":[".","."],"mfrac":{"mn":"1","mi":"s"}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"s"},"mo":"\u2062","mrow":{"msub":[{"mi":["v","i"]},{"mi":"D","mrow":{"mi":["i","l"],"mo":","}}],"mo":"\u2062"}}],"mo":"\u2062"},"mo":"\u2264","msubsup":{"mi":["aD","l","opt"]}}}}}],"mo":["\u2264","\u2264"],"msub":{"mi":"\u03b3","mrow":{"mi":["i","l"],"mo":","}}},{"mo":"\u2200","mrow":{"mi":"l","mo":"."}}],"mo":[",",",","\u2062",",",",",",","\u2062",",",",",","],"mstyle":[{"mtext":{}},{"mtext":{}}]}],"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}}}},{"mrow":{"mo":["(",")"],"mn":"17"}}]}}}}},"This problem formulation applies to any family of successive refinement source codes that can operate at a finite set of possible R-D points.","In general, the coding rate overhead at each bit-plane may be allocated such that each stage of the multi-stage BP decoder works with enough reliability in order to: 1) provide reliable hard decisions () in order to avoid error propagation in the multi-stage decoder; 2) achieve output distortion close enough to the designed quantization distortion. For each p-th bit plane of the i-th source component, the overhead \u03b8can be optimized numerically, for the given family of codes. The values \u03b8) depend, in general, on the bit-plane entropy H, on the source statistics and on the l-th user channel capacity, as well as on the coding block length k. Fortunately, for the family of Raptor codes used herein, we have observed that the dependence of the factors \u03b8on the source statistics and on the channel capacity is very weak. In fact, for a given block length k, we can choose a set of values {\u03b8: p=1, . . . , P} that is uniformly good for all sources and channel capacities.","Once the coding overhead factors {\u03b8: p=1, . . . , P} have been found (this can be done off-line and is not part of the encoding algorithm), system optimization may proceed. For a given bit-plane to layers allocation defined by the integers {\u03c0} (see ), the number of coded bpp is given by:",{"@attributes":{"id":"p-0091","num":"0090"},"maths":{"@attributes":{"id":"MATH-US-00023","num":"00023"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"b","mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"s"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"l","mo":"=","mn":"1"},"mi":"L"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"p","mo":"=","mrow":{"msub":{"mi":"\u03c0","mrow":{"mi":"i","mo":",","mrow":{"mi":"l","mo":"-","mn":"1"}}},"mo":"+","mn":"1"}},"msub":{"mi":"\u03c0","mrow":{"mi":["i","l"],"mo":","}}},"mo":"\u2062","mrow":{"mfrac":{"mrow":{"msubsup":{"mi":["H","p"],"mrow":{"mo":["(",")"],"mi":"i"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"+","msub":{"mi":["\u03b8","p"]}}}},"msub":{"mi":["C","l"]}},"mo":"."}}}}}},{"mrow":{"mo":["(",")"],"mn":"18"}}]}}}}},"This is the same bandwidth efficiency corresponding to ideal channel coding (zero overhead), and a modified quantizer operational R-D function characterized by the set of R-D points:",{"@attributes":{"id":"p-0093","num":"0092"},"maths":{"@attributes":{"id":"MATH-US-00024","num":"00024"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"p"},"mo":"\u2062","mrow":{"msubsup":{"mi":["H","j"],"mrow":{"mo":["(",")"],"mi":"i"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"+","msub":{"mi":["\u03b8","j"]}}}}},{"msub":{"mi":"D","mrow":{"mi":["Q","i"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"p"}}],"mo":","}},{"mi":"p","mo":"=","mn":"1"},{"mi":"P","mo":"."}],"mo":[",",",","\u2062",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}},{"mrow":{"mo":["(",")"],"mn":"19"}}]}}}}},"It follows that the corresponding system optimization problems are formally identical to what given before, in the case of ideal channel coding, e.g., letting \u03b8=0 in (17), with the piecewise linear R-D functions (15) defined for the modified R-D points.","As an example of the application of the system optimization framework developed above, we consider the case of lossy multicasting of digital (gray-scale) images over the BEBC. Two embodiments are designed according to the MMDP criterion. The former is based on the concatenation of a state-of-the art image coder, JPEG2000, with standard non-systematic Raptor codes. The latter is based on scalar quantization with linear index coding, as presented above.","JPEG2000 and Raptor codes\u2014this case can be cast within the problem formulation set forth above, for a single source component (s=1) and where JPEG2000 plays the role of the successive refinement source code. JPEG2000 produces a bit-stream of source-encoded bits with the property that the encoder output for any given bit rate is a prefix of the output for any higher bit rate. For a given source image we can generate a set of R-D points by considering an increasing number of JPEG2000 output bits, sequentially. We considered the points (r, d) on the successive refinement R-D curve generated by applying JPEG2000 to the test image that we wish to encode. In particular, we considered the rate points {r: p=1, . . . , P} uniformly spaced in the interval [0.001; 1.2]. The lower convex envelope of the straight lines interpolating these points defines the operational R-D function R(D) of the source encoder.","For the MMDP problem, since the \u201ctrue\u201d R-D function of a given test image is not defined, we consider the operational single-user bounds D=R(bC). The solution of the system optimization problem yields a set of user distortions {D: l=1, . . . , L}. Correspondingly, the JPEG2000 encoder produces L successive refinement layers of size m=k (R(D)\u2212R(D)), where R(D=0). Eventually, each l-th layer output by the JPEG2000 encoder is mapped into a channel codeword of length n=m(1+\u03b8)\/Cby the Raptor encoder, and the L codewords are transmitted in sequence over the BEBC. The degree sequence and the LDPC precoder for the Raptor code used in this example are set forth in Appendix A. Raptor codes over the binary-erasure channel have an overhead \u03b8that depends on the coding block length but it is independent of the channel erasure probability. For the Raptor code ensemble used here and block lengths of the example, the overhead is \u03b8=0.065.","JPEG2000 produces a header containing very sensitive information, the loss of which prevents from reconstructing the source at any level of refinement. In this example, the header is formed by 255 bytes. The header information was separately encoded using channel coding rate 0.1 (much lower than the worst user capacity Cfor which the technique is optimized), so that the header is decoded by all users with negligible probability of error. The encoded header accounts for less than 5% of the overall bandwidth efficiency.","Quantization with linear index coding\u2014in this case, the source image is decomposed into a set of \u201cparallel\u201d source components by a suitable linear transform. In this example, the DWT of JPEG2000 set forth above was used. Using W levels of DWT, the transformed image is partitioned into 3 W+1 \u201csubbands\u201d.  is a block diagram showing DWT subband decomposition and corresponding source component blocks considered in the numerical example. The shaded blocks are actually encoded (non-zero rate) while the others are discarded, as a result of the MMDP system optimization for L=3, user capacities, C=0.3645, C=0.6, C=0.9 and b=0.5. The subband decomposition example shown in  is for W=3. This produces 3 W+1=10 subbands, which in the figure are indicated by LL, HL, LH, HH, HL, LH, HH, HL, LH, HH, respectively. The subbands have different lengths, all multiples of the LL subband length. For simplicity, we partition the whole DWT coefficient array into source component blocks of the same size of the LL subband. This yields s=2source component blocks of equal length k=N\/s, where N\u00d7N indicates the size of the original image. The MSE distortion is given by a weighted sum of subband distortions, where the weights correspond to the l\u2212norm of the synthesis filters impulse responses. For W=3, the weight of a source component block in subband w={1, . . . , 10} is given by the w-th coefficient of the vector [l, lh, lh, lh, lh, lh, lh, lh, lh, h], where, for the particular wavelet we have considered here (e.g., the CDF 9\/7 wavelet used by JPEG2000 for lossy compression), l=1.96 and h=2.08.","Similarly to the case of parallel Gaussian sources, each source component block may be independently quantized using a suitably optimized non-uniform embedded scalar quantizer. Then, the quantized bit-planes are linearly encoded using a systematic Raptor encoder (again, the Raptor degree sequence and LDPC precoder for this example are given in Appendix A). The quantizer R-D points are obtained by measuring the actual quantizer distortion on each source component block. In this example, for a test image of size 1024\u00d71024 with DWT depth W=3 we have source component blocks of length k=16384, which coincides with the size of the LL subband.","The multi-stage BP decoder needs the knowledge of the conditional bit-plane pmfs for each source component. While for the parallel Gaussian source model these probabilities are known a priori, in the case of digital images this must be estimated from the sequence of quantized DWT coefficients. In conventional source coding schemes (e.g., JPEG2000) these probabilities are implicitly estimated by using sequential probability estimators implemented together with the bit-plane context model and arithmetic coding. With linear index coding, these probabilities may be estimated by ML probability estimators and encoded into a header. The corresponding a-priori LLRs defined above are encoded and suitably quantized by using a number of bits dictated by the Rissanen bound, as discussed above. The bit-plane a-priori probabilities and the source components empirical variances and means form the header information for the linear index coding scheme. In this example, the header for linear index coding is formed by 255 bytes, as for the corresponding JPEG2000 case, and it is separately encoded with channel coding rate 0.1. Thus, the header information overhead is identical for both schemes.","A test system was used with L=3 users with capacities C=0.3645, C=0.6 and C=0.9 and a target bandwidth efficiency of b=0.5 channel-coded bits per pixel. The standard test image Lena was used with a size of 1024\u00d71024, with grey-scale 8-bit pixels. For the linear index coding scheme, 3 levels of DWT are used. The solution of the MMDP problem yields that only the shaded source blocks in  are effectively encoded, i.e., are allocated non-zero rate for at least the fundamental layer. All the other source blocks are reconstructed at their mean values.",{"@attributes":{"id":"p-0103","num":"0102"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0104","num":"0103"},"figref":["FIG. 8","FIG. 7","FIG. 8"],"sub":["1","2","3"]},"Robustness to mismatched channel capacity\u2014as anticipated, the main advantage of linear index coding consists of its robustness to residual post-decoding erasures. In order to illustrate this aspect, we consider the case of a mismatched user with channel capacity less than the minimum channel capacity C for which the system is designed. We simulated the performances of the systems designed for the nominal capacities C, C, Cgiven before, over a BEC with capacity C varying in an interval [C, C], with C<C. For C \u03b5[C, C], the system transmits at rate above the channel capacity. Therefore, even the fundamental layer (layer ) operates \u201cabove threshold\u201d, and it is affected by a very large number of post-decoding erasures.",{"@attributes":{"id":"p-0106","num":"0105"},"figref":"FIG. 8","sub":["1","1 ","1 "]},"For the above experiments the \u201ckakadu\u201d open source JPEG2000 image coder was used. The error resilience tools provided by the JPEG2000 standard (i.e., the SOP, SEGMARK, ERTERM and RESTART strategies) were enabled. In particular, with RESTART, the context statistics was restarted at the beginning of each coding pass. With ERTERM, the encoder enforces a predictable termination policy for the entropy coder. With SEGMARK, a special symbol was encoded at the end of each bit-plane. With SOP start of packet markers were inserted in front of every packet. At the decoder side the kakadu \u201cresilient\u201d feature was enabled, in order to obtain the best possible result. The codeblock size used was the standard in kakadu, i.e., 32\u00d732.","It should be understood that many variations are possible based on the disclosure herein. Although features and elements are described above in particular combinations, each feature or element may be used alone without the other features and elements or in various combinations with or without other features and elements. The methods or flow charts provided herein may be implemented in a computer program, software, or firmware incorporated in a non-transitory computer-readable storage medium for execution by a general purpose computer or a processor. Examples of computer-readable storage mediums include a read only memory (ROM), a random access memory (RAM), a register, cache memory, semiconductor memory devices, magnetic media such as internal hard disks and removable disks, magneto-optical media, and optical media such as CD-ROM disks, and digital versatile disks (DVDs).","Suitable processors include, by way of example, a general purpose processor, a special purpose processor, a conventional processor, a digital signal processor (DSP), a plurality of microprocessors, one or more microprocessors in association with a DSP core, a controller, a microcontroller, Application Specific Integrated Circuits (ASICs), Field Programmable Gate Arrays (FPGAs) circuits, any other type of integrated circuit (IC), and\/or a state machine. Such processors may be manufactured by configuring a manufacturing process using the results of processed hardware description language (HDL) instructions and other intermediary data including netlists (such instructions capable of being stored on non-transitory computer readable media). The results of such processing may be maskworks that are then used in a semiconductor manufacturing process to manufacture a processor which implements aspects of the present invention."],"GOVINT":[{},{}],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE FIGURES","p":[{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 2","i":"a "},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 2","i":"b "},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 4","sub":"i,l"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 5","sub":"i"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 9"}]},"DETDESC":[{},{}]}
