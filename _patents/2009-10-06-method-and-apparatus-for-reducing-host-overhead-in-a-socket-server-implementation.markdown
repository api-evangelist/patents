---
title: Method and apparatus for reducing host overhead in a socket server implementation
abstract: A network application executing on a host system provides a list of application buffers in host memory stored in a queue to a network services processor coupled to the host system. The application buffers are used for storing data transferred on a socket established between the network application and a remote network application executing in a remote host system. Using the application buffers, data received by the network services processor over the network is transferred between the network services processor and the application buffers. After the transfer, a completion notification is written to one of the two control queues in the host system. The completion notification includes the size of the data transferred and an identifier associated with the socket. The identifier identifies a thread associated with the transferred data and the location of the data in the host system.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07930349&OS=07930349&RS=07930349
owner: Cavium Networks, Inc.
number: 07930349
owner_city: Mountain View
owner_country: US
publication_date: 20091006
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["RELATED APPLICATIONS","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION"],"p":["This application is a continuation of U.S. application Ser. No. 11\/225,373, filed Sep. 12, 2005, now U.S. Pat. No. 7,613,813 which claims the benefit of U.S. Provisional Application No. 60\/669,742, filed on Apr. 8, 2005 and U.S. Provisional Application No. 60\/609,211, filed on Sep. 10, 2004. The entire teachings of the above applications are incorporated herein by reference.","The Open Systems Interconnection (OSI) Reference Model defines seven network protocol layers (L1-L7) used to communicate over a transmission medium. The upper layers (L4-L7) represent end-to-end communications and the lower layers (L1-L3) represent local communications.","Networking application aware systems need to process, filter and switch a range of L3 to L7 network protocol layers, for example, L7 network protocol layers such as, HyperText Transfer Protocol (HTTP) and Simple Mail Transfer Protocol (SMTP), and L4 network protocol layers such as Transmission Control Protocol (TCP) and User Datagram Protocol (UDP). In addition to processing the network protocol layers, the networking application aware systems need to simultaneously secure these protocols with access and content based security through L4-L7 network protocol layers including Firewall, Virtual Private Network (VPN), Secure Sockets Layer (SSL), Intrusion Detection System (IDS), Internet Protocol Security (IPSec), Anti-Virus (AV) and Anti-Spam functionality at wire-speed.","A socket interface such as the Berkley Socket (BSD) socket interface is a standard Application Programming Interface (API) that includes a set of functions that can be called by networking applications such as web browsers to provide networking services. The application calls an API function which makes a system call to the operating system. The socket interface provides the service requested by the system call by calling low-level functions in the operating system that handle the networking protocol. The underlying protocols and network mechanisms are hidden from the application.","The BSD socket interface is typically used to interface to the TCP\/IP protocol stack. TCP\/IP is based on a client\/server model in which client and server processes identified by network endpoints (IP address and port number) are represented as sockets. A socket is set up between a client process and a server process for a particular domain and network protocol for example, UDP or TCP. The server process is identified by binding an IP address and port number to the socket. After the server process is bound to the socket, the socket can be used to listen for new connection requests and to service them. All requests through the socket interface are directed through an TCP\/IP network stack in the operating system (kernel) and involve multiple copies of packet data. Packet data for each connection is copied between a device driver buffer in the kernel and application (user) space, that is, data is first copied to the network stack in kernel memory and must then be copied to buffers in application (user) space before it can be used by the application.","The transfer of packet data for the connection can involve transferring the data over a bus such as the PCI bus. Typically, the transfer is performed by a Direct Memory Access (DMA) engine in a PCI interface. In a host-based descriptor ring DMA engine, descriptors are fetched from host PCI shared memory and data to be transferred over the PCI bus is transferred to buffers identified by pointers stored in the descriptors.","There are different mechanisms to communicate from the host to the DMA engine about the availability of buffers and size of buffers. Also, DMA engines have different ways to communicate to the host about data transfer completion (one such mechanism can be coalesced interrupt). In such a mechanism, there is only one program on the host side that can manage buffers in the descriptor ring and all buffers are treated the same, that is, data is stored in the next available buffer in the ring irrespective of the connection they belong to. Generally, a driver in the kernel manages such descriptor rings. Most generic network interface cards (NICs) support this type of a DMA engine.","In a memory to memory data mover DMA engine, the DMA engine can transfer data across the PCI bus within shared PCI memory space. The transmit side of the PCI bus provides data buffer pointers from where data should be gathered and a list of pointers (typically provided by the receive side) where it must be written on the other side of the PCI bus.","Typically, there are thousands to a million TCP connections established concurrently in a high end server. If a standard DMA engine is used to move the data, applications running on the host that are using the TCP connection sleep waiting for data on their corresponding TCP connection. There can be thousands of such application program threads. When data arrives, the kernel running on the host is interrupted, the kernel finds which sleeping process the received data belongs to by walking a long list of sleeping processes and then the kernel wakes up the process after copying the data to application space in host memory. Finding the appropriate process and copying data is very expensive.","As network transfer speeds increase above 1 Giga bits per second (1 Gb\/s) and more data is transferred over the network, the CPU bandwidth required to process the multiple copies of the data increases. The overhead for copying reduces the available memory bus bandwidth and adds latency to when the data is available for use by the application.","Remote Direct Memory Access (RDMA) eliminates the need for copying received user data between the network protocol stack in kernel space and application buffers in application space in host memory. RDMA avoids this copy by allowing the sender to identify data to be placed directly into application buffers (buffers in user space) and transmitting the identification with the data so that the receiver of the data can place the data directly into the application buffers when it is received.","However, RDMA requires the addition of RDMA support in the network interface card (NIC), and also requires modifications to the application in order to use RDMA.","The present approach provides a network services processor that includes direct DMA to\/from a host system application user space which improves host performance in a non-RDMA capable host system and allows the ability to handle a large number of concurrent TCP connections and high data throughput.","A network processor includes local memory and an interface to a host system. The local memory is shared by a plurality of processors for storing data transferred over a network coupled to the network processor. The interface to the host system manages a queue for a socket established between an application executing in host memory and another application executing in a remote host coupled to the network. The queue stores a unique identifier for a process identifier associated with the socket. The interface transfers data asynchronously between host memory and local memory and an indication is received by a waiting process in the host system that data has been written to host memory based on a process identifier stored in the queue.","The socket may be a Transmission Control Protocol socket or a User Datagram Protocol socket. Furthermore, processing bandwidth is reduced by the need to copy the data from kernel space to application space in host memory. In order to reduce the overhead, the present invention provides a mechanism for direct data placement through the use of a virtual queue. Data transferred over the network may be transferred directly between local memory and application space in host memory in the host system or may be indirectly transferred between local memory and application memory in host memory in the host system via kernel space in the host memory.","The queue may be implemented as a ring buffer and address pointers for managing the queue are stored in local memory.","A network processor includes a plurality of processors and a local memory shared by the plurality of processors. A first processor or group of processors execute an application and a second processor executes an operating system. The local memory stores data transferred over a network coupled to the network processor. The second processor manages a queue for a socket established between the application and another application executing in a remote host coupled to the network. The queue stores a unique identifier for the socket and the data is transferred directly between the application buffers and network buffers in the local memory.","A networking system includes a host system executing a network application. The host system includes a network services processor, a network interface and a socket switch. The socket switch directs network service calls from the application to the network services processor or the network interface card based on an Internet Protocol address included in a network service call.","A description of preferred embodiments of the invention follows.",{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 1","b":["100","100","108","100","100","102","104","106","108"],"i":["a","b ","a","b "]},"As is well-known to those skilled in the art, the Open System Interconnection (OSI) reference model defines seven network protocol layers (L1-L7). The physical layer (L1) represents the actual interface, electrical and physical that connects a device to a transmission medium. The data link layer (L2) performs data framing. The network layer (L3) formats the data into packets. The transport layer (L4) handles end to end transport. The session layer (L5) manages communications between devices, for example, whether communication is half-duplex or full-duplex. The presentation layer (L6) manages data formatting and presentation, for example, syntax, control codes, special graphics and character sets. The application layer (L7) permits communication between users, for example, file transfer and electronic mail.","TCP is a connection-oriented protocol defined in Request For Comments (RFC) 793 incorporated herein by reference in its entirety that provides reliable and in-order byte stream transfer to upper network protocol layers. TCP is based on a client\/server model. The host sets up a TCP connection between a client process (application) and a server process (application) to transfer packets using the TCP protocol.","A TCP connection consists of two flows. Each flow is unidirectional and is defined as the 5-tuple made by: (1) IP Source Address, (2) IP Destination Address, (3) IP Protocol, (4) TCP Source Port, and (5) TCP Destination Port stored in the TCP header and the IP header. Packets having exactly the same 5-tuple belong to the same flow. A TCP connection between two end points A and B is identified by a pair of flows in opposite directions. The five tuple that identifies these flows only differs in the order in which the source and destination addresses are listed.","Client and server processes (applications) identified by network endpoints (IP address and port number) are represented as sockets. A socket is set up for a particular domain and network protocol for example, UDP or TCP. The server process is identified by binding an IP address and port number to the socket. After the server process is bound to the socket, the socket can be used to listen for new connection requests and to service them.","The network stack  is in the kernel, that is, the core of an operating system. The socket  provides an Application Program Interface (API) between the application  and the network stack  in the kernel. An application program (programming) interface (API) is a set of routines used by an application program  to direct the performance of procedures by the operating system. In addition, APIs are also provided by the socket  to enhance the application performance.","Data transferred over the network  is typically stored in user space (application space) in host memory accessible by the application  and in network buffers accessible by the network stack . Data copying between the network buffers in kernel space in host memory and user application memory is responsible for a substantial amount of system overhead in a network stack. This situation has motivated development of techniques to eliminate copying between the networking stack and the application. In order to provide better performance, offload socket APIs provide a copy avoidance mechanism also referred to as a zero-copy mechanism by sending and receiving data to\/from the network directly to user space to avoid copying it between network buffers and user space in host memory.",{"@attributes":{"id":"p-0043","num":"0042"},"figref":["FIG. 2","FIG. 1"],"b":["206","100","206"],"i":"a "},"The host system includes a processor , memory  and an Input\/Output (I\/O) interface  through which data is transferred over an I\/O bus . The application , socket module  and network stack  are stored in memory  and executed by the processor . The host system is coupled to the network services processor  through the Input\/Output bus . In one embodiment, the Input\/Output interface  is the Peripheral Connect Interface (PCI), a standard bus interface developed by the PCI Special Interest Group (SIG). The network services processor  includes hardware packet processing, buffering, work scheduling, ordering, synchronization, and cache coherence support to accelerate packet processing tasks.","The network services processor  performs work (packet processing operations) for upper level network protocols, for example, OSI network L4-L7 layer protocols encapsulated in received packets. The packet processing (work) to be performed on a particular packet includes a plurality of packet processing operations (pieces of work). The network services processor  allows processing of upper level network protocols in received packets to be performed to forward packets at wire-speed. Wire-speed is the rate of data transfer of the network over which data is transmitted and received. By processing the protocols to forward the packets at wire-speed, the network services processor does not slow down the network data transfer rate. The network protocol processing can include processing of network security protocols such as Firewall, Application Firewall, Virtual Private Network (VPN) including IP Security (IPSec) and\/or Secure Sockets Layer (SSL), Intrusion detection System (IDS) and Anti-virus (AV).",{"@attributes":{"id":"p-0046","num":"0045"},"figref":["FIG. 3","FIG. 3","FIG. 1"],"b":["300","106","102","108","300","300","302","304","306","308","310","314","316","318","320","322"]},"TCP maintains a checksum of the entire segment and any segment with an invalid checksum is discarded, that is, not acknowledged. One segment fits into a single non-fragmented IP packet. As IP datagrams can be lost, when a segment is sent, a timer is maintained and if an acknowledgement of reception (ACK) is not received, the segment is transmitted again after timeout. IP datagrams can arrive out of order and so can TCP segments but must be passed to the upper layer in order. IP datagrams can be duplicated but a receiver of a TCP segment must discard duplicate segments. TCP also provides flow control, that is, each end of the connection has a finite buffer space and a sender is not allowed to send more than the data that can be stored in the buffer space.","The Acknowledge Number  is valid only when the ACK flag in the flags field  is set. It is the next sequence number that is expected by the sender of the ACK. Acknowledgements are sent with each data transmission on an established connection. TCP supports only positive acknowledgement up to the last continuous data stream received. No negative acknowledgement is supported.","The TCP flags field  includes one or more flags that can be set at the same time. The flags include an URG flag indicating whether the urgent pointer is valid; an ACK flag indicating whether the acknowledgement number in the TCP header is valid; and a PSH flag indicating whether data should be passed to application as soon as possible instead of waiting for more data to come. The PSH flag is typically set by the stack when sender's buffer is empty. A RST flag indicates whether there is a reset connection; a SYN flag indicates whether to synchronize sequence number for connection start; and a FIN flag indicates whether the sender is closing the connection","The Window Size field  is used for flow control. The Window Size is a 16-bit field storing the number of bytes that receiver can accept (starting from ACK number). The window size is limited to 65535 bytes and can be scaled.","The TCP Checksum field  covers the entire segment plus a 12-byte pseudo-header that includes IP source and destination header, protocol, and TCP segment length. The TCP checksum is calculated by the sender and verified by the receiver. The TCP checksum is calculated as ones complement sum of 16-bit words. If the calculated length is not multiple of 16-bit words, a zero byte is padded for calculation only.","The Urgent Pointer field  is valid only if URG flag is set in the flags field . It is a way to transmit emergency data to the other end. The urgent pointer  points to the last byte of urgent data. Urgent data can be sent even if available window size is zero.","A TCP connection is established by the exchange of three TCP segments and is typically referred to as a three-way handshake: The first TCP segment is referred to as the first SYN. The side that sends the first SYN (client) that is, a TCP segment in which the SYN flag is set, and valid source and destination ports and an initial sequence number (ISN) are specified, is said to perform an active open. The second TCP segment is referred to as the SYN\/ACK segment. The side that receives the first SYN and sends back the next SYN (server), i.e., ACK-ing the received SYN, thus transmitting the SYN\/ACK packet, performs a passive open. The TCP segment includes valid source and destination ports, an initial sequence number (ISN), an acknowledge number (client's ISN plus one) and the SYNC and ACK bits are set. The third TCP segment is referred to as the ACK segment in which the client ACKs the server's SYN\/ACK. The TCP segment includes the acknowledge number (server's ISN plus one) and the ACK flag is set.",{"@attributes":{"id":"p-0054","num":"0053"},"figref":["FIG. 4","FIG. 2"],"b":["206","400","400","404","404"],"i":["a","b. "]},"A security appliance  is a standalone system that can switch packets received at one Ethernet port (Gig E) to another Ethernet port (Gig E) and perform a plurality of security functions on received packets prior to forwarding the packets. For example, the security appliance  can be used to perform security processing on packets received on a Wide Area Network prior to forwarding the processed packets to a Local Area Network.","The network services processor  receives packets from the Ethernet ports (Gig E) through the physical interfaces PHY , , performs L2-L7 network protocol processing on the received packets and forwards processed packets through the physical interfaces , to another hop in the network or the final destination or through the PCI bus  for further processing by the host system (). Packets are transferred between the host system and the network services processor  according to the principles of the present invention.","A DRAM controller in the network services processor  controls access to an external Dynamic Random Access Memory (DRAM)  that is coupled to the network services processor . The DRAM  stores data packets received from the PHYs interfaces , or the Peripheral Component Interconnect Extended (PCI-X) interface  for processing by the network services processor . In one embodiment, the DRAM interface supports 64 or 128 bit Double Data Rate II Synchronous Dynamic Random Access Memory (DDR II SDRAM) operating up to 800 MHz.","A boot bus  provides the necessary boot code which is stored in flash memory  and is executed by the network services processor  when the network services processor  is powered-on or reset. Application code can also be loaded into the network services processor  over the boot bus , from a device  implementing the Compact Flash standard, or from another high-volume device, which can be a disk, attached via the PCI bus.","Packets can also be received over the PCI bus  and data received in packets over the RGMII connections interface can be forwarded to a host system coupled to PCI bus .","The miscellaneous I\/O interface  offers auxiliary interfaces such as General Purpose Input\/Output (GPIO), Flash, IEEE 802 two-wire Management Interface (MDIO), Universal Asynchronous Receiver-Transmitters (UARTs) and serial interfaces.","The network services processor  includes another memory controller for controlling Low latency DRAM . The low latency DRAM  is used for Internet Services and Security applications allowing fast lookups, including the string-matching that may be required for Intrusion Detection System (IDS) or Anti Virus (AV) applications.",{"@attributes":{"id":"p-0062","num":"0061"},"figref":"FIG. 5","b":["100","206","102"],"i":"a "},"An application  executes in application (user) space in memory in the host system . The application  issues API calls using API functions provided by the Direct Socket Library module  or the enhanced socket system calls module . Requests through the socket interface ,  are either directed through the system TCP\/IP (host) network stack  to a NIC driver  or through a host driver  to the network services processor .","The application  makes socket API calls to the socket modules ,  to establish a TCP connection between a client process and a server process. A server process sets up a communication endpoint and passively waits for a connection from the client process. A client process initiates communication with a server process that is passively waiting to be contacted.","Data transfer for an application  for the TCP connection can be directed through the host networking stack  or the processor networking stack  in the network services processor  dependent on the API call issued by the application . Host socket APIs are a superset of legacy BSD socket APIs. The host socket APIs call functions in the enhanced socket system calls module . These API calls include flags used by the operating system to select one of the stacks, that is, the networking stack  in the network services processor  or the host networking stack  in kernel space in the host system . The socket modules ,  provide a uniform interface to the network and communications protocols. Protocol independent requests are mapped to the protocol specific implementation selected when the socket is created. The socket APIs provide a BSD like socket interface which enables applications written for the BSD socket interface to direct requests to the network services processor  with minimal or no changes.","API calls that avoid use of the host networking stack (network protocol stack)  in the host system are provided by the direct socket calls module  and the application  can be modified to use these API calls instead of the BSD socket calls that use the host networking stack  in the host system ","The host driver  and the network services processor  communicate using Direct Data Output Queues (DDOQ) and control queues (CNTQ). The TCP stack running on a core (processor) in the network services processor  sends packets\/messages to the Host via the DDOQ. Data is moved asynchronously between local memory in the network services processor  and remote (PCI) memory in the host in both directions using Direct Memory Access (DMA).","In more detail, two counters and associated interrupts are provided to support two queues located within the host memory. The queues are referred to as CNTQ and CNTQ. Each CNTQ is managed by software running on the cores and has a respective ring buffer stored within host memory along with associated head and tail pointers, and available-size values. Registers can be used to set packet count threshold and timeout values for flexible host support. The host driver  configures the base pointer and size for each of the CNTQs at boot time. Software running on the cores sets the current header and tail pointers to the base address of the ring. The available-size value is initialized to the size of the ring buffer.","Information about the connection data transferred is written by software running on the cores along with connection identifier into the memory location in PCI host processor memory indicated by the base pointer plus head pointer. Software running on the cores also manages \u201cwrap-around\u201d of the head pointer when it reaches the size of the ring buffer. Host software guarantees that the available size never goes below zero by crediting space in CNTQ back to core software. The ring is strictly in order, meaning that host software does not send the \u201ccntq_credit\u201d instruction (or in other words increases the available size) until the oldest packet has been removed. When the available size value reaches zero, software running on the cores stops writing packets into the ring.","Core software inserts information about the connection data transferred at the head of the ring buffer in host memory. The host software extracts this information from the tail of the ring and reports to core software the amount of space that it freed up.",{"@attributes":{"id":"p-0071","num":"0070"},"figref":"FIGS. 6A-D"},"Referring to , an input ring  in the host system stores buffer\/information pointers ,  to memory ,  in which packet data and information is stored. The network services processor  reads the buffer\/information pointers from the host system , writes packet and information to the host system and updates the local packet\/byte count in a packet\/byte count register . The network services processor  stores a tail pointer  to the input ring  in the host system and the host system stores a head pointer  to the input ring . The host system rings a doorbell  in the network services processor  to indicate buffer availability in the host system . The host system can read the count register  to determine how many packets have arrived. Through the use of the packet\/byte count register , the network services processor  can generate an interrupt to the host system on a count threshold or when the count is non-zero for a period of time.",{"@attributes":{"id":"p-0073","num":"0072"},"figref":"FIG. 6B","b":["600","600","602","604","606","619","602","102"]},"Each entry (descriptor)  in the descriptor list is an information\/buffer pointer pair. The information pointer  stores a pointer to information  and the buffer pointer stores the location of the buffer in host memory corresponding to the entry . The DDOQs are stored in host memory and base pointers and head\/tail pointers for the DDOQs are stored in memory in both the host processor and the network services processor . Control queues are also stored in host memory and the base and head\/tail pointers for the queues are stored in memory in both the host and the network services processor.","The core driver  maintains a descriptor ring in \u201clocal\u201d memory directly accessible by the network services processor for each DDOQ. A unique 32-bit value is used to identify each DDOQ . Normally one DDOQ queue is used for each \u201cdata flow\u201d. A \u201cdata flow\u201d is defined as a connection between two communicating end-points. A simple example of \u201cdata flow\u201d is a TCP socket connection. The host driver  issues an instruction to initialize a DDOQ . Upon receiving the instruction the core driver  allocates a buffer in local memory for storing base and head\/tail pointers for the DDOQ  and maintains a descriptor list provided by the host driver .","DDOQs  are helpful in dividing the traffic among several data flows for example TCP sessions. A DDOQ is created by the network services processor  based on a request from the host system that includes the size of a descriptor ring and a list of descriptors. When a packet is moved into the host memory (at the buffer address identified by the information\/buffer pointer pair), the information about the packet is stored in the host memory pointed to by the information pointer . If a packet is to span more than one descriptor , then the network services processor  writes the packets in the multiple descriptors. The host processors determines the numbers of descriptors by looking at the packet_length and buffer_size stored in the information pointer .","The list of descriptors is used to move packets to the host memory using Direct Memory Access (DMA). Instead of copying the next descriptor from host memory, the network service processor  creates a descriptor ring in local memory and fills it with the descriptors provided by the host . The network services processor  manages head\/tail pointers internally and stops sending packets to the host when all descriptors have been used. Under normal operation, the host credits descriptors after receiving one or more packets from the network services processor .","The information pointer  in the descriptor  points to an information structure . Each descriptor  may have a different buffer size indicated by the buffer size stored in the buffer size field  in the information structure . The buffer size field  stores the size of the contiguous host memory block pointed to by the buffer pointer . The host driver , at DDOQ and descriptor creation time, initializes the buffer size field . The packet size field  stores the size of the packet copied by the network services processor in this descriptor. The packet size field  is updated by the network services processor when it copies the packet to the host memory pointed to by the descriptor .","All instruction responses and packets sent to the host from the network services processor  include a response header . The response header includes an opcode  that identifies the packet type and error code. The response header also includes a SPORT field  that stores the source port of the packet\/instruction that is used by the host driver  if the packet is a response to an instruction sent by the host driver . The DSPORT field  in the response header stores the destination port of the packet. The destination port is used to identify the DDOQ. The Request ID (RID) field  stores a request identifier sent by the host driver . This is used in conjunction with the destination port stored in the SPORT field . If a packet spans more than one descriptor  only the information structure of the first descriptor is updated. The host can calculate the number of descriptors used by looking at the packet size field  of the first descriptor's information structure.",{"@attributes":{"id":"p-0080","num":"0079"},"figref":"FIG. 6C","b":["642","508","650","652","508","100","654","512","656","520","206"],"i":"a"},{"@attributes":{"id":"p-0081","num":"0080"},"figref":"FIG. 6D","b":["690","690","658","660","662","664","670","672"]},{"@attributes":{"id":"p-0082","num":"0081"},"figref":["FIG. 7","FIG. 6B"]},"The standard BSD socket API calls socket( ), bind( ), listen( ), accept( ), write( ) and close( ) are well known to those skilled in the art and beyond the scope of the present invention.","At step , at boot time, the host driver  creates two control queues (CNTQ). These queues serve as a low latency message communication mechanism for messages from the network services processor  to the host system ","At step , the application calls the socket( ) API to create a new socket file descriptor. The stack socket module  returns the socket file descriptor to the host driver .","At step , the application calls the bind( ) API to assign an address to the socket file descriptor.","At step , the application calls the listen( ) API to listen for connections on the socket file descriptor. A DDOQ is created and associated to the listening socket. This DDOQ will be used by the stack socket module  to push new connections to the host socket module. The host socket module  keep a linked list of new connections which have been accepted by the networking stack , but are not being accepted by the application.","At step , the application calls the accept( ) API to get a connection.","At step , the stack socket module sends a \u201cconnection socket file descriptor\u201d to the application using the DDOQ created in the previous step. The connection socket file descriptor is sent via DMA to the Host memory location identified by the next descriptor in the listen socket's DDOQ.","At step , a DDOQ is allocated for the connection. For each connection socket, the Host socket APIs create a new DDOQ. From now on all communications for the listen socket file descriptor are via the Direct Data Output Queue (DDOQ).","At step , when data arrives on a socket, the network services processor  sends the data via DMA to the host memory location identified by the next descriptor in the socket's DDOQ. The network services processor also sends a packet to one of the control queues in the host to notify the Host driver  that new data has arrived on the socket connection.","At step , the application  calls the read( ) socket API to read the data arrived in step . The application  can also call write( ) socket API to send the data over the socket connection. The network services processor  sends a packet to the host to notify the host about the successful transmission of data.","At step , the application calls close( ) to close the socket connection.","At step , the socket switch module sends a packet to a DMA control queue (CNTQ) for the socket when the connection closes.",{"@attributes":{"id":"p-0095","num":"0094"},"figref":"FIG. 8","b":["100","206"],"i":"a "},"The network services processor  includes two counters and associated interrupts to support two control queues (CNTQ-) in the host memory. The network services processor  manages the counters. Each counter has an associated ring buffer in host memory and associated head\/tail pointers and available size values maintained by the network services processor .","The network services processor  inserts received packets at the head of the ring buffer in host memory. The host extracts packets from the tail of the ring buffer and indicates the amount of space that it freed up by sending an instruction to the network services processor.","The network services processor  maintains the following values to control and represent a control queue CNTQ. (a) A base pointer  to the start of the memory block in the host memory that contains the ring buffer. The base pointer is initialized at boot time and not subsequently modified. (b) A size  that indicates the size of the ring buffer in 8-byte blocks. This value is initialized at boot time and not subsequently modified. (c) A head pointer  that indicates the position in the ring where the network services processor will write the next packet. Under normal operation, the network services processor increments\/wraps this value as it writes in a new packet. (d) An available size  that indicates the number of 8-byte blocks that are available for the network services processor to write in the ring buffer. Under normal operation, the network services processor  decrements this value as it writes in a new packet, and the host software increments this value via an instruction as it removes packets from the ring buffer (in order). The available size value is initialized by the network services processor to the size of the ring.","The network services processor  and the host driver  maintain the control queues. As with the DDOQ rings described in conjunction with , the base and head\/tail pointers for each of the two control queues are maintained in local memory, and communication between the host and the network service processor is performed via PCI instructions sent to the network services processor .","The network services processor  includes registers to set the interrupt threshold for packet count or timeout and DMA counters to indicate the number of packets sent to the host system in the ring buffer. The core driver  acknowledges the packet receipt by writing registers with the number of packets it has extracted from the respective ring. In other words, it indicates the number of unacknowledged packets. Under normal operation, the network services processor  increments this register after it writes each packet into the ring, and the host decrements this count to acknowledge packets.","The core driver  can set interrupt levels when the number of unacknowledged packets increases above a programmed threshold or on a timeout. The core driver  configures the base pointer and size for each of the control queues at boot time. The current header and tail pointers are set to the base address of the ring and the available size is initialized to the size of the ring buffer. The packet is moved via DMA from the network services processor  into the memory location in PCI host processor memory indicated by the base pointer plus head pointer.","The ring is strictly in order and the available size is not increased until the oldest packet has been removed. Data is transferred to the buffer addresses supplied by the host driver in a descriptor (info\/buffer pointer) queue implemented using a ring buffer. Each entry in the ring is an info\/buffer pointer pair as shown in . Through the use of PCI instruction, the host driver rings a \u201cdoorbell\u201d to indicate space availability in a ring to the network services processor.","When the network services processor  wishes to send a packet to one of the DDOQ rings, it sends the packet data and also sends a control packet to a selected one of the two available control rings (CNTQ). This control packet includes a Response Header  to identify the DDOQ ring and packet size. The control packet serves as the indication to the host driver  in the host system that a packet arrived.","The network services processor  consults the base address and head\/tail pointers that it maintains in local memory to determine where both the packet data and the control packet data should be written in the host's memory. To transfer the packet data from local memory into the host memory, the network services processor constructs PCI DMA local pointers and PCI DMA host pointers. The network services processor  also appends additional PCI DMA local pointers and PCI DMA host pointers to transfer the control packet into the host memory after it has transferred the data packet.","Returning to , to transfer data directly between application memory and the network services processor , the application  uses a specialized zero-copy buffer pool in host memory for data transfers. The conventional mechanism of copying data from the user application to a networking buffer in kernel space that is, non-zero copy and vice versa is also supported. The option of selecting the zero-copy mechanism is controlled either through special flags in API calls or through the use of special API calls. For a TCP connection, the choice of using the zero-copy mechanism is made before the TCP connection is established. For a UDP connection, the selection is made before the first data transfer operation.","To set up the zero-copy mechanism, the application  issues an API call to request allocation of a buffer or multiple buffers from a specialized zero-copy buffer pool in host memory and to select zero copy for send and receive. The application  uses buffers for the I\/O operation which have been allocated from the zero copy buffer pool. The buffers are freed later by the application .","The enhanced socket system calls module  provides standard BSD calls for host applications  executing in a host system that is executing a standard operating system. The requests are directed through the modified socket system calls module  through the socket switch module  and the host networking stack  to the Network Interface Card (NIC) driver .","Direct socket APIs call functions in the direct socket calls module . The direct socket APIs have similar functionality to the BSD socket APIs in the enhanced socket system calls module  but the API calls are directed to the direct socket calls module  instead of the enhanced socket system calls module . The direct socket APIs are then directed to the host socket module  in the kernel through a system call to the kernel. The host socket module  includes functions for obtaining a unique file descriptor, ensuring that socket binding restrictions are met and maintaining connection related information.","The application  and enhanced socket system calls module  are stored in user space in memory in the host system . The enhanced socket system call module  includes a set of Application Programming Interface (API) functions that can be called by the host application to provide networking services. The application calls an API function in the enhanced socket system calls module  which makes a system call to the kernel. The underlying protocols and network mechanisms are hidden from the application . The socket interface provided through the enhanced socket system calls module  and the socket switch block  in the kernel provides the service requested by the system call by calling low-level functions in the kernel that handle the networking protocol.","A system can have multiple networking interfaces, for example, the NIC interface through the NIC driver  and the network interface through the network services processor  shown in . The interface's corresponding network stack can be used for all data communication. For example, if a server process in the host system receives a connection on a particular network interface, the corresponding stack is used for all subsequent sends and receives for the connection.","The enhanced socket system calls module  provides a mechanism to avoid use of the host networking stack  in cases for which the application  cannot be modified to use API calls provided by the Direct Socket Library module . If the application  is running in a system that is executing a standard operating system, modified BSD calls are provided through the enhanced socket system calls module . If the application does not specify a flag in create socket API call to choose a stack, additional checks are performed to select the stack.","There may be cases where it is not possible to modify an existing host application, but there may be a need to both bypass and use the host networking stack . This support is provided by allowing a decision to be made as to which networking stack to use when a socket is created. An application using host socket APIs makes a decision as to whether to use the host networking stack  through the use of flags in the API call when issuing an API call to create the socket.","An address-stack lookup table  is maintained in the kernel or operating system in which the IP address of each interface is associated with the type of networking stack, that is, the processor networking stack  in the network services processor  or the host networking stack  in the host system . Each entry  of the table includes the following fields: Interface IP address , Networking stack type (processor or system)  and Processor ID .","Each interface IP address is either for the host networking stack  or the processor processing stack . As there can be multiple processors in the host system , in the case of a processor interface, the Processor ID field  indicates which processor the interface belongs to. The address-stack lookup table  is initialized and updated through a separate utility executed in the host system. The standard API calls getsockopt and setsockopt allow socket options to be queried and set. Options such as stack type, socket type, zero-copy receive and send can be set through the use of the setsockopt API call. The values of the options can be obtained using the getsockopt API call.","Both the client and the server process create a Transmission Control Block (TCB) during the establishment of the connection, The TCB is a data structure used for a TCP\/UDP connection that stores information about the connection, such as, pointers to buffers in which incoming and outgoing data are stored, the number of bytes received and acknowledged and socket identifiers for the client and server processes that identify the connection. The use of a TCB for sending information for a TCP connection is known to these skilled in the art.","The socket identifier typically includes the IP address and the port number of the end point of the connection. As the client process knows the IP addresses and port numbers for both the client process and the server process, these are typically used to uniquely identify the connection, as discussed above. The TCB is maintained throughout the connection.","The socket switch module  interprets socket API calls from the modified socket system calls module  and directs the API calls to the appropriate networking stack , . To support existing unmodified applications, both the client process and server process cases are handled differently.",{"@attributes":{"id":"p-0118","num":"0117"},"figref":["FIG. 9","FIG. 9","FIG. 5"]},"On the server process side, a network stack selection decision is based on which network stack the connection request from the client process is coming from, that is, through the network services processor  or the NIC driver . Typically, a server process creates, binds and listens on a socket. Later, an accept that is received from the client process, returns a new file descriptor to be used for subsequent sends and receives.","At step , the application  first issues a BSD socket( ) API call to create a socket. This request to create a socket is sent through the enhanced socket system calls module  in user space in the host system and the modified socket system calls module  in kernel space and is buffered by the socket switch module . The request is not passed to either of the networking stacks. However, a file descriptor for the connection is allocated and returned to the application . The file descriptor is also referred to as a handle or socket descriptor and is passed to other API functions subsequently called by the application  to identify the particular socket.","The socket switch module  allocates structures to store the parameters of the socket, bind API as well as additional file descriptor related information. After copying the socket API parameters, the socket switch block  allocates a file descriptor. The file descriptor and other parameters for the API are stored in a socket parameters structure  which has already been described in conjunction with . Once the type of application is identified as a server application through the listen API, all the requests held by the socket switch block  are passed down to the stacks ,  using functions provided for this function. The host socket module  creates a host socket structure for each socket and sends the request to the Host Driver .","The host socket structure  has already been described in conjunction with .","When a socket is created by the application , it has an associated protocol (TCP or UDP), but no IP address and port number. The server socket cannot receive messages from a remote application until it is bound to a port number.","At step , the application issues a bind( ) API call to set up the connection to the port known to the client process. If at step , an IP address is specified in the bind API call, at step , the address stack lookup table in the kernel is consulted to find the stack type associated with the IP address. At step , once the networking stack type (processor or host) has been selected, the buffered socket and the bind requests are sent to the selected stack, and all the subsequent calls to this socket descriptor use the selected stack.","The host driver  sends a socket creation request to the network services processor . This request is passed to the Core driver  and then to the stack socket module . The stack socket module  sends it to the network stack , waits for the response and then returns a socket identifier and a response header to the core driver . The core driver  issues a DMA request to send the results and the completion code using the pointer to the application provided in the socket creation request. The host driver  passes the results back to the host socket module . The host socket module  updates the structure and returns status (success or failure) to the socket switch module .","The host socket module  sends a bind request (bind API) to the host driver  which sends the request to the network services processor . On receipt of the request by the network services processor, the information is passed to the core driver  and later to the stack socket module . The stack socket module  sends the request to the networking stack , waits for the response and later returns to the core driver  along with the response code. The core driver  issues a DMA request to send the results and the completion code back to the host driver . Upon receiving the result, the host driver  returns the result to the host socket module . The host socket module  returns status to the socket switch module .","If at step , the application specifies a wild card address in the bind API call instead of identifying a particular IP address, this information is buffered in the socket switch module . The wild card address allows the server process to receive connections addressed to any of its IP addresses. The socket and bind APIs are buffered by the socket switch module  because at this point, it is not clear whether the application is going to behave as a server or a client process. A subsequent API call, either listen at step  or connect at step  indicates whether the host application is a server or a client process (program).","As discussed earlier, a client program initiates communication with a server process and a server process (program) waits for clients to contact it. Prior to initiating communication, the client program needs to know the server's IP address and port. The client program can find out a server program's IP address by providing the Uniform Resource Location (URL) to a name resolution service. The port number is typically dependent on the type of application, for example, fixed port numbers are assigned to applications such as the File Transfer Protocol (FTP) and Telnet.","At step , the server program calls the listen API. If the address in the bind API was not a wild card address, at step , the listen API is sent to the selected stack. Otherwise, at step , if the server program calls the listen API, then at step , the listen API along with all the previously buffered requests, that is, a socket API call and bind API call are first sent to the host networking stack . The host networking stack  creates a listening socket. On a successful return, all the requests are also sent to the processor networking stack . If both stacks establish listening sockets, a successful completion status is returned to the application. From the application point of view there is only one socket descriptor, but actually there are two different socket descriptors listening on two different network stacks. The socket switch module  multiplexes and de-multiplexes the requests using the correct socket descriptor.","When the application  issues the accept API call, the socket switch module  passes the request to the host socket module . The host socket module  checks if there are new connections in the accept list. If there is no connection available, the host socket module  returns. The accept API is also sent to host socket module to indicate that application is willing to accept new connections.","In the blocking case, the process making the accept API call goes to sleep. When a new connection is available through network stack , the host driver  calls a dispatch function that is registered when initializing the DDOQ. The dispatch function inserts the new connection information in the accept list and wakes up the sleeping process.","Once the new connections are available, a DDOQ structure is allocated for each connection and the fields are updated accordingly.","For each connection, a corresponding DDOQ is set up to hold the received data, and pass the DDOQ parameters to a DDOQ initialization function. The network services processor  maintains base and head\/tail pointers for each DDOQ in local memory. The DDOQ identifier is associated to the socket identifier. The host socket module  passes this information the host driver  which sends it to the network services processor . After the DDOQ identifier and the accept confirmation is sent to the network services processor  and a response is received, the accept connections are sent to the host socket module . The host socket module  allocates memory to store the structure for each of the newly accepted connections. The host socket module  also allocates a field descriptor for each connection.","At step , when the server program issues an accept API call, both network stacks ,  are checked for the availability of a new connection. Once a new connection is available, a new socket descriptor is created and is associated with that particular stack. From this point onwards, at step , all the send and receive API calls as well as any other socket API requests are directed to the associated stack by the socket switch module .","If the application is a client program and does not specify an address in the bind API call, that is, uses the wild card address, and later at step , calls a connect API specifying a non-local (foreign) host address and a port, a decision is made on which stack to use. One or more IP routing tables are consulted to find out which interface could be used to access this foreign address. There may be multiple IP routing tables. There is an IP routing table maintained by the host networking stack, and an IP routing table maintained by each processor networking stack. If the foreign host address is accessible through the host networking stack, the socket descriptor is marked to use it and all the socket send and receive API calls that use that stack and vice versa. There is a possibility that the foreign host address is accessible from multiple or all interfaces, in this case a more intelligent route lookup mechanism is implemented which returns additional information, for example, whether the foreign host address is accessible though a default gateway or if it can be accessed directly through an interface. Even in this case, there is a possibility that all (or multiple) routing tables return the same information back. If this is the case, a decision is made to use a stack.","Once the stack has been selected, at step , all the buffered requests along with the connect API are sent to the particular stack. All the subsequent send, receive and other socket API requests are directed to this stack by the socket switch module.","The above case applies to the TCP protocol. The UDP protocol does not have any listening sockets. Thus, the send and receive cases differ from the TCP case that has been described.","For UDP, in the send case, the stack selection is performed based on the IP address specified in the send request as well as the IP routing table because the UDP send operation can only send the datagram through one stack. Once the selection is completed, this socket descriptor always uses the selected stack. In the receive case, the application can receive the datagram from both stacks unless the socket is bound to a particular IP address, in which case, the application only receives the datagram destined for that IP address. In order to achieve this, both stacks are setup to receive any datagram available and later pass it on to the application.","The host socket module  maintains a list of all the accepted connections. The socket switch block  issues an accept API call to the host socket module . The accept API call is not passed to the network services processor . Instead a new connection is pushed to the host system when it is available. When the stack in the network services processor  accepts new connections, it passes the accepted sockets to the stack socket module  in the network services processor , which in turn passes them to the core driver  in the network services processor  along with the associated Direct Data Output Queue (DDOQ) Identifier (ID) . The core driver  issues a DMA request to send the information of all the newly accepted connections to the host application  and returns the number of successful DMA transfers in terms of number of connections.","To transfer data packets on an accepted socket, the application  calls the send API. The socket switch block  finds the structures corresponding to the socket using the file descriptor. The socket switch block  sends the data to the host socket module . The host socket module  checks the connection state and sends the data to the host driver . Data can be transferred by the host driver  using zero or non-zero copy.","For non-zero copy send case, the data to be sent is passed to the host driver  by the host socket module . The host driver  copies the data between kernel buffers and application memory, creates a command, puts it in the command queue and indicates to the network services processor  that the command is in the queue. In non-blocking case, the control is returned to the application at this point, and application can used select API to check the sent request completion. For a blocking case, the application goes to sleep, waiting for some response from the network service processor . Later when the DMA operation is completed, that is, the send command is fetched by the network services processor  and an indication is returned that the data fetch is complete, the kernel buffers are freed, and application is notified of the operation completion.","For zero copy send case, the host socket module  sends the data and request to the host driver . The host driver  creates a command, puts it in a command queue. In non-blocking case, the control is returned to the application, and later application uses the select API to check the sent request status. In blocking case, the application waits for the response (goes to sleep while waiting). After the response is received, the number of bytes transferred is sent back to the application  through the host socket module . The host driver  does not free the buffers and the application  can reuse these buffers for a subsequent data transfer request.","For non-zero copy receive case, the host application  issues a receive API call on an accepted socket to receive data. The socket switch module  finds the corresponding socket structure using the file descriptor. The socket switch block  calls a receive data function defined by host socket module , to receive the data. The receive DDOQ for this socket has already been setup in the accept phase and the socket and the DDOQ association information has already been passed to the stack socket module . If the host socket module  has data available, it is sent back to the application. If this is not the case, the application  waits for the data (goes to sleep) if using the blocking mode. For non-blocking cases, if there is no data available in host socket module , the control is returned to application  indicating that no data is available. Later, application can use the select API to check the availability of the data.","When the data is available, the host driver  calls the DDOQ dispatch function. This function inserts the data in the receive list and also checks if someone is waiting for the data. If a process is waiting, the dispatch function also wakes up the sleeping process.","For the zero-copy receive case, the DDOQ descriptor list is empty. In this case, the application allocates buffers in application space (userspace) to hold the receive data. The host socket module  indicates to the network services processor  the availability of a new descriptor buffer. The socket structure in the host socket module  maintains a list of the receive data pointers for a particular socket. A dispatch function is registered for the host driver while initializing the DDOQ. This indicates to the host socket module  the availability of the received data in the DDOQ. Each list includes a head list entry, length flags and pointer to allocated memory. The receive mechanism is based on \u201cpush\u201d model. The network services processor networking stack  pushes the data to the host when the data is available. The first receive request sent to the stack initiates the push model. Before the receipt of this request, the stack  buffers all the available data and does not push it to the host.","The stack  always checks that there is descriptor buffer space available before pushing the data. If the space available is more than the data received, all the data will be pushed. If that is not the case the amount of data pushed is dependent on the descriptor buffer space available.","In the zero-copy receive case, the DDOQ is initialized with no descriptor buffers. If the socket module stack  has some data available to push, the absence of the descriptor buffers in the DDOQ stops it doing the push operation. When the application is ready to receive, it allocates the receive buffer and calls a function to get the receive data providing a pointer to the receive buffer. The host socket module  inserts a new buffer pointer into the DDOQ.","In the zero-copy blocking receive, because of the absence of the descriptor buffers, no data is available when the zero-copy read request is received by the host socket module . After performing a descriptor credit operation, the host socket module  sends a receive request to the stack socket module  and sleeps, waiting for the dispatch function to wake it up and indicate the availability of the received data. When the data is available, the DDOQ dispatch function wakes up the sleeping process. The number of bytes available is sent back to the application  along with a pointer to the buffer storing the data.","In the zero-copy non-blocking receive, the buffer is credited, and a receive request is sent to the networking stack . The control is returned immediately to the application  indicating that no data is available. When the data is available, the stack pushes it to the host descriptor buffers using the DDOQ. The DDOQ dispatch function will eventually transfer it to the host socket module . The DMA transfer between the host system and the network services processor will be described later in conjunction with . The application can use the select( ) API to query the availability of the received data.","In the non-zero copy case, descriptor buffers in the kernel are already allocated and the DDOQ has been initialized using these buffers. When the data is available, the core driver  pushes it to the DDOQ. The host driver calls the registered DDOQ dispatch callback function that indicates to the host socket module , the availability of data. The list in the corresponding socket structure is updated and the new data buffers are inserted in the list.","One of the flags field in the socket structure indicates that the connection has been terminated. Network services processor  notifies the host socket module  when a connection is terminated. Host socket module  updates the flags field accordingly. In case of connection termination, if the application  tries to read data, the indication of connection termination is sent back using the appropriate error code.",{"@attributes":{"id":"p-0152","num":"0151"},"figref":["FIG. 10","FIG. 2"],"b":["206","206","1002"]},"In one embodiment, each processor core  is a dual-issue, superscalar processor with instruction cache , Level 1 data cache , and built-in hardware acceleration (crypto acceleration module)  for cryptography algorithms with direct access to low latency memory over the low latency memory bus .","The network services processor  includes a memory subsystem. The memory subsystem includes level 1 data cache memory  in each core , instruction cache in each core , level 2 cache memory , a DRAM controller  for access to external DRAM memory  () and an interface  to external low latency memory.","The memory subsystem is architected for multi-core support and tuned to deliver both high-throughput and low-latency required by memory intensive content networking applications. Level 2 cache memory  and external DRAM memory  () are shared by all of the cores  and I\/O co-processor devices over a coherent memory bus . The coherent memory bus  is the communication channel for all memory and I\/O transactions between the cores , an I\/O Bridge (IOB)  and the Level 2 cache and controller .","Frequently used data values stored in DRAM  () may be replicated for quick access in cache (L1 or L2). The cache stores the contents of frequently accessed locations in DRAM  () and the address in DRAM where the contents are stored. If the cache stores the contents of an address in DRAM requested by a core , there is a \u201chit\u201d and the data stored in the cache is returned. If not, there is a \u201cmiss\u201d and the data is read directly from the address in DRAM  ().","A Free Pool Allocator (FPA)  maintains pools of pointers to free memory locations (that is, memory that is not currently used and is available for allocation) in DRAM  (). In one embodiment, the FPA unit  implements a bandwidth efficient (Last In First Out (LIFO)) stack for each pool of pointers.","In one embodiment, pointers submitted to the free pools are aligned on a 128 byte boundary and each pointer points to at least 128 bytes of free memory. The free size (number of bytes) of memory can differ in each pool and can also differ within the same pool. In one embodiment, the FPA unit  stores up to 2048 pointers. Each pool uses a programmable portion of these 2048 pointers, so higher priority pools can be allocated a larger amount of free memory. If a pool of pointers is too large to fit in the Free Pool Allocator (FPA) , the Free Pool Allocator (FPA)  builds a tree\/list structure in level 2 cache  or DRAM using freed memory in the pool of pointers to store additional pointers.","The I\/O Bridge (IOB)  manages the overall protocol and arbitration and provides coherent I\/O partitioning. The IOB  includes a bridge  and a Fetch and Add Unit (FAU) . The bridge  includes queues for storing information to be transferred between the I\/O bus , coherent memory bus , and the IO units including the packet input unit  and the packet output unit . The bridge  also includes a Don't Write Back (DWB) engine  that monitors requests to free memory in order to avoid unnecessary cache updates to DRAM  () when cache blocks are no longer required (that is, the buffers in memory are freed) by adding them to a free pool in the FPA unit . Prior to describing the operation of the bridge  in further detail, the IO units coupled to the IO bus  in the network services processor  will be described.","Packet Input\/Output processing is performed by an interface unit , , a packet input unit (Packet Input)  and a packet output unit (PKO) . The input controller and interface units , perform all parsing of received packets and checking of results to offload the cores .","The packet input unit  allocates and creates a work queue entry for each packet. This work queue entry includes a pointer to one or more buffers (blocks) stored in L2 cache  or DRAM  (). The packet input unit  writes packet data into buffers in Level 2 cache  or DRAM  in a format that is convenient to higher-layer software executed in at least one processor core  for further processing of higher level network protocols. The packet input unit  supports a programmable buffer size and can distribute packet data across multiple buffers in DRAM  () to support large packet input sizes.","A packet (datagram) is received by any one of the interface units , through a SPI-4.2 or RGM II interface. A packet can also be received by the PCI interface . The interface unit , handles L2 network protocol pre-processing of the received packet by checking various fields in the L2 network protocol header included in the received packet. After the interface unit , has performed L2 network protocol processing, the packet is forwarded to the packet input unit . The packet input unit  performs pre-processing of L3 and L4 network protocol headers included in the received packet. The pre-processing includes checksum checks for Transmission Control Protocol (TCP)\/User Datagram Protocol (UDP) (L3 network protocols).","The Packet order\/work (POW) module (unit) 1028 queues and schedules work (packet processing operations) for the processor cores . Work is defined to be any task to be performed by a core that is identified by an entry on a work queue. The task can include packet processing operations, for example, packet processing operations for L4-L7 layers to be performed on a received packet identified by a work queue entry on a work queue. The POW module  selects (i.e., schedules) work for a core  and returns a pointer to the work queue entry that describes the work to the core .","After the packet has been processed by the cores , a packet output unit (PKO)  reads the packet data stored in L2 cache  or memory (DRAM  (FIG. )), performs L4 network protocol post-processing (e.g., generates a TCP\/UDP checksum), forwards the packet through the interface unit , and frees the L2 cache  or DRAM  locations used to store the packet by adding pointers to the locations in a pool in the FPA unit .","The network services processor  also includes application specific co-processors that offload the cores  so that the network services processor achieves high-throughput. The application specific co-processors include a DFA co-processor  that performs Deterministic Finite Automata (DFA) and a compression\/decompression co-processor  that performs compression and decompression.","The Fetch and Add Unit (FAU)  is a 2KB register file supporting read, write, atomic fetch-and-add, and atomic update operations. The PCI interface controller  has a DMA engine that allows the processor cores  to move data asynchronously between local memory in the network services processor and remote (PCI) memory in the host in both directions according to the principles of the present invention.","As shown in , the network services processor includes a plurality of cores , one of which can be dedicated to executing an operating system that includes the socket modules described in conjunction with .",{"@attributes":{"id":"p-0168","num":"0167"},"figref":["FIG. 11","FIG. 10"],"b":["1002","1012","408","1106"]},"The PCI interface controller  has a DMA engine  that allows the cores  to move data asynchronously between local memory ,  and remote (PCI) host memory in the host system in both directions. The DMA engine  can be used to support per-flow queuing in PCI host memory. The DMA engine transfers data between local memory and memory on the host side of the PCI bus and updates a control queue on the host side.","A processor (core) can move data asynchronously between local (DRAM) memory and remote (PCI) memory in the host system by submitting PCI DMA instructions to the PCI DMA engine in the PCI Interface controller . The instruction to initiate the DMA operation enters a write buffer in the core , is reflected across the coherent memory bus , sent through the IO bridge (IOB)\/FAU , onto the IO bus , and enters the PCI interface module . The direction of transfer of the data that is, to memory in the host system (outbound) or from memory in the host system (inbound) can be indicated in the PCI DMA instruction executed by a DMA engine  in the PCI interface module . In one embodiment the PCI interface module  has two DMA engines .",{"@attributes":{"id":"p-0171","num":"0170"},"figref":["FIG. 12","FIG. 11","FIG. 13"],"b":["1200","1102","1102","1024","1200","1202","1204","1206"]},"The DMA HDR  describes the PCI DMA operation to be performed. The DIR field  indicates whether the PCI DMA transfer is inbound (store into local memory) or outbound (store into PCI memory space (host memory)). The WPQ field  indicates what is stored in the PTR field . The C field  indicates whether counter  or counter  is to be used. The CA field  indicates whether the selected counter is to be updated after the PCI DMA instruction  is complete. The FT field  indicates whether a force interrupt-bit is set after the PCI DMA instruction  is complete. The II field  indicates whether the buffers in local memory are to be freed. The FL field  indicates whether for an outbound operation, the associated memory buffer in host memory is freed after the local memory is read. The NR field  stores the number of PCI pointers in the PCI DMA instruction . The NL field  stores the number of Local pointers in the instruction . The PTR field  stores a work queue entry pointer or a local memory pointer dependent on the state of the WQP field. When the PTR  stores a work queue entry pointer the work queue entry indicated by the PTR field  is inserted into the work queue after the PCI DMA operation is complete. When the PTR field  stores a local memory pointer, the single byte in local memory indicated by the PTR is set to zero after the PCI DMA operation is complete. When the pointer is zero, no operation is performed after the PCI operation is complete. An outbound transfer completes after the last PCI write transaction appears on the PCI bus. An inbound transfer completes only after all its local memory stores commit.","The local pointer  describes a memory fragment in the local memory , . The number of pointers is set by the NL bit . The F field  indicates whether full block writes are allowed; The A field  indicates whether cache blocks should be allocated in L2 cache; The L field  indicates whether the data stored in the location identified by the pointer is in little endian format; The I , Back  and pool fields  are only used if the associated local memory buffer is freed, which can only happen in the outbound case. The I field  indicates whether buffers can be freed independently. The Back field  specifies the amount to subtract from the pointer to reach the start of the buffer. The pool field  selects a free list to return the block to. The size field  specifies the size of the contiguous space pointed at by ptr (in bytes). The ptr specifies the location in local memory , where the start of the packet is stored.","The PCI component  is a compressed list of PCI memory space pointers and their associated lengths. Up to 4 lengths are encoded in a length word. A component is a length word  and its associated pointers . A typical component  is 5 words, one word  stores the length of each PCI memory space pointer and each of the next four words  store a PCI memory space pointer. In a system with a word size of 64-bits, each PCI pointer  is 64-bits. Together with a length, the PCI pointer describes a fragment in PCI memory space. The pointer format is identical to the DPTR format defined above for the PCI input instructions.","As previously discussed, in one embodiment, the PCI interface includes two DMA engines  () that service PCI DMA instructions that are stored in two queues. At any time, the PCI DMA instructions  are selected from one of the two queues at a fixed priority. If two outbound instructions use the same queue, the writes for the two instructions appear on the PCI bus in the order that the instructions were queued. PCI DMA instructions are added at the head of a queue and a doorbell register is written to indicate the arrival of an instruction for the corresponding queue (typically with the number of words in the PCI DMA instructions in a chunk). The doorbell writes are accumulated and when read from the tail of the queue, the PCI DMA operation is performed. When the chunk at the tail of the instruction output queue is exhausted, that is, all the instructions that filled the chunk have been read, the tail chunk is freed and the instructions are read from the next chunk on the queue.",{"@attributes":{"id":"p-0176","num":"0175"},"figref":"FIG. 13","b":["1300","1302","1302","1200"]},"In one embodiment, each host queue (DDOQ) is implemented as a ring buffer. A ring buffer is a fixed size memory in which data is written consecutively from the start to the end of the buffer. When the end of the buffer is reached, data is once again written to the start of the buffer overwriting previously written data. Other queue\/buffer structures are possible in other embodiments. Each host queue has a head pointer , tail pointer  and outstanding doorbell count which are stored in L2 cache  or external DRAM . The host software issues a doorbell ring by submitting a work queue entry with a pointer to the packet stored in host memory to indicate space availability.","The doorbell count (that is, the distance between the head and tail pointers) indicates when there are words to be fetched from the queue. Multiple instructions may be launched with a single doorbell write and multiple doorbell writes can be used to announce the availability of a single instruction.","For an outbound transfer (to the host system), the doorbell counter is not incremented and interrupts are not set until the last PCI write for a chunk appears on the PCI bus. For an inbound transfer (from the host system), the order of local memory stores is not guaranteed. However, all memory stores are complete\/committed before the instruction completes. Memory read or Memory read multiple commands are created to service an inbound DMA request, and Memory write commands are created to service outbound DMA transfers.","For the packet output queues, the door bell ring is a direct PCI command\/status register in the PCI interface  that is written by the host processor over the PCI bus. For the host queues, the doorbell ring is a special work queue entry or packet received from the host. The host processor can create\/submit the special work queue entry or packet accomplishing the \u201cdoorbell ring\u201d via a PCI instruction issued over the PCI bus. Two additional rings are provided for control purposes. The base and head\/tail pointers for the control rings are also maintained in L1 cache or external DRAM and \u201cdoorbell rings\u201d for the control rings are also received via a PCI instruction.","To set up a DMA outbound transfer, that is, to send a packet to the host system, a PCI DMA instruction  is created. To create the PCI DMA instruction, PCI DMA local pointers and PCI DMA host pointers (PCI components) are selected to transfer the packet data from local memory ,  into the host memory. Additional PCI DMA local pointers and PCI DMA host pointers (PCI components) to the lists are appended to transfer the control packet into the host memory after the packet data. The packet data is written to an appropriate host memory ring and a control packet is written to one of the two available control rings. The control packet includes ring identifier  () and packet size fields  () and is the indication to the host system that packet data arrived at the identified ring. Both the packet and the control packet can be transferred to the host memory via a single outbound PCI DMA instruction.","The base and head\/tail pointers maintained in L2\/DRAM determine the locations where both the packet data and the control packet data are placed in the host memory. These addresses are supplied in the PCI DMA Instruction PCI components for the outbound transfer. The CA-bit in the PCI DMA instruction header is set to indicate the arrival of the control packet and the C-bit is set to identify which of the two control queues was used.","Thus, many simultaneous host queues can be handled. In one embodiment, up to one million host queues are handled simultaneously with each host queue implemented as a single ring. The network services processor maintains base and head\/tail pointers to the one million host queues, the base and head\/tail pointers for each of the two control queues are also maintained in local memory associated with the network services processor.","Processing time is further reduced by reducing the number of interrupts. An interrupt is only generated after n data has been transferred. For example, after five entries have been added to the queue. The interrupt is serviced by checking the entries on the queue, taking the entries from the queue and scheduling processors to handle the entries on the queue.",{"@attributes":{"id":"p-0185","num":"0184"},"figref":"FIG. 14"},"Core to core transfer has a similar API interface from the software point of view but there is no DMA engine to transfer data. Instead the TCP stack side cores act as a DMA engine and move data from the TCP connection buffers to application buffers. The other data structures are similar except that notification is sent via core-to-core communication mechanism, for example through Inter Processor Communication (IPC) or an interrupt.","To provide the zero-copy mechanism, an application running in a core allocates buffers from the zero-copy pool. Buffers are allocated and returned by the networking stack in the core.","While this invention has been particularly shown and described with references to preferred embodiments thereof, it will be understood by those skilled in the art that various changes in form and details may be made therein without departing from the scope of the invention encompassed by the appended claims."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The foregoing and other objects, features and advantages of the invention will be apparent from the following more particular description of preferred embodiments of the invention, as illustrated in the accompanying drawings in which like reference characters refer to the same parts throughout the different views. The drawings are not necessarily to scale, emphasis instead being placed upon illustrating the principles of the invention.",{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":["FIG. 2","FIG. 1"]},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":["FIG. 4","FIG. 2"]},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 5","b":"100","i":"a "},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIGS. 6A-D"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":["FIG. 7","FIG. 6B"]},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":["FIG. 10","FIG. 2"]},{"@attributes":{"id":"p-0031","num":"0030"},"figref":["FIG. 11","FIG. 10"]},{"@attributes":{"id":"p-0032","num":"0031"},"figref":["FIG. 12","FIG. 11"]},{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 14"}]},"DETDESC":[{},{}]}
