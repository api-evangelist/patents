---
title: Method and apparatus for efficient generation of texture coordinate displacements for implementing emboss-style bump mapping in a graphics rendering system
abstract: A graphics system including a custom graphics and audio processor produces exciting 2D and 3D graphics and surround sound. The system includes a graphics and audio processor including a 3D graphics pipeline and an audio digital signal processor. Emboss style effects are created using fully pipelined hardware including two distinct dot-product computation units that perform a scaled model view matrix multiply without requiring the Normal input vector and which also compute dot-products between the Binormal and Tangent vectors and a light direction vector in parallel. The resulting texture coordinate displacements are provided to texture mapping hardware that performs a texture mapping operation providing texture combining in one pass. The disclosed pipelined arrangement efficiently provides interesting embossed style image effects such as raised and lowered patterns on surfaces.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=06980218&OS=06980218&RS=06980218
owner: Nintendo Co., Ltd.
number: 06980218
owner_city: Kyoto
owner_country: JP
publication_date: 20001128
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","FIELD OF THE INVENTION","BACKGROUND AND SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF EXAMPLE EMBODIMENTS OF THE INVENTION"],"p":["This application claims the benefit of U.S. Provisional Application Ser. No. 60\/226,892, filed Aug. 23, 2000, the entire content of which is hereby incorporated by reference.","This application is also related to the following commonly assigned co-pending applications identified below, which focus on various aspects of the graphics system described herein. Each of the following applications are incorporated herein by reference:\n\n","The present invention relates to computer graphics, and more particularly to interactive graphics systems such as home video game platforms. Still more particularly this invention relates to efficient generation of texture coordinate displacements for implementing emboss-style bump-mapping effects for diffuse-lit textures on a rendered object.","Many of us have seen films containing remarkably realistic dinosaurs, aliens, animated toys and other fanciful creatures. Such animations are made possible by computer graphics. Using such techniques, a computer graphics artist can specify how each object should look and how it should change in appearance over time, and a computer then models the objects and displays them on a display such as your television or a computer screen. The computer takes care of performing the many tasks required to make sure that each part of the displayed image is colored and shaped just right based on the position and orientation of each object in a scene, the direction in which light seems to strike each object, the surface texture of each object, and other factors.","Because computer graphics generation is complex, computer-generated three-dimensional graphics just a few years ago were mostly limited to expensive specialized flight simulators, high-end graphics workstations and supercomputers. The public saw some of the images generated by these computer systems in movies and expensive television advertisements, but most of us couldn't actually interact with the computers doing the graphics generation. All this has changed with the availability of relatively inexpensive 3D graphics platforms such as, for example, the Nintendo 64\u00ae and various 3D graphics cards now available for personal computers. It is now possible to interact with exciting 3D animations and simulations on relatively inexpensive computer graphics systems in your home or office.","One problem that graphics system designers have often confronted in the past was the efficient rendering of a 3D object that displays realistic-looking surface characteristics that react to various lighting conditions in a manner similar to the surface of an actual object having, for example, random surface flaws, irregularities, roughness, bumps or other slight non-planar surface variations. While in some instances such minute surface characteristics might be actually modeled, the time required for translating and rendering a 3D object with such a complex surface would be prohibitive for most real-time or interactive gaming applications. Consequently, various solutions to this problem were offered. For example, a technique generally known as \u201cbump-mapping\u201d was developed which allowed one to approximate the effect that non-planar surface variations would produce on lighted object. See, for example, J. F. Blinn \u201cSimulation of Wrinkled Surfaces\u201d , (SIGRAPH '78 Proceedings), vol. 12, No. 3, pp. 286\u2013292 (August 1978); \u201cModels of Light Reflection for Computer Synthesized Pictures\u201d, Proc. 4Conference on Computer Graphics and Instructive Techniques, 1977; and \u201cProgramming with OpenGL: Advanced Rendering\u201d by Tom McReynolds and David Blythe\u201497 \u2014Section 8.3 \u201cBump Mapping with Textures\u201d. Basically, this technique allows a graphics application programmer to add realism to an image without using a lot of geometry by modeling small surface variations as height differences and then applying those difference values over a surface as perturbations to a surface Normal vector used in computing surface lighting effects. Effectively, a bump-map modifies the shading of a polygon by perturbing the surface Normal on a per-pixel basis. The shading makes the surface appear bumpy, even though the underlying geometry is relatively flat.","Most conventional approaches toward implementing simple forms of bump-mapping effects with diffuse-lit textured surfaces generally entail computing, for each pixel, the difference between a first sample of a bump map texture image at a particular texture coordinate and a second sample of the same texture image at a texture coordinate displacement. In addition, computing a texture coordinate displacement map generally involves computations using eye-space components of surface Tangent and Binormal vectors (binormals). In particular, to implement a simple form of bump-mapping having an embossing type effect on a texture image, it is most efficient to compute and apply the texture coordinate displacements in the eye-space (view-space\/camera-space) reference frame\u2014which is more conducive to a subsequent rasterizing process prior rendering for display. Consequently, texture coordinate displacement for emboss-style bump-mapping is preferably computed and generated after vertex position and surface binormals at a vertex are transformed from model-space into eye-space for pixel rendering.","Typically, in low cost graphics processing systems such as a home video game system, vertex transformation and lighting (T&L) operations are commonly performed by the application program using the graphics system host CPU\u2014primarily because a software T&L implementation, although more computationally taxing on the host CPU, is usually less expensive than using specialized hardware. Hardware implementation of T&L, however, may be preferable in gaming systems because it typically results in much faster renderings and can free up host CPU processing time for performing other desirable tasks such as game strategy and AI computations for improved game performance. Moreover, in graphics rendering arrangements where T&L operations are performed by the application software on the host CPU, additional processing tasks such as performing texture coordinate computations for bump-mapping can significantly add to the processing overhead.","In graphics rendering systems where the T&L operations are performed by dedicated graphics hardware, the host CPU typically provides model-space vertex attributes to the dedicated T&L hardware and then allows the hardware to perform all the coordinate space transformations and lighting computations. Consequently, it is not particularly efficient to require the host CPU to compute texture coordinate displacements for bump mapping purposes subsequent to the T&L hardware performing space transformations of the vertex position and surface normal\/binormal vectors. Essentially, this would effectively undermine rendering speed improvements gained from utilizing dedicated T&L hardware whenever bump mapping operations are performed.","The present invention solves this problem by providing techniques and arrangements in a graphics rendering system for the efficient generation of texture coordinate displacements for implementing at least an emboss-style bump-mapping texture effect without the need for the host CPU application software to compute the required texture coordinate displacements. An enhanced API (applications program interface) vertex attribute function capable of specifying three surface normals per vertex (i.e., the Normal, Tangent and Binormal) is utilized and the host CPU application software need only compute the required additional Tangent and Binormal surface vectors per vertex in object-space (model-space), in addition to providing the surface Normal and other conventional per-vertex attributes.","Some of the features provided by aspects of this invention include:\n\n","In accordance with one aspect of the present invention, a graphics rendering system is provided with enhanced vertex transformation and lighting (T&L) hardware that is capable of performing at least simple emboss-style bump-mapping in addition to the conventional T&L operations. This style of bump-mapping is useful when the surface geometry of an object is being animated. The vector geometry processing portion of the T&L hardware is enhanced to accommodate processing a transformation of object-space vertex surface binormals (i.e., the Tangent and Binormal vectors) to eye-space and the computation of a texture coordinate displacement based on light direction (light-to-vertex) vector dot products with the transformed binormals.","In accordance with another aspect of the present invention, an enhanced vertex attribute description API function provides three vertex surface normals (N, B and T) to the T&L vector geometry processing hardware along with vertex position and light source position. The geometry processing hardware then transforms the surface normals to eye-space, computes the light vector in eye-space and uses the vector components to compute the appropriate texture coordinate displacements for use in producing an emboss-style bump mapped texture effect.",{"@attributes":{"id":"p-0025","num":"0053"},"figref":"FIG. 1","b":["50","50"]},"In this example, system  is capable of processing, interactively in real time, a digital representation or model of a three-dimensional world. System  can display some or all of the world from any arbitrary viewpoint. For example, system  can interactively change the viewpoint in response to real time inputs from handheld controllers , or other input devices. This allows the game player to see the world through the eyes of someone within or outside of the world. System  can be used for applications that do not require real time 3D interactive display (e.g., 2D display generation and\/or non-interactive display), but the capability of displaying quality 3D images very quickly can be used to create very realistic and exciting game play or other graphical interactions.","To play a video game or other application using system , the user first connects a main unit  to his or her color television set  or other display device by connecting a cable  between the two. Main unit  produces both video signals and audio signals for controlling color television set . The video signals are what controls the images displayed on the television screen , and the audio signals are played back as sound through television stereo loudspeakers L, R.","The user also needs to connect main unit  to a power source. This power source may be a conventional AC adapter (not shown) that plugs into a standard home electrical wall socket and converts the house current into a lower DC voltage signal suitable for powering the main unit . Batteries could be used in other implementations.","The user may use hand controllers , to control main unit . Controls  can be used, for example, to specify the direction (up or down, left or right, closer or further away) that a character displayed on television  should move within a 3D world. Controls  also provide input for other applications (e.g., menu selection, pointer\/cursor control, etc.). Controllers  can take a variety of forms. In this example, controllers  shown each include controls  such as joysticks, push buttons and\/or directional switches. Controllers  may be connected to main unit  by cables or wirelessly via electromagnetic (e.g., radio or infrared) waves.","To play an application such as a game, the user selects an appropriate storage medium  storing the video game or other application he or she wants to play, and inserts that storage medium into a slot  in main unit . Storage medium  may, for example, be a specially encoded and\/or encrypted optical and\/or magnetic disk. The user may operate a power switch  to turn on main unit  and cause the main unit to begin running the video game or other application based on the software stored in the storage medium . The user may operate controllers  to provide inputs to main unit . For example, operating a control  may cause the game or other application to start. Moving other controls  can cause animated characters to move in different directions or change the user's point of view in a 3D world. Depending upon the particular software stored within the storage medium , the various controls  on the controller  can perform different functions at different times.","Example Electronics of Overall System",{"@attributes":{"id":"p-0031","num":"0059"},"figref":"FIG. 2","b":"50","ul":{"@attributes":{"id":"ul0005","list-style":"none"},"li":{"@attributes":{"id":"ul0005-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0006","list-style":"none"},"li":["a main processor (CPU) ,","a main memory , and","a graphics and audio processor ."]}}}},"In this example, main processor  (e.g., an enhanced IBM Power PC 750) receives inputs from handheld controllers  (and\/or other input devices) via graphics and audio processor . Main processor  interactively responds to user inputs, and executes a video game or other program supplied, for example, by external storage media  via a mass storage access device  such as an optical disk drive. As one example, in the context of video game play, main processor  can perform collision detection and animation processing in addition to a variety of interactive and control functions.","In this example, main processor  generates 3D graphics and audio commands and sends them to graphics and audio processor . The graphics and audio processor  processes these commands to generate interesting visual images on display  and interesting stereo sound on stereo loudspeakers R, L or other suitable sound-generating devices.","Example system  includes a video encoder  that receives image signals from graphics and audio processor  and converts the image signals into analog and\/or digital video signals suitable for display on a standard display device such as a computer monitor or home color television set . System  also includes an audio codec (compressor\/decompressor)  that compresses and decompresses digitized audio signals and may also convert between digital and analog audio signaling formats as needed. Audio codec  can receive audio inputs via a buffer  and provide them to graphics and audio processor  for processing (e.g., mixing with other audio signals the processor generates and\/or receives via a streaming audio output of mass storage access device ). Graphics and audio processor  in this example can store audio related information in an audio memory  that is available for audio tasks. Graphics and audio processor  provides the resulting audio output signals to audio codec  for decompression and conversion to analog signals (e.g., via buffer amplifiers L, R) so they can be reproduced by loudspeakers L, R.","Graphics and audio processor  has the ability to communicate with various additional devices that may be present within system . For example, a parallel digital bus  may be used to communicate with mass storage access device  and\/or other components. A serial peripheral bus  may communicate with a variety of peripheral or other devices including, for example:\n\n","A further external serial bus  may be used to communicate with additional expansion memory  (e.g., a memory card) or other devices. Connectors may be used to connect various devices to busses , , .","Example Graphics And Audio Processor",{"@attributes":{"id":"p-0037","num":"0071"},"figref":"FIG. 3","b":["114","114","114"],"ul":{"@attributes":{"id":"ul0009","list-style":"none"},"li":{"@attributes":{"id":"ul0009-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0010","list-style":"none"},"li":["a processor interface ,","a memory interface\/controller ,","a 3D graphics processor ,","an audio digital signal processor (DSP) ,","an audio memory interface ,","an audio interface and mixer ,","a peripheral controller , and","a display controller ."]}}}},"3D graphics processor  performs graphics processing tasks. Audio digital signal processor  performs audio processing tasks. Display controller  accesses image information from main memory  and provides it to video encoder  for display on display device . Audio interface and mixer  interfaces with audio codec , and can also mix audio from different sources (e.g., streaming audio from mass storage access device , the output of audio DSP , and external audio input received via audio codec ). Processor interface  provides a data and control interface between main processor  and graphics and audio processor .","Memory interface  provides a data and control interface between graphics and audio processor  and memory . In this example, main processor  accesses main memory  via processor interface  and memory interface  that are part of graphics and audio processor . Peripheral controller  provides a data and control interface between graphics and audio processor  and the various peripherals mentioned above. Audio memory interface  provides an interface with audio memory .","Example Graphics Pipeline",{"@attributes":{"id":"p-0040","num":"0082"},"figref":"FIG. 4","b":["154","154","200","180","110","200","110","115","111","114","111","114"]},"Command processor  receives display commands from main processor  and parses them\u2014obtaining any additional data necessary to process them from shared memory . The command processor  provides a stream of vertex commands to graphics pipeline  for 2D and\/or 3D processing and rendering. Graphics pipeline  generates images based on these commands. The resulting image information may be transferred to main memory  for access by display controller\/video interface unit \u2014which displays the frame buffer output of pipeline  on display .",{"@attributes":{"id":"p-0042","num":"0084"},"figref":"FIG. 5","b":["154","110","210","212","214","112","200","150","110","210","110","200"],"ul":{"@attributes":{"id":"ul0011","list-style":"none"},"li":{"@attributes":{"id":"ul0011-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0012","list-style":"none"},"li":["command streams from main memory  via an on-chip FIFO memory buffer  that receives and buffers the graphics commands for synchronization\/flow control and load balancing,","display lists  from main memory  via an on-chip call FIFO memory buffer , and","vertex attributes from the command stream and\/or from vertex arrays  in main memory  via a vertex cache ."]}}}},"Command processor  performs command processing operations that convert attribute types to floating point format, and pass the resulting complete vertex polygon data to graphics pipeline  for rendering\/rasterization. A programmable memory arbitration circuitry  (see ) arbitrates access to shared main memory  between graphics pipeline , command processor  and display controller\/video interface unit .",{"@attributes":{"id":"p-0044","num":"0089"},"figref":"FIG. 4","b":"180","ul":{"@attributes":{"id":"ul0013","list-style":"none"},"li":{"@attributes":{"id":"ul0013-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0014","list-style":"none"},"li":["a transform unit ,","a setup\/rasterizer ,","a texture unit ,","a texture environment unit , and","a pixel engine ."]}}}},"Transform unit  performs a variety of 2D and 3D transform and other operations (see ). Transform unit  may include one or more matrix memories for storing matrices used in transformation processing . Transform unit  transforms incoming geometry per vertex from object space to screen space; and transforms incoming texture coordinates and computes projective texture coordinates (). Transform unit  may also perform polygon clipping\/culling (). Lighting processing also performed by transform unit provides per vertex lighting computations for up to eight independent lights in one example embodiment. As discussed herein in greater detail, Transform unit  also performs texture coordinate generation () for emboss-style bump mapping effects.","Setup\/rasterizer  includes a setup unit which receives vertex data from transform unit  and sends triangle setup information to one or more rasterizer units () performing edge rasterization, texture coordinate rasterization and color rasterization.","Texture unit  (which may include an on-chip texture memory (TMEM) ) performs various tasks related to texturing including for example:\n\n","Texture unit  performs texture processing using both regular (non-indirect) and indirect texture lookup operations. A more detailed description of the example graphics pipeline circuitry and procedures for performing regular and indirect texture look-up operations is disclosed in commonly assigned co-pending patent application, Ser. No. 09\/722,382, entitled \u201cMethod And Apparatus For Direct And Indirect Texture Processing In A Graphics System\u201d and its corresponding provisional application, Ser. No. 60\/226,891, filed Aug. 23, 2000, both of which are incorporated herein by reference.","Texture unit  outputs filtered texture values to the Texture Environment Unit  for texture environment processing (). Texture environment unit  blends polygon and texture color\/alpha\/depth, and can also perform texture fog processing () to achieve inverse range based fog effects. Texture environment unit  can provide multiple stages to perform a variety of other interesting environment-related functions based for example on color\/alpha modulation, embossing, detail texturing, texture swapping, clamping, and depth blending. Texture environment unit  can also combine (e.g., subtract) textures in hardware in one pass. For more details concerning the texture environment unit , see commonly assigned application Ser. No. 09\/722,367 entitled \u201cRecirculating Shade Tree Blender for a Graphics System\u201d and its corresponding provisional application, Ser. No. 60\/226,888, filed Aug. 23, 2000, both of which are incorporated herein by reference.","Pixel engine  performs depth (z) compare () and pixel blending (). In this example, pixel engine  stores data into an embedded (on-chip) frame buffer memory . Graphics pipeline  may include one or more embedded DRAM memories  to store frame buffer and\/or texture information locally. Z compares \u2032 can also be performed at an earlier stage in the graphics pipeline  depending on the rendering mode currently in effect (e.g., z compares can be performed earlier if alpha blending is not required). The pixel engine  includes a copy operation that periodically writes on-chip frame buffer  to main memory  for access by display\/video interface unit . This copy operation can also be used to copy embedded frame buffer  contents to textures in the main memory  for dynamic texture synthesis effects. Anti-aliasing and other filtering can be performed during the copy-out operation. The frame buffer output of graphics pipeline  (which is ultimately stored in main memory ) is read each frame by display\/video interface unit . Display controller\/video interface  provides digital RGB pixel values for display on display .","Example Emboss-Style Bump Mapping Texture Coordinate Generation",{"@attributes":{"id":"p-0051","num":"0104"},"figref":["FIG. 6","FIG. 6"],"b":["300","200","200","110","112"]},"Briefly, the graphics pipeline renders and prepares images for display at least in part in response to polygon vertex attribute data and texel color data stored as a texture image in an associated memory. The graphics rendering pipeline is provided with vertex transformation and lighting (T&L) hardware that is capable of performing simple bump-mapping operations in addition to the more conventional T&L operations. Pipelined hardware efficiently generates texture coordinate displacements for implementing emboss-style bump-mapping effects utilizing object-space (model-space) surface normals supplied per vertex, for example, by a graphics application running on the main CPU of the graphics system. An enhanced vertex attribute description command function facilitates the communication and processing of plural surface normals per-vertex in addition to other vertex attributes such as vertex position, light source position and texture coordinates. The enhanced vertex attribute function specifies Normal, Tangent and Binormal surface vectors (N, T & B) provided by the host CPU in object space coordinates and uses separate memory indexes per vertex for each of the three surface vectors so as to effectively compress the amount of data needed for bump mapping. A vector geometry processing portion of the T&L hardware is also enhanced by providing two distinct dot-product computation units to transform the Tangent and Binormal surface vectors to eye-space using a scaled model view matrix, compute a light direction vector in eye-space and perform parallel dot-product computations between the computed light direction vector and the transformed Tangent and Binormal vectors to efficiently generate the appropriate texture coordinate displacements for use in creating an embossed texture effect.","In one example embodiment, system  first stores a texture image in texture memory  (see ) for use with the bump mapping operation (block ). Command Processor  then provides object-space basis Tangent and Binormal vector data to transform Transform Unit  using vertex attribute functions defined in an appropriate graphics API (block ). The Transform Unit  transforms the Tangent and Binormal vector data to eye space (block ). Transform Unit  also computes a light direction (light-to-vertex) vector and a normalized light direction vector (block ). Transform Unit  then computes texture coordinate displacements and new texture coordinate values per vertex (blocks , ). Texture Environment (TEV) unit  develops an embossed texture from the original texture stored in texture memory  minus the offset texture defined by the displacements (block ). In other words, the original texture is looked-up using both non-displaced coordinates (s, t) and displaced coordinates (s+\u0394s, t+\u0394t) and the texture values are subtracted per-pixel. The result is combined with per-vertex local diffuse lighting in graphics pipeline  and the resulting embossed image is rendered for display on display  (block ). The embossed image results may also be combined with other textures.","In more detail, bump mapping described above generates at least: (1) texture coordinate displacements (\u0394s, \u0394t) based on incoming texture coordinates (block ), (2) a normalized light direction (block ) and (3) a per-vertex coordinate basis function (block ). The preferred basis function used is an orthogonal object-space coordinate basis. The three orthogonal axes of this coordinate basis are defined by the surface Normal vector, a surface \u201cTangent\u201d vector and a second mutually perpendicular surface tangent \u201cBinormal\u201d vector with the Tangent (T) and Binormal (B) vectors oriented in directions corresponding to the texture gradient in s and the texture gradient in t (i.e., increasing s or t). The two orthogonal surface tangent vectors, T and B, are also called \u201cbinormals\u201d. Block  provides these values. An object-space coordinate light vector projected onto this coordinate basis (block ) is then used to compute texture coordinate displacements for bump-mapping. More specifically, the projection of the light direction vector onto each of the two binormals, T and B, gives the amount of texture space displacement the light causes. Basically, the light on the texture (i. e., the light direction vector) is decomposed into its surface normal component and its (s, t) coordinate components corresponding to the respective texture gradients. These (s, t) coordinate components of the light direction vector are the (\u0394s, \u0394t) texture coordinate displacements (block ) used for bump mapping.","To perform the above operations properly for efficient rendering, object oriented Tangent and Binormal vectors at each vertex, which map in object space to the texture s and t axis, are preferably first converted to eye-space. Consequently, in the example implementation of the present invention, Command Processor  supplies these two binormals per-vertex to Transform Unit  (block ). The Transform Unit will then transform the binormals to eye-space (block ). (For the present example embodiment, even where the supplied binormals are constant, for example, with flat surfaces, Command Processor  supplies the binormals to Transform Unit  on a per-vertex basis.) Mathematically, the following operations are performed by Transform Unit  are: \n\n\n","Given the binormal basis system, the light rotation matrix used by Transform Unit  (block ) is as follows: \n\n\n","The light vector is computed (block ) by normalizing the difference between the light position (in eye-space) and the current, transformed, vertex in eye space as follows: \n\n","The texture coordinate displacement (\u0394s, \u0394t) is then computed per-vertex (block ) as follows: \n\n","Note that this preferred example algorithm does not use the Normal input vector to compute displacements. Only the Binormal and Tangent vectors are required. Other implementations specify a 3\u00d73 matrix multiply including the eye-space Normal as an extra row.","The computed per-vertex delta offsets, (\u0394s, \u0394t), are then added to the post-transform (i.e., after transform to eye-space) texture coordinate generated per-vertex (block ) to obtain new texture coordinates S1 and T1: \n\n\nExample Emboss Bump-Mapping Texture Coordinate Generation Hardware Implementation\n","To efficiently implement the above computation for emboss-style bump-mapping, Transform Unit  includes hardwired computational logic circuitry to perform at least the following emboss bump-mapping related vector and coordinate computations:",{"@attributes":{"id":"p-0062","num":"0117"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"168pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Compute T= MV \u00b7 T"]},{"entry":[{},"Compute B= MV \u00b7 B"]},{"entry":[{},"Compute L = V\u2212 L"]},{"entry":[{},"Compute L"]},{"entry":[{},"Compute T\u00b7 L"]},{"entry":[{},"Compute B\u00b7 L"]},{"entry":[{},"Compute 1\/\u2225L\u2225 = 1\/sqrt (L)"]},{"entry":[{},"Compute \u0394s = T \u00b7 L\/\u2225L\u2225"]},{"entry":[{},"Compute \u0394t = B \u00b7 L\/\u2225L\u2225"]},{"entry":[{},"Compute (S1, T1) = (S0 + \u0394s, T0 + \u0394t)"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}},"ul":{"@attributes":{"id":"ul0021","list-style":"none"},"li":{"@attributes":{"id":"ul0021-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0022","list-style":"none"},"li":"where T and B are the respective object-space Tangent and Binormal vectors; MV is a transformation matrix having element values for converting vectors to eye-space; Lis the light position vector; Vis the vertex position vector; L is the light-to-vertex vector; ||L|| is the normalized light direction vector; (S0, T0) are the regular transformed texture coordinates, (\u0394s, \u0394t), are the generated texture coordinate displacement values; and (S1, T1) are the new texture coordinates from which an \u201coffset\u201d texture used in emboss bump-mapping is obtained."}}}},{"@attributes":{"id":"p-0063","num":"0119"},"figref":["FIG. 7","FIGS. 7 and 8"],"b":["300","110","300","200","200","300","300"]},"Referring again to , block outlines specific vector dot-product processing hardware which may also be employed by Transform Unit  in performing computations other than that related to emboss-style bump-mapping. While block outlines Transform Unit hardware more specifically useful in emboss bump-mapping computations, block hardware may also be useful in performing other functions. For emboss-style bump-mapping, a first dot-product computation unit, , computes the eye-space transformation of the Tangent and Binormal vectors. The transformed results are temporarily stored in multiplexing\/staging buffer . A light-to-vertex vector computation  is performed on vertex position vector data, V, and light position vector data L, to provide light direction vector data, L. A second dot-product computation unit  is utilized to compute, in parallel, the following:\n\n","The Lvector product is subsequently provided to inverse square-root computation unit  for computing an inverse magnitude value of the light direction vector. The Binormal and Tangent vector lighting dot-products T\u2022L and B\u2022L from dot unit  are provided to floating multiplier  alone with the computed inverse magnitude value of the light direction vector from unit . Floating point multiplier  then computes the texture coordinate displacements \u0394S and \u0394T which are passed to floating point adder . Transformed texture coordinates S0 and T0 are provided per vertex to delay FIFO  and are passed in a timely fashion to floating point adder  for combination with computed coordinate displacements \u0394S and \u0394T. The new texture coordinates generated, S1 and T1, are then passed to a vertex buffer unit (not shown) within transform unit  and subsequently passed via graphics pipeline  to texture unit  for texture lookup. In the preferred embodiment, the texture combining unit used is capable of performing texture subtraction in one pass instead of multiple passes. The preferred texture combining operation does not use an accumulation buffer, but instead does texture combining in texture hardware.",{"@attributes":{"id":"p-0066","num":"0124"},"figref":["FIG. 8","FIG. 7","FIG. 8","FIG. 5"],"b":["301","303","300","301","310","311","312","313","314","310","311","312","220","315","300","310","311","312","304"],"i":"b "},"Vector dot unit  includes floating multipliers ,  and  and floating point adders  and  for computing vector dot products of the light direction vector and the Tangent and Binormal eye space vector components. Dot unit  may also include multiplexor  for receiving and staging light direction vector and transformed eye-space Tangent and Binormal vector data from floating point adder  and dot unit . Floating point multipliers  through  are used in combination with floating point adders  and  to provide a light direction vector squared product. L, a Tangent lighting vector dot-product (T\u2022L) and a Binormal lighting dot product (B\u2022L) at the output of floating point adder .","A table illustrating an example schedule of computational events for accomplishing emboss-style bump-mapping occurring per pipeline data clocking cycle\/stage within Transform Unit  using dot unit  and dot unit  is provided immediately below:",{"@attributes":{"id":"p-0069","num":"0127"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"56pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"70pt","align":"center"}}],"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}},{"entry":["Cycle #","Vector Dot Unit #1","VFAdder","Vector Dot Unit #2"]},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"2","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"56pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"70pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["1","Load T",{},{}]},{"entry":["2","T= M0 \u00b7 T"]},{"entry":["3","T= M1 \u00b7 T"]},{"entry":["4","T= M2 \u00b7 T"]},{"entry":["5","Load B"]},{"entry":["6","B= M0 \u00b7 B"]},{"entry":["7","B= M1 \u00b7 B"]},{"entry":["8","B= M2 \u00b7 B","Lx = Vex \u2212 Lpx"]},{"entry":["9",{},"Ly = Vey \u2212 Lpy"]},{"entry":["10",{},"Lz = Vez \u2212 Lpz"]},{"entry":["11","Out T"]},{"entry":["12","Out T"]},{"entry":["13","Out T"]},{"entry":["14",{},{},"Ld T, L"]},{"entry":["15","Out B",{},"Out T \u00b7 L; Ld L"]},{"entry":["16","Out B",{},"Out L"]},{"entry":["17","Out B"]},{"entry":["18",{},{},"Ld B"]},{"entry":["19",{},{},"Out B \u00b7 L"]},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}}]}}]}}},"During, relative cycles\/stages numbered 1 through 8, the Tangent and Binormal vectors are loaded into dot unit  and the transforms to eye space are computed. During cycles 9 through 11, light direction vector components L, L, and Lare computed by floating point adder  using eye space vertex composition components and negative signed light position components. During cycles 11\u201313, the computed Tangent vector eye space components are loaded into multiplexing\/staging buffer . During Cycle 14, the computed light direction vector, L, and the computed Tangent eye space vector. Teye=(T, T, T), are loading into the vector dot unit  for computing the T\u2022L dot product. On cycle 15, the computed light direction vector, L, is again loaded into the vector dot unit  to compute the light direction vector squared product, L2. Finally, the binormal eye space vector, Beye=(B, B, B) is loaded on cycle 18 to compute the B\u2022L dot product. The hardware described above is fully pipelined and can compute the required values in a minimal number of distinct operations.","Example API Function Commands","In the preferred embodiment, an enhanced graphics API function is used to initiate texture coordinate veneration within transform unit . In addition to conventional texture coordinate generation wherein current vertex attribute information is used to generate a texture coordinate, the preferred graphics API supports an enhanced texture generation function that is capable of calling and using other texture coordinate generation functions. An example enhanced API texture coordinate generation function may be defined as follows:","GXSetTexCoordGen","Arguments:",{"@attributes":{"id":"p-0072","num":"0000"},"ul":{"@attributes":{"id":"ul0025","list-style":"none"},"li":{"@attributes":{"id":"ul0025-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0026","list-style":"none"},"li":["GXTexCoord DstCoord; \/\/ name of generated texture coordinates","GxtexGenType Func; \/\/ coordinate generation function type","GXTexGenSrc Srcparam; \/\/ Source parameters for coord generation","u32 MatIdx; \/\/ Texture Matrix Index."]}}}},"The above example API function defines general texture coordinate generation in addition to supporting other texture coordinate generation functions. The MatIdx is set as the default texture matrix index by which the generated texture coordinates are to be transformed. In the present example embodiment, to implement emboss-style bump-mapping, the above API function is used with Func set to GXTGBUMP*, where * is a number from 0\u20137 indicative of one of up to eight possible different lights (light source positions) which may be selected for embossing.","The following is an example C\/C++ language implementation of the above general texture coordinate generation function:\n\n","With \u201cfunc\u201d set to GXTGBUNIP0\u20137, system  performs emboss-style bump mapping by perturbing input texture coordinates based on per-vertex specified binormals and light direction information. The original and offset texture coordinates are used to look up texels from a height-field bump map texture stored in memory . TEV unit  can be used to subtract these values in hardware in one pass to find the bump height, which value can be added to the final color of the pixel to provide emboss-style bump mapping. GXBUMP0 indicates that light 0 will be used, GXBUMP1 indicates that light 1 will be used, etc., in the bump map calculation.","The dstcoord for bump maps should be numbered sequentially, i.e. base texture coordinate=n, and bump offset texture coordinate=n+1. Bump map texture coordinates should be generated after coordinates generated from transforms (GXTGMTX2\u00d74 and GXTGMTX3\u00d74) and before coordinates generated from lighting channels (GXTGSRTG). An example follows:\n\n","Certain of the above-described system components  could be implemented as other than the home video game console configuration described above. For example, one could run graphics application or other software written for system  on a platform with a different configuration that emulates system  or is otherwise compatible with it. If the other platform can successfully emulate, simulate and\/or provide some or all of the hardware and software resources of system , then the other platform will be able to successfully execute the software.","As one example, an emulator may provide a hardware and\/or software configuration (platform) that is different from the hardware and\/or software configuration (platform) of system . The emulator system might include software and\/or hardware components that emulate or simulate some or all of hardware and\/or software components of the system for which the application software was written. For example, the emulator system could comprise a general purpose digital computer such as a personal computer, which executes a software emulator program that simulates the hardware and\/or firmware of system .","Some general purpose digital computers (e.g., IBM or MacIntosh personal computers and compatibles) are now equipped with 3D graphics cards that provide 3D graphics pipelines compliant with DirectX or other standard 3D graphics command APIs. They may also be equipped with stereophonic sound cards that provide high quality stereophonic sound based on a standard set of sound commands. Such multimedia-hardware-equipped personal computers running emulator software may have sufficient performance to approximate the graphics and sound performance of system . Emulator software controls the hardware resources on the personal computer platform to simulate the processing, 3D graphics, sound, peripheral and other capabilities of the home video game console platform for which the game programmer wrote the game software.",{"@attributes":{"id":"p-0080","num":"0152"},"figref":"FIG. 9","b":["1201","1303","62","1201","1303","1201","62","1201","1303","50","62","1201"]},"As one example, in the case where the software is written for execution on a platform using an IBM PowerPC or other specific processor and the host  is a personal computer using a different (e.g. Intel) processor, emulator  fetches one or a sequence of binary-image program instructions from storage medium  and converts these program instructions to one or more equivalent Intel binary-image program instructions. The emulator  also fetches and\/or generates graphics commands and audio commands intended for processing by the graphics and audio processor , and converts these commands into a format or formats that can be processed by hardware and\/or software graphics and audio processing resources available on host . As one example, emulator  may convert these commands into commands that can be processed by specific graphics and\/or or sound hardware of the host  (e.g., using standard DirectX, OpenGL and\/or sound APIs).","An emulator  used to provide some or all of the features of the video game system described above may also be provided with a graphic user interface (GUI) that simplifies or automates the selection of various options and screen modes for games run using the emulator. In one example, such an emulator  may further include enhanced functionality as compared with the host platform for which the software was originally intended. In the case where particular graphics support hardware within an emulator does not include the embossed bump mapping functions shown in , the emulator designer has a choice of either:\n\n","While the  flowchart can be implemented entirely in software, entirely in hardware or by a combination of hardware and software, the preferred embodiment performs most of these calculations in hardware to obtain increased speed performance and other advantages. Nevertheless, in other implementations (e.g., where a very fast processor is available), the computations and steps of  may be implemented in software to provide similar or identical imaging results.",{"@attributes":{"id":"p-0084","num":"0159"},"figref":"FIG. 10","b":["1201","1303","1201","1203","1205","1207","1205","1203","1207","1207","1252","1254","1256","1201","1252","1201","1209","1211","1213","1215","1217","1219","1209","1217","1207","1221","1225","1201"]},"A number of program modules including emulator  may be stored on the hard disk , removable magnetic disk , optical disk  and\/or the ROM  and\/or the RAM  of system memory . Such program modules may include an operating system providing graphics and sound APIs, one or more application programs, other program modules, program data and game data. A user may enter commands and information into personal computer system  through input devices such as a keyboard , pointing device , microphones, joysticks, game controllers, satellite dishes, scanners, or the like. These and other input devices can be connected to processing unit  through a serial port interface  that is coupled to system bus , but may be connected by other interfaces, such as a parallel port, game port Fire wire bus or a universal serial bus (USB). A monitor  or other type of display device is also connected to system bus  via an interface, such as a video adapter .","System  may also include a modem  or other network interface means for establishing communications over a network  such as the Internet. Modem , which may be internal or external, is connected to system bus  via serial port interface . A network interface  may also be provided for allowing system  to communicate with a remote computing device  (e.g., another system ) via a local area network  (or such communication may be via wide area network  or other communications path such as dial-up or other communications means). System  will typically include other peripheral output devices, such as printers and other standard peripheral devices.","In one example, video adapter  may include a 3D graphics pipeline chip set providing fast 3D graphics rendering in response to 3D graphics commands issued based on a standard 3D graphics application programmer interface such as Microsoft's DirectX 7.0 or other version. A set of stereo loudspeakers  is also connected to system bus  via a sound generating interface such as a conventional \u201csound card\u201d providing hardware and embedded software support for generating high quality stereophonic sound based on sound commands provided by bus . These hardware capabilities allow system  to provide sufficient graphics and sound speed performance to play software stored in storage medium .","All documents referenced above are hereby incorporated by reference.","While the invention has been described in connection with what is presently considered to be the most practical and preferred embodiment, it is to be understood that the invention is not to be limited to the disclosed embodiment, but on the contrary, is intended to cover various modifications and equivalent arrangements included within the scope of the appended claims."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["These and other features and advantages provided by the invention will be better and more completely understood by referring to the following detailed description of presently preferred embodiments in conjunction with the drawings, of which:",{"@attributes":{"id":"p-0016","num":"0044"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0017","num":"0045"},"figref":["FIG. 2","FIG. 1"]},{"@attributes":{"id":"p-0018","num":"0046"},"figref":["FIG. 3","FIG. 2"]},{"@attributes":{"id":"p-0019","num":"0047"},"figref":["FIG. 4","FIG. 3"]},{"@attributes":{"id":"p-0020","num":"0048"},"figref":["FIG. 5","FIG. 4"]},{"@attributes":{"id":"p-0021","num":"0049"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0022","num":"0050"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0023","num":"0051"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0024","num":"0052"},"figref":"FIGS. 9 and 10"}]},"DETDESC":[{},{}]}
