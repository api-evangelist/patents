---
title: Training an automatic speech recognition system using compressed word frequencies
abstract: Respective word frequencies may be determined from a corpus of utterance-to-text-string mappings that contain associations between audio utterances and a respective text string transcription of each audio utterance. Respective compressed word frequencies may be obtained based on the respective word frequencies such that the distribution of the respective compressed word frequencies has a lower variance than the distribution of the respective word frequencies. Sample utterance-to-text-string mappings may be selected from the corpus of utterance-to-text-string mappings based on the compressed word frequencies. An automatic speech recognition (ASR) system may be trained with the sample utterance-to-text-string mappings.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09123331&OS=09123331&RS=09123331
owner: Google Inc.
number: 09123331
owner_city: Mountain View
owner_country: US
publication_date: 20130815
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATION","BACKGROUND","SUMMARY","DETAILED DESCRIPTION","1. Overview","2. Communication System and Device Architecture","3. Example Automatic Speech Recognition System Training","4. Example Automatic Speech Recognition System Operation","5. Example Embodiments to Compress Word Frequencies","6. Example Operations","7. Conclusion"],"p":["The present application claims priority to U.S. patent application Ser. No. 13\/666,223, filed on Nov. 1, 2012, and herein incorporated by reference in its entirety. U.S. patent application Ser. No. 13\/666,223, in turn, claims priority to U.S. provisional patent application No. 61\/604,849, filed on Feb. 29, 2012, and herein incorporated by reference in its entirety.","A goal of automatic speech recognition (ASR) technology may be to map a particular audio utterance to an accurate textual representation of that utterance. For instance, ASR performed on the utterance \u201ccat and dog\u201d would ideally be mapped to the text string \u201ccat and dog,\u201d rather than the nonsensical text string \u201cskate and hog,\u201d or the sensible but inaccurate text string \u201cKate and Doug.\u201d ASR systems can be trained based on a large corpus of utterance-to-text-string mappings. However, ASR system performance may vary based on the characteristics of this corpus.","In a first example embodiment, respective word frequencies may be obtained from a corpus of utterance-to-text-string mappings. The corpus of utterance-to-text-string mappings may contain associations between audio utterances and respective text string transcriptions of the audio utterances. The respective word frequencies may be based on occurrences of words in the text string transcriptions. Respective compressed word frequencies may be determined based on the respective word frequencies. For instance, a first distribution of the respective word frequencies may have a higher variance than a second distribution of the respective compressed word frequencies. Sample utterance-to-text-string mappings may be selected from the corpus of utterance-to-text-string mappings based on the compressed word frequencies, and an ASR system may be trained with the sample utterance-to-text-string mappings.","A second example embodiment may include a non-transitory computer-readable storage medium, having stored thereon program instructions that, upon execution by a computing device, cause the computing device to perform operations in accordance with the first example embodiment.","A third example embodiment may include a computing system comprising at least one processor, data storage, and program instructions stored in the data storage that, upon execution by the at least one processor, cause the computing system to operate in accordance with the first example embodiment.","These as well as other aspects, advantages, and alternatives will become apparent to those of ordinary skill in the art by reading the following detailed description with reference where appropriate to the accompanying drawings. Further, it should be understood that the description provided in this summary section and elsewhere in this document is intended to illustrate the claimed subject matter by way of example and not by way of limitation.","A corpus of utterance-to-text-string mappings may be used to train an ASR system. In particular, these mappings may contain a quantity of audio utterances (e.g., audio files of human speech). In some implementations, there may be thousands, tens of thousands, hundreds of thousands, tens of millions, or more audio utterances. The mappings may associate each audio utterance with a manually-transcribed and\/or automatically-transcribed text string of that audio utterance. Possibly using various mathematical and\/or machine learning techniques, the ASR system may use the corpus to develop probabilistic mappings between sounds and phonemes, phoneme patterns and words, and\/or words and word frequencies. These probabilistic mappings may be represented as a search graph.","In some embodiments, the ASR system may be trained offline, prior to becoming operational. For example, training an ASR system with a large corpus may take several hours or days. On the other hand, the ASR system may instead be trained online. In the latter case, the ASR system may be dynamically updated while operating.","The accuracy of the ASR system may depend, to some extent, on the size and\/or quality of the data in the corpus. For example, words and phrases that appear more frequently in the corpus's audio utterances are more likely to be properly recognized by the ASR system, because the ASR system may have a greater opportunity to learn the proper mappings of these utterances to text strings. Conversely, words and phrases that appear less frequently in the audio utterances are less likely to be properly recognized by the ASR system, because the ASR system may have a lesser opportunity to learn the proper text string mappings.","Modern ASR systems are often trained with utterances from actual text-to-speech computer applications, perhaps used together with sound-to-text mappings generated by the ASR system. Thus, in some corpora, the utterances include actual human voice signals made during the use of, e.g., voice search and voice messaging applications.","However, the distribution of word frequencies is highly skewed (e.g., heavy-tailed) in many human languages. For example, empirical studies have shown that word frequencies of American English roughly follow Zipf's Law, in that the frequency of any word is approximately inversely proportional to its rank in frequency. Thus, the most frequent word will occur about twice as often as the second most frequent word, three times as often as the third most frequent word, and so on. Therefore, in a corpus of American English utterances, one might expect the most common word, \u201cthe\u201d to be approximately 7% of all words, the second most common word, \u201cof\u201d to be approximately 3.5% of all words, the third most common word, \u201cand\u201d to be approximately 2.3% of all words, and so on.","Given that an ASR system can be trained by actual human speech and that this speech is likely to exhibit a Zipf distribution (or a distribution proximate thereto), the ASR system may become inherently biased against properly recognizing less common words and phrases. In the discussion below, various embodiments are presented that may compensate for this bias. Particularly, by training an ASR system with an increased amount of less common words and phrases and a decreased amount of more common words and phrases, the effect of this bias may be mitigated.","ASR systems have been deployed in various environments. Some ASR systems are just a single machine (e.g., a personal computer) into which a user speaks utterances and the ASR system transcribes the utterances into one or more text strings. Other ASR systems are client\/server based, in which the user speaks an utterance into a client device, and the client device may encode the utterance and transmit it to a server device. Then, the server device performs speech recognition on the encoded utterance and transmits one or more text string mappings to the client device for presentation to the user. Particularly, on wireless communication devices such as mobile phones, client\/server based ASR can be supported by Internet search applications, geo-location and mapping applications, text messaging and instant messaging applications, and by virtually any third-party application as well.","The server component of an ASR system may include just a single server device, or may be distributed in various ways across a number of server devices. The following section describes example client and server device(s) and an example communication system that could be used for client\/server communication and processing by an ASR system.","The methods, devices, and systems described herein can be implemented using client devices and\/or so-called \u201ccloud-based\u201d server devices. Under various aspects of this paradigm, client devices, such as mobile phones and tablet computers, may offload some processing and storage responsibilities to remote server devices. At least some of the time, these client services are able to communicate, via a network such as the Internet, with the server devices. As a result, applications that operate on the client devices may also have a persistent, server-based component. Nonetheless, it should be noted that at least some of the methods, processes, and techniques disclosed herein may be able to operate entirely on a client device or a server device.","Furthermore, the \u201cserver devices\u201d described herein may not necessarily be associated with a client\/server architecture, and therefore may be interchangeably referred to as \u201ccomputing devices.\u201d Similarly, the \u201cclient devices\u201d described herein also may not necessarily be associated with a client\/server architecture, and therefore may be interchangeably referred to as \u201cuser devices.\u201d","This section describes general system and device architectures for such client devices and server devices. However, the methods, devices, and systems presented in the subsequent sections may operate under different paradigms as well. Thus, the embodiments of this section are merely examples of how these methods, devices, and systems can be enabled.","A. Communication System",{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 1","b":["100","100","102","104","106","108"]},"Network  may be, for example, the Internet, or some other form of public or private Internet Protocol (IP) network. Thus, client devices , , and  may communicate using packet-switching technologies. Nonetheless, network  may also incorporate at least some circuit-switching technologies, and client devices , , and  may communicate via circuit switching alternatively or in addition to packet switching.","A server device  may also communicate via network . Particularly, server device  may communicate with client devices , , and  according to one or more network protocols and\/or application-level protocols to facilitate the use of network-based or cloud-based computing on these client devices. Server device  may include integrated data storage (e.g., memory, disk drives, etc.) and may also be able to access a separate server data storage . Communication between server device  and server data storage  may be direct, via network , or both direct and via network  as illustrated in . Server data storage  may store application data that is used to facilitate the operations of applications performed by client devices , , and  and server device .","Although only three client devices, one server device, and one server data storage are shown in , communication system  may include any number of each of these components. For instance, communication system  may comprise millions of client devices, thousands of server devices and\/or thousands of server data storages. Furthermore, client devices may take on forms other than those in .","B. Server Device",{"@attributes":{"id":"p-0036","num":"0035"},"figref":["FIG. 2A","FIG. 2A"],"b":["200","110","112","200","202","204","206","208","214"]},"User interface  may comprise user input devices such as a keyboard, a keypad, a touch screen, a computer mouse, a track ball, a joystick, and\/or other similar devices, now known or later developed. User interface  may also comprise user display devices, such as one or more cathode ray tubes (CRT), liquid crystal displays (LCD), light emitting diodes (LEDs), displays using digital light processing (DLP) technology, printers, light bulbs, and\/or other similar devices, now known or later developed. Additionally, user interface  may be configured to generate audible output(s), via a speaker, speaker jack, audio output port, audio output device, earphones, and\/or other similar devices, now known or later developed. In some embodiments, user interface  may include software, circuitry, or another form of logic that can transmit data to and\/or receive data from external user input\/output devices.","Communication interface  may include one or more wireless interfaces and\/or wireline interfaces that are configurable to communicate via a network, such as network  shown in . The wireless interfaces, if present, may include one or more wireless transceivers, such as a BLUETOOTH\u00ae transceiver, a Wifi transceiver perhaps operating in accordance with an IEEE 802.11 standard (e.g., 802.11b, 802.11g, 802.11n), a WiMAX transceiver perhaps operating in accordance with an IEEE 802.16 standard, a Long-Term Evolution (LTE) transceiver perhaps operating in accordance with a 3rd Generation Partnership Project (3GPP) standard, and\/or other types of wireless transceivers configurable to communicate via local-area or wide-area wireless networks. The wireline interfaces, if present, may include one or more wireline transceivers, such as an Ethernet transceiver, a Universal Serial Bus (USB) transceiver, or similar transceiver configurable to communicate via a twisted pair wire, a coaxial cable, a fiber-optic link or other physical connection to a wireline device or network.","Processor  may include one or more general purpose processors (e.g., microprocessors) and\/or one or more special purpose processors (e.g., digital signal processors (DSPs), graphical processing units (GPUs), floating point processing units (FPUs), network processors, or application specific integrated circuits (ASICs)). Processor  may be configured to execute computer-readable program instructions  that are contained in data storage , and\/or other instructions, to carry out various functions described herein.","Thus, data storage  may include one or more non-transitory computer-readable storage media that can be read or accessed by processor . The one or more computer-readable storage media may include volatile and\/or non-volatile storage components, such as optical, magnetic, organic or other memory or disc storage, which can be integrated in whole or in part with processor . In some embodiments, data storage  may be implemented using a single physical device (e.g., one optical, magnetic, organic or other memory or disc storage unit), while in other embodiments, data storage  may be implemented using two or more physical devices.","Data storage  may also include program data  that can be used by processor  to carry out functions described herein. In some embodiments, data storage  may include, or have access to, additional data storage components or devices (e.g., cluster data storages described below).","C. Server Clusters","Server device  and server data storage device  may store applications and application data at one or more places accessible via network . These places may be data centers containing numerous servers and storage devices. The exact physical location, connectivity, and configuration of server device  and server data storage device  may be unknown and\/or unimportant to client devices. Accordingly, server device  and server data storage device  may be referred to as \u201ccloud-based\u201d devices that are housed at various remote locations. One possible advantage of such \u201ccould-based\u201d computing is to offload processing and data storage from client devices, thereby simplifying the design and requirements of these client devices.","In some embodiments, server device  and server data storage device  may be a single computing device residing in a single data center. In other embodiments, server device  and server data storage device  may include multiple computing devices in a data center, or even multiple computing devices in multiple data centers, where the data centers are located in diverse geographic locations. For example,  depicts each of server device  and server data storage device  potentially residing in a different physical location.",{"@attributes":{"id":"p-0045","num":"0044"},"figref":["FIG. 2B","FIG. 2B"],"b":["110","112","220","220","220","220","200","222","224","226","220","200","222","224","226","220","200","222","224","226","220","220","220","108","228","228","228"]},"In some embodiments, each of the server clusters A, B, and C may have an equal number of server devices, an equal number of cluster data storages, and an equal number of cluster routers. In other embodiments, however, some or all of the server clusters A, B, and C may have different numbers of server devices, different numbers of cluster data storages, and\/or different numbers of cluster routers. The number of server devices, cluster data storages, and cluster routers in each server cluster may depend on the computing task(s) and\/or applications assigned to each server cluster.","In the server cluster A, for example, server devices A can be configured to perform various computing tasks of server device . In one embodiment, these computing tasks can be distributed among one or more of server devices A. Server devices B and C in server clusters B and C may be configured the same or similarly to server devices A in server cluster A. On the other hand, in some embodiments, server devices A, B, and C each may be configured to perform different functions. For example, server devices A may be configured to perform one or more functions of server device , and server devices B and server device C may be configured to perform functions of one or more other server devices. Similarly, the functions of server data storage device  can be dedicated to a single server cluster, or spread across multiple server clusters.","Cluster data storages A, B, and C of the server clusters A, B, and C, respectively, may be data storage arrays that include disk array controllers configured to manage read and write access to groups of hard disk drives. The disk array controllers, alone or in conjunction with their respective server devices, may also be configured to manage backup or redundant copies of the data stored in cluster data storages to protect against disk drive failures or other types of failures that prevent one or more server devices from accessing one or more cluster data storages.","Similar to the manner in which the functions of server device  and server data storage device  can be distributed across server clusters A, B, and C, various active portions and\/or backup\/redundant portions of these components can be distributed across cluster data storages A, B, and C. For example, some cluster data storages A, B, and C may be configured to store backup versions of data stored in other cluster data storages A, B, and C.","Cluster routers A, B, and C in server clusters A, B, and C, respectively, may include networking equipment configured to provide internal and external communications for the server clusters. For example, cluster routers A in server cluster A may include one or more packet-switching and\/or routing devices configured to provide (i) network communications between server devices A and cluster data storage A via cluster network A, and\/or (ii) network communications between the server cluster A and other devices via communication link A to network . Cluster routers B and C may include network equipment similar to cluster routers A, and cluster routers B and C may perform networking functions for server clusters B and C that cluster routers A perform for server cluster A.","Additionally, the configuration of cluster routers A, B, and C can be based at least in part on the data communication requirements of the server devices and cluster storage arrays, the data communications capabilities of the network equipment in the cluster routers A, B, and C, the latency and throughput of the local cluster networks A, B, C, the latency, throughput, and cost of the wide area network connections A, B, and C, and\/or other factors that may contribute to the cost, speed, fault-tolerance, resiliency, efficiency and\/or other design goals of the system architecture.","D. Client Device",{"@attributes":{"id":"p-0053","num":"0052"},"figref":"FIG. 3","b":["300","300"]},"As shown in , client device  may include a communication interface , a user interface , a processor , and data storage , all of which may be communicatively linked together by a system bus, network, or other connection mechanism .","Communication interface  functions to allow client device  to communicate, using analog or digital modulation, with other devices, access networks, and\/or transport networks. Thus, communication interface  may facilitate circuit-switched and\/or packet-switched communication, such as POTS communication and\/or IP or other packetized communication. For instance, communication interface  may include a chipset and antenna arranged for wireless communication with a radio access network or an access point. Also, communication interface  may take the form of a wireline interface, such as an Ethernet, Token Ring, or USB port. Communication interface  may also take the form of a wireless interface, such as a Wifi, BLUETOOTH\u00ae, global positioning system (GPS), or wide-area wireless interface (e.g., WiMAX or LTE). However, other forms of physical layer interfaces and other types of standard or proprietary communication protocols may be used over communication interface . Furthermore, communication interface  may comprise multiple physical communication interfaces (e.g., a Wifi interface, a BLUETOOTH\u00ae interface, and a wide-area wireless interface).","User interface  may function to allow client device  to interact with a human or non-human user, such as to receive input from a user and to provide output to the user. Thus, user interface  may include input components such as a keypad, keyboard, touch-sensitive or presence-sensitive panel, computer mouse, trackball, joystick, microphone, still camera and\/or video camera. User interface  may also include one or more output components such as a display screen (which, for example, may be combined with a presence-sensitive panel), CRT, LCD, LED, a display using DLP technology, printer, light bulb, and\/or other similar devices, now known or later developed. User interface  may also be configured to generate audible output(s), via a speaker, speaker jack, audio output port, audio output device, earphones, and\/or other similar devices, now known or later developed. In some embodiments, user interface  may include software, circuitry, or another form of logic that can transmit data to and\/or receive data from external user input\/output devices. Additionally or alternatively, client device  may support remote access from another device, via communication interface  or via another physical interface (not shown).","Processor  may comprise one or more general purpose processors (e.g., microprocessors) and\/or one or more special purpose processors (e.g., DSPs, GPUs, FPUs, network processors, or ASICs). Data storage  may include one or more volatile and\/or non-volatile storage components, such as magnetic, optical, flash, or organic storage, and may be integrated in whole or in part with processor . Data storage  may include removable and\/or non-removable components.","Generally speaking, processor  may be capable of executing program instructions  (e.g., compiled or non-compiled program logic and\/or machine code) stored in data storage  to carry out the various functions described herein. Therefore, data storage  may include a non-transitory computer-readable medium, having stored thereon program instructions that, upon execution by client device , cause client device  to carry out any of the methods, processes, or functions disclosed in this specification and\/or the accompanying drawings. The execution of program instructions  by processor  may result in processor  using data .","By way of example, program instructions  may include an operating system  (e.g., an operating system kernel, device driver(s), and\/or other modules) and one or more application programs  (e.g., address book, email, web browsing, social networking, and\/or gaming applications) installed on client device . Similarly, data  may include operating system data  and application data . Operating system data  may be accessible primarily to operating system , and application data  may be accessible primarily to one or more of application programs . Application data  may be arranged in a file system that is visible to or hidden from a user of client device .","Application programs  may communicate with operating system  through one or more application programming interfaces (APIs). These APIs may facilitate, for instance, application programs  reading and\/or writing application data , transmitting or receiving information via communication interface , receiving or displaying information on user interface , and so on.","In some vernaculars, application programs  may be referred to as \u201capps\u201d for short. Additionally, application programs  may be downloadable to client device  through one or more online application stores or application markets. However, application programs can also be installed on client device  in other ways, such as via a web browser or through a physical interface (e.g., a USB port) on client device .","Before describing ASR system training in detail, it may be beneficial to understand overall ASR system operation. Thus, this section describes ASR systems in general, including how the language model can interact with other logical components of an ASR system in order to facilitate speech recognition.",{"@attributes":{"id":"p-0063","num":"0062"},"figref":"FIG. 4","b":"400"},"It should be noted that the discussion in this section, and the accompanying figures, are presented for purposes of example. Other methods of training an ASR system, including different modules, different configurations of modules, and\/or different training steps, may be possible.","A. Acoustic Model","A phoneme may be considered to be the smallest segment of an utterance that encompasses a meaningful contrast with other segments of utterances. Thus, a word typically includes one or more phonemes. For purposes of simplicity, phonemes may be thought of as utterances of letters, but this is not a perfect analogy, as some phonemes may present multiple letters. An example phonemic spelling for the American English pronunciation of the word \u201ccat\u201d is \u201ckaet,\u201d consisting of the phonemes \u201ck,\u201d \u201cae,\u201d and \u201ct.\u201d Another example phonemic spelling is \u201cd aw g,\u201d consisting of the phonemes \u201cd,\u201d \u201caw,\u201d and \u201cg.\u201d","Different phonemic alphabets exist, and these alphabets may have different textual representations for the various phonemes therein. For example, the letter \u201ca\u201d may be represented by the phoneme \u201cae\u201d when used to make the \u201ca\u201d sound in \u201ccat,\u201d by the phoneme \u201cey\u201d when used to make the \u201ca\u201d sound in \u201cate,\u201d and by the phoneme \u201cah\u201d when used to make the \u201ca\u201d sound in \u201cbeta.\u201d Other phonemic representations are possible.","Common phonemic alphabets for American English contain about 40 distinct phonemes. Each of these phonemes may be associated with a different set of nominal output vector values. Thus, acoustic model  may be able to estimate the phoneme in the sample by analyzing the sample in the time and\/or frequency domains, and finding the phoneme with nominal output vector values (e.g., frequency characteristics) that best match the output vector values of the sample. Or, put another way, acoustic model  can be used to provide scores every s milliseconds that describe how well the current sound in an utterance matches some or all possible context dependent phonemic sounds.","This process is illustrated in . For the input utterance \u201ccat and dog\u201d , acoustic model  may phonemically interpret this utterance as \u201ck ae t ae n d d aw g\u201d .  assumes that the input utterance is clean and that acoustic model  is well-trained. In some environments, the input utterance may be distorted by background noise, clipping, or some other form of interference. Also, for some input utterances, particularly those with uncommon words or words spoken with an unknown accent, acoustic model  may incorrectly evaluate the input utterance.","One way of implementing an acoustic model, such as acoustic model , is by using a hidden Markov model (HMM). Some HMM-based acoustic models may also consider context when performing this mapping. For example, acoustic model  may consider the phoneme that precedes the current sample to provide a better estimate of the phoneme represented by the current sample. The use of context in this fashion can account for certain phoneme combinations (e.g., \u201caet\u201d) being more common than other phoneme combinations (e.g., \u201ctk\u201d). But, HMMs are just one technology that can be employed to develop an acoustic model, and acoustic model  can be based on technology other than HMMs.","Furthermore, acoustic model  may operate based on syllables or a segment of language other than context-dependent phonemic sounds. For instance, acoustic model  may interpret a series of phonemes as syllables, or as one or more words. For purposes of simplicity, throughout this specification and the accompanying drawings, it is assumed that acoustic models represent one or more phonemes as context-dependent phonemic sounds. However, acoustic models that use other types of representations are within the scope of the embodiments herein.","B. Dictionary","Once one or more phonemes are interpreted from an input utterance, dictionary  may be used to determine a pre-established mapping (e.g., from a list of tens or hundreds of thousands of phoneme pattern to word mappings) of these phonemes into words. This process is illustrated by . For the input phonemic interpretation \u201ck ae t ae n d d aw g\u201d , dictionary  provides a mapping to the text string \u201ccat and dog.\u201d",{"@attributes":{"id":"p-0074","num":"0073"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"112pt","align":"left"}}],"thead":{"row":[{"entry":[{},"TABLE 1"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]},{"entry":[{},"Word","Phonemic Interpretation"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"cat","k ae t"]},{"entry":[{},"and","ay n d"]},{"entry":[{},"dog","d aw g"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}}},"In some embodiments, dictionary  may include a lookup table, such as Table 1. Table 1 illustrates how dictionary  may list the phonemic sequences that search graph  uses for the words that the ASR system is attempting to recognize.","C. Language Model","Turning back to , one output of the ASR system training process may be language model . Language model  may define the conditional probability of w(the nth word in a phrase transcribed from an utterance), given the values of the pattern of n\u22121 previous words in the phrase. More formally, language model  may define\n\n()\n\nIn general, a language model may operate on n-grams, which, for example, may be sequences of n words that were recognized from the utterances in corpus , via acoustic model  and dictionary . Alternatively or additionally, the n-grams may be derived from a corpus of phrases and sentences written in a target language.\n","In some embodiments, a language model may operate on a sequence of n phonemes, syllables, words, or series of words. In practice, language models with values of n greater than 5 are rarely used, because of their computational complexity, and also because smaller n-grams (e.g., 3-grams, which are also referred to as tri-grams) tend to yield acceptable results. In the example described below, tri-grams are used for purposes of illustration. Nonetheless, any value of n may be may be used with the embodiments herein.","Thus, through analysis of the corpus , tri-gram probabilities can be estimated based on their respective number of appearances in the training corpus. In other words, if C(w, w, w) is the number of occurrences of the word pattern w, w, win corpus , then",{"@attributes":{"id":"p-0080","num":"0079"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":[{"mi":"w","mn":"3"},{"mi":"w","mn":"1"}],"mo":"|"},"mo":",","msub":{"mi":"w","mn":"2"}}}},"mo":"\u2248","mfrac":{"mrow":[{"mi":"C","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"w","mn":"1"},{"mi":"w","mn":"2"},{"mi":"w","mn":"3"}],"mo":[",",","]}}},{"mi":"C","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"w","mn":"1"},{"mi":"w","mn":"2"}],"mo":","}}}]}}}}},"Therefore, a language model may be represented as a table of conditional probabilities. Table 2 illustrates a simple example of such a table that could form the basis of language model . Particularly, Table 2 contains tri-gram conditional probabilities.",{"@attributes":{"id":"p-0082","num":"0081"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":"TABLE 2"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Tri-gram Conditional Probabilities"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"119pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},"P(dog|cat,and) = 0.5",{}]},{"entry":[{},{},"P(mouse|cat,and) = 0.35",{}]},{"entry":[{},{},"P(bird|cat,and) = 0.14",{}]},{"entry":[{},{},"P(fiddle|cat,and) = 0.01"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]}}]}}},"For the 2-gram prefix \u201ccat and,\u201d Table 2 indicates that, based on the observed occurrences in corpus , 50% of the time the next 1-gram is \u201cdog.\u201d Likewise, 35% of the time, the next 1-gram is \u201cmouse,\u201d 14% of the time the next 1-gram is \u201cbird,\u201d and 1% of the time the next 1-gram is \u201cfiddle.\u201d Clearly, in a fully-trained ASR system, the language model would contain many more entries, and these entries would include more than just one 2-gram prefix.","Nonetheless, using the observed frequencies of word patterns from a corpus of speech (and\/or from other sources) is not perfect, as some acceptable tri-grams may not appear in the training corpus, and may therefore be assigned a probability of zero. Consequently, when given a zero-probability tri-gram at run time, the language model may instead attempt to map this tri-gram to a different tri-gram associated with a non-zero probability.","In order to reduce this likelihood, the language model may be smoothed so that zero-probability tri-grams have small non-zero probabilities, and the probabilities of the tri-grams in the training corpus are reduced accordingly. In this way, tri-grams not found in the training corpus can still be recognized by the language model.","D. Search Graph","Another possible output from the ASR training process illustrated in  is a search graph, such as search graph . A search graph may be a data structure that represents the totality (or a large part of) the speech patterns of an input corpus, and may serve to enable rapid recognition of new input utterances in an operational ASR system. Thus, search graph  may be based on output from acoustic model , dictionary , and language model .",{"@attributes":{"id":"p-0088","num":"0087"},"figref":"FIG. 6","b":["408","408","600"]},"Each circle in search graph  may represent a state associated with the processing of an input utterance that has been mapped to phonemes. These states are named based on the current phoneme context of the input utterance, using the format \u201cx[y]z\u201d to indicate that the current phoneme being considered, y, has a left-context of the phoneme x and a right context of the phoneme z. In other words, the state \u201cx[y]z\u201d indicates a point in processing an utterance in which the current phoneme being considered is y, the previously phoneme in the utterance is x, and the next phoneme in the utterance is z. The beginning of an utterance and the end of an utterance are represented by the \u201c#\u201d character, and also may be referred to as null phonemes.","Terminal states may be represented by a recognized word or phrase in quotes. Search graph  includes five terminal states, representing recognition of the words or phrases \u201ccatapult,\u201d \u201ccat and mouse,\u201d \u201ccat and dog,\u201d \u201ccat,\u201d and \u201ccap.\u201d","Transitions from one state to another may represent an observed ordering of phonemes in the corpus. For instance, the state \u201c#[k]ae\u201d represents the recognition of a \u201ck\u201d phoneme with a left context of a null phoneme and a right context of an \u201cae\u201d phoneme. There are two transitions from the state \u201c#[k]ae\u201d\u2014one for which the next phoneme (the phoneme after the \u201cae\u201d) is a \u201ct\u201d and another for which the next phoneme is a \u201cp.\u201d","Based on acoustic model , dictionary , and language model , costs may be assigned to one or more of the states and\/or transitions. For example, if a particular phoneme pattern is rare, a transition to a state representing that phoneme pattern may have a higher cost than a transition to a state representing a more common phoneme pattern. Similarly, the conditional probabilities from the language model (see Table 2 for examples) may also be used to assign costs to states and\/or transitions. For instance, in Table 2, given a phrase with the words \u201ccat and,\u201d the conditional probability of the next word in the phrase being \u201cdog\u201d is 0.5, while the conditional probability of the next word in the phrase being \u201cmouse\u201d is 0.35. Therefore, the transition from state \u201cae[n]d\u201d to state \u201cn[d]m\u201d may have a higher cost than the transition from state \u201cae[n]d\u201d to state \u201cn[d]d.\u201d","Once an ASR system is trained, search graph , possibly including any states, transitions between states, and associated costs therein, may be used to estimate text string transcriptions for new input utterances. The next section describes ASR system operation in more detail.","An illustrative model of an operational ASR system is shown in . Example ASR system  may include representations of acoustic model , dictionary , language model , and search graph . Alternatively, ASR system  may omit one or more of these modules. For example, characteristics of dictionary  and language model  may be incorporated into the structure of search graph , and therefore may not be necessary in ASR system .","Input to ASR system  may be an input utterance, such as a word, a phrase, a sentence, or a series of sentences. The input utterance may take the form of an analog or digital audio signal. Output from ASR system  may be one or more text strings that the ASR system has transcribed based on the input utterance. While ASR system  may seek to produce accurate text string transcriptions of input utterances, this may not always be possible. Thus, for some input utterances, ASR system  may produce more than one possible text string transcription that could match the input utterance. For instance, ASR system  may estimate the N-best transcriptions of an input utterance, and output one or more of these transcriptions.","Additionally,  shows search module  being coupled with search graph , and search graph  being coupled with acoustic model , dictionary , and language model . However, other arrangements are possible. For instance, search module  may interact directly with acoustic model  and\/or dictionary .","Search module  may be used to determine a sequence of one or more words that matches an input utterance. Formally, search module  may attempt to find\n\n*=argmax()()\n\nwhere a is a stream of feature vectors derived from the input utterance, P(a|w) represents the probability of those feature vectors being produced by a word sequence w, and P(w) is the probability assigned to w by language model . For example, P(w) may be based on n-gram conditional probabilities as discussed above, as well as other factors. The function argmaxmay return the value of w that maximizes P(a|w)P(w).\n","Particularly, as part of the process of transcribing the input utterance to one or more text strings, search module  may apply acoustic model  to the input utterance. The result of this step may be a sequence of phonemes. Then, the sequence may serve as input to search graph . In some embodiments, search module  may attempt to find paths from an initial state in search graph  to a terminal state in search graph  based on this sequence. This process may involve search module  performing a breadth-first search, depth-first search, beam search, or some other type of search. Search module  may assign a total cost to one or more paths based on costs associated with the states and\/or transitions of each path. Some of these costs may reflect, for instance, a confidence level that a particular segment of the utterance maps to a particular phoneme context in the path.","As an example, suppose that the input utterance is the phrase \u201ccat and dog.\u201d Referring back to , in a possible scenario, search module  would step through search graph  phoneme by phoneme and find the path beginning with initial state \u201c#[k]ae\u201d and ending with terminal state \u201ccat and dog.\u201d Search module  may also find one or more additional paths through search graph . For example, search module  may also associate the input utterance with the path with initial state \u201c#[k]ae\u201d and ending with terminal state \u201ccat and mouse,\u201d and with the path with initial state \u201c#[k]ae\u201d and ending with terminal state \u201ccatapult.\u201d Nonetheless, search module  may assign a lower cost to the path with terminal state \u201ccat and dog\u201d than to each of the other paths. Consequently, the path with terminal state \u201ccat and dog\u201d may be selected as the \u201cbest\u201d transcription for the input utterance.","It should be understood that ASR systems can operated in many different ways. The embodiments described above are presented for purposes of illustration and may not be the only way in which an ASR system operates.","As noted above, in many human languages, the distributions of words used in speech follow Zipf's Law. Thus, for a given corpus of utterance-to-text-string mappings, the probabilities assigned to n-grams by an ASR system's language model may be in accordance with these distributions. Consequently, an ASR system is likely to misinterpret words that appear rarely in speech as more commonly-occurring words.","In some cases, the utterances in the training corpus may further bias the ASR system due to their source. For instance, the corpus may include voice-to-text utterances from text messaging and instant messaging applications. The distribution of words used for text messaging and instant messaging applications may differ from the distribution of words used in other contexts, such as human conversation. In particular, text messages and instant messages typically contain sentence fragments rather than whole sentences, and may include a disproportionate amount of slang.","Moreover, at least in American English, common usage results in the slurring of phonemes in various phrases. For example, the input utterance \u201ctalk to you later,\u201d may be pronounced as \u201ctalk tuh ya later.\u201d Thus, for example, the highest scoring (e.g., lowest cost) phoneme string may be: \u201ct ao k t ah y aa l ey t er.\u201d","However, due to the training process (and possibly the longer span constraints from the dictionary and the language model), an ASR system may learn to transcribe this phoneme string to the proper text string of \u201ctalk to you later.\u201d As a result, the acoustic model, which may use phoneme context to perform its interpretation, may learn that an \u201cuw\u201d phoneme with a left context of a \u201ct\u201d phoneme and a right context of a \u201cy\u201d phoneme sometimes sounds like an \u201cah\u201d phoneme.","This mapping can be problematic, as new utterances introduced to the ASR system during operation may be subject to the model of the \u201cuw\u201d phoneme being contaminated with the \u201cah\u201d sound in some contexts. This may cause problems in other contexts where that type of substitution (\u201cuw\u201d to \u201cah\u201d) is not common. For instance, the input utterance \u201cabout you\u201d might be interpreted as the phoneme string \u201caa b aw t ah y uw.\u201d (Here, the speaker has put an emphasis on the \u201ct\u201d in \u201cabout\u201d so that it sounds as if the \u201cah\u201d phoneme is appended). Since the \u201cah\u201d phoneme has a left context of a \u201ct\u201d phoneme and a right context of a \u201cy\u201d, the ASR system may map the phonemes in \u201cabout you\u201d to the words \u201ca bow to you.\u201d","Regardless, this is just one possible example of how common phrases and\/or high-frequency words can lead an ASR system to misinterpret input utterances. Many other examples are possible. The example techniques introduced below may reduce the likelihood of these (and possibly other) types of ASR system misinterpretations.",{"@attributes":{"id":"p-0107","num":"0106"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"6"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"56pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"49pt","align":"center"}}],"thead":{"row":[{"entry":[{},"TABLE 3"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"5","align":"center","rowsep":"1"}}]},{"entry":[{},{},{},"True word","Compressed ","Word "]},{"entry":[{},{},{},"frequency ","frequency","selection"]},{"entry":[{},"Rank","Word","in corpus","(0.5 power)","probability"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"5","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"6"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"56pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"42pt","align":"char","char":"."}},{"@attributes":{"colname":"5","colwidth":"49pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"1","the","56,265,001","7501","0.00013"]},{"entry":[{},"2","of","33,942,276","5826","0.00017"]},{"entry":[{},"3","and","29,942,784","5472","0.00018"]},{"entry":[{},"4","to","25,948,836","5094","0.00020"]},{"entry":[{},"5","in","17,413,929","4173","0.00024"]},{"entry":[{},". . . ",{},{},{},{}]},{"entry":[{},"100","own","921,600","960","0.00104"]},{"entry":[{},". . . ",{},{},{},{}]},{"entry":[{},"1046","dog","76176","276","0.00362"]},{"entry":[{},". . . ",{},{},{},{}]},{"entry":[{},"2567","cat","30,625","175","0.00571"]},{"entry":[{},". . . ",{},{},{},{}]},{"entry":[{},"20734 ","catapult","441","21","0.04762"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"5","align":"center","rowsep":"1"}}]}]}}]}}},"Table 3 contains an illustrative list of words from an example corpus. These words are ranked according to their true word frequencies. (The term \u201ctrue word frequency\u201d is used herein to refer to the frequency of words in the corpus used to train an ASR system, and to distinguish from the term \u201ccompressed word frequency\u201d that is discussed below.) The rank ordering indicates that the word \u201cthe\u201d has a rank of 1 and therefore is the most common, the word \u201cof\u201d has a rank of 2 and therefore is the second most common, and so on.","As noted earlier, true word frequencies typically follow Zipf's Law. Thus, in the example corpus, \u201cof\u201d appears about half as often as \u201cthe,\u201d and the third most common word, \u201cand\u201d appear about one-third as often as \u201cthe.\u201d (While the example word frequencies in Table 3 do not follow Zipf's Law exactly, they are close enough to be well-modeled with Zipf's Law.)","Further, and again in accordance with Zipf's Law, the true word frequency distribution in the example corpus is \u201chigh variance\u201d or \u201cheavy tailed,\u201d in that there are many low-frequency words. For instance, the word \u201ccatapult\u201d has a rank of 20734, but appears only 441 times in the example corpus. Since the word \u201ccat\u201d appears about 70 times more frequently than \u201ccatapult,\u201d the ASR system may be biased toward misinterpreting \u201ccatapult\u201d as \u201ccat,\u201d followed by one or more additional words, when \u201ccatapult\u201d appears in an input utterance.","One possible way of mitigating misinterpretations due to word frequency bias is to train the ASR system with input utterances that are more likely to contain low-frequency words. For example, Table 3 contains, for each word, a compressed frequency which is formed by taking the square root of the true word frequency. As can be observed from Table 3, and verified mathematically, the word distribution formed in accordance with the compressed word frequencies has a lower variance and is less heavy-tailed than the true word frequency distribution.","Compressed frequencies can be derived in other ways as well. For example, a compressed word frequency can be obtained by raising the true word frequency to a power between 0.0 and 1.0 (taking the square root of the true word frequency is equivalent to raising the true word frequency to a power of 0.5).","Generally speaking, the value of the power may be used as a knob to control the variance of the distribution of compressed word frequencies. The higher the power, the greater the variance of the distribution. For example, if the power is 1.0, the distribution of compressed word frequencies is the same as that of the high-variance true word frequencies. However, if the power is 0.5, as shown in Table 3, the distribution of compressed word frequencies has a lower variance than the distribution of true word frequencies. Further, if the power is 0.0, the distribution of compressed word frequencies is uniform, with a variance of 0 (i.e., the compressed word frequency for every word is 1).","A word selection probability may be derived by dividing the compressed word frequency by the true word frequency. Alternatively, the word selection probability may be derived in other ways. The word selection probability may be used to sample the training corpus. For instance, the word selection probability for a particular word represents the approximate likelihood that an input utterance containing the particular word in the training corpus will be sampled from the training corpus. With reference to Table 3, this means that any one of the 76176 instances of the word \u201cdog\u201d in the training corpus may be selected with a probability of 0.00362.","More specifically, for an input utterance containing n words, the input utterance may be selected from the training corpus based on the word selection probabilities of each of the n words. In some embodiments, the average (mean) of the word selection probabilities of n words may be calculated, and the input utterance may be sampled with a probability equivalent to this average. Alternatively, the geometric mean, harmonic mean, or some other measure of central tendency may be used. A geometric mean of n numbers, a, a. . . amay be calculated as",{"@attributes":{"id":"p-0116","num":"0115"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mroot":{"mrow":[{"munderover":{"mo":"\u220f","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["a","i"]}},{"mi":"n","mo":"\u2062","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}]},"mo":"."}}},"br":{}},{"@attributes":{"id":"p-0117","num":"0116"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mfrac":{"mi":"n","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mfrac":{"mn":"1","msub":{"mi":["a","i"]}}}},"mo":"."}}}},"Determining the selection probability of an utterance is illustrated in . Utterance  maps to text string \u201ccat and dog\u201d . For each word in text string \u201ccat and dog\u201d , a true word frequency and a compressed word frequency is determined, as shown in table . Then, the compressed word frequencies are divided by their respective true word frequencies, resulting in selection probabilities for each word. (The true word frequencies, compressed word frequencies, and selection probabilities for these example words are taken from Table 3.)","The geometric mean of the selection probabilities  is then calculated, resulting in the value 0.00155. This value may be used as the probability of selecting utterance  from the corpus of training data for the ASR system. For instance, a uniformly-distributed random number between 0 and 1 may be generated. If the random number is less than or equal to 0.00155, the utterance may be selected. If the random number is greater than 0.00155, the utterance may not be selected.","After a number of samples (e.g., thousands, tens of thousands, hundreds of thousands, or millions, etc.) are selected in this fashion from the corpus, the ASR system may be trained with these samples. This sample-based training may occur after the ASR system has already been trained with the full set of utterances from the corpus, or instead of training the ASR system with the full set of utterances from the corpus.","A possible result of training an ASR with the samples selected based on the compressed probabilities is that low-frequency words will be more heavily represented in the ASR system. Thus, the ASR system's bias toward high-frequency words may be reduced.",{"@attributes":{"id":"p-0122","num":"0121"},"figref":"FIG. 9","b":["200","220"]},"At step , respective word frequencies may be obtained from a corpus of utterance-to-text-string mappings. The corpus of utterance-to-text-string mappings may contain associations between audio utterances and a respective text string transcription of each audio utterance, and the respective word frequencies may be based on occurrences of words in the text string transcriptions. For example, a histogram may be developed that associates words in the text string transcriptions in the corpus of utterance-to-text-string mappings with a respective count of occurrences of the words, and the respective word frequencies may be obtained from the respective counts.","At step , respective compressed word frequencies may be determined based on the respective word frequencies. In some embodiments, the distribution of the respective compressed word frequencies may have a lower variance than the distribution of the respective word frequencies. For example, each of the respective word frequencies may be raised to a power less than 1 to form the respective compressed word frequencies. The power may also be greater than 0, and thus may be anywhere within the range of 0 to 1, including, for instance, 0.5.","At step , sample utterance-to-text-string mappings may be selected from the corpus of utterance-to-text-string mappings based on the compressed word frequencies. The sample utterance-to-text-string mappings may include a particular utterance mapped to a particular text string, and wherein the particular text string contains a first word. The selecting may involve determining a first word selection probability for the first word based on a first compressed word frequency of the first word divided by a first word frequency of the first word. Then, the particular utterance may be selected based on the first word selection probability.","Additionally, the particular text string may also contain a second word, and the selecting may also involve determining a second word selection probability for the second word based on a second compressed word frequency of the second word divided by a second word frequency of the second word. Then, selecting the particular utterance may be based on the first word selection probability and second word selection probability.","In some embodiments, selecting the particular utterance based on the first word selection probability and second word selection probability may include calculating an arithmetic mean of the first word selection probability and the second word selection probability, and selecting the particular utterance with a probability of the arithmetic mean. Alternatively or additionally, selecting the particular utterance based on the first word selection probability and second word selection probability may include calculating a geometric mean of the first word selection probability and the second word selection probability, and selecting the particular utterance with a probability of the geometric mean.","At step , an ASR system may be trained with the sample utterance-to-text-string mappings. In some implementations, before training the ASR system with the sample utterance-to-text-string mappings, the ASR system may be trained with the entire corpus of utterance-to-text-string mappings.","The above detailed description describes various features and functions of the disclosed systems, devices, and methods with reference to the accompanying figures. In the figures, similar symbols typically identify similar components, unless context dictates otherwise. The illustrative embodiments described in the detailed description, figures, and claims are not meant to be limiting. Other embodiments can be utilized, and other changes can be made, without departing from the spirit or scope of the subject matter presented herein. It will be readily understood that the aspects of the present disclosure, as generally described herein, and illustrated in the figures, can be arranged, substituted, combined, separated, and designed in a wide variety of different configurations, all of which are explicitly contemplated herein.","With respect to any or all of the message flow diagrams, scenarios, and flow charts in the figures and as discussed herein, each step, block and\/or communication may represent a processing of information and\/or a transmission of information in accordance with example embodiments. Alternative embodiments are included within the scope of these example embodiments. In these alternative embodiments, for example, functions described as steps, blocks, transmissions, communications, requests, responses, and\/or messages may be executed out of order from that shown or discussed, including in substantially concurrent or in reverse order, depending on the functionality involved. Further, more or fewer steps, blocks and\/or functions may be used with any of the message flow diagrams, scenarios, and flow charts discussed herein, and these message flow diagrams, scenarios, and flow charts may be combined with one another, in part or in whole.","A step or block that represents a processing of information may correspond to circuitry that can be configured to perform the specific logical functions of a herein-described method or technique. Alternatively or additionally, a step or block that represents a processing of information may correspond to a module, a segment, or a portion of program code (including related data). The program code may include one or more instructions executable by a processor for implementing specific logical functions or actions in the method or technique. The program code and\/or related data may be stored on any type of computer-readable medium such as a storage device, including a disk drive or a hard drive, or other storage media.","The computer-readable medium may also include non-transitory computer-readable storage media such as computer-readable media that stores data for short periods of time like register memory, processor cache, and\/or random access memory (RAM). The computer-readable media may also include non-transitory computer-readable media that stores program code and\/or data for longer periods of time, such as secondary or persistent long term storage, like read only memory (ROM), optical or magnetic disks, and\/or compact-disc read only memory (CD-ROM), for example. The computer-readable media may also be any other volatile or non-volatile storage systems. A computer-readable medium may be considered a computer-readable storage medium, for example, or a tangible storage device.","Moreover, a step or block that represents one or more information transmissions may correspond to information transmissions between software and\/or hardware modules in the same physical device. However, other information transmissions may be between software modules and\/or hardware modules in different physical devices.","While various aspects and embodiments have been disclosed herein, other aspects and embodiments will be apparent to those skilled in the art. The various aspects and embodiments disclosed herein are for purposes of illustration and are not intended to be limiting, with the true scope and spirit being indicated by the following claims."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE FIGURES","p":[{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 2A"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 2B"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 5A"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 5B"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 9"}]},"DETDESC":[{},{}]}
