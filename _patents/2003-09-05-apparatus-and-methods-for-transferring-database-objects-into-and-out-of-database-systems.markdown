---
title: Apparatus and methods for transferring database objects into and out of database systems
abstract: Techniques for transferring objects between database systems. A transfer is controlled by a master table in the RDBMS that is performing the transfer operation. The master table specifies the kind of transfer operation to be performed, a set of objects to be transferred, operations to be performed on the objects as they are being transferred, and filters for selecting a subset of the objects. During execution of the transfer, the transfer mechanism maintains and updates state in the master table and thereby makes it possible for the entity that is doing the transfer to determine the current status of the transfer and to restart the transfer after it has been stopped. The entity that is performing the transfer may also detach from the transfer without stopping the transfer and later again attach to the transfer.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08341120&OS=08341120&RS=08341120
owner: Oracle International Corporation
number: 08341120
owner_city: Redwood Shores
owner_country: US
publication_date: 20030905
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["CROSS REFERENCES TO RELATED APPLICATIONS","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION","CONCLUSION"],"p":["This application incorporates U.S. Ser. No. 09\/672,914, George Claborn, et al., Aggregating and manipulating dictionary metadata in a database system, filed Sep. 28, 2000, and U.S. Ser. No. 10\/014,038, Lee B. Barton, System having heterogenous object types, filed Oct. 10, 2001, by reference for all purposes. Also incorporated by reference is Oracle 9i Database Utilities, Release 2, Mar. 2002, Part No. A96652-01, available from Oracle Corporation, Redwood Shores, Calif.","1. Field of the Invention","The invention relates to database systems generally and more particularly to transferring database objects between database systems.","2. Description of Related Art","A database system provides persistent storage for items of data, organizes the items of data into objects, and manipulates the items of data by means of queries which describe a set of objects. A common kind of database system is a relational database system. Among the objects in a relational database system are tables. Each table has one or more columns and zero or more rows. For example, a two-column table called employees may look like this:",{"@attributes":{"id":"p-0007","num":"0006"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"140pt","align":"center"}}],"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":[{},"Name","Pay"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"140pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Bob","50,000.00"]},{"entry":[{},"Tom","60,000.00"]},{"entry":[{},"Jack","110,000.00"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}]}}},"The objects in the Name column are the names of employees; the objects in the Pay column are values specifying the employee's annual salary. A query on the table employees might look like this:\n\n","This query will return the set of values from the Name column that belong to rows which satisfy the condition that the value of Pay in the row be greater than 100,000. Here, the set of values consists of the name from the third row, namely \u201cJack\u201d. The query is written in the SQL (Structured Query Language) language. SQL is used to manipulate objects in relational database systems. SQL has two sublanguages: DML (Data Manipulation Language) which is used to manipulate non-metadata objects, and DDL (Data Definition Language), which is used to manipulate metadata objects. For example, the above query is written in the DML sublanguage; DDL was used to define the table employees.","Each object in the database system has an object type that describes how the object is defined and what can be done with it. For example, all table objects in the database system are objects of type table. In terms of how the objects are used in a relational database system, there are two kinds of objects: user objects like the one above that contain information belonging to users of the database system and system objects that contain information belonging to the database system itself. The database system uses the information in the system objects to manage the database system. Among the system objects are tables that contain information which describes all of the other objects in the database system. The information that describes an object is termed the object's metadata and an object that contains metadata for an object is termed herein a metadataobject. An object that does not contain metadata is termed herein an ordinaryobject For example, one of the metadata objects is a table that has an entry for each of the tables, both ordinary and metadata, in the relational database system. Metadata in this table and in other metadata objects associated with it specify the name of the table employees, the names of its columns, the type of data that may be contained in each column, the number of rows currently in the table, and the location in the database system of the actual data for the table employees.","Transferring database objects between database systems is made difficult by the fact that the objects may be very large and by the fact that any such transfer must maintain the objects' organization, including how the objects are ordered and the relationships between them. For example, even a simple table like employees must be transferred in a way which maintains the relationships between employee names and pay. Most database systems have export and import utilities for performing such transfers. The export utility puts the objects into a form which permits another database system to reconstruct the objects and the import utility reconstructs the objects from the exported form. Utilities also exist for importing data into a database from a non-database source.","Import and Export Between Databases",{"@attributes":{"id":"p-0012","num":"0012"},"figref":"FIG. 1","i":["Oracle ","i Database Utilities"],"b":["101","103","105","105","105","103","105","103"]},"Once the export file has been made, it is transferred to the destination database management system which is to import the database on the file. Transfer may be by sending the export file across a network or by making a copy of the export file on a portable medium and transferring the portable medium to the database management system which is to import the database on the file. Once the export file is available to the importing database management system, that system's import utility  serially reads the export file and uses the DDL to create the objects in file  in the destination database management system. Because the export file is ordered such that objects that have dependent objects precede the dependent objects in the export file, the information that export needs to create an object in database  is always available at the time the object is being created.","Import from Non-Database Sources","When data is imported into a database system from a non-database source such as a data file, the import operation must describe how the data in the source relates to the objects in the database. For example, if a data file to be loaded into the table employees has the data in the form Bob; 50,000; Tom; 60,000.00; Jack; 110,000; the import operation must specify that the first item in the file corresponds to the first field in a row of employees and the second to the second field in a row.","In the Oracle 9i database system, the utility for importing data into the database system from a non-database source is the SQL*Loader, which is described in the 9reference cited above.  is an overview of the operation of the SQL*Loader. SQL*Loader  takes one or more input data files , extracts data items from the files, and writes them into tables in database . A log file  records the course of the load operation, bad files  contain data items that should have been written to database  but could not be, and discard files  contain data items that were filtered from the items to be written to database . Loader control file  specifies how the desired data items are to be located in the input data files, what fields in the database tables the data items are to be written to, and how the data items are to be filtered. One way in which loader control file  specifies how the desired data items are to be located in the data file is by creating an external table definition in the database system. The data for the external table specified in the external table definition does not exist in the database system. Instead, the definition describes how fields in the external table relate to data items in the external source. The load operation is then specified by means of an update operation which updates fields in the database system's tables from fields in the external table. An important advantage of the external table definition is that it permits the use of facilities in the database system for operating on database objects in parallel in importing data from the external source. If the external table definition defines a table which, if it were a table in the database system, could be operated on in parallel, the load can be done in parallel.","Problems with Existing Export and Import Utilities","Most commercial database systems include utilities like the export, import, and SQL*Loader utilities employed in the Oracle9i database system. While these utilities do perform their intended functions, they have a number of drawbacks which make them inefficient and hard to use:\n\n","The above object of the invention is attained by using a control object in the database management system to control performance of a job which transfers a set of objects into or out of the data base management system by a mechanism that transfers database objects. In a preferred embodiment, the control object is a table, the master table, in the database system that is performing the transfer. The control object may specify a set of the objects to be transferred, an order in which the transfer mechanism transfers objects and which may be an order based on the size of the object, or a filter that further specifies the set of objects. The control object may further specify an operation on one or more objects being transferred, including a transformation of the object or a remapping of a name in the object. The control object may further specify one or more parameters for the job.","The control object further includes a specification of a status of the job which is updated by the transfer mechanism during the transfer. The control object is queryable to obtain a current status of the job from the specification of the status and the data transfer mechanism uses the specification of the status of the job to restart the job after the job has been stopped.","The kinds of transfer operations which the transfer mechanism performs under control of the control object include an export operation which transfers objects from either the DBMS in which the transfer mechanism is executing or a remote DBMS to a dump file set accessible to the DBMS and an import operation which transfers objects to the DBMS in which the transfer system is executing from either a dump file set or a remote DBMS.","In another aspect, the invention is a set of files for transferring a set of database objects into a database management system. The set of files includes at least one file containing the objects belonging to the set and a control object in the set of objects that specifies for each object the location of the object in the set of files and an order in which the database management system transfers objects during the transfer.","In a further aspect, the invention is a method of performing a job that transfers a set of database objects using a transfer mechanism that operates under control of the control database object. The method includes the steps of defining the job's control database object and executing the job by causing the transfer mechanism to transfer the set of database objects under control of the job's control object. In either the step of defining the job or executing it, the steps of attaching to the job in order to read and\/or modify the control object or of getting the job's current status may be performed. Once the step of attaching to the job has been performed, the steps of stopping the job, starting a stopped job, or specifying a degree of parallelism for the job may be performed. In the step of defining the job, the steps of specifying a source and\/or destination for the job, specifying the set of database objects, the step of defining a filter or the step of defining an operation on objects of the set may be performed.","In a still further aspect of the invention, the transfer mechanism performs the method of obtaining metadata for an object type, using the metadata to make a determination of the composition of the objects of the type, and selecting a transfer technique for objects belonging to the type from a plurality thereof according to the determination, and transferring the objects belonging to the type according to the selected transfer technique. In the selected technique, the objects may be transferred in parallel.","Other objects and advantages will be apparent to those skilled in the arts to which the invention pertains upon perusal of the following Detailed Description and drawing, wherein:","Reference numbers in the drawing have three or more digits: the two right-hand digits are reference numbers in the drawing indicated by the remaining digits. Thus, an item with the reference number  first appears as item  in .","The following Detailed Description will first present an overview of the invention and its use and will then present details of a presently-preferred embodiment.","Overview of the Invention","Overview of a Database System Including the Invention: ",{"@attributes":{"id":"p-0042","num":"0048"},"figref":"FIG. 3","b":["301","301","305","307","315","305","303","307","309","313","305","315","305","307","315","303"]},"DBMS programs  executed by processor  organize part of persistent storage  into a database , i.e., the persistent storage is organized into database objects that can be manipulated by DBMS programs . In , DBMS  is presumed to be a relational database management system, so the objects are predominantly tables, but the techniques described herein can be applied to any other kind of database management system and to other kinds of objects as well. As indicated above, there are two kinds of objects in DBMS : user objects, which are defined by users of DBMS , and system objects, which are defined by DBMS  itself and which in part contain the metadata that describes both the user objects and the system objects. One such metadata object is shown: table metadata table , which has an entry  in it for every table in database . The entry shown is for master table , a system object in database . The function of this table will be described later. Entry , like that for any of the tables in database , contains the definition of table  itself, the definitions of all of the columns in the table, a specification of the number of rows currently in the table, and a specification of one or more data segments  in persistent storage  that actually contain table 's data items. Each object in database system  has at least one segment  corresponding to it. In the following, a segment  that contains data items from a metadata object is termed a metadata segment and one that contains data items from an ordinary object is termed a data segment. The set of segments that contains an object's data items may be further divided into partitions. Segments of a particular object that belong to different partitions may be read and\/or written in parallel by processor .","Export of objects from DBMS  and import of objects into system  is done by a program  in DBMS programs  which will be termed herein the Data Pump program. As may be seen by dashed arrow , Data Pump program  operates on database . Import may be by way of a network, as shown by arrow ; as shown by arrows  and , either import or export may be by way of one or more dump files , which are stored in persistent storage .","As shown by arrow , what objects Data Pump  exports or imports, how Data Pump  exports or imports the objects, and what operations it performs on the objects are determined by master table , a table that exists in database  at least for the duration of the export or import job. Master table  contains information about the current status of an export or import job and restart information. Data Pump  makes the current status information available to a user of system  who is monitoring the export or import job. Data pump  uses the restart information to restart the operation after a user of system  has stopped it or after a shutdown due to an error. Because master table  is an object in the database system, all of the operations offered by the database system are available to manipulate master table . For example, the current status may be read from the table by means of a query. Another advantage of master table  is that it is a persistent object: it represents the import or export operation and makes the operation's status available for the entire life of the operation, from the definition of the operation through its execution to its completion and even thereafter, and this in turn makes it possible for a user to attach to and detach from the import or export operation, obtain status information about the operation, and also for a user to stop or restart the operation.","In a preferred embodiment of DBMS , Data Pump  can perform the following operations:\n\n","In , the import and export operations involving dump file set  are shown by arrows  and ; arrow  indicates that the objects in the import and export operations may be obtained from or written to database ; arrow , finally, indicates that Data Pump  may obtain objects from a remote DBMS via network  for either an export operation to a set of dump files  or a fileless import operation to database . If there are n dump files in set , dump file () includes a copy  of master table  which controlled Data Pump  while Data Pump  was making dump file set . Master table copy  contains a list of dump files (. . . n) and thus serves as a directory for the dump files. As will be explained in more detail later, copy  is used to make the master table  used for an import operation that uses dump files . The dump files may be moved from the exporting system  to the importing system  by any available method.","Overview of Master Table : ",{"@attributes":{"id":"p-0048","num":"0059"},"figref":"FIG. 5","b":["321","321"],"ul":{"@attributes":{"id":"ul0007","list-style":"none"},"li":{"@attributes":{"id":"ul0007-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0008","list-style":"none"},"li":["object information , which describes the database objects being imported or exported;","job state information , which describes how far the export or import job has progressed and what its current state is;","restart status information , which describes the results of restarting the import or export job after it has been stopped either intentionally or as a result of an error;","dump file information , which specifies the dump files  being written by the export job or read by the import job;","filter information , which describes filters that may be applied to the objects being imported or exported;","transform information , which describes transformations and remappings that may be applied to the metadata being imported or exported; and","parameter information , which describes parameters used to define the import or export operation.\n\nThe information other than object information  specifies attributes of the export or import job and will be termed generally attribute information herein.\n"]}}}},"The information in master table  is contained in rows . Although  shows the kinds of information as being located in discrete portions of master table , rows containing different kinds of information may occur anywhere in the table. All of the rows have the same fields , but the kind of information contained in a particular row determines which fields are used in that row. Unused fields are set to NULL. The kind of information contained in a given row is specified by process_order field . Duplicate 504 is used to distinguish different rows with the same process_order value 503 from each other. A record's <process_order, duplicate> pair uniquely identifies the record in master table . In the case of rows containing object information , process order field  has a positive integer value; each row that contains object info has a different positive integer value in field  and the order of the values determines the order in which the rows are processed in an export or import operation. With rows containing other kinds of information, the type of the row is indicated by a negative value; for most of the other kinds of information, there are two such negative values; one indicates that the row's information is to be used in an export operation and the other indicates that the row's information is to be used in an import operation.","Further details of a row of object information may be seen at . As previously mentioned, process_order field  has an integer value greater than 0. Duplicate field  is used when the information for the object may be subdivided in a fashion which permits parallel copying of the object. In this case, there is a separate object row  for each such subdivision of the object. Object type sequence field  is used together with process_order field  to order processing of the objects represented by the object rows. Each object has a type and all objects belonging to a particular type are processed together, with the object's process_order field indicating the order in which the object is processed relative to other objects of its type. Field  indicates the order in which the types of objects are processed. The remaining fields in a row  contain object dump file information , which specifies where the data for the object represented by the row is located in dump files , particular object information , which describes the particular object represented by row , and object processing information , which describes how the object represented by row  is to be or has been processed in the export or import operation.","An important advantage of master table  is that it is a database table. As such, information may be written to and read from master table  using any of the operations available in database management system . To cite two examples: filters that filter objects can be applied directly to master table copy  instead of to dump files  and status information can be retrieved simply by querying master table . Further, as with any database table in system , operations may be performed on different rows of the table in parallel. Another advantage is that compatibility between Data Pump implementations that use different versions of master table  can be maintained simply by providing transforms that transform a master table  for one Data Pump implementation into a master table  for a different implementation.","Overview of Dump File (): ",{"@attributes":{"id":"p-0052","num":"0070"},"figref":"FIG. 9","b":["323","325","323","325","901","323","903","325","323","904","902","323","323","903"],"i":["n","n","n","n","n"]},"Dump file data  is located between header  and master table copy . The data is stored in segments. Different segment types are used for metadata and non-metadata. Metadata is contained in metadata segments . The metadata in the metadata segments is represented using the well-know extended markup language (XML) standard. The XML for the metadata for one object is shown at () in segment . A metadata segment may contain the XML for the metadata for a number of objects. Data segment  contains the data for a non-metadata object. Such objects are termed in the following regular objects. A segment may contain data from only one regular object, but data from a single large regular object may be contained in many segments. Dump file information  in an object row  identifies the location of an object's data in a metadata segment  or a data segment  of a dump file (), as shown with regard to rows () and () in .","Overview of Operation of the Invention: ",{"@attributes":{"id":"p-0054","num":"0072"},"figref":"FIG. 6","b":["601","311","311"],"ul":{"@attributes":{"id":"ul0009","list-style":"none"},"li":{"@attributes":{"id":"ul0009-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0010","list-style":"none"},"li":["a defining stage  in the job is defined. In this stage, the job type is specified, the master table  for the job is created in the DBMS doing the job and initialized, and rows are added as required to define object sources or destinations, transforms to be applied to the objects, and parameters for the job.","an executing stage  in which the job specified in master table  is performed; objects are moved from a source to a destination as specified for the job and the master table is updated to reflect the current state of the job. When the job is finished, there is a row in the master table for each object that was moved."]}}}},"Defining stage  is initiated by an OPEN call to Data Pump  and ended by a START_JOB call to the Data Pump. At any point in defining stage  or executing stage , a DBA or other user with the proper access privileges may make an ATTACH call to the Data Pump which gives the DBA access to the job and permits the DBA to examine the current state of the job. An ATTACH is a side effect of the OPEN call. An ATTACHed DBA is thus one who has OPENed a job or made an ATTACH call and has not yet made a DETACH call. In defining stage , the ATTACHed DBA may add rows to master table  which define filters, transforms, and parameters. The ATTACHed DBA may also terminate the job with a STOP_JOB call to Data Pump ; at this point, STOP_JOB destroys master table  and the job ceases to exist.","In executing stage , an ATTACHed DBA may make the START_JOB call, may further define the job by adding files to dump file set  or changing the degree of parallelism with which objects are transferred, may examine the current state of the job, and may stop the job with the STOP_JOB call. As long as STOP_JOB is executed after a START_JOB call and does not specify that the master table be destroyed, an ATTACHed DBA may make the START_JOB call to restart the job. At any point after performing an OPEN or an ATTACH call, the user may perform a DETACH call. A DETACH does not stop execution of the job. In the preferred embodiment, ATTACH and DETACH may be done via the Internet from a computer which is remote from the system upon which the export or import job is being performed. An ATTACHed user may also perform a GET_STATUS call which runs a query on master table  to obtain the job's status at any point in the defining and executing stages.","As set forth in more detail at block  of , the OPEN call specifies the topmost object for the job (for example, an entire database), and specifies whether the job is an import or an export job and whether it uses a dump file set. Import jobs that do not use dump file sets are termed herein fileless import jobs. Data Pump  uses the information provided in the OPEN call to make initial rows in master table . The initial rows indicate whether the job is an export job or an import job and if the job involves a network connection to a database system, they specify the connection.","As shown at , following the OPEN call or after an ATTACH call, the DBA may further define the job by means of calls to Data Pump  that specify parameters, dump files (if needed), and filters and transforms. The filters and transforms are specified relative to the source of the objects being transferred; thus in an export operation, they are specified relative to the objects as they exist in the database system which is their source; in an import operation from dump files, the filters and transforms are specified relative to the objects as they exist in the dump files; in an import operation from a data base system, they are specified relative to the objects as they exist in the database system. Specifications of filters, transforms, parameters, and dump files result in rows being added to master table  for each filter, transform, parameter, and dump file. Particular rows for these entities are identified by their <process_order, duplicate> pairs. The job's defining stage  is terminated by the first START_JOB call to Data Pump  for the job, as shown at . START_JOB may follow an OPEN call if there has been no DETACH call made since the OPEN call; otherwise, it must follow an ATTACH call.","The first START_JOB call begins executing the job as it has been defined in the master table  that resulted from defining stage  and thus commences executing stage . As shown in detail at , in the execution stage, Data Pump  copies the objects from the source to the destination as specified in the master table and adds and\/or updates rows to the master table to represent the current state of the job. Thus, at any point in the job, a GET_STATUS call can obtain the current state of the job from master table  and at the end of the job, master table  contains a complete record of the job.","Export: ","The Export operation performed by Data Pump  always copies the exported objects to a dump file set; the objects being exported may come either from the DBMS which is performing the export operation or from a DBMS to which the DBMS performing the operation is linked by a network connection. Whether an export job gets its objects from the local DBMS or a remote DBMS is specified in the OPEN call for the job and indicated in the job's job status row in master table .  is a flowchart  for export jobs. Flowchart  is similar in general form to flowchart . The job begins with an OPEN call, as shown at . Then the job is defined further at . Included in this further definition is a specification of the dump file set that the objects are to be written to. Rows specifying the files in the dump file set are added to master table . Also specified and added as rows to master table  are filters that select the objects to be transferred in the job.","In executing stage , the export operation copies the objects specified in master table  from the source database to dump file set . All objects of a particular object type are copied together. The order of the types is predetermined and may be selected to optimize operation of Data Pump  as required for a particular operation (). The current object is copied () and then a row representing the object is added to master table  (). When all of the objects have been transferred, master table  is copied to the dump file (), finishing the export operation ().","If master table  specifies filtering for the object, the filtering is done as the object is moved from the source DBMS to dump file set . If the object contains metadata, what is fetched from the source DBMS and written to dump file set  is an XML representation of the metadata. In each object row , the row's object type sequence field , process_order field , and duplicate field  are set to values that ensure that an import operation that uses dump file set  will process the objects in the right order. Each object row  also specifies the location of the object's data in dump file set  in object dump file info . The location is specified by dump file ID , so that the dump files can be renamed without affecting the operation of Data Pump . As will be explained in more detail later, with table objects that do not contain metadata, the table's contents are treated as a separate object having the table data type. Data Pump  processes table objects being exported by the size of the table object's table data object, beginning with the largest table data objects. This ensures that during an import operation, the objects will be imported in an order which not only respects object dependencies but also permits the most efficient transfer of the objects to the destination DBMS.","As indicated above, the source of the objects for the export operation may be either the DBMS that is performing the export operation or another DBMS to which the exporting DBMS has a network link. The difference between what is shown in  and what is done when the source DBMS is remote is simply that the object fetching step  is performed via the network connection. The technique of fetching objects from a remote DBMS to the DBMS  doing the export operation is useful in situations where the remote DBMS cannot execute Data Pump . For example, if the database to be exported from the source DBMS is read only in the source DBMS, Data Pump  cannot create master table  in the database. Data Pump  on the exporting DBMS can, however, read the objects in the database from the source and create master table  and the dump files on the exporting DBMS. Similarly, if the DBMS doing the export operation has more free storage space or processing power available than the source DBMS, it may be better able to do the export operation than the source DBMS.","Imports from Dump File Sets: ",{"@attributes":{"id":"p-0064","num":"0084"},"figref":"FIG. 8","b":["801","323","323","611","321","805","323","807","611","325","323","321","809","805","321","810","321"]},"In executing stage , Data Pump  processes the object rows in master table  in the order specified by object type sequence field , process order field , and duplicate field . Thus, all of the objects belonging to a type with the object type sequence number n are processed before objects belonging to a type with the object type sequence number n+1 are processed. As shown by decision block , loop  runs until all object rows have been processed. For each object row , Data Pump  reads the row from master table . What happens next depends on whether the object is a metadata object (). If it is, Data Pump  reads the XML for the metadata object from the dump file, applies any transforms specified in master table , and then uses the metadata as transformed to create the metadata object in the DBMS performing the import (). If the object is an ordinary object, Data Pump  reads the object data from the dump file, applies any transforms, and creates the object in the DBMS performing the import.","Fileless Import Jobs. ","In a fileless import job, there is no dump file set  and hence no copy of master table . Instead, master table  for the fileless import job is built in the same fashion as in an export job: as each object is fetched from the remote source DBMS and written to the importing DBMS, a row for the object is added to the job's master table  in the importing DBMS.  presents a flowchart  of a fileless import operation. In defining stage , the job is defined and master table  is initialized in the same fashion as for an export job (). The further definition of the job at  does not specify any dump files and the filters and transforms are to be applied to the objects being imported from the remote source DBMS. As before, defining stage  ends with a START_JOB call.","Executing stage  also resembles that of an export job. All objects of a particular type are imported together and the types are selected according to a predetermined order that ensures that objects required to create dependent objects are already in the destination DBMS when the dependent object is imported. At , Data Pump  fetches the metadata for the next object from the source DBMS; if there is no is metadata left, loop  terminates and the import job is done (). What happens next depends on whether the object is a TABLE_DATA object (one that contains data for a table that is an ordinary object) (). If the object is not a TABLE_DATA object, what is of interest is the object's metadata. First, a row for the object is created in the master table (). Then the metadata is used to create an object or objects in the destination DBMS (). If the object is a TABLE_DATA object, the metadata retrieved at  is used to fetch the table data object from the source DBMS (). Then the object's row is written to the master table (). Finally, the object is inserted into the table data object's table in the destination DBMS (). In either case, loop  is then iterated.","Examples of Operation of a Preferred Embodiment. ",{"@attributes":{"id":"p-0068","num":"0088"},"figref":"FIGS. 19 and 20","b":["311","321","311"]},"Export Example: ","Beginning with code example , this example defines and begins execution of a Data Pump export operation. At , an OPEN call specifies that that a job named MYDBMOVE_EXPORT is the export of the full database belonging to the user who makes the OPEN call. As a result of the OPEN call, a master table  is created with the job's name and a group of initial non-object rows and a handle is returned which is used in succeeding calls to Data Pump  to identify the job. Then, at , a succession of ADD_FILE calls creates three dump files and for each dump file a row in dump file info . Each ADD_FILE call specifies the handle for the job, the file name for the dump file, the directory for the dump file, and the maximum size of the dump file. At , the METADATA_FILTER call adds a row in filter info  which specifies that the entire database except the schema identified by BLAKE is to be exported. This row thus specifies what is to be exported as well as what is not to be exported. The SET_PARALLEL call at  specifies that the job is to be done with a maximum degree of parallelism of three. The START_JOB call at  starts the job and the DETACH call detaches the DBA from the job, which, however, continues executing.","Sometime later, other demands on the exporting DBMS reach a point at which the DBA is required to stop the export job MYDBMOVE_EXPORT. How the DBA does this is shown at . First, the DBA uses an ATTACH call with the job's name to retrieve the handle for the job; then the DBA uses the STOP_JOB call with the handle to stop the job. As a side effect, STOP_JOB also DETACHes the DBA making the STOP_JOB call.","Still later, the demands on the exporting DBMS have lessened to the point that the DBA can not only restart the job, but increase the degree of parallelism. The code at  shows how this is done. First the DBA uses the ATTACH call to reattach to the job (); then, at , the DBA adds two more dump files to accommodate the added degrees of parallelism. Addition of the dump files of course results in the creation of rows for the files in dump file info . Then the DBA sets the degree of parallelism to 5 (), restarts the job (), and detaches from it (), leaving the job to run to completion with a maximum of five degrees of parallelism.","Import Example: ",{"@attributes":{"id":"p-0072","num":"0092"},"figref":["FIG. 20","FIG. 19"],"b":["2001","2003","321","321","2005","321","2007","323","301","311","521","311","311","325"]},"The definition stage is completed by a METADATA_REMAP call and a SET_PARALLEL call. The METADATA_REMAP call describes a transformation to be applied to the metadata in the export job. The transformation replaces all occurrences of the table space name USER1 in the metadata with the new table space name NEWUSER1, which will be the name of the table space in the destination database system . At this point, the METADATA_REMAP call results in the creation of a row of transform information . The SET_PARALLEL call specifies the maximum degree of parallelism that is to be used in the import operation. Here, the degree \u201c4\u201d is specified. The effect of the SET_PARALLEL call at the definition stage is to set a field that specifies the maximum degree of parallelism in the row in job state info .","The executing stage begins with the START_JOB call. With an import job, Data Pump  responds to the START_JOB call by copying the entire copy  of the master table for the export job into the import master table . Then Data Pump  executes the job as specified in import master table , using the degree of parallelism specified at . As the creation DDL for an object to be created in the destination database system is made from the metadata for the object, the DDL is transformed as specified in the row created by the METADATA_REMAP call. At any time during the executing stage, the DBA may execute a DETACH call as at . This time, there is no need to stop the job, so Data Pump  continues to execute the job until it is finished and all of the objects in the dump files have been imported into the destination DBMS system.","Overview of Operation of Data Pump : ",{"@attributes":{"id":"p-0075","num":"0095"},"figref":"FIG. 4","b":"311","ul":{"@attributes":{"id":"ul0011","list-style":"none"},"li":{"@attributes":{"id":"ul0011-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0012","list-style":"none"},"li":["UNDEFINED : no master table  exists for the job; hence, the only Data Pump operation that is possible in this state is OPEN.","DEFINING : a master table exists, but execution of the job has not yet begun. Operations which define the job prior to execution, ATTACH and DETACH, GET_STATUS, and START_JOB are possible.","EXECUTING : a master table exists and execution of the job has begun. Operations which define the job during execution, ATTACH, DETACH, GET_STATUS, and STOP_JOB are possible.","STOP PENDING : A non-immediate STOP_JOB call has been made but a stopping point for the job has not yet been reached; ATTACH, DETACH, and GET_STATUS are possible.","STOPPING : an orderly stop of the process is being carried out; ATTACH, DETACH, and GET_STATUS are possible.","STOPPED : the process is stopped; ATTACH, DETACH, and GET STATUS are possible.","IDLING : the process is stopped, but a client is still attached; ATTACH, DETACH, GET_STATUS, and START_JOB are possible.","COMPLETING : an orderly finish of the job is being carried out; ATTACH, DETACH, and GET_STATUS are possible.","COMPLETED : the job is finished and the master table still exists; ATTACH is possible."]}}}},"As for the state transitions, transition  from UNDEFINED  to DEFINING  occurs when Data Pump  executes an OPEN call; transition  from DEFINING to EXECUTING occurs when Data Pump  completes execution of the first START_JOB call received for the job. If the DBA wishes to end the job in the DEFINING state, he or she makes a STOP_JOB call; in response to this call, Data Pump  deletes the master table and makes transition  to UNDEFINED state .","In EXECUTING state , Data Pump  stops a job in response to a STOP_JOB call or a fatal error. The STOP_JOB call has an argument that specifies an immediate shutdown; in that case or in the case of a fatal error, Data Pump  executes transition  to STOPPING state ; otherwise it executes transition  to STOP PENDING state , in which it continues processing until the job is in a state which permits easy restart and then makes transition  to STOPPING . When the shutdown performed in STOPPING  is finished, Data Pump  makes transition  to STOPPED state . If STOP_JOB includes an argument that specified that Data Pump  delete master table  for the job, Data Pump  makes transition  to UNDEFINED . Otherwise, if any DBA is still ATTACHed or no DBA is still attached and any DBA makes an ATTACH call, transition  from STOPPED  to IDLING  occurs.","When all objects have been transferred, Data Pump  makes transition  to COMPLETING ; if the KEEP_MASTER parameter has been set to indicate that master table  be destroyed on completion of the job and the last DBA DETACHes, Data Pump  destroys the table and makes transition  to UNDEFINED state ; otherwise, if KEEP_MASTER indicates that master table  is to be kept on completion and the last DBA DETACHes, Data Pump  makes transition  to COMPLETED state . If a DBA ATTACHes in COMPLETED state , transition  occurs to COMPLETING state .","When the above state diagram is applied to Data Pump  as it executes the example export job of , it will be seen that that Data Pump  begins in UNDEFINED state , transitions on the OPEN call to DEFINING state , remains in that state until the START_JOB call is executed at , transitions then to EXECUTING state , in which it remains until the STOP_JOB call at  results in transitions from STOP PENDING  through STOPPING  to STOPPED . The ATTACH call at  places Data Pump  in IDLING . In IDLING state , Data Pump  executes the calls at  and , and when Data Pump  executes START_JOB at , Data Pump  makes the transition to EXECUTING state  and remains there until all of the objects have been transferred, at which point it makes the transition to COMPLETING  and from there to COMPLETED  or UNDEFINED , depending on the setting of KEEP_MASTER.","Details of a Preferred Embodiment","In the following, details of a preferred embodiment will be presented, beginning with a preferred embodiment of master table , continuing with the Application Program Interface (API) for a preferred embodiment of Data Pump , and concluding with a detailed description of the implementation of Data Pump  in a preferred embodiment.","Preferred Embodiment of Master Table . ","These figures present the definitions of all of the different kinds of master table rows  in a preferred embodiment. The definition for each row includes the non-null fields for that kind of row. There is a table in the figures for each kind of row. Each table has a row for each non-null field in that kind of row. The table rows have three columns: one giving the name of the field, one giving its data type, and one giving the field's meaning. For the most part, the tables are self-explanatory; where particular details need to be pointed out, reference numbers have been provided for that purpose.","Object Row : ","The reference numbers A, B, and C in  identify the portions of the figures that show the non-null fields in object row . The outer level of brackets relates these fields to the classes of information shown in  for object row . Reference numbers inside the outer level of brackets refer to features of object row  that are of particular interest in the present context.","As already explained, there are one or more object rows  for each object that was exported or is to be imported in master table . In a preferred embodiment, table objects which do not contain metadata are treated specially. In the context of master table , such table objects are termed TABLE objects. A TABLE object always has at least two rows in master table , one representing the TABLE object itself, and at least one row representing the contents of the TABLE object. In the context of master table , objects which represent the contents of a TABLE object are termed TABLE_DATA objects. Fields which are of particular relevance to TABLE or TABLE_DATA objects are pointed out in the Meaning column of .","With all objects, the value in process order field  identifies the object the row belongs to and the order in which it is to be processed relative to other objects having the same value in OBJECT_PATH_SEQ_NO ; where there is more than one row for an object, duplicate field  identifies and orders the rows. Object dump file info  is implemented as shown at : the dump file () in which the object's data resides is identified by dump file ('s) dump file ID , the position at which the object's data begins in the dump file, the length in bytes of the object's data, and the number of blocks allocated in the dump file for the object.","The part of object processing information  which indicates the current state of processing for the object is indicated at . Data Pump  reads these fields for objects that are currently being processed in response to a GET_STATUS call. The part of particular object info  which completely identifies the object is shown at ; included are the information needed to identify the object's metadata and the object itself in the source or destination database system.","Continuing with , COMPLETION_TIME  is a timestamp that is used on an import operation to indicate the time at which the object was created in the destination database. IN_PROGRESS  is used in an import operation to indicate whether the object creation was completely defined when the executing stage stopped. If the object represented by the row is dependent from another object, the other object is identified at . When the object row represents a table object,  indicates the maximum degree of parallelism with which the table object's data may be processed. Data Pump  computes the value for this field from information in the table's metadata when it makes the row. When the row represents a TABLE_DATA object, UNLOAD_METHOD  indicates the method that Data Pump  uses to unload the row's object into the dump file on export or load the object from the file on import. Data Pump  computes the value for this field from the mode and remote link information received via the OPEN call and from the metadata for the object. The Direct Path and External Table methods will be described in detail later. GRANULES indicates the number of granules in the object being operated on. A granule is the unit of allocation for the object's data. SCN , finally is the system change number for the last transaction that altered the object. In an export operation, the SCN is used to maintain consistency between different partitions of the object being copied. In an import operation, the SCN is used to determine whether the imported object needs changes applied to it to make it a replica of an object on a remote DBMS.","Continuing with , object row C contains the remaining non-null fields in an object row. If the row represents a domain index object, the fields at  identify the object.  is used only for a grant object. The field specifies the user that made the grant object. In object rows for TABLE objects, XML_CLOB  is a character large object that contains an XML representation of the TABLE object's metadata. During an export operation, the XML representation is stored in the object row when the row is made and is kept in the object row for easy access until the export operation is complete. When master table  is copied to the dump file, the XML representation is also copied to the dump file.","JOB_STATE Row : ","JOB_STATE row , shown at A in  in , retains the parameters supplied on the OPEN call and provides the current state of the job. In an export operation, there is only one JOB_STATE row  in master table ; in an import operation from a dump file set, there are two JOB_STATE rows , one from master table  for the import job and one from the dump file set's master table copy . A and B show the non-null fields for this row. Those that are of particular interest in the present context are PROCESS_ORDER , which is for a non-object row and consequently has a negative value. As is the case for most of the non-object rows, the value depends on whether master table  has been made as part of an export or import job, as specified in the OPEN call that started the definition of the table. The information labeled  comes from arguments supplied with the OPEN call that defines the master table. Other fields of interest are STATE , which specifies the current state of the job the master table  belongs to. START_TIME  is a date stamp that indicates when the OPEN call that began the job definition was made. DEGREE  specifies the maximum degree of parallelism that may be used in the job. The value of the field is set by the DBA using a SET_PARALLEL call to Data Pump , which approaches the specified degree to the extent possible given the nature of the job and the resources available. ERROR_COUNT  gives the total number of errors reported by Data Pump  thus far in performing the job.","In , the meaning of TOTAL_BYTES in an export job depends on when the field is given its value. At the beginning of the job, the value is an estimate of the total number of bytes that will be transferred in the job. Data Pump  makes the estimate by retrieving the metadata for the job and making the estimate from the information about the storage required for the objects in the metadata. The estimate is used by GET_STATUS to determine the percentage of the job which has been completed. When the job is completed, TOTAL_BYTES is set to the total number of bytes actually transferred. On import from a dump file set, TOTAL_BYTES is taken from master table copy  and is modified to reflect any filtering specified in master table .","MAX_PROCESS_ORDER Row , ","This row is used to coordinate parallel operations during an export job. As already pointed out, objects are processed as ordered by the values of the PROCESS_ORDER fields in the object rows  for the objects. The SEED field contains the largest PROCESS_ORDER value from the set of object rows  whose objects have already been or are currently being processed by Data Pump .","TYPE_COMPLETION Row , ","The TYPE_COMPLETION rows  indicate the order in which the object types are processed and the current state of processing of objects of a particular type for a job. OBJECT_PATH_SEQ_NO field  is used to sequence the object types in an import operation. It is the value of this field which appears in object type sequence field  in all of the object rows in table  for objects of the object type. Otherwise, TYPE_COMPLETION rows  are part of job state info  and are used to restart a job. Restarting is done by restarting processing of objects of types currently being processed at the beginning of the processing for each of the types. OBJECT_TYPE_PATH specifies the object type represented by the row. If all of the objects of the type have been processed, COMPLETION_TIME indicates the time of completion and COMPLETED_ROWS indicates how many objects of the type have been processed. When an export operation is restarted, the object rows  for processed objects belonging to partially-processed types are returned to their state prior to being processed and are again processed. When an import operation is restarted, the information is used to ignore \u201cobject already exists\u201d errors caused by writing objects to the importing DBMS that had been written earlier in the import operation.","FILE Row ","Each file in dump file set  has a FILE row  (shown in ) in master table . The value of DUPLICATE field  is used as the value of dump file ID  in the dump file's header and as the value of DUMP_FILE_ID field in object rows  whose objects are stored in the dump file represented by the FILE row . This arrangement provides a way of identifying dump files within master table  which is independent of the names used for the dump files in the DBMS doing the export or import. The name of the file in the exporting or importing DBMS is specified at 1405 and its maximum size at . The DBA can use an ADD_FILE call to cause Data Pump  to add FILE rows  to master table  or can specify one or more templates for file names; in that case, Data Pump  generates file names using the templates and makes FILE rows  for them as new files are needed in dump file set .","WILDCARD_FILE Row ","Continuing with , there is a WILDCARD_FILE row  in master table  for each file template defined by the user in an ADD_FILE call. Non-null fields include the template (), the maximum size of files made using the template (), the last value used to make a file name from the template (,) and LAST_FILE , which specifies the last file made using the template by the value of the DUPLICATE field in FILE row  for the file.","WORKER Row : ","A preferred embodiment of Data Pump  uses a separate worker process to write data to or read data from each of the dump files  in the dump file set. The maximum number of worker processes is specified by the field DEGREE  in JOB_STATE row A. Each of the worker processes has a WORKER row  in master table  whose fields contain current and cumulative information about the work the row's worker process has done. The information is used by Data Pump  in controlling the worker processes and is also used to compute restart status information . The WORKER rows thus belong to job state info .","A WORKER row 's worker process is identified by fields  and ; field  is an internal ID for the process; field  contains the process's name. The fields indicated by  identify the object the row's worker process is presently working on; the fields indicated by  show the status of the work on the object currently being processed by the worker process.","TOTAL_BYTES is an estimate of the total size of the object currently being processed by the worker process represented by the row; the value is used by the GET_STATUS call to compute the percentage of the object that has been processed. The rows indicated by , finally, indicate the cumulative work that the worker process has done since the last restart of the job and the time that the worker process has spent working on the job since the last START_JOB call. Data Pump  uses the information in the workers' rows to compute the overall state for the job.","RESTART_STATUS Row : ","Data Pump  uses RESTART_STATUS row  to record information in area  of the row from the JOB_STATE field TOTAL_BYTES  and the WORKER row fields METADATA_IO, DATA_IO, and CUMULATIVE_TIME each time a restart operation is performed. There is only a single RESTART_STATUS row  in master table . If no restart operation has been performed, the relevant fields for this row all contain the value 0. When GET_STATUS returns a report indicating how much total work has been done to date, the total work is computed by adding the current values of the WORKER row fields, METADATA_IO, DATA_IO, and CUMULATIVE_TIME to the current values of the corresponding fields of area  of RESTART_STATUS row .","RESTART_row : ","There is a RESTART row  in master table  for each restart operation performed during an export or import job. Portion  of the row contains copies of the values in the corresponding fields of RESTART_STATUS_ROW  as of the time of the restart represented by RESTART row ; the value of ERROR_COUNT field  is copied from the corresponding field  of JOB_STATE row A. Portion  contains information about the environment of the job at the time of the stop that occasioned the restart operation represented by the row. Included are the location of the final message output by Data Pump  when the job stopped, the time that elapsed between the immediately preceding stop operation and the START_JOB call which preceded that stop operation, the starting time of the stopped job, and information about the system that was running the job when it was stopped. One advantage of the arrangement of RESTART rows in the prefered embodiment is that the DBA can simply compare RESTART rows to determine whether a job that has stopped, been restarted, and has stopped again has made any progress between stops.","FILTER Rows  and : ","Each of these rows defines a filter for a set of objects in the export or import operation. The set may include all of the objects involved in the export or import operation. There is a separate row for each filter that is to be applied during the job. In an export operation or a fileless import operation, the filter is defined with regard to the set of objects as they are in the source database for the export operation. In an import operation that imports from a dump file, the filter is defined with regard to the objects as they are in the dump files. DATA_FILTER row  defines a filter for a data object. Filter specification  defines the filter. As seen at , each filter has a name defined by Data Pump  and a specification of the set of objects to which the filter applies by schema and name. A value that is used in the filter is defined at  and ; in the case of VALUE_T field , the field may contain a subquery written in SQL.","METADATA_FILTER row  defines a filter for a set of metadata objects; it works the same way as the row for data objects, except that there the definition of the filter will only involve text values, and the filter is applied to a set of metadata objects.","TRANSFORM Rows ","These rows define transforms and remaps to be applied to a set of metadata objects during an import operation. A transform does what its name implies: it gives each metadata object belonging to the set a different form. Transforms are typically used to suppress certain clauses in the creation DDL for an object. The new form of the object is specified in VALUE_N field . A remap maps a name in the metadata object to a different name; when the object is imported, the name is replaced by the different name. OLD_VALUE  specifies the name before the remap; VALUE_T specifies the name after the remap.","PARAMETER Rows  and : ","The DBA may set parameters defined by Data Pump  that control how Data Pump  does an export or import job. Each of the parameters that have been set by the DBA or that have a default value has a PARAMETER row in master table . As shown at , each row contains the name of its parameter, the parameter's default value if Data Pump  gives it one, and any non-default value the parameter has. Some parameters have text values, which are stored in the VALUE_T field and others have numeric values, which are stored in the VALUE_N field. There is a single NLS parameter row . The row specifies globalization parameters. These parameters determine how character sets and field names are interpreted and thus permit the database management system to be adapted to different linguistic and cultural environments. The globalization parameters specified in the row are those that were in force on the database system that is the source of the data being exported at the time of the export operation. The parameters are specified at  by a character string which is a DDL command that, when executed in the target database management system, will establish the NLS settings that the objects had in the source database. The NLS settings may be further qualified by NLS settings in the session of the user who made the OPEN call for the job.","A Presently-Preferred Embodiment of the Data Pump API: ",{"@attributes":{"id":"p-0103","num":"0132"},"figref":["FIG. 21","FIG. 4"],"b":["2101","2103","2129","403","439","321"]},{"@attributes":{"id":"p-0104","num":"0133"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"91pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"126pt","align":"left"}}],"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}},{"entry":["API","Effect on Master Table 321"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["ADD_FILE","Adds a FILE or WILDCARD_FILE"]},{"entry":[{},"row to the master table"]},{"entry":["DATA_FILTER","Adds a DATA_FILTER row to"]},{"entry":[{},"the master table"]},{"entry":["METADATA_FILTER","Adds a METADATA_FILTER"]},{"entry":[{},"row to the master table"]},{"entry":["METADATA_REMAP","Adds a METADATA_TRANSFORM"]},{"entry":[{},"row to the master table"]},{"entry":["METADATA_TRANSFORM","Same"]},{"entry":["OPEN","On export: makes the JOB_STATE,"]},{"entry":[{},"MAX_PROCESS_ORDER,"]},{"entry":[{},"MASTER_TABLE (the object row"]},{"entry":[{},"for master table 321), RESTART_STATUS,"]},{"entry":[{},"and NLS_PARAMETERS; on import:"]},{"entry":[{},"makes the JOB_STATE, "]},{"entry":[{},"RESTART_STATUS, and"]},{"entry":[{},"NLS_PARAMETERS rows."]},{"entry":["SET_PARALLEL","Sets the field DEGREE in "]},{"entry":[{},"the JOB_STATE row"]},{"entry":["SET_PARAMETER","Adds a PARAMETER row"]},{"entry":[{},"to the master table"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},"In the following, API calls that are of particular interest are discussed in more detail. Complete specifications of the API are given for OPEN and GET_STATUS.","SET_PARAMETER ","This call makes a PARAMETER row in master table . There are two calls: one for parameters with numeric values and one for parameters with character string values. In each case, the arguments are the handle for the job, the parameter name, and the parameter's value. Parameters that are of particular importance for the present discussion are:\n\n","This call determines a maximum degree of parallelism that will be used in the job. During an export job, the number of dump files should be greater than or equal to the degree specified. The call has two arguments: the job handle and the degree of parallelism desired. Because this call determines the maximum number of resources the job may consume and thereby the speed with which the job is done, it may be termed herein the throttle for the job.","START_JOB ","This call begins or resumes execution of a job. The arguments are the handle for the job and a skip_current argument which is valid only for restarts of import jobs and which specifies that the work that was \u201cin progress\u201d when the job stopped be skipped. This is used to get around actions that cause fatal bugs in the import.","STOP_JOB .","This call stops execution of a job. The arguments are the handle for the job, an argument indicating whether the stop is to be immediate, an argument indicating whether master table  is to be kept after the job is stopped, and a delay time to wait before forcibly detaching other users that are attached to the job. If a stop is immediate, the worker processes that are involved in the job are terminated immediately; otherwise, they are allowed to complete the item they are currently working on before they stop.","OPEN ","The complete API specification for OPEN follows:",{"@attributes":{"id":"p-0111","num":"0143"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"77pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"140pt","align":"left"}}],"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["FUNCTION open",{}]},{"entry":["( \u2003operation","IN VARCHAR2,"]},{"entry":["\u2003\u2003mode","IN VARCHAR2,"]},{"entry":["\u2003\u2003remote_link","IN VARCHAR2 DEFAULT NULL,"]},{"entry":["\u2003\u2003job_name","IN VARCHAR2 DEFAULT NULL,"]},{"entry":["\u2003\u2003version","IN VARCHAR2 DEFAULT \u2018COMPATIBLE\u2019"]},{"entry":") RETURN NUMBER;"},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},"OPEN is used to declare a new job using the Data Pump API. The handle returned is used as a parameter for all other API calls except ATTACH.","Parameters",{"@attributes":{"id":"p-0113","num":"0000"},"ul":{"@attributes":{"id":"ul0015","list-style":"none"},"li":"operation\u2014the type of operation to be performed. The valid operation types are:"}},{"@attributes":{"id":"p-0114","num":"0146"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"168pt","align":"left"}}],"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}},{"entry":["Operation","Description"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["EXPORT","Saves data and metadata to a dump file set or obtain"]},{"entry":[{},"an estimate of the data size of an operation."]},{"entry":["IMPORT","Restores data and metadata from a dump file set or"]},{"entry":[{},"across a database link."]},{"entry":["SQL_FILE","Displays the metadata from a dump file set or across"]},{"entry":[{},"a database link as a SQL script. The location of the"]},{"entry":[{},"SQL script is specified through the ADD_FILE API."]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}},"ul":{"@attributes":{"id":"ul0016","list-style":"none"},"li":"mode\u2014the scope of the operation to be performed. The valid modes are:"}},{"@attributes":{"id":"p-0115","num":"0148"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"147pt","align":"left"}}],"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}},{"entry":["Mode","Description"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["FULL","Operates on the full database or full dump file"]},{"entry":[{},"set except for the SYS, XDB, ORDSYS, MDSYS,"]},{"entry":[{},"CTXSYS, ORDPLUGINS, and LBACSYS schemas"]},{"entry":[{},"and some TBD portion of the XDB schema."]},{"entry":["SCHEMA","Operates on a set of selected schemas. Defaults to"]},{"entry":[{},"the schema of the current user. All objects in the"]},{"entry":[{},"selected schemas are processed. Users cannot specify"]},{"entry":[{},"SYS, XDB, ORDSYS, MDSYS, CTXSYS, ORD-"]},{"entry":[{},"PLUGINS, or LBACSYS schemas for this mode."]},{"entry":["TABLE","Operates on a set of selected tables. Defaults to all of"]},{"entry":[{},"the tables in the current user's schema. Only tables"]},{"entry":[{},"and their dependent objects are processed."]},{"entry":["TABLESPACE","Operates on a set of selected tablespaces. No"]},{"entry":[{},"defaulting is performed. Tables which have storage"]},{"entry":[{},"in the specified tablespaces are processed parallel"]},{"entry":[{},"to what is done using a Table mode."]},{"entry":["TRANSPORTABLE","Operates on metadata for tables (and their dependent"]},{"entry":[{},"objects) within aset of selected tablespaces to"]},{"entry":[{},"perform a transportable tablespace export\/import."]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},"In the SCHEMA, TABLE, and TABLESPACE modes, the selected objects are specified by means of filter rows in master table . If specified as NULL, job_mode will default to FULL for Import and Sql_file operations. Specifying NULL for Export operations or networked Import operations will generate an error.\n\n","Returns an opaque handle for the job. This handle is used as input to the SET_PARALLEL, ADD_FILE, DETACH, STOP_JOB, GET_STATUS, LOG_ENTRY, METADATA_FILTER, DATA_FILTER, METADATA_TRANSFORM, METADATA_REMAP, SET_PARAMETER and START_JOB calls.","Usage: Creates a job that is to be executed by the Data Pump API. A master table will be created for the job under the caller's schema within the caller's default tablespace. A handle referencing the job is returned that attaches the current session to the job. Once attached to the handle, the handle will remain valid until a detach even if the job finishes. The handle is only valid in the caller's session. Other handles may be attached to the same job via the ATTACH API.","GET_STATUS ","The complete API specification for this call follows:",{"@attributes":{"id":"p-0120","num":"0159"},"tables":{"@attributes":{"id":"TABLE-US-00006","num":"00006"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"98pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"105pt","align":"left"}}],"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"FUNCTION get_status",{}]},{"entry":[{},"\u2003\u2003( \u2003handle","IN NUMBER,"]},{"entry":[{},"\u2003\u2003\u2003\u2003mask","IN BINARY_INTEGER,"]},{"entry":[{},"\u2003\u2003\u2003\u2003timeout","IN NUMBER DEFAULT NULL"]},{"entry":[{},"\u2003\u2003) RETURN ku$_Status;"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}}}},"There is also a procedural version of the GET_STATUS API which has two out values: ku$_status as above and job_state. The out values will be explained in the following.","GET_STATUS is used to monitor the status of a job or wait for the completion of a job.","Parameters:",{"@attributes":{"id":"p-0123","num":"0000"},"ul":{"@attributes":{"id":"ul0019","list-style":"none"},"li":["handle\u2014The handle of a job. The current session must have previously attached to the handle through an OPEN or ATTACH call.","mask\u2014A bit mask to tell the interface which of four kinds of information to return in the ku$_Status object returned by the function. The four kinds of information are:\n    \n    "]}},"Multiple types of information can be requested by ORing together any combination of the above values. The actual types of information returned will be in the MASK attribute of the returned ku$_Status.\n\n","Returns: A ku$_Status object is returned. The object looks like this:",{"@attributes":{"id":"p-0126","num":"0171"},"tables":{"@attributes":{"id":"TABLE-US-00007","num":"00007"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"196pt","align":"left"}}],"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"CREATE TYPE sys.ku$_Status AS OBJECT"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"77pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"119pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"(",{}]},{"entry":[{},"\u2003mask","NUMBER,\u2003\/* Indicates which"]},{"entry":[{},{},"status types are present*\/"]},{"entry":[{},"\u2003wip","ku$_LogEntry,\u2003\/* Work-In-"]},{"entry":[{},{},"Progress: std. exp\/imp msgs *\/"]},{"entry":[{},"\u2003job_description","ku$_JobDesc, \/*Complete job"]},{"entry":[{},{},"description *\/"]},{"entry":[{},"\u2003job_status","ku$_JobStatus,\u2003\/* Detailed job"]},{"entry":[{},{},"\u2003status + per-worker sts *\/"]},{"entry":[{},"\u2003error","ku$_LogEntry \/* Multi-level"]},{"entry":[{},{},"\u2003contextual errors *\/"]},{"entry":[{},")"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}]}}},"Out values: ku$_Status as above; job_state: current values of the fields in job state row  for the job.","Usage","GET_STATUS is used to monitor the progress of an ongoing job and receive error notification. Various kinds of information can be requested via the mask parameter. JOB_DESC and JOB_STATUS are classified as synchronous information in that they can be handled directly by the client's shadow process. On the other hand, WIP and ERROR are classified as asynchronous: the messages that embody these types of information can be generated at any time by various layers in the Data Pump architecture.\n\n","There are two types of error scenarios that are handled via GET_STATUS:\n\n","A client's main processing loop once the job is underway will most likely consist of a call to GET_STATUS with an infinite timeout (\u22121) \u201clistening\u201d for work-in-progress and error messages. If the user requested periodic status, then JOB_STATUS information will also be in the request with a fixed timeout value.","The ku$_Status is interpreted as follows in a preferred embodiment:\n\n","The primary task of Data Pump  is to move objects into or out of a database management system. Data Pump  may perform this task for a variety of clients, and in performing the task, Data Pump  takes advantages of data access services provided by other utilities in the Oracle data base management system in which Data Pump  is operating.  provides an overview of these clients and utilities and of their relationship to Data Pump .","Beginning with Data Pump clients , these clients may specify operations of Data Pump  by means of the Data Pump API described above. Among the clients are the new Export and Import utilities made possible by Data Pump , and clients  that are part of an Oracle DBMS system and make use of Data Pump  such as the Oracle Enterprise Manager, other Oracle utilities that use the API, and user application software that uses the API.","In moving objects, Data Pump  makes use of three Oracle utilities:\n\n","Data Pump  uses DBMS_METADATA package  to move all metadata objects. With other objects, Data Pump  uses the object's metadata to determine which utility will provide the most efficient access method for moving the object's data and uses that method for the object. Data Pump  accesses all of the utilities by means of the utilities' APIs.","DPAPI ","Continuing in more detail concerning the utilities, DPAPI  is the fastest single stream data movement technology provided in the Oracle DBMS; it is able to unload objects almost twice as fast as prior art export  and to load objects 15-40 times faster than prior art import  and almost twice as fast as the SQL*Loader. The DPAPI supports all Oracle data types except BFILES and opaque objects. DPAPI  cannot, however, be used in all situations. Moreover, DPAPI  does not take advantage of Data Pump 's ability to process the objects being moved in parallel (though it does permit parallel processing of different objects or partitions within an object). Further, where DML transformations are required, it is not as flexible as External Tables . For further details on DPAPI , see Oracle 9i Database Utilities, Release 2, March 2002, Part No. A96652-01, Chapter 9, \u201cConventional and direct path loads\u201d.","External Tables ","External Tables  will work for all cases, including those not handled by DPAPI . External Tables  has two components: Loader , which loads objects into a database management system but does not unload objects from a DBMS, and Stream , which does both. The preferred embodiment of data pump  uses Stream  for transfers to or from dump files. External tables  is very flexible. It can represent almost any external data source as an SQL row source, and once that is done, a load operation can be expressed merely as a\n\n",{"@attributes":{"id":"p-0138","num":"0194"},"tables":{"@attributes":{"id":"TABLE-US-00008","num":"00008"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"CREATE TABLE external_table . . . ORGANIZATION EXTERNAL"},{"entry":"AS SELECT . . . FROM internal_table."},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}}}},"Though External Tables' single stream performance is somewhat less than DPAPI , its relative slowness is made up for by the Data Pump's ability to process objects being moved in parallel. DPAPI  also allows unlimited DML transformations and filtering using SQL. The External Tables access method is also extensible: New access drivers can be written and plugged into the External Tables infrastructure to support new data sources. Data Pump  takes advantage of External Tables' extensibility by employing a new access driver that unloads and loads tables using DPAPI 's high performance, binary stream format. This allows DPAPI  to load streams written by External Tables  and vice-versa for maximum flexibility. For more details on External Tables , see Chapter 11 of the Utilities documentation cited above.","Metadata Package ","All metadata operations within the Data Pump are handled by DBMS_METADATA package . DBMS_METADATA package  provides an intuitive interface for the extraction and transformation of all database object definitions as either XML (if transformations are to be done in a downstream process as will be the case with export\/import) or creation DDL. SQL DDL is generated by transforming the XML documents using native database XML\/XSL services and XSL-T stylesheets stored in the dictionary. The package allows any number of transformations to be specified at both extraction and creation time. It also supports a number of canned transforms for DDL generation such as suppress storage clauses, suppress constraint definitions, etc. As used by Data Pump , DBMS_METADATA package  includes the following enhancements:\n\n","For further details on DBMS_METADATA package , see Chapter 15 of the Utilities documentation cited above. See also U.S. Ser. No. 09\/672,914, George Claborn, et al., Aggregating and manipulating dictionary metadata in a database system, filed Sep. 28, 2000, and U.S. Ser. No. 10\/014,038, Lee B. Barton, System having heterogenous object types, filed Oct. 10, 2001.","Detailed Architecture of a Preferred Embodiment of Data Pump : ",{"@attributes":{"id":"p-0142","num":"0202"},"figref":"FIG. 23","b":["2203","317","323","323","2203"],"i":"a "},"A Master Control Process (MCP)  is initiated for each Data Pump operation. The MCP is responsible for work sequencing, worker process management, communication with attached clients and dump file\/log file management. In defining state  of the operation, MCP  creates master table  and places entries in master table . In response to a START_JOB call, MCP  creates the files in dump file set  and then creates a first tier pool of worker processes . The worker processes actually perform the unloads and loads of data (sometimes via parallel slaves ) and metadata as well as update the Master Table as they complete work requests. The maximum number of worker processes and\/or slave processes is determined by the degree of parallelism specified using the SET_PARALLEL call, but the degree of parallelism should in general not exceed the number of dump files. Here, there are four dump files, the specified degree is 4, and there are four processes writing to the dump file set: worker process (), worker process (), and worker process ()'s two slave processes () and (). Since the maximum degree of parallelism is 4, worker process () is not in use. In the following, worker or slave processes that are writing to or reading from a dump file set or collecting the data to make the size estimate are termed active worker or slave processes. Each active worker or slave process loads objects to or unloads objects from a separate dump file (), and the active worker or slave processes can thus operate across tables, partitions and metadata in parallel.","For data movement, MCP  assigns each worker a partition or table (if unpartitioned) to load to the worker process's dump file. The worker process initially enumerates the table it is to load or unload and decides how the table should be loaded based upon the data types of its objects. It assigns one of four methods to the table: directpath, externaltable, either, or neither. Next the worker refines the assignment by considering the access type specified by the DATA_ACCESS_METHOD parameter and whether master table  has specified a filter for the object which involves a query clause. If such a filter has been specified, the external table method must be used. The refinement may cause the choice to be left at either or forced into a specific access method or to be set to neither if the requirements of the data type conflict layers conflict with the requirements of the DATA_ACCESS_METHOD parameter and the requirement for a query clause.","Finally, when the MCP generates the request to load\/unload the object's data segment, it does not make the request if neither access method is appropriate. It preserves the access method if a specific access method is requested. If either was specified, it chooses the method that will lead to the earliest completion of the job: Normally, direct path (which is faster) is chosen. However, if a TABLE_DATA object is sufficiently large and a high degree of parallelism is enabled, the MCP will choose external table as the method. The MCP code for this final decision is:",{"@attributes":{"id":"p-0146","num":"0206"},"tables":{"@attributes":{"id":"TABLE-US-00009","num":"00009"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"203pt","align":"left"}}],"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"IF (datasize < ((2* v_remaining_data) \/ v_degree))"]},{"entry":[{},"\u2003\u2003OR datasize < SCHEDULING_CLUMP)"]},{"entry":[{},"\u2003THEN ebug_log_text (\u2018Direct path selected for next"]},{"entry":[{},"\u2003\u2003work item\u2019) ;"]},{"entry":[{},"\u2003\u2003method := KUPCC.MT_UNLOAD_DIRECT_PATH;"]},{"entry":[{},"\u2003\u2003parallel := 1;"]},{"entry":[{},"\u2003ELSE"]},{"entry":[{},"\u2003\u2003debug_log_text (\u2018External table selected for"]},{"entry":[{},"\u2003\u2003next work item\u2019) ;"]},{"entry":[{},"\u2003\u2003method := KUPCC.MT_UNLOAD_EXTERNAL_TABLE;"]},{"entry":[{},"END IF;"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},"If MCP  chooses External Tables, worker process () may select from a pool of parallel slave processes  that perform parallel query or DML operations. This provides a second tier of intra-partition parallelism within the Data Pump.","All communication between cooperating processes (client shadow processes, MCP, workers, parallel slaves) takes place in queues  and  that use the Advanced Queues technology of the Oracle 9i database management system. Advanced queues are asynchronous, have point-to-point and multicast capabilities, and work with clustered database management systems.","File operations are done in the Worker and parallel slave processes. Both worker and parallel slave processes will dynamically request files from the Data Pump's file management services. The file managers for these processes communicate with their counterpart in the MCP for centralized file synchronization. File assignment is \u201csticky\u201d\u2014a worker process does not relinquish a file unless requested by the file manager. Data Pump 's support for multiple directory specifications and file wildcarding permits easy creation and distribution of the dump file set across any I\/O configuration.","Continuing in more detail with the operation of DBMS_DATAPUMP  as shown in , Client A () opens a session with DBMS . Opening the session creates the client's shadow process (). It is shadow process () that makes the client's calls to Data Pump . To start the export operation, Client A's shadow process makes an OPEN call to Data Pump  to start an export operation. Data Pump  responds to the OPEN call by setting up queues  and  and master table  and then starting MCP , which initializes master table  and returns a handle identifying this session's access to the export operation. Continuing with queues  and , queue  is the command and control queue. All server processes comprising a Data Pump job (client shadow , MCP , workers  and parallel slaves ) subscribe to this queue. The shadow processes send API requests to the MCP on this queue and the MCP in turn controls all the worker process via this queue. This is a bi-directional queue and the only one the MCP listens on. Status queue  is used to send status, logging and error information to shadow processes  that have clients attached to them. This queue is strictly unidirectional: The MCP posts status and errors and the client shadow processes consume them.","In defining state , client () makes calls to various DBMS_DATAPUMP methods like SET_FILTER, SET_PARAMETER and ADD_FILE to establish parameter values like the maximum number of parallel streams, which files to use as dump files, schemas\/tables to export, etc. Once all the parameters and filters are established, the client calls START_JOB, which begins execution of the job. In this example, client () has set the degree of parallelism to 4 and has set up four dump files (. . . d) accordingly. MCP  starts four worker processes which will subscribe to the command and control queue. MCP  will at this time also create the four initial dump files of dump file set . As more space is needed as the job progresses, the dump files are extended. If more dump files are needed because no extensions are possible or because the DBA increased the maximum degree of parallelism, these files will automatically be created if wildcard template file specifications were provided by the client. In some embodiments, the user may be prompted for additional files as needed. Each file in the dump file set may hold all three kinds of required information: data from ordinary objects, XML representations of metadata objects, and directory\/control information in the dump file header.","Once the worker processes  have been initialized and have sent messages to MCP  by queue  requesting something to do, MCP  directs one of the worker processes to initiate retrieval of metadata for one of the object types. The selected worker process will do so using heterogeneous object support within DBMS_METADATA package . All metadata will be written to the dump file set as XML. This facilitates import's ability to effect transformations on the metadata prior to generating creation DDL. Object retrieval begins with the metadata table for the objects type for two reasons:\n\n","As the metadata is retrieved, the worker process () retrieving the metadata makes an object row  for each object described in the metadata. The object row  specifies the order in which the object is to be processed, location and size information about the object, and information about the granularity of the data in the object. When a worker process () writes an object to worker process ()'s dump file () in dump file set , it writes the object to the dump file and then writes the object's object row .","As mentioned above, if the degree of parallelism specified in master table  is greater than one, fetching metadata and unloading data belonging to TABLE_DATA objects proceed in parallel. The total number of active worker processes plus active slave processes must always be no greater than the maximum degree of parallelism currently specified for the job. Each process that is operating in parallel writes to a separate dump file If wild cards have been specified for the job, Data Pump  creates new files as needed to achieve the maximum parallelism currently specified.","Unloads of TABLE_DATA objects in the export operation are ordered by size with the largest tables\/partitions being unloaded first. If the DATA_ACCESS_METHOD parameter has the value \u201cautomatic\u201d, MCP  determines on a per-partition basis which data access method to use for the TABLE_DATA object, DPAPI  or External Tables . Multiple partitions may be sent in a single work item to a worker process in order to provide a significant chunk of work per request. MCP  sends the unload request to the next available worker process unless that would cause Data Pump  to exceed the degree of parallelism currently specified for the job. The worker process uses the data access method specified by MCP  to make the request. If External Tables is the chosen access method, a DDL statement such as the following will be generated to initiate the external table unload:",{"@attributes":{"id":"p-0156","num":"0218"},"tables":{"@attributes":{"id":"TABLE-US-00010","num":"00010"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"CREATE TABLE foo_ext (. . .) ORGANIZATION EXTERNAL . . . AS"},{"entry":"SELECT . . . FROM foo_int"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}}}},"If the degree of parallelism specified by the throttle has not yet been exceeded, a worker process that employs the External Tables access method may use as many slave processes as the degree of parallelism permits to process the object in parallel.","In , worker process () is unloading a TABLE_DATA object using External Tables . The resources available in the exporting system permit worker process () to use two slave processes (and b) to do the unloading. Worker process () has taken the two slave processes from the pool of slave processes maintained by MCP  and is coordinating their activities. While this is going on, worker process () is continuing to unload metadata and worker process () is unloading another TABLE_DATA object using Direct Path API . Since four streams of unload are currently active (the maximum currently permitted by the throttle), worker process D is temporarily idle. Regardless of which unload method is used, the worker process () or one of its slave process  puts each chunk of row data into DPAPI stream format and unloads it into the worker process's dump file () at a file and byte offset provided by the file management layer of MCP . Only one worker or slave process writes to a given dump file at a time. If a file fills, the process obtains a file extension from MCP 's file manager. If the file is not extendible, the file manager will return a new file to the worker if it is able to do so. If not, Data Pump  stops the job. Once the problem has been solved, the job may be restarted as described above. The worker processes  and the master control process  communicate via queue . Messages sent via the queue post progress, request additional file space, report errors, and so forth. MCP  writes work-in-progress information to queue , where it is available to shadow processes . Each worker process writes its status information to the worker process's row  in master table . A shadow process  may make a GET_STATUS call on behalf of its client  to retrieve this information from master table .","Parallelism in Import Jobs that use Dump Files","The use of parallelism in import jobs from dump files is limited by the fact that an object must be created in the DBMS  into which the object is being imported before the object's data can be loaded. In such an import job, Data Pump  simply works through master table 's object rows  in the order specified by fields , , and . Since that is so, objects belonging to a given type are processed together and TABLE_DATA objects are processed by decreasing size. For each object, the metadata for the object's type in the dump files is used to make the creation DDL for the object, and that is used to create the object in DBMS . When the object has been created in DBMS , the data for the object is copied from the dump file to the object in DBMS . Where the benefits of parallelism outweigh the costs of setting it up, parallel processing may be used to copy the data for large objects from dump file set  to DBMS . The fact that the method used to unload a TABLE_DATA object is recorded in field  of its object row  makes determination of the benefits of parallelism easier.","Parallelism with Remote Sources of Objects","As described with regard to , where the source of objects for an export operation or a fileless import operation is remote from the DBMS upon which Data Pump  is executing, the metadata for each object to be fetched from the source is first obtained from the source and then used to make a query to the source which fetches the object's data from the source. In the case of export, the object's metadata or data is written to dump file set . In the case of fileless import, the object's metadata is used to create the object in the DBMS upon which Data Pump  is executing and then the object's data is written to the newly-created object. Parallelism may be achieved in both cases by means of a number of worker processes. Each worker process has its own link to the remote source and retrieves objects via the link as just described. Here, too, of course, a worker process may use slave processes where it makes sense to write an object's data to the dump file set or to the destination DBMS.","Advantages of Master Table ","As is apparent from the foregoing, the ease of use, flexibility, and speed of Data Pump  are greatly enhanced by the use of master table  in DBMS  in which Data Pump  is executing to represent the job. As a table in DBMS , master table  provides a persistent representation of the job upon which any operation that can be done on a table in DBMS  can be performed. Master table  includes the following information:\n\n","As a persistent description of the job, master table  is available to define the job before it starts executing, while the job is executing, while the job is stopped, and after it is completed. It is independent of the session in which it was created and interested clients may use master table  to access information about the job at any time during its existence. The persistence of mater table  and the information it contains thus make the following possible:\n\n","Because master table  is a database table in the DBMS doing the job, the following is possible:\n\n","The foregoing Detailed Description has disclosed to those skilled in the relevant technologies how to make and use a system for transferring objects between DBMS's that operates under a control database object in the DBMS performing the transfer and has further disclosed the best mode presently known to the inventors of making and using their system. It will be immediately apparent to those skilled in the relevant technologies that the techniques disclosed herein are not restricted to the particular relational database management system in which they are implemented or indeed to relational database management systems at all, but can be used in any kind of database management system. It will be further apparent that many of the particular techniques used in implementing the system are determined by characteristics of the Oracle 9i database system such as the utilities available for retrieving metadata and moving objects, and that even in the Oracle 9i environment, many other ways of implementing the system are possible. Since that is the case, the Detailed Description is to be regarded as being in all respects exemplary and not restrictive, and the breadth of the invention disclosed here in is to be determined not from the Detailed Description, but rather from the claims as interpreted with the full breadth permitted by the patent laws."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWING","p":[{"@attributes":{"id":"p-0024","num":"0030"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0025","num":"0031"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0026","num":"0032"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0027","num":"0033"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0028","num":"0034"},"figref":"FIG. 5","b":"321"},{"@attributes":{"id":"p-0029","num":"0035"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0030","num":"0036"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0031","num":"0037"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0032","num":"0038"},"figref":"FIG. 9","b":"323","i":"n"},{"@attributes":{"id":"p-0033","num":"0039"},"figref":"FIGS. 10-18","b":"321"},{"@attributes":{"id":"p-0034","num":"0040"},"figref":"FIG. 19"},{"@attributes":{"id":"p-0035","num":"0041"},"figref":"FIG. 20"},{"@attributes":{"id":"p-0036","num":"0042"},"figref":"FIG. 21"},{"@attributes":{"id":"p-0037","num":"0043"},"figref":"FIG. 22","b":"311"},{"@attributes":{"id":"p-0038","num":"0044"},"figref":"FIG. 23","b":"311"},{"@attributes":{"id":"p-0039","num":"0045"},"figref":"FIG. 24"}]},"DETDESC":[{},{}]}
