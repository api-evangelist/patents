---
title: Method of controlling view of stereoscopic image and stereoscopic image display using the same
abstract: Provided is a method of controlling view of a stereoscopic image. The method includes changing the view angle and depth information of a 3D image displayed on a display according to position information for each user; and combining at least two of a spatial division technique spatially dividing light from the display, a time division technique temporally dividing light from the display and a polarization division technique dividing light from the display into lights having polarization characteristics to divide the 3D image for the each user by using the combined technique and producing a binocular disparity of for the each user.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08537206&OS=08537206&RS=08537206
owner: LG Display Co., Ltd.
number: 08537206
owner_city: Seoul
owner_country: KR
publication_date: 20091110
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"p":["This application claims the benefit of Korea Patent Application No. 10-2009-0010889, filed on Feb. 11, 2009, the entire contents of which is incorporated herein by reference for all purposes as if fully set forth herein.","1. Field of the Invention","This disclosure relates to a method of controlling view of a stereoscopic image and a stereoscopic image display using the same.","2. Discussion of the Related Art","Stereoscopic image displays are classified into displays using a stereoscopic technique and displays using an autostereoscopic technique.","The stereoscopic technique uses a disparity image between the left and right eyes, which has large 3D effect and includes a technique using glasses and a technique using no glasses. The technique using glasses changes polarization directions of left and right disparity images and displays the left and right disparity images on a direct view display or a projector or displays the left and right disparity images on the direct view display or the projector using a time division method. The technique using glasses produces a stereoscopic image using polarizing glasses or liquid crystal shutter glasses. The technique using no glasses locates an optical plate such as a parallax barrier for separating optical axes of left and right disparity images from each other in front of or behind a display screen.","The technique using glasses alternately displays left-eye and right-eye images on a display and switches characteristic of polarized light incident on polarizing glasses. Accordingly, the technique using glasses can time-divide the left-eye and right-eye images to produce a stereoscopic image without deteriorating resolution.","Recently, the use of 3D (three dimensional) contents using stereoscopic images has been diversified into 3D games, 3D advertisements, 3D movies, etc. To extend the application fields and contents of stereoscopic images, there is a need to adaptively control the view angle and depth of a stereoscopic image according to a motion of a user.","A method of controlling view of a stereoscopic image includes detecting position information of each user and changing view angle and depth information of a 3D image displayed on a display according to the position information; and combining at least two of a spatial division technique spatially dividing light from the display, a time division technique temporally dividing light from the display and a polarization division technique dividing light from the display into lights having polarization characteristics which are different from one another to divide the 3D image for the each user by using the combined technique, and producing a binocular disparity of the each user.","According to another aspect of this disclosure, a stereoscopic image display device includes a controller that changes the view angle and depth information of a 3D image displayed on a display according to position information for each user; and a 3D driving element that divides the 3D image for respective users by using at least two of a spatial division technique spatially dividing light from the display, a time division technique temporally dividing light from the display and a polarization division technique dividing light from the display into lights having polarization characteristics which are different from one another, and produces a binocular disparity of the each user.","Embodiments of the invention will be explained in detail with reference to ","Referring to , a method of controlling view of a stereoscopic image according to an embodiment of the invention simultaneously acquires position information of a plurality of users (or observers) based on images obtained by an image sensor or outputs of an infrared sensor in step S. The image sensor may be a camera. Position information of a user corresponds to 3D position information including X-axis position information FaceX of the face of the user, Y-axis position information FaceY of the user face, and a distance (or Z-axis position information Dist) between the user and the stereoscopic image display.","The method of controlling view of a stereoscopic image according to embodiment of the invention adjusts parameters for rendering view angles and depth information of left-eye and right-eye images displayed on the stereoscopic image display according to 3D position information of each user in step S. A rendering parameter FaceXL of the left-eye image is \u201cFaceXL=FaceX-offset\/2\u201d, which is determined according to 3D position information of the face of each user. A rendering parameter FaceXR of the right-eye image is \u201cFaceXR=FaceX+offset\/2\u201d, which is determined according to the 3D position information of the face of each user. Here, \u201coffset\u201d corresponds to a distance between the left eye and the right eye of a user, that is, a distance between a camera for generating the left-eye image and a camera for generating the right-eye image. X-axis related parameters of the left-eye and right-eye images can be varied according to 3D position information of each user and Y-axis and Z-axis related parameters of the left-eye and right-eye images can be varied in real time according to Y-axis and Z-axis position information of each user.","The method of controlling view of a stereoscopic image according to the embodiment of the invention generates left-eye and right-eye images according to left-eye and right eye image rendering parameters and position information of each user and displays the left-eye and right-eye images on the stereoscopic image display in steps S, S and S. Views of the left-eye and right-eye images displayed on the stereoscopic image display are changed in real time according to the position of each user because the rendering parameters of the left-eye and right-eye images are updated whenever the position of each user is changed. Accordingly, the stereoscopic image display according to the invention can generate the view angle and depth information of a stereoscopic image, which are varied according to the position of each user, in real time and provide an unlimited stereoscopic image views to each user.",{"@attributes":{"id":"p-0033","num":"0032"},"figref":["FIG. 2","FIG. 1","FIG. 3","FIG. 4"],"b":["1","10","10","11","10"]},"Referring to ,  and , the stereoscopic image display  includes a display screen for displaying left-eye and right-eye images, a frame surrounding the display screen, and the camera  attached to the frame. When a user is located in a view angle of the camera , an image captured by the camera  includes a user image. The camera capture image illustrated in  includes a pixel matrix having a size of CW(px)\u00d7CH(px) determined according to the resolution of the camera.","The method of controlling view of a stereoscopic image analyzes the camera capture image of the current frame, as illustrated in , to determine whether position information of the user, calculated in a previous frame, exists in the camera capture image. When the position information calculated in the previous frame exists in the camera capture image of the current frame, the method of controlling view of a stereoscopic image sets a user face detection range to a region around the position of the user, calculated in the previous frame, in steps S, S and S. When the position information calculated in the previous frame does not exist in the camera capture image of the current frame, the method of controlling view of a stereoscopic image sets the user face detection range to the entire camera capture image of the current frame in steps S, S and S.","The method of controlling view of a stereoscopic image detects the face of the user within the user face detection range set in step S or S by using a known face detection algorithm to extract XYZ 3D position information FaceX, FaceY and Dist corresponding to the user face from the camera capture image in steps S and S. Although \u201cface detection algorithm of Viola & Jones\u201d may be used as the face detection algorithm, the face detection algorithm is not limited thereto and any face detection algorithm can be applied. The face detection algorithm of Viola & Jones is represented by Equations 1, 2, 3 and 4. In Equations 1, 2, 3 and 4, parameters FW(mm), SH(mm), \u03b8(\u00b0), CW(px) and CH(px) are constants determined by the stereoscopic image display , the camera  and the camera capture image illustrated in  and DW(px), and DC(px) calculated from the camera capture image are variables calculated in real time according to a motion of a user by the face detection algorithm of Viola & Jones. Herein, FW denotes the face width of the user, SH denotes the screen height of the stereoscopic image display , \u03b8 represents the angle at which the user watches a stereoscopic image, CW represents the width of the camera capture image, CH denotes the height of the camera capture image, DW represents a detected width of the user face, which is detected from the camera capture image, and DC represents a detected center of the user face, which is detected from the camera capture image.",{"@attributes":{"id":"p-0037","num":"0036"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"RPP","mo":"=","mrow":{"mfrac":{"mi":"\u03c0","mn":"360"},"mo":["\u00b7","\u2062","\u00b7"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":["\u03b8","CW"]}}},{"mrow":{"mo":["[","]"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mrow":{"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mo":"\u2062"},"mn":"1"}}}]}}}}},"Here, RPP denotes \u201cRadians per pixel\u201d and CW represents the width of a camera capture image.",{"@attributes":{"id":"p-0039","num":"0038"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"Dist","mo":"=","mfrac":{"mrow":[{"mo":["(",")"],"mfrac":{"mi":"FW","mn":"2"}},{"mi":"tan","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"RPP","mo":"\u00b7","mfrac":{"mi":"DW","mn":"2"}}}}]}}},{"mrow":{"mo":["[","]"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mrow":{"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mo":"\u2062"},"mn":"2"}}}]}}}}},"Here, Dist denotes a face distance between the stereoscopic image display  and the user in .",{"@attributes":{"id":"p-0041","num":"0040"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":["Face","X"],"mo":"\u2062"},{"mrow":{"mi":"sin","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"RPP","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["DC","X"],"mo":"."},"mo":"-","mfrac":{"mi":"CW","mn":"2"}}}}}},"mo":"\u00b7","mi":"Dist"}],"mo":"="}},{"mrow":{"mo":["[","]"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mrow":{"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mo":"\u2062"},"mn":"3"}}}]}}}}},"Here, FaceX represents the position of the user face on the X axis in  and DC.X denotes the X-axis pixel position of the center of the user face detected from the camera capture image.",{"@attributes":{"id":"p-0043","num":"0042"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"FaceY","mo":"=","mrow":{"mrow":{"mrow":{"mi":"sin","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"RPP","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["DC","Y"],"mo":"."},"mo":"-","mfrac":{"mi":"CH","mn":"2"}}}}}},"mo":"\u00b7","mi":"Dist"},"mo":"-","mfrac":{"mi":"SH","mn":"2"}}}},{"mrow":{"mo":["[","]"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mrow":{"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mo":"\u2062"},"mn":"4"}}}]}}}}},"Here, FaceY represents the position of the user face on the Y axis in  and DC.Y denotes the Y-axis pixel position of the center of the user face, which is detected from the camera capture image.","If detection of the user face from the camera capture image fails in steps S through S, the method of controlling view of a stereoscopic image recognizes a region including a widest portion in a skin color in the camera capture image as the user face and re-performs steps S through S to detect the user face and extract XYZ 3D position information FaceX, FaceY and Dist in steps S and S.","The method of controlling view of a stereoscopic image averages 3D position information FaceX, FaceY and Dist of the user face facing the stereoscopic image display , which is extracted by repeating steps S, S and S for a predetermined frame period, for example, a period corresponding to several tens of frames in step S. This is for the purpose of smoothing user position information on the time base to determine 3D position information of the final user face because the user position information may slightly change according to minute noise included in the camera capture image even when the user does not move.",{"@attributes":{"id":"p-0047","num":"0046"},"figref":["FIGS. 5 and 6","FIG. 1"],"b":"1"},"Referring to , the stereoscopic image display  includes a display screen displaying left-eye and right-eye images, a frame surrounding the display screen, and an infrared sensor  attached to the frame. A user wears 3D polarizing glasses  having left and right infrared ray emitting devices L and R. When the user who is wearing the 3D polarizing glasses  is located in the sensing range of the infrared sensor , the infrared sensor  receives infrared rays from the left and right infrared ray emitting devices L and R of the 3D polarizing glasses . The infrared sensor  includes a sensor matrix having a size of IRW(px)\u00d7IRH(px) determined according to sensor resolution, as illustrated in .","The method of controlling view of a stereoscopic image detects infrared receiving points DP and DP from the output of the infrared sensor , as illustrated in , in step S of  and calculates the detected width DW of the user face, XY coordinates DC.X and DC.Y of the center point of the user face and 3D position information FaceX, FaceY and Dist of the user face by using Equations 5 through 11.\n\n=\u221a{square root over ((12)+(12))}{square root over ((12)+(12))}\u2003\u2003[Equation 5]\n","Here, DW denotes a distance between the left and right infrared ray emitting devices L and R, which is detected from the infrared sensor .",{"@attributes":{"id":"p-0051","num":"0050"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":["DC","X"],"mo":"."},"mo":"=","mfrac":{"mrow":{"mrow":[{"mi":["DP","X"],"mo":["\u2062","\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"1."},{"mi":["DP","X"],"mo":["\u2062","\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"2."}],"mo":"+"},"mn":"2"}}},{"mrow":{"mo":["[","]"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mrow":{"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mo":"\u2062"},"mn":"6"}}}]}}}}},"Here, DC.X denotes the coordinate value of the center point of the user face on the X-axis.",{"@attributes":{"id":"p-0053","num":"0052"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":["DC","Y"],"mo":"."},"mo":"=","mfrac":{"mrow":{"mrow":[{"mi":["DP","Y"],"mo":["\u2062","\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"1."},{"mi":["DP","Y"],"mo":["\u2062","\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"2."}],"mo":"+"},"mn":"2"}}},{"mrow":{"mo":["[","]"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mrow":{"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mo":"\u2062"},"mn":"7"}}}]}}}}},"Here, DC.Y represents the coordinate value of the center point of the user face on the Y axis.","[Equation 8]",{"@attributes":{"id":"p-0055","num":"0054"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["RPP","IR"]},"mo":"=","mrow":{"mrow":{"mfrac":{"mi":"\u03c0","mn":"360"},"mo":["\u00b7","\u2062","\u00b7"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":["\u03b8","E"]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"IRW"}}},{"mrow":{"mo":["[","]"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mrow":{"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mo":"\u2062"},"mn":"8"}}}]}}}}},"Here, RPPrepresents \u201cRadians per pixel\u201d and IRW denotes the width of the infrared sensor output.","[Equation 9]",{"@attributes":{"id":"p-0057","num":"0056"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"Dist","mo":"=","mfrac":{"mrow":[{"mo":["(",")"],"mfrac":{"mi":"FW","mn":"2"}},{"mi":"tan","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["RPP","IR"]},"mo":"\u00b7","mfrac":{"mi":"DW","mn":"2"}}}}]}}},{"mrow":{"mo":["[","]"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mrow":{"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mo":"\u2062"},"mn":"9"}}}]}}}}},"Here, Dist denotes the face distance between the stereoscopic image display  and the user in .","[Equation 10]",{"@attributes":{"id":"p-0059","num":"0058"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":["Face","X"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mrow":{"mi":"sin","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["RPP","IR"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["DC","X"],"mo":"."},"mo":"-","mfrac":{"mi":"IRW","mn":"2"}}}}}},"mo":"\u00b7","mi":"Dist"}],"mo":"="}},{"mrow":{"mo":["[","]"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mrow":{"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mo":"\u2062"},"mn":"10"}}}]}}}}},"Here, FaceX denotes the X-axis position of the user face in  and DC.X represents the X-axis coordinate value of the center of the user face, which is detected from the infrared sensor output.","[Equation 11]",{"@attributes":{"id":"p-0061","num":"0060"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"FaceY","mo":"=","mrow":{"mrow":{"mrow":{"mi":"sin","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["RPP","IR"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["DC","Y"],"mo":"."},"mo":"-","mfrac":{"mi":"IRH","mn":"2"}}}}}},"mo":"\u00b7","mi":"Dist"},"mo":"-","mfrac":{"mi":"SH","mn":"2"}}}},{"mrow":{"mo":["[","]"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mrow":{"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mo":"\u2062"},"mn":"11"}}}]}}}}},"Here, FaceY denotes the Y-axis position of the user face in  and DC.Y represents the Y-axis coordinate value of the center of the user face, which is detected from the camera capture image. IRW denotes the width of the infrared sensor output and SH represents the screen height of the display screen of the stereoscopic image display .",{"@attributes":{"id":"p-0063","num":"0062"},"figref":["FIG. 7","FIG. 5"],"b":["22","22","21"]},"Referring to , each of the infrared ray emitting devices L and R includes an infrared light emitting diode (LED) module , a switch element  and a battery . The infrared LED module  includes one or more infrared LEDs connected in series or in parallel. The switch element  switches a current path between the infrared LED module  and the battery  according to an operation of a user.","Although  illustrate a single user, position information of a plurality of users can be simultaneously sensed by using cameras or infrared sensors.","The method of controlling view of a stereoscopic image renders left-eye and right-eye images using the 3D position information of the user, calculated in steps S and S, as rendering parameters, as illustrated in . The method of controlling view of a stereoscopic image of the invention does not store images of a 3D object at every angle and generates left-eye and right-eye images obtained when left-eye and right-eye cameras CAM and CAM, which are located at a distance corresponding to the offset between the left and right eyes of the user from each other, face the 3D object by using 3D modeling application programming interface (API), such as OpenGL and Direc3D. The invention adjusts the left-eye camera position and the right-eye camera position at a distance corresponding to the offset of the left and right eyes of the user from each other, which face the same 3D object, that is, parameters determining view angles and depth information of left-eye and right-eye images, in real time using the API according to the 3D position information of each user, which is calculated in steps S and S.","The stereoscopic image display of the invention renders the left-eye and right-eye images according to the rendering parameters varied according to the 3D position information of each user, separately displays the left-eye and right-eye images on the stereoscopic image display  and adjusts the view angle and depth information of a stereoscopic image. Consequently, the stereoscopic image display of the invention varies rendering of the left-eye and right-eye images in real time according to a motion of each user to produce a stereoscopic image that can be seen as an unlimited number of views according to a motion of each user.","Referring to , when a user moves from the front of the stereoscopic image display  to the left, the 3D position information of the user is changed. Accordingly, left-eye and right-eye images displayed on the stereoscopic image display  are rendered according to the changed 3D position information of the user, and thus the user can watch a 3D object image having a view angle and depth information which are moved from the center of  to the left as a stereoscopic image. When the user moves from the front of the stereoscopic image display  to the right, as illustrated in , the 3D position information of the user is changed. Accordingly, the left-eye and right-eye images displayed on the stereoscopic image display  are rendered according to the changed 3D position information of the user, and thus the user can watch a 3D object image having a view angle and depth information which are moved from the center of  to the right as a stereoscopic image.","The invention detects the position of each user in real time using the user position detection algorithm and adjusts rendering parameters of a 3D object image according to 3D position information of each user. Furthermore, the invention combines a spatial division 3D technique (various parallel barrier or lens techniques), a time division 3D technique and a polarization division 3D technique (polarizing glasses type) to separate 3D images of users, as illustrated in . Since 3D images seen to respective users are separated according to the spatial division, time division or polarization division technique, each user can watch a 3D image having a view angle and depth information which are adjusted in real time according to the position of each user.","Referring to , a stereoscopic image display according to a first embodiment of the invention includes a display , a dynamic barrier , a pattern retarder , a plurality of polarizing glasses  and , and a 3D controller . The dynamic barrier , the pattern retarder  and the polarizing glasses  and  are 3D driving elements which separate 3D images for respective users and produce binocular disparities of the respective users.","The display  may be a liquid crystal display, a field emission display, a plasma display panel, or an electroluminescence device (EL) including an inorganic electroluminescence device and an organic light emitting diode (OLED). When the display  is a liquid crystal display, a polarizer  is arranged between the display unit  and the dynamic barrier . The display  displays a 2D image in a 2D mode and displays a 3D image rendered according to a user position under the control of the 3D controller  in a 3D mode.","The dynamic barrier  may be implemented as a liquid crystal barrier having two transparent substrates on which electrodes are formed and a liquid crystal layer formed between the transparent substrates or a liquid crystal lens. The dynamic barrier  electrically controls liquid crystal molecules under the control of the 3D controller  to spatially divide lights of left-eye and right-eye images.  illustrates an example of the operation of the dynamic barrier . The dynamic barrier  can electrically control liquid crystal molecules to move positions of a transmitting part transmitting light and a blocking part blocking light in the horizontal direction. Accordingly, the dynamic barrier  spatially divides lights incident on the left and right eyes of each user to produce a binocular disparity of each user.","The pattern retarder  includes first and second retarders having different light absorption axes and splits a 3D image into polarized lights for each user. The first retarder is formed on an odd-numbered line of the pattern retarder  and transmits a first polarized light (linearly polarized light or circularly polarized light) of lights input through the dynamic barrier . The second retarder is formed on an even-numbered line of the pattern retarder  and transmits a second polarized light (linearly polarized light or circularly polarized light) of the lights input through the dynamic barrier . In , the first retarder is implemented as a polarization filter which transmits right-circularly polarized light and the second retarder is implemented as a polarization filter which transmits left-circularly polarized light.","The polarizing glasses  and  have different light absorption axes according to polarized lights projected from the pattern retarder . For example, the first polarizing glasses  worn by a first user transmits the right-circularly polarized light received from the first retarder of the pattern retarder  and blocks other polarized components. The left and right glasses of the first polarizing glasses  include right-circular polarizing filters. The second polarizing glasses  worn by a second user transmits the left-circularly polarized light received from the second retarder of the pattern retarder  and blocks other polarized components. The left and right glasses of the second polarizing glasses  include left-circular polarizing filters.","The 3D controller  acquires 3D position information of each user from a camera or an infrared sensor by using the aforementioned user position detection algorithm. In addition, the 3D controller  adjusts rendering parameters of left-eye and right-eye images of a 3D image and controls the dynamic barrier  according to the 3D position information of each user.","The stereoscopic image display illustrated in  can separate users according to polarization division using the pattern retarder  and the polarizing glasses  and  and produce a disparity of each user according to spatial division using the dynamic barrier . The stereoscopic image display detects the position of each user by using the camera or the infrared sensor, adjusts the view angle and depth information of a 3D image and controls the dynamic barrier  to achieve multi-user tracking. Accordingly, users can differently watch a 3D object image displayed on the stereoscopic image display at different view angles and watch a 3D object having depth information varied according to positions of the users, as illustrated in .",{"@attributes":{"id":"p-0077","num":"0076"},"figref":"FIG. 13"},"Referring to , the stereoscopic image display according to the second embodiment of the invention includes the display , the polarizer , the dynamic barrier , a dynamic retarder , a plurality of polarizing glasses  and , and a 3D controller . The dynamic barrier , the dynamic retarder  and the polarizing glasses  and  are 3D driving elements which separate 3D images for respective users and produce a binocular disparity of each user.","The display  displays a 2D image in the 2D mode and displays a 3D image rendered according to the position of a user under the control of the 3D controller  in the 3D mode.","The dynamic barrier  may be implemented as a liquid crystal barrier having two transparent substrates on which electrodes are formed and a liquid crystal layer formed between the transparent substrates or a liquid crystal lens. The dynamic barrier  electrically controls liquid crystal molecules under the control of the 3D controller  to spatially divide lights of left-eye and right-eye images.","The dynamic retarder  has a liquid crystal layer formed between transparent substrates on which electrodes are formed. The liquid crystal layer of the dynamic retarder  varies phase delay of light according to an electric field applied to liquid crystal molecules to adjust polarization of light. The dynamic barrier  changes polarization characteristic of light input to the polarizing glasses  and  at regular intervals under the control of the 3D controller . For example, the dynamic barrier  changes lights of left-eye and right-eye images, which travel to the polarizing glasses  and , to right-circularly polarized lights for an Nth frame (N is a positive integer) and changes lights of the left-eye and right-eye images, which travel to the polarizing glasses  and , to left-circularly polarized lights for an (N+1)th frame.","The polarizing glasses  and  have different light absorption axes according to polarized lights projected from the dynamic retarder . For example, the first polarizing glasses  worn by a first user transmits the right-circularly polarized light received from the dynamic retarder  and blocks other polarized components. The left and right glasses of the first polarizing glasses  include right-circular polarizing filters. The second polarizing glasses  worn by a second user transmits the left-circularly polarized light received from the dynamic retarder  and blocks other polarized components. The left and right glasses of the second polarizing glasses  include left-circular polarizing filters.","The 3D controller  acquires 3D position information of each user from a camera or an infrared sensor by using the aforementioned user position detection algorithm. In addition, the 3D controller  adjusts rendering parameters of left-eye and right-eye images of a 3D image and changes the position of the dynamic barrier  and the polarization property of the dynamic retarder  according to the 3D position information of each user.","The stereoscopic image display illustrated in  separates users according to polarization division using the dynamic retarder  and the polarizing glasses  and  and produces a binocular disparity of each user according to spatial division using the dynamic barrier . The stereoscopic image display detects the position of each user and adjusts the view angle and depth information of a 3D image to achieve multi-user tracking and changes the polarization property of the dynamic retarder  at regular intervals to prevent the resolution of a 3D image seen to each user from deterioration.",{"@attributes":{"id":"p-0085","num":"0084"},"figref":"FIGS. 14A","b":["14","14"]},"Referring to , B and C, the stereoscopic image display according to the third embodiment of this disclosure includes the display , the polarizer , the dynamic barrier , a plurality of active glasses ,  and , and a 3D controller . The dynamic barrier  and the active glasses ,  and  are 3D driving elements which separate 3D images for respective users and produce a binocular disparity of each user.","The display  displays a 2D image in the 2D mode and displays a 3D image rendered according to the position of a user under the control of the 3D controller  in the 3D mode. The display  displays 3D images time-divided by users in synchronization with the active glasses ,  and  under the control of the 3D controller . For example, the display  displays a 3D image of a first user, which is synchronized with the first active glasses , for an Nth frame, and then displays a 3D image of a second user, which is synchronized with the second active glasses , for an (N+1)th frame. Subsequently, the display  displays a 3D image of a third user, which is synchronized with the third active glasses , for an (N+2)th frame.","The dynamic barrier  may be implemented as a liquid crystal barrier having two transparent substrates on which electrodes are formed and a liquid crystal layer formed between the transparent substrates or a liquid crystal lens. The dynamic barrier  electrically controls liquid crystal molecules under the control of the 3D controller  to spatially divide lights of left-eye and right-eye images to produce a binocular disparity.","Each of the left and right glasses of each of the active glasses ,  and  may be implemented as an electrically controlled optical shutter including a liquid crystal layer formed between transparent substrates on which electrodes are formed, a power supply supplying a driving voltage to the electrodes and a control circuit controlling the driving voltage under the control of the 3D controller . The active glasses ,  and  are sequentially turned on\/off (opened\/shielded) in synchronization with 3D images time-divided by users and displayed on the display . For example, the left and right optical shutters of the first active glasses  are turned on during the Nth frame for which the 3D image of the first user is displayed to transmit light and turned off during the (N+1)th and (N+2)th frames to block light under the control of the 3D controller . The left and right optical shutters of the second active glasses  are turned on during the (N+1)th frame for which the 3D image of the second user is displayed to transmit light and turned off during the Nth and (N+2)th frames to block light under the control of the 3D controller . The left and right optical shutters of the third active glasses  are turned on during the (N+2)th frame for which the 3D image of the third user is displayed to transmit light and turned off during the Nth and (N+1)th frames to block light under the control of the 3D controller .","The 3D controller  acquires 3D position information of each user from a camera or an infrared sensor by using the aforementioned user position detection algorithm. The 3D controller  adjusts rendering parameters of left-eye and right-eye images according to the 3D position information of each user. In addition, the 3D controller  synchronizes display times of 3D images of users with turn-on times of the active glasses ,  and  through a wired\/wireless interface and transmits an optical shutter control signal for controlling turn-on\/turn-off of the active glasses ,  and  to the active glasses ,  and  through the wired\/wireless interface. For example, the 3D controller  displays the 3D image of the first user, which has a view angle and depth information varied according to the position of the first user, on the display  for the Nth frame and, simultaneously, turns on the left and right optical shutters of the first active glasses . The 3D controller  displays the 3D image of the second user, which has a view angle and depth information varied according to the position of the second user, on the display  for the (N+1)th frame, changes the position of the dynamic barrier  and, simultaneously, turns on the left and right optical shutters of the second active glasses . Subsequently, the 3D controller  displays the 3D image of the third user, which has a view angle and depth information varied according to the position of the third user, on the display  for the (N+2)th frame, changes the position of the dynamic barrier  and, simultaneously, turns on the left and right optical shutters of the third active glasses .","The stereoscopic image display illustrated in , B and C produces a binocular disparity of each user by using the dynamic barrier  and time-division-drives 3D images of users, displayed on the display, and the active glasses ,  and  to separate the users. The stereoscopic image display can detect the position of each user and adjust the view angle and depth information of a 3D image to achieve multi-user tracking.",{"@attributes":{"id":"p-0092","num":"0091"},"figref":"FIGS. 15A","b":["15","15"]},"Referring to , B and C, the stereoscopic image display according to the fourth embodiment of this disclosure includes the display , the pattern retarder , a plurality of active glasses ,  and , and a 3D controller . The pattern retarder  and the active glasses ,  and  are 3D driving elements which separate 3D images of users and produce a binocular disparity of each user.","The display  displays a 2D image in the 2D mode and displays a 3D image rendered according to the position of a user under the control of the 3D controller  in the 3D mode. The display  displays 3D images time-divided by users and synchronized with the active glasses ,  and  under the control of the 3D controller . For example, the display  displays a 3D image of a first user, which is synchronized with the first active glasses , for an Nth frame, and then displays a 3D image of a second user, which is synchronized with the second active glasses , for an (N+1)th frame. Subsequently, the display  displays a 3D image of a third user, which is synchronized with the third active glasses , for an (N+2)th frame. Reference numeral \u2018\u2019 denotes a polarizing film arranged between a liquid crystal display panel and the pattern retarder .","The pattern retarder  includes first and second retarders having different light absorption axes and splits left-eye and right-eye images of 3D images of users into polarized lights. The first retarder is formed on an odd-numbered line of the pattern retarder  and transmits left-eye image light of a first polarized light (linearly polarized light or circularly polarized light) of lights received from the display . The second retarder is formed on an even-numbered line of the pattern retarder  and transmits right-eye image light of a second polarized light (linearly polarized light or circularly polarized light) of the lights received from the display . , B and C illustrate that the first retarder is implemented as a polarizing filter which transmits right-circularly polarized lights and the second retarder is implemented as a polarizing filter which transmits left-circularly polarized lights.","Each of the left and right glasses of each of the active glasses ,  and  may be implemented as an electrically controlled optical shutter including a liquid crystal layer formed between transparent substrates on which electrodes are formed, a power supply supplying a driving voltage to the electrodes and a control circuit controlling the driving voltage under the control of the 3D controller . The left glass of each of the active glasses ,  and  includes a polarizing film which transmits only left-eye image light of the first polarized light (right-circularly polarized light) and the right glass of each of the active glasses ,  and  includes a polarizing film which transmits only left-eye image light of the second polarized light (left-circularly polarized light). The optical shutters of the active glasses ,  and  are sequentially turned on\/off in synchronization with 3D images time-divided by users and displayed on the display and split left-eye and right-eye images according to polarization division. For example, the left and right optical shutters of the first active glasses  are turned on during the Nth frame for which the 3D image of the first user is displayed to transmit light and turned off during the (N+1)th and (N+2)th frames to block light under the control of the 3D controller . During the Nth frame, the left optical shutter of the first active glasses  transmits the left-eye image of the first polarized light (right-circularly polarized light) while the right optical shutter of the first active glasses  transmits the right-eye image of the second polarized light (left-circularly polarized light). The left and right optical shutters of the second active glasses  are turned on during the (N+1)th frame for which the 3D image of the second user is displayed to transmit light and turned off during the Nth and (N+2)th frames to block light under the control of the 3D controller . During the (N+1)th frame, the left optical shutter of the second active glasses  transmits the left-eye image of the first polarized light (right-circularly polarized light) due to the first polarizing film while the right optical shutter of the second active glasses  transmits the right-eye image of the second polarized light (left-circularly polarized light) due to the second polarizing film. The left and right optical shutters of the third active glasses  are turned on during the (N+2)th frame for which the 3D image of the third user is displayed to transmit light and turned off during the Nth and (N+1)th frames to block light under the control of the 3D controller . During the (N+2)th frame, the left optical shutter of the third active glasses  transmits the left-eye image of the first polarized light (right-circularly polarized light) due to the first polarizing film while the right optical shutter of the third active glasses  transmits the right-eye image of the second polarized light (left-circularly polarized light) due to the second polarizing film.","The 3D controller  acquires 3D position information of each user from a camera or an infrared sensor by using the aforementioned user position detection algorithm. The 3D controller  adjusts rendering parameters of left-eye and right-eye images according to the 3D position information of each user. In addition, the 3D controller  synchronizes display times of 3D images of users with turn-on times of the active glasses ,  and  through a wired\/wireless interface and transmits an optical shutter control signal for controlling turn-on\/turn-off of the active glasses ,  and  to the active glasses ,  and  through the wired\/wireless interface. For example, the 3D controller  displays the 3D image of the first user, which has a view angle and depth information varied according to the position of the first user, on the display  for the Nth frame and, simultaneously, turns on the left and right optical shutters of the first active glasses . The 3D controller  displays the 3D image of the second user, which has a view angle and depth information varied according to the position of the second user, on the display  for the (N+1)th frame and, simultaneously, turns on the left and right optical shutters of the second active glasses . Subsequently, the 3D controller  displays the 3D image of the third user, which has a view angle and depth information varied according to the position of the third user, on the display  for the (N+2)th frame and, simultaneously, turns on the left and right optical shutters of the third active glasses .","The stereoscopic image display illustrated in , B and C produces a binocular disparity of each user according to polarization division and time-division-drives 3D images of users, displayed on the display, and the active glasses ,  and  to separate the users. The stereoscopic image display can detect the position of each user and adjust the view angle and depth information of a 3D image to achieve multi-user tracking.",{"@attributes":{"id":"p-0099","num":"0098"},"figref":"FIGS. 16A and 16B"},"Referring to , the stereoscopic image display according to the fifth embodiment of this disclosure includes the display , the pattern retarder , the polarizing film , a plurality of active glasses  and , and a 3D controller . The pattern retarder  and the active glasses  and  are 3D driving elements which separate 3D images for users and produce a binocular disparity of each user.","The display  displays a 2D image in the 2D mode and displays a 3D image rendered according to the position of a user under the control of the 3D controller  in the 3D mode. The display  time-division-displays left-eye and right-eye images of each user in synchronization with the active glasses  and  under the control of the 3D controller . For example, the display  displays the left-eye image of each user, which is synchronized with left optical shutters of the active glasses  and , for the Nth frame, and then displays the right-eye image of each user, which is synchronized with right optical shutters of the active glasses  and , for the (N+1)th frame.","The pattern retarder  includes first and second retarders having different light absorption axes and splits left-eye and right-eye images of 3D images of users into polarized lights. The first retarder is formed on an odd-numbered line of the pattern retarder  and transmits left-eye image light of a first polarized light (linearly polarized light or circularly polarized light) of lights received from the display . The second retarder is formed on an even-numbered line of the pattern retarder  and transmits right-eye image light of a second polarized light (linearly polarized light or circularly polarized light) of the lights received from the display .  illustrate that the first retarder is implemented as a polarizing filter which transmits right-circularly polarized lights and the second retarder is implemented as a polarizing filter which transmits left-circularly polarized lights.","Each of the left and right glasses of each of the active glasses  and  may be implemented as an electrically controlled optical shutter including a liquid crystal layer formed between transparent substrates on which electrodes are formed, a power supply supplying a driving voltage to the electrodes and a control circuit controlling the driving voltage under the control of the 3D controller . The left and right optical shutters of the first active glasses  include a polarizing film which transmits only the first polarized light (right-circularly polarized light) and the left and right optical shutters of the second active glasses  include a polarizing film which transmits only the second polarized light (left-circularly polarized light). The left and right optical shutters of the active glasses  and  are alternately turned on\/off in synchronization with left-eye and right-eye images of a 3D image time-division-displayed. For example, the left optical shutter of the first active glasses  is turned on during the Nth frame to transmit the left-eye image light of the first polarized light and turned off during the (N+1)th frame to block light under the control of the 3D controller . The right optical shutter of the first active glasses  is turned off during the Nth frame to block light and turned on during the (N+1)th frame to transmit the right-eye image light of the first polarized light under the control of the 3D controller . The left optical shutter of the second active glasses  is turned on during the Nth frame to transmit the left-eye image light of the second polarized light and turned off during the (N+1)th frame to block light under the control of the 3D controller . The right optical shutter of the second active glasses  is turned off during the Nth frame to block light and turned on during the (N+1)th frame to transmit the right-eye image light of the second polarized light under the control of the 3D controller .","The 3D controller  acquires 3D position information of each user from a camera or an infrared sensor by using the aforementioned user position detection algorithm. The 3D controller  adjusts rendering parameters of left-eye and right-eye images of a 3D image according to the 3D position information of each user. In addition, the 3D controller  synchronizes display times of 3D images of users with turn-on times of the active glasses  and  through a wired\/wireless interface and transmits an optical shutter control signal for controlling turn-on\/turn-off of the active glasses  and  to the active glasses  and  through the wired\/wireless interface. For example, the 3D controller  displays the left-eye image of each user on the display  during the Nth frame and, simultaneously, turns on the left optical shutters of the active glasses  and . The 3D controller  displays the right-eye image of each user on the display  during the (N+1)th frame and, simultaneously, turns on the right optical shutters of the active glasses  and .","The stereoscopic image display illustrated in  splits left-eye and right-eye images of each user according to the time division technique to produce a binocular disparity of each user and separates users according to polarization division. The stereoscopic image display can detect the position of each user and adjust the view angle and depth information of a 3D image to achieve multi-user tracking.","Although left-circular polarization and right-circular polarization division methods are exemplified as the polarization division technique in the above-described embodiments of the invention, the polarization division technique is not limited thereto. Left-circular polarization can be replaced by horizontal linear polarization (or vertical linear polarization) and right-circular polarization can be replaced by vertical linear polarization (or horizontal linear polarization).","As described above, the method of controlling view of a stereoscopic image and the stereoscopic image display using the method according to the embodiments of the invention can detect the position of each user in real time through the user position detection algorithm and change rendering parameters of 3D object images according to 3D position information of each user to provide realistic 3D images to users. Furthermore, the invention can combine spatial division, time division and polarization division to separate 3D images for users and produce a binocular disparity of each user.","While this disclosure has been particularly shown and described with reference to exemplary embodiments thereof, it will be understood by those of ordinary skill in the art that various changes in form and details may be made therein without departing from the spirit and scope of this disclosure as defined by the following claims."],"heading":["BACKGROUND","BRIEF SUMMARY","DETAILED DESCRIPTION OF THE DRAWINGS AND THE PRESENTLY PREFERRED EMBODIMENTS"],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The accompanying drawings, which are included to provide a further understanding of the invention and are incorporated in and constitute a part of this specification, illustrate embodiments of the invention and together with the description serve to explain the principles of the invention. In the drawings:",{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":["FIG. 2","FIG. 1"],"b":"1"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":["FIG. 4","FIG. 3"]},{"@attributes":{"id":"p-0017","num":"0016"},"figref":["FIG. 5","FIG. 1"],"b":"1"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":["FIG. 6","FIG. 5"]},{"@attributes":{"id":"p-0019","num":"0018"},"figref":["FIG. 7","FIG. 5"]},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":["FIG. 11","FIG. 10"]},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIGS. 14","i":["a","b ","c "],"b":["14","14"]},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIGS. 15","i":["a","b ","c "],"b":["15","15"]},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIGS. 16","i":["a ","b "],"b":"16"}]},"DETDESC":[{},{}]}
