---
title: Nonvolatile media dirty region tracking
abstract: A storage set (e.g., an array of hard disk drives) may experience a failure, such as a loss of power, a software crash, or a disconnection of a storage device, while writes to the storage set are in progress. Recover from the failure may involve scanning the storage set to detect and correct inconsistencies (e.g., comparing mirrors of a data set or testing checksums). However, lacking information about the locations of pending writes to the storage set during the failure, this “cleaning” process may involve scanning the entire storage set, resulting in protracted recovery processes. Presented herein are techniques for tracking writes to the storage set by apportioning the storage set into regions of a region size (e.g., one gigabyte), and storing on the nonvolatile storage medium descriptors of “dirty” regions comprising in-progress writes. The post-failure recovery process may then be limited to the regions identified as dirty.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09003103&OS=09003103&RS=09003103
owner: Microsoft Technology Licensing, LLC
number: 09003103
owner_city: Redmond
owner_country: US
publication_date: 20110912
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["Within the field of computing, many scenarios involve a storage set provided by a set of storage devices (e.g., an array of hard disk drives interoperating according to a Redundant Array of Inexpensive Disks (RAID) array), and that may be accessed by various devices and processes to store and retrieve various types of data. In many such scenarios, data stored in different portions of the storage set may have a relationship. As a first example, a first data set and a second data set stored in the storage set may reference each other, such as related records in a database system. As a second example, two or more identical versions of the data may be retained in order to provide various advantages. For example, two storage devices may store and provide access to the same data set, thereby effectively doubling the access rate to the data. Identical copies of the data may be also retained in order to protect the integrity of the data; e.g., if a first copy of the data is lost due to a failure, such as data corruption or a hardware fault (e.g., a hard drive crash), an identical second copy of the data set may be accessed and replicated to recover from the failure.","As a third such example, data may be associated in order to detect and\/or safeguard against errors or unintended changes to the data. For example, an error in the reading or storing logic of the device, a buffer underrun or overrun, a flaw in the storage medium, or an external disruption (such as a cosmic ray) may occasionally cause an inadvertent change in the data stored on the storage medium or in the reading of data from the storage medium. Therefore, in many such scenarios, for respective portions of data stored on the storage devices, a verifier, such as a checksum, may be calculated and stored, and may be used to confirm that the contents of the data set have been validly stored to and\/or read from the storage device. As one such example, in the context of storing a data set comprising a set of bits, an exclusive OR (XOR) operation may be applied to the bits, resulting in a one-bit checksum that may be stored and associated with this data set. When the data set is later read, another XOR operation may be applied thereto, and the result may be compared with the one-bit checksum. A change of any one bit results in a mismatch of these XOR computations, indicating that the data has been incorrectly stored, altered, or incorrectly read from the storage device. Many types of verifiers may be identified, which may vary in some features (e.g., ease of computation, a capability of identifying which bit of the data set has changed, and an error-correction capability whereby an incorrectly read portion of data may be corrected).","Various forms of data replication are often achieved through the use of a Redundant Array of Inexpensive Disks (RAID) arrays, such as a set of hard disk drives that are pooled together to achieve various aggregate properties, such as improved throughput and automatic data mirroring. As a first such example, in a RAID 1 array, a set of two or more hard disk drives of the same size store identical copies of the storage set, and any update to the storage set is identically propagated across all of the hard disk drives. The storage set therefore remains accessible in the event of hard disk drive failures, even multiple such failures, as long as even one hard disk drive remains functional and accessible. As a second such example, a RAID 4 array involves a set of two or more disks, where one disk is included in the array not to store user data, but to store verifiers of the data stored on the other disks. For example, for a RAID 4 array involving four disks each storing one terabyte of data, the capacity of the first three disks is pooled to form a three-terabyte storage space for user data, while the fourth disk is included in the array to hold verifiers for data sets stored on the first three disks (e.g., for every three 64-bit words respectively stored on the other three disks, the fourth disk includes a 64-bit verifier that verifies the integrity of the three 64-bit words). The RAID array controller comprises circuitry that is configured to implement the details of a selected RAID level for a provided set of hard disk drives (e.g., upon receiving a data set, automatically apportioning the data across the three user data disks, calculating the verifier of the data set, and storing the verifier on the fourth disk). The RAID techniques used may also enable additional protections or features; e.g., if any single storage device in a RAID 4 array fails, the data stored on the failed device may be entirely reconstructed through the use of the remaining storage devices.","This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key factors or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter.","While writing associated sets of data on a storage system, problems may arise during the writing that cause an inconsistency among the associated data sets. As a first example, in a mirrored data set, an error may occur while writing to one of the mirrors, such as an inadvertent change of the data due to a cosmic ray, a flaw in the physical medium, or a logical error in the read\/write process. As a result, the mirrors of the data set may not match, and it may be difficult to choose a correct version of the data. As a second example, problems may arise due to the delay between storing a data set and its verifier (or vice versa). For example, many storage devices only support a write to one location at a time (e.g., the location underneath the write head of a hard disk drive, or the location specified by an address register in a solid-state storage device), and the sequential storing of data involves writing the data set before the verifier, or vice versa. As another example, if the data set and verifier are stored on different storage devices, it may be difficult to synchronize the moment that the first storage device stores the data set with the moment that the second storage device stores the verifier of the data set. As a result, storing a data set and a corresponding verifier occur not a synchronous manner, but in a sequential manner. Many sources of failure may interrupt the storage process, such as power loss, a hardware failure, a software crash, or an unanticipated removal of a storage device from the array. If such failures arise in the moment after storing a data set and before storing the verifier, then a verifier error may later arise during a read that jeopardizes the confidence in the accuracy of the data. In addition to the data being actively written by the storage device(s), a failure may also disrupt other writes that have not completed, such as data stored in a write buffer and scheduled for imminent writing.","In these and other scenarios, the consistency of the data set may be detected through a verification process, e.g., by comparing identical data sets or comparing verifiers with the corresponding data. Accordingly, upon detecting a catastrophic failure, the data on one or more storage devices may be \u201ccleaned\u201d by verifying the data and possibly correcting inconsistencies. However, while the occurrence of a failure may be easily detected (e.g., an incorrect shutdown or a replacement of a failed storage device), it may not be possible to determine which portions of the storage set were being written at the time of the writing and may have been compromised by the failure. Without such information, the entire storage set may have to be cleaned by verifying every data set in the storage set (e.g., comparing the entirety of each copy of a mirrored data set to detect inconsistencies, and testing the verifiers of every data set in the storage set). Cleaning the entire storage set may take a long time, particularly in view of the growth of storage set capacities into the range of petabytes, resulting in a protracted recovery period following even a brief period of catastrophic failure; and although this extensive cleaning process may ensure the integrity of the storage set, the extent of the cleaning process of the entire volume (often involving data sets and even storage devices that have not been written in a long time) may be disproportionate to the comparatively small number of writes that may have been in process at the time of the failure.","Presented herein are techniques for focusing the cleaning of a storage set on data that may have been compromised by a failure. In accordance with these techniques, before writing to a location in the storage set, a storage device may store on the same nonvolatile physical medium a descriptor indicating the location where the write is to occur, and may after erase the descriptor after the write has completed. In this manner, the storage devices may track the \u201cdirty\u201d areas of the storage set, and upon recovering from a failure, may initiate a cleaning of only the \u201cdirty\u201d areas of the storage set. However, it may be appreciated that recording (and flushing) the address on the nonvolatile physical medium of every write, and erasing (and flushing) the address after confirming the completion of the write to the physical medium, may significantly increase (e.g., tripling) the number of seeks and writes involved in writing each data set, thereby reducing the performance (e.g., latency and throughput) of the storage set. At the other extreme, tracking \u201cdirty\u201d information only at a high level, such as per storage device, partition, or volume, may insufficiently focus the cleaning process. For example, if \u201cdirty\u201d information is tracked per volume, the cleaning process may focus on a particular volume that was in use at the time of the writing, but the volume may comprise multiple terabytes of data that result in an extensive cleaning process, even if only a few megabytes of data were being written. Consequently, according to the techniques presented herein, the storage set may be apportioned into regions of a region size, and \u201cdirty\u201d information may be tracked for each region. For example, apportioning the storage set into regions of one gigabyte may enable a comparatively precise tracking of regions, while also reducing the frequency of updating the dirty region information (e.g., when a write is initiated in a region, it is marked as \u201cdirty,\u201d and may remain so marked through extensive sequential writes within the region). The selection of the region size may therefore be viewed as a tradeoff between precision in the of dirty region information, leading to more focused and shorter cleaning processes, and reduction in the performance costs of implementing the dirty region tracking process.","Additional techniques may further reduce the frequency of updates to the dirty region information. As a first example, before writing a dirty region descriptor for a region to which a write has been requested, an embodiment may determine whether the region is already marked as dirty on the storage medium, and may avoid redundantly marking the region as dirty. As a second example, a region to which a write has recently completed may present a high probability of an imminent subsequent write to the same region. Therefore, rather than promptly marking the region as clean following completion of the write, the \u201cclean\u201d remaking may be deferred for a brief duration; and if a subsequent write to the same region is received, the region may remain marked as dirty, rather than having to mark the region as dirty shortly after having marked the region as clean. In one such embodiment, a volatile memory representation of the dirty region descriptors may be generated and used to determine when to mark such regions as dirty or clean. For example, the in-memory representation may record the times of the last write request to each region, and may mark as clean any regions that have not been written to for a defined duration. Additionally, the in-memory representation may enable a batch recording of \u201cclean\u201d regions, further reducing the amount of storage media writes involved in the dirty region tracking process. These and other uses of the in-memory tracking of a \u201cworking set\u201d of dirty regions may reduce the number of accesses to the storage devices, and hence the performance costs of the nonvolatile dirty region tracking, without significantly reducing the precision of the cleaning process. These and other techniques may further reduce the performance costs of implementing the dirty region tracking without significantly prolonging the cleaning process during a recovery from a failure of the storage set.","To the accomplishment of the foregoing and related ends, the following description and annexed drawings set forth certain illustrative aspects and implementations. These are indicative of but a few of the various ways in which one or more aspects may be employed. Other aspects, advantages, and novel features of the disclosure will become apparent from the following detailed description when considered in conjunction with the annexed drawings.","The claimed subject matter is now described with reference to the drawings, wherein like reference numerals are used to refer to like elements throughout. In the following description, for purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of the claimed subject matter. It may be evident, however, that the claimed subject matter may be practiced without these specific details. In other instances, structures and devices are shown in block diagram form in order to facilitate describing the claimed subject matter.","A. Introduction","Within the field of computing, many scenarios involve the storage of data comprising a storage set on one or more nonvolatile storage devices (e.g., platter-based magnetic and\/or optical hard disk drives, solid-state storage devices, and nonvolatile memory circuits). Many details of the data storage may vary, such as the word size, the addressing method, the partitioning of the storage space of the storage device into one or more partitions, and the exposure of allocated spaces within the storage device as one or more volumes within a computing environment. Additionally, the storage devices may operate independently or with loose cooperation to provide the storage set. For examine, in an array structured according to a RAID 0 scheme, a storage set may simply be aggregated from the capacity provided by each storage device, each of which may operate independently of the other disks. Alternatively, the storage devices may be configured to provide various features through varying degrees of tight coordination. For example, in an array structured as according to a RAID 1 scheme, a set of storage devices may each store an identical copy of the entire storage set; e.g., a one-terabyte storage set may be stored as four identical copies on four one-terabyte storage devices, which may interoperate to ensure that any request to alter the storage set is equivalently applied to all four storage devices in order to maintain synchrony. A \u201cmirrored\u201d archive of this nature may present improved access rates; e.g., different processes may access the storage set through different storage devices, potentially quadrupling the access rate to the storage set that may be achievable by any single device. Additionally, this type of array demonstrates robust durability; if one or more hard disk drives fails (e.g., due to data corruption, damage, or loss), the storage set is still accessible through the other hard disk drives, and remains intact as long as at least one hard disk drive remains valid and accessible. However, the advantages of these features are offset in a RAID 1 array by the considerable loss of capacity (e.g., the use of four terabytes of hard disk drive space to store one terabyte of data). Other storage schemes may provide some of these security and performance features with less reduction of the capacity (e.g., in a RAID 4 scheme, a set of (n) drives of space (s) exposes the full storage capacity of all but one hard disk drive that is reserved to store parity information, and can recover from the failure of any one hard disk drive in the array).","In these and other scenarios, data sets may be stored to the storage set by various devices and processes. However, many forms of failure may occur during the use of the storage set that may compromise the storage of data. For example, while writes to the storage set on behalf of various processes are accessing the data set, a software failure may occur in the writing process, in a storage set management process, in a device driver for the software, or in the operating system; the computer may lose communication with the storage device (e.g., the storage device may be disconnected, or a wired or wireless network connecting the computer and the storage device may fail); or the storage device may experience a hardware failure (e.g., a head crash in a hard disk drive or an interruption of power). These and other forms of failure may occur suddenly and without warning, and may interrupt the process of writing data to the storage set.","In the event of a failure of the storage set during one or more writes, some types of failure may be recoverable, or may be result in an insignificant loss of data. For example, the failure may occur shortly after the data was written to the storage set and is recoverable; may cause the loss of data that may be regenerated or that is not valuable; or may cause the loss of a small amount of data in a comparatively large data set, such as the loss of a few email messages in a very large email message archive. However, other types of failure may present significant problems. In particular, some associations may exist among two or more sets of data that are written to the storage set, and a failure that occurs while writing a first data set may result in an inconsistency in the associated data set. Such an inconsistency may not only compromise the integrity of the data set being written at the moment of failure, but also the associated data sets. Indeed, in some scenarios, a complete failure of the write and a loss of the data to be written may cause fewer problems than an incomplete write to the storage set. As a first example, if a failure occurs during an updating of a first copy of a mirrored data set, the detected inconsistency between this copy and another copy may call into question the integrity of both copies. Moreover, if both copies were being updated at the moment of failure, and an inconsistency is subsequently detected, it may be difficult to determine which copy successfully completed the write before the failure and which copy failed to do so. As a second example, if an inconsistency is detected between a data set and its checksum, it may be difficult to determine whether data set or the checksum is in error. Moreover, if a checksum is calculated from several data sets, a failure to complete a write to one data set may result in an incorrect checksum and a reduction of trust not only in the integrity of the data set and the checksum, but in all of the other data sets represented by the checksum. In these and other scenarios, the failure to complete a write to a data set may result in inconsistencies that compromise the reliability of a broad range of data in the storage set, even including other data sets that are only tangentially related to the incompletely written data set.",{"@attributes":{"id":"p-0027","num":"0026"},"figref":["FIGS. 1 and 2","FIG. 1"],"b":["100","102","104","106","104","106","100","104","106","106","102","112","110","104","108","102","106","110","108","100","106","110","106","106","106","104","106","104","106","114","110","106","106","110","104","102"]},"Such lapses in the synchrony of storage devices  may result in significant problems, e.g., if a failure occurs after the completion of a write  by a first storage device  and before the completion of the same write  by a second storage device . For example, at a third time point , while a third write  is being performed to the storage devices , a failure  occurs that interrupts the write  to all storage devices . Even if the failure  is temporary (e.g., a momentary loss of power), and even ephemeral, the failure  may result in an inconsistency  due to the timing differences in the storage device  (e.g., the write  may have been completed prior to the failure  on the first and second storage devices , but not on the third storage device ). While recovering from the failure , upon identifying the inconsistency  among data sets  that are expected to be identical, it may be difficult to choose the determine which data set  is accurate. In this exemplary scenario , it may be logical to choose the version of the data sets  stored on the majority of storage devices . However, minor variations of this exemplary scenario  may render such a choice less logical; e.g., the inconsistency  may arise in scenarios involving an even number of storage devices , such that a majority choice may not be available.","In scenarios such as the exemplary scenario  of , the inconsistency  may apply only to data sets  that have recently been written. However, in other scenarios, similar failures  may also jeopardize the reliability of data sets  that have not been written or even accessed in a long time, and even those stored on other storage devices  that were not in use at the time of the failure . In the exemplary scenario  of , a set of four storage devices  are configured in the manner of a RAID 4 scheme, wherein a storage set  is allocated across three storage devices , and a fourth storage device  is configured to store a set of verifiers  corresponding to various data sets  stored on the first three storage devices . The verifiers  may comprise a checksum, such as a parity bit that is computed by XORing together the bits stored in a corresponding location on each of the other storage devices . (For example, for a one-megabyte data set  stored at a particular physical or logical location in the storage set  of each storage device , each of the one-megabyte data sets  may be XORed together to generate one megabyte of parity data that is stored on the fourth storage device ). This interoperation may enable the storage devices  to maintain the integrity of the storage set  through some forms of data loss. For example, if any one of the first three storage devices  is lost, the portion of the storage set  stored on the lost storage device  may be reconstructed by XORing together the data stored on the other two storage devices  and the corresponding parity data stored on the fourth storage device ; and if the fourth storage device  is lost, the parity data may simply be regenerated onto a replacement storage device  from the corresponding data sets  stored on the first three storage devices.","In this manner, the interoperation of the storage devices  as illustrated in the exemplary scenario  of  enables fault-tolerance even in the event of a sudden and permanent loss of any one storage device . However, the storage devices  in this exemplary scenario  also exhibit the incomplete synchrony presented in the exemplary scenario  of . For example, at a first time point , a write  may be initiated to a data set  stored on the second storage device , which involves updating the verifier  for the corresponding data sets  stored on the fourth storage device . However, the writes  may not complete at the same time; e.g., at a second time point , the write  to the second storage device  may have completed, but the write  of the verifier  on the fourth storage device  may not. Moreover, a failure  may arise before the latter write  completes, and the fourth storage device  may retain an incorrect verifier . Even if limited and brief, this failure  may jeopardize not only the verifier  that has been incompletely written to the fourth storage device  and the data set  stored on the second storage device  that was recently but completely written, but the data sets  stored on the first and third storage devices  that are also represented by the verifier . For example, at a third time point , the third storage device  may experience a failure ; and at a fourth time point , a recovery process may be initiated to reconstruct the data sets  on a replacement storage device  by XORing together the corresponding data sets  on the first and second storage devices  and the corresponding verifier  on the fourth storage device . However, because of the inconsistency  caused by the failure  of the write  to the fourth storage device  at the second time point , the XORing may result in a reconstruction of incorrect data  on the third storage device . This incorrect data  may be generated even though this data set  on the third storage device  was not involved in the failure  of the write , even if this data set  has not been written in a long time, and even if the third storage device  was not in use or even accessible during the failure  of the write  to the fourth storage device . Thus, the single failure of a write  may comprise the fault-tolerant capabilities of the storage set  (i.e., despite the implementation of a RAID 4 scheme, the storage set  loses the capability to recover from the failure  of a single storage device ). Even more severe consequences may arise in other scenarios; e.g., if the failed write  occurred to a data set  comprising a master boot record, an entire volume within the storage set , possibly comprising the entire storage set , may be compromised and inaccessible.","Due to the potentially catastrophic consequences of failures  of writes , techniques may be utilized to detect and correct resulting inconsistencies . As a first such technique, various types of cleaning processes may be utilized to detect inconsistencies in the storage set . For example, data sets  may be compared with verifiers  to detect a mismatch, or copies of data sets  that are expected to be identical may be compared. Even data sets  that are not associated with another data set  or verifier  may be examined for inconsistencies, such as data corruption, and may occasionally be automatically repaired. However, a scan of an entire storage set  may be inefficient and\/or prohibitive in many scenarios. For example, the scan may be protracted due to the size of the storage set , the throughput of the storage set  (e.g., a geographically distributed storage set  may be accessible through comparatively low-throughput network connections), and\/or the complexity of the scan (e.g., a math-intensive computation of a sophisticated parity check). During the scan, the storage set  remains possibly inconsistent and vulnerable in the event of a failure  of a storage device  as depicted in the exemplary scenario  of , and it may be unwise to allow processes to access to the storage set  due to the possibility of providing incorrect data. Such processes may therefore have to be blocked until the scan is complete (or at least until the data sets  utilized by the process have been cleaned), resulting in outage of services or downtime. Additionally, this protracted and costly scanning of the entire storage set  may be triggered by a potentially brief failure  of even a single storage device , and may be disproportionate to the number of pending writes  during the failure . Indeed, even if no writes  were in progress at the time of failure , a scan of the entire storage set  may have to be invoked if the number of pending writes  cannot be determined, due to the possibility of an inconsistency  that may have catastrophic results.","In view of these considerations, it may be desirable to provide mechanisms to store information about pending writes  during the ordinary operation of the storage set  in case of a sudden failure  of hardware and\/or software. This information may be recorded as information about the \u201cdirty\u201d status of the storage set  that may present an inconsistency  if a failure occurs. Moreover, it may be desirable to record this information in a nonvolatile memory in order to retain the information in the event of a power failure. This tracking may enable a cleaning process invoked after a failure , such that the cleaning process may be limited to scanning only the areas of the storage set  that were involved in a pending write  at a moment of failure .",{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 3","b":["102","106","300","102","302","202","302","202","202","102","302","302","102","106"]},"This first exemplary technique  may present some advantages; e.g., it may be achieve the storage of information about pending writes  in a nonvolatile storage medium, may entail a comparatively simple cleaning process involving a small amount of additional hardware or software, and may reduce the computational complexity of the cleaning as compared with other detection methods (e.g., performing an XOR operation on large data sets  retrieved from several storage devices ). However, this first exemplary technique  also presents many disadvantages. As a first example, the \u201cdirty\u201d indicators  consume half of the storage space of the storage set . As a second example, the scanning process still involves scanning the entire storage set , and even if the relative simplicity of the scanning detection reduces the computational power involved, the accessing of the entire storage set  may still result in a protracted cleaning process. Moreover, if the access rate to the storage set  is the bottleneck in the cleaning process, the duration of the cleaning process may be unchanged. As a third example, if the \u201cdirty\u201d indicators  are stored in physical proximity with the tracked bits of the storage device  (e.g., following each bit or bytes on the physical medium of the storage device ), a failure  resulting in the corruption of a bit may also result in the corruption of the \u201cdirty\u201d indicator . Alternatively, if the \u201cdirty\u201d indicators  are stored in a different area of the same storage device , then a write  of any single bit involves three accesses to three different portions of the storage device \u2014e.g., a first seek to and access of the area storing the \u201cdirty\u201d indicators  to mark the bit as dirty; a second seek to and access of the area storing the bit to perform the write ; and a third seek to and access of the area storing the \u201cdirty\u201d indicators  to mark the bit as clean. These multiple accesses may greatly reduce the performance (e.g., latency and throughput) of the storage device . Moreover, the increased physical wear-and-tear involved in regular use of the storage device  caused by this variation of tracking the \u201cdirty\u201d bits may inadvertently hasten the physical failure  of the storage device .",{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 3","b":["304","102","106","304","306","308","102","202","104","306","202"]},"This second exemplary technique  presents several advantages, both in general and with respect to the first exemplary technique  illustrated in . As a first example, because the accessing of the separate nonvolatile memory  may occur concurrently with the performance of the write  to the storage set , the performance (e.g., latency and throughput) of the storage device  may be undiminished by the inclusion of this technique. As a second example, storing the \u201cdirty\u201d indicators  in a separate portion of memory may avoid reducing the capacity of the storage set . As a third example, the separation of the storage of the \u201cdirty\u201d indicators  from the physical media of the storage devices  may preserve the \u201cdirty\u201d indicators  in the event of a failure  of the storage set ; e.g., a physical flaw in a portion of a storage device  may not affect the storage or retrieval of the \u201cdirty\u201d indicator  for the bytes stored in the flawed portion, and in the event of a power failure, the separate nonvolatile memory  may continue to operate using the battery . As a fourth example, the cleaning process may be considerably protracted by focusing only on the bytes indicated as dirty in the separate nonvolatile memory .","However, this second exemplary technique  also presents distinct disadvantages. As a first example, the inclusion of separate hardware significantly increases the cost and complexity of the storage set . As a second example, in addition to the possibility of a failure of a storage device , the management of the storage set  may also have to contend with a failure of the separate nonvolatile memory . Moreover, because the separate nonvolatile memory  is not stored in the storage set , it is not included in mechanisms for promoting the redundancy and fault tolerance of the storage set , and a single failure of the separate nonvolatile memory  may result in a loss of \u201cdirty\u201d indicators  for the entire storage set . As a third example, the tracking of \u201cdirty\u201d indicators  for respective bytes of the storage set  may involve a high number and rate of accesses to the separate nonvolatile memory , which may have to provide high throughout and large capacity to satisfy this task. Indeed, the separate nonvolatile memory  may have to provide sufficient throughput to record write activities not just for the fastest storage device  in the storage set , but for the combined rate of activity of all storage devices  served by the separate nonvolatile memory . As a fourth example, if the separate nonvolatile memory  is not integrated with a storage device  (e.g., if it is implemented in a RAID controller), the \u201cdirty\u201d indicators  may be less portable than the storage devices . For example, in the event of an unrelated failure of a RAID controller, the storage devices  may be relocated to and accessed by a different RAID controller, but the \u201cdirty\u201d indicators  may remain within the separate nonvolatile memory  of the failed RAID controller is fully operational. Indeed, it may be unclear to the user why storage devices  that were not involved in the failure of the RAID controller may be inconsistent, and may therefore have to be cleaned, when inserted into a separate RAID controller. These and other disadvantages may be exhibited by many techniques resembling those in  for facilitating the recovery of the storage set  from a failure  by tracking the status of writes  to the storage set .","B. Presented Techniques","Presented herein are techniques for tracking the status of writes  to a storage set  provided by a set of storage devices  that may enable a rapid cleaning process in a fault-tolerant, performant, and cost-effective manner. In accordance with these techniques, the storage set  may be apportioned into regions of a particular region size (e.g., regions of one gigabyte), and a region descriptor may be generated to record the pendency of writes  to one or more locations  within each region. The region descriptor may be stored on the same storage device  where the regions are located, or on a different storage device  of the storage set . When a request for a write  to a particular location  is received, an embodiment of these techniques may first identify the region comprising the location , and may then determine whether the region indicator of the region comprising the location  is already marked as dirty. If not, the embodiment may first update the region indicator to mark the region as dirty; but if so, then the embodiment may proceed with the write  without having to update the region indicator. After the write  is completed, the embodiment may mark the region indicator of the region as clean. Notably, the embodiment may be configured not to update the region as clean in a prompt manner, but may wait for a brief period before doing so, in case subsequent requests for writes  to the same region (either an overwrite of the same location , or a write  to a sequentially following data set  or an otherwise nearby data set ) promptly follows the first write . This delay may avoid a rewriting of the region indicator to \u201cclean\u201d followed promptly by rewriting the region indicator as \u201cdirty,\u201d and may therefore economize accesses to the storage device  in furtherance of the performance and life span of the storage device .",{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 4","b":["110","102","106","106","102","102","400","102","106","402","402","104","108","102","106","404","402","110","104","402","408","110","108","102","106","110","402","108","406","402","404","402","106","110","108","102","410","106","110","402","404","106","410","106","110"]},"As further illustrated in the exemplary scenario  of , at a third time point , a second write  may be requested to a different location  in the storage set . The storage devices  therefore identify the region  associated with the location  of the second write , and determine that this location  is also within the first region . Moreover, because the first region  is already marked as dirty in the region descriptor , the storage devices  refrain from redundantly updating the region descriptor . The storage devices  then begin performing the write , and the second storage device  eventually completes the write  and then (perhaps after a brief delay, during which no further writes  to the first region  are requested) updates the region descriptor  to mark the first region  as clean. However, the write  performed by the first storage device  is interrupted by a failure  (e.g., a temporary disconnection, a software crash, or a power failure). At a fourth time point , the first storage device  becomes accessible again, and the task of cleaning  the first storage device  is initiated to ensure the correction of writes  that were interrupted by the failure  and that, if uncorrected, may present a potentially catastrophic inconsistency  in the storage set  (e.g., a divergence of the identical copies of the storage set  stored on the storage device ). However, rather than comparing all of the data sets  with the corresponding data set  of the second storage device , the cleaning  may utilize the region descriptor  and may clean only the regions  marked as dirty. Thus, at the fourth time point , because only the first region  is marked as dirty in the region descriptor  of the first storage device , the cleaning  may compare the data sets  of the first region  with the corresponding data sets  of the second storage device , and upon finding an inconsistency, may copy the data set  over the incorrect data set  on the first storage device .","In this manner, the techniques presented herein, of which one example is depicted in the exemplary scenario  of , may enable a cleaning  of a storage device  following a failure . Moreover, these techniques achieve this cleaning  in an advantageous manner as compared with other techniques, including those illustrated in . As a first exemplary example, the cleaning  of the first storage device  is limited to the data sets  stored in the subset of regions  where at least one write  had recently occurred at the time of a crash ; e.g., because the second region  has not experienced a write  in a while, the second region  is not included in the cleaning . As a second exemplary advantage, the marking  of dirty and clean information for regions  comprising a set of regions , as well as the deferred marking of such regions  as clean following a write , may significantly reduce the performance loss caused by the tracking of dirty regions. For example, by leaving the marking  of the first region  as dirty for a brief period following the first write , the storage devices  are able to omit marking  the region  as dirty, only to mark it as dirty again upon receiving the second write  to another location  within the same region , thereby reducing the accesses by each storage device  to the region descriptor  from three to one. This efficiency gain may not have been achieved if either the dirty region information was tracked per location  or if the storage devices  promptly updated the region descriptor  following each write  (e.g., as illustrated in the first exemplary technique  of ). As a third exemplary advantage, the storage of the region descriptor  for regions , as opposed to single locations , does not significantly reduce the available capacity of the storage devices  (in contrast with the first exemplary technique  of , wherein 50% of the total capacity of the storage device  is used to track the dirty or clean status of the available capacity). As a fourth exemplary advantage, the tracking of dirty regions is achieved without additional hardware, and thus reduces the cost, complexity, power consumption, and opportunities for failure of the tracking techniques as compared with the second exemplary technique  of . These and other exemplary advantages may be achievable through the tracking of dirty region information for storage devices  providing a storage set  in accordance with the techniques presented herein.","C. Exemplary Embodiments",{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 5","b":["500","104","500","500","502","504","106","506","404","102","106","402","508","104","108","102","510","402","102","108","102","512","402","404","514","104","108","102","104","108","102","516","402","404","110","102","500","518"]},{"@attributes":{"id":"p-0045","num":"0044"},"figref":"FIG. 6","b":["600","416","102","106","600","600","602","604","106","606","404","102","106","402","608","402","404","610","416","104","402","104","402","612","402","404","102","122","110","600","518"]},{"@attributes":{"id":"p-0046","num":"0045"},"figref":["FIG. 7","FIG. 5","FIG. 6"],"b":["700","702","712","710","710","702","110","106","102","500","702","106","102","404","106","600"]},"D. Variations","The techniques discussed herein may be devised with variations in many aspects, and some variations may present additional advantages and\/or reduce disadvantages with respect to other variations of these and other techniques. Moreover, some variations may be implemented in combination, and some combinations may feature additional advantages and\/or reduced disadvantages through synergistic cooperation. The variations may be incorporated in various embodiments (e.g., the exemplary method  of  and the exemplary method  of ) to confer individual and\/or synergistic advantages upon such embodiments.","D1. Scenarios","A first aspect that may vary among embodiments of these techniques relates to the scenarios wherein such techniques may be utilized. As a first variation of this first aspect, these techniques may be used in conjunction with many types of storage sets  comprising various types of data sets , including binary storage systems storing various types of binary objects; file systems storing files; media libraries storing media objects; object systems storing many types of objects; databases storing records; and email systems storing email messages. As a second variation of this first aspect, these techniques may be used with many types of storage devices , including hard disk drives, solid-state storage devices, nonvolatile memory circuits, tape-based storage devices, and magnetic and optical discs. Such storage devices  may also be directly connected to a device  (such as a computer) implementing these techniques; may be accessible through a wired or wireless local area network (e.g., an 802.11 WiFi network or ad-hoc connection, or an infrared connection); and\/or may be accessible through a wired or wireless wide-area network (e.g., a cellular network or the internet). Moreover, these techniques may be used with two or more storage devices  operating independently (e.g., storage devices  that are accessed independently through a software process); operating with loose interoperation (e.g., storage devices  that operate independently but that are informed of and may communicate with the other storage devices  sharing the storage set ); or operating with tight interoperation (e.g., a Redundant Array of Inexpensive Disks (RAID) controller managing several storage devices  as components of a storage system). As a fourth variation of this first aspect, portions or all of these techniques may be implemented within one or more components within the computing environment, such as a set of software instructions stored in a volatile or nonvolatile of a computer or device having access to the storage devices  (e.g., an operating system process or a hardware driver); by a storage system configured to interface with the storage devices  (e.g., a RAID controller); or in respective storage devices  of the storage set .","As a fifth variation of this first aspect, many types of cleaning techniques may be used to clean the data sets  within a region . As a first example, and as illustrated in the exemplary scenario  of , the storage set  may store (on the same storage device  and\/or on different storage devices ) identical copies of a data set . In this variation, a mirroring cleaning technique may be used to clean a copy of a data set  by comparing it with another copy of the data set  to detect and correct inconsistencies  (e.g., for respective data sets  of the region  corresponding with a mirror data set stored in a second region  of the storage set , synchronizing the data set  with the mirror data set). As a second example, and as illustrated in the exemplary scenario  of , the storage set  may store (on the same storage device  and\/or on different storage devices ) verifiers  of respective data sets , such as a checksum or hashcode, which may be compared with the contents of a data set  (and possibly other data sets  in the storage set ) to detect inconsistencies . Many types of verifiers may be included in such scenarios. For example, simpler verifiers , such as a parity bit, may be efficiently computed for a data set , and may lead to a rapid detection of the presence or absence of an inconsistency  in a data set . Alternatively, complex verifiers  may be utilized that present additional features, such as increased reliability, increased detail (e.g., indicating the portion of a data set  that is inconsistent), and\/or error correction capabilities. In these variations, a verifier cleaning technique may be used to clean a data set  by verifying the verifier of the data set  to identify and possibly correct inconsistencies . As a third example, a data set  may be cleaned simply by inspecting the contents, such as corruption of a data set  that did not exist at an earlier time. When inconsistencies  are detected, reconstruction techniques may be utilized to repair the corrupted data and\/or salvage the remaining data in the data set . Those of ordinary skill in the art may identify many types and variations of scenarios wherein the techniques presented herein may be useful.","D2. Region Descriptor Structural Variations","A second aspect that may vary among embodiments of these techniques relates to the nature of the region descriptor . As a first variation of this second aspect, the storage set  may be apportioned into regions  identified within the region descriptor  in many ways. As a first example of this first variation, the selection of a region size may affect various aspects of these techniques. For example, it may be appreciated (particularly in view of the exemplary techniques illustrated in of ) that tracking the dirty or clean state of every location  in the storage set  may result in a considerable loss of capacity of the storage set \u2014perhaps as much as 50%\u2014and, indeed, may not significantly improve the duration of the cleaning process, since scanning the entire storage set  to read the dirty and clean bits for each bit may last as long as scanning the entire storage set  to detect inconsistencies . It may also be appreciated that tracking the clean or dirty status for a very large portion of the storage set , such as storing a single \u201cclean\u201d or \u201cdirty\u201d marking  for each volume, each partition, or each storage device , may not acceptably reduce the highly protracted nature of the recovery, since a write to a single location  in a large volume or storage device  may lead to a cleaning  of the entire volume or storage device . In view of these considerations, it may be appreciated that the selection of the region size may considerably affect the performance of the techniques presented herein. However, there may exist a range of acceptable options for the region size, each of which may present some tradeoffs. For example, selecting a larger region size may enable the tracking of \u201cdirty\u201d and \u201cclean\u201d regions at a coarser level of granularity that reduces the consumption of the capacity of the storage set  for the use of tracking pending writes , and also reduces the frequency with which the region descriptor  is updated to mark regions  as clean or dirty, but may also entail a longer cleaning  after a failure , since the marking  of a region  as dirty results in the cleaning  of a larger region . Conversely, selecting a smaller region size may result in the tracking of pending writes  with finer granularity, enabling a comparatively rapid cleaning  after a failure  due to more limited and precise specifications of the data sets  that are to be cleaned, but may result in greater consumption of the capacity of the storage set  (since the region descriptor  contains information for more regions ) and\/or a greater performance penalty in the routine operation of the storage set  (since a set of writes  to various locations  is likely to be associated with more regions  respectively covering smaller portions of the storage set).","In view of these considerations, many techniques may be used to select or specify the region size(s) of the regions  of the storage set . For example, a region size may be fixedly defined for a particular storage set  or storage device . Different region sizes may also be selected for different storage devices  (e.g., storage devices  used in circumstances where performance characteristics do not significantly affect the computing environment, such as archiving, may use a smaller region size that more significantly reduces performance but provides faster cleaning  and recovery from failures ) and\/or for different storage sets  (e.g., data for which accessibility is of great value may be tracked using a smaller region size that enables faster cleaning  and recovery from failures ), and may even utilize different region sizes for different regions  in the same data set . The region size may also be adjusted based on the performance characteristics of the storage devices  and\/or storage sets , and\/or based on user preferences. Alternatively or additionally, a user may be permitted to choose a region size; e.g., an embodiment may present several options to a user for region sizes, and may predict a recovery time involved in recovering from a failure  in view of each region size. As another alternative, the user may specify a maximum acceptable recovery period, and an embodiment of these techniques may select a region size that likely enables recovery from failures  within the maximum acceptable recovery period.","As a second variation of this second aspect, the region descriptor  may be structured in various ways, e.g., as an array, a linked list, a table, a database, or a bitmap. Various data structures may present particular advantages. As one such example, for storage sets  comprising addresses stored with an addressing system having a single dimension (e.g., as a single, numeric sequence of addresses), the region descriptor  may be implemented as a dirty region array, comprising a set of array entries that sequentially correspond to the regions  of the storage set , and may mark a region  as dirty or clean simply by accessing the array entry of the dirty region array, thereby achieving O() access time to the marking  for any region . Alternatively, the region descriptor  may be implemented as a sequential journal, where each marking  is written in sequence following the previously committed marking  regardless of the location  in the storage set  of the data set  so marked. This region descriptor  may result in slower read access to the marking  for a particular region  (since identifying the current marking  for an entry entails scanning a significant portion of the journal, and perhaps the entire journal, in order to find an entry), but may enable the further performance advantage of sequential writes to the region descriptor . Additionally, the semantics of the contents of the region descriptor  may contain information in different ways. For example, the region descriptor  may mark a region  as \u201cclean\u201d by specifically marking  a record or bit representing the region , or may do so by evicting from the region descriptor  records for regions  that have been cleaned. These variations represent a semantic difference about whether a region  that does not have a record in the region descriptor  has been marked as clean (perhaps recently), or has simply not been marked either dirty or clean (e.g., has not been subjected to a write  for a significant amount of time).","As a third variation of this second aspect, the region descriptor  may indicate the locations  of regions  within the storage set  in various ways.  presents an illustration of an exemplary scenario  featuring some different techniques for specifying the locations  within a storage set  that are represented by respective regions . In this exemplary scenario , a storage device , such as a hard disk drive, is configured to store data comprising at least a portion of a storage set . The storage device  is configured to store data at respective physical addresses , and contains hardware that translates the addresses into physical locations on the storage medium (e.g., a platter, sector, and track). The data stored on the storage device  may also be stored according to a standardized format; e.g., the data stored on the physical medium may begin with a partition table, specifying the locations and descriptions of one or more partitions , each of which comprises one or more logical volumes  (e.g., the drives presented in the operating environment of a computer). The storage set  may be stored within one or more logical volumes , and may allocate data sets  in a logical volume  or across several logical volumes . Accordingly, a region descriptor  may identify regions  for various locations  within the storage set  in various ways. For example, the region descriptor  may utilize a physical region referencing scheme that identifies physical locations of respective regions  on the storage devices  of the storage set . As a first such example, the region descriptor  may be implemented as a first region table  comprising records that indicate a range of physical locations  on the storage medium of the storage device  that are represented by each region . As a second such example, the region descriptor  may be implemented as a second region table  comprising records indicating, for respective regions , the partition  and the offset within the partition  where each region  begins. The region descriptor  may also store and represent the region size(s) of one or more regions  (e.g., as part of a region record or a separate descriptor of the storage set ). As a third example, the region descriptor  may be implemented as a region bitmap , where partitions  may be apportioned into a series of regions  of a particular region size, and the bitmap may store a one-bit indicator of the \u201cdirty\u201d or \u201cclean\u201d status of the region . Alternatively, the region descriptor  may utilize a logical region referencing scheme that identifies logical locations of respective regions  within the logical arrangement of data in the storage set . As one such example, the region descriptor  may be implemented as a third region table  that identifies the logical location within each logical volume  where each region  begins. It may be appreciated that addressing techniques that are more closely identified with the physical locations  on the storage medium of the storage device  may involve little computation for the storage device  but more computation for processes that access the regions  logically, and vice versa.","As a fourth variation of this second aspect, embodiments of these techniques may be configured to store the region descriptor  in various locations on the storage device . As a first example of this fourth variation, the region descriptor  may be stored in a particular location on the storage device  (e.g., at the top of a partition  storing data sets  of the storage set , or at a defined location outside of the partitions  storing the data sets ). As a second example of this fourth variation, the region descriptor  may be stored on a first storage device  to track pending writes  to regions  of a second storage device .","As a fifth variation of this second example, an embodiment of these techniques may be configured to store two or more region descriptors  for the storage set . As a first example of this fifth variation, two or more identical copies of a region descriptor , representing the same set of regions  of the storage set , may be stored on the same and\/or different storage devices . The storage of identical copies may improve the fault tolerance of the storage set  (e.g., in the event of an unrecoverable failure of a storage device , a failure of a portion of a storage device  storing the region descriptor , or a corruption of the data within a region descriptor , such as the occurrence of a failure  while performing a write  to the region descriptor ), recovery from a failure  may be performed through the use of another copy of the region descriptor . It may be appreciated that while a failure of the region descriptor  is unlikely to be catastrophic, because a full cleaning  of the storage set  may be performed as a backup measure, it may nevertheless be advantageous to store a second copy of the region descriptor  in order to expedite the recovery even if the first copy becomes corrupt or unavailable. Additionally, it may be advantageous to store the identical copies of the region descriptor  on multiple storage devices , and\/or in different areas of the same storage device  (e.g., in the event of damage to a portion of a storage device  where a first copy of the region descriptor  is stored, such as a first physical sector or a first portion of a file system, a second, identical copy of the region descriptor  that has been stored in a different portion of the storage device  may be retrievable and usable).","As a second example of this fifth variation, multiple copies of the region descriptor  for a particular set of regions  may be stored on one or more storage devices , but rather than being identical at all times, the region descriptors  may be updated sequentially to reflect the clean and dirty state of the storage set  at different times. This sequential updating may be advantageous, e.g., if a failure  occurs while updating a copy of the region descriptor  that leaves the region descriptor  in an inconsistent or corrupt state, because the other copy may be available for use in the cleaning  and recovery. As a first scenario for this second example, two copies of the region descriptor  may be retained on a storage device  that both represent the same set of regions ; but in order to update the region descriptors , a first update may occur by overwriting one copy of the region descriptor , and the following update may occur by overwriting the other copy of the region descriptor . Moreover, when an embodiment of these techniques overwrites a region descriptor , the embodiment may also write a verifier  (e.g., a checksum) for the region descriptor , and a sequence indicator (e.g., an incrementing integer, or the time of the overwriting). Following a failure , the embodiment may examine all of the available region descriptors , and may select for use in the cleaning the region descriptor  having the highest sequence indicator (e.g., the highest incrementing integer, or the latest time of writing) that is also valid according to its verifier . It may be appreciated that performing the cleaning  using a slightly stale version of the region set may not result in the failure to clean a region  having an inconsistency  (e.g., a false negative), because a write  to a region  only commences after the marking  of the region  is fully committed to the storage set  in the form of a fully written region descriptor  having a highest sequence indicator and a valid verifier , so a failure  while writing the region descriptor  does not result in the failure to record a pending write . Conversely, using a stale region descriptor  may result in a false positive (e.g., a failure  during the writing of a region descriptor  may result in the loss of a marking  of a formerly dirty region  as clean, and the region  may incorrectly be treated as dirty during the recovery due to the marking  found in the stale region descriptor ), but this inaccuracy simply results in a marginally protracted recovery involving the cleaning  of a region  that was known to be clean.","Alternatively or additionally, two or more copies of a region descriptor  for a set of regions  may be redundantly stored on each of the storage devices  of a storage set . In order to update the region descriptor  for the storage set , an embodiment may initiate an overwriting of the region descriptor  in parallel on all of the storage devices . Additionally, each storage device  may include a verifier  and a sequence indicator with the region descriptor  in case a failure  occurs while one or more of the storage devices  is updating the region descriptor . The recovery from a failure  of one or more storage devices  of the storage set  may involve first examining all of the available region descriptors  to identify, among all of the copies that are validated by the respective verifiers , the copy having the highest sequence indicator. Indeed, it may be advantageous to configure the storage set  to store two copies of the region descriptor  on each storage devices ; e.g., in the event that a failure  occurs while updating the region descriptors  on all of the storage devices , the slightly stale copy on any storage device  may be usable. Alternatively, it may be similarly advantageous not to update all of the region descriptors  at the same time, but to perform a first update of the region descriptors  to the copies stored on a first subset of the storage devices , and to perform a following update of the region descriptors  to the copies stored on a second subset of storage devices .","As a third example of this fifth variation, a storage set  may include two or more region descriptors  that respectively record updates to different regions  of the storage set . As a first such example, a first region descriptor  may record dirty and clean region information for regions  of the storage set  stored on the first storage device , and a second region descriptor  may record dirty and clean region information for regions  of the storage set  stored on the second storage device . As a second such example, after apportioning the storage set  into regions , an embodiment of these techniques may generate two or more region descriptors  that each stores the \u201cdirty\u201d and \u201cclean\u201d information for a region set, comprising a subset of regions  (irrespective of the physical locations of those regions  among the storage devices  of the storage set ), may be generated and stored on one or more storage devices . The apportionment of regions  into region sets may be selected, e.g., in view of the characteristics of the respective data sets  (e.g., a first region set may comprise regions  storing more valuable or sensitive data, while a second region set may comprise regions  storing less valuable data). Alternatively or additionally, such distribution of the regions over two or more region descriptors  may be achieved explicitly (e.g., by generating two or more independent region descriptors  that each represents a designated set of regions ) or implicitly (e.g., by distributing the storage space for the region descriptor  across two or more allocations of space, such as two or more storage devices ). Moreover, in some scenarios, it may be desirable to track dirty and clean information only for a subset of regions  of the storage set ; e.g., regions  comprising data that is disposable, such as a cache or index that, if corrupted, may be regenerated from other data of the storage set , may not be tracked at all.","As a sixth variation of this second aspect, the region descriptor(s)  may be stored in a manner compatible with the synchronization techniques of the storage set  may provide various features and advantages to the region descriptors , such as increased performance through the capability of accessing any identical copy of the region descriptor  on different storage devices , and\/or similar fault tolerance as the storage set  (e.g., if the storage set  implements a fault-tolerant storage technique, such as a capability of recovering from a failure of storage devices  within a storage device failure tolerance, the region descriptor  may be stored using the same fault-tolerant storage technique). Alternatively, a region descriptor  may be stored in a manner that enables a separate set of techniques; e.g., respective region descriptors  may be stored with a region descriptor verifier of the region descriptor  and updated when a region  is marked as \u201cclean\u201d or \u201cdirty\u201d in the region descriptor , and may therefore enable a verification of the integrity of the region descriptor . Those of ordinary skill in the art may conceive many types and formats of region descriptors  that may be generated and utilized according to the techniques presented herein.","D3. Region Descriptor Usage Variations","A third aspect that may vary among embodiments of these techniques relates to the usage of the region descriptor  to mark regions  of the storage set  as clean or dirty. As a first variation of this third aspect, the use of the region descriptor  may be designed to facilitate management of the capacity of the region descriptor . In some types of region descriptors , the capacity may not change as regions  are marked, such as the region bitmap  in the exemplary scenario  of ; however, in other scenarios, the marking  of regions  may affect the capacity of the region descriptor , such as in the region tables depicted in the exemplary scenario  of . For example, if the region descriptor  stores records for respective regions , the region descriptor  may mark a region  as \u201cclean\u201d by simply evicting the record. Alternatively, the region descriptor  may continue to store records for regions  marked as \u201cclean,\u201d and may evict the records for \u201cclean\u201d regions  at a later time (e.g., after a specified period time has passed without another write  to the region , or when the capacity of the region descriptor  is exhausted). As a second example of this third aspect, the capacity of the region descriptor  may be allocated statically (e.g., a fixed-size region descriptor ) or dynamically (e.g., a resizable region descriptor  that is initially allocated with a region descriptor space, but that may be expanded upon filling the region descriptor  with records for regions  that together exhaust the capacity of the region descriptor ).","As a second variation of this third aspect, the use of the region descriptor  may be selected to reduce the number and frequency of writes  to the region descriptor . For example, because the region descriptor  is stored on the physical medium of one or more storage devices , updating the region descriptor  may add a seek and write  to the region descriptor  in addition to the seek and write  to the location  of the data set . Thus, if the region descriptor  is promptly and automatically updated to reflect changes to the \u201cdirty\u201d and \u201cclean\u201d marking  of various regions , the inclusion of the region descriptor  may considerably diminish the performance of the storage device  and the storage set , possibly increasing the cost of each access by up to three times. Even further performance degradation may occur if an embodiment of these techniques accesses the region descriptor  to read the \u201cclean\u201d and \u201cdirty\u201d status of respective regions  (e.g., in order to first determine whether a region  is marked as \u201cclean\u201d before marking  it as \u201cdirty\u201d). Accordingly, it may be desirable to reduce accesses to the region descriptor .","As a first example of this second variation, it may be appreciated that the semantics of marking  a region  as \u201cclean\u201d and \u201cdirty\u201d are somewhat asymmetric. For example, it may not be acceptable to defer an access to the region descriptor  to mark it as \u201cdirty,\u201d since such delay may incorrectly list the region  as clean if a failure occurs during the deferment (thereby presenting the false negative situation where a region  that may be inconsistent due to an incomplete write  during a failure  is marked as clean, and therefore is excluded from a cleaning  during a recovery from the failure ). However, it may be acceptable to defer marking  a region  as \u201cclean\u201d after writes have been completed. This scenario simply leads to a false positive result, where a region  that was known to be clean (e.g., no pending writes ) at the time of failure  is included in a cleaning  during the recovery\u2014but this scanning may only marginally increase the duration of the cleaning  (particularly if the region size is small), and does not compromise the integrity of the storage set . Moreover, an embodiment that promptly marks a region  as \u201cclean\u201d after the completion of a write  may shortly thereafter receive another request for a write  to a data set  in the same region  (e.g., a sequentially following data set  or an overwrite of the same data set ), and may have to re-mark the region  as \u201cdirty.\u201d Both markings , each involving an access to the region descriptor , may be avoided by deferring the marking  of regions  as \u201cclean\u201d for a brief period, while the possibility of an imminent second write  to the region  may be elevated.","In view of these considerations, an embodiment of these techniques may be configured to defer the marking  of regions  as \u201cclean\u201d following the completion of writes  to the data sets  stored in the region . The implementation of deferred commitment of \u201cclean\u201d markings  to the region descriptor  may be achieved in various ways. As a first such example, this deferment may be implemented by providing a volatile memory a write buffer that stores only \u201cclean\u201d markings  and (periodically or upon request) committing all such writes as one batch. For example, the embodiment may be configured to, upon completing storing a data set  at a location  within a region , identify the region  as clean; and may mark the cleaned regions  as clean in the region descriptor  on the storage device  upon receiving a request to mark cleaned regions  as clean in the region descriptor . This request may comprise, e.g., a specific request by a process (including the operating system of the device) to flush the markings , or may be triggered by various criteria, such as a cleaned duration criterion (e.g., committing the markings  of regions  periodically) or a region descriptor capacity criterion involving the dirty region capacity of the region descriptor  (e.g., committing the markings  of regions  when the region descriptor  reaches a particular capacity, such as a designated number of \u201cclean\u201d markings  or a threshold of total available capacity of the region descriptor , which may prompt the eviction of \u201cclean\u201d records ). As a second such example, the deferment may involve holding a \u201cclean\u201d marking  for a brief duration, and only committing the \u201cclean\u201d marking  to the region descriptor  after a brief duration when no subsequent writes  are requested to the same region . Thus, an embodiment may, upon receiving a request to write  to a location  in a region , first determine whether the region  is already marked as clean in the region descriptor , and then mark the region  as dirty only upon determining that the region  is presently marked as clean in the region descriptor .","As a third example of this second variation, an embodiment of these techniques may reduce the number of accesses to a region descriptor  by implementing a volatile memory representation of the region descriptor . For example, in addition to storing the markings  of respective regions  on the physical medium of a storage device , an embodiment of these techniques operating on a device may also store the markings  in the volatile memory of the device. The use of this representation may facilitation the deferred writing of batches of \u201cclean\u201d markings , and the determination of whether a region  that is to be marked \u201cclean\u201d is currently marked \u201cclean\u201d or \u201cdirty\u201d in the on-media region descriptor . Thus, the representation may indicate that in addition to regions  marked as clean in the region descriptor  on the storage device  and in the volatile memory representation, and regions  marked as dirty in the region descriptor  on the storage device  and in the volatile memory representation, some regions  may be marked as dirty in the region descriptor  but may be (temporarily) marked as clean in the volatile memory representation, and such clean markings  may later be committed to the on-media region descriptor .",{"@attributes":{"id":"p-0069","num":"0068"},"figref":"FIG. 9","b":["900","902","404","900","106","102","104","106","404","902","106","102","106","110","402","102","904","906","404","404","902","908","110","104","402","102","110","402","404","902","906","904","402","910","110","402","104","402","906","904","404","902","912","110","402","104","402","906","904","404","902","914","402","404","902","906","904","402","404","902","406","404","902","902","110","420","912","402","906","904","406","402","404","902"]},{"@attributes":{"id":"p-0070","num":"0069"},"figref":"FIG. 10","b":["1000","404","1002","402","404","110","102","108","402","1004","406","402","404","106","406","1004","402","110","402","1004","402","106","106","106","106","106","1008","110","104","1010","102","110","402","402","404","106","906","904","402","404","406","406","1002","110","402","1008","1010","1006","404","402"]},"D4. Recovery Variations","A fourth aspect that may vary among embodiments of these techniques relates to the manner of using a region descriptor  to perform a recovery (including a cleaning ) of a storage set  in the event of a failure . As a first variation, the recovery may involve many types of techniques for cleaning  the storage set  to detect and remove inconsistencies  in the storage set  arising from the failure . As a first example, for a data set  of the storage set  that is stored as two or more copies that are expected to be identical, the cleaning  may involve comparing the copies to detect inconsistencies  (e.g., a bitwise comparison of the actual content; a logical comparison of the content, such as a detection of equivalence among the data sets  even in the case of insubstantial structural differences, such as two file systems that comprise the same set of files but represent the file system with different bitwise structures; or a comparison of hallmarks of the data sets , such as the comparison of a hashcode calculated against the contents of each copy). As a second example, the cleaning  may involve verifying one or more data sets  against a verifier  representing the data sets , such as a checksum, to verify the consistency of the contents of the data sets . As a third example, the cleaning  may simply involve examining a data set  for errors, such as verifying that the binary contents of a data set  of a standardized type presents a valid structure according to a formal definition. Additionally, the cleaning  may enable a simple detection of the presence of an inconsistency ; may enable an identification of the location of an inconsistency  within and\/or among the at least one compared data set  (e.g., not only detecting that an error exists, but that a particular portion of the data set  is inconsistent or not as expected); and\/or may enable a correction of the inconsistency  (e.g., an error-correcting checksum, such as a Hamming code, that enables a determination of the correct data, or a repair technique applied to a binary object that enables a correction of the formatting inconsistencies and may restore or reduce data loss). Different types of cleaning  may be applied to different data sets  within the storage set  (e.g., more valuable data sets  may be stored in a manner compatible with more complicated but sophisticated error-correction techniques, while less valuable data sets  may be stored in a manner that is simple and comparatively uncomplicated). Additionally, the type of cleaning  invoked may depend on the number of available copies of the respective data sets , the type of failure  detected, the types of storage devices  (e.g., the RAID scheme implemented on the devices), and the interests of a user (e.g., the user may be permitted to select among various cleaning options that vary in terms of complexity, reliability, and duration of cleaning ). In some scenarios, multiple types of cleaning  may be performed (e.g., a quick cleaning  involving a rapid identification of easily detected inconsistencies  and the prompt restoration of access to the data set , followed by a complex cleaning  that verifies the integrity of the storage set  to address inconsistencies  that are more difficult to correct and repair).","As a second variation of this third aspect, the recovery of the storage set  may be performed in particular order. For example, the storage set  may comprise many types of data sets  stored on many storage devices. In some scenarios, a na\u00efve recovery may apply the cleaning  to the storage set  in an arbitrary order, while an ordered recovery may apply the cleaning  first to some portions of the storage set  that comprise valuable or more heavily utilized data sets  before applying the cleaning  (e.g., data in use) to other portions of the storage set  that comprise less valuable or infrequently requested data sets  (e.g., archival data). As one such example, respective regions  of the storage set  may be apportioned into one or more region sets, each having a region descriptor , and the recovery may involve cleaning  the regions  of a first region set represented by a first region descriptor  before cleaning the regions  of a second region set represented by a second region descriptor .","As a third variation of this third aspect, the recovery may comprise, before initiating a cleaning  based on a region descriptor , verifying the integrity of the region descriptor  (e.g., according to a verifier  stored with the region descriptor ). If part of the region descriptor  is found to be unverifiable (e.g., inconsistent or corrupt due to a failure  while writing to a portion of the region descriptor ), the recovery may involve using the verifiable portions of the region descriptor , and performing a full cleaning  of the regions  represented in the corrupted portion of the region descriptor . Alternatively, the recovery may involve locating and using another copy of the region descriptor  (e.g., an identical or slightly stale but verifiable copy of the region descriptor  stored elsewhere on the same storage device , or on another storage device  of the storage set ). Indeed, if two or more region descriptors  are each found to be partially corrupt, the recovery may be fully achieved by using the valid portions of each region descriptor . Alternatively, if no verifiable copy of the region descriptor  is available, the recovery may initiate a full cleaning of the storage set  (e.g., a full scan and resynchronization of the entire storage set ). In these ways, an embodiment of these techniques may be configured to recover from different types of failures  of the storage set . Those of ordinary skill in the art may devise many ways of recovering from failures  of the storage set  in accordance with the techniques presented herein.","E. Computing Environment",{"@attributes":{"id":"p-0076","num":"0075"},"figref":"FIG. 11","b":"1102"},{"@attributes":{"id":"p-0077","num":"0076"},"figref":["FIG. 11","FIG. 11"],"b":["1100","1102","1102","1106","1108","1108","1104"]},"In some embodiments, device  may include additional features and\/or functionality. For example, device  may include one or more additional storage components , including, but not limited to, a hard disk drive, a solid-state storage device, and\/or other removable or non-removable magnetic or optical media. In one embodiment, computer-readable and processor-executable instructions implementing one or more embodiments provided herein are stored in the storage component . The storage component  may also store other data objects, such as components of an operating system, executable binaries comprising one or more applications, programming libraries (e.g., application programming interfaces (APIs), media objects, and documentation. The computer-readable instructions may be loaded in the memory component  for execution by the processor .","The computing device  may also include one or more communication components  that allows the computing device  to communicate with other devices. The one or more communication components  may comprise (e.g.) a modem, a Network Interface Card (NIC), a radiofrequency transmitter\/receiver, an infrared port, and a universal serial bus (USB) USB connection. Such communication components  may comprise a wired connection (connecting to a network through a physical cord, cable, or wire) or a wireless connection (communicating wirelessly with a networking device, such as through visible light, infrared, or one or more radiofrequencies.","The computing device  may include one or more input components , such as keyboard, mouse, pen, voice input device, touch input device, infrared cameras, or video input devices, and\/or one or more output components , such as one or more displays, speakers, and printers. The input components  and\/or output components  may be connected to the computing device  via a wired connection, a wireless connection, or any combination thereof. In one embodiment, an input component  or an output component  from another computing device may be used as input components  and\/or output components  for the computing device .","The components of the computing device  may be connected by various interconnects, such as a bus. Such interconnects may include a Peripheral Component Interconnect (PCI), such as PCI Express, a Universal Serial Bus (USB), firewire (IEEE 1394), an optical bus structure, and the like. In another embodiment, components of the computing device  may be interconnected by a network. For example, the memory component  may be comprised of multiple physical memory units located in different physical locations interconnected by a network.","Those skilled in the art will realize that storage devices utilized to store computer readable instructions may be distributed across a network. For example, a computing device  accessible via a network  may store computer readable instructions to implement one or more embodiments provided herein. The computing device  may access the computing device  and download a part or all of the computer readable instructions for execution. Alternatively, the computing device  may download pieces of the computer readable instructions, as needed, or some instructions may be executed at the computing device  and some at computing device .","F. Usage of Terms","As used in this application, the terms \u201ccomponent,\u201d \u201cmodule,\u201d \u201csystem\u201d, \u201cinterface\u201d, and the like are generally intended to refer to a computer-related entity, either hardware, a combination of hardware and software, software, or software in execution. For example, a component may be, but is not limited to being, a process running on a processor, a processor, an object, an executable, a thread of execution, a program, and\/or a computer. By way of illustration, both an application running on a controller and the controller can be a component. One or more components may reside within a process and\/or thread of execution and a component may be localized on one computer and\/or distributed between two or more computers.","Furthermore, the claimed subject matter may be implemented as a method, apparatus, or article of manufacture using standard programming and\/or engineering techniques to produce software, firmware, hardware, or any combination thereof to control a computer to implement the disclosed subject matter. The term \u201carticle of manufacture\u201d as used herein is intended to encompass a computer program accessible from any computer-readable device, carrier, or media. Of course, those skilled in the art will recognize many modifications may be made to this configuration without departing from the scope or spirit of the claimed subject matter.","Various operations of embodiments are provided herein. In one embodiment, one or more of the operations described may constitute computer readable instructions stored on one or more computer readable media, which if executed by a computing device, will cause the computing device to perform the operations described. The order in which some or all of the operations are described should not be construed as to imply that these operations are necessarily order dependent. Alternative ordering will be appreciated by one skilled in the art having the benefit of this description. Further, it will be understood that not all operations are necessarily present in each embodiment provided herein.","Although the subject matter has been described in language specific to structural features and\/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather, the specific features and acts described above are disclosed as example forms of implementing the claims. Moreover, the word \u201cexemplary\u201d is used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as \u201cexemplary\u201d is not necessarily to be construed as advantageous over other aspects or designs. Rather, use of the word exemplary is intended to present concepts in a concrete fashion. As used in this application, the term \u201cor\u201d is intended to mean an inclusive \u201cor\u201d rather than an exclusive \u201cor\u201d. That is, unless specified otherwise, or clear from context, \u201cX employs A or B\u201d is intended to mean any of the natural inclusive permutations. That is, if X employs A; X employs B; or X employs both A and B, then \u201cX employs A or B\u201d is satisfied under any of the foregoing instances. In addition, the articles \u201ca\u201d and \u201can\u201d as used in this application and the appended claims may generally be construed to mean \u201cone or more\u201d unless specified otherwise or clear from context to be directed to a singular form.","Also, although the disclosure has been shown and described with respect to one or more implementations, equivalent alterations and modifications will occur to others skilled in the art based upon a reading and understanding of this specification and the annexed drawings. The disclosure includes all such modifications and alterations and is limited only by the scope of the following claims. In particular regard to the various functions performed by the above described components (e.g., elements, resources, etc.), the terms used to describe such components are intended to correspond, unless otherwise indicated, to any component which performs the specified function of the described component (e.g., that is functionally equivalent), even though not structurally equivalent to the disclosed structure which performs the function in the herein illustrated exemplary implementations of the disclosure. In addition, while a particular feature of the disclosure may have been disclosed with respect to only one of several implementations, such feature may be combined with one or more other features of the other implementations as may be desired and advantageous for any given or particular application. Furthermore, to the extent that the terms \u201cincludes\u201d, \u201chaving\u201d, \u201chas\u201d, \u201cwith\u201d, or variants thereof are used in either the detailed description or the claims, such terms are intended to be inclusive in a manner similar to the term \u201ccomprising.\u201d"],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 11"}]},"DETDESC":[{},{}]}
