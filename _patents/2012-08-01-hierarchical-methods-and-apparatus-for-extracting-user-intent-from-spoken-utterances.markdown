---
title: Hierarchical methods and apparatus for extracting user intent from spoken utterances
abstract: Improved techniques are disclosed for permitting a user to employ more human-based grammar (i.e., free form or conversational input) while addressing a target system via a voice system. For example, a technique for determining intent associated with a spoken utterance of a user comprises the following steps/operations. Decoded speech uttered by the user is obtained. An intent is then extracted from the decoded speech uttered by the user. The intent is extracted in an iterative manner such that a first class is determined after a first iteration and a sub-class of the first class is determined after a second iteration. The first class and the sub-class of the first class are hierarchically indicative of the intent of the user, e.g., a target and data that may be associated with the target.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08560325&OS=08560325&RS=08560325
owner: Nuance Communications, Inc.
number: 08560325
owner_city: Burlington
owner_country: US
publication_date: 20120801
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["RELATED APPLICATIONS","FIELD OF INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS"],"p":["This application claims the benefit under 35 U.S.C. \u00a7120 as a continuation of U.S. patent application Ser. No. 11\/216,483 filed Aug. 31, 2005 and entitled \u201cHIERARCHICAL METHODS AND APPARATUS FOR EXTRACTING USER INTENT FROM SPOKEN UTTERANCES,\u201d the entire contents of which is incorporated herein by reference.","The present invention relates generally to speech processing systems and, more particularly, to systems for hierarchically extracting user intent from spoken utterances, such as spoken instructions or commands.","The use of a speech recognition system (or a voice system) to translate a user's spoken command to a precise text command that the target system can input and process is well known. For example, in a conventional voice system based in a vehicle, a user (e.g., driver) interacts with the voice system by uttering very specific commands that must be consistent with machine-based grammar that is understood by the target system.","By way of example, assume that the climate control system in the vehicle is the target system. In order to decrease the temperature in the vehicle, the user of a conventional voice system may typically have to utter several predetermined machine-based grammar commands, such as the command \u201cclimate control\u201d followed by the command \u201cair conditioner\u201d followed by the command \u201cdecrease temperature\u201d followed by the command \u201cfive degrees.\u201d","Unfortunately, people do not talk or think in terms of specific machine-based grammar, and may also forget the precise predetermined commands that must be uttered to effectuate their wishes.","One approach that attempts to overcome the machine-based grammar problem is to use a single-stage front end action classifier that detects a very general subject from the user's speech, which is then provided to a human operator for further intent determination. This is typically the approach used in the General Motors' OnStar\u2122 system. However, a major problem with this approach is that a human operator is required.","Another approach is to build a full-fledged statistical parser, which takes the input as transcribed and builds a parse tree which is mined later to extract intent. One major difficulty in this second approach is that statistical parsers are huge in terms of storage requirements. Further, they require hand-tuning in every step. That is, every time data is added, the statistical parser requires a tremendous amount of hand-tuning and balancing of the new data with the old data.","Accordingly, improved techniques are needed that permit a user to employ more human-based grammar (i.e., free form or conversational input) while addressing a target system via a voice system.","Principles of the present invention provide improved techniques for permitting a user to employ more human-based grammar (i.e., free form or conversational input) while addressing a target system via a voice system.","In one aspect of the invention, a technique for determining intent associated with a spoken utterance of a user comprises the following steps\/operations. Decoded speech uttered by the user is obtained. An intent is then extracted from the decoded speech uttered by the user. The intent is extracted in an iterative manner such that a first class is determined after a first iteration and a sub-class of the first class is determined after a second iteration. The first class and the sub-class of the first class are hierarchically indicative of the intent of the user, e.g., a target and data that may be associated with the target.","The multi-stage intent extraction approach may have more than two iterations. By way of example only, the user intent extracting step may further determine a sub-class of the sub-class of the first class after a third iteration, such that the first class, the sub-class of the first class, and the sub-class of the sub-class of the first class are hierarchically indicative of the intent of the user.","In a preferred embodiment, as will be explained in further detail below, the first class may represent a target (e.g., topic) associated with the user intent, the sub-class of the first class may represent an action (e.g., function) associated with the target, and the sub-class of the sub-class of the first class may represent data associated with the action. One or more commands may then be provided to a target system based on the class and sub-class determinations.","These and other objects, features and advantages of the present invention will become apparent from the following detailed description of illustrative embodiments thereof, which is to be read in connection with the accompanying drawings.","While the present invention may be illustratively described below in the context of a vehicle-based voice system, it is to be understood that principles of the invention are not limited to any particular computing system environment or any particular speech recognition application. Rather, principles of the invention are more generally applicable to any computing system environment and any speech recognition application in which it would be desirable to permit the user to provide free form or conversational speech input.","Principles of the invention address the problem of extracting user intent from free form-type spoken utterances. For example, returning to the vehicle-based climate control example described above, principles of the invention permit a driver to interact with a voice system in the vehicle by giving free form voice instructions that are different than the precise (machine-based grammar) voice commands understood by the climate control system. Thus, in this particular example, instead of saying the precise commands \u201cdecrease temperature\u201d and \u201cfive degrees,\u201d in accordance with principles of the invention, the drivers may say \u201cmake it cooler.\u201d The system interprets \u201cit\u201d and \u201ccooler\u201d and associates the phrase with a temperature and asks one or more additional questions to clarify the user intent.","To do this, the system detects a dialog domain, such as in the following examples (the illustrative free form-type spoken utterance is to the left of the arrow and the illustrative detected dialog domain is to the right of the arrow):","Turn the AC up\u2192CLIMATE","Set the temperature to 76 degrees\u2192CLIMATE","Set the radio to one oh one point seven FM\u2192AUDIO and AUDIO_RadioStation","What features are available in this system\u2192HELP","Switch off the CD player\u2192AUDIO or AUDIO_CD","What are the current traffic conditions\u2192TRAFFIC","How is the rush hour traffic in New York city\u2192TRAFFIC","What is tomorrow's weather forecast for Boston\u2192WEATHER","What are the road conditions for my route\u2192TRAFFIC","How do I use the point of interest application\u2192HELP","How far is Hollywood\u2192NAVIGATION","Increase volume\u2192AUDIO or AUDIO_Volume","Raise fan speed\u2192CLIMATE","Scan for a rock-and-roll station in this area\u2192AUDIO and AUDIO_RadioStation","I am looking for Chinese food\u2192RESTAURANTS","My destination is the Mid-Hudson bridge\u2192NAVIGATION","As will be illustratively explained herein, principles of the invention are able to determine intent associated with a spoken utterance of a user by obtaining decoded speech uttered by the user (e.g., from a speech recognition engine), and extracting an intent from the decoded speech uttered by the user, wherein the intent is extracted in an iterative manner such that a first class is determined after a first iteration and a sub-class of the first class is determined after a second iteration. The first class and the sub-class of the first class are hierarchically indicative of the intent of the user, e.g., a target and data that may be associated with the target. Of course, the multi-stage approach may have more than two iterations. By way of example only, the user intent extracting step may further determine a sub-class of the sub-class of the first class after a third iteration, such that the first class, the sub-class of the first class, and the sub-class of the sub-class of the first class are hierarchically indicative of the intent of the user.","In a preferred embodiment, as will be explained in further detail below, the first class may represent a target (e.g., topic) associated with the user intent, the sub-class of the first class may represent an action (e.g., function) associated with the target, and the sub-class of the sub-class of the first class may represent data associated with the action. One or more commands may then be provided to a target system based on the class and sub-class determinations.","Advantageously, principles of the invention provide a multi-stage system that extracts more and more information from the same sentence as it goes along.","In another example where the target system is an audio system of the vehicle, the free form utterance \u201cturn the volume up\u201d may result in a detected class \u201cAudio\u201d after a first stage (or first iteration), a sub-class \u201cAudio_Volume\u201d after a second stage (or second iteration), and a sub-class \u201cAudio_Volume_Up\u201d (which is a sub-class of the sub-class \u201cAudio\u201d) after a third stage (or third iteration).","In a preferred embodiment, this may be accomplished via attribute value pair (AVP) extraction in a top-down fashion. Thus, each stage or level in the multi-stage system acts as an elemental AVP extractor or semantic analyzer of the sentence. The advantage is that the multi-stage system of the invention is not tagging each word with labels as would occur in a statistical parser or attaching a semantic label as would occur in a linguistic parser, rather the multi-stage system is adding class, sub-class, and sub-class (of the sub-class) information, which is far simpler to do. Also, the methodology is iterative because the same process is applied at each subsequent level with only finer and finer class labels.","Table 1 below is an example of the multi-level class labels (e.g., hierarchical structure) that may be associated with the audio example:",{"@attributes":{"id":"p-0047","num":"0046"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"4","colwidth":"56pt","align":"left"}}],"thead":{"row":[{"entry":"TABLE 1"},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Level 1:","AUDIO",{},{}]},{"entry":["Level 2:","AUDIO_RADIO","AUDIO_VOLUME",{}]},{"entry":["Level 3:","Aud._Radio_on","Aud._Radio_off","A_Radio_Station"]},{"entry":[{},"Aud._volume_down","Aud._volume_up"]},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}}]}}}}},"In order to be able to decode (or recognize) the free form speech, an initial training data set may be used. The process is automated wherein a small model is built with a relatively small data set. Then, the training process iterates when new data is added, using the initial model to label the new data set.","Further, the multi-stage system can also be employed with lower level parsers or metadata. That is, most of the intent determination processing uses the hierarchical action classification approach of the invention. However, when the system gets down to some very specific part of the user request, e.g., complicated navigation request that has a \u201cto city,\u201d a \u201cfrom city,\u201d and\/or some other peripheral information like avoiding the most congested roads, this can make the request complicated. Within the hierarchical action classification of the invention, while this lower level information in the utterance can be annotated, the system can utilize added metadata and\/or use a simple kind of parser, at the lowest stage or level, for extracting items such as \u201cto\u201d and \u201cfrom\u201d information. Thus, instead of building an entire statistical parser for the entire corpus of data, principles of the invention are able to use a smaller domain dependent subset of the data.","Referring initially to , a hierarchical system for extracting user intent from a spoken utterance, according to an embodiment of the invention, is depicted. As shown, the system referred to as a dialog domain detection (DDE) engine  comprises conversational system , command bus , hierarchy manager , intent recognition manager , question module , situation manager , audio input , speech recognition system , and sensors .","Conversational system  functions as a dialog manager. Audio input  represents the spoken utterances captured by the system that are being processed to determine intent. Conversational system  sends the audio input to speech recognition engine , which then decodes the audio and returns text, representative of what the speech recognition engine recognized, back to conversational system . It is to be appreciated that the invention is not limited to any particular speech recognition engine and, thus, any suitable speech recognition system can be employed. By way of example only, the IBM Corporation (Armonk, N.Y.) Embedded ViaVoice\u2122 engine could be employed.","The command bus  serves as a central communication bus between the components of the DDE engine.","Hierarchy manager  (as will be explained in further detail below in the context of ) imposes the top-down iterative structure used by intent recognition manager  (as will be explained in further detail below in the context of ) to extract intent from the spoken utterance of the user. For example, in the audio example, the above-described multi-level class labels in Table 1 may serve as the imposed hierarchical structure.","That is, hierarchy manager  sets the number of levels or stages that intent recognition manager  will traverse for a given intent determination session. More particularly, hierarchy manager dictates, at each level, the permitted inputs and the permitted results (e.g., class labels). Then, intent recognition manager  traverses (top to bottom) the hierarchical structure set by the hierarchy manager. As it traverses down the structure, intent recognition manager  expects hierarchy manager  to inform it, at this level, what structure can be imposed. Thus, intent recognition manager keeps referring back to the hierarchy manager.","Intent recognition manager  has an additional function. It is also serves as an interface for the logical, multi-tiered view of the user-input sentence. Conversational system  may utilize such a logical view of the sentence.","Thus, the intent gets clarified as the intent recognition manager walks down the structure. As the hierarchy manager informs that it can provide certain information, the intent recognition manager walks down the structure and determines a particular intent at each level, from broad to narrow. The particular intent determined at each level is referred to herein as an \u201cinterpretation.\u201d In the audio example, the top level intent is going to be the audio system. However, this does not mean much since there are any number of actions that can be taken with respect to the audio system. The next level could determine that the user is referring to a radio station. The next level could determine a particular radio station that the user wishes to be selected. Thus, instead of saying \u201cXM Radio,\u201d \u201cset radio channel,\u201d and \u201cchannel ,\u201d the DDE engine of the invention permits the user to say \u201cI want to listen to channel .\u201d Therefore, the intent recognition manager starts with a vague picture, or actually with nothing, and tries to come up with a highly tuned view of what the intent is.","Question module  generates questions that can be asked of the user that may be used to assist the system with determining intent. As is known, dialog managers are able to coordinate the asking of questions to a speaker, the responses to which further clarify any ambiguity that remains from the previous user input. Thus, as is known, question module may comprise a text-to-speech engine capable of generating questions that are audibly output to the user. The responses are processed through the speech recognition engine and provided to the conversational system which coordinates their use with the intent recognition manager. Further, when an intent is determined by the system, question module  could serve to ask the user to confirm that intent before the system sends the appropriate command(s) to the target system.","Sensors  may comprise one or more sensors that describe external situations (e.g., weather, speed, humidity, temperature, location via a global positioning system, etc.) and personal characteristics (e.g., biometrics\u2014voice, face characteristics, tired, sleepiness conditions). This information, coordinated by situation manager , may also be used to determine intent of the user and\/or assist in providing a response to the user.","While the invention is not limited to any particular question module architecture or external situation manager architecture, examples of techniques that could be employed here are described in U.S. Pat. Nos. 6,092,192; 6,587,818; and 6,236,968.","Referring now to , a hierarchy manager, according to an embodiment of the invention, is depicted. As shown, hierarchy manager ( in ) comprises parser , labeler , semantic processing module , sequencing module , topic , function and data , text input , and training module .","Parser  receives as input text . It is to be appreciated that text  represents the decoded speech, i.e., the result of the audio input ( in ) being decoded by the speech recognition engine ( in ). The role of parser  is to tag the parts of speech of the decoded text, e.g., nouns, verbs, other grammatical terms or phrases. The parser can utilize meta information or even external mark up to describe the tagged portions of the text.","Labeler  separates function and non-function words in the text. That is, it is understood that some words in the text are more valuable (function words) than other words (non-function words) in determining intent. To do this, the words in the text are weighted by the labeler. The weighting may be done by accessing the domain dependent model and scoring the words in the text against all potential words. The importance of the word depends on its score, i.e., words with higher scores are considered more important. Words at or above a threshold score may be considered function words, while words below a threshold score may be considered non-function words.","Semantic processor  then interprets the scores assigned by the labeler. For example, the semantic processor may determine for a given input sentence that terms associated with audio have more weight than terms associated with climate control. Thus, the semantic processor accepts all the interpretations, does a relative scoring, applies a threshold, and decides, for example, that the top three interpretations should be taken as the most relevant ones.","Interpretation means intent in this context. Thus, for each input utterance, the labeler produces a list of interpretations and attendant scores. Since this is a statistical approach, there are no unambiguously correct labels produced, but instead a list of interpretations covering all possible interpretations. The semantic processor applies intelligent thresholding to discard low scores that are possible but of low probability based on prior knowledge or simple thresholding. Prior knowledge can include user knowledge derived from the training data, and simple thresholding can includes retaining a fixed number of interpretations (e.g., three), or retaining all interpretations within a fixed percentage of the best scoring label. These are all parameters that can be made available to an agent deploying the system via operating panels. By way of one example, semantic processor  may employ techniques disclosed in U.S. Pat. No. 6,236,968.","The interpreted result is a three-tuple (a group of three sub-results). That is, in this particular embodiment, to \u201cunderstand\u201d a command three entities are extracted and analyzed: (1) the machine (target or topic ) that is operated upon (e.g., Audio. Navigation); (2) the action (function ) to be performed (e.g., switch, turn, move); and (3) the data  that is provided with the action (e.g., on\/off, up\/down, left\/right). By way of example, Table 1 above illustrates the hierarchical structure from which the three-tuple may be determined. It is to be understood that while hierarchy manager  and intent recognition manager  are illustrated in  as logically separate components, the components may be implemented in a single functional module due to their tightly coupled functionality.","Sequencing module  is used to apply global rules on which part of the sentence is more important because, for example, it is first in order in the sentence or because it is the premise of the sentence or because the user used more emphasis on it.","The idea of sequencing or timing here relates to separating, within a complex request from the user, the primary request from a secondary one. For example, where the target system is a navigation system, assume a user says \u201cFind me a McDonald's with parking.\u201d The principal request is find me a McDonald's. The parking is a secondary request. The sequencer informs the semantic processor that the concept of \u201cfinding a McDonald's\u201d should take precedence or is more important than the concept of \u201cparking.\u201d","Such sequencing may be determined from any nuances in the user's utterance that guide the search for the correct interpretation. An emphasized word or phrase carries more weight. The speeding up of a phrase within a sentence may carry additional indicators of importance, etc. So this module attempts to perform a fine-grained analysis of the user's nuances.","Training module  serves to train parser , labeler , and semantic processor .","Referring now to , an intent recognition manager, according to an embodiment of the invention, is depicted. As shown, intent recognition manager ( in ) comprises weight computation module , pruning module , list preparation module , feedback , and external input .","Weight computation module  computes the weights of the different words in the user utterance and applies two kinds of quantitative tests. The first is to compute whether the words in the utterance are above a fixed threshold. This is the rejection mechanism which decides whether to accept the user utterance for analysis or reject it outright as being outside the realm of its capability. Systems built for use in a car are unlikely to \u201cunderstand\u201d questions about other general subjects. In other words, it has to be able to detect that the user used words that are outside its vocabulary. The rejection mechanism is one way to do this. The second quantitative test is the confidence scores. These are the relative scores of the multiple interpretations of the user utterance.","Pruning module  prunes the list from weight computation module . The output from weight computation module  nominally will include all possible candidate interpretations. Pruning module  decides which ones are worth keeping. Some scores from weight computation module  may be too small to consider, not relevant, or too small in magnitude relative to the top scoring interpretations. A \u201cworthiness\u201d test may be derived from the training data. Further, the pruning module can include a control panel and additional controls that can be adjusted with input from customer satisfaction tests (feedback ).","List preparation module  prepares the final intent list. The search for the interpretation is usually done in a hierarchical fashion with each level in turn revealing the topic, function, and data. Hence, the scoring, pruning and list preparing tasks are iterative as the scores are carried from one level to the next. In one embodiment, the top three scorers from the top level are expanded to the next level. The top three are appropriate it has been proven from computing with training data that 98.5% of the time the correct interpretation is within the top three results.","In addition, external inputs  (e.g., other intent recognition scores) can be utilized to generate the list in .","Referring now to , a confidence\/rejection module, according to an embodiment of the invention, is depicted. It is to be understood that  depicts the confidence score and rejection mechanisms shown in weight computation module  of .","More particularly, in one embodiment, the confidence score for an utterance is the ratio of words in-vocabulary to the total number of words in the utterance. Hence, if all the words in the utterance are found in the system's vocabulary, then the confidence score is 1. If none are, it is zero. If the ratio is less than 0.5, then the utterance is rejected. Block  computed the confidence score and block  applies the rejection mechanism.","This operation can also be understood as follows. The confidence score tries to determine how many of the words are in the system vocabulary versus out of the system vocabulary. If all of the words are in the vocabulary, the word scores are accepted as is.","If a fraction of the words are not in the vocabulary, then those words are handicapped to the extent they are not in the vocabulary. For example, if 75 percent of the words are in the vocabulary, every score coming out of the word score computation is handicapped (i.e., by multiplying by 0.75). That cascades down the hierarchy. The siblings are also penalized to that extent.","Referring now to , a run-time methodology for use in hierarchically extracting user intent from a spoken utterance, according to an embodiment of the invention, is depicted.","In general, the input utterance is applied to the system (i.e., applied against the system model) and the system will return an interpretation, e.g., a three-tuple comprising [topic][function][data]. Hence, an input \u201cturn the volume up\u201d will generate multiple interpretations:","[Audio][Volume][up]","[Climate][temperature][up]","[Audio][Volume][down]","Each will have a computed score associated with it.  shows a flow chart of how these interpretations are generated. An initial model tree created during training contains all possible paths that can yield a result. Traversing down this tree from the top node to a leaf node yields several interpretations per level. So, for example, nine interpretations from the top level are pruned down to three. Each of the nodes of the tree are expanded to their child nodes. For example, \u201cAudio\u201d above may yield \u201cAudio_Volume,\u201d \u201cAudio_Treble,\u201d and \u201cAudio_CD\u201d), and \u201cClimate\u201d may yield three more of its children. Similarly, \u201cAudio_Volume\u201d will be split into its children. The process stops after three levels. In some cases, there may be fewer than three levels simply because there is not adequate data to warrant a third level.","Thus, as specifically shown in :","Step \u2014Push top-level interpretation that operates with the text input .","Step \u2014Assign scores for interpretations from step .","Step \u2014Get next interpretation.","Step \u2014Check if anything is left (None Left?).","Step \u2014If \u201cNo\u201d for step , then check if node is expandable","Step \u2014If not expandable, then add to interpretation list and go to get next interpretation (step ).","Step \u2014Otherwise (if expandable), calculate children and go to assign scores (step ).","If none left in step , then methodology is done ().","Referring now to , a training methodology for use in hierarchically extracting user intent from a spoken utterance, according to an embodiment of the invention, is depicted.","In general, first, we decide on the domain in which this system will operate. Data is then collected in that domain, rejecting all data that is outside the domain. These data are then carefully divided into multiple \u201ctopic\u201d domains. Within each \u201ctopic,\u201d the sentences are further bucketed into sub-domains by \u201cfunction,\u201d and then each function into \u201cdata.\u201d This process of bucketing may be done using a tool that allows for easy \u201ctagging\u201d of such data in a visual manner. We may then gather more data in sub-domains that do not have adequate representation. The more common approach is to build a model, run a test with data withheld from the training set. \u201cTopics\u201d that perform poorly are candidates for adding more sentences. This approach allows for more targeted data collection.","Thus, as specifically shown in :","Step \u2014Collect text data in domain.","Step \u2014Split data into individual domains.","Step \u2014Tag domains.","Step \u2014Gather more data.","Step \u2014None left? If no, go to step .","Step \u2014Build system model, if yes in step .","Further, we preferably split training data into one set for each node in the hierarchy, and build a model for each node.","Referring lastly to , a block diagram of an illustrative implementation of a computing system for use in implementing techniques of the invention is shown. More particularly,  represents a computing system which may implement the user intent extraction components and methodologies of the invention, as described above in the context of . The architecture shown may also be used to implement a target system.","In this particular implementation, a processor  for controlling and performing methodologies described herein is coupled to a memory  and a user interface  via a computer bus .","It is to be appreciated that the term \u201cprocessor\u201d as used herein is intended to include any processing device, such as, for example, one that includes a CPU (central processing unit) or other suitable processing circuitry. For example, the processor may be a digital signal processor (DSP), as is known in the art. Also the term \u201cprocessor\u201d may refer to more than one individual processor. However, the invention is not limited to any particular processor type or configuration.","The term \u201cmemory\u201d as used herein is intended to include memory associated with a processor or CPU, such as, for example, RAM, ROM, a fixed memory device (e.g., hard drive), a removable memory device (e.g., diskette), flash memory, etc. However, the invention is not limited to any particular memory type or configuration.","In addition, the term \u201cuser interface\u201d as used herein is intended to include, for example, one or more input devices, e.g., keyboard, for inputting data to the processing unit, and\/or one or more output devices, e.g., CRT display and\/or printer, for providing results associated with the processing unit. The user interface may also include one or more microphones for receiving user speech. However, the invention is not limited to any particular user interface type or configuration.","Accordingly, computer software including instructions or code for performing the methodologies of the invention, as described herein, may be stored in one or more of the associated memory devices (e.g., ROM, fixed or removable memory) and, when ready to be utilized, loaded in part or in whole (e.g., into RAM) and executed by a CPU.","In any case, it should be understood that the components\/steps illustrated in  may be implemented in various forms of hardware, software, or combinations thereof, e.g., one or more digital signal processors with associated memory, application specific integrated circuit(s), functional circuitry, one or more appropriately programmed general purpose digital computers with associated memory, etc. Given the teachings of the invention provided herein, one of ordinary skill in the related art will be able to contemplate other implementations of the elements of the invention.","Although illustrative embodiments of the present invention have been described herein with reference to the accompanying drawings, it is to be understood that the invention is not limited to those precise embodiments, and that various other changes and modifications may be made by one skilled in the art without departing from the scope or spirit of the invention."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 7"}]},"DETDESC":[{},{}]}
