---
title: Method and system for iterative computed tomography reconstruction
abstract: Methods and systems for iterative computed tomography add an auxiliary variable to the reconstruction process, while retaining all variables in the original formulation, A weighting operator or filter can be applied that causes the Hessian with respect to an image of the cost function to be well-conditioned. An auxiliary sinogram variable distinct from both a set of actual image measurements and from the set of projections computed based on an image can be applied to iteratively update during the statistical iterative image reconstruction, with applied conditions that causes the Hessian with respect to an image of the cost function to be well-conditioned.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09524567&OS=09524567&RS=09524567
owner: InstaRecon
number: 09524567
owner_city: Urbana
owner_country: US
publication_date: 20150622
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["PRIORITY CLAIM AND REFERENCE TO RELATED APPLICATION","STATEMENT OF GOVERNMENT INTEREST","FIELD","BACKGROUND","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS"],"p":["The application claims priority under 35 U.S.C. \u00a7119 from prior provisional application Ser. No. 62\/015,446, which was filed Jun. 22, 2014.","This invention was made with government support under R44EB014669 awarded by National Institutes of Health, National Center for Advancing Translational Sciences (NCATS) and National Institute of Biomedical Imaging and Bioengineering (NIBIB). The government has certain rights in the invention.","A field of the invention is computed tomography. The invention is applicable to various computed tomography techniques, including, for example, X-ray Computed Tomography (CT), single photon emission computed tomography (SPECT), and positron emission tomography (PET)","Computed tomography techniques including X-ray Computed Tomography (CT), single photon emission computed tomography (SPECT), positron emission tomography (PET) are well-established imaging modalities. Conventional image reconstruction in such imaging modalities and others has been performed using a method known as filtered backprojection (MP). More recently, iterative image reconstruction methods have been introduced, with the main motivation being x-ray dose reduction. One goal of the iterative techniques is to lower the dose of radiation experience by a subject being imaged. The lower doses and under sampling (when used, e.g., to reduce dose) provide challenges to compute high contrast and clear images. Another goal of iterative techniques, is to provide high-quality reconstruction from data acquired by advanced detector systems such as photon counting detectors, for which FBP may not provide the desired quality. Recent iterative techniques utilized an image fidelity function to encourage fidelity to the measured data, and an edge-preserving regularization function such as total variation or qGGIVERF that encourages smoothness in image areas with little variation, while recovering sharp boundaries to preserver image resolution.","DeMan et al. U.S. Pat. No. 8,971,599 discloses methods for iterative tomographic reconstruction, Three embodiments are discussed that use a ramp filter as a preconditioner. in a first embodiment, the iteration applies a deconvolution filter and reevaluates to see if completion criteria have been satisfied. This first embodiment uses a change of variables (that is, replacing all instances of the image variable x with another variable z, using the equivalence x=Fz for a filter F) . The approach of the first embodiment is highly similar to the classical formulation. of preconditioning. In a second embodiment, a preconditioner is applied to the simultaneous image update steps of an iterative reconstruction, where the preconditioner approximates the inverse Hessian of the cost function being optimized. In a variation, the preconditioner is applied only to the data, fidelity term (called data fit term in the \u2018599 patent) and not to the regularization term. This strategy is intended to accommodate edge preserving priors, which are commonly used in current iterative reconstruction methods. However, by applying the preconditioner in this way, it ceases to be a proper preconditioner. Instead, this approach fundamentally alters the minimization process such that the fixed point of the iteration no longer corresponds to the fixed point of the original update sequence. One drawback of the techniques in the \u2018599 patent arises in the common case that statistical weighting matrices are used in the update steps. These weighting matrices degrade the approximation of the ramp filter (preconditioner) as an inverse Hessian operation, and therefore degrade the preconditioning effect. In particular, the presence of the weighting matrix W, which often has large dynamic range and therefore causes the Hesssian of the data fidelity term in the cost function to be poorly conditioned, has a detrimental effect on the rate of convergence of the iterative algorithm, ultimately requiring more iterations to achieve a desired accuracy.","Embodiments of the invention include methods and systems for iterative computed tomography. In preferred methods and systems of the invention, an auxiliary variable is added to the reconstruction process, while retaining all variables in the original formulation. Preferred embodiments apply a weighting operator or filter that causes the Hessian with respect to image of the cost function to be well-conditioned. In preferred embodiments, an auxiliary sinogram variable distinct from both a set of actual image measurements and from a set of projections computed based on the image is applied to iteratively update during the statistical iterative image reconstruction, with applied conditions that causes the Hessian with respect to image of the cost function to be well-conditioned:","Unlike the techniques in the \u2018599 patent discussed in the background, preferred methods of the invention do not degrade the approximation of the ramp filter as an inverse Hessian operation. Preferred methods instead relate the image to an auxiliary variable without statistical weighting, but using a ramp filter (or other norm weighting), while the statistical weighting can remain in a different term, relating the measured projection data derived from the CT scan data to the auxiliary variable, without using the ramp filter. More generally, preferred embodiments apply a weighting operator or filter that causes the Hessian with respect to image of the cost function to be well-conditioned. The auxiliary variable is added to the reconstruction process, while retaining all variables in the original formulation. The preferred iterative reconstruction methods use both the image variable and the auxiliary variable, and specifically updates to the image variable depend on the auxiliary variable, but not on the measured projection data, and the updates to the image variable do not need to include the statistical weighting. Also unlike the variations of the \u2018599 patent that alter the minimization process such that the fixed point of the iteration no longer corresponds to the fixed point of the original update sequence, methods of the invention maintain mathematical equivalence with the solution of the original update process.","Preferred methods and systems provide for iterative reconstruction tomography images including, for example, CT images, SPECT and PET images. Preferred embodiments can reduce the computation needed to produce an image of desired quality using an iterative algorithm. This can be used to obtain results more quickly using the same computing hardware, or to reduce the cost and\/or the size and weight and\/or the power consumption of the computing hardware needed to produce the results in a desired time. Because iterative reconstruction can be used to reduce the x-ray dose to which the subject is exposed during a CT scan, and\/or improve image quality and reduce artifacts, e.g., due to metal implants, the preferred methods and systems can be used to provide the same benefits at a lower cost and\/or at a higher patient throughput and smaller delay for reconstruction compared to traditional CT methods and systems.","Those knowledgeable in the art will appreciate that embodiments of the present invention lend themselves well to practice in the form of computer program products. Accordingly, it will be appreciated that embodiments of the present invention may comprise computer program products comprising computer executable instructions stored on a non-transitory computer readable medium that, when executed, cause a computer to undertake methods according to the present invention, or a computer configured to carry out such methods. The executable instructions may comprise computer program language instructions that have been compiled into a machine-readable format. The non-transitory computer-readable medium may comprise, by way of example, a magnetic, optical, signal-based, and\/or circuitry medium useful for storing data. The instructions may be downloaded entirely or in part from a networked computer. Also, it will be appreciated that the term \u201ccomputer\u201d as used herein is intended to broadly refer to any machine capable of reading and executing recorded instructions, it will also be understood that results of methods of the present invention may be displayed on one or more monitors or displays (e.g., as text, graphics, charts, code, etc.), printed on suitable media, stored in appropriate memory or storage, etc.","Preferred embodiments of the invention will now be discussed with respect to the drawings. The drawings may include schematic representations, which will be understood by artisans in view of the general knowledge in the art and the description that follows. Features may be exaggerated in the drawings for emphasis, and features may not be to scale",{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 1","b":["10","12","14","16","20","22","28","24","26","28","30","22","32","20","14","34","10","20","12","10","10"]},"A preferred iterative reconstruction module applies an iterative reconstruction with an auxiliary variable and a weighting operator or filter that causes the Hessian with respect to image of the cost function to be well-conditioned. In preferred embodiments, the weighting operator or filter is a ramp filter. Generally, the reconstruction method of preferred embodiments is related to iterative reconstruction techniques that use the Augmented Lagrangian. Augmented Lagrangian methods of iterative reconstruction (IR) converge more quickly than techniques that directly minimize the objective function used in the iterative reconstruction, in particular in the case of weighted or complicated discrepancy terms between the acquired sinogram and the sinogram computed based on the image.","Before discussing introductory concepts and the preferred the iterative reconstruction, symbols used in the following description and the in claims will be defined. The following table defines symbols that are used.",{"@attributes":{"id":"p-0018","num":"0017"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"168pt","align":"left"}}],"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}},{"entry":["Symbol","Definition"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["f","Image variable - Representing a 2D array of values"]},{"entry":[{},"defining pixels in the case of 2D imaging, or a 3D array"]},{"entry":[{},"of values defining voxels in the case of 3D imaging"]},{"entry":["g","A measured sinogram variable that is a set of actual"]},{"entry":[{},"measurements derived from a set of actual CT data. It is"]},{"entry":[{},"a 2D array of values in the case of 2D imaging, and can"]},{"entry":[{},"be either a 2D or a 3D array of values in the case"]},{"entry":[{},"of 3D imaging"]},{"entry":["R","Projection operator (\u201cProjector\u201d), also known as"]},{"entry":[{},"the \u201csystem matrix\u201d"]},{"entry":["Rf","Set of measurements computed based on the image"]},{"entry":["h(Rf, g)","A measure of the discrepancy between the acquired"]},{"entry":[{},"measurement data g and the computed measurements Rf"]},{"entry":["\u03a6(f)","Regularization functional that represents prior"]},{"entry":[{},"information about the image f."]},{"entry":["W","Singoram discrepancy weighting matrix or operator"]},{"entry":["u","Auxiliary sinogram variable representing an approxi-"]},{"entry":[{},"mation to the set of measurements computed based on"]},{"entry":[{},"the image. This variable has the same dimensions"]},{"entry":[{},"as the measured sinogram variable g."]},{"entry":["\u03b7","Auxiliary sinogram variable, serving as a Lagrange"]},{"entry":[{},"multiplier (also known as dual variable)"]},{"entry":["R","Adjoint of the Projection operator, also known as"]},{"entry":[{},"backprojection operator"]},{"entry":["f, u, \u03b7, etc.","The values of f, u, \u03b7, etc. at iteration number n"]},{"entry":["L(u, f, \u03b7)","Augmented Lagrangian (AL) (with completion"]},{"entry":[{},"of the square)"]},{"entry":["L(u, f, \u03b7)","\u03c1 - weighted Augmented Lagrangian (\u03c1 - weighted AL)"]},{"entry":[{},"(with completion of the square)"]},{"entry":[{"o":"V","sub":"f","sup":"2 "},"The Hessian of the AL with respect to f"]},{"entry":["R\u0393 R","The Hessian with respect to f of the image discrepancy"]},{"entry":[{},"term of the AL"]},{"entry":["s","Auxiliary sinogram variable representing an approxi-"]},{"entry":[{},"mation to the auxiliary sinogram variable u, or (in the"]},{"entry":[{},"case of ADMM-\u03c1 with extended weighting"]},{"entry":[{},"and additional splitting) chosen such that its"]},{"entry":[{},"rebinned version Es represents an approximation"]},{"entry":[{},"to the auxiliary sinogram variable u"]},{"entry":["\u03b7, \u03b7, \u03b7, \u03b7","Auxiliary sinogram variables, serving as Lagrange"]},{"entry":[{},"multipliers (also known as dual variables)"]},{"entry":["z","Auxiliary image domain variable approximating"]},{"entry":[{},"the image"]},{"entry":["\u0393","Weighting operator"]},{"entry":["E\u03c1E","Weighted ramp filtering operation"]},{"entry":["\u03c1","Square root of the \u03c1 operator (e.g., square root of"]},{"entry":[{},"the ramp filter)."]},{"entry":["{circumflex over (\u03c1)}","The image domain representation or approximate"]},{"entry":[{},"representation of the \u03c1 operator."]},{"entry":{}},{"entry":[{"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"msup":{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}}}}},"Square root of the image domain representation or approximate representation of the \u03c1 operator."]},{"entry":{}},{"entry":["x","Image-domain auxiliary variable defined such that"]},{"entry":[{},{"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msup":{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}},"mo":"\u2062","mi":"x"}}}}]},{"entry":[{},"is the image, or approximates the image."]},{"entry":[{},"\u201cpre-image auxiliary variable\u201d."]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},"Denote the reconstructed image by the image variable f. Image variable f can represent a 2D array of numbers in the case of 2D imaging, e.g., cross-sectional CT, or a 3D array of numbers in the case of 3D imaging, e.g., conebeam or helical conebeam CT. As known to artisans, such arrays may be organized in various desired forms in computer memory. Because any data array of any dimension can be arranged into a 1D array, or vector, for the purposes of this disclosure, it will be convenient to consider all data arrays, regardless of dimension, as vectors. It will also be noted, as used in the disclosure and the claims, that a vector approximately proportional to the gradient indicates that the vector is not perpendicular to the gradient (not equal to \u03c0\/2, and preferably less than \u03c0\/2).","A typical formulation for conventional iterative reconstruction involves minimization of a cost function to generate the reconstructed image f. This minimization problem and cost ftinction can be generally written as",{"@attributes":{"id":"p-0021","num":"0020"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["f","opt"]},"mo":"=","mrow":{"mrow":[{"munder":{"mrow":{"mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":["m","in"],"mo":"\u2062"}},"mo":"\u2062","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mi":"f"},"mo":"\u2062","mrow":{"mi":"h","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["Rf","g"],"mo":","}}}},{"mi":"\u03a6","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"1"}}]}}}}},"where fis the final reconstructed image. The measured sinogram variable g usually corresponds to a 2D or 3D data array. For example, it would correspond to a 2D data array in the case of 2D imaging, or in the case of 3D helical imaging with a line detector when g is indexed by view angle and detector index. It would correspond to a 3D array, for example in the case of 3D conebeam CT imaging with a multirow or area detector. In some applications, such as dual or multi-energy (also called multispectral) 3D CT, the measured sinogram variable g may correspond to a 4 dimensional array. Again, for the purposes of this disclosure, we will consider g to be a vector.","The measured sinogram variable g is generated from CT data by conventional techniques, such as those described by J. Hsieh, \u201cComputed Tomography: Principles, Design, Artifacts, and Recent Advances, Second Edition, SPIE, Wash. USA, 2009, which may include negative logarithm, and various corrections steps for deviations from ideal models. In the case of iterative reconstruction, fewer corrections may be used in generating the measured sinogram data g, and instead some of the non-idealities and even the negative logarithm operation may be included, if desired, in the modeling incorporated into the iterative reconstruction, as described below. The projection operator R (\u201cProjector\u201d), also known as the system matrix, incorporates the model of the data acquisition process. For example, it might account for the line integral measurement, for the shape of the X-ray beam and of the focal spot, for the shape of the voxel or other basis function used to represent the image, for the geometry of the CT scanner, for the shape and properties of the detector, etc. Given an image f, the quantity Rf is the set of measurements (also called \u201csinogram\u201d or \u201cprojections\u201d) computed based on the image f. The function h(Rf,g) is a measure of the discrepancy between the acquired projection data g and the computed projection data Rf generated by the current image estimate off using the projection operator R. Accordingly, the first term in the cost function in (1) is sometimes called a data fidelity term. The adjoint Rof R is a backprojector operator. This backprojection operator may however, be somewhat different from conventional approximate backprojection operators used in filtered back projection algorithms, which may not have an exact adjoint relationship to a desired projector R.","Examples of h include a negative log-likelihood term, such as a Poisson-likelihood, a weighted sum-of-squared deviations, or other nondecreasing and typically convex functional, such as a weighted sum of generalized Huber functions of elements of the difference g\u2212Rf. With such or other choices of h, it too can be used to express physical and\/or statistical modeling aspects of the CT data acquisition, which may result in g not exactly corresponding to true projections or sinogram, as described by J. Hsieh, \u201cComputed Tomography: Principles, Design, Artifacts, and Recent Advances, Second Edition, SPIE, Wash. USA, 2009. The step of generating g from the CT data may be even reduced, if desired, to no operations (i.e., using the actual CT data in unmodified form), by appropriate choice of function h (e.g, incorporating desired transformations or corrections into h), although it is usually computationally advantageous to perform at least some of these transformations and corrections in a separate step of generating g.","The \u03a6(f) term in (1) is a regularization functional that represents prior information about the image f. Common choices for \u03a6(f) encourage smoothness in the image by penalizing local differences, that is, differences between voxels in a small neighborhood about each voxel. Typically non-quadratic penalties, with notable examples including generalized Huber functions of the local differences, the so-called qGGMRF (random field) penalty, total variation, or other penalties that promote sparsity of the local differences or of some transform of the image, are used to introduce an edge preserving property to the regularization, or capture additional properties of the images of interest.","The iterative reconstruction algorithm involves updates to the algorithm variables to progressively perform the minimization in (1). A stopping criterion is employed to terminate the iterative algorithm. The stopping criterion might include, for example, running for a fixed number of iterations, examining the discrepancy (e.g., some norm of the difference g\u2212Rf) between the measured sinogram g and computed measurements (projections) Rf, or detecting when the maximum absolute value of an element (pixel or voxel, as the case may be) of an update of the image f is below some threshold.","As a specific example, we consider the penalized weighted least squares (PWLS) formulation for h, although the method described here is generally applicable to other functions. The PWLS cost function and associated optimization problem are",{"@attributes":{"id":"p-0028","num":"0027"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["f","opt"]},"mo":"=","mrow":{"mrow":[{"munder":{"mrow":{"mi":["arg","min"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mi":"f"},"mo":["\u2062","\u2062"],"mfrac":{"mn":["1","2"]},"msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":["g","Rf"],"mo":"-"}},"mi":"W","mn":"2"}},{"mi":"\u03a6","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"2"}}]}}}}},"The weighted squared norm \u2225\u03bd\u2225of a vector \u03bd denotes the quantity \u03bd\u0393\u03bd, for a positive-definite symmetric operator or matrix \u0393, where \u03bddenotes the transpose of vector v, or equivalently \u2225\u03bd\u2225=<\u0393\u03bd, \u03bd>, where <\u2018,\u2019>denotes the Euclidean inner product between vectors, and \u0393\u03bd is the application of the linear operator \u0393to \u03bd. (Alternatively, the square of the weighted norm can be implemented as \u2225\u0393\u03bd\u2225=<\u0393\u03bd, \u0393\u03bd>, where \u0393is a square root of operator \u0393, and satisfies (\u0393)\u0393=\u0393, where Adenotes the adjoint of operator A.) The notation for the norm simplifies to \u2225\u03bd\u2225when the weight operator is the identity operator. Here too, as in the rest of this disclosure, vector v may represent a physical multidimensional array. In Equation (2) W is a diagonal matrix that encapsulates the statistical or other modeling information about the data g. For example, the elements on the diagonal of W can be set to the inverse of an estimate of the variance of each measurement g, as follows\n\nW=Ie\u2003\u2003(3)\n","where Idenotes the number of incident photons when measured in an air scan, with or without a bow-tie filter as appropriate. Alternatively, W can be set to some other weighting that represents the confidence about particular measurements. Another typical usage is to down-weight rays passing through highly attenuating objects such as metal.","Direct minimization of the cost function in (1) can be challenging, due to the large scale of the problem along with the presence of the weighting matrix W, which often has large dynamic range and therefore causes the Hesssian of the data fidelity term in the cost function to be poorly conditioned, or other aspects of h(Rf\u2212g), which make evaluation of it or of its gradients computationally demanding, and\/or require many iterations to approach convergence. A variable splitting technique addresses these issues. The variable splitting technique converts the unconstrained minimization of (1) to the constrained minimization problem:",{"@attributes":{"id":"p-0032","num":"0031"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["f","opt"]},"mo":"=","mrow":{"mrow":[{"munder":{"mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":["m","in"],"mo":"\u2062"}},"mi":"f"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"h","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["u","g"],"mo":","}}}},{"mi":"\u03a6","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}}}},"br":{}},"This is in turn converted back to an unconstrained minimization problem using the Augmented Lagrangian (AL) (with completion of the square)",{"@attributes":{"id":"p-0034","num":"0033"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","u","\u03b7"],"mo":[",",","]}}},{"mrow":[{"mi":"h","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["u","g"],"mo":","}}},{"mi":"\u03a6","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}},{"mfrac":{"mi":"\u03bc","mn":"2"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":["u","Rf","\u03b7"],"mo":["-","-"]}},"mn":["2","2"]}}],"mo":["+","+"]}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"5"}}]}}}}},"Here u is an auxiliary sinogram-domain variable, representing an approximation to the measurements Rf computed based on the image. Variable \u03b7 is another auxiliary sinogram variable, serving as a Lagrange multiplier (also known as dual variable). We refer to the first term in Equation (5) as the sinogram discrepancy term, and to the third term as the image discrepancy term. Significantly, the constant \u03bc only affects the rate of convergence of the algorithm, not the final resulting image at the point of complete convergence.","The AL can be used in the Augmented Lagrangian Method (ALM), which involves the iteration",{"@attributes":{"id":"p-0037","num":"0036"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msup":[{"mi":"u","mrow":{"mi":"n","mo":"+","mn":"1"}},{"mi":"f","mrow":{"mi":"n","mo":"+","mn":"1"}}],"mo":","}},{"munder":{"mrow":[{"mi":["arg","min"],"mo":"\u2062"},{"mi":["u","f"],"mo":","}]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["u","f"],"mo":[",",","],"msup":{"mi":["\u03b7","n"]}}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"6"}}]}}}},"br":{},"in-line-formulae":[{},{}],"sup":["n+1","n","n+1","n+1"],"i":["u","\u2212Rf"]},"which is repeated for increasing values of n starting from some appropriate initial values at n=0, until some convergence criterion is met. Here symbols with superscript n denote the values of the corresponding variable in iteration number n. Typical choices for initial values for the variables are to set fas the FBP reconstruction of sinogram data g, set u=Rf, and set \u03b7=0.","Variations on the ALM include algorithms that perform the minimization in (6) approximately by block coordinate descent, alternating between minimization with respect to f and u, with the other variable held fixed [J. Eckstein, \u201cAugmented Lagrangian and Alternating Direction Methods for Convex Optimization; A Tutorial and Some Illustrative Computational Results,\u201d RUTCOR Research Report RRR 32-2012, December 2012]. Typically, the most challenging minimization is the one with respect to f, because of the presence of R.","Auxiliary Variable Preferred Method","The present inventors have recognized that the image discrepancy term, i.e., the norm term in (5) can be replaced by a weighted two-norm \u2225\u00b7\u2225or even semi-norm with a positive non-negative definite operator \u0393, which can provide advantages in implementation or in convergence rate. Depending on the choice of weighting operator \u0393, existing theoretical convergence analyses for the resulting optimization algorithms may not exactly apply, but empirical results can still be favorable. In preferred embodiments, particular choices of weighting operators provide advantages. Advantageous choices for \u0393 are those for which the Hessian \u2207L(u, f, \u03b7)=R\u0393R+\u2207\u03a6(f) of the AL with respect to f (which is well-defined when \u03a6 is twice differentiable) or the Hessian R\u0393R with respect to f of the image discrepancy term (last term) of the AL, are well-conditioned (e.g., approximately equal or approximately proportional to the identity operator). This will speed up convergence of the minimization with respect to f, and therefore reduce the computation for the entire iterative algorithm.","Example advantageous choices of \u0393 that improve the conditioning of the composite operator R\u0393R are described next. Consider data in the projection domain denoted by q(t, \u03b8) for 2D (cross-sectional) imaging, or by q(t, \u03b8, r) for 3D imaging, where t denotes the detector index along a detector row, \u03b8 denotes view (or x-ray source) angle, and r denotes detector row index. Then, one favorable choice for the operator \u0393 is the \u03c1 operator, which performs a 1-D filtration (in the t variable) along each row in the projection domain. The frequency response of the \u03c1 operator is the so-called ramp filter response |\u03c9| for frequency variable \u03c9. Owing to the length of the impulse response of this filter in the spatial domain, the \u03c1 operator is typically applied in the frequency domain, using the FFT (Fast Fourier Transform), as is done with conventional filtered backprojection reconstruction. An alternative realization of the \u03c1 operator is to use lower order IIR (Infinite Impulse Response) or FIR (Finite Impulse Response) filters applied in the projection domain that approximate the |\u03c9| response at lower computational cost.","This choice \u0393=\u03c1 of weight operator in the AL produces the \u03c1-weighted AL L, given in the following Equation (8)",{"@attributes":{"id":"p-0044","num":"0043"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["L","\u03c1"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","u","\u03b7"],"mo":[",",","]}}},{"mrow":[{"mi":"h","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["u","g"],"mo":","}}},{"mi":"\u03a6","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}},{"mfrac":{"mi":"\u03bc","mn":"2"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":["u","Rf","\u03b7"],"mo":["-","-"]}},"mi":"\u03c1","mn":"2"}}],"mo":["+","+"]}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"8"}}]}}}}},"This leads to the Hessian of the image discrepancy term (the third term) in the \u03c1-weighted AL (8) being equal to R\u03c1R, which is the identity operation for parallel beam geometries in the continuous variable formulation, or an approximation of the identity operation in the discrete and divergent beam (e.g. fan and cone, with circular or helical) geometries. Thus, minimization of (8) with respect to f is approximately preconditioned by this construction.","Artisans will appreciate that the auxiliary sinogram variable u is distinc from both the measured sinogram g, and from the measurements computed based on the image, Rf.","The utility in this choice of weight can be demonstrated, with the specific example of the PWLS (Penalized Weighted Least Squares) choice for h, and a particular AL-based scheme, although the technique is applicable to other cost function formulations, and other AL-based schemes.","The Alternating Direction Method of Multipliers (ADMM) is a technique that can be interpreted as a particular variant of ALM, involving alternating minimization, which is used to make finding the solution more tractable.  shows a preferred embodiment, with the weighting \u03c1 applied to the image discrepancy term (norm tem) in Equation (5), that is, with the \u03c1-weighted AL in (8) . Initial steps involve data acquisition  and then the initial reconstruction  of the image f. The initial reconstruction can be performed, for example, by a conventional FBP method. Each iteration of the preferred embodiment ADMM algorithm with the \u03c1-weighted AL involves three steps: (i) update the image  by minimizing the \u03c1-weighted AL L with respect to f, to a desired degree of accuracy, i.e., perform the minimization in Equation (9) to a desired degree of accuracy, e.g, using a nonlinear iterative conjugate gradient algorithm, while keeping variables u and \u03b7 fixed at their values at the end of the previous iteration; (ii) update the auxiliary sinogram variable u  by minimizing the \u03c1-weighted AL L with respect to the auxiliary variable u to a desired degree of accuracy, which, in the case of the PWLS choice for h involves performing the minimization in Equation (10) to a desired degree of accuracy, while keeping f and \u03b7 fixed; and (iii) gradient update  step of the Lagrangian Multiplier variable \u03b7 while keeping f and u fixed, per Equation (11). This sequence of steps is repeated, until an appropriate convergence (completion) criterion is met. In variations of the  embodiment, the order of the update steps , , and  is modified, and one or more of the updates is performed more than once per iteration. Equations (9)-(11) respectively illustrate the updates of steps ,  and , in the case of the PWLS choice for h:",{"@attributes":{"id":"p-0049","num":"0048"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"f","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"mrow":[{"munder":{"mrow":{"mi":["arg","min"],"mo":"\u2062"},"mi":"f"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"mi":"\u03a6","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}}},{"mfrac":{"mi":"\u03bc","mn":"2"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msup":[{"mi":["u","n"]},{"mi":["\u03b7","n"]}],"mo":["-","-"],"mi":"Rf"}},"mi":"\u03c1","mn":"2"}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"9"}}]}}}}},{"@attributes":{"id":"p-0050","num":"0049"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"u","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"mrow":[{"munder":{"mrow":{"mi":["arg","min"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mi":"u"},"mo":["\u2062","\u2062"],"mfrac":{"mn":["1","2"]},"msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":["g","u"],"mo":"-"}},"mi":"W","mn":"2"}},{"mfrac":{"mi":"\u03bc","mn":"2"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"u","mo":["-","-"],"msup":[{"mi":"Rf","mrow":{"mi":"n","mo":"+","mn":"1"}},{"mi":["\u03b7","n"]}]}},"mi":"\u03c1","mn":"2"}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"10"}}]}}}},"br":{},"in-line-formulae":[{},{}],"sup":["n+1","n","n+1","n+1"],"i":["u","\u2212Rf"]},"Here, the second term in the cost functions of (9) and (10) is called the image discrepancy term, and the first term in the cost function in (10) is called the sinogram discrepancy term.","The preferred updating with p weighting is labelled \u201cADMM- \u03c1\u201d. The most challenging step from a computational standpoint is the image update (9) which itself must usually be solved by an iterative method. We call these iterations \u201cinner iterations\u201d, and refer to the iterations (for increasing n) over the steps in (9)-(11) as \u201couter iterations\u201d. Through the choice of a \u03c1 weighting, the optimization problem has been implicitly preconditioned. For example, approximating a solution to (9) using a conjugate-gradients (CG) method involves calculating the gradient of the cost function in (9) with respect to f, yielding\n\n\u2207\u03a6()+\u03bc\u03c1(+\u03b7)\u2003\u2003(12)\n","which contains the term R\u03c1R, which as described before, is a well-conditioned matrix (or operator), approximating an identity operation. Because the convergence rate of CG is affected by the conditioning (more specifically, the clustering of eigenvalues) of the Hessian of the cost function, the convergence of the inner iterations for minimization of (9) will be accelerated by the \u03c1 weighting, resulting in a speedup of the entire process.","Other minimization strategies can be employed in minimizing (9). The ordered subsets (OS) method approximates the gradient calculation of (12) by using only a subset of views, the subsets indexed by in m=1, . . . , M,\n\n\u2207\u03a6()+\u03bc\u03c1(+\u03b7)\u2003\u2003(13)\n","Each of the M subsets contains",{"@attributes":{"id":"p-0056","num":"0055"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mfrac":{"mn":"1","mi":"M"}}},"br":{}},"The introduction of \u03c1 weighting increases somewhat the complexity of the minimization problem in (10), whose solution has the form\n\n=(+\u03bc\u03c1)(\u03bc\u03c1(+\u03b7)\u2212)\u2003\u2003(14)\n","The operation (W+\u03bc\u03c1) is difficult to invert directly, and similar to (9), an iterative process with a few \u201cinner iterations\u201d is also required, using the gradient with respect to u of the cost function in (10), which is given by\n\n+\u03bc\u03c1(\u2212\u03b7)\u2212\u2003\u2003(15)\n","However, unlike finding the solution of (9), the only operators that need to be applied in the iterative solver used to compute the update in (14) are the diagonal W and the \u03c1 weightings, which are significantly cheaper computationally than the projector R, or backprojector R, especially if low order projection-domain filter approximations to \u03c1 are used. Finallly, the step in (11) is a simple update and requires negligible computation compared to the other steps.","Alternatively, an additional variable split s=u can be introduced to detangle the application of the W and \u03c1 weightings, allowing for closed-form updates of the auxiliary variables. The \u03c1-weighted AL formulation in this case is",{"@attributes":{"id":"p-0061","num":"0060"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["L","\u03c1"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","u","s"],"mo":[",",",",",",","],"msub":[{"mi":["\u03b7","u"]},{"mi":["\u03b7","s"]}]}}},{"mrow":[{"mi":"h","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["s","g"],"mo":","}}},{"mi":"\u03a6","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}},{"mfrac":{"msub":{"mi":"\u03bc","mn":"1"},"mn":"2"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":["u","Rf"],"mo":["-","-"],"msub":{"mi":["\u03b7","u"]}}},"mi":"\u03c1","mn":"2"}},{"mfrac":{"msub":{"mi":"\u03bc","mn":"2"},"mn":"2"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":["s","u"],"mo":["-","-"],"msub":{"mi":["\u03b7","s"]}}},"mn":["2","2"]}}],"mo":["+","+","+"]}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"16"}}]}}}}},"Variable s is another singoram-domain auxiliary variable, which represents an approximation to the auxiliary sinogram variable u, and which is distinct from all three quantities g, u, and Rf. The \u03c1-weighted AL with the indicated additional variable split in (16) is useful more generally when h(s, g) is a relatively complicated function of s, for example when h(s, g) comprises a statistical model for g such as a negative log likelihood for a Poisson measurement model in SPECT or PET.","Again, applying the ALM algorithm to the \u03c1-weighted augmented Lagrangian in (16), the updates for the different variables can be performed one at a time, in sequence. For the ADMM applied to this AL, the update sequence for one iteration consists of three minimization steps, with respect to the variables f, u, and s, respectively, and two updates for the Lagrangian variables \u03b7, \u03b7.",{"@attributes":{"id":"p-0064","num":"0063"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"f","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"mrow":[{"munder":{"mrow":{"mi":["arg","min"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mi":"f"},"mo":"\u2062","mrow":{"mi":"\u03a6","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}}},{"mfrac":{"msub":{"mi":"\u03bc","mn":"1"},"mn":"2"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msup":{"mi":["u","n"]},"mo":["-","-"],"mrow":{"mi":["R","f"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"msubsup":{"mi":["\u03b7","u","n"]}}},"mi":"\u03c1","mn":"2"}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"17"}}]}}}}},{"@attributes":{"id":"p-0065","num":"0064"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"u","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"mrow":[{"munder":{"mrow":{"mi":["arg","min"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mi":"u"},"mo":["\u2062","\u2062"],"mfrac":{"msub":{"mi":"\u03bc","mn":"1"},"mn":"2"},"msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"u","mo":["-","-"],"msup":{"mi":"Rf","mrow":{"mi":"n","mo":"+","mn":"1"}},"msubsup":{"mi":["\u03b7","u","n"]}}},"mi":"\u03c1","mn":"2"}},{"mfrac":{"msub":{"mi":"\u03bc","mn":"2"},"mn":"2"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msup":{"mi":["s","n"]},"mo":["-","-"],"mi":"u","msubsup":{"mi":["\u03b7","s","n"]}}},"mn":["2","2"]}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"18"}}]}}}}},{"@attributes":{"id":"p-0066","num":"0065"},"maths":{"@attributes":{"id":"MATH-US-00015","num":"00015"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"s","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"mrow":[{"munder":{"mrow":{"mi":["arg","min"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mi":"s"},"mo":"\u2062","mrow":{"mi":"h","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["s","g"],"mo":","}}}},{"mfrac":{"msub":{"mi":"\u03bc","mn":"2"},"mn":"2"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"s","mo":["-","-"],"msup":{"mi":"u","mrow":{"mi":"n","mo":"+","mn":"1"}},"msubsup":{"mi":["\u03b7","s","n"]}}},"mn":["2","2"]}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"19"}}]}}}},"br":[{},{},{}],"in-line-formulae":[{},{},{},{}],"sub":["u","u","s","s","2","1","2"],"sup":["n+1","n","n+1","n+1","n+1","n","n+1","n+1"],"i":["u","\u2212Rf","s","\u2212u"]},"A similar additional variable split z=f, using an axillary image variable z that is distinct from f, can be performed in the image domain to detangle the application of the R operator and the regularization calculations \u03a6, resulting in",{"@attributes":{"id":"p-0068","num":"0067"},"maths":{"@attributes":{"id":"MATH-US-00016","num":"00016"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["L","\u03c1"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","u","z"],"mo":[",",",",",",","],"msub":[{"mi":["\u03b7","u"]},{"mi":["\u03b7","z"]}]}}},{"mrow":[{"mi":"h","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["u","g"],"mo":","}}},{"mi":"\u03a6","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"z"}},{"mfrac":{"msub":{"mi":"\u03bc","mn":"1"},"mn":"2"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":["u","Rf"],"mo":["-","-"],"msub":{"mi":["\u03b7","u"]}}},"mi":"\u03c1","mn":"2"}},{"mfrac":{"msub":{"mi":"\u03bc","mn":"2"},"mn":"2"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":["z","f"],"mo":["-","-"],"msub":{"mi":["\u03b7","z"]}}},"mn":["2","2"]}}],"mo":["+","+","+"]}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"22"}}]}}}}},"Applying the ALM algorithm to the \u03c1-weighted augmented Lagrangian in (22) yields simpler updates than the original algorithm.",{"@attributes":{"id":"p-0070","num":"0069"},"maths":{"@attributes":{"id":"MATH-US-00017","num":"00017"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"f","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"mrow":[{"munder":{"mrow":{"mi":["arg","min"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mi":"f"},"mo":["\u2062","\u2062"],"mfrac":{"msub":{"mi":"\u03bc","mn":"1"},"mn":"2"},"msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msup":{"mi":["u","n"]},"mo":["-","-"],"mi":"Rf","msubsup":{"mi":["\u03b7","u","n"]}}},"mi":"\u03c1","mn":"2"}},{"mfrac":{"msub":{"mi":"\u03bc","mn":"2"},"mn":"2"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msup":{"mi":["z","n"]},"mo":["-","-"],"mi":"f","msubsup":{"mi":["\u03b7","z","n"]}}},"mn":["2","2"]}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"23"}}]}}}}},{"@attributes":{"id":"p-0071","num":"0070"},"maths":{"@attributes":{"id":"MATH-US-00018","num":"00018"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"u","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"mrow":[{"munder":{"mrow":{"mi":["arg","min"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mi":"u"},"mo":"\u2062","mrow":{"mi":"h","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["u","g"],"mo":","}}}},{"mfrac":{"msub":{"mi":"\u03bc","mn":"1"},"mn":"2"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"u","mo":["-","-"],"msup":{"mi":"Rf","mrow":{"mi":"n","mo":"+","mn":"1"}},"msubsup":{"mi":["\u03b7","u","n"]}}},"mi":"\u03c1","mn":"2"}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"24"}}]}}}}},{"@attributes":{"id":"p-0072","num":"0071"},"maths":{"@attributes":{"id":"MATH-US-00019","num":"00019"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"z","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"mrow":[{"munder":{"mrow":{"mi":["arg","min"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mi":"z"},"mo":"\u2062","mrow":{"mi":"\u03a6","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"z"}}},{"mfrac":{"msub":{"mi":"\u03bc","mn":"2"},"mn":"2"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"z","mo":["-","-"],"msup":{"mi":"f","mrow":{"mi":"n","mo":"+","mn":"1"}},"msubsup":{"mi":["\u03b7","z","n"]}}},"mn":["2","2"]}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"25"}}]}}}},"br":[{},{},{}],"in-line-formulae":[{},{},{},{}],"sub":["u","u","z","z"],"sup":["n+1","n","n+1","n+1","n+1","n","n+1","n+1"],"i":["u","\u2212Rf","z","\u2212f"]},"The stopping criteria for the iterative method can be modified to use the new variables. For example, the sinogram variable u can be compared to the actual measurements g, or to calculated measurements Rf. Or in the case of using the additional auxiliary sinogram variable s as in (16), it can be compared to g, or u can be compared to Rf. The comparison between variables might involve using a desired function of the variables, such as the function h, or a norm or weighted norm. Additionally, the amount of change in any of these variables in a given iteration can be calculated and compared to a threshold. For example, the values of the sinogram discrepancy term and the image discrepancy term may be compared, or one or both terms may be monitored for sufficiently small change from one iteration to another.","The ADMM algorithm is not the only method by which the AL can be minimized. One such alternative is the alternating minimization algorithm (AMA). AMA minimizes the AL through a sequence of simpler update steps. AMA applied to the \u03c1-weighted AL in (8) yields the AMA-\u03c1 procedure",{"@attributes":{"id":"p-0075","num":"0074"},"maths":{"@attributes":{"id":"MATH-US-00020","num":"00020"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"f","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"mrow":[{"mrow":[{"munder":{"mrow":{"mi":["arg","min"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mi":"f"},"mo":"\u2062","mrow":{"mi":"\u03a6","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}}},{"mi":"\u03bc","mo":"\u2062","msub":{"mrow":{"mo":["\u2329","\u232a"],"mrow":{"msup":{"mi":["\u03b7","n"]},"mo":",","mi":"Rf"}},"mi":"\u03c1"}}],"mo":"+"},{"mrow":[{"munder":{"mrow":{"mi":["arg","min"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mi":"f"},"mo":"\u2062","mrow":{"mi":"\u03a6","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}}},{"mi":"\u03bc","mo":"\u2062","msub":{"mrow":{"mo":["\u2329","\u232a"],"mrow":{"msup":[{"mi":["R","T"]},{"mi":["\u03b7","n"]}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mi":["\u03c1","f"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},"mn":"2"}}],"mo":"+"}],"mo":"="}}},{"mrow":{"mo":["(",")"],"mn":"28"}}]}}}}},{"@attributes":{"id":"p-0076","num":"0075"},"maths":{"@attributes":{"id":"MATH-US-00021","num":"00021"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"u","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"mrow":[{"munder":{"mrow":{"mi":["arg","min"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mi":"u"},"mo":["\u2062","\u2062"],"mfrac":{"mn":["1","2"]},"msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":["g","u"],"mo":"-"}},"mi":"W","mn":"2"}},{"mi":"\u03bc","mo":"\u2062","msub":{"mrow":{"mo":["\u2329","\u232a"],"mrow":{"msup":{"mi":["\u03b7","n"]},"mo":",","mi":"u"}},"mi":"\u03c1"}},{"mfrac":{"mi":"\u03bc","mn":"2"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"u","mo":"-","msup":{"mi":"Rf","mrow":{"mi":"n","mo":"+","mn":"1"}}}},"mi":"\u03c1","mn":"2"}}],"mo":["-","+"]}}},{"mrow":{"mo":["(",")"],"mn":"29"}}]}}}},"br":{},"in-line-formulae":[{},{}],"sup":["n+1","n","n+1","n+1"],"i":["u","\u2212Rf"]},"Here the notation (a, b)=a\u03c1b denotes the \u03c1-weighted inner product, whereas (a, b)=ab denotes the regular (Euclidean) inner product. In (28), we refer to the inner product term as an image discrepancy term, whereas in (29) the image discrepancy term is the third term. This sequence of steps has a simpler form in (28) for the update in the image variable f than in (9): due to the non-quadratic regularizer \u03a6, the respective updates will require an iterative strategy, but here the backprojector Rneed only be applied once during the minimization with respect to f (because \u03b7remains fixed) whereas in (9) both R and Rare applied in every iteration in minimizing f. Updates to u and \u03b7 remain comparable to the ADMM algorithm.","Preferred Variations with the Auxiliary Variable Iteration Methods and the \u03c1-weighted AL","Linearized ADMM-\u03c1 Embodiment","The so-called Linearized ADMM framework (also known as the split inexact Uzawa method) can also benefit from the weighting with \u03c1. A preferred linearized ADMM (L-ADMM) approach modifies the image update step in (9) by adding an additional penalty (or so-called inertia) term. In the typical formulation of L-ADMM (without the \u03c1 weighting), the image update is",{"@attributes":{"id":"p-0081","num":"0080"},"maths":{"@attributes":{"id":"MATH-US-00022","num":"00022"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"f","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"mrow":[{"munder":{"mrow":{"mi":["arg","min"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mi":"f"},"mo":"\u2062","mrow":{"mi":"\u03a6","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}}},{"mfrac":{"mi":"\u03bc","mn":"2"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msup":[{"mi":["u","n"]},{"mi":["\u03b7","n"]}],"mo":["-","-"],"mi":"Rf"}},"mn":["2","2"]}},{"mfrac":{"mn":["1","2"]},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"f","mo":"-","msup":{"mi":["f","n"]}}},"mi":"Q","mn":"2"}}],"mo":["+","+"]}}},{"mrow":{"mo":["(",")"],"mn":"31"}}]}}}}},"where must be positive semi-definite, or positive definite. The choice of Q can make finding the solution to (31) much easier. In particular, the main computational burden in minimizing in (31) is the application of the operator R and its adjoint R. The operator Q is chosen as",{"@attributes":{"id":"p-0083","num":"0082"},"maths":{"@attributes":{"id":"MATH-US-00023","num":"00023"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"Q","mo":"=","mrow":{"mrow":[{"mfrac":{"mn":"1","mi":"\u03b4"},"mo":"\u2062","mi":"I"},{"mi":["\u03bc","R"],"mo":["\u2062","\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mi":["R","T"]}}],"mo":"-"}}},{"mrow":{"mo":["(",")"],"mn":"32"}}]}}}}},"The coefficient \u03b4 is must satisfy \u03b4\u22661\/\u03bc\u2225RR\u2225 so that Q will be at least positive semi-definite. Here the norm applied to the operator is the induced operator norm equal to the largest singular value in the case of a matrix. The minimization problem of (31) then becomes solving",{"@attributes":{"id":"p-0085","num":"0084"},"maths":{"@attributes":{"id":"MATH-US-00024","num":"00024"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"f","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"mrow":[{"munder":{"mrow":{"mi":["arg","min"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mi":"f"},"mo":"\u2062","mrow":{"mi":"\u03a6","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}}},{"mfrac":{"mn":"1","mrow":{"mn":"2","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"\u03b4"}},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"f","mo":["-","-"],"msup":{"mi":["f","n"]},"mrow":{"mi":["\u03bc","\u03b4"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mrow":{"msup":{"mi":["R","T"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msup":[{"mi":["u","n"]},{"mi":["Rf","n"]},{"mi":["\u03b7","n"]}],"mo":["-","-"]}}}}}},"mn":["2","2"]}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"33"}}]}}}}},"A significant improvement is that the operators R and Ract only on the previous values of each variable and thus only need to be applied once each during the entire minimization in (33) to determine f. In contrast, solving (9) via gradient methods requires application of each operator during each inner iteration of the minimization algorithm for solving (9). A potential limitation of L-ADMM is that typically the entire algorithm requires many more outer iterations for convergence. The objective when using L-ADMM is that the increased number of outer ADMM iterations is offset by the improved speed of each outer iteration, in particular solving the image minimization via (33). The convergence of L-ADMM is improved by picking \u03b4 as large as possible, subject to the constraint with respect to the operator R mentioned previously.","Combining L-ADMM with the \u03c1 weighting yields an algorithm with similar structure but significantly improved performance. The choice of operator Q becomes",{"@attributes":{"id":"p-0088","num":"0087"},"maths":{"@attributes":{"id":"MATH-US-00025","num":"00025"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"Q","mo":"=","mrow":{"mrow":[{"mfrac":{"mn":"1","mi":"\u03b4"},"mo":"\u2062","mi":"I"},{"mi":["\u03bc","\u03c1","R"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msup":{"mi":["R","T"]}}],"mo":"-"}}},{"mrow":{"mo":["(",")"],"mn":"34"}}]}}}}},"The minimization problem of (31) is similarly modified by the inclusion of the \u03c1 weighting in both the second term and in Q, yielding the image update equation as a solution to the minimization problem",{"@attributes":{"id":"p-0090","num":"0089"},"maths":{"@attributes":{"id":"MATH-US-00026","num":"00026"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"f","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"mrow":[{"munder":{"mrow":{"mi":["arg","min"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mi":"f"},"mo":"\u2062","mrow":{"mi":"\u03a6","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}}},{"mfrac":{"mn":"1","mrow":{"mn":"2","mo":"\u2062","mi":"\u03b4"}},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"f","mo":["-","-"],"msup":{"mi":["f","n"]},"mrow":{"msup":{"mi":["\u03bc\u03b4R","T"]},"mo":"\u2062","mrow":{"mi":"\u03c1","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msup":[{"mi":["u","n"]},{"mi":["Rf","n"]},{"mi":["\u03b7","n"]}],"mo":["-","-"]}}}}}},"mn":["2","2"]}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"35"}}]}}}}},"This improvement concerns the choice of the parameter \u03b4, which now must satisfy \u03b4\u22661\/(\u03bc\u2225R\u03c1R\u2225). The inclusion of the \u03c1 weighting allows for a larger \u03b4, because \u2225R\u03c1R\u2225<<\u2225RR\u2225. In this variation the image f is updated (step  in ) by solving Equation (35) to a desired degree of accuracy, e.g, using a nonlinear iterative conjugate gradient algorithm. The update steps for u and \u03b7 are the same as in Equations (10) and (11), respectively. This modified L-ADMM algorithm has the benefits of reducing the computational effort of the image update step in (9) by replacing it with the update (35) while retaining convergence rates in terms of number of outer iterations comparable to the original ADMM formulation.","Extended Weighting Embodiment","In this variation, the \u03c1 weighting operator is augmented with operator F to form aggregate weighting operator \u0393=E\u03c1E. The operator E is applied on both sides of \u03c1 to maintain symmetry of the operator. The extended weighting operator E can be constructed to make the composite operator RE\u03c1ER become closer to an identity operation, i.e. improve the implicit preconditioning nature of the weighting.","For example, E can be a rebinning operation, converting from fan-beam to parallel-beam data. The use of the \u03c1 operator for parallel beam geometries makes R\u03c1 a more exact analytical inverse of R than when applied to fan-beam geometries. Similarly in 3D, the E operator can rehin from true cone-beam data to cone-parallel data (parallel in-plane, but divergent beam along the axial direction). Applying the \u03c1 operator on this geometry is equivalent to filtering along slanted lines in the original cone-beam domain, which has been shown to be advantageous in FBP reconstructions. Alternatively, E could simply rotate the projection data on the detector panel itself to implement filtering along slanted lines. The adjoint operation Erotates the projection data back to its original orientation. The operator E can also be adopted when using the square root of the ramp filter",{"@attributes":{"id":"p-0095","num":"0094"},"maths":{"@attributes":{"id":"MATH-US-00027","num":"00027"},"math":{"@attributes":{"overflow":"scroll"},"msup":{"mi":"\u03c1","mfrac":{"mn":["1","2"]}}}},"br":{}},{"@attributes":{"id":"p-0096","num":"0095"},"maths":{"@attributes":{"id":"MATH-US-00028","num":"00028"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msup":{"mi":"\u03c1","mfrac":{"mn":["1","2"]}},"mo":"\u2062","mrow":{"mi":"E","mo":"."}}}}},"In general, the most effective choice of E, in terms of the approximation of the identity by the composite operator RE\u03c1ER, would be to make the composite operator RE\u03c1E approximate the inverse of R. To the extent that an FBP reconstruction algorithm provides a good approximation to the inverse of R, a good choice for E would result in RE\u03c1E behaving like the FBP reconstruction operator.","Alternatively, the weighting operator \u0393 can be chosen to correspond to filtering along slanted lines or curves in the original cone-beam projection domain (that is, jointly in the variables t, \u03b4, r or a subset thereof, similar to the filtering used in reconstruction procedures for divergent beam CT geometries (e.g. helical coneheam).","The update equations analogous to (9) and (10) replace the \u03c1 weighting by the extended weighting operator \u0393=E\u03c1E. A similar change applies to Equations (12)-(14). As before, the corresponding operation (W+\u03bc\u0393) is hard to invert directly, and similar to the case of \u03c1 weighting, the minimization with respect to u of the cost function corresponding to the one in (10) is performed by an iterative process with a few \u201cinner iterations\u201d using the gradient with respect to u, which takes the form in (14), with \u03c1 replaced by \u0393=E\u03c1E. The same comments about applying W and \u0393 at low computational cost apply, except that the application of \u0393=E\u03c1E will now involve rebirming as well.","As before, the need for an iterative algorithm to compute the update of u can be overcome by introducing another variable split. While the split u=s used before can work in this situation as well, the resulting update for u involves inverting (\u03bcI+\u03bc\u0393)=\u03bcI+\u03bcE\u03c1E, which is no longer a simple shift invariant filter. Instead, using the split u=Es retains the linear shift invariant filtering form for the u update, and the update for s involves instead inversion of W+\u03bcEE. For most rehinning operators E, the operator EE will be very well conditioned, and often structured, facilitating inversion or iterative solution.",{"@attributes":{"id":"p-0101","num":"0100"},"maths":{"@attributes":{"id":"MATH-US-00029","num":"00029"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"f","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"mrow":[{"munder":{"mrow":{"mi":["arg","min"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mi":"f"},"mo":"\u2062","mrow":{"mi":"\u03a6","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}}},{"mfrac":{"msub":{"mi":"\u03bc","mn":"1"},"mn":"2"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msup":{"mi":["u","n"]},"mo":"-","mrow":{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"Rf","mo":"+","msubsup":{"mi":["\u03b7","s","n"]}}}}}},"mi":"\u03c1","mn":"2"}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"36"}}]}}}}},{"@attributes":{"id":"p-0102","num":"0101"},"maths":{"@attributes":{"id":"MATH-US-00030","num":"00030"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"s","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"mrow":[{"munder":{"mrow":{"mi":["arg","min"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mi":"s"},"mo":"\u2062","mrow":{"mi":"h","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["s","g"],"mo":","}}}},{"mfrac":{"msub":{"mi":"\u03bc","mn":"2"},"mn":"2"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msup":{"mi":["u","n"]},"mo":["-","-"],"mi":"Es","msubsup":{"mi":["\u03b7","u","n"]}}},"mn":["2","2"]}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"37"}}]}}}}},{"@attributes":{"id":"p-0103","num":"0102"},"maths":{"@attributes":{"id":"MATH-US-00031","num":"00031"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"u","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"mrow":[{"munder":{"mrow":{"mi":["arg","min"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mi":"u"},"mo":["\u2062","\u2062"],"mfrac":{"msub":{"mi":"\u03bc","mn":"1"},"mn":"2"},"msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"u","mo":"-","mrow":{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msup":{"mi":"Rf","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"+","msubsup":{"mi":["\u03b7","s","n"]}}}}}},"mi":"\u03c1","mn":"2"}},{"mfrac":{"msub":{"mi":"\u03bc","mn":"2"},"mn":"2"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"u","mo":["-","-"],"msup":{"mi":"Es","mrow":{"mi":"n","mo":"+","mn":"1"}},"msubsup":{"mi":["\u03b7","u","n"]}}},"mn":["2","2"]}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"38"}}]}}}},"br":[{},{}],"in-line-formulae":[{},{},{},{}],"sub":["u","u","s","s"],"sup":["n+1","n","n+1","n+1","n+1","n","n+1","n+1"],"i":["u","\u2212Es","s","\u2212Rf"]},"The extended weighting can be combined with the L-ADMM similarly to the way the \u03c1 weighting was combined with the L-ADMM, obtaining the benefit of both features.","Regularizer-Aware Weighting Embodiment","Another variation on the previous choices of weighting operator \u0393, is directed explicitly to improve the conditioning of the Hessian with respect to f \u2207L(u, f, \u03b7)=R\u0393R+\u2207\u03a6(f) of the \u0393-weighted AL. Typically, with quadratic regularization, \u03a6(f)=\u2225f\u2225, where G is a weighting operator with a high-pass filter characteristics. This is typically true to a good approximation, even with edge-preserving, non-quadratic regularization, for small values of the image gradient. It follows, that to make the Hessian \u2207L(u, f, \u03b7) well-conditioned, or approximate the identity, the weighting \u0393 can be chosen so that R\u0393R has frequency response complementary to that of G. This can be achieved by replacing the RamLak ramp filter \u03c1 in E\u03c1E by an appropriately apodized (frequency-weighted) ramp filter, with similar modifications when using the square root filter.","Filtering in the Image Domain Embodiment","Artisans will appreciate that the composite operator R\u0393R is an operation that produces an output image from an input image. Hence, this operation can be performed entirely in the image domain. This can lead to savings, if the operation can be performed at lower cost than a projection R or backprojection R. Now, with the weighting \u0393 chosen to one of the forms described in this invention, so that R\u0393R is well-conditioned or an approximate identity, R\u0393R can be approximated by a low-order and inexpensive digital filtering operation, in the image domain. This will further reduce the cost of the iterative algorithm.","Introduction of \u03c1 through Image Domain Split (left-right) Preconditioner Embodiment","In another preferred variation, instead of incorporating the \u03c1 operator in the sinogram domain, it can instead be introduced into the formulation using an image domain representation. Consider the minimization with respect to f of the augmented Lagrangian without the \u03c1 weighting in (5). It involves the solution of the zero gradient condition expressed by the following equation\n\n\u2207\u03a6()+\u03bc(n\u2212)=0\u2003\u2003(41)\n","This equation for f is typically ill-conditioned because of the ill-conditioning of the Hessian RR of the image discrepancy term with respect to f. Denote the image domain representation or approximate representation of the \u03c1 operator as {circumflex over (\u03c1)}. For example, when \u03c1 corresponds to the ramp filter with 1-D frequency response |\u03c9| in the projection domain, the operator {circumflex over (\u03c1)} may be chosen as an image domain filter with frequency response \u221a{square root over (\u03c9+\u03c9)}, where \u03c9and \u03c9denote the frequency variables in the transverse plane. More generally, {circumflex over (\u03c1)} may be chosen as an image domain filter with a high-pass response in the transverse plane. Denote further a square root of tins operator as",{"@attributes":{"id":"p-0112","num":"0111"},"maths":{"@attributes":{"id":"MATH-US-00032","num":"00032"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mover":{"mi":"\u03b6","mo":"^"},"mo":"=","mrow":{"msup":{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}},"mo":"."}}}},"br":[{},{}],"in-line-formulae":[{},{}],"sup":["T","T","T","T","T"],"sub":"f","i":["f","R","R{circumflex over (\u03b6)}x+\u03bc{circumflex over (\u03b6)}","R","u","f={circumflex over (\u03b6)}x"]},"where x is an image-domain auxiliary variable that is related to the image f by the relation",{"@attributes":{"id":"p-0114","num":"0113"},"maths":{"@attributes":{"id":"MATH-US-00033","num":"00033"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"f","mo":"=","mrow":{"mrow":[{"mover":{"mi":"\u03b6","mo":"^"},"mo":"\u2062","mi":"x"},{"msup":{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}},"mo":"\u2062","mrow":{"mi":"x","mo":"."}}],"mo":"="}}}}},"That is, appocanon of the image-domain filer {circumflex over (\u03b6)} to x produces f. We refer to x as a pre-image. Next, using the identity that holds for a linear operator A: \u2207\u03a6(Ax)=A\u2207\u03a6(f) whenever f=Ax, Eq. (42) reduces to\n\n\u2207\u03a6({circumflex over (\u03b6)})+\u03bc{circumflex over (\u03b6)}(\u03b7\u2212)=0 where \u2003\u2003(43)\n","To obtain this preconditioned form of the gradient with respect to x, the AL in Eq. (5) is modified to the following form",{"@attributes":{"id":"p-0117","num":"0116"},"maths":{"@attributes":{"id":"MATH-US-00034","num":"00034"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["L","\u03c1"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","u","\u03b7"],"mo":[",",","]}}},{"mrow":[{"mrow":[{"mi":"h","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["u","g"],"mo":","}}},{"mi":"\u03a6","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mover":{"mi":"\u03b6","mo":"^"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"x"}}},{"mfrac":{"mi":"\u03bc","mn":"2"},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":["u","\u03b7"],"mo":["-","-"],"mrow":{"mi":["R","x"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mover":{"mi":"\u03b6","mo":"^"}}}},"mn":["2","2"]},"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":["where","f"]}],"mo":["+","+"]},{"mover":{"mi":"\u03b6","mo":"^"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"x"}],"mo":"="}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"44"}}]}}}}},"Eq. (43) is made to be well-conditioned for solving for the variable x by choosing the image domain filter {circumflex over (\u03b6)} so that {circumflex over (\u03b6)}RR{circumflex over (\u03b6)} is well-conditioned (e.g, approximates or is proportional to the identity operator), for example by using the square root of the image domain ramp filter as described above, or its approximation using an FIR or an IIR image domain filter, or using the FFT. In the case of an IIR or FIR filter, it may be advantageous to use in the definition",{"@attributes":{"id":"p-0119","num":"0118"},"maths":{"@attributes":{"id":"MATH-US-00035","num":"00035"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mover":{"mi":"\u03b6","mo":"^"},"mo":"=","msup":{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}}}}},"br":{}},{"@attributes":{"id":"p-0120","num":"0119"},"maths":{"@attributes":{"id":"MATH-US-00036","num":"00036"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msup":{"mrow":{"mo":["(",")"],"msup":{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}}},"mi":"T"},"mo":"\u2062","mrow":{"mo":["(",")"],"msup":{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}}}},{"mover":{"mi":"\u03c1","mo":"^"},"mo":"."}],"mo":"="}}},"br":{}},{"@attributes":{"id":"p-0121","num":"0120"},"maths":{"@attributes":{"id":"MATH-US-00037","num":"00037"},"math":{"@attributes":{"overflow":"scroll"},"msup":{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}}}},"br":{}},"An alternative favorable choice for {circumflex over (\u03b6)} is one that makes the Hessian with respect to x of the AL L(x, u, \u03b7), which is given by\n\n\u2207\u03a6({circumflex over (\u03b6)}x)+\u03bc{circumflex over (\u03b6)}RR{circumflex over (\u03b6)}\u2003\u2003(45)\n","well-conditioned (e.g., approximately equal or proportional to the identity operator).","Because the relation in Eq. (44) between the variables f and x does not appear in the expression for L(x, u, \u03b7), the block-coordinate optimization of L(x, u, \u03b7) with respect to the variables x, u and \u03b7 can proceed without reference to the image variable f. The image f can be computed only at the end of the iterations, when the completion criterion has been satisfied, or more frequently if desired, e.g., for monitoring the iterations.","The steps corresponding to the ADMM algorithm in this case, i.e. the block-coordinate optimization of L(x, u, \u03b7), are analogous to those in Eq. (9)-(11), and in the PWLS case that the sinogram discrepancy term has the form of a weighted two-norm are given by",{"@attributes":{"id":"p-0126","num":"0125"},"maths":{"@attributes":{"id":"MATH-US-00038","num":"00038"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"x","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"mrow":[{"munder":{"mrow":{"mi":["arg","min"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mi":"x"},"mo":"\u2062","mrow":{"mi":"\u03a6","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msup":{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}},"mo":"\u2062","mi":"x"}}}},{"mfrac":{"mi":"\u03bc","mn":"2"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msup":[{"mi":["u","n"]},{"mi":["\u03b7","n"]}],"mo":["-","-"],"mrow":{"mi":["R","x"],"mo":["\u2062","\u2062"],"msup":{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}}}}},"mn":["2","2"]}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"46"}}]}}}}},{"@attributes":{"id":"p-0127","num":"0126"},"maths":{"@attributes":{"id":"MATH-US-00039","num":"00039"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"u","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"mrow":[{"munder":{"mrow":{"mi":["arg","min"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mi":"u"},"mo":["\u2062","\u2062"],"mfrac":{"mn":["1","2"]},"msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":["g","u"],"mo":"-"}},"mi":"W","mn":"2"}},{"mfrac":{"mi":"\u03bc","mn":"2"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"u","mo":["-","-"],"mrow":{"mi":"R","mo":["\u2062","\u2062"],"msup":[{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}},{"mi":"x","mrow":{"mi":"n","mo":"+","mn":"1"}}]},"msup":{"mi":["\u03b7","n"]}}},"mi":"\u03c1","mn":"2"}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"47"}}]}}}}},{"@attributes":{"id":"p-0128","num":"0127"},"maths":{"@attributes":{"id":"MATH-US-00040","num":"00040"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"\u03b7","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"msup":{"mi":["\u03b7","n"]},"mo":"-","mrow":{"mo":["(",")"],"mrow":{"msup":{"mi":"u","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"-","mrow":{"mi":"R","mo":["\u2062","\u2062"],"msup":[{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}},{"mi":"x","mrow":{"mi":"n","mo":"+","mn":"1"}}]}}}}}},{"mrow":{"mo":["(",")"],"mn":"48"}}]}}}}},"In this embodiment the preconditioning operator",{"@attributes":{"id":"p-0130","num":"0129"},"maths":{"@attributes":{"id":"MATH-US-00041","num":"00041"},"math":{"@attributes":{"overflow":"scroll"},"msup":{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}}}},"br":{}},{"@attributes":{"id":"p-0131","num":"0130"},"maths":{"@attributes":{"id":"MATH-US-00042","num":"00042"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"u","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"msup":{"mrow":[{"mo":["(",")"],"mrow":{"mi":"W","mo":"+","mrow":{"mi":["\u03bc","I"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}},{"mo":"-","mn":"1"}]},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"\u03bc","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"R","mo":["\u2062","\u2062"],"msup":[{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}},{"mi":"x","mrow":{"mi":"n","mo":"+","mn":"1"}}]},"mo":"+","msup":{"mi":["\u03b7","n"]}}}},"mo":"-","mi":"Wg"}}}}},{"mrow":{"mo":["(",")"],"mn":"49"}}]}}}}},"which only requires multiplication of previously calculated quantities by a diagonal matrix.","The only numerically difficult step is the x image variable update, which will have a similar level of complexity to the f updates of the sinogram-domain \u03c1 operator, e.g. Eq. (9), and can be similarly performed to desired accuracy using an iterative minimization algorithm such as nonlinear conjugate gradients.","Introduction of \u03c1 through image Domain Variable Splitting Embodiment","This is another embodiment with \u03c1 applied in the image domain. The same notation and meaning for {circumflex over (\u03c1)} and",{"@attributes":{"id":"p-0136","num":"0135"},"maths":{"@attributes":{"id":"MATH-US-00043","num":"00043"},"math":{"@attributes":{"overflow":"scroll"},"msup":{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}}}},"br":{}},{"@attributes":{"id":"p-0137","num":"0136"},"maths":{"@attributes":{"id":"MATH-US-00044","num":"00044"},"math":{"@attributes":{"overflow":"scroll"},"msup":{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}}}},"br":{}},{"@attributes":{"id":"p-0138","num":"0137"},"maths":{"@attributes":{"id":"MATH-US-00045","num":"00045"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["f","opt"]},"mo":"=","mrow":{"mrow":[{"munder":{"mrow":{"mrow":{"mi":["arg","min"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mo":"\u2062","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mi":"f"},"mo":"\u2062","mrow":{"mi":"h","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["u","g"],"mo":","}}}},{"mi":"\u03a6","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msup":{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}},"mo":"\u2062","mi":"x"}}}],"mo":"+"}},{"mrow":[{"mrow":{"mi":["such","that","u"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]},"mo":"=","mi":"Rf"},{"mi":"f","mo":"=","mrow":{"msup":{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}},"mo":"\u2062","mi":"x"}}],"mo":","}],"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}}}},{"mrow":{"mo":["(",")"],"mn":"50"}}]}}}}},"Following from the non-\u03c1 weighted case in (5), the extended \u03c1-weighted Augmented Lagrangian for this is written",{"@attributes":{"id":"p-0140","num":"0139"},"maths":{"@attributes":{"id":"MATH-US-00046","num":"00046"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["L","\u03c1"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","x","u"],"mo":[",",",",",",","],"msub":[{"mi":"\u03b7","mn":"1"},{"mi":"\u03b7","mn":"2"}]}}},{"mrow":[{"mi":"h","mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mi":["u","g"],"mo":","}}},{"mi":"\u03a6","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msup":{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}},"mo":"\u2062","mi":"x"}}},{"mfrac":{"msub":{"mi":"\u03bc","mn":"1"},"mn":"2"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"u","mo":["-","-"],"mrow":{"mi":["R","x"],"mo":["\u2062","\u2062"],"msup":{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}}},"msub":{"mi":"\u03b7","mn":"1"}}},"mn":["2","2"]}},{"mfrac":{"msub":{"mi":"\u03bc","mn":"2"},"mn":"2"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"f","mo":["-","-"],"mrow":{"msup":{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}},"mo":"\u2062","mi":"x"},"msub":{"mi":"\u03b7","mn":"2"}}},"mn":["2","2"]}}],"mo":["+","+","+"]}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"51"}}]}}}}},"Focusing specifically on the corresponding ADMM update equations for f and \u03b7for this Augmented Lagrangian, we have",{"@attributes":{"id":"p-0142","num":"0141"},"maths":{"@attributes":{"id":"MATH-US-00047","num":"00047"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"f","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"mrow":[{"munder":{"mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":["m","in"],"mo":"\u2062"}},"mi":"f"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"f","mo":["-","-"],"mrow":{"msup":[{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}},{"mi":"x","mrow":{"mi":"n","mo":"+","mn":"1"}}],"mo":"\u2062"},"msubsup":{"mi":["\u03b7","n"],"mn":"2"}}},"mn":["2","2"]}},{"mrow":{"msup":[{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}},{"mi":"x","mrow":{"mi":"n","mo":"+","mn":"1"}}],"mo":"\u2062"},"mo":"+","msubsup":{"mi":["\u03b7","n"],"mn":"2"}}],"mo":"="}}},{"mrow":{"mo":["(",")"],"mn":"52"}}]}}}}},{"@attributes":{"id":"p-0143","num":"0142"},"maths":{"@attributes":{"id":"MATH-US-00048","num":"00048"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msubsup":{"mi":"\u03b7","mn":"2","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"msubsup":{"mi":["\u03b7","n"],"mn":"2"},"mo":"-","mrow":{"mo":["(",")"],"mrow":{"msup":{"mi":"f","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"-","mrow":{"msup":[{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}},{"mi":"x","mrow":{"mi":"n","mo":"+","mn":"1"}}],"mo":"\u2062"}}}}}},{"mrow":{"mo":["(",")"],"mn":"53"}}]}}}}},"With the initialization \u03b7=0, these recurrence relations resolve to",{"@attributes":{"id":"p-0145","num":"0144"},"maths":{"@attributes":{"id":"MATH-US-00049","num":"00049"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msup":{"mi":"f","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"msup":[{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}},{"mi":"x","mrow":{"mi":"n","mo":"+","mn":"1"}}],"mo":"\u2062"}},{"msubsup":{"mi":["\u03b7","n"],"mn":"2"},"mo":"=","mn":"0"}],"mo":[",",","]}}},"br":{},"sub":"2 "},"The modified set of ADMM update equations for all variables are then",{"@attributes":{"id":"p-0147","num":"0146"},"maths":{"@attributes":{"id":"MATH-US-00050","num":"00050"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"x","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"mrow":[{"munder":{"mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"munder":{"mrow":{"mi":["m","in"],"mo":"\u2062"},"mi":"x"}},"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}},"mo":"\u2062","mrow":{"mi":"\u03a6","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msup":{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}},"mo":"\u2062","mi":"x"}}}},{"mfrac":{"msub":{"mi":"\u03bc","mn":"1"},"mn":"2"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msup":{"mi":["u","n"]},"mo":["-","-"],"mrow":{"mi":["R","x"],"mo":["\u2062","\u2062"],"msup":{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}}},"msubsup":{"mi":["\u03b7","n"],"mn":"1"}}},"mn":["2","2"]}},{"mfrac":{"msub":{"mi":"\u03bc","mn":"2"},"mn":"2"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msup":{"mi":["f","n"]},"mo":"-","mrow":{"mi":["R","x"],"mo":["\u2062","\u2062"],"msup":{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}}}}},"mn":["2","2"]}}],"mo":["+","+"]}}},{"mrow":{"mo":["(",")"],"mn":"54"}}]}}}}},{"@attributes":{"id":"p-0148","num":"0147"},"maths":{"@attributes":{"id":"MATH-US-00051","num":"00051"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"u","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"mrow":[{"munder":{"mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":["m","in"],"mo":"\u2062"}},"mi":"u"},"mo":["\u2062","\u2062"],"mfrac":{"mn":["1","2"]},"msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":["g","u"],"mo":"-"}},"mi":"W","mn":"2"}},{"mfrac":{"msub":{"mi":"\u03bc","mn":"1"},"mn":"2"},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"u","mo":["-","-"],"mrow":{"mi":"R","mo":["\u2062","\u2062"],"msup":[{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}},{"mi":"x","mrow":{"mi":"n","mo":"+","mn":"1"}}]},"msubsup":{"mi":["\u03b7","n"],"mn":"1"}}},"mn":["2","2"]}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"55"}}]}}}}},{"@attributes":{"id":"p-0149","num":"0148"},"maths":{"@attributes":{"id":"MATH-US-00052","num":"00052"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msubsup":{"mi":"\u03b7","mn":"1","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"msubsup":{"mi":["\u03b7","n"],"mn":"1"},"mo":"-","mrow":{"mo":["(",")"],"mrow":{"msup":{"mi":"u","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"-","mrow":{"mi":"R","mo":["\u2062","\u2062"],"msup":[{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}},{"mi":"x","mrow":{"mi":"n","mo":"+","mn":"1"}}]}}}}}},{"mrow":{"mo":["(",")"],"mn":"56"}}]}}}}},{"@attributes":{"id":"p-0150","num":"0149"},"maths":{"@attributes":{"id":"MATH-US-00053","num":"00053"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"f","mrow":{"mi":"n","mo":"+","mn":"1"}},"mo":"=","mrow":{"msup":[{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}},{"mi":"x","mrow":{"mi":"n","mo":"+","mn":"1"}}],"mo":"\u2062"}}},{"mrow":{"mo":["(",")"],"mn":"57"}}]}}}}},"This embodiment shares the benefits of the above image domain embodiment defined by Equations (44) and (46)-(49), particularly the preconditioning operator is not directly applied to the auxiliary sinogram variable u, which makes the update steps for u simpler and they may have closed-form solutions. The update step for f also has a trivial closed-form in this formulation. The only numerically difficult step is the x image variable update, which will have a similar level of complexity to the f updates of the sinogram-domain \u03c1 operator, e.g. (9), and can be similarly performed to desired accuracy using an iterative minimization algorithm such as nonlinear conjugate gradients. However, in this embodiment and Equations (51)-(57) above the quantities f and",{"@attributes":{"id":"p-0152","num":"0151"},"maths":{"@attributes":{"id":"MATH-US-00054","num":"00054"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msup":{"mover":{"mi":"\u03c1","mo":"^"},"mfrac":{"mn":["1","2"]}},"mo":"\u2062","mi":"x"}}},"br":{}},"Additional Variations","The particular examples of using the different variants of weighting operators \u0393 of this invention with the alternating minimization of the \u03c1-modified AL in the ADMM-\u03c1 and L-ADMM-\u03c1 algorithms are not meant to be exhaustive. The different variants of weightings and iterative schemes can be combined in various ways. The same weighting operators can be used to provide similar advantages in other alternating minimization schemes known in the art, for example versions studied of Gauss-Seidel and of the so-called diagonal-quadratic approximation method [J. Eckstein, \u201cAugmented Lagrangian and Alternating Direction Methods for Convex Optimization: A Tutorial and Some Illustrative Computational Results,\u201d RUTCOR Research Report RRR 32-2012, December 2012], or DQA, or related methods such as Gradient projection for sparse reconstruction (GPSR) [M. T. Figueiredo, R. D. Nowak, and S. J. Wright, Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems, 2007], Proximal Forward-backward splitting methods (PFBS) [P. L. Combettes and V. R. Wajs, Signal recovery by proximal forward-backward splitting, 2005], and Uzawa-type iterations, or as another set of examples, versions with Nesterov-type or Barzilai-Borwein type acceleration of the inner and\/or the outer iterations in versions of the ALM or ADMM or L-ADMM, such as NESTA, FISTA, etc.","Artisans will also appreciate that the description and embodiments of different variations of this invention can be modified and manipulated to other, mathematically equivalent forms, in which not all features previously described would be apparent. For example, auxiliary sinogram variables u and \u03b7 in Eqns. (8)-(12) could be represented together by a single vector, say v, by stacking the vectors u and \u03b7 as \u03bd=[u, \u03b7]. Eqns. (8)-(12) could then be rewritten in terms of the single auxiliary sinogram variable \u03bd, with a corresponding change in any computer code or system executing the instructions to perform the operations indicated by these equations, obscuring the actual presence of more than one auxiliary variable. The distinct update steps for u and \u03b7 in  may then also be replaced by a more elaborate single update of variable \u03bd.","Simulation Result","A brief example demonstrates the efficacy of the present approach for the PWLS formulation of the cost function. The projection data used is simulated from a thorax phantom, scanned in a 2D fan beam geometry with curved detector panel. The projection data has 983 detectors and 1024 views over a full circular rotation. Poisson noise was added to the simulated data, such that the standard deviation of the noise in the FBP reconstruction was around 20 HU. The baseline iterative algorithm solves (2) using the method of conjugate gradients (CG). The majority of the computation will involve the application of R or R, so the computational cost of the algorithm will be measured in terms of the number of such operator applications. For example, in CG, each iteration calculates the gradient of (2), which involves application of both R and R, yielding 2 applications per iteration.","We consider the resulting image after 200 CG iterations to be the converged image. For each approach, we calculate the progress of the algorithm as the distance or root-mean-square error (RMSE) from this converged image.  plots this RMSE error versus computational cost to compare the effective convergence rates. For reference, 200 CG iterations would require 400 operator applications. For each method, we tuned the \u03bc parameter for optimal convergence, and for linearized ADMM algorithms set \u03b4=0.95 (\u03bc\u2225RR\u2225)or \u03b4=0.95 (\u03bc\u2225R\u03c1R\u2225), as appropriate.","As  demonstrates, the use of ADMM by itself does not necessarily confer accelerated convergence. Indeed, the convergence of ADMM is qualitatively similar to the CG algorithm for this case. However, incorporation of the \u03c1 weighting operator (ADMM-\u03c1) provides tremendous improvement: the same reduction in RMSE as achieved by CG in 120 operator applications, is achieved by ADMM-\u03c1 in as few as 45 operator applications, corresponding to an almost 3 fold speedup of ADMM-\u03c1 over CG. Similarly, linearized ADMM makes the image update steps computationally easier at the expense of the convergence rate. The linearized ADMM-\u03c1, on the other hand, takes advantage of better conditioning to further improve the convergence rate: the same reduction in RMSE as achieved by CG in 120 operator applications, is achieved by linearized ADMM-\u03c1 in just 25 operator applications, corresponding to an almost 5 fold speedup of linearized ADMM-\u03c1 over CG.","While specific embodiments of the present invention have been shown and described, it should be understood that other modifications, substitutions and alternatives are apparent to one of ordinary skill in the art. Such modifications, substitutions and alternatives can be made without departing from the spirit and scope of the invention, which should be determined from the appended claims.","Various features of the invention are set forth in the appended claims. In the claims, when it is stated that some quantity q is a function of another quantity x, this functional dependence is not exhaustive, that is, quantity q can be also a function of some additional quantities. Thus we say that h(s, g) is a function of s, a function of g, and function of both s and g."],"GOVINT":[{},{}],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 3"}]},"DETDESC":[{},{}]}
