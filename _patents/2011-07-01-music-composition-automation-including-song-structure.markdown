---
title: Music composition automation including song structure
abstract: A music composition automation system includes logic to assign metadata to an audio recording to divide a melody in the recording into sections, and to identify song form section types for the sections. In addition, logic to associate audio accompaniment with the sections based on the identified section types can be based on a style library, that is arranged to provide data executable by a data processing system to generate musical phrases associated with respective styles. Musical phrases in the style library include metadata specifying characteristics of the phrase according to song form section type.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08710343&OS=08710343&RS=08710343
owner: Ujam Inc.
number: 08710343
owner_city: Dover
owner_country: US
publication_date: 20110701
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"p":["Benefit of U.S. Provisional Application No. 61\/495,330, filed 9 Jun. 2011 is claimed.","1. Field of the Invention","The present invention relates to technology for computer-based, musical composition automation.","2. Description of Related Art","Songs include a melody comprising a succession of notes having a tempo, and accompaniment that can include chords arranged with the notes of the melody. The accompaniment typically is played using instrumental phrases which characterize a chosen style of music. The process of composing of songs can be very complex, given the range of choices presented.","Technology to assist the musical composition has been developed that provides tools for creation and editing of songs. See, U.S. Pat. No. 7,790,974, entitled METADATA-BASED SONG CREATION AND EDITING, by Sherwani et al. However, the variety of musical styles, instruments, phrasings and so on that can be applied to a composition makes the technological problem of providing good sounding accompaniment very difficult.","Typical consumers using these prior art technologies have difficulty creating good sounding music. As a result, products in this field have had only limited success. It is desirable therefore provide solutions to the problem of automatically analyzing an input audio file that includes a melody, and of creating good sounding accompaniment for the melody. It is also desirable to provide solutions to the problem of producing data that can characterize an input audio file in terms of the structure of a melody in the recording, in order to facilitate computer-assisted, music composition automation.","Technologies are described here for automatically characterizing an input audio file for use in music composition automation, and for providing accompaniment for a melody in the input audio file. The technologies include techniques for splitting a melody or audio recording into song form sections, for assigning a type to each section, and for automatically providing musical accompaniment based on the assigned sections and section types. The technology can be applied to produce a composition including a melody with accompaniment that varies in an interesting and relevant way through the course of the song.","Most popular songs consist of sections (verse, chorus, etc.), where the musical accompaniment varies in each type of section. A typical short song might consist of the following song form sections: Intro, Verse 1, Chorus, Verse 2, Chorus, Chorus, Ending. A song section comprises a set of more than one sequential bars of a melody, which can be grouped, and have a type that corresponds with the kind of musical accompaniment to be applied. Thus different section types for a song form sections can be characterized by different rules for the assignment of the accompaniment. For example, in some sections, the number and selection of instruments and instrument phrases, used for accompaniment with the melody can be different than the number and selection of instruments and instrument phrases used with a melody in different types of sections. Also, the types of phrasing used can vary among section types.","The following set of song form section types typically can provide enough scope for reproducing popular songs with good fidelity:\n\n","All or some of these section types can be used as a default setting in the section assignment process herein. Most songs do not contain every type of song section. For example if all choruses have a similar intensity then Chorus 1 can be used a number of times in the song. Some songs contain a \u201cbridge\u201d or \u201cbreakdown\u201d section with significantly different accompaniment to the other section. Here a Variation section can be used. There will always be certain songs with special features that do not map easily to a selected set of sections, but that is more a matter of a particular arrangement than of the song itself The method described here is flexible enough to apply any style of music to any arrangement of song sections and produce reasonable, musical results.","The automatic song structure (or song form) is based on a melody, or an audio recording which has been analyzed to extract the \u201cmelody\u201d (events with pitch and duration arranged on a beat grid). A melody can be based on audio recordings of singing, rapping, beatboxing, musical instrument performances and other audio signals with identifiable events. There may be more than one melody or recording, sequential or overlapping, but for the purposes of applying automatic song structure described below, these can be considered to have been merged into a single sequence which will be referred to below as \u201cthe melody\u201d. Initially the whole length of the melody can be considered to be one long \u201cChorus 1\u201d section.","The melody can be characterized by a data structure that includes the audio file, and metadata including a list of notes, each with the following properties:\n\n","Given a tempo and number of beats per bar for the song, melody note positions and lengths can be freely converted between time in seconds, position in beats, or position in bars and beats. The audio file can include a recording in any computer readable format, such as WAV, AIFF, MP3 and FLV format files.","If the total length of the melody is 12 bars or more, then in one embodiment it is split into multiple sections of 6 to 12 bars in length. In the absence of any information to the contrary from the melody itself, each section can be 8 bars long, as this is the most common length in popular songs (the last section may end up with an odd length such as 11 bars but this does not usually sound like a \u201cmistake\u201d when listening to the resulting song, so can be retained). In the absence of any information to the contrary from the melody itself, the section types can be assigned in the following sequence: Verse 1, Chorus 1, Verse 2, Variation 1, Chorus 2, Variation 2, Chorus 2 (repeat as necessary if there are more than 7 sections). Then, in one embodiment, the last section is always assigned to be Chorus 2.","A number of measures can be applied to the melody to extract information to help decide the section boundaries and types. Each measure is usually not conclusive by itself, but in combination may provide a strong enough hint to deviate from the default splitting described above (it typically should be a strong hint, as splitting a song which should have 8 bar sections into different length sections will usually sound worse than splitting a song that should have different length sections into the default 8-bar sections). Measures include:\n\n","It has been observed that the last measure, \u201cself-similarity,\u201d may be reliable on its own. Measuring \u201cself-similarity\u201d through the melody can detect repeats of a similar phrase during the melody. If repeats are separated by 2 or 4 bars then both repeats are likely to be in the same section. If the repeats are separated by 12 or more bars then both repeats are likely to be in different sections of the same type. Self-similarity can be measured in the following way: For each region of the melody \u2018A\u2019 (say a 1-bar length) compare to each later region in the melody \u2018B\u2019 by offsetting the pitch of A to minimize the overall pitch differences between A and B, then summing the difference in pitch between A and B over their length. In this way, regions that are playing similar pitch intervals at similar positions in the bar will be detected as having a high similarity, but small differences in timing or accidental notes will not have much effect. It may be desirable to weight differences in pitch lower than differences in position, as phrases are often repeated with a similar rhythmic pattern but different notes.","Self-similarity can be measured for each bar of the melody, but it could also work with 2-bar sections, or other combinations. Comparing longer sections like 4 bars may not work so well because there is more chance of differences (intentional or random) between say the first and second verse, whereas it is likely that at least one bar in both verses will be very similar. There are 2 separate parameters for the measurement of self-similarity: The interval along the melody each comparison takes place, and the length of material included in each comparison. Shorter-scale similarities or similarities separated by distances other than a whole number of bars, might not be relevant to section assignment. An alternative type of self-similarity measure which might produce good results is to use the front-end of a speech recognition system to convert the recording to a sequence of phonemes, MFCCs or similar, and find regions of high similarity. In typical songs the melody is similar in all sections of the same type, but song lyrics are typically different in each verse section, but very similar in each chorus section, therefore a sequence that has near-repeats at different points in the recording is likely to be the chorus.","Information learned by analysis of the audio file can be applied to adjust the section assignment using rules that can be empirically derived.","In one embodiment, an introduction section and ending section are always added, as additional sections in the file with the melody. In one embodiment, by default a Short Intro and a Short Ending are picked. If the total length of the melody is long (more than 16 bars) then the Long Ending is picked. If you the total length of the melody is long and the first note of the melody occurs in the second half of a bar, then the Long Intro is picked and the melody is aligned so it starts in the last half of the last bar of the intro (as a \u201cpickup\u201d into the next section).","Song sections do not have to be created automatically for the purpose of the assignment of accompaniment as described herein. Rather, the section assignment can be created in response to commands received via a user interface, or otherwise. In systems implementing automatic assignment of sections and section types, the results can be edited. Normal editing operations include:\n\n","Other editing operations such as deleting or duplicating sections are not purely editing the song structure but also edit the melody itself, and likewise edits to the melody such as deleting a short section should affect the length of the song section containing that part of the melody. Boundaries between song sections are linked to positions along the melody.","Given an audio recording having a melody divided into sections, and section types, accompaniment can be produced using the technology here. Each musical style of accompaniment can be represented in a style library based on one or more instruments. Each instrument can be represented in the style library by a range of preset phrases it can play (in the current key\/chord for pitched instruments) for a given style. To create realistic song arrangements, each instrument may have the predefined settings for each type of song section in a given style, including:\n\n","A data structure to implement one example of the settings for instruments in a style can be understood with reference to , described below. An entry or entries in a style library identifies a number of instruments to play in the song on a section by section basis. Each instrument (which may all be from one style or a user-created mixture from different styles) can be represented by a set of phrases to play during a section. The process uses the above settings to decide what to play in the start, middle and end of each section. As each instrument has its own settings for each style and section type, this results in complex and realistic musical arrangements that fit the instrument and style of music being played, for any sequence of song sections.","In one embodiment, to play a style to accompany a melody or audio recording, a list of instructions for each instrument in the style is generated, based on the song section data (which can be derived from the melody) and the musical style data. Each instruction can have a timestamp measured in beats relative to the start of playback. Instructions can include:\n\n","The resulting list of instructions for each instrument can be independent of the technology used to generate audio playback for the instrument, for example the instructions could be translated to MIDI and sent to a MIDI synthesizer for playback.","In one implementation, a series of audio buffers are filled with the audio playback of each instrument. First, the buffer is split if necessary at positions corresponding to the beat positions in the list of instructions. The resulting series of buffers are sent to the instrument playback renderer with the instructions inserted at the relevant split points. The instrument playback renderer fills the buffer with an audio signal based on the currently selected phrase, chord and volume. The audio signal is based on a set of audio files saved on disk for each instrument, with embedded metadata to decide how the audio file data should be played back (length in beats, position of transients in file, etc.). The audio files can have a naming convention such as:\n\n","For example, a file name can be: \u201cHard Rock\/Rock Bass\/Rock Bass Intro 1 Cmaj.wav\u201d. To find the relevant files during playback, a pre-prepared text file having a known organization, or other data structure readable by the rendering engine, can be used to map the phrase number, chord type and chord root note to the matching audio file name.","As a result of procedures described above, a musical composition can be automatically parsed and analyzed, and accompaniment can be automatically assigned. A basic process flow for music composition automation can be characterized by the following outline:\n\n",{"@attributes":{"id":"p-0040","num":"0095"},"figref":"FIG. 1"},"The system includes a computer system  configured as a server including resources for assigning song form sections in an audio recording, and for associating audio accompaniment with the audio recording in response to the assigned sections. In addition, the computer system  includes resources for interacting with a client system (e.g. ) to carry out the process in a client\/server architecture.","Computer system  typically includes at least one processor  which communicates with a number of peripheral devices via bus subsystem . These peripheral devices may include a storage subsystem , comprising for example memory devices and a file storage subsystem, user interface input devices , user interface output devices , and a network interface subsystem . The input and output devices allow user interaction with computer system . Network interface subsystem  provides an interface to outside networks, and is coupled via communication network  to corresponding interface devices in other computer systems. Communication network  may comprise many interconnected computer systems and communication links. These communication links may be wireline links, optical links, wireless links, or any other mechanisms for communication of information. While in one embodiment, communication network  is the Internet, in other embodiments, communication network  may be any suitable computer network.","User interface input devices  may include a keyboard, pointing devices such as a mouse, trackball, touchpad, or graphics tablet, a scanner, a touchscreen incorporated into the display, audio input devices such as voice recognition systems, microphones, and other types of input devices. In general, use of the term \u201cinput device\u201d is intended to include possible types of devices and ways to input information into computer system  or onto communication network .","User interface output devices  may include a display subsystem, a printer, a fax machine, or non-visual displays such as audio output devices. The display subsystem may include a cathode ray tube (CRT), a flat-panel device such as a liquid crystal display (LCD), a projection device, or some other mechanism for creating a visible image. The display subsystem may also provide non-visual display such as via audio output devices. In general, use of the term \u201coutput device\u201d is intended to include all possible types of devices and ways to output information from computer system  to the user or to another machine or computer system.","Storage subsystem  includes memory accessible by the processor or processors, and by other servers arranged to cooperate with the system . The storage subsystem  stores programming and data constructs that provide the functionality of some or all of the processes described herein. Generally, storage subsystem  will include server management modules, a style library as described herein, programs for identification of song form sections, programs for selection of accompaniment using the style library were otherwise, and other programs and data utilized in the automated music composition technologies described herein. These software modules are generally executed by processor  alone or in combination with other processors in the system  or distributed among other servers in a cloud-based system.","Memory used in the storage subsystem can include a number of memories arranged in a memory subsystem , including a main random access memory (RAM)  for storage of instructions and data during program execution and a read only memory (ROM)  in which fixed instructions are stored. A file storage subsystem  can provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges. The modules implementing the functionality of certain embodiments may be stored by file storage subsystem in the storage subsystem , or in other machines accessible by the processor.","Bus subsystem  provides a mechanism for letting the various components and subsystems of computer system  communicate with each other as intended. Although bus subsystem  is shown schematically as a single bus, alternative embodiments of the bus subsystem may use multiple busses. Many other configurations of computer system  are possible having more or less components than the computer system depicted in .","The computer system  can comprise one of a plurality of servers, which are arranged for distributing processing of data among available resources. The servers include memory for storage of data and software applications, and a processor for accessing data and executing applications to invoke its functionality.","The system in  shows a plurality of client computer systems - arranged for communication with the computer system  via network . The client computer system  can be of varying types including a personal computer, a portable computer, a workstation, a computer terminal, a network computer, a television, a mainframe, a smartphone, a mobile device, or any other data processing system or computing device. Typically the client computer system - will include a browser or other application enabling interaction with the computer system , audio playback devices which produce sound from a rendered composition, and audio input devices such as a microphone which provide input audio data or files that can be utilized in the composition of music. In some embodiments, a client computer system - includes audio input devices such as a keyboard, other electronic audio input devices, audio synthesis sources, and the like, which can be applied to produce audio data used in the composition process.","In a client\/server architecture, the computer system  provides an interface to a client via the network . The client executes a browser, and renders the interface on the local machine. For example, a client can render a graphical user interface in response to a webpage, programs linked to a webpage, and other known technologies, delivered by the computer system  to the client . The graphical user interface provides a tool by which a user is able to receive information, and provide input using a variety of input devices. The input can be delivered to the computer system  in the form of commands, data files such as audio recordings, parameters for use in performing the automated composition processes described herein, and the like, via messages or sequences of messages transmitted over the network .","In one embodiment, a client interface for the music composition automation processes describe here can be implemented using Flash and run in a browser. The client communicates with an audio render server that gets selected based on the region the user logs in from. The amount of audio servers per region is designed to be scalable by making use of cloud computing techniques. The different protocols that get used for communication with the servers can include RPC, streaming via Realtime Messaging Protocol (RTMP) with data encoded in AMF (Action Message Format), and REST via HTTP with data encoded as JSON\/XML.","Although the computing resources are described with reference to  as being implemented in a distributed, client\/server architecture, the technologies described herein can also be implemented using locally installed software on a single data processing system including one or more processors, such as a system configured as a personal computer, as a workstation or as any other machine having sufficient data processing resources. In such system, the single data processing system can provide an interface on a local display device, and accept input using local input devices, via a bus system, like the bus subsystem , or other local communication technologies.",{"@attributes":{"id":"p-0053","num":"0108"},"figref":["FIG. 2","FIG. 2"]},"The sequence shown in  begins with presenting an interface on a client system prompting music composition (). This can be presented on a local interface, or in a client\/server architecture as mentioned above. An interface as described herein provides a means for prompting a client to begin the session, and for uploading an audio recording created or selected by the client. Thus, the client using the interface can create or select an audio recording, and upload the file to the server (). At the server, the audio recording is received and stored, in whole or in part, for processing (). The audio recording is processed to identify notes, tempo, bars and chords which characterize a melody in the recording (). Metadata is associated with the audio recording based on the identified characteristics of the melody, that can be edited during the process of musical composition, and that can be used in rendering a new composition based on the melody.","The audio file and associated metadata is then processed to identify sections of the melody, and song form section types for the identified sections (). One example procedure for identifying sections of the melody and song form section types can be understood with reference to . Next, the interface on the client system can be updated to show the section assignment ().","The updated interface on the client that shows the section assignment, also provides tools for editing the section assignment, to select a musical style, and to upload commands to the server as a result of the editing and selecting steps (). The server receives commands from the client and changes the metadata associated with the audio recording in response (). When the client has indicated that the editing has completed, or that the client is otherwise ready to proceed with the composition process, the server performs a procedure to select accompaniment for the audio recording based on the section assignment, and other metadata associated with the audio file, including the selected musical style, optionally the chords, notes, tempo and so on (). In a system described herein, the accompaniment is selected from a style library (). The style library can be comprised of sets of instruments, and phrases played by the instruments, which are composed according to particular styles of music. Entries in the style library can include a pre-recorded audio file, along with metadata used in the selection process and used in the manner in which the phrases are combined with the bars in the melody.","After selecting accompaniment for the audio recording, the server can prepare a data structure specifying arrangement of the selected accompaniment with the melody, and update the user interface to indicate the status of the procedure, and to present tools for prompting further action (). A user interacting with the interface can upload commands to render the composition (). In response to such a command, the server can render the composition using the arrangement identified in the data structure produced in step  (). Next, the server can download the rendered composition to the client (), where the client can store and play the composition ().",{"@attributes":{"id":"p-0058","num":"0113"},"figref":["FIG. 3","FIG. 2","FIG. 3"],"b":"304"},"The sequence is shown in  begins with receiving an audio recording including metadata that identifies at least the bars in the melody, and preferably additional information including the notes, tempo, chords and so on (). The next step, the algorithm determines whether the number of bars in the melody is sufficient to be effectively divided into song form section. In one example, the sequence can determine whether the number of bars exceeds a fixed integer N, such as 12 (). If the melody is too short, then the song can be considered a single section with an arbitrary or preset section type (). If the melody includes a sufficient number of bars, then the sequence can divide the melody into sections having a fixed length M in bars. In one example, the fixed length M can equal eight bars (). The sections can be defined the beginning with the first bar in the melody, and assigning the first M bars to a first section, the second M bars to a second section and so on. Any remainder of bars that is less than M can be appended to the final section. Once the melody is divided into sections of bars, the process can assigned section types to the sections according to a preset pattern, that corresponds with the section organization of a chosen, common song form (). Additional processing can be performed based on the notes or other musical content of the audio file. In procedures using this additional processing, the audio file is analyzed to determine characteristics of the components of the melody, including for example loudness, pitch, intervals, similarity of a given note with other notes in the melody having particular temporal relationships (). Using the results of this analysis, rules can be applied to just the starting and ending bars in the sections of bars that have been identified, and to change the section types associated with a given section (). Finally, in some embodiments, the process of assigning section types (e.g. after the steps of blocks  or ) can include adding introduction and ending sections to the melody ().",{"@attributes":{"id":"p-0060","num":"0115"},"figref":"FIG. 4A","b":["570","551","558","572","552","556","557","553","554","568","565","566","567","565","566","567","575","560","555"]},"The interface provides the functions of identifying the song form sections (e.g. Verse 1, Verse 2, etc.) of a song and allowing a user to manually edit the section boundaries and section types of the song structure.","The song intro and ending are special sections that are selected by using  and . There are three choices for each of these song sections. The Song Form editor gets updated according to the selection made by the user. By changing the intro and ending the overall song length and position of the vocal or melody gets adjusted accordingly.","In one example, the interface can be configured so that the Song Form can be edited by dragging any standard section type (e.g. ) from the song section list to the Song Form editor region . For example, a sequence of dragging and dropping operations can be used. For example dragging to the label of a standard song section (e.g. ) replaces the section in the song; dragging to the left of a label splits the section and inserts the new song section at the left; and dragging to the right of the section label in the song form editor behaves accordingly. The type of a standard song section in the song form editor can also be changed by clicking on the label of a section (e.g. ) and selecting the new section type from a dropdown list. The length of a song section can be changed by dragging the separating line between two song sections (e.g. ) left or right. The neighbor section will be shortened accordingly unless it is an intro or ending in which case the length cannot be altered.","A standard section in the Song Form editor can be selected by clicking it with the mouse. A selected standard section can be deleted by clicking a delete icon that appears when a section is selected. Removing a standard section in one interface embodiment will not affect the song length but merge the section to be deleted with its neighbor standard section. Controls can be implemented to limit some kinds of operations by a user. For example, a rule can require that if there is only one standard section left it cannot be deleted.",{"@attributes":{"id":"p-0065","num":"0120"},"figref":["FIG. 4A","FIGS. 4B to 4F"]},{"@attributes":{"id":"p-0066","num":"0121"},"figref":"FIG. 4B","b":["590","592"]},{"@attributes":{"id":"p-0067","num":"0122"},"figref":"FIG. 4C","b":"573"},{"@attributes":{"id":"p-0068","num":"0123"},"figref":"FIG. 4D","b":"574"},{"@attributes":{"id":"p-0069","num":"0124"},"figref":"FIG. 4E","b":["576","577"]},{"@attributes":{"id":"p-0070","num":"0125"},"figref":"FIG. 4F","b":["580","581"]},"As mentioned above, an audio recording is associated with metadata that characterizes the song. The audio recording can be associated with metadata using a variety of standard data processing techniques, including appending the data structure to the audio data file as a header or trailing section. Also, the audio recording can be associated with metadata by recording the metadata in a database keyed by a name of the audio recording. A variety of other techniques can be utilized to form the association, so that the metadata is linked to the audio recording for the purposes of data processing.","In  illustrates the data structure to be associated with an audio recording, which identifies the sections and section types for a melody in the recording. The data structure has a form providing a first field \u201cName\u201d which identifies the name of the audio recording, or the section of the audio recording, with which the metadata is associated. The data structure form includes a second field \u201cSection_type\u201d, which identifies the section type for a corresponding section in the melody. When the section type is \u201cnot set\u201d, then the section type has not been assigned and the metadata is associated with the whole recording.","The data structure includes a third field \u201cTempo\u201d which indicates the tempo of the melody. The data structure includes a fourth field \u201cBeats_per_bar\u201d which identifies the length of a bar in quarter notes in this example. The data structure includes a fifth field \u201cKey\u201d which indicates that key of the melody using the integers which map to members of a set of available keys. The data structure includes a final field \u201cChords\u201d which comprises a list of the number of beats and the name of the chord for all the chords in the sequence.","In alternative implementations, different data structure organizations can be utilized. For example, in the illustrated structure, the chord sequences are associated with each section. In other implementations, the chord sequence for an entire song can be stored in a separate data structure; and in the data structure associated with each section, an indicator of the length of the section measured in beats or in the number of chords could be provided.","Storing chords with each section has the advantage of making editing operations such as inserting or deleting a section easier to implement.","According to this example data structure, an initial song before a song form is applied could look like , where the section type is \u201cChord 1\u201d (or another default value, such as \u201cnot set\u201d) and the \u201cchords\u201d field includes all of the chords in the song. After Song form is applied each section will be associated with a corresponding structure like that of . In , the song of  has been divided into four sections, including a \u201cintro\u201d section, a \u201cverse\u201d section, a \u201cchorus\u201d section, and a \u201cending\u201d section. Each of the four sections has a key identified by the integer \u201c0.\u201d The 16 bars of the melody shown in  are divided into the verse and chorus sections. A one bar intro has been added, and a four bar ending has been added.",{"@attributes":{"id":"p-0077","num":"0132"},"figref":"FIG. 8"},"In the style, there will be a number of instruments that can be played. For each instrument in the style, there is set of fields that apply to the instrument independent of the phrase, including a \u201cName\u201d field for the instrument. A \u201cType\u201d field for the instrument is used to hold metadata for sorting, filtering, selecting style instruments. A \u201cRetrigger\u201d field includes a control flag indicating whether phrases should restart on each new chord.","Each instrument in the style is associated with a \u201cChordMask\u201d field using this data structure. The \u201cChordMask\u201d field provides a bit mask over a list of chord types {maj, min, maj7, min7, 7, dim, sus4, sus2}, where the bit mask indicates the chords that can be played using the instrument. Instruments with no pitch such as drums will have a ChordMask of 0 to indicate that the instrument does not need to follow the chord sequence. Some instruments (typically bass instruments) only need to support a few chord types to be able to play along with any other chord, reducing the amount of audio material required. If the chord sequence contains a chord not supported by the instrument, the best matching available chord can be played instead.","Each instrument in the style is associated with a \u201cBassNotChord\u201d field using this data structure. Chords may have a \u201cpedal bass\u201d note which is different to the root note of the chord, for example Cmaj\/E is a C Major chord with an E bass note. Instruments tagged with BassNotChord=1 should play the bass note, and other instruments (BassNotChord=0) should ignore the bass note.","Each instrument in the style includes a set of phrases that can be played, associated with a specific song section type using this data structure. The data structure includes a \u201cPhrase\u201d field which carries an integer that corresponds with one of the available phrases. The data structure includes \u201cStart_level\u201d and \u201cEnd_level\u201d fields, which carry values that specify the playback volume for the instrument at the start and end of the song section, allowing fade-ins, fade-outs, or any fixed or ramping volume to be achieved. The data structure includes \u201cTrig_phrase\u201d and \u201cTrig_beat\u201d fields. Optionally, a phrase can be specified to play once, either at the start of the song section (typically used for crash cymbals) or starting a specified number of quarternote beats before the end of the section, overriding the phrase that was playing until that point (typically used for drum fills). The \u201ctrig_phrase\u201d and \u201ctrig_beat\u201d fields provide metadata specifying the functions.","The data structure for the style also includes links to audio sources to play the selected phrases using the parameters assigned to the phrase. The phrases can be prerecorded audio clips, synthesizer loops, or other types of audio data sources.","A style library which is utilized to produce a composition in response to song section type can be created by defining a plurality of styles using a data structure such as that of . The data structure for a style library comprises data executable to generate musical phrases according to song form section types for accompaniment with corresponding songs. A style library can then be utilized in music composition automation machine, that characterizes an underlying melody according to song form and other song structures, including chords, notes, tempo and so on.",{"@attributes":{"id":"p-0084","num":"0139"},"figref":["FIG. 9","FIG. 1"],"b":["900","901"]},"In one embodiment, the music composition automation system includes technology for extracting information about at melody, from an audio file created or selected by a user, that has not been associated with metadata identifying the melody, and characteristics of the melody. In this case, such metadata can be deduced from processing the audio file.","An input audio file can be provided. The user can provide a tempo value along with the recording, or as mentioned above the tempo can be deduced from the recorded audio data.","In one process for deducing characteristics of an audio file, the audio signal is passed through steep highpass and lowpass filters to remove frequencies outside the range of speech and most musical instruments that would otherwise interfere with the accuracy of the subsequent measurements. The filtered signal is then downsampled to reduce the computational load of the following calculations: The peak level is measured every 10 ms and also an autocorrelation is calculated to give the most likely pitch period and periodicity (to what extent is the local signal strongly pitched rather than random\/noisy or a mixture of multiple pitches). Interpolation is used so the exact period and height of peaks in the autocorrelation are known and the highest peak found. Alternatively or additionally a harmonic product spectrum can be calculated by using a STFT to calculate the spectrum and multiplying the spectrum by multiples of itself (i.e. the level at each frequency gets multiplied by the spectrum level at 2.0 and 3.0 times that frequency) resulting in spectral peaks at the fundamental frequency of any input pitches\u2014this method gives more robust pitch measurements than autocorrelation in the presence of noise or multiple pitches, but is less accurate than autocorrelation for clean monophonic signals.","When the recording has finished, the level and periodicity measurements in each 10 ms window are used to decide which sections of the analysis correspond to singing and which sections are background noise. Large and fast drops in level are assumed to be the start of background noise, even if the signal does not fall to as low a level as the background noise in other parts of the recording, to guarantee that deliberate short gaps between sung notes are not lost. The pitch period measurements in the time windows are converted to values representing semitones (middle C has the value 60.0 for compatibility with MIDI). In case the singing is out of tune relative to standard tuning, an offset is calculated that minimizes the mean squared difference from all measured pitches to the standard pitch of the nearest semitone. This offset is added to all pitches to bring them into tune. Because recordings are typically short, tuning is assumed to have the same offset throughout rather than drifting during the course of the recording.","The continuous pitch track is deleted at obvious pauses (where the audio level is low or the signal is not strongly pitched) and then tidied so there are no very short regions of pitch or silence. Each region with a continuous pitch track must consist of one or more notes, so each region is examined for significant changes in pitch which could be transitions between notes. Transitions which return close to the original pitch within a certain time window (half a period or the lowest vibrato rate, around 4 Hz) are ignored as vibrato or unintentional random variation. The continuous pitch track is then split at the note transitions, and the pitch of each continuous region is calculated as the average of all contained pitches. This usefully averages out vibrato and notes that sweep in pitch rather than sitting at a steady pitch, but if the region does not contain a pitch close to the average pitch then it must consist of more than one note and should be split again, or for short notes the final pitch can be taken as a more likely intended pitch for the note. While vibrato is ignored by the above method it is useful to know when vibrato is present. If local maxima and minima alternate and are between half a cycle at 4-8 Hz apart, then the difference between the maximum and minimum pitch is the vibrato amplitude.","For additional error detection and correction, if the pitch track increases or decreases near-monotonically for more than a certain distance (say 1.5 semitones) it is likely to be a deliberate transition between notes, so the pitch track can be split at the midpoint of the transition, and assigned to the preceding or following steady pitch. Regions of vibrato can be identified if the pitch varies with an approximately sinusoidal shape at a rate between 4 and 8 Hz. If the pitch alternates up and down at this rate the whole region can have smoothing applied to reveal any underlying long-term pitch variation\u2014the crucial decision is whether vibrato is superimposed on a steady pitch, or on a pitch glide or transition of 1 or 2 semitones which is otherwise invisible if the amplitude of the vibrato is greater than the transition.","At this point the regions represent a series of notes positioned in time. Next the tempo and barline positions are considered in order to align the notes to a timing grid. The note start positions are compared to a series of grids, from 120 to 450 divisions per minute. For each grid an offset is calculated that minimizes the mean squared difference between note start positions and the nearest grid position. The grid with the lowest MSD will therefore have grid positions close to many of the note start positions and is assumed to be a good match. Next it needs to be decided if one grid interval represents a quarternote, eighth-note, or some other musical note length, from which the tempo can be calculated. Lastly, given a bar length (typically 3 or 4 quarternotes per bar), a grid position is chosen to represent the first barline such that more note star positions are closer to barlines than any other grid position.","There are many assumptions in the above process and the result is not always accurate, so the user is given the chance to adjust the offset and speed of the recording\/melody relative to a metronome and graphic display of beat and bar positions, and also to correct or delete individual melody notes, before song structure is generated.","In some embodiments, automatic chord assignment processes can assign chords to bars in the melody in support of the process of assigning sections and section types, and of the process of selecting accompaniment. Also, chord assignments can be edited or created by users of the program.","Automatic chord assignment processes can include creating a histogram of the notes present in each bar of the melody, and comparing the histogram to a set of pre-prepared histograms representing the most likely melody notes that would normally be accompanied by each candidate chord. The set of candidate chords are chosen based on the key of the melody. The best matching chord (using least squared difference between the histograms, or some other measure) is then selected for each bar.","Optionally, if the same chord is selected a number of times (e.g. 3) consecutively, the middle occurrence can be replaced by the second best matching chord to make the chord sequence more interesting.","For some styles of music such as Jazz, it is desirable to substitute each of the selected chords with a more complex, colorful chord using a lookup table.","While the present invention is disclosed by reference to the preferred embodiments and examples detailed above, it is understood that these examples are intended in an illustrative rather than in a limiting sense. Computer-assisted processing is implicated in the described embodiments. Accordingly, the present invention may be embodied in methods for perform processes described herein, systems including logic and resources to perform processes described herein, systems that take advantage of computer-assisted methods for performing processes described herein, media impressed with logic to perform processes described herein, data streams impressed with logic to perform processes described herein, or computer-accessible services that carry out computer-assisted methods for perform processes described herein. It is contemplated that modifications and combinations will readily occur to those skilled in the art, which modifications and combinations will be within the spirit of the invention and the scope of the following claims."],"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","SUMMARY","DETAILED DESCRIPTION"],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":["FIGS. 4A-4F","FIG. 2","FIG. 3"]},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":["FIG. 6","FIG. 5"]},{"@attributes":{"id":"p-0016","num":"0015"},"figref":["FIG. 7","FIG. 6"]},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 9"}]},"DETDESC":[{},{}]}
