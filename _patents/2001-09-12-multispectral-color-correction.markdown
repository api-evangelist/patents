---
title: Multi-spectral color correction
abstract: A system and method for performing color correction based on physical measurements (or estimations) of color component spectra (e.g. red, green, blue color component spectra). A color correction system may comprise a spectrum sensing device, a color calibration processor, and a calculation unit. The spectrum sensing device may be configured to measure color component power spectra for pixels generated by one or more display devices on a display surface. The color calibration processor may receive power spectra for a given pixel from the spectrum sensing device and compute a set of transformation parameters in response to the power spectra. The transformation parameters characterize a color correction transformation for the given pixel. The color calibration processor may compute such a transformation parameter set for selected pixels in the pixel array. The calculation unit may be configured to (a) compute initial color values for an arbitrary pixel in the pixel array, (b) compute modified color values in response to the initial color values and one or more of the transformation parameter sets corresponding to one or more of the selected pixels, and (d) transmit the modified color values to the display device.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=06950109&OS=06950109&RS=06950109
owner: Sun Microsystems, Inc.
number: 06950109
owner_city: Santa Clara
owner_country: US
publication_date: 20010912
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS REFERENCE TO RELATED APPLICATIONS","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS"],"p":["This application claims the benefit of priority to U.S. Provisional Application No. 60\/242,640 filed on Oct. 23, 2000 entitled \u201cMULTI-SPECTRAL COLOR CORRECTION\u201d.","1. Field of the Invention","This invention relates generally to the field of computer graphics and, more particularly, to a system and method for correcting the presentation of color by one or more display devices (e.g. projection devices).","2. Description of the Related Art","A light beam may be described as a superposition of beams having a continuum of wavelengths. The amount of power concentrated at each wavelength \u03bb of the continuum is given by a function F(\u03bb) known as the power spectrum. The power spectrum may be measured by a spectrum sensing device such as a spectroradiometer. The power spectrum determines the perceived color of the light beam for a given observer. The space of possible power spectra is infinite dimensional. However, because the human eye has only three types of color sensitive cells, the space of perceived colors is generally considered to be three dimensional. Thus, the mapping between the power spectra and perceived colors is many to one. In other words, an infinite collection of power spectra may induce the same color perception for a given observer. Two power spectra that give the same color perception are said to be \u201cmetamers\u201d.","An image on a display screen (or projection screen) comprises an array of physical pixels. Each physical pixel radiates a light beam to the observer's eye(s). Each pixel light beam has a power spectrum that determines the perceived color of the corresponding physical pixel.","Display devices generate color by mixing varying amounts of Q fundamental colors, where Q is an integer representing the number of fundamental colors. Typically, Q equals three and the fundamental colors are red, green and blue. Thus, each pixel light beam may comprise a red component beam, a green component beam and a blue component beam having power spectra \u0192(\u03bb), \u0192(\u03bb) and \u0192(\u03bb) respectively. Therefore, the pixel power spectrum F(\u03bb) is a linear combination of the three component spectra: \n\n\nwhere the scalar values \u03b3, \u03b3and \u03b3control the relative amounts of red, green and blue respectively which are combined in the pixel light beam. Let \u03b3 denote the vector whose components are the scalar values \u03b3, \u03b3and \u03b3, i.e. \n\n\nThe vector \u03b3 may be referred to herein as the color intensity vector. The display device receives a video signal that determines the vector \u03b3 for each pixel in the pixel array. The video signal may be an analog or digital video signal. The red, green and blue beams comprising the pixel beam are referred to herein as color component beams, and their corresponding spectra are referred to herein as color component spectra.\n","For various reasons, the color component spectra \u0192(\u03bb), \u0192(\u03bb) and \u0192(\u03bb) of a pixel beam may change with the passage of time. Thus, the perceived color C(t) of the pixel beam may vary in time even though the color intensity vector \u03b3 is held constant. For example, in certain types of projection devices, the color component beams are generated by passing beams of white light through red, green and blue color filters respectively. The color filters experience thermal stresses due to the absorption of light energy. The thermal stresses may, over time, induce changes in the absorption properties of the color filters. Also, the absorbing materials in the color filters may change their filtering characteristics over time as they age. Thus, there exists a need for a system and method which can correct and stabilize the color generated by displayed pixels in spite of time variation in their color component output spectra.","Suppose the pixel array is parameterized by a horizontal pixel index I and a vertical pixel index J. In addition to variations with respect to time, the color component output spectra \u0192(\u03bb), \u0192(\u03bb) and \u0192(\u03bb) may vary spatially, i.e. with respect to indices I and J. For example, a color filter used in the projector may have non-uniform absorption properties across its surface. Thus, the perceived color C(I,J) observed on a display (or projection screen) may vary spatially even when all pixels of the pixel array are driven by the same color intensity vector \u03b3. Thus, there exists a need for a system and method which can correct and uniformize the spatial distribution of color generated by a display device (e.g. a projection device) to compensate for these spatial variations.","Suppose multiple units of a given model of display device (e.g. projection device) are generated in a manufacturing batch. Because of the difficulty of exactly reproducing all manufacturing conditions from unit to unit, the color component output spectra \u0192(\u03bb), \u0192(\u03bb) and \u0192(\u03bb) generated by pixels of a first unit may not agree with the corresponding spectra of a second unit. Thus, the color C() generated by the first unit may not agree with the color C() generated by the second unit even when both units are driven by the same color intensity vector \u03b3. In particular, the color C(I,J,) generated by pixel (I,J) in the first unit may not agree with the color C(I,J,) generated by the corresponding pixel (I,J) in the second unit, even when both units are driven by the same color vector \u03b3. This problem may be especially noticeable when the multiple units are used in a single display system. For example, multiple units of a given model of projector may be used to project an integrated image onto a common projection screen. The non-repeatability of color between the units may be especially offensive in areas of the projection screen where projected images overlap. Thus, there exists a need for a system and method which could correct and uniformize the presentation of color between multiple units of a given display device, especially where the multiple units are employed in a single display system.","The problem of non-repeatable color is exacerbated when a display system uses display devices (a) from different manufacturers, (b) from the same manufacturer but conforming to different models, and\/or (c) based on differing technologies. For example, a first display device may use LCD technology while a second display device uses digital micro-mirror technology (e.g., Texas Instruments DLP\u2122 technology). In another example, a first projection device and second projection device may use different light sources and\/or color filter materials. Thus, the color C(A) generated by a first display device A may not agree with the color C(B) generated by a second display device B even when the two display devices are driven with the same color intensity vector \u03b3. Therefore, there exists a need for a system and method which can correct and uniformize the presentation of color between multiple display devices from potentially different manufacturers and\/or based on different underlying technologies, etc.","The present invention contemplates a system and method for performing color correction based on physical measurements of color component spectra (e.g. red, green, blue color component spectra). In one embodiment, a color correction system may comprise a spectrum sensing device, a color calibration processor, and a calculation unit. The spectrum sensing device may be configured to measure a plurality of power spectra for one or more pixels generated by a display device. The color calibration processor may receive the power spectra from the spectrum sensing device and may compute a set of transformation parameters in response to the power spectra. The calculation unit may be configured to (a) compute initial color values for a given pixel, (b) compute modified color values based on the initial color values and the set of transformation parameters, and (c) transmit the modified color values to the display device. The given pixel may be inside or outside the group of one or more pixels for which the transformation parameter set was computed.","In one embodiment, the set of transformation parameters may comprise a lookup table which characterizes a color correction transformation. In this case, the calculation unit may generate a read address for the lookup table using the initial color values, and may read the modified color values from the lookup table using the read address. Alternatively, the lookup table may store color correction difference values. In this case, the calculation unit may add the difference values obtained from the lookup table to the initial color values respectively to determine the modified color values.","The power spectra may include, for example, a red power spectrum, a green power spectrum and a blue power spectrum. The spectrum sensing device may measure the red power spectrum in response to (or in synchronization with) the calculation unit driving the one or more pixels with a maximal red intensity value, a zero green intensity value and a zero blue intensity value. The spectrum sensing device may measure the green power spectrum in response to (or in synchronization with) the calculation unit driving the pixel with a maximal green intensity value, a zero red intensity value, and a zero blue intensity value. Similarly, the spectrum sensing device may measure the blue power spectrum in response to (or in synchronization with) the calculation unit driving the pixel with a maximal blue intensity value, a zero red intensity value and a zero green intensity value.","The spectrum sensing device may be a spectroradiometer, a charge coupled device (CCD) array, or other type of spectrum sensor. In one embodiment, the spectrum sensing device comprises a lens and an array of light-sensitive electronic detectors. Each power spectrum may comprise a sequence of N spectral power measurements at N corresponding wavelengths, wherein N is an integer. In the preferred embodiment, N is greater than three. In one alternative embodiment, the spectrum sensing device presents the power spectra to the color calibration processor as analog signals.","In one embodiment, the calculation unit is configured to receive samples from a sample buffer and compute the initial color values for the pixel by filtering sample color values belonging to the samples.","The color calibration unit may multiply each of the power spectra by each of three response curves respectively, and integrate the resulting product functions (i.e. sequences) in order to compute the first set of transformation parameters. In one embodiment, the three response curves may comprise three color sensitivity curves (e.g. cone sensitivity curves) for a human eye.","The color correction system may be configured to perform color correction on one or more pixel arrays generated on a display surface (e.g. a projection screen) by one or more display devices. A subset of the pixels in each pixel array may be selected for spectral calibration. The spectrum sensing device may measure a plurality of power spectra for each pixel in a first subset of a first pixel array generated by a first display device. The color calibration processor may compute a transformation parameter set (e.g. a color correction matrix, a lookup table, etc.) for each pixel in the first subset in response to the corresponding plurality of power spectra. The calculation unit may (a) compute initial color values for a first pixel of the first pixel array, (b) compute modified color values for the first pixel based on the initial color values and one or more of the transformation parameter sets corresponding to one or more of pixels in the first subset, and (c) transmit the modified color values to the first display device.","In one embodiment, the transformation parameter set for each pixel in the first subset comprises a lookup table which characterizes a color correction transformation for the pixel. Each lookup table may contain modified color values indexed by initial color values (or high order bits thereof) for a particular pixel of the first subset. In this embodiment, the calculation unit may determine one or more of nearest pixels in the first subset to the first pixel, and may read a table entry from each of the lookup tables corresponding to the nearest pixels. The read addresses into the lookup tables are generated from the initial color values. The calculation unit may perform an interpolation on the one or more table entries to determine the modified color values for the first pixel.","In a second embodiment, the lookup tables store color correction difference values, i.e. differences between the modified color values and the corresponding initial color values for a particular pixel of the first subset. In this second embodiment, each of the interpolated correction values resulting from the interpolation of table entries may be added to a corresponding one of the initial color values to generate the modified color values.","In one embodiment, the transformation parameter set for each pixel of the first subset may comprise a brief characterization (such as a matrix) of a color correction transformation for the pixel. The calculation unit may be configured to interpolate a first parameter set for the first pixel based on the transformation parameter sets of the one or more nearest neighbor pixels in the first subset. The calculation unit may generate the modified color values by applying the color correction transformation to the initial color values using the first parameter set.","The plurality of power spectra for each pixel in the first subset may comprise a first power spectrum, a second power spectrum and a third power spectrum. The spectrum sensing device may measure the first power spectrum for each pixel of the first subset in response to the calculation unit driving the pixel with a red calibration pattern comprising a maximum red intensity value, a zero green intensity value and a zero blue intensity value. The spectrum sensing device may measure the second power spectrum for each pixel of the first subset in response to the calculation unit driving the pixel with a green calibration pattern comprising a maximum green intensity value, a zero red intensity value and a zero blue intensity value. The spectrum sensing device may measure the third power spectrum for each pixel of the first subset in response to the calculation unit driving the pixel with a blue calibration pattern comprising a maximum blue intensity value, a zero red intensity value and a zero green intensity value.","The calculation unit may be configured to drive one or more pixels (through the first display device) with the red calibration pattern in response to a first control signal asserted by the color calibration processor. In addition, the spectrum sensing device may be configured to measure the first power spectrum for the one or more pixels in response to a second control signal asserted by the color calibration processor. Thus, the color calibration processor may control the synchronization of the calibration pattern display and the power spectrum measurements.","The first subset of the first pixel array may comprise a rectangular grid. The density of the rectangular grid in the pixel array may assume any desired value. The first subset of the first pixel array may also comprise a non-uniform grid.","Each of the power spectra for each pixel in the first subset may comprise N spectral power measurements at N corresponding wavelengths in the visible region, where N is a positive integer. The integer N is preferably greater than three. Large values of N may serve to characterize the power spectra more precisely than small values of N, and allow more effective color correction. However, large values of N also increase memory storage and processing bandwidth requirements.","In the preferred embodiment, the color correction system is further configured to provide calibration for a second pixel array generated on the display surface by a second display device. The spectrum sensing device measures a plurality of power spectra for each pixel in a second subset of a second pixel array. The color calibration processor computes a transformation parameter set for each pixel in the second subset in response to the corresponding plurality of power spectra. The second calculation unit may additionally (e) compute initial color values for a second pixel of the second pixel array, (f) compute modified color values for the second pixel based on the initial color values and one or more transformation parameter sets corresponding to one or more of the pixels of the second subset, and (g) transmit the modified color values to the second display device.","In some embodiments, the second calculation unit may be configured to turn off all pixels of the second pixel array while the spectrum sensing device measures the plurality of power spectra for the first subset of the first pixel array.","In one embodiment, a graphics system comprising a processor and memory may be configured to perform color correction for one or more display devices. The memory may store program instructions executable by the processor. In response to execution of the program instructions, the processor is operable to perform color correction on computed color values of a pixel by performing a color correction transformation. The color correction transformation may be implemented by one or more table lookup operations. Lookup tables may be generated for a subset of grid pixels in a pixel array in response to measurements of color component power spectra at the subset of grid pixels. The corrected color values for a given pixel may be determined by accessing lookup tables corresponding to one or more grid pixels which are neighbors to the given pixel.","Alternatively, the color correction transformation for each pixel in the subset of grid pixels may be more succinctly characterized by a limited set of parameters such as a matrix. For a non-grid pixel in the pixel array, the calculation unit may interpolate an instantaneous parameter set appropriate for the non-grid pixel based on the parameter sets of nearest neighbor pixels in the pixel grid. The calculation unit may use the instantaneous parameter set to apply the color correction transformation on the initial color values to determine the corrected color values. For example, the instantaneous parameter set may control a combination of arithmetic operations (e.g. additions, subtractions, multiplications, etc.) which determine the corrected color values.","While the invention is susceptible to various modifications and alternative forms, specific embodiments thereof are shown by way of example in the drawings and will herein be described in detail. It should be understood, however, that the drawings and detailed description thereto are not intended to limit the invention to the particular forms disclosed, but on the contrary, the intention is to cover all modifications, equivalents, and alternatives falling within the spirit and scope of the present invention as defined by the appended claims. Note the headings used herein are for organizational purposes only and are not meant to limit the description provided herein or the claims attached hereto.",{"@attributes":{"id":"p-0059","num":"0058"},"figref":"FIG. 1","b":["80","80","82","84","84","82"],"sub":["1 ","G","1 ","G "]},"System unit  may also couple to various input devices such as a keyboard , a mouse , a spectrum sensing device SSD (e.g. a spectrophotometer), a video camera, a trackball, a digitizing tablet, a six-degree of freedom input device, a head tracker, an eye tracker, a data glove, body sensors, etc. Application software may be executed by computer system  to display 3-D graphical objects on the display devices.","As used herein the term \u201cspectrum sensing device\u201d means any device which is capable of measuring the amount of energy radiated from a surface and\/or points on a surface at two or more wavelengths in the visible region. Examples of spectrum sensing devices may include a spectroradiometer, a CCD array, and an array of photodiodes with a wavelength-dispersing device such as a lens.",{"@attributes":{"id":"p-0062","num":"0061"},"figref":"FIG. 2A","b":["80","80","80","102","112","104","106","104","104"]},"Host CPU  may be realized by any of a variety of processor technologies. For example, host CPU  may comprise one or more general purpose microprocessors, parallel processors, vector processors, digital signal processors, etc., or any combination thereof. System memory  may include one or more memory subsystems representing different types of memory technology. For example, system memory  may include read-only memory (ROM) and\/or random access memory (RAM)\u2014such as static random access memory (SRAM), synchronous dynamic random access memory (SDRAM) and\/or Rambus dynamic access memory (RDRAM).","System bus  may comprise one or more communication buses or host computer buses (e.g. for communication between host processors and memory subsystems). In addition, various peripheral devices and peripheral buses may be connected to system bus .","Graphics system  may comprises one or more graphics boards that generate video signals for display devices DDthrough DDin response to graphics data received from host CPU  and\/or system memory . Display devices DDthrough DDmay include monitors and\/or projection devices.","In one embodiment, the graphics boards comprising graphics system  may be partitioned into one or more chains with each chain driving one or more display devices. For example, R graphics boards may be coupled together in a linear chain as suggested by FIG. B. The R graphics boards may collaborate in the generation of video signals Vand Vfor display devices DDand DDrespectively. The number R of graphics boards may be chosen to match the combined pixel input bandwidth of display devices DDand DD. The graphics boards may also couple to system bus  (e.g. by crossbar switches or any other type of bus connectivity logic). The first graphics board in the linear chain is denoted GB(), and the generic Kgraphics board in the linear chain is denoted GB(K). In one alternative embodiment, some or all of graphics boards comprising graphics system  may be optimized for \u201cnon-collaborative\u201d operation. Each non-collaborative graphics board may omit logic for interfacing with other graphics boards.","In one embodiment, host CPU  may transfer information to\/from each graphics board GB(K) according to a programmed input\/output (I\/O) protocol over system bus . In a second embodiment, each graphics board GB(K) may access system memory  according to a direct memory access (DMA) protocol or through intelligent bus-mastering. In yet another embodiment, the graphics boards may be coupled to system memory  through a direct port, such as an Advanced Graphics Port (AGP) promulgated by Intel Corporation.","One or more graphics applications conforming to an application programming interface (API) such as OpenGL\u00ae or Java\u00ae 3D may execute on host CPU . The graphics application(s) may construct a scene composed of geometric objects in a world coordinate system, and may decompose the scene into a collection of graphics primitives (e.g. triangles). The graphics application may compress the graphics primitives, and transfer the compressed graphics data to the graphics boards GB(), GB(), GB(), . . . , GB(R-).","The first graphics board GB() may generate digital video streams Xand Y. The second graphics board GB() may receive digital video streams Xand Yfrom the first graphics board GB(), and transmit digital video streams Xand Yto the third graphics board GB(). In general, graphics board GB(K), for K between 1 and (R-) inclusive, may receive digital video streams Xand Yfrom a previous graphics board GB(K\u22121), and transmit digital video streams Xand Yto a next graphics board GB(K+1).","Each graphics board is responsible for filling in a portion of first video signal Vand\/or the second video signal V. Thus, each digital video stream Xis no less \u201cfilled in\u201d with pixel data than its predecessor X. The same observation holds for the digital video streams Y, Y, . . . , Y. The last graphics board GB(R-) receives digital video streams Xfrom the next-to-last graphics board GB(R-), and generates digital video streams Xand Y. The last graphics board GB(R-) converts the digital video streams Xand Yinto analog video signals Vand Vrespectively for presentation to display devices DDand DDrespectively. Thus, the last graphics board GB(R-) may include digital-to-analog conversion (DAC) hardware. In one embodiment, the graphics boards are interchangeable, and thus, each of the graphics boards includes DAC hardware. It is noted that display devices DDand DDmay be configured to receive and\/or output digital video data, in which case the digital-to-analog conversion may be bypassed.","It is noted that the graphics boards comprising 3-D graphics system  may couple to one or more busses of various types in addition to system bus . Furthermore, some or all of the graphics boards may couple to a communication port, and thereby, directly receive graphics data from an external source such as the Internet or a local area network. Computer system  or system unit  may be configured as a server accessible by client computers through a computer network.","Graphics boards may receive graphics data from any of various sources including: host CPU , system memory  or any other memory, external sources such as a local area network, or a broadcast medium (e.g. television). While graphics system  is depicted as part of computer system , graphics system  may also be configured as a stand-alone device.","Graphics system  may be comprised in any of various systems, including a network PC, a gaming play-station, an Internet appliance, a television (including an HDTV system, an interactive television system, etc.), or other devices which display 2D and\/or 3D graphics.","FIG. : Graphics Board GB(K)",{"@attributes":{"id":"p-0074","num":"0073"},"figref":"FIG. 3","b":["1","90","162","0","1","178","178"]},"Graphics processing unit  may comprise any combination of processor technologies. For example, graphics processing unit  may comprise specialized graphics processors or calculation units, multimedia processors, DSPs, general purpose processors, programmable logic, reconfigurable logic, discrete logic, or any combination thereof. Graphics processing unit  may comprise one or more rendering units such as rendering units A-D. Graphics processing unit  may also comprise one or more control units such as control unit , one or more data memories such as data memories A-D, and one or more schedule units such as schedule unit . Sample buffer  may comprise one or more sample memories A-N.","Graphics board GB(K) may include two digital video input ports for receiving digital video streams Xand Yfrom a previous graphics board GB(K\u22121) in cases where graphics board GB(K) is not the first graphics board in the linear chain of graphics boards. Similarly, graphics board GB(K) may include two digital video output ports for transmitting digital video streams Xand Yto the next graphics board GB(K+1) in cases where graphics board GB(K) is not the last graphics board in the linear chain.","A. Control Unit ","Control unit  operates as the interface between graphics board GB(K) and computer system  by controlling the transfer of data between graphics board GB(K) and computer system . In embodiments of graphics board GB(K) that comprise two or more rendering units A-D, control unit  may also partition the stream of data received from computer system  into a corresponding number of parallel streams that are routed to the individual rendering units A-D. The graphics data may be received from computer system  in a compressed form. Graphics data compression may advantageously reduce the required transfer bandwidth between computer system  and graphics board GB(K). In one embodiment, control unit  may be configured to split and route the received data stream to rendering units A-D in compressed form.","The graphics data may comprise one or more graphics primitives. As used herein, the term graphics primitive includes polygons, parametric surfaces, splines, NURBS (non-uniform rational B-splines), sub-division surfaces, fractals, volume primitives, and particle systems. These graphics primitives are described in detail in the text book entitled \u201cComputer Graphics: Principles and Practice\u201d by James D. Foley, et al., published by Addison-Wesley Publishing Co., Inc., 1996.","It is noted that the embodiments and examples presented herein are described in terms of polygons for the sake of simplicity. However, any type of graphics primitive may be used instead of or in addition to polygons in these embodiments and examples.","B. Rendering Units Rendering units A-D (also referred to herein as draw units) are configured to receive graphics instructions and data from control unit  and then perform a number of functions which depend on the exact implementation. For example, rendering units A-D may be configured to perform decompression (if the received graphics data is presented in compressed form), transformation, clipping, lighting, texturing, depth cueing, transparency processing, set-up, visible object determination, and virtual screen rendering of various graphics primitives occurring within the graphics data. Rendering units A-D are intended to represent an arbitrary number of rendering units.","The graphics data received by each rendering unit  may be decompressed into one or more graphics \u201cprimitives\u201d which may then be rendered. The term primitive refers to components of objects that define the object's shape (e.g., points, lines, triangles, polygons in two or three dimensions, polyhedra, or free-form surfaces in three dimensions). Each of rendering units A-D may be any suitable type of high performance processor (e.g., a specialized graphics processor or calculation unit, a multimedia processor, a digital signal processor, or a general purpose processor).","Graphics primitives or portions of primitives which survive a clipping computation may be projected onto a 2-D viewport. Instead of clipping in 3-D, graphics primitives may be projected onto a 2-D view plane (which includes the 2-D viewport) and then clipped with respect to the 2-D viewport.","Virtual screen rendering refers to calculations that are performed to generate samples for projected graphics primitives. For example, the vertices of a triangle in 3-D may be projected onto the 2-D viewport. The projected triangle may be populated with samples, and values (e.g. red, green, blue, z and alpha values) may be assigned to the samples based on the corresponding values already determined for the projected vertices. (For example, the red value for each sample in the projected triangle may be interpolated from the known red values of the vertices.) These sample values for the projected triangle may be stored in sample buffer . A virtual image accumulates in sample buffer  as successive primitives are rendered. Thus, the 2-D viewport is said to be a virtual screen on which the virtual image is rendered. The sample values comprising the virtual image are stored into sample buffer . Points in the 2-D viewport are described in terms of virtual screen coordinates x and y, and are said to reside in \u201cvirtual screen space\u201d. See  for an illustration of the two-dimensional viewport  populated with samples.","When the virtual image is complete, e.g., when all graphics primitives comprising the virtual image have been rendered, sample-to-pixel calculation units CU() through CU(V-) may read the rendered samples from sample buffer , and filter the samples to generate pixel values. Each sample-to-pixel calculation unit CU(J) may be assigned a region of the virtual screen space, and may operate on samples corresponding to the assigned region. Sample-to-pixel calculation units CU() through CU(V-) may operate in parallel.","In the embodiment of graphics board GB(K) shown in , rendering units A-D calculate sample values instead of pixel values. This allows rendering units A-D to perform super-sampling, i.e. to calculate more than one sample per pixel. Super-sampling in the context of the present invention is discussed more thoroughly below. More details on super-sampling are discussed in the following books:\n\n","Sample buffer  may be double-buffered so that rendering units A-D may write samples for a first virtual image into a first portion of sample buffer , while a second virtual image is simultaneously read from a second portion of sample buffer  by sample-to-pixel calculation units CU.","C. Data Memories","Each of rendering units A-D may be coupled to a corresponding one of instruction and data memories A-D. In one embodiment, each of memories A-D may be configured to store both data and instructions for a corresponding one of rendering units A-D. While implementations may vary, in one embodiment, each data memory A-D may comprise two 8 MByte SDRAMs, providing a total of 16 MBytes of storage for each of rendering units A-D. In another embodiment, RDRAMs (Rambus DRAMs) may be used to support the decompression and set-up operations of each rendering unit, while SDRAMs may be used to support the draw functions of each rendering unit. Data memories A-D may also be referred to as texture and render memories A-D.","D. Schedule Unit","Schedule unit  may be coupled between rendering units A-D and sample memories A-N. Schedule unit  is configured to sequence the completed samples and store them in sample memories A-N. Note in larger configurations, multiple schedule units  may be used in parallel. In one embodiment, schedule unit  may be implemented as a crossbar switch.","E. Sample Memories","Super-sampled sample buffer  comprises sample memories A-N, which are configured to store the plurality of samples generated by rendering units A-D. As used herein, the term \u201csample buffer\u201d refers to one or more memories which store samples. As previously noted, samples may be filtered to form each output pixel value. Output pixel values may be provided to display device DDand\/or display device DD.","Sample buffer  may be configured to support super-sampling, critical sampling, or sub-sampling with respect to pixel resolution. In other words, the average distance between samples (x,y) may be smaller than, equal to, or larger than the average distance between pixel centers in virtual screen space. Furthermore, because the convolution kernel C(x,y) may take non-zero functional values over a neighborhood which spans several pixel centers, a single sample may contribute to several output pixel values.","Sample memories A-N may comprise any of various types of memories (e.g., SDRAMs, SRAMs, RDRAMs, 3DRAMs, or next-generation 3DRAMs) in varying sizes. In one embodiment, each schedule unit  is coupled to four banks of sample memories, where each bank comprises four 3DRAM-64 memories. Together, the 3DRAM-64 memories may form a 116-bit deep super-sampled sample buffer that stores multiple samples per pixel. For example, in one embodiment, each sample memory A-N may store up to sixteen samples per pixel.","3DRAM-64 memories are specialized memories configured to support full internal double buffering with single buffered Z in one chip. The double buffered portion comprises two RGBX buffers, where X is a fourth channel that can be used to store other information (e.g., alpha). 3DRAM-64 memories also have a lookup table that takes in window ID information and controls an internal 2-1 or 3-1 multiplexer that selects which buffer's contents will be output. 3DRAM-64 memories are next-generation 3DRAM memories that may soon be available from Mitsubishi Electric Corporation's Semiconductor Group. In one embodiment, 32 chips used in combination are sufficient to create a double-buffered 1280\u00d71024 super-sampled sample buffer with eight samples per pixel.","Since the 3DRAM-64 memories are internally double-buffered, the input pins for each of the two frame buffers in the double-buffered system are time multiplexed (using multiplexers within the memories). The output pins may be similarly time multiplexed. This allows reduced pin count while still providing the benefits of double buffering. 3DRAM-64 memories further reduce pin count by not having z output pins. Since z comparison and memory buffer selection are dealt with internally, use of the 3DRAM-64 memories may simplify the configuration of sample buffer . For example, sample buffer  may require little or no selection logic on the output side of the 3DRAM-64 memories. The 3DRAM-64 memories also reduce memory bandwidth since information may be written into a 3DRAM-64 memory without the traditional process of reading data out, performing a z comparison, and then writing data back in. Instead, the data may be simply written into the 3DRAM-64 memory, with the memory performing the steps described above internally.","Graphics processing unit  may be configured to generate a plurality of sample positions according to a particular sample positioning scheme (e.g., a regular grid, a perturbed regular grid, etc.). Alternatively, the sample positions (or offsets that are added to regular grid positions to form the sample positions) may be read from a sample position memory (e.g., a RAM\/ROM table). Upon receiving a polygon that is to be rendered, graphics processing unit  determines which samples fall within the polygon based upon the sample positions. Graphics processing unit  renders the samples that fall within the polygon and stores rendered samples in sample memories A-N. Note as used herein the terms render and draw are used interchangeably and refer to calculating color values for samples. Depth values, alpha values, and other per-sample values may also be calculated in the rendering or drawing process.","F. Sample-To-Pixel Calculation Units","Sample-to-pixel calculation units CU() through CU(V-) (collectively referred to as sample-to-pixel calculation units CU) may be coupled together in a linear succession as shown in FIG. . The first sample-to-pixel calculation unit CU() in the linear succession may be programmed to receive digital video streams Xand Yfrom a previous graphics board GB(K\u22121), and the last sample-to-pixel calculation unit CU(V-) in the linear succession may be programmed to transmit digital video streams Xand Yto the next graphics board GB(K+1).","If graphics board GB(K) is the first graphics board in the linear chain of graphics boards shown in , first sample-to-pixel calculation unit CU() may be programmed to disable reception of input signals Xand Y. If graphics board GB(K) is the last graphics board in the linear chain, the last sample-to-pixel calculation unit CU(V-) may be programmed to provide the digital video streams Xand Yto digital-to-analog conversion units A and B respectively.","In cases where J takes a value between 1 and V- inclusive, sample-to-pixel calculation unit CU(J) is configured to receive digital video input streams Aand Bfrom a previous sample-to-pixel calculation unit CU(J\u22121), and to transmit digital video output streams Aand Bto the next sample-to-pixel calculation unit CU(J+1). The first sample-to-pixel calculation unit CU() is configured to receive digital video streams Xand Yfrom a previous graphics board GB(K\u22121), and to transmit digital video stream Aand Bto the second sample-to-pixel calculation unit CU(). For notational uniformity, digital video streams Xand Yare also referred to as digital video streams Aand B. The last sample-to-pixel calculation unit CU(V-) receives digital video streams Aand Bfrom the previous sample-to-pixel calculation unit CU(V-), and generates digital video streams Xand Y(which are also referred to herein as video streams Aand B). Sample-to-pixel calculation unit CU(V-) may be programmed to supply the digital video streams Xand Yto a next graphics board GB(K+1) or to DAC units A\/B.","Video streams X, X, . . . , Xgenerated by the linear chain of graphics boards, and video streams A, A, . . . , Agenerated by the sample-to-pixel calculation units in each of the graphics boards are said to belong to video stream A. Similarly, video streams Y, Y, . . . , Ygenerated by the linear chain of graphics boards, and video streams B, B, . . . , Bgenerated by the sample-to-pixel calculation units in each of the graphics boards are said to belong to video stream B.","As described above, rendering units A-D are configured to generate samples for graphics primitives, and to store the samples into sample buffer . As successive graphics primitives are rendered, a sampled virtual image accumulates in sample buffer . When the sampled virtual image is complete, i.e., when all graphics primitives comprising the virtual image have been rendered, each sample-to-pixel calculation unit CU(J) may access samples of the virtual image from sample buffer , and may filter the samples to generate pixel values. Each sample-to-pixel calculation unit CU(J) operates on samples residing in a corresponding region of the virtual screen space. The region assigned to each sample-to-pixel calculation unit CU(J) is programmed at system initialization time. Thus, the sample-to-pixel calculation units may partition the labor of transforming sample values into pixel values.","Sample-to-pixel calculation unit CU(J) may perform a spatial convolution of a portion of the sampled virtual image with respect to a convolution kernel K(x,y) to generate pixel values. For example, a red value Rfor a pixel may be computed at a location (x,y) in virtual screen space based on the relation \n\n\nwhere the summation is evaluated at samples (x,y) in the vicinity of location (x,y). Since convolution kernel K(x,y) is non-zero only in a neighborhood of the origin, the displaced kernel K(x\u2212x, y\u2212y) may take non-zero values only in a neighborhood of location (x,y).\n","The value E is a normalization value that may be computed according to the relation \n\n\nwhere the summation is evaluated for the same samples (x,y) as in the red pixel value summation above. The summation for the normalization value E may be performed in parallel with the red pixel value summation. The location (x,y) may be referred to herein as a virtual pixel center or virtual pixel origin.  shows the support  (i.e. footprint) of a circularly symmetric convolution kernel. In this case, the virtual pixel center (x,y) corresponds to the center of the support disk .\n","Similar summations to compute green, blue and alpha pixel values in terms of the green, blue and alpha sample values respectively may be performed in parallel with the red pixel value summation and the normalization constant summation.","At system initialization time, sample-to-pixel calculation unit CU(J) will have been programmed to mix (or inject) its computed pixel values into either video stream A or video stream B. For example, if sample-to-pixel calculation unit CU(J) has been assigned to video stream A, sample-to-pixel calculation unit CU(J) may mix (or inject) its computed pixel values into video stream A, and pass video stream B unmodified to the next sample-to-pixel calculation unit CU(J+1), or next graphics board. In other words, sample-to-pixel calculation unit CU(J) may mix (or replace) at least a subset of the dummy pixel values present in video stream Awith its locally computed pixel values. The resultant video stream Ais transmitted to the next sample-to-pixel calculation unit or graphics board.","In one embodiment, sample-to-pixel calculation units CU(J) may implement a super-sampled reconstruction band-pass filter to compute pixel values from samples stored in sample buffer . The support of the band-pass filter may cover a rectangular area in virtual screen space which is Mpixels high and Npixels wide. Thus, the number of samples covered by the band-pass filter is approximately equal to MNR, where Ris the number of samples per pixel region. A variety of values for M, Nand Rare contemplated. For example, in one embodiment of the band-pass filter M=N=5. It is noted that with certain sample positioning schemes (see the discussion attending , B and C), the number of samples that fall within the filter support may vary as the filter center (i.e. the virtual pixel center) is moved in the virtual screen space.","In other embodiments, sample-to-pixel calculation units CU(J) may filter a selected number of samples to calculate an output pixel value. The selected samples may be multiplied by a spatial weighting function that gives weights to samples based on their position with respect to the filter center (i.e. the virtual pixel center).","The filtering operations performed by sample-to-pixel calculation unit CU(J) may use any of a variety of filters, either alone or in combination. For example, the filtering operations may comprise convolution with a box filter, a tent filter, a cylindrical filter, a cone filter, a Gaussian filter, a Catmull-Rom filter, a Mitchell-Netravali filter, a windowed sinc filter, etc. Furthermore, the support of the filters used by sample-to-pixel calculation unit CU(J) may be circular, elliptical, rectangular (e.g. square), triangular, hexagonal, etc.","Sample-to-pixel calculation unit CU(J) may also be configured with one or more of the following features: per-pixel color correction based on spectral measurements, color look-up using pseudo color tables, direct color, inverse gamma correction, and conversion of pixels to non-linear light space. Other features of sample-to-pixel calculation unit CU(J) may include programmable video timing generators, programmable pixel clock synthesizers, cursor generators, and crossbar functions.","G. Digital-to-Analog Converters","Digital-to-analog converter (DAC) A may receive digital video stream Xfrom last sample-to-pixel calculation unit CU(V-), and convert digital video stream Xinto an analog video signal Vfor transmission to display device DD. Similarly, DAC B may receive digital video stream Yfrom last sample-to-pixel calculation unit CU(V-), and convert digital video stream Yinto an analog video signal Vfor transmission to display device DD. Digital-to-Analog Converters (DACs) A and B are collectively referred to herein as DACs . It is noted that DACs  may be disabled in all graphics boards except for the last graphics board GB(R-) which is physically coupled to display devices DDand DD. See FIG. B.","In the preferred embodiment, last sample-to-pixel calculation unit CU(V-) provides digital video stream Xto DAC A without an intervening frame buffer. Similarly, last sample-to-pixel calculation unit CU(V-) provides digital video stream Yto DAC B without an intervening frame buffer. However, in one alternative embodiment, one or more frame buffers intervene between last sample-to-pixel calculation unit CU(V-) and DAC A and\/or DAC B.","DAC A and\/or DAC B may be bypassed or omitted completely in order to output digital pixel data in lieu of analog video signals. This may be useful where display devices DDand\/or DDare based on a digital technology (e.g., an LCD projector, an LCOS projector, or a digital micro-mirror projector).","It is noted that various embodiment of graphics board GB(K) are contemplated with varying numbers of render units , varying numbers of sample-to-pixel calculation units CU. Furthermore, alternative embodiments of graphics board GB(K) are contemplated for generating more than (or less than) two simultaneous video streams.","FIGS. A-C: Super-Sampling",{"@attributes":{"id":"p-0118","num":"0120"},"figref":"FIG. 5A","b":["74","70","150"]},"Turning now to , an example of one embodiment of super-sampling is illustrated. In this embodiment, two samples are computed per rectangular region. For example, samples A and B are located in region  (denoted in cross hatch). The samples are distributed according to a regular grid. Even though the spatial density of samples is twice the spatial density of pixels, output pixel values could be computed using one sample per pixel, e.g. by throwing out all but the sample nearest to the center of each pixel. However, a number of advantages arise from computing pixel values based on multiple samples.","A support region  is superimposed over the center pixel (corresponding to the center region) of , and illustrates the support of a convolution filter. The support of a filter is the set of locations over which the filter (i.e. the filter kernel) takes non-zero values. In this example, the support region  is a circular disc. The output pixel values (e.g. red, green, blue and z values) for the corresponding pixel center are determined only by samples C and D, because these are the only samples which fall within support region . This filtering operation may advantageously improve the realism of a displayed image by smoothing abrupt edges in the displayed image (i.e., by performing anti-aliasing). The filtering operation may simply average the values of samples C and D to form the corresponding output values for the center pixel. More generally, the filtering operation may generate a weighted sum of the values of samples C and D, where the contribution of each sample is weighted according to some function of the sample's position (or distance) with respect to the center of support region . The filter, and thus support region , may be repositioned for each output pixel being calculated. For example, the filter center may visit the center of each rectangular region for which pixel values are to be computed. Other filters and filter positioning schemes are also possible and contemplated.","In the example of , there are two samples per pixel. In general, however, there is no requirement that the number of samples be related to the number of pixels. The number of samples may be completely independent of the number of pixels. For example, the number of samples may be smaller than the number of pixels. (This is the condition that defines sub-sampling).","Turning now to , another embodiment of super-sampling is illustrated. In this embodiment, the samples are positioned randomly (i.e. stochastically). Thus, the number of samples used to calculate output pixel values may vary from pixel to pixel. Render units A-D calculate color information at each sample position.","FIGS. -: Super-Sampled Sample Buffer with Real-Time Convolution",{"@attributes":{"id":"p-0123","num":"0125"},"figref":"FIG. 6","b":["350","352","352","140","150","152","154","350"]},"In addition to the vertex data, draw process  (which may be performed by rendering units A-D) also receives sample position information from a sample position memory . The sample position information defines the location of samples in virtual screen space, i.e. in the 2-D viewport. Draw process  selects the samples that fall within the polygon currently being rendered, calculates a set of values (e.g. red, green, blue, z, alpha, and\/or depth of field information) for each of these samples based on their respective positions within the polygon. For example, the z value of a sample that falls within a triangle may be interpolated from the known z values of the three vertices. Each set of computed sample values are stored into sample buffer .","In one embodiment, sample position memory  is embodied within rendering units A-D. In another embodiment, sample position memory  may be realized as part of data memories A-D, or as a separate memory.","Sample position memory  may store sample positions in terms of their virtual screen coordinates (x,y). Alternatively, sample position memory  may be configured to store only offsets dx and dy for the samples with respect to positions on a regular grid. Storing only the offsets may use less storage space than storing the entire coordinates (x,y) for each sample. The sample position information stored in sample position memory  may be read by a dedicated sample position calculation unit (not shown) and processed to calculate sample positions for graphics processing unit . More detailed information on the computation of sample positions is included below.","In another embodiment, sample position memory  may be configured to store a table of random numbers. Sample position memory  may also comprise dedicated hardware to generate one or more different types of regular grids. This hardware may be programmable. The stored random numbers may be added as offsets to the regular grid positions generated by the hardware. In one embodiment, sample position memory  may be programmable to access or \u201cunfold\u201d the random number table in a number of different ways, and thus, may deliver more apparent randomness for a given length of the random number table. Thus, a smaller table may be used without generating the visual artifacts caused by simple repetition of sample position offsets.","Sample-to-pixel calculation process  uses the same sample positions as draw process . Thus, in one embodiment, sample position memory  may generate a sequence of random offsets to compute sample positions for draw process , and may subsequently regenerate the same sequence of random offsets to compute the same sample positions for sample-to-pixel calculation process . In other words, the unfolding of the random number table may be repeatable. Thus, it may not be necessary to store sample positions at the time of their generation for draw process .","As shown in , sample position memory  may be configured to store sample offsets generated according to a number of different schemes such as a regular grid (e.g. a rectangular grid, hexagonal grid, etc.), a perturbed regular grid, or a random (stochastic) distribution. Graphics board GB(K) may receive an indication from the operating system, device driver, or the geometry data  that indicates which type of sample positioning scheme is to be used. Thus, sample position memory  may be configurable or programmable to generate position information according to one or more different schemes.","In one embodiment, sample position memory  may comprise a RAM\/ROM that contains stochastically determined sample points or sample offsets. Thus, the density of samples in virtual screen space may not be uniform when observed at small scale. Two bins (i.e. regions) with equal area centered at different locations in virtual screen space may contain different numbers of samples.","An array of bins may be superimposed over the 2-D viewport  of , and the storage of samples in sample buffer  may be organized in terms of bins. Sample buffer  may comprise an array of memory blocks which correspond to the bins. Each memory block may store the sample values (e.g. red, green, blue, z, alpha, etc.) for the samples that fall within the corresponding bin. (See the exploded view of Bin #I in ) The approximate location of a sample is given by the bin in which it resides. The memory blocks may have addresses which are easily computable from the corresponding bin locations in virtual screen space, and vice versa. Thus, the use of bins may simplify the storage and access of sample values in sample buffer .","Suppose (for the sake of discussion) that the 2-D viewport  ranges from (0000,0000) to (FFFF,FFFF) in hexadecimal virtual screen coordinates. Also suppose that 2-D viewport  is overlaid with a rectangular array of bins whose lower-left corners reside at the locations (XX,YY) where XX and YY independently run from 0\u00d700 to 0\u00d7FF. Thus, there are 256 bins in each of the vertical and horizontal directions with each bin spanning a square in virtual screen space with side length of 256. Suppose that each memory block is configured to store sample values for up to 16 samples, and that the set of sample values for each sample comprises 4 bytes. In this case, the address of the memory block corresponding to the bin located at (XX,YY) may be simply computed by the relation BinAddr=(XX+YY*256)*16*4. For example, the sample SMP=(1C3B,23A7) resides in the bin located at (1C00,2300). The sample value set for sample SMP is then stored in the memory block residing at address 0\u00d78C700=(0\u00d7231C)(0\u00d740) in sample buffer .","The bins may tile the 2-D viewport in a regular array, e.g. in a square array, rectangular array, triangular array, hexagonal array, etc., or in an irregular array. Bins may occur in a variety of sizes and shapes. The sizes and shapes may be programmable. The maximum number of samples that may populate a bin is determined by the storage space allocated to the corresponding memory block. This maximum number of samples is referred to herein as the bin sample capacity, or simply, the bin capacity. The bin capacity may take any of a variety of values. The bin capacity value may be programmable. Henceforth, the memory blocks in sample buffer  which correspond to the bins in virtual screen space will be referred to as memory bins.","The specific position of each sample within a bin may be determined by looking up the sample's offset in the RAM\/ROM table, i.e., the sample's offset with respect to the bin position (e.g. the lower-left corner or center of the bin, etc.). However, depending upon the implementation, not all choices for the bin capacity may have a unique set of offsets stored in the RAM\/ROM table. Offsets for a first bin capacity value may be determined by accessing a subset of the offsets stored for a second larger bin capacity value. In one embodiment, each bin capacity value supports at least four different sample positioning schemes. The use of different sample positioning schemes may reduce final image artifacts that would arise in a scheme of naively repeating sample positions.","In one embodiment, sample position memory  may store pairs of 8-bit numbers, each pair comprising an x-offset and a y-offset. (Other offsets are also possible, e.g., a time offset, a z-offset, etc. In addition, bit lengths other an eight are contemplated.) When added to a bin position, each pair defines a particular position in virtual screen space, i.e. in 2-D viewport . To improve read access times, sample position memory  may be constructed in a wide\/parallel manner so as to allow the memory to output more than one sample location per read cycle.","Once the sample positions have been read from sample position memory , draw process  selects the samples that fall within the polygon currently being rendered. Draw process  then calculates the z and color information (which may include alpha or other depth of field information values) for each of these samples and stores the data into sample buffer . In one embodiment, sample buffer  may only single-buffer z values (and perhaps alpha values) while double-buffering other sample components such as color. Unlike prior art systems, graphics system  may use double-buffering for all samples (although not all components of samples may be double-buffered, i.e., the samples may have some components that are not double-buffered). In one embodiment, the samples are stored into sample buffer  in bins. In some embodiments, the bin capacity may vary from frame to frame. In addition, the bin capacity may vary spatially for bins within a single frame rendered into sample buffer . For example, bins on the edge of 2-D viewport  may have a smaller bin capacity than bins corresponding to the center of 2-D viewport . Since viewers are likely to focus their attention mostly on the center of a displayed image, more processing bandwidth may be dedicated to providing enhanced image quality in the center of 2-D viewport . Note that the size and shape of bins may also vary from region to region, or from frame to frame. The use of bins will be described in greater detail below in connection with FIG. .","Filter process  represents the action of sample-to-pixel calculation units CU in generating digital video streams Xand Ywhich are transmitted to the next graphics board GB(K+1), or converted into video signals Vand Vfor presentation to display devices DDand DD. Thus, any description of sample-to-pixel calculation units CU may be interpreted as a description of filter process . Filter process  operates in parallel with draw process .","Generic sample-to-pixel calculation unit CU(J) may be configured to (a) read sample positions from sample position memory , (b) read corresponding sample values from sample buffer , (c) filter the sample values, and (d) mix (or inject) the resulting pixel values into video stream A or B. Sample-to-pixel calculation unit CU(J) may generate the red, green, blue and alpha values for an output pixel based on a spatial filtering of the corresponding data for a selected plurality of samples, e.g. samples falling in a neighborhood of a pixel center. In one embodiment, sample-to-pixel calculation unit CU(J) may be configured to: (i) determine the distance of each sample from the pixel center; (ii) multiply each sample's attribute values (e.g., red, green, blue, alpha) by a filter weight that is a specific (programmable) function of the sample's distance; (iii) generate sums of the weighted attribute values, one sum per attribute (e.g. a sum for red, a sum for green, . . . ), and (iv) normalize the sums to generate the corresponding pixel attribute values.","In the embodiment just described, the filter kernel is a function of distance from the pixel center. However, in alternative embodiments, the filter kernel may be a more general function of x and y displacements from the pixel center. Also, the support of the filter, i.e. the 2-D neighborhood over which the filter kernel takes non-zero values, may not be a circular disk. Any sample falling within the support of the filter kernel may affect the output pixel value being computed.",{"@attributes":{"id":"p-0140","num":"0142"},"figref":"FIG. 7","b":["354","354","354","354","354","354","354","354","352","354","360","354","354","354","352","354","360","354","354","352","360"]},"Yet another alternative embodiment may store tags with the sample values in super-sampled sample buffer . These tags may be used to look-up the offsets (i.e. perturbations) dx and dy associated with each particular sample.",{"@attributes":{"id":"h-0009","num":"0000"},"figref":"FIGS. 8A","b":["8","8"]},{"@attributes":{"id":"p-0142","num":"0144"},"figref":"FIG. 8A","b":"190"},"In the perturbed regular positioning scheme , sample positions are defined in terms of perturbations from a set of fixed positions on a regular grid or tiling. In one embodiment, the samples may be displaced from their corresponding fixed grid positions by random x and y offsets, or by random angles (ranging from 0 to 360 degrees) and random radii (ranging from zero to a maximum radius). The offsets may be generated in a number of ways, e.g. by hardware based upon a small number of seeds, by reading a table of stored offsets, or by using a pseudo-random function. Once again, perturbed regular grid scheme  may be based on any type of regular grid or tiling. Samples generated by perturbation with respect to a grid or hexagonal tiling may particularly desirable due to the geometric properties of these configurations.","Stochastic sample positioning scheme  represents a third potential type of scheme for positioning samples. Stochastic sample positioning involves randomly distributing the samples across the 2-D viewport. Random positioning of samples may be accomplished through a number of different methods, e.g., using a random number generator such as an internal clock to generate pseudo-random numbers. Random numbers or positions may also be pre-calculated and stored in memory.","Turning now to , details of one embodiment of perturbed regular positioning scheme  are shown. In this embodiment, samples are randomly offset from a regular square grid by x- and y-offsets. As the enlarged area shows, sample  has an x-offset  that specifies its horizontal displacement from its corresponding grid intersection point . Similarly, sample  also has a y-offset  that specifies its vertical displacement from grid intersection point . The random x-offset  and y-offset  may be limited to a particular range of values. For example, the x-offset may be limited to the range from zero to x, where xis the width of the a grid rectangle. Similarly, the y-offset may be limited to the range from zero to y, where yis the height of a grid rectangle. The random offset may also be specified by an angle and radius with respect to the grid intersection point .",{"@attributes":{"id":"p-0146","num":"0148"},"figref":"FIG. 8C","b":["192","138","198","124","126","132","138","354"]},"FIG. \u2014Converting Samples Into Pixels","As discussed earlier, 2-D viewport  may be covered with an array of spatial bins. Each spatial bin may be populated with samples whose positions are determined by sample position memory . Each spatial bin corresponds to a memory bin in sample buffer . A memory bin stores the sample values (e.g. red, green, blue, z, alpha, etc.) for the samples that reside in the corresponding spatial bin. Sample-to-pixel calculation units CU are configured to read memory bins from sample buffer  and to convert sample values contained within the memory bins into pixel values.",{"@attributes":{"id":"p-0148","num":"0150"},"figref":"FIG. 9","b":["420","0","1","2","3","0","3","1","1","162"]},{"@attributes":{"id":"p-0149","num":"0151"},"figref":["FIG. 9","FIG. 9"]},"The amount of the overlap between columns may depend upon the horizontal diameter of the filter support for the filter kernel being used. The example shown in  illustrates an overlap of two bins. Each square (such as square ) represents a single bin comprising one or more samples. Advantageously, this configuration may allow sample-to-pixel calculation units CU to work independently and in parallel, with each sample-to-pixel calculation units CU(J) receiving and convolving samples residing in the memory bins of the corresponding column. Overlapping the columns will prevent visual bands or other artifacts from appearing at the column boundaries for any operators larger than a pixel in extent.","Furthermore, the embodiment of  may include a plurality of bin caches  which couple to sample buffer . In addition, each of bin caches  couples to a corresponding one of sample-to-pixel calculation units CU. Bin cache -I (where I takes any value from zero to three) stores a collection of memory bins from Column I, and serves as a cache for sample-to-pixel calculation unit CU(I). Bin cache -I may have an optimized coupling to sample buffer  which facilitates access to the memory bins for Column I. Since the convolution calculation for two adjacent convolution centers may involve many of the same memory bins, bin caches  may increase the overall access bandwidth to sample buffer .",{"@attributes":{"id":"p-0152","num":"0154"},"figref":"FIG. 10A","b":["162","400","406","400","176","402","404","176","400","400"],"sub":["L ","L ","v ","L ","v "]},"After completing convolution computations at a convolution center, convolution filter kernel  shifts to the next convolution center. Kernel  may be visualized as proceeding horizontally within Column I in the direction indicated by arrow . When kernel  reaches the right boundary  of Column I, it may shift down one or more bin rows, and then, proceed horizontally starting from the left column boundary . Thus the convolution operation proceeds in a scan line fashion, generating successive rows of output pixels for display.","In one embodiment, the cache line-depth parameter Nis set equal to N+1. In the example of , the filter support covers N=5 bins vertically. Thus, the cache line-depth parameter N=6=5+1. The additional bin row in bin cache -I allows the processing of memory bins (accessed from bin cache -I) to be more substantially out of synchronization with the loading of memory bins (into bin cache -I) than if the cache line-depth parameter Nwere set at the theoretical minimum value N.","In one embodiment, sample buffer  and bin cache -I may be configured for row-oriented burst transfers. If a request for a memory bin misses in bin cache -I, the entire bin row containing the requested memory bin may be fetched from sample buffer  in a burst transfer. Thus, the first convolution of a scan line may fill the bin cache -I with all the memory bins necessary for all subsequent convolutions in the scan line. For example, in performing the first convolution in the current scan line at the first convolution center , sample-to-pixel calculation unit CU(I) may assert a series of requests for memory bins, i.e. for the memory bins corresponding to those spatial bins (rendered in shade) which intersect the support of filter kernel . Because the filter support  intersects five bin rows, in a worst case scenario, five of these memory bin requests will miss bin cache -I and induce loading of all five bin rows from sample buffer . Thus, after the first convolution of the current scan line is complete, bin cache -I may contain the memory bins indicated by the heavily outlined rectangle . Memory bin requests asserted by all subsequent convolutions in the current scan line may hit in bin cache -I, and thus, may experience significantly decreased bin access time.","In general, the first convolution in a given scan line may experience fewer than the worst case number of misses to bin cache -I because bin cache -I may already contain some or all of the bin rows necessary for the current scan line. For example, if convolution centers are located at the center of each spatial bin, the vertical distance between successive scan lines (of convolution centers) corresponds to the distance between successive bin rows, and thus, the first convolution of a scan line may induce loading of a single bin row, the remaining four bin rows having already been loaded in bin cache -I in response to convolutions in previous scan lines.","If the successive convolution centers in a scan line are expected to depart from a purely horizontal trajectory across Column I, the cache line-depth parameter Nmay be set to accommodate the maximum expected vertical deviation of the convolution centers. For example, in , the convolution centers follow a curved path across Column I. The curved path deviates from a horizontal path by approximately two bins vertically. Since the support of the filter kernel covers a 3 by 3 array of spatial bins, bin cache -I may advantageously have a cache line-depth Nof at least five (i.e. two plus three).","As mentioned above, Columns  through  of 2-D viewport  may be configured to overlap horizontally. The size of the overlap between adjacent Columns may be configured to accommodate the maximum expected horizontal deviation of convolution centers from nominal convolution centers on a rectangular grid.","FIG. \u2014Rendering Samples Into a Super-Sampled Sample Buffer",{"@attributes":{"id":"p-0159","num":"0161"},"figref":["FIG. 10","FIG. 11"],"b":["200","102","106","202","150","204","150","150","206","208"]},"If graphics board GB(K) implements variable resolution super-sampling, then the triangles are compared with a set of sample-density region boundaries (step B). In variable-resolution super-sampling, different regions of 2-D viewport  may be allocated different sample densities based upon a number of factors (e.g., the center of the attention of an observer as determined by eye or head tracking). If the triangle crosses a sample-density region boundary (step ), then the triangle may be divided into two smaller polygons along the region boundary (step ). The polygons may be further subdivided into triangles if necessary (since the generic slicing of a triangle gives a triangle and a quadrilateral). Thus, each newly formed triangle may be assigned a single sample density. In one embodiment, graphics board GB(K) may be configured to render the original triangle twice, i.e. once with each sample density, and then, to clip the two versions to fit into the two respective sample density regions.","In step , one of the sample positioning schemes (e.g., regular, perturbed regular, or stochastic) is selected from sample position memory . The sample positioning scheme will generally have been pre-programmed into the sample position memory , but may also be selected \u201con the fly\u201d. In step , rendering units A-D may determine which spatial bins contain samples located within the triangle's boundaries, based upon the selected sample positioning scheme and the size and shape of the spatial bins. In step , the offsets dx and dy for the samples within these spatial bins are then read from sample position memory . In step , each sample's position is then calculated using the offsets dx and dy and the coordinates of the corresponding bin origin, and is compared with the triangle's edges to determine if the sample is within the triangle.","For each sample that is determined to be within the triangle, one of rendering unit A-D draws the sample by calculating the sample's color, alpha and other attributes. This may involve a lighting calculation and an interpolation based upon the color and texture map information associated with the vertices of the triangle. Once the sample is rendered, it may be forwarded to schedule unit , which then stores the sample in sample buffer  (as indicated in step ).","Note the embodiment of the rendering method described above is used for explanatory purposes only and is not meant to be limiting. For example, in some embodiments, the steps shown in  as occurring serially may be implemented in parallel. Furthermore, some steps may be reduced or eliminated in certain embodiments of the graphics system (e.g., steps - in embodiments that do not implement geometry compression, or steps - in embodiments that do not implement a variable resolution super-sampled sample buffer).","FIG. \u2014Generating Output Pixel Values from Sample Values",{"@attributes":{"id":"p-0164","num":"0166"},"figref":["FIG. 12","FIG. 10A"],"b":["162","250","162","252","176","254","400"]},"Each sample in the selected bins (i.e. bins that have been identified in step ) is then individually examined to determine if the sample does indeed contribute (as indicated in steps -) to the current output pixel. This determination may be based upon the distance of the sample from the filter center.","In one embodiment, sample-to-pixel calculation units CU may be configured to calculate this sample distance (i.e., the distance of the sample from the filter center) and then use it to index into a table storing filter weight values (as indicated in step ). In another embodiment, however, the potentially expensive calculation for determining the distance from the center of the pixel to the sample (which typically involves a square root function) may be avoided by using distance squared to index into the table of filter weights. In one embodiment, this squared-distance indexing scheme may be facilitated by using a floating point format for the distance (e.g., four or five bits of mantissa and three bits of exponent), thereby allowing much of the accuracy to be maintained while compensating for the increased range in values. In one embodiment, the table of filter weights may be implemented in ROM. However, RAM tables may also be used. Advantageously, RAM tables may, in some embodiments, allow sample-to-pixel calculation unit CU(J) to vary the filter coefficients on a per-frame or per-session basis. For example, the filter coefficients may be varied to compensate for known shortcomings of the display devices or for the user's personal preferences.","The filter coefficients may also vary as a function of filter center position within the 2-D viewport , or on a per-output pixel basis. In one embodiment, specialized hardware (e.g., multipliers and adders) may be used to compute filter weights for each sample. Samples which fall outside the support of filter kernel  may be assigned a filter weight of zero (step ), or they may be removed from the calculation entirely.","In one alternative embodiment, the filter kernel may not be expressible as a function of distance with respect to the filter center. For example, a pyramidal tent filter is not expressible as a function of distance from the filter center. Thus, filter weights may be tabulated (or computed) in terms of x and y sample-displacements with respect to the filter center.","Once the filter weight for a sample has been determined, the attribute values (e.g. red, green, blue, alpha, etc.) for the sample may then be multiplied by the filter weight (as indicated in step ). Each of the weighted attribute values may then be added to a corresponding cumulative sum\u2014one cumulative sum for each attribute\u2014as indicated in step . The filter weight itself may be added to a cumulative sum of filter weights (as indicated in step ). Step  may be performed in parallel with step  and\/or .","After all samples residing in the support of the filter have been processed, the cumulative sums of the weighted attribute values may be divided by the cumulative sum of filter weights (as indicated in step ) to generate pixel attributes values including pixel color values. It is noted that the number of samples which fall within the filter support may vary as the filter center moves within the 2-D viewport. The normalization step  compensates for the variable gain which is introduced by this nonuniformity in the number of included samples, and thus, prevents the computed pixel color values from appearing too bright or too dark due to the sample number variation.","In step , the pixel color values may be modified to compensate for errors in color presentation. For example, the color presented by display devices DDand\/or DDmay have an non-uniform appearance because of non-uniformities in (a) the color filters of the display devices, (b) the ambient light illuminating the display\/projection screen(s), (c) the color and\/or material properties of the display\/projection screen(s), etc. In step , the corrected pixel color values may be gamma corrected, and mixed (or injected) into video stream A or video stream B.","FIG. \u2014Example Output Pixel Convolution",{"@attributes":{"id":"p-0172","num":"0174"},"figref":"FIG. 13","b":["288","288","288","296","294","292","290","290"]},"Example attribute values for samples - are illustrated in boxes -. In this example, each sample comprises red, green, blue and alpha values, in addition to the sample's positional data. Block  illustrates the calculation of each pixel attribute value prior to normalization. As previously noted, the filter values may be summed to obtain a normalization value . Normalization value  is used to divide out the unwanted gain arising from the non-constancy of the number of samples captured by the filter support. Block  illustrates the normalization process and the final normalized pixel attribute values.","The filter presented in  has been chosen for descriptive purposes only and is not meant to be limiting. A wide variety of filters may be used for pixel value computations depending upon the desired filtering effect(s). It is a well known fact that the sinc filter realizes an ideal band-pass filter. However, the sinc filter takes non-zero values over the whole of the x-y plane. Thus, various windowed approximations of the sinc filter have been developed. Some of these approximations such as the cone filter or Gaussian filter approximate only the central lobe of the sinc filter, and thus, achieve a smoothing effect on the sampled image. Better approximations such as the Mitchell-Netravali filter (including the Catmull-Romm filter as a special case) are obtained by approximating some of the negative lobes and positive lobes which surround the central positive lobe of the sinc filter. The negative lobes allow a filter to more effectively retain spatial frequencies up to the cutoff frequency and reject spatial frequencies beyond the cutoff frequency. A negative lobe is a portion of a filter where the filter values are negative. Thus, some of the samples residing in the support of a filter may be assigned negative filter values (i.e. filter weights).","A wide variety of filters may be used for the pixel value convolutions including filters such as a box filter, a tent filter, a cylinder filter, a cone filter, a Gaussian filter, a Catmull-Rom filter, a Mitchell-Netravali filter, any windowed approximation of a sinc filter, etc. Furthermore, the support of the filters used for the pixel value convolutions may be circular, elliptical, rectangular (e.g. square), triangular, hexagonal, etc.","The piecewise constant filter function shown in  with four constant regions is not meant to be limiting. For example, in one embodiment the convolution filter may have a large number of regions each with an assigned filter value (which may be positive, negative and\/or zero). In another embodiment, the convolution filter may be a continuous function that is evaluated for each sample based on the sample's distance (or x and y displacements) from the pixel center. Also note that floating point values may be used for increased precision.","Color Correction System and Method","As described above, display devices generate color by mixing varying amounts of Q fundamental colors. Various sets of fundamental colors are contemplated. However, typically, Q equals three and the fundamental colors are red, green and blue. Thus, a first pixel light beam radiated to an observer's eye may comprise a red component beam, a green component beam and a blue component beam having power spectra \u0192(\u03bb), \u0192(\u03bb) and \u0192(\u03bb) respectively. Therefore, the power spectrum \u0192(\u03bb) of the first pixel beam may be a linear combination of the three color component spectra: \n\n\nwhere the scalar values \u03b3, \u03b3and \u03b3control the relative amounts of red, green and blue respectively which are combined in the first pixel light beam. Let \u03b3 denote the vector whose components are the scalar values \u03b3, \u03b3and \u03b3, i.e.\n\n\u03b3=[\u03b3, \u03b3, \u03b3].\n\nThe superscript t denotes vector transpose. The vector \u03b3 may be referred to herein as the color intensity vector.\n","Let g(\u03bb), g(\u03bb) and g(\u03bb) represent the red, green and blue spectral response curves for a human eye. According to one model, the human brain perceives a color in response to the pixel power spectrum F(\u03bb) based on the three numbers\n\n(\u03bb)(\u03bb)\u2003\u2003(2A)\n\n(\u03bb)(\u03bb)\u2003\u2003(2B)\n\n(\u03bb)(\u03bb)\u2003\u2003(2C)\n\nwhere the integrals are evaluated over the range of visible wavelengths. In other words, the vector G=[G, G,G]determines the perceived color. The vector G is referred to herein as the first perceptual color vector.=\n","Suppose that a second pixel light beam radiated to the observer's eye has color component power spectra {circumflex over (\u0192)}(\u03bb), {circumflex over (\u0192)}(\u03bb) and {circumflex over (\u0192)}(\u03bb) respectively. Thus, the power spectrum for the second pixel beam is given by \n\n\nwhere scalar values {circumflex over (\u03b3)}, {circumflex over (\u03b3)}and {circumflex over (\u03b3)}control the amount of red, green and blue which are mixed to form the second pixel light beam. The second pixel light beam induces a second perceptual color vector \u011c=[\u011c\u011c\u011c]given by\n\n(\u03bb)(\u03bb)\u2003\u2003(4A)\n\n(\u03bb)(\u03bb)\u2003\u2003(4B)\n\n(\u03bb)(\u03bb)\u2003\u2003(4C)\n\nThe first pixel light beam and second pixel light beam may induce the same perceived color in the observer when the first and second perceptual color vectors are equal (or approximately equal), i.e. \n\n\nfor j=1,2,3. Defining\n\n\u0192 and\u2003\u2003(8)\n\n\u2003{circumflex over (\u0192)}\u2003\u2003(9)\n\nas components of the 3\u00d73 square matrices L and M respectively, linear system (7) may be expressed as a matrix-vector equation:\n\nL\u03b3=M{circumflex over (\u03b3)}.\u2003\u2003(10)\n\nSolving for the second color intensity vector in terms of the first color intensity vector gives\n\n{circumflex over (\u03b3)}=ML\u03b3\u2003\u2003(11)\n\nwhere the superscript \u22121 denotes matrix inversion. The matrix L may be referred to as the unperturbed perceptual matrix. The matrix M may be referred to as the perturbed perceptual matrix.\n","The first pixel light beam may represent a theoretical standard, i.e. the color component power spectra \u0192(\u03bb), \u0192(\u03bb) and \u0192(\u03bb) of the first light beam may be interpreted as ideal power spectra generated by an ideal display device. For example, the color component power spectra \u0192(\u03bb), \u0192(\u03bb) and \u0192(\u03bb) may be assumed to be equal to industry standardized spectra for red, green and blue respectively. In contrast, the second pixel light beam may be the energy radiated by a given pixel of an actual display device coupled to graphics system . The color component power spectra {circumflex over (\u0192)}(\u03bb), {circumflex over (\u0192)}(\u03bb) and {circumflex over (\u0192)}(\u03bb) of the second light beam may be measured with an appropriate spectrum sensing device such as a spectroradiometer. Thus, equation (11) may be interpreted as an explanation of how to generate a desired color perception with the actual display device. A sample-to-pixel calculation unit in graphics system  may compute an initial color, i.e. initial color intensity vector \u03b3=[\u03b3, \u03b3, \u03b3], for the given pixel, and may apply the correction indicated by equation (11) to compute a corrected color intensity vector {circumflex over (\u03b3)} which is then provided to the actual display device (e.g. embedded in an analog video signal). In response to the corrected color intensity vector, the actual display device generates the second pixel light beam at the given pixel. Because of the color correction applied by the sample-to-pixel calculation unit, the second pixel light beam radiated from the given pixel induces the same (or approximately the same) color perception as would the first \u201cideal\u201d pixel light beam generated by an ideal display device driven with the initial color vector \u03b3.","The actual color component power spectra {circumflex over (\u0192)}(\u03bb), {circumflex over (\u0192)}(\u03bb) {circumflex over (\u0192)}(\u03bb) radiated by the given pixel may be measured, and used to compute perturbed perceptual matrix M. The ideal color component power spectra may be used to compute the unperturbed perceptual matrix L. The matrix product P=ML may be computed and stored in a memory accessible to the sample-to-pixel calculation unit. The sample-to-pixel calculation unit may compute the corrected color intensity vector {circumflex over (\u03b3)} by performing the matrix-vector multiplication P\u03b3 in real-time. The matrix P is referred to herein as the correction matrix. The sample-to-pixel calculation unit may comprise dedicated logic (e.g. a network of multipliers and adders) or a generic programmable processor (e.g. a DSP core) to perform the matrix-vector multiplication.","Alternatively, the first pixel light beam may also be interpreted as the light beam generated by an actual display device. Thus, the color component power spectra \u0192(\u03bb), \u0192(\u03bb) and \u0192(\u03bb) of the first light beam may also be measured spectra. In this case, equation (11) may be interpreted as an explanation of how to generate a consistent color perception between the first light beam and second light beam.","In general, a display device generates a pixel array on a display surface such as projection screen SCR or the display screen of a CRT or LCD display. For the sake of discussion, suppose that the pixel array is parameterized by a horizontal pixel index I and a vertical pixel index J. The sample-to-pixel calculation unit(s) assigned to the display device may apply a per-pixel color correction for the pixel array. For each pixel (I,J) in the pixel array, a corresponding correction matrix P(I,J) may be used to perform color correction. In other words, after computing a color intensity vector \u03b3 for pixel (I,J), a sample-to-pixel calculation unit may multiply the color intensity vector \u03b3 by the corresponding color correction matrix P(I,J) to determine a corrected color intensity vector {circumflex over (\u03b3)}. The corrected color intensity vector may then be transmitted to the display device. Because of the per-pixel color correction, color images presented through the display device may be more consistent and true to some standardized color.","In one embodiment, the measurement of color component spectra {circumflex over (\u0192)}(\u03bb), {circumflex over (\u0192)}(\u03bb) and {circumflex over (\u0192)}(\u03bb) and computation of correction matrix P may be performed at the four corners of the pixel array. These four comer correction matrices may be transmitted to and stored in the sample-to-pixel calculation unit(s) assigned to the display device. A sample-to-pixel calculation unit may interpolate correction matrix P(I,J) for each pixel (I,J) from the four comer correction matrices. The interpolation may be performed in real-time. Sample-to-pixel calculation units may include dedicated hardware (such as multipliers, adders, etc.) to speed the interpolation computation.","In a second embodiment, the measurement of color component spectra {circumflex over (\u0192)}(\u03bb), {circumflex over (\u0192)}(\u03bb) and {circumflex over (\u0192)}(\u03bb) and computation of correction matrix P may be performed for each pixel in a grid which covers (i.e. spans) the pixel array. For example,  shows a grid of pixels in a pixel array. The grid pixels are shown in cross hatch. Each sample-to-pixel calculation unit assigned to the display device may store the correction matrices P for at least a portion of the grid pixels. For example, each sample-to-pixel calculation unit may be assigned a rectangular section of the pixel array, and thus, may store the correction matrices for that portion of the pixel grid which covers the rectangular section.","A sample-to-pixel calculation unit may interpolate a correction matrix P(I,J) for a pixel (I,J) based on the correction matrices at nearest neighbor grid pixels. The interpolation may be performed in real-time. It is noted that the four-by-four pixel grid size of  is not meant to be limiting. The pixel grid may have any number NR of grid rows and any number NC of grid columns.","In one embodiment, the number NR of grid rows and the number NC of grid columns may be chosen based on the spatial rate of change of the color component spectra {circumflex over (\u0192)}(\u03bb), {circumflex over (\u0192)}(\u03bb) and {circumflex over (\u0192)}(\u03bb) across the pixel array. Large values for NR and\/or NC may be required when the spatial rate of change of the color component spectra is large. In one embodiment, NC=NR=16.","If the color component power spectra {circumflex over (\u0192)}(\u03bb), {circumflex over (\u0192)}(\u03bb) and {circumflex over (\u0192)}(\u03bb) of pixels do not change substantially across the pixel array, one set of color component power spectra {circumflex over (\u0192)}(\u03bb), {circumflex over (\u0192)}(\u03bb) and {circumflex over (\u0192)}(\u03bb) may be used to characterize the whole pixel array or any portion thereof. For example, the color component power spectra for one particular pixel (e.g. a pixel centrally located in the pixel array) may be used as the characterizing set. Alternatively, the color component spectra for a collection of pixels may be averaged to obtain a characterizing set. For example, the corresponding color component power spectra for the four corner pixels of the pixel array may be averaged. The correction matrix P=ML computed from the characterizing set based on equations (8) and (9) may be used to correct every pixel (I,J) of the pixel array (or pixels in some subset of the pixel array). In other words, the sample-to-pixel calculation unit(s) assigned to the display device may use the same correction matrix P to correct pixel color values for each pixel (I,J) in the pixel array (or some subset of the pixel array).","As mentioned above, graphics system  may be used with a plurality of display devices DD, through DD. Because there may be noticeable differences in color presentation between distinct display devices (even when they are of the same model and from the same manufacturer), the above process of measuring color component spectra {circumflex over (\u0192)}(\u03bb), {circumflex over (\u0192)}(\u03bb) and {circumflex over (\u0192)}(\u03bb) and computing correction matrices P on a pixel grid may be repeated for each display device to be used with graphics system . A group of one or more sample-to-pixel calculation units may be assigned to each of the display devices DD, through DD. Each group performs color correction as described above for the corresponding display device.","In order to correct for time variation in color presentation, the above process for estimating a grid of correction matrices for a display device at a particular time (or in a particular time interval) may be repeated periodically or intermittently, or in response to user request.","It is noted that the model of color perception suggested by expressions (2A), (2B) and (2C) may be replaced by or augmented with any desired color perception model. In general, a color perception model may be interpreted as a method for mapping an arbitrary power spectrum for a light beam to a space of perceived colors. Thus, given a color intensity vector \u03b3, the color perception model specifies a perceived color C(F) generated by the first pixel spectrum \n\n\nof the first light beam. Similarly, the color perception model also specifies the perceived color C({circumflex over (F)}) generated by the second pixel spectrum \n\n\nof the second light beam in response to color intensity vector \u03bd. For optimal color correction, it is desirable to choose a color intensity vector \u03bd={circumflex over (\u03b3)} which minimizes the difference between the ideal color C(F) and the actual color C({circumflex over (F)}). An optimal color correction transformation for a given pixel maps an arbitrary color intensity vector \u03b3 to the corresponding minimizing vector {circumflex over (\u03b3)}.\n","As described above, graphics system  may comprise one or more graphics boards which generate video signals for display devices DD, through DDin response to graphics data received from system bus . In the preferred embodiment, the sample-to-pixel calculation units CU comprised within the one or more graphics boards are configured to modify pixel color values (e.g. red, green and blue color values) to compensate for errors in the color presented by the display devices.","A spectrum sensing device SSD may be configured to measure the visible power spectrum of pixels generated by display devices DD, through DDor any subset of the display devices. For example, the spectrum sensing device SSD may measure the color component power spectra of pixels generated on screen SCR by projection devices PDand PDas shown in FIG. . The spectrum sensing device SSD may comprise a spectroradiometer. In one embodiment, the spectrum sensing device SSD comprises a lens to spatially separate wavelength components of an impinging light beam, and an array of phototransistors to measure the power in each wavelength component. A plurality of spectrum sensing devices may be used to measure the color component power spectra for the display devices DD, through DD, each spectrum sensing device performing power spectrum measurements for a corresponding subset of the display devices.","The spectrum sensing device SSD may be pointed at and\/or focused on a display surface such as projection screen SCR or the screen surface of monitors A and\/or B (as shown in FIG. ). The spectrum sensing device is configured to measure the radiated power spectrum for pixels or groups of pixels appearing on the display surface. In one alternative embodiment, the spectrum sensing device may be focused at the apparent source of radiated pixels. For example, the spectrum sensing device may \u201clook\u201d into the radiating lens of a projection device.","The spectrum sensing device may measure the color component power spectra {circumflex over (\u0192)}(\u03bb), {circumflex over (\u0192)}(\u03bb) and {circumflex over (\u0192)}(\u03bb) for a given pixel (or group of pixels) radiated from the display surface. Graphics system , i.e. one of the sample-to-pixel calculation units in one of the graphics boards, may drive the given pixel with:\n\n","A color calibration processor  may assert control signals which coordinate the display of the red, green and blue intensity patterns and the measurement of color component power spectra {circumflex over (\u0192)}(\u03bb), {circumflex over (\u0192)}(\u03bb) and {circumflex over (\u0192)}(\u03bb) respectively. For each color component i=1,2,3 of the given pixel, the spectrum sensing device SSD may measure the color component power spectrum {circumflex over (\u0192)}(\u03bb) at N distinct wavelengths as suggested by FIG. . Thus, spectrum sensing device SSD may generate a vector of length N for each color component power spectrum. Color calibration processor  may receive the color component power spectrum vectors, and compute a set of parameters for a color correction transformation. The color correction transformation may approximate the optimal color correction transformation as defined above.","In one embodiment, the set of parameters computed by color calibration processor  comprises a lookup table. For example, if the input color vector \u03b3 comprises a 24 bit word (i.e. 8 bits each for red, green and blue), a lookup table with 2entries may be used to exhaustively characterize the color correction transformation. The input color vector may be used as an address into the lookup table. The table entry addressed by the input color vector may contain the corresponding corrected color intensity vector {circumflex over (\u03b3)}, or alternatively, the difference {circumflex over (\u03b3)}\u2212\u03b3. The difference may have a smaller magnitude on average, and thus, may lead to a smaller table size. It is noted that the word size of 24 bits is given for illustrative purposes only, and a wide of variety of sizes are contemplated.","In a second embodiment, a non-exhaustive lookup table may be used. For example, a lookup table with 2entries may be addressed with the 6 high order bits of R, G and B in the input color intensity vector. Interpolation may be applied to obtain corrected color intensity vectors with higher precision.","In the embodiment shown in , color calibration processor  resides outside of graphics system . In a second embodiment, color calibration processor is situated within graphics system . In a third embodiment, the functions performed by color calibration processor  are distributed to some or all of the graphics boards comprising graphics system . For example, each graphics board GB(K) may have its own local color calibration processor. In a fourth embodiment, host CPU  performs the functions of color calibration processor , and thus, color calibration processor  may be omitted.","In one embodiment, the set of transformation parameters computed by color calibration processor  comprises the correction matrix P=ML described above. Thus, color calibration processor  may compute the perturbed perceptual matrix M in response to the measured color component power spectrum vectors of the given pixel, and multiply the inverse of matrix M with the unperturbed perceptual matrix L. The components of perturbed perceptual matrix M may be computed according to a discretized form of expression (9), e.g. \n\n\nThe color calibration processor may similarly compute the components of the unperturbed perceptual matrix L according to the expression \n\n\nThe values {circumflex over (\u0192)}(\u03bb)of the unperturbed color component spectra are known by industry standard definition, or perhaps by measurements obtained with the spectrum sensing device SSD.\n","In some embodiments, the color correction transformation may be nonlinear, and thus, the set of parameters which characterize the color correction transformation may not be a matrix. A variety of forms for the set of parameters are contemplated.","After computing the set of transformation parameters for a given pixel, color calibration processor  may transfer the parameter set to one of the graphics boards comprising graphics system . In one embodiment, color calibration processor  more specifically targets a sample-to-pixel calculation unit which has been designated to compute pixel color values for the given pixel. The transfer may occur through control\/data bus . In response to computing pixel color values (e.g. RGB values) for the given pixel, the sample-to-pixel calculation unit applies a color correction transformation to the computed pixel color values based on the set of transformation parameters. The corrected pixel color values are transmitted to the display device which generates the given pixel.","As mentioned above, the set of transformation parameters may comprise a lookup table. In this case, color calibration processor  may transfer the lookup table to a memory resident within a graphics board. A sample-to-pixel calculation unit resident within the graphics board may compute pixel color values for the given pixel, and may read the lookup table to generate the corresponding corrected color values. For non-exhaustive lookup table, the sample-to-pixel calculation unit may access the lookup table two or more times and interpolate the corrected color intensity vector corresponding to the input color intensity vector.","In one embodiment, color calibration processor  couples to system bus , and may transfer parameter set data to the graphics boards comprising graphics system  through system bus  instead of or in addition to control\/data bus .","The process of measuring color component power spectra and computing a set of parameters for a color correction transformation is referred to herein as color calibration. The color calibration may be performed for a representative subset of a pixel array generated by a given display device. In one embodiment, the representative subset comprises pixels on a grid as suggested by FIG. . (The grid pixels are denoted in crosshatch.) Color calibration processor  may calibrate the grid pixels, and thus, may generate a set of transformation parameters for each grid pixel. Color calibration processor  may transmit the transformation parameter sets for the pixel grid onto a data bus . Each graphics board assigned to the given display device receives the parameter sets for at least a portion of the grid.","A sample-to-pixel calculation unit may generate pixel color values for a pixel (I,J) in the pixel array, and may transform the pixel color values using a corresponding set S(I,J) of transformation parameters. The transformation may be implemented as a combination of arithmetic operations such as additions, subtractions, multiplications and\/or divisions, etc. Alternatively, the transformation may implemented by table lookup, or table lookup and addition as in the case of the lookup table which stores difference entries {circumflex over (\u03b3)}\u2212\u03b3 as described above. The transformed pixel color values (also referred to as corrected color intensity vector {circumflex over (\u03b3)}) may be transmitted to the display device.","The sample-to-pixel calculation unit may determine the parameter set S(I,J) for a non-grid pixel by interpolating the transformation parameter sets of the grid pixels nearest to the non-grid pixel. Alternatively, in the lookup table embodiments, the sample-to-pixel calculation unit may determine the corrected color intensity vector for a non-grid pixel (I,J) by reading the color intensity vectors of the nearest grid pixels from the corresponding lookup tables, and performing interpolation using these later color intensity vectors.",{"@attributes":{"id":"p-0208","num":"0213"},"figref":"FIG. 17A","b":["301","302","303","304","301","162","250","270","12","303","304","303","304","302","301","304","302"],"sub":["1","H"]},"Color correction unit  may comprise dedicated hardware for performing the color correction transformation. Similarly, interpolation unit  may comprise dedicated hardware for performing the interpolation function. In one alternative embodiment, some or all of units , ,  and  may be realized by a processor (e.g. a DSP core) executing under program control (e.g. microcode instructions).","In one embodiment, color calibration processor  may perform the interpolation operation instead of interpolation unit , and may transmit parameter set S(I,J) to color correction unit  through data bus . The parameter set S(I,J) may arrive at color correction unit  in time to perform the color correction transformation on the color intensity vector \u03b3. In this embodiment, sample-to-pixel calculation unit KU may omit color correction memory  and\/or interpolation unit .","Interpolation unit  may be coupled to the same pixel clock that drives convolve subsystem . Thus, interpolation unit  may transfer one set of transformation parameters to color correction unit  per pixel clock cycle. Similarly, convolve subsystem  may transfer one color intensity vector \u03b3 to color correction unit  per pixel clock cycle.","If pixel (I,J) is a grid pixel, interpolation unit  may read the corresponding parameter set S(I,J) from color correction memory  and pass this parameter set to color correction unit  without performing interpolation.",{"@attributes":{"id":"p-0213","num":"0218"},"figref":"FIG. 17B","b":["308","309","308","162","250","270","12","309","308","310","309"],"sub":["1 ","H"]},"Color calibration processor  may load table memory  with one or more lookup tables. Each lookup table characterizes the color transformation for a single pixel (or group of pixels) in the pixel array. For example, the color calibration processor  may compute one lookup table for each grid pixel of  as described above.","For a non-grid pixel (I,J), color correction unit  may determine grid pixels which are nearest neighbors to the non-grid pixel, and may read a corrected color intensity vector from each of the corresponding lookup tables with corresponding addresses which are derived from the color intensity vector \u03b3. Color correction unit  may perform an interpolation based on these \u201cnearest neighbor\u201d correction vectors to determine the corrected color intensity vector for the non-grid pixel. Color correction unit  may transmit this final corrected color intensity vector to a display device.","It is noted that several consecutive non-grid pixels may share the same set of nearest neighbor grid pixels. Thus, in some embodiments, this observation may be used to minimize transfer bandwidth to the table memory .","In one collection of embodiments, the lookup tables store color difference vectors {circumflex over (\u03b3)}\u2212\u03b3 as described above. Thus, color correction unit  may include an adder to add color difference vector {circumflex over (\u03b3)}\u2212\u03b3 (or an interpolated average of such color difference vectors) to the color intensity vector \u03b3 for pixel (I,J) to determine the corrected color intensity vector \u03b3.","Convolve subsystem  and\/or color correction unit  may be realized by a processor (e.g. a DSP) executing under program control. Alternatively, convolve subsystem  and\/or color correction unit  may comprise dedicated circuitry.","If pixel (I,J) is a grid pixel, color calibration correction unit  may read the corrected color intensity vector from the corresponding lookup table. In other words, the interpolation with respect to multiple grid pixels described above may be by-passed when the current pixel (I,J) is a grid pixel.","It is noted that the table memory  may be shared among multiple sample-to-pixel calculation units within one or more of the graphics boards GB(K). For example, each graphics board GB(K) may include an instance of table memory . Calibration processor  may load each table memory  with lookup tables for a corresponding subset of the grid pixels.","In one embodiment, a non-uniform grid is contemplated where the density of grid pixels in the general pixel array varies in accordance with the local rate of spatial variation of the color component power spectra. For example, a discolored spot on a projection screen or a degenerating portion of a color filter may be assigned a larger density of grid pixels than surrounding areas.","As mentioned above, graphics system  may drive multiple projection devices PDthrough PD. The multiple projection devices may generate an integrated image IMG on screen SCR as suggested in FIG. . The color calibration described above in connection with one display device may be performed for the pixel array generated by each projection device on screen SCR. When the color component spectra of pixels corresponding to a given projection device are being measured, the pixels generated by other projection devices may be turned off, so as to offer minimal interference.","For each projection device, color calibration processor  may (a) measure color component power spectra on a pixel grid of the corresponding pixel array, (b) compute a field of parameter sets\u2014one set per pixel in the pixel grid, and (c) transmit the field of parameters sets onto data bus . Each sample-to-pixel calculation unit in graphics system  may configured as described in the embodiment of , may be assigned to one of the projection devices, and may read from data bus  at least a portion of the corresponding field of parameter sets. This portion of the corresponding field may be stored in color correction memory  of FIG. A. Each sample-to-pixel calculation unit may apply a color correction in real-time to computed pixel color values based on the parameter sets stored in its color correction memory . The pixel grids for different display devices may have different numbers of grid rows and\/or grid columns.","In another set of embodiments, each sample-to-pixel calculation unit in graphics system  may be configured as described in the embodiment of , and may be assigned to one of the projection devices. It is noted that multiple sample-to-pixel calculation units may be assigned to a single projection device to partition the labor of generating pixel data for the projection device. For each projection device, color calibration processor  may measure color component power spectra on a pixel grid of the corresponding pixel array, and generate one field of lookup tables, i.e. one lookup table per grid pixel. The color calibration processor  may transmit the lookup tables to the graphics boards GB() through GB(R-) through data bus  and\/or through system bus . The table memory  in each graphics board receives and stores the lookup tables to be used by the sample-to-pixel calculation units on that graphics board. Each sample-to-pixel calculation unit may apply a color correction in real-time to computed pixel color values by performing read accesses to lookup tables stored in the local table memory , and may transmit the resulting corrected pixel color values to the assigned projection device.","In one embodiment, color calibration may be performed in response to a user command. The user of computer system  may issue a color calibration command through a graphical user interface running on host CPU . Graphics system  and color calibration processor  may calibrate some or all of the display devices DD, through DDin response to the color calibration command. The user may select which display devices are to be calibrated.","Color calibration processor  may be realized by any of a variety of processing devices including a general purpose processor, a DSP core, a programmable gate array, discrete logic, analog circuitry (e.g. operational amplifiers), etc., or any combination thereof. Color calibration processor  may be situated inside of graphics system . Alternatively, color calibration processor  may be identical to host CPU . In this alternative case, color calibration processor  may communicate with graphics system  through system bus .","In a second embodiment, color calibration processor  and graphics system  may be configured to perform color calibration periodically or intermittently. For example, graphics system  may repeatedly display red, blue and green calibration patterns subliminally. In other words, one video frame out of T video frames may be dedicated to displaying color calibration patterns. If T is large enough, the user never notices the presence of the calibration patterns. Thus, calibration may be performed on an on-going basis while graphics system  generates normal video output.","In one embodiment, a manufacturer of display devices (or projection screens) may perform color calibration for a display device (or projection screen) in anticipation that the display device (or projection screen) will be used with a graphics system similar to graphics system . In this case, color calibration processor  computes calibration information, i.e. a field of transformation parameter sets (e.g. lookup tables), for the display device (or projection device) at the manufacture site. The transformation parameter sets may be stored on a storage medium such as floppy disk, CD-ROM, etc., and shipped to a customer along with the display device (or projection screen). The customer may download the transformation parameter sets from the storage medium to an appropriate set of sample-to-pixel calculation units in a separate instance of graphics system . Thus, graphics system  may be conveniently configured to correct any errors in color presentation inherent in the display device (or projection screen).","As described above, a color calibration involves displaying and measuring multiple \u201csingle pixel\u201d calibration patterns to characterize a display device. However, sophisticated spectrum sensing devices capable of grabbing a whole screen's worth of pixel spectra in a single shot are contemplated. In this case, graphics system  may generate a solid red calibration pattern, i.e. may drive all pixels of all display devices with the red intensity vector [,,]. This solid red calibration pattern may be measured by spectrum sensing device SSD in a single shot, e.g., in a single video frame. Similarly, graphics system  may display solid green and blue calibration patterns which are each measured by spectrum sensing device SSD in a single shot.",{"@attributes":{"id":"p-0230","num":"0235"},"figref":"FIG. 18","b":["111","104","112","104"]},{"@attributes":{"id":"p-0231","num":"0236"},"figref":"FIG. 19","b":["111","112","0","1"]},"Personalized Color Correction","In one embodiment, color calibration processor  is configured to use information specific to an individual observer to compute transformation parameter sets for graphics system . For example, an eye scanning device (not shown) may be used to determine spectral filtering properties of the observer's eye(s), e.g. retina(s), cornea(s), lens (lenses), etc., or any combination thereof. Color calibration processor  may compute parameters for a color correction transformation which compensates the non-ideal spectral filtering properties of an individual's eye. In a virtual reality environment, graphics system  may compute personalized color corrections for multiple observers.","Projection Screen with Distributed Spectrum Sensing Elements","In one alternative embodiment, an array of sensing elements may be incorporated as part of a projection screen. In a front-projection scenario (where the projection devices are on the same side of the projection screen as the observers), an array of spectrum sensing devices may be distributed on the back surface of the projection screen. These spectrum sensing device may detect light which is transmitted through the screen material at a grid of locations on the screen surface.","Color Correction for Non-Super-Sampled Graphics Systems","The principles of color correction discussed above in the context of a super-sampled graphics system may be applied to a non-super-sampled graphics system. A non-super-sampled graphics system may comprise one or more pixel calculation units configured to compute initial color values (e.g. RGB values) for an array of pixels in response to received graphics data. A color calibration processor may receive spectral measurements characterizing the color component spectra for each pixel in a subset (e.g. a grid) of the pixel array. The color calibration processor may compute a set of transformation parameters (e.g. a lookup table) for each pixel in the subset. The color calibration processor may transfer the transformation parameter sets to the one or more pixel calculation units, or to a memory accessible by the one or more pixel calculation units. Each pixel calculation unit may correct computed color values based on the transformation parameter sets (e.g. by performing read accesses to one or more lookup tables using addresses generated from the initial color values).","Decay Model for Color Filters","Color calibration processor  may store parameters which model the temporal decay of color filters. Thus, color calibration processor  may compute the color correction parameter sets in response to such decay models in addition to or instead of spectral measurements from spectrum sensing device SSD. In one embodiment, color calibration processor  may store the decay parameters for display devices DD, through DDwhich are being used with graphics system . These parameters may be provided by a display manufacturer.","Color Correction on Samples","In one embodiment, one or more rendering units  may be configured to perform color correction on samples prior to storing the samples into sample buffer . A rendering unit, e.g. rendering unit A, may correct sample color values by reference to one or more lookup tables, each lookup table characterizing a color correction transformation for a known position in the virtual screen space. Alternatively, the rendering unit may correct sample color values by performing a transformation computation based on a compact set of parameters (e.g. a matrix). The rendering unit may interpolate a set of parameters for the current sample (or group of samples) based on one or more parameter sets corresponding to known positions (e.g. grid positions) in the virtual screen space. The lookup tables or parameter sets at the known positions may be computed as described in embodiments above, i.e. in response to measurements of pixel-component output spectra (e.g. red, green and blue output spectra) obtained by a spectrum sensing device. In this embodiment, sample-to-pixel calculation units CU may not perform color correction on pixel color values."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The foregoing, as well as other objects, features, and advantages of this invention may be more completely understood by reference to the following detailed description when read together with the accompanying drawings in which:",{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 1","b":["80","112","84","84"],"sub":["1 ","G "]},{"@attributes":{"id":"p-0033","num":"0032"},"figref":["FIG. 2A","FIG. 1"],"b":"80"},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 2B","b":"112"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 4","b":"420"},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 5A"},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 5B"},{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 5C"},{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0041","num":"0040"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0042","num":"0041"},"figref":"FIG. 8A"},{"@attributes":{"id":"p-0043","num":"0042"},"figref":"FIG. 8B"},{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 8C"},{"@attributes":{"id":"p-0045","num":"0044"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0046","num":"0045"},"figref":["FIG. 10A","FIG. 9"],"b":"400"},{"@attributes":{"id":"p-0047","num":"0046"},"figref":["FIG. 10B","FIG. 9"],"b":"400"},{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0050","num":"0049"},"figref":"FIG. 13","b":"420"},{"@attributes":{"id":"p-0051","num":"0050"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0052","num":"0051"},"figref":"FIG. 15","b":["111","112"]},{"@attributes":{"id":"p-0053","num":"0052"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0054","num":"0053"},"figref":"FIG. 17A"},{"@attributes":{"id":"p-0055","num":"0054"},"figref":"FIG. 17B"},{"@attributes":{"id":"p-0056","num":"0055"},"figref":"FIG. 18","b":["111","104","112","104"]},{"@attributes":{"id":"p-0057","num":"0056"},"figref":"FIG. 19","b":["111","112"]}]},"DETDESC":[{},{}]}
