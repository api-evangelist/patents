---
title: Techniques for displaying information stored in multiple multimedia documents
abstract: Techniques for providing a graphical user interface (GUI) that displays a representation of stored information that may include information of one or more types. The displayed representation may include representations of information of the one or more types. The GUI enables a user to navigate and skim through the stored information and to analyze the contents of the stored information. The stored information may include information captured along the same timeline or along different timelines.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08635531&OS=08635531&RS=08635531
owner: Ricoh Company, Ltd.
number: 08635531
owner_city: Tokyo
owner_country: JP
publication_date: 20030618
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCES TO RELATED APPLICATIONS","COPYRIGHT","BACKGROUND OF THE INVENTION","BRIEF SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION"],"p":["The present application claims priority from and is a continuation-in-part (CIP) of the following applications, the entire contents of which are herein incorporated by reference for all purposes:\n\n","The present application also claims priority from and is a non-provisional application of U.S. Provisional Application No. 60\/434,314 filed Dec. 17, 2002, the entire contents of which are herein incorporated by reference for all purposes.","A portion of the disclosure of this patent document contains material that is subject to copyright protection. The copyright owner has no objection to the xerographic reproduction by anyone of the patent document or the patent disclosure in exactly the form it appears in the U.S. Patent and Trademark Office patent file or records, but otherwise reserves all copyright rights whatsoever.","The present application also incorporates by reference for all purposes the entire contents of:\n\n","The present invention relates to user interfaces for displaying information and more particularly to user interfaces for retrieving and displaying multimedia information that may be stored in one or more multimedia documents.","With rapid advances in computer technology, an increasing amount of information is being stored in the form of electronic (or digital) documents. These electronic documents include multimedia documents that store multimedia information. The term \u201cmultimedia information\u201d is used to refer to information that comprises information of several different types in an integrated form. The different types of information included in multimedia information may include a combination of text information, graphics information, animation information, sound (audio) information, video information, slides information, whiteboard information, and other types of information. Multimedia information is also used to refer to information comprising one or more objects wherein the objects include information of different types. For example, multimedia objects included in multimedia information may comprise text information, graphics information, animation information, sound (audio) information, video information, slides information, whiteboard information, and other types of information. Multimedia documents may be considered as compound objects that comprise video, audio, closed-caption text, keyframes, presentation slides, whiteboard capture information, as well as other multimedia type objects. Examples of multimedia documents include documents storing interactive web pages, television broadcasts, videos, presentations, or the like.","Several tools and applications are conventionally available that allow users to play back, store, index, edit, or manipulate multimedia information stored in multimedia documents. Examples of such tools and\/or applications include proprietary or customized multimedia players (e.g., RealPlayer\u2122 provided by RealNetworks, Microsoft Windows Media Player provided by Microsoft Corporation, QuickTime\u2122 Player provided by Apple Corporation, Shockwave multimedia player, and others), video players, televisions, personal digital assistants (PDAs), or the like. Several tools are also available for editing multimedia information. For example, Virage, Inc. of San Mateo, Calif. (www.virage.com) provides various tools for viewing and manipulating video content and tools for creating video databases. Virage, Inc. also provides tools for face detection and on-screen text recognition from video information.","Given the vast number of electronic documents, readers of electronic documents are increasingly being called upon to assimilate vast quantities of information in a short period of time. To meet the demands placed upon them, readers find they must read electronic documents \u201chorizontally\u201d rather than \u201cvertically,\u201d i.e., they must scan, skim, and browse sections of interest in one or more electronic documents rather then read and analyze a single document from start to end. While tools exist which enable users to \u201chorizontally\u201d read or skim electronic documents containing text\/image information (e.g., the reading tool described in U.S. Non-Provisional patent application Ser. No. 08\/995,616), conventional tools cannot be used to \u201chorizontally\u201d read or skim multimedia documents which may contain audio information, video information, and other types of information. None of the multimedia tools described above allow users to \u201chorizontally\u201d read or skim a multimedia document.","In light of the above, there is a need for techniques that allow users to skim or read a multimedia document \u201chorizontally.\u201d Techniques that allow users to view, analyze, and navigate multimedia information stored in multimedia documents are desirable.","Embodiments of the present invention provide techniques for providing a graphical user interface (GUI) that displays a representation of stored information that may include information of one or more types. The displayed representation may include representations of information of the one or more types. The GUI enables a user to navigate and skim through the stored information and to analyze the contents of the stored information. The stored information may include information captured along the same timeline or along different timelines.","According to an embodiment of the present invention, a first representation of first stored information is displayed. The first stored information comprises information of a first type and information of a second type. The first representation comprises a representation of information of the first type included in the first stored information and a representation of the information of the second type included in the first stored information. One or more portions of the first representation are highlighted, the highlighted one or more portions of the first representation corresponding to portions of the first representation that include a first criterion.","According to another embodiment of the present invention, in addition to displaying a first representation of first stored information, a second representation of second stored information is displayed. The second stored information comprises information of a first type and information of a second type. The second representation comprises a representation of information of the first type included in the second stored information and a representation of information of the second type included in the second stored information. One or more portions of the second representation are highlighted, the highlighted one or more portions of the first representation corresponding to portions of the second representation that include the first criterion.","According to another embodiment of the present invention, techniques are provided for displaying multimedia information. A first thumbnail is displayed comprising a representation of information of a first type included in a first recorded information. A second thumbnail is displayed comprising a representation of information of a second type included in the first recorded information. A third thumbnail is displayed comprising a representation of information of a first type included in a second recorded information. A fourth thumbnail is displayed comprising a representation of information of a second type included in the second recorded information. According to one embodiment, one or more portions of the first thumbnail and the third thumbnail (or the second thumbnail and the fourth thumbnail) that comprise at least one word from the set of words (or a topic of interest) are highlighted.","According to yet another embodiment of the present invention, techniques are provided for displaying information included in a first recorded information and a second recorded information, the first recorded information comprising audio information and video information, the second recorded information comprising audio and video information. A first representation of information included in the first recorded information is displayed, the first representation comprising a first thumbnail and a second thumbnail, the first thumbnail comprising text information obtained from the audio information included in the first recorded information, the second thumbnail comprising one or more keyframes extracted from the video information included in the first recorded information. A second representation of information included in the second recorded information is displayed, the second representation comprising a third thumbnail and a fourth thumbnail, the third thumbnail comprising text information obtained from the audio information included in the second recorded information, the fourth thumbnail comprising one or more keyframes extracted from the video information included in the second recorded information. According to one embodiment, one or more portions of the first representation and the second representation that include the user criterion are highlighted, wherein a highlighted portion of the first representation covers a section of the first thumbnail and the second thumbnail and a highlighted portion of the second representation covers a section of the third thumbnail and the fourth thumbnail.","According to another embodiment of the present invention, techniques are provided for displaying information. A representation of stored information is displayed. Information indicative of one or more portions of the stored information that have been output is received. One or more portions of the representation of the stored information corresponding to the one or more portions of the stored information that have been output are highlighted.","According to an embodiment of the present invention, techniques are provided for displaying information. A representation of stored information is displayed. Information indicative of one or more portions of the stored information that have been output is received. One or more portions of the representation of the stored information corresponding to the one or more portions of the stored information that have not been output are highlighted.","The foregoing, together with other features, embodiments, and advantages of the present invention, will become more apparent when referring to the following specification, claims, and accompanying drawings.","Embodiments of the present invention provide techniques for retrieving and displaying multimedia information. According to an embodiment of the present invention, a graphical user interface (GUI) is provided that displays multimedia information that may be stored in a multimedia document. According to the teachings of the present invention, the GUI enables a user to navigate through multimedia information stored in a multimedia document. The GUI provides both a focused and a contextual view of the contents of the multimedia document. The GUI thus allows a user to \u201chorizontally\u201d read or skim multimedia documents.","As indicated above, the term \u201cmultimedia information\u201d is intended to refer to information that comprises information of several different types. The different types of information included in multimedia information may include a combination of text information, graphics information, animation information, sound (audio) information, video information, slides information, whiteboard images information, and other types of information. For example, a video recording of a television broadcast may comprise video information and audio information. In certain instances the video recording may also comprise close-captioned (CC) text information which comprises material related to the video information, and in many cases, is an exact representation of the speech contained in the audio portions of the video recording. Multimedia information is also used to refer to information comprising one or more objects wherein the objects include information of different types. For example, multimedia objects included in multimedia information may comprise text information, graphics information, animation information, sound (audio) information, video information, slides information, whiteboard images information, and other types of information.","The term \u201cmultimedia document\u201d as used in this application is intended to refer to any electronic storage unit (e.g., a file, a directory, etc.) that stores multimedia information. Various different formats may be used to store the multimedia information. These formats include various MPEG formats (e.g., MPEG 1, MPEG 2, MPEG 4, MPEG 7, etc.), MP3 format, SMIL format, HTML+TIME format, WMF (Windows Media Format), RM (Real Media) format, Quicktime format, Shockwave format, various streaming media formats, formats being developed by the engineering community, proprietary and customary formats, and others. Examples of multimedia documents include video recordings, MPEG files, news broadcast recordings, presentation recordings, recorded meetings, classroom lecture recordings, broadcast television programs, or the like.",{"@attributes":{"id":"p-0059","num":"0063"},"figref":["FIG. 1","FIG. 1","FIG. 1","FIG. 1"],"b":["100","100","102","104","106","108","110","100"]},"Communication network  provides a mechanism allowing the various computer systems depicted in  to communicate and exchange information with each other. Communication network  may itself be comprised of many interconnected computer systems and communication links. While in one embodiment, communication network  is the Internet, in other embodiments, communication network  may be any suitable communication network including a local area network (LAN), a wide area network (WAN), a wireless network, an intranet, a private network, a public network, a switched network, or the like.","Communication links  used to connect the various systems depicted in  may be of various types including hardwire links, optical links, satellite or other wireless communications links, wave propagation links, or any other mechanisms for communication of information. Various communication protocols may be used to facilitate communication of information via the communication links. These communication protocols may include TCP\/IP, HTTP protocols, extensible markup language (XML), wireless application protocol (WAP), protocols under development by industry standard organizations, vendor-specific protocols, customized protocols, and others.","Computer systems connected to communication network  may be classified as \u201cclients\u201d or \u201cservers\u201d depending on the role the computer systems play with respect to requesting information and\/or services or providing information and\/or services. Computer systems that are used by users to request information or to request a service are classified as \u201cclient\u201d computers (or \u201cclients\u201d). Computer systems that store information and provide the information in response to a user request received from a client computer, or computer systems that perform processing to provide the user-requested services are called \u201cserver\u201d computers (or \u201cservers\u201d). It should however be apparent that a particular computer system may function both as a client and as a server.","Accordingly, according to an embodiment of the present invention, server system  is configured to perform processing to facilitate generation of a GUI that displays multimedia information according to the teachings of the present invention. The GUI generated by server system  may be output to the user (e.g., a reader of the multimedia document) via an output device coupled to server system  or via client systems . The GUI generated by server  enables the user to retrieve and browse multimedia information that may be stored in a multimedia document. The GUI provides both a focused and a contextual view of the contents of a multimedia document and thus enables the multimedia document to be skimmed or read \u201chorizontally.\u201d","The processing performed by server system  to generate the GUI and to provide the various features according to the teachings of the present invention may be implemented by software modules executing on server system , by hardware modules coupled to server system , or combinations thereof. In alternative embodiments of the present invention, the processing may also be distributed between the various computer systems depicted in .","The multimedia information that is displayed in the GUI may be stored in a multimedia document that is accessible to server system . For example, the multimedia document may be stored in a storage subsystem of server system . The multimedia document may also be stored by other systems such as MIS  that are accessible to server . Alternatively, the multimedia document may be stored in a memory location accessible to server system .","In alternative embodiments, instead of accessing a multimedia document, server system  may receive a stream of multimedia information (e.g., a streaming media signal, a cable signal, etc.) from a multimedia information source such as MIS . According to an embodiment of the present invention, server system  stores the multimedia information signals in a multimedia document and then generates a GUI that displays the multimedia information. Examples of MIS  include a television broadcast receiver, a cable receiver, a digital video recorder (e.g., a TIVO box), or the like. For example, multimedia information source  may be embodied as a television that is configured to receive multimedia broadcast signals and to transmit the signals to server system . In alternative embodiments, server system  may be configured to intercept multimedia information signals received by MIS . Server system  may receive the multimedia information directly from MIS  or may alternatively receive the information via a communication network such as communication network .","As described above, MIS  depicted in  represents a source of multimedia information. According to an embodiment of the present invention, MIS  may store multimedia documents that are accessed by server system . For example, MIS  may be a storage device or a server that stores multimedia documents that may be accessed by server system . In alternative embodiments, MIS  may provide a multimedia information stream to server system . For example, MIS  may be a television receiver\/antenna providing live television feed information to server system . MIS  may be a device such as a video recorder\/player, a DVD player, a CD player, etc. providing recorded video and\/or audio stream to server system . In alternative embodiments, MIS  may be a presentation or meeting recorder device that is capable of providing a stream of the captured presentation or meeting information to server system . MIS  may also be a receiver (e.g., a satellite dish or a cable receiver) that is configured to capture or receive (e.g., via a wireless link) multimedia information from an external source and then provide the captured multimedia information to server system  for further processing.","Users may use client systems  to view the GUI generated by server system . Users may also use client systems  to interact with the other systems depicted in . For example, a user may use user system  to select a particular multimedia document and request server system  to generate a GUI displaying multimedia information stored by the particular multimedia document. A user may also interact with the GUI generated by server system  using input devices coupled to client system . In alternative embodiments, client system  may also perform processing to facilitate generation of a GUI according to the teachings of the present invention. A client system  may be of different types including a personal computer, a portable computer, a workstation, a computer terminal, a network computer, a mainframe, a kiosk, a personal digital assistant (PDA), a communication device such as a cell phone, or any other data processing system.","According to an embodiment of the present invention, a single computer system may function both as server system  and as client system . Various other configurations of the server system , client system , and MIS  are possible.",{"@attributes":{"id":"p-0070","num":"0074"},"figref":["FIG. 2","FIG. 1","FIG. 2"],"b":["200","200","200","202","204","206","208","210","212","214","216","200","216"]},"Bus subsystem  provides a mechanism for letting the various components and subsystems of computer system  communicate with each other as intended. The various subsystems and components of computer system  need not be at the same physical location but may be distributed at various locations within network . Although bus subsystem  is shown schematically as a single bus, alternative embodiments of the bus subsystem may utilize multiple busses.","User interface input devices  may include a keyboard, pointing devices, a mouse, trackball, touchpad, a graphics tablet, a scanner, a barcode scanner, a touchscreen incorporated into the display, audio input devices such as voice recognition systems, microphones, and other types of input devices. In general, use of the term \u201cinput device\u201d is intended to include all possible types of devices and ways to input information using computer system .","User interface output devices  may include a display subsystem, a printer, a fax machine, or non-visual displays such as audio output devices. The display subsystem may be a cathode ray tube (CRT), a flat-panel device such as a liquid crystal display (LCD), a projection device, or the like. The display subsystem may also provide non-visual display such as via audio output devices. In general, use of the term \u201coutput device\u201d is intended to include all possible types of devices and ways to output information from computer system . According to an embodiment of the present invention, the GUI generated according to the teachings of the present invention may be presented to the user via output devices .","Storage subsystem  may be configured to store the basic programming and data constructs that provide the functionality of the computer system and of the present invention. For example, according to an embodiment of the present invention, software modules implementing the functionality of the present invention may be stored in storage subsystem  of server system . These software modules may be executed by processor(s)  of server system . In a distributed environment, the software modules may be stored on a plurality of computer systems and executed by processors of the plurality of computer systems. Storage subsystem  may also provide a repository for storing various databases that may be used by the present invention. Storage subsystem  may comprise memory subsystem  and file storage subsystem .","Memory subsystem  may include a number of memories including a main random access memory (RAM)  for storage of instructions and data during program execution and a read only memory (ROM)  in which fixed instructions are stored. File storage subsystem  provides persistent (non-volatile) storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a Compact Disk Read Only Memory (CD-ROM) drive, an optical drive, removable media cartridges, and other like storage media. One or more of the drives may be located at remote locations on other connected computers.","Computer system  can be of varying types including a personal computer, a portable computer, a workstation, a computer terminal, a network computer, a mainframe, a kiosk, a personal digital assistant (PDA), a communication device such as a cell phone, or any other data processing system. Server computers generally have more storage and processing capacity then client systems. Due to the ever-changing nature of computers and networks, the description of computer system  depicted in  is intended only as a specific example for purposes of illustrating the preferred embodiment of the computer system. Many other configurations of a computer system are possible having more or fewer components than the computer system depicted in .",{"@attributes":{"id":"p-0077","num":"0081"},"figref":["FIG. 3","FIG. 3"],"b":["300","300"]},"GUI  displays multimedia information stored in a multimedia document. The multimedia information stored by the multimedia document and displayed by GUI  may comprise information of a plurality of different types. As depicted in , GUI  displays multimedia information corresponding to a television broadcast that includes video information, audio information, and possibly closed-caption (CC) text information. The television broadcast may be stored as a television broadcast recording in a memory location accessible to server system . It should however be apparent that the present invention is not restricted to displaying television recordings. Multimedia information comprising other types of information may also be displayed according to the teachings of the present invention.","The television broadcast may be stored using a variety of different techniques. According to one technique, the television broadcast is recorded and stored using a satellite receiver connected to a PC-TV video card of server system . Applications executing on server system  then process the recorded television broadcast to facilitate generation of GUI . For example, the video information contained in the television broadcast may be captured using an MPEG capture application that creates a separate metafile (e.g., in XML format) containing temporal information for the broadcast and closed-caption text, if provided. Information stored in the metafile may then be used to generate GUI  depicted in .","As depicted in , GUI  comprises several viewing areas including a first viewing area , a second viewing area , a third viewing area , a fourth viewing area , and a fifth viewing area . It should be apparent that in alternative embodiments the present invention may comprise more or fewer viewing areas than those depicted in . Further, in alternative embodiments of the present invention one or more viewing areas may be combined into one viewing area, or a particular viewing area may be divided in multiple viewing areas. Accordingly, the viewing areas depicted in  and described below are not meant to restrict the scope of the present invention as recited in the claims.","According to an embodiment of the present invention, first viewing area  displays one or more commands that may be selected by a user viewing GUI . Various user interface features such as menu bars, drop-down menus, cascading menus, buttons, selection bars, buttons, etc. may be used to display the user-selectable commands. According to an embodiment of the present invention, the commands provided in first viewing area  include a command that enables the user to select a multimedia document whose multimedia information is to be displayed in the GUI. The commands may also include one or more commands that allow the user to configure and\/or customize the manner in which multimedia information stored in the user-selected multimedia document is displayed in GUI . Various other commands may also be provided in first viewing area .","According to an embodiment of the present invention, second viewing area  displays a scaled representation of multimedia information stored by the multimedia document. The user may select the scaling factor used for displaying information in second viewing area . According to a particular embodiment of the present invention, a representation of the entire (i.e., multimedia information between the start time and end time associated with the multimedia document) multimedia document is displayed in second viewing area . In this embodiment, one end of second viewing area  represents the start time of the multimedia document and the opposite end of second viewing area  represents the end time of the multimedia document.","As shown in , according to an embodiment of the present invention, second viewing area  comprises one or more thumbnail images . Each thumbnail image displays a representation of a particular type of information included in the multimedia information stored by the multimedia document. For example, two thumbnail images - and - are displayed in second viewing area  of GUI  depicted in . Thumbnail image - displays text information corresponding to information included in the multimedia information stored by the multimedia document being displayed by GUI . The text displayed in thumbnail image - may represent a displayable representation of CC text included in the multimedia information displayed by GUI . Alternatively, the text displayed in thumbnail image - may represent a displayable representation of a transcription of audio information included in the multimedia information stored by the multimedia document whose contents are displayed by GUI . Various audio-to-text transcription techniques may be used to generate a transcript for the audio information. The text displayed in a thumbnail image may also be a representation of other types of information included in the multimedia information. For example, the text information may be a representation of comments made when the multimedia information was recorded or viewed, annotations added to the multimedia information, etc.","Thumbnail image - displays a representation of video information included in the multimedia information displayed by GUI . In the embodiment depicted in , the video information is displayed using video keyframes extracted from the video information included in the multimedia information stored by the multimedia document. The video keyframes may be extracted from the video information in the multimedia document at various points in time using a specified sampling rate. A special layout style, which may be user-configurable, is used to display the extracted keyframes in thumbnail image - to enhance readability of the frames.","One or more thumbnail images may be displayed in second viewing area  based upon the different types of information included in the multimedia information being displayed. Each thumbnail image  displayed in second viewing area  displays a representation of information of a particular type included in the multimedia information stored by the multimedia document. According to an embodiment of the present invention, the number of thumbnails displayed in second viewing area  and the type of information displayed by each thumbnail is user-configurable.","According to an embodiment of the present invention, the various thumbnail images displayed in second viewing area  are temporally synchronized or aligned with each other along a timeline. This implies that the various types of information included in the multimedia information and occurring at approximately the same time are displayed next to each other. For example, thumbnail images - and - are aligned such that the text information (which may represent CC text information, a transcript of the audio information, or a text representation of some other type of information included in the multimedia information) displayed in thumbnail image - and video keyframes displayed in thumbnail - that occur in the multimedia information at a particular point in time are displayed close to each other (e.g., along the same horizontal axis). Accordingly, information that has a particular time stamp is displayed proximal to information that has approximately the same time stamp. This enables a user to determine the various types of information occurring approximately concurrently in the multimedia information being displayed by GUI  by simply scanning second viewing area  in the horizontal axis.","According to the teachings of the present invention, a viewing lens or window  (hereinafter referred to as \u201cthumbnail viewing area lens \u201d) is displayed in second viewing area . Thumbnail viewing area lens  covers or emphasizes a portion of second viewing area . According to the teachings of the present invention, multimedia information corresponding to the area of second viewing area  covered by thumbnail viewing area lens  is displayed in third viewing area .","In the embodiment depicted in , thumbnail viewing area lens  is positioned at the top of second viewing area  and emphasizes a top portion (or starting portion) of the multimedia document. The position of thumbnail viewing area lens  may be changed by a user by sliding or moving lens  along second viewing area . For example, in , thumbnail viewing area lens  may be moved vertically along second viewing area .","In response to a change in the position of thumbnail viewing area lens  from a first location in second viewing area  to a second location along second viewing area , the multimedia information displayed in third viewing area  is automatically updated such that the multimedia information displayed in third viewing area  continues to correspond to the area of second viewing area  emphasized by thumbnail viewing area lens . Accordingly, a user may use thumbnail viewing area lens  to navigate and scroll through the contents of the multimedia document displayed by GUI . Thumbnail viewing area lens  thus provides a context and indicates a location of the multimedia information displayed in third viewing area  within the entire multimedia document.",{"@attributes":{"id":"p-0090","num":"0094"},"figref":["FIG. 4","FIG. 4","FIG. 3"],"b":["314","314","318","320","314","304","318","320","314","304","318","320","314","304","314","304","314","304","314","306","314","304","306","306","304","314"],"sub":["1","2","2","1","1 ","2 ","1 ","2","2 ","1 "]},"As shown in  and , thumbnail viewing area lens  comprises a sub-lens  which further emphasizes a sub-portion of the portion of second viewing area  emphasized by thumbnail viewing area lens . According to an embodiment the present invention, the portion of second viewing area  emphasized or covered by sub-lens  corresponds to the portion of third viewing area  emphasized by lens . Sub-lens  can be moved along second viewing area  within edges  and  of thumbnail viewing area lens . When sub-lens  is moved from a first location to a second location within the boundaries of thumbnail viewing area lens , the position of lens  in third viewing area  is also automatically changed to correspond to the changed location of sub-lens . Further, if the position of lens  is changed from a first location to a second location over third viewing area , the position of sub-lens  is also automatically updated to correspond to the changed position of lens . Further details related to lens  are described below.","As described above, multimedia information corresponding to the portion of second viewing area  emphasized by thumbnail viewing area lens  is displayed in third viewing area . Accordingly, a representation of multimedia information occurring between time tand t(corresponding to a segment of time of the multimedia document emphasized by thumbnail viewing area lens ) is displayed in third viewing area . Third viewing area  thus displays a zoomed-in representation of the multimedia information stored by the multimedia document corresponding to the portion of the multimedia document emphasized by thumbnail viewing area lens .","As depicted in , third viewing area  comprises one or more panels . Each panel displays a representation of information of a particular type included in the multimedia information occurring during the time segment emphasized by thumbnail viewing area lens . For example, in GUI  depicted in , two panels - and - are displayed in third viewing area . According to an embodiment of the present invention, each panel  in third viewing area  corresponds to a thumbnail image  displayed in second viewing area  and displays information corresponding to the section of the thumbnail image covered by thumbnail viewing area lens .","Like thumbnail images , panels  are also temporally aligned or synchronized with each other. Accordingly, the various types of information included in the multimedia information and occurring at approximately the same time are displayed next to each other in third viewing area . For example, panels - and - depicted in  are aligned such that the text information (which may represent CC text information, a transcript of the audio information, or a text representation of some other type of information included in the multimedia information) displayed in panel - and video keyframes displayed in panel - that occur in the multimedia information at a approximately the same point in time are displayed close to each other (e.g., along the same horizontal axis). Accordingly, information that has a particular time stamp is displayed proximal to other types of information that has approximately the same time stamp. This enables a user to determine the various types of information occurring approximately concurrently in the multimedia information by simply scanning third viewing area  in the horizontal axis.","Panel - depicted in GUI  corresponds to thumbnail image - and displays text information corresponding to the area of thumbnail image - emphasized or covered by thumbnail viewing area lens . The text information displayed by panel - may correspond to text extracted from CC information included in the multimedia information, or alternatively may represent a transcript of audio information included in the multimedia information, or a text representation of some other type of information included in the multimedia information. According to an embodiment of the present invention, the present invention takes advantage of the automatic story segmentation and other features that are often provided in close-captioned (CC) text from broadcast news. Most news agencies who provide CC text as part of their broadcast use a special syntax in the CC text (e.g., a \u201c>>>\u201d delimiter to indicate changes in story line or subject, a \u201c>>\u201d delimiter to indicate changes in speakers, etc.). Given the presence of this kind of information in the CC text information included in the multimedia information, the present invention incorporates these features in the text displayed in panel -. For example, a \u201c>>>\u201d delimiter may be displayed to indicate changes in story line or subject, a \u201c>>\u201d delimiter may be displayed to indicate changes in speakers, additional spacing may be displayed between text portions related to different story lines to clearly demarcate the different stories, etc. This enhances the readability of the text information displayed in panel -.","Panel - depicted in GUI  corresponds to thumbnail image - and displays a representation of video information corresponding to the area of thumbnail image - emphasized or covered by thumbnail viewing area lens . Accordingly, panel - displays a representation of video information included in the multimedia information stored by the multimedia document and occurring between times tand tassociated with thumbnail viewing area lens . In the embodiment depicted in , video keyframes extracted from the video information included in the multimedia information are displayed in panel -. A special layout style (which is user-configurable) is used to display the extracted keyframes to enhance readability of the frames.","Various different techniques may be used to display video keyframes in panel -. According to an embodiment of the present invention, the time segment between time tand time tis divided into sub-segments of a pre-determined time period. Each sub-segment is characterized by a start time and an end time associated with the sub-segment. According to an embodiment of the present invention, the start time of the first sub-segment corresponds to time twhile the end time of the last sub-segment corresponds to time t. Server  then extracts a set of one or more video keyframes from the video information stored by the multimedia document for each sub-segment occurring between the start time and end time associated with the sub-segment. For example, according to an embodiment of the present invention, for each sub-segment, server  may extract a video keyframe at 1-second intervals between a start time and an end time associated with the sub-segment.","For each sub-segment, server  then selects one or more keyframes from the set of extracted video keyframes for the sub-segment to be displayed in panel -. The number of keyframes selected to be displayed in panel - for each sub-segment is user-configurable. Various different techniques may be used for selecting the video keyframes to be displayed from the extracted set of video keyframes for each time sub-segment. For example, if the set of video keyframes extracted for a sub-segment comprises 24 keyframes and if six video keyframes are to be displayed for each sub-segment (as shown in ), server  may select the first two video keyframes, the middle two video keyframes, and the last two video keyframes from the set of extracted video keyframes for the sub-segment.","In another embodiment, the video keyframes to be displayed for a sub-segment may be selected based upon the sequential positions of the keyframes in the set of keyframes extracted for sub-segment. For example, if the set of video keyframes extracted for a sub-segment comprises 24 keyframes and if six video keyframes are to be displayed for each sub-segment, then the 1st, 5th, 9th, 13th, 17th, and 21st keyframe may be selected. In this embodiment, a fixed number of keyframes are skipped.","In yet another embodiment, the video keyframes to be displayed for a sub-segment may be selected based upon time values associated with the keyframes in the set of keyframes extracted for sub-segment. For example, if the set of video keyframes extracted for a sub-segment comprises 24 keyframes extracted at a sampling rate of 1 second and if six video keyframes are to be displayed for each sub-segment, then the first frame may be selected and subsequently a keyframe occurring 4 seconds after the previously selected keyframe may be selected.","In an alternative embodiment of the present invention, server  may select keyframes from the set of keyframes based upon differences in the contents of the keyframes. For each sub-segment, server  may use special image processing techniques to determine differences in the contents of the keyframes extracted for the sub-segment. If six video keyframes are to be displayed for each sub-segment, server  may then select six keyframes from the set of extracted keyframes based upon the results of the image processing techniques. For example, the six most dissimilar keyframes may be selected for display in panel -. It should be apparent that various other techniques known to those skilled in the art may also be used to perform the selection of video keyframes.","The selected keyframes are then displayed in panel -. Various different formats may be used to display the selected keyframes in panel -. For example, as shown in , for each sub-segment, the selected keyframes are laid out left-to-right and top-to-bottom.","In an alternative embodiment of the present invention, the entire multimedia document is divided into sub-segments of a pre-determined time period. Each sub-segment is characterized by a start time and an end time associated with the sub-segment. According to an embodiment of the present invention, the start time of the first sub-segment corresponds to the start time of the multimedia document while the end time of the last sub-segment corresponds to the end time of the multimedia document. As described above, server  then extracts a set of one or more video keyframes from the video information stored by the multimedia document for each sub-segment based upon the start time and end time associated with the sub-segment. Server  then selects one or more keyframes for display for each sub-segment. Based upon the position of thumbnail viewing area lens , keyframes that have been selected for display and that occur between tand tassociated with thumbnail viewing area lens  are then displayed in panel -.","It should be apparent that various other techniques may also be used for displaying video information in panel - in alternative embodiments of the present invention. According to an embodiment of the present invention, the user may configure the technique to be used for displaying video information in third viewing area .","In GUI  depicted in , each sub-segment is 8 seconds long and video keyframes corresponding to a plurality of sub-segments are displayed in panel -. Six video keyframes are displayed from each sub-segment. For each sub-segment, the displayed keyframes are laid out in a left-to-right and top-to-bottom manner.","It should be apparent that, in alternative embodiments of the present invention, the number of panels displayed in third viewing area  may be more or less than the number of thumbnail images displayed in second viewing area . According to an embodiment of the present invention, the number of panels displayed in third viewing area  is user-configurable.","According to the teachings of the present invention, a viewing lens or window  (hereinafter referred to as \u201cpanel viewing area lens \u201d) is displayed covering or emphasizing a portion of overview region . According to the teachings of the present invention, multimedia information corresponding to the area of third viewing area  emphasized by panel viewing area lens  is displayed in fourth viewing area . A user may change the position of panel viewing area lens  by sliding or moving lens  along third viewing area . In response to a change in the position of panel viewing area lens  from a first location in third viewing area  to a second location, the multimedia information displayed in fourth viewing area  is automatically updated such that the multimedia information displayed in fourth viewing area  continues to correspond to the area of third viewing area  emphasized by panel viewing area lens . Accordingly, a user may use panel viewing area lens  to change the multimedia information displayed in fourth viewing area .","As described above, a change in the location of panel viewing area lens  also causes a change in the location of sub-lens  such that the area of second viewing area  emphasized by sub-lens  continues to correspond to the area of third viewing area  emphasized by panel viewing area lens . Likewise, as described above, a change in the location of sub-lens  also causes a change in the location of panel viewing area lens  over third viewing area  such that the area of third viewing area  emphasized by panel viewing area lens  continues to correspond to the changed location of sub-lens .",{"@attributes":{"id":"p-0109","num":"0113"},"figref":["FIG. 5A","FIG. 5A"],"b":["322","322","326","328","322","306","326","328","322","306","326","328","322","306","322","306","322","306","322","308","322","306","308","308","306","322","306","308"],"sub":["3","4","4","3 ","1","3","4","2","3 ","1","4 ","2","3 ","4","3 ","4 "]},"According to an embodiment of the present invention, a particular line of text (or one or more words from the last line of text) emphasized by panel viewing area lens  may be displayed on a section of lens . For example, as depicted in , the last line of text  \u201cEnvironment is a national\u201d that is emphasized by panel viewing area lens  in panel - is displayed in bolded style on panel viewing area lens .","According to an embodiment of the present invention, special features may be attached to panel viewing area lens  to facilitate browsing and navigation of the multimedia document. As shown in , a \u201cplay\/pause button\u201d  and a \u201clock\/unlock button\u201d  are provided on panel viewing area lens  according to an embodiment of the present invention. Play\/Pause button  allows the user to control playback of the video information from panel viewing area lens . Lock\/Unlock button  allows the user to switch the location of the video playback from area - of fourth viewing area  to a reduced window on top of panel viewing area lens .",{"@attributes":{"id":"p-0112","num":"0116"},"figref":["FIG. 5B","FIG. 5B","FIG. 5B"],"b":["322","334","322","336","322","322","342","2","336","336","340","1","308"],"sub":["3 ","4 "]},"According to an embodiment of the present invention, window  has transparent borders so that portions of the underlying third viewing area  (e.g., the keyframes displayed in panel -) can be seen. This helps to maintain the user's location focus while viewing third viewing area . The user may use play\/pause button  to start and stop the video displayed in window . The user may change the location of panel viewing area lens  while the video is being played back in window . A change in the location of panel viewing area lens  causes the video played back in window  to change corresponding to the new location of panel viewing area lens . The video played back in window  corresponds to the new time values tand tassociated with panel viewing area lens .",{"@attributes":{"id":"p-0114","num":"0118"},"figref":["FIG. 5C","FIG. 5C"],"b":["322","322","104","324","2","322","338","322","322","324","2","322","342","2","338"],"sub":["3 ","4 "]},"As described above, multimedia information corresponding to the section of third viewing area  covered by panel viewing area lens  (i.e., multimedia information occurring in the time segment between tand t) is displayed in fourth viewing area . As depicted in , fourth viewing area  may comprise one or more sub viewing areas  (e.g., -, -, and -). According to an embodiment of the present invention, one or more of sub-regions  may display a particular type of information included in the multimedia information corresponding to the section of third viewing area  emphasized by panel viewing area lens .","For example, as depicted in , video information corresponding to (or starting from) the video information emphasized by panel viewing area lens  in third viewing area  is displayed in sub viewing area -. According to an embodiment of the present invention, video information starting at time t(time corresponding to the top edge of panel viewing area lens ) may be played back in sub viewing area -. In alternative embodiments, the video information played back in area - may start at time tor some other user-configurable time between tand t. The playback of the video in sub viewing area - may be controlled using control bar . Control bar  provides a plurality of controls including controls for playing, pausing, stopping, rewinding, and forwarding the video played in sub viewing area -. The current time and length  of the video being played in area - is also displayed. Information identifying the name of the video , the date  the video was recorded, and the type of the video  is also displayed.","In alternative embodiments of the present invention, instead of playing back video information, a video keyframe from the video keyframes emphasized by panel viewing area lens  in panel - is displayed in sub viewing area -. According to an embodiment of the present invention, the keyframe displayed in area - represents a keyframe that is most representative of the keyframes emphasized by panel viewing area lens .","According to an embodiment of the present invention, text information (e.g., CC text, transcript of audio information, text representation of some other type of information included in the multimedia information, etc.) emphasized by panel viewing area lens  in third viewing area  is displayed in sub viewing area -. According to an embodiment of the present invention, sub viewing area - displays text information that is displayed in panel - and emphasized by panel viewing area lens . As described below, various types of information may be displayed in sub viewing area -.","Additional information related to the multimedia information stored by the multimedia document may be displayed in fifth viewing area  of GUI . For example, as depicted in , words occurring in the text information included in the multimedia information displayed by GUI  are displayed in area  of fifth viewing area . The frequency of each word in the multimedia document is also displayed next to each word. For example, the word \u201cquestion\u201d occurs seven times in the multimedia information CC text. Various other types of information related to the multimedia information may also be displayed in fifth viewing area .","According to an embodiment of the present invention, GUI  provides features that enable a user to search for one or more words that occur in the text information (e.g., CC text, transcript of audio information, a text representation of some other type of information included in the multimedia information) extracted from the multimedia information. For example, a user can enter one or more query words in input field  and upon selecting \u201cFind\u201d button , server  analyzes the text information extracted from the multimedia information stored by the multimedia document to identify all occurrences of the one or more query words entered in field . The occurrences of the one or more words in the multimedia document are then highlighted when displayed in second viewing area , third viewing area , and fourth viewing area . For example, according to an embodiment of the present invention, all occurrences of the query words are highlighted in thumbnail image -, in panel -, and in sub viewing area -. In alternative embodiments of the present invention, occurrences of the one or more query words may also be highlighted in the other thumbnail images displayed in second viewing area , panels displayed in third viewing area , and sub viewing areas displayed in fourth viewing area .","The user may also specify one or more words to be highlighted in the multimedia information displayed in GUI . For example, a user may select one or more words to be highlighted from area . All occurrences of the keywords selected by the user in area  are then highlighted in second viewing area , third viewing area , and fourth viewing area . For example, as depicted in , the user has selected the word \u201cNational\u201d in area . In response to the user's selection, according to an embodiment of the present invention, all occurrences of the word \u201cNational\u201d are highlighted in second viewing area , third viewing area , and third viewing area .","According to an embodiment of the present invention, lines of text  that comprise the user-selected word(s) (or query words entered in field ) are displayed in sub viewing area - of fourth viewing area . For each line of text, the time  when the line occurs (or the timestamp associated with the line of text) in the multimedia document is also displayed. The timestamp associated with the line of text generally corresponds to the timestamp associated with the first word in the line.","For each line of text, one or more words surrounding the selected or query word(s) are displayed. According to an embodiment of the present invention, the number of words surrounding a selected word that is displayed in area - is user configurable. For example, in GUI  depicted in , a user can specify the number of surrounding words to be displayed in area - using control . The number specified by the user indicates the number of words that occur before the select word and the number of words that occur after the selected word that are to be displayed. In the embodiment depicted in , control  is a slider bar that can be adjusted between a minimum value of \u201c3\u201d and a maximum value of \u201c10\u201d. The user can specify the number of surrounding words to be displayed by adjusting slider bar . For example, if the slider bar is set to \u201c3\u201d, then three words that occur before a selected word and three words that occur after the selected word will be displayed in area -. The minimum and maximum values are user configurable.","Further, GUI  depicted in  comprises an area  sandwiched between thumbnail images - and - that indicates locations of occurrences of the query words or other words specified by the user. For example, area  comprises markers indicating the locations of word \u201cNational\u201d in thumbnail image -. The user can then use either thumbnail viewing area lens , or panel viewing area lens  to scroll to a desired location within the multimedia document.  depicts a simplified zoomed-in view of second viewing area  showing area  according to an embodiment of the present invention. As depicted in , area  (or channel ) comprises markers  indicating locations in thumbnail image - that comprise occurrences of the word \u201cNational\u201d. In alternative embodiments of the present invention, markers in channel  may also identify locations of the user-specified words or phrases in the other thumbnail images displayed in second viewing area . In alternative embodiments, locations of occurrences of the query words or other words specified by the user may be displayed on thumbnail images  (as depicted in ).","As shown in , the position of thumbnail viewing area lens  has been changed with respect to . In response to the change in position of thumbnail viewing area lens , the multimedia information displayed in third viewing area  has been changed to correspond to the section of second viewing area  emphasized by thumbnail viewing area lens . The multimedia information displayed in fourth viewing area  has also been changed corresponding to the new location of panel viewing area lens .","According to an embodiment of the present invention, multimedia information displayed in GUI  that is relevant to user-specified topics of interest is highlighted or annotated. The annotations or highlights provide visual indications of information that is relevant to or of interest to the user. GUI  thus provides a convenient tool that allows a user to readily locate portions of the multimedia document that are relevant to the user.","According to an embodiment of the present invention, information specifying topics that are of interest or are relevant to the user may be stored in a user profile. One or more words or phrases may be associated with each topic of interest. Presence of the one or more words and phrases associated with a particular user-specified topic of interest indicates presence of information related to the particular topic. For example, a user may specify two topics of interest\u2014\u201cGeorge W. Bush\u201d and \u201cEnergy Crisis\u201d. Words or phrases associated with the topic \u201cGeorge Bush\u201d may include \u201cPresident Bush,\u201d \u201cthe President,\u201d \u201cMr. Bush,\u201d and other like words and phrases. Words or phrases associated with the topic \u201cEnergy Crisis\u201d may include \u201cindustrial pollution,\u201d \u201cnatural pollution,\u201d \u201cclean up the sources,\u201d \u201camount of pollution,\u201d \u201cair pollution\u201d, \u201celectricity,\u201d \u201cpower-generating plant,\u201d or the like. Probability values may be associated with each of the words or phrases indicating the likelihood of the topic of interest given the presence of the word or phrase. Various tools may be provided to allow the user to configure topics of interest, to specify keywords and phrases associated with the topics, and to specify probability values associated with the keywords or phrases.","It should be apparent that various other techniques known to those skilled in the art may also be used to model topics of interest to the user. These techniques may include the use of Bayesian networks, relevance graphs, or the like. Techniques for determining sections relevant to user-specified topics, techniques for defining topics of interest, techniques for associating keywords and\/or key phrases and probability values are described in U.S. application Ser. No. 08\/995,616, filed Dec. 22, 1997, the entire contents of which are herein incorporated by reference for all purposes.","According to an embodiment of the present invention, in order to identify locations in the multimedia document related to user-specified topics of interest, server  searches the multimedia document to identify locations within the multimedia document of words or phrases associated with the topics of interest. As described above, presence of words and phrases associated with a particular user-specified topic of interest in the multimedia document indicate presence of the particular topic relevant to the user. The words and phrases that occur in the multimedia document and that are associated with user specified topics of interest are annotated or highlighted when displayed by GUI .",{"@attributes":{"id":"p-0130","num":"0134"},"figref":["FIG. 8","FIG. 8"],"b":["800","800","800"]},"In the embodiment depicted in , the user has specified four topics of interest . A label  identifies each topic. The topics specified in GUI  include \u201cEnergy Crisis,\u201d \u201cAssistive Tech,\u201d \u201cGeorge W. Bush.\u201d and \u201cNepal.\u201d In accordance with the teachings of the present invention, keywords and key phrases relevant to the specified topics are highlighted in second viewing area , third viewing area , and fourth viewing area . Various different techniques may be used to highlight or annotate the keywords and\/or key phrases related to the topics of interest. According to an embodiment of the present invention, different colors and styles (e.g., bolding, underlining, different font size, etc.) may be used to highlight words and phrases related to user-specified topics. For example, each topic may be assigned a particular color and content related to a particular topic might be highlighted using the particular color assigned to the particular topic. For example, as depicted in , a first color is used to highlight words and phrases related to the \u201cEnergy Crisis\u201d topic of interest, a second color is used to highlight words and phrases related to the \u201cAssistive Tech\u201d topic of interest, a third color is used to highlight words and phrases related to the \u201cGeorge W. Bush\u201d topic of interest, and a fourth color is used to highlight words and phrases related to the \u201cNepal\u201d topic of interest.","According to an embodiment of the present invention, server  searches the text information (e.g., CC text, transcript of audio information, or a text representation of some other type of information included in the multimedia information) extracted from the multimedia information to locate words or phrases relevant to the user topics. If server  finds a word or phrase in the text information that is associated with a topic of interest, the word or phrase is annotated or highlighted when displayed in GUI . As described above, several different techniques may be used to annotate or highlight the word or phrase. For example, the word or phrase may be highlighted, bolded, underlined, demarcated using sidebars or balloons, font may be changed, etc.","Keyframes (representing video information of the multimedia document) that are displayed by the GUI and that are related to user specified topics of interest may also be highlighted. According to an embodiment of the present invention, server system  may use OCR techniques to extract text from the keyframes extracted from the video information included in the multimedia information. The text output of the OCR techniques may then be compared with words or phrases associated with one or more user-specified topics of interest. If there is a match, the keyframe containing the matched word or phrase (i.e., the keyframe from which the matching word or phrase was extracted by OCR techniques) may be annotated or highlighted when the keyframe is displayed in GUI  either in second viewing area , third viewing area , or fourth viewing area  of GUI . Several different techniques may be used to annotate or highlight the keyframe. For example, a special box may be drawn around a keyframe that is relevant to a particular topic of interest. The color of the box may correspond to the color associated with the particular topic of interest. The matching text in the keyframe may also be highlighted or underlined or displayed in reverse video. As described above, the annotated or highlighted keyframes displayed in second viewing area  (e.g., the keyframes displayed in thumbnail image - in ) may be identified by markers displayed in channel area . In alternative embodiments, the keyframes may be annotated or highlighted in thumbnail image -.","According to an embodiment of the present invention, as shown in , a relevance indicator  may also be displayed for each user topic. For a particular topic, the relevance indicator for the topic indicates the degree of relevance (or a relevancy score) of the multimedia document to the particular topic. For example, as shown in , the number of bars displayed in a relevance indicator associated with a particular topic indicates the degree of relevance of the multimedia document to the particular topic. Accordingly, the multimedia document displayed in GUI  is most relevant to user topic \u201cEnergy Crisis\u201d (as indicated by four bars) and least relevant to user topic \u201cNepal\u201d (indicated by one bar). Various other techniques (e.g., relevance scores, bar graphs, different colors, etc.) may also be used to indicate the degree of relevance of each topic to the multimedia document.","According to an embodiment of the present invention, the relevancy score for a particular topic may be calculated based upon the frequency of occurrences of the words and phrases associated with the particular topic in the multimedia information. Probability values associated with the words or phrases associated with the particular topic may also be used to calculate the relevancy score for the particular topic. Various techniques known to those skilled in the art may also be used to determine relevancy scores for user specified topics of interest based upon the frequency of occurrences of words and phrases associated with a topic in the multimedia information and the probability values associated with the words or phrases. Various other techniques known to those skilled in the art may also be used to calculate the degree of relevancy of the multimedia document to the topics of interest.","As previously stated, a relevance indicator is used to display the degree or relevancy or relevancy score to the user. Based upon information displayed by the relevance indicator, a user can easily determine relevance of multimedia information stored by a multimedia document to topics that may be specified by the user.",{"@attributes":{"id":"p-0137","num":"0141"},"figref":["FIG. 9","FIG. 9"],"b":["900","900","302","900"]},"A user may specify a topic of interest in field . A label identifying the topic of interest can be specified in field . The label specified in field  is displayed in the GUI generated according to the teachings of the present invention to identify the topic of interest. A list of keywords and\/or phrases associated with the topic specified in field  is displayed in area . A user may add new keywords to the list, modify one or more keywords in the list, or remove one or more keywords from the list of keywords associated with the topic of interest. The user may specify new keywords or phrases to be associated with the topic of interest in field . Selection of \u201cAdd\u201d button  adds the keywords or phrases specified in field  to the list of keywords previously associated with a topic. The user may specify a color to be used for annotating or highlighting information relevant to the topic of interest by selecting the color in area . For example, in the embodiment depicted in , locations in the multimedia document related to \u201cAssistive Technology\u201d will be annotated or highlighted in blue color.","According to the teachings of the present invention, various different types of information included in multimedia information may be displayed by the GUI generated by server .  depicts a simplified user interface  that displays multimedia information stored by a meeting recording according to an embodiment of the present invention. It should be apparent that GUI  depicted in  is merely illustrative of an embodiment incorporating the present invention and does not limit the scope of the invention as recited in the claims. One of ordinary skill in the art would recognize other variations, modifications, and alternatives.","The multimedia information stored by the meeting recording may comprise video information, audio information and possibly CC text information, slides information, and other type of information. The slides information may comprise information related to slides (e.g., a PowerPoint presentation slides) presented during the meeting. For example, slides information may comprise images of slides presented at the meeting. As shown in , second viewing area  comprises three thumbnail images -, -, and -. Text information (e.g., CC text information, a transcript of audio information included in the meeting recording, or a text representation of some other type of information included in the meeting recording) extracted from the meeting recording multimedia information is displayed in thumbnail image -. Video keyframes extracted from the video information included in the meeting recording multimedia information are displayed in thumbnail image -. Slides extracted from the slides information included in the multimedia information are displayed in thumbnail image -. The thumbnail images are temporally aligned with one another. The information displayed in thumbnail image - provides additional context for the video and text information in that, the user can view presentation slides that were presented at various times throughout the meeting recording.","Third viewing area  comprises three panels -, -, and -. Panel - displays text information corresponding to the section of thumbnail image - emphasized or covered by thumbnail viewing area lens . Panel - displays video keyframes corresponding to the section of thumbnail image - emphasized or covered by thumbnail viewing area lens . Panel - displays one or more slides corresponding to the section of thumbnail image - emphasized or covered by thumbnail viewing area lens . The panels are temporally aligned with one another.","Fourth viewing area  comprises three sub-viewing areas -, -, and -. Sub viewing area - displays video information corresponding to the section of panel - covered by panel viewing area lens . As described above, sub-viewing area - may display a keyframe corresponding to the emphasized portion of panel -. Alternatively, video based upon the position of panel viewing area lens  may be played back in area -. According to an embodiment of the present invention, time tassociated with lens  is used as the start time for playing the video in area - of fourth viewing area . A panoramic shot  of the meeting room (which may be recorded using a 360 degrees camera) is also displayed in area - of fourth viewing area . Text information emphasized by panel viewing area lens  in panel - is displayed in area - of fourth viewing area . One or more slides emphasized by panel viewing area lens  in panel - are displayed in area - of fourth viewing area . According to an embodiment of the present invention, the user may also select a particular slide from panel - by clicking on the slide. The selected slide is then displayed in area - of fourth viewing area .","According to an embodiment of the present invention, the user can specify the types of information included in the multimedia document that are to be displayed in the GUI. For example, the user can turn on or off slides related information (i.e., information displayed in thumbnail -, panel -, and area - of fourth viewing area ) displayed in GUI  by selecting or deselecting \u201cSlides\u201d button . If a user deselects slides information, then thumbnail - and panel - are not displayed by GUI . Thumbnail - and panel - are displayed by GUI  if the user selects button . Button  thus acts as a switch for displaying or not displaying slides information. In a similar manner, the user can also control other types of information displayed by a GUI generated according to the teachings of the present invention. For example, features may be provided for turning on or off video information, text information, and other types of information that may be displayed by GUI .",{"@attributes":{"id":"p-0144","num":"0148"},"figref":["FIG. 11","FIG. 11"],"b":["1100","1100"]},"The multimedia document whose contents are displayed in GUI  comprises video information, audio information or CC text information, slides information, and whiteboard information. The whiteboard information may comprise images of text and drawings drawn on a whiteboard. As shown in , second viewing area  comprises four thumbnail images -, -, -, and -. Text information (e.g., CC text information, or a transcript of audio information included in the meeting recording, or a text representation of some other type of information included in the multimedia information) extracted from the multimedia document is displayed in thumbnail image -. Video keyframes extracted from the video information included in the multimedia document are displayed in thumbnail image -. Slides extracted from the slides information included in the multimedia information are displayed in thumbnail image -. Whiteboard images extracted from the whiteboard information included in the multimedia document are displayed in thumbnail image -. The thumbnail images are temporally aligned with one another.","Third viewing area  comprises four panels -, -, -, and -. Panel - displays text information corresponding to the section of thumbnail image - emphasized or covered by thumbnail viewing area lens . Panel - displays video keyframes corresponding to the section of thumbnail image - emphasized or covered by thumbnail viewing area lens . Panel - displays one or more slides corresponding to the section of thumbnail image - emphasized or covered by thumbnail viewing area lens . Panel - displays one or more whiteboard images corresponding to the section of thumbnail image - emphasized or covered by thumbnail viewing area lens . The panels are temporally aligned with one another.","Fourth viewing area  comprises three sub-viewing areas -, -, and -. Area - displays video information corresponding to the section of panel - covered by panel viewing area lens . As described above, sub-viewing area - may display a keyframe or play back video corresponding to the emphasized portion of panel -. According to an embodiment of the present invention, time t(as described above) associated with lens  is used as the start time for playing the video in area - of fourth viewing area . A panoramic shot  of the location where the multimedia document was recorded (which may be recorded using a 360 degrees camera) is also displayed in area - of fourth viewing area . Text information emphasized by panel viewing area lens  in panel - is displayed in area - of fourth viewing area . Slides emphasized by panel viewing area lens  in panel - or whiteboard images emphasized by panel viewing area lens  in panel - may be displayed in area - of fourth viewing area . In the embodiment depicted in , a whiteboard image corresponding to the section of panel - covered by panel viewing area lens  is displayed in area -. According to an embodiment of the present invention, the user may also select a particular slide from panel - or select a particular whiteboard image from panel - by clicking on the slide or whiteboard image. The selected slide or whiteboard image is then displayed in area - of fourth viewing area .","As described above, according to an embodiment of the present invention, the user can specify the types of information from the multimedia document that are to be displayed in the GUI. For example, the user can turn on or off a particular type of information displayed by the GUI. \u201cWB\u201d button  allows the user to turn on or off whiteboard related information (i.e., information displayed in thumbnail image -, panel -, and area - of fourth viewing area ) displayed in GUI .",{"@attributes":{"id":"p-0149","num":"0153"},"figref":["FIG. 12","FIG. 12"],"b":["1200","1200"]},"As depicted in , preview areas  and  are provided at the top and bottom of third viewing area . In this embodiment, panel viewing area lens  can be moved along third viewing area  between edge  of preview area  and edge  of preview area . Preview areas  and  allow the user to preview the contents displayed in third viewing area  when the user scrolls the multimedia document using panel viewing area lens . For example, as the user is scrolling down the multimedia document using panel viewing area lens , the user can see upcoming contents in preview area  and see the contents leaving third viewing area  in preview area . If the user is scrolling up the multimedia document using panel viewing area lens , the user can see upcoming contents in preview area  and see the contents leaving third viewing area  in preview area . According to an embodiment of the present invention, the size (or length) of each preview region can be changed and customized by the user. For example, in GUI  depicted in , a handle  is provided that can be used by the user to change the size of preview region . According to an embodiment of the present invention, preview areas may also be provided in second viewing area .",{"@attributes":{"id":"p-0151","num":"0155"},"figref":["FIG. 13","FIG. 13"],"b":["1300","1300"]},"As depicted in , text information is displayed in panel - of third viewing area  in compressed format, i.e., the white spaces between the text lines have been removed. This enhances the readability of the text information. The lines of text displayed in panel - are then used to determine the video frames to be displayed in panel -. According to an embodiment of the present invention, a timestamp is associated with each line of text displayed in panel -. The timestamp associated with a line of text represents the time when the text occurred in the multimedia document being displayed by GUI . In one embodiment, the timestamp associated with a line of text corresponds to the timestamp associated with the first word in the line of text. The lines of text displayed in panel - are then grouped into groups, with each group comprising a pre-determined number of lines.","Video keyframes are then extracted from the video information stored by the multimedia document for each group of lines depending on time stamps associated with lines in the group. According to an embodiment of the present invention, server  determines a start time and an end time associated with each group of lines. A start time for a group corresponds to a time associated with the first (or earliest) line in the group while an end time for a group corresponds to the time associated with the last line (or latest) line in the group. In order to determine keyframes to be displayed in panel - corresponding to a particular group of text lines, server  extracts a set of one or more video keyframes from the portion of the video information occurring between the start and end time associated with the particular group. One or more keyframes are then selected from the extracted set of video keyframes to be displayed in panel - for the particular group. The one or more selected keyframes are then displayed in panel - proximal to the group of lines displayed in panel - for which the keyframes have been extracted.","For example, in , the lines displayed in panel - are divided into groups wherein each group comprises 4 lines of text. For each group, the time stamp associated with the first line in the group corresponds to the start time for the group while the time stamp associated with the fourth line in the group corresponds to the end time for the group of lines. Three video keyframes are displayed in panel - for each group of four lines of text displayed in panel - in the embodiment depicted in . According to an embodiment of the present invention, the three video keyframes corresponding to a particular group of lines correspond to the first, middle, and last keyframe from the set of keyframes extracted from the video information between the start and end times of the particular group. As described above, various other techniques may also be used to select the video keyframes that are displayed in panel -. For each group of lines displayed in panel -, the keyframes corresponding to the group of lines are displayed such that the keyframes are temporally aligned with the group of lines. In the embodiment depicted in , the height of keyframes for a group of lines is approximately equal to the vertical height of the group of lines.","The number of text lines to be included in a group is user configurable. Likewise, the number of video keyframes to be extracted for a particular group of lines is also user configurable. Further, the video keyframes to be displayed in panel - for each group of lines can also be configured by the user of the present invention.","The manner in which the extracted keyframes are displayed in panel - is also user configurable. Different techniques may be used to show the relationships between a particular group of lines and video keyframes displayed for the particular group of lines. For example, according to an embodiment of the present invention, a particular group of lines displayed in panel - and the corresponding video keyframes displayed in panel - may be color-coded or displayed using the same color to show the relationship. Various other techniques known to those skilled in the art may also be used to show the relationships.","GUI Generation Technique According to an Embodiment of the Present Invention","The following section describes techniques for generating a GUI (e.g., GUI  depicted in ) according to an embodiment of the present invention. For purposes of simplicity, it is assumed that the multimedia information to be displayed in the GUI comprises video information, audio information, and CC text information. The task of generating GUI  can be broken down into the following tasks: (a) displaying thumbnail - displaying text information extracted from the multimedia information in second viewing area ; (b) displaying thumbnail - displaying video keyframes extracted from the video information included in the multimedia information; (c) displaying thumbnail viewing area lens  emphasizing a portion of second viewing area  and displaying information corresponding to the emphasized portion of second viewing area  in third viewing area , and displaying panel viewing area lens  emphasizing a portion of third viewing area  and displaying information corresponding to the emphasized portion of third viewing area  in fourth viewing area ; and (d) displaying information in fifth viewing area .",{"@attributes":{"id":"p-0159","num":"0163"},"figref":["FIG. 14","FIG. 14","FIG. 14"],"b":["1400","312","1","304","104","102","104","102","104","102","104","102","104"]},"As depicted in , the method is initiated when server  accesses multimedia information to be displayed in the GUI (step ). As previously stated, the multimedia information may be stored in a multimedia document accessible to server . As part of step , server  may receive information (e.g., a filename of the multimedia document) identifying the multimedia document and the location (e.g., a directory path) of the multimedia document. A user of the present invention may provide the multimedia document identification information. Server  may then access the multimedia document based upon the provided information. Alternatively, server  may receive the multimedia information to be displayed in the GUI in the form of a streaming media signal, a cable signal, etc. from a multimedia information source. Server system  may then store the multimedia information signals in a multimedia document and then use the stored document to generate the GUI according to the teachings of the present invention.","Server  then extracts text information from the multimedia information accessed in step  (step ). If the multimedia information accessed in step  comprises CC text information, then the text information corresponds to CC text information that is extracted from the multimedia information. If the multimedia information accessed in step  does not comprise CC text information, then in step , the audio information included in the multimedia information accessed in step  is transcribed to generate a text transcript for the audio information. The text transcript represents the text information extracted in step . The text information extracted in step  may also be a text representation of some other type of information included in the multimedia information.","The text information determined in step  comprises a collection of lines with each line comprising one or more words. Each word has a timestamp associated with it indicating the time of occurrence of the word in the multimedia information. The timestamp information for each word is included in the CC text information. Alternatively, if the text represents a transcription of audio information, the timestamp information for each word may be determined during the audio transcription process. Alternatively, if the text information represents a text representation of some other type of information included in the multimedia information, then the time stamp associated with the other type of information may be determined.","As part of step , each line is assigned a start time and an end time based upon words that are included in the line. The start time for a line corresponds to the timestamp associated with the first word occurring in the line, and the end time for a line corresponds to the timestamp associated with the last word occurring in the line.","The text information determined in step , including the timing information, is then stored in a memory location accessible to server  (step ). In one embodiment, a data structure (or memory structure) comprising a linked list of line objects is used to store the text information. Each line object comprises a linked list of words contained in the line. Timestamp information associated with the words and the lines is also stored in the data structure. The information stored in the data structure is then used to generate GUI .","Server  then determines a length or height (in pixels) of a panel (hereinafter referred to as \u201cthe text canvas\u201d) for drawing the text information (step ). In order to determine the length of the text canvas, the duration (\u201cduration\u201d) of the multimedia information (or the duration of the multimedia document storing the multimedia document) in seconds is determined. A vertical pixels-per-second of time (\u201cpps\u201d) value is also defined. The \u201cpps\u201d determines the distance between lines of text drawn in the text canvas. The value of pps thus depends on how close the user wants the lines of text to be to each other when displayed and upon the size of the font to be used for displaying the text. According to an embodiment of the present invention, a 5 pps value is specified with a 6 point font. The overall height (in pixels) of the text canvas (\u201ctextCanvasHeight\u201d) is determined as follows:\n\ntextCanvasHeight=duration*\n\nFor example, if the duration of the multimedia information is 1 hour (i.e., 3600 seconds) and for apps value of 5, the height of the text canvas (textCanvasHeight) is 18000 pixels (3600*5).\n","Multipliers are then calculated for converting pixel locations in the text canvas to seconds and for converting seconds to pixels locations in the text canvas (step ). A multiplier \u201cpix_m\u201d is calculated for converting a given time value (in seconds) to a particular vertical pixel location in the text canvas. The pix_m multiplier can be used to determine a pixel location in the text canvas corresponding to a particular time value. The value of pix_m is determined as follows:\n\n=textCanvasHeight\/duration\n\nFor example, if duration=3600 seconds and textCanvasHeight=18000 pixels, then pix_m=18000\/3600=5.\n","A multiplier \u201csec_m\u201d is calculated for converting a particular pixel location in the text canvas to a corresponding time value. The sec_m multiplier can be used to determine a time value for a particular pixel location in the text canvas. The value of sec_m is determined as follows:\n\n=duration\/textCanvasHeight\n\nFor example, if duration=3600 seconds and textCanvasHeight=18000 pixels, then sec_m=3600\/18000=0.2.\n","The multipliers calculated in step  may then be used to convert pixels to seconds and seconds to pixels. For example, the pixel location in the text canvas of an event occurring at time t=1256 seconds in the multimedia information is: 1256*pix_m=1256*5=6280 pixels from the top of the text canvas. The number of seconds corresponding to a pixel location p=231 in the text canvas is: 231*sec_m=231*0.2=46.2 seconds.","Based upon the height of the text canvas determined in step  and the multipliers generated in step , positional coordinates (horizontal (X) and vertical (Y) coordinates) are then calculated for words in the text information extracted in step  (step ). As previously stated, information related to words and lines and their associated timestamps may be stored in a data structure accessible to server . The positional coordinate values calculated for each word might also be stored in the data structure.","The Y (or vertical) coordinate (W) for a word is calculated by multiplying the timestamp (W) (in seconds) associated with the word by multiplier pix_m determined in step . Accordingly:\n\n(in pixels)=\n\nFor example, if a particular word has W=539 seconds (i.e., the words occurs 539 seconds into the multimedia information), then W=539*5=2695 vertical pixels from the top of the text canvas.\n","The X (or horizontal) coordinate (W) for a word is calculated based upon the word's location in the line and the width of the previous words in the line. For example if a particular line (L) has four words, i.e., L: WWWW, then\n\nof =0\n\nof =(of )+(Width of )+(Spacing between words)\n\nof =(of )+(Width of )+(Spacing between words)\n\nof =(of )+(Width of )+(Spacing between words)\n","The words in the text information are then drawn on the text canvas in a location determined by the X and Y coordinates calculated for the words in step  (step ).","Server  then determines a height of thumbnail - that displays text information in second viewing area  of GUI  (step ). The height of thumbnail - (ThumbnailHeight) depends on the height of the GUI window used to displaying the multimedia information and the height of second viewing area  within the GUI window. The value of ThumbnailHeight is set such that thumbnail - fits in the GUI in the second viewing area .","Thumbnail - is then generated by scaling the text canvas such that the height of thumbnail - is equal to ThumbnailHeight and the thumbnail fits entirely within the size constraints of second viewing area  (step ). Thumbnail -, which represents a scaled version of the text canvas, is then displayed in second viewing area  of GUI  (step ).","Multipliers are then calculated for converting pixel locations in thumbnail - to seconds and for converting seconds to pixel locations in thumbnail - (step ). A multiplier \u201ctpix_m\u201d is calculated for converting a given time value (in seconds) to a particular pixel location in thumbnail -. Multiplier tpix_m can be used to determine a pixel location in the thumbnail corresponding to a particular time value. The value of tpix_M is determined as follows:\n\n=ThumbnailHeight\/duration\n\nFor example, if duration=3600 seconds and ThumbnailHeight=900, then tpix_m=900\/3600=0.25\n","A multiplier \u201ctsec_m\u201d is calculated for converting a particular pixel location in thumbnail - to a corresponding time value. Multiplier tsec_m can be used to determine a time value for a particular pixel location in thumbnail -. The value of tsec_m is determined as follows:\n\n=duration\/ThumbnailHeight\n\nFor example, if duration=3600 seconds and ThumbnailHeight=900, then tsec_m=3600\/900=4.\n","Multipliers tpix_m and tsec_m may then be used to convert pixels to seconds and seconds to pixels in thumbnail -. For example, the pixel location in thumbnail - of a word occurring at time t=1256 seconds in the multimedia information is: 1256*tpixm=1256*0.25=314 pixels from the top of thumbnail -. The number of seconds represented by a pixel location p=231 in thumbnail - is: 231*tsec_m=231*4=924 seconds.",{"@attributes":{"id":"p-0178","num":"0182"},"figref":["FIG. 15","FIG. 15","FIG. 15"],"b":["1500","312","2","304","300","104","102","104","102","104","102","104","102","104"]},"For purposes of simplicity, it is assumed that thumbnail - displaying text information has already been displayed according to the flowchart depicted in . As depicted in , server  extracts a set of keyframes from the video information included in the multimedia information (step ). The video keyframes may be extracted from the video information by sampling the video information at a particular sampling rate. According to an embodiment of the present invention, keyframes are extracted from the video information at a sampling rate of 1 frame per second. Accordingly, if the duration of the multimedia information is 1 hour (3600 seconds), then 3600 video keyframes are extracted from the video information in step . A timestamp is associated with each keyframe extracted in step  indicating the time of occurrence of the keyframe in the multimedia information.","The video keyframes extracted in step  and their associated timestamp information is stored in a data structure (or memory structure) accessible to server  (step ). The information stored in the data structure is then used for generating thumbnail -.","The video keyframes extracted in step  are then divided into groups (step ). A user-configurable time period (\u201cgroupTime\u201d) is used to divide the keyframes into groups. According to an embodiment of the present invention, groupTime is set to 8 seconds. In this embodiment, each group comprises video keyframes extracted within an 8 second time period window. For example, if the duration of the multimedia information is 1 hour (3600 seconds) and 3600 video keyframes are extracted from the video information using a sampling rate of 1 frame per second, then if groupTime is set to 8 seconds, the 3600 keyframes will be divided into 450 groups, with each group comprising 8 video keyframes.","A start and an end time are calculated for each group of frames (step ). For a particular group of frames, the start time for the particular group is the timestamp associated with the first (i.e., the keyframe in the group with the earliest timestamp) video keyframe in the group, and the end time for the particular group is the timestamp associated with the last (i.e., the keyframe in the group with the latest timestamp) video keyframe in the group.","For each group of keyframes, server  determines a segment of pixels on a keyframe canvas for drawing one or more keyframes from the group of keyframes (step ). Similar to the text canvas, the keyframe canvas is a panel on which keyframes extracted from the video information are drawn. The height of the keyframe canvas (\u201ckeyframeCanvasHeight\u201d) is the same as the height of the text canvas (\u201ctextCanvasHeight\u201d) described above (i.e., keyframeCanvasHeight==textCanvasHeight). As a result, multipliers pix_m and sec_m (described above) may be used to convert a time value to a pixel location in the keyframe canvas and to convert a particular pixel location in the keyframe canvas to a time value.","The segment of pixels on the keyframe canvas for drawing keyframes from a particular group is calculated based upon the start time and end time associated with the particular group. The starting vertical (Y) pixel coordinate (\u201csegmentStart\u201d) and the end vertical (Y) coordinate (\u201csegmentEnd\u201d) of the segment of pixels in the keyframe canvas for a particular group of keyframes is calculated as follows:\n\nsegmentStart=(Start time of group)*\n\nsegmentEnd=(End time of group)*\n\nAccordingly, the height of each segment (\u201csegmentHeight\u201d) in pixels of the text canvas is:\n\nsegmentHeight=segmentEnd\u2212segmentStart\n","The number of keyframes from each group of frames to be drawn in each segment of pixels on the text canvas is then determined (step ). The number of keyframes to be drawn on the keyframe canvas for a particular group depends on the height of the segment (\u201csegmentHeight\u201d) corresponding to the particular group. If the value of segmentHeight is small only a small number of keyframes may be drawn in the segment such that the drawn keyframes are comprehensible to the user when displayed in the GUI. The value of segmentHeight depends on the value of pps. If pps is small, then segmentHeight will also be small. Accordingly, a larger value of pps may be selected if more keyframes are to be drawn per segment.","According to an embodiment of the present invention, if the segmentHeight is equal to 40 pixels and each group of keyframes comprises 8 keyframes, then 6 out of the 8 keyframes may be drawn in each segment on the text canvas. The number of keyframes to be drawn in a segment is generally the same for all groups of keyframes. for example, in the embodiment depicted in , six keyframes are drawn in each segment on the text canvas.","After determining the number of keyframes to be drawn in each segment of the text canvas, for each group of keyframes, server  identifies one or more keyframes from keyframes in the group of keyframes to be drawn on the keyframe canvas (step ). Various different techniques may be used for selecting the video keyframes to be displayed in a segment for a particular group of frames. According to one technique, if each group of video keyframes comprises 8 keyframes and if 6 video keyframes are to be displayed in each segment on the keyframe canvas, then server  may select the first two video keyframes, the middle two video keyframes, and the last two video keyframes from each group of video keyframes be drawn on the keyframe canvas. As described above, various other techniques may also be used to select one or more keyframes to display from the group of keyframes. For example, the keyframes may be selected based upon the sequential positions of the keyframes in the group of keyframes, based upon time values associated with the keyframes, or based upon other criteria.","According to another technique, server  may use special image processing techniques to determine similarity or dissimilarity between keyframes in each group of keyframes. If six video keyframes are to be displayed from each group, server  may then select six keyframes from each group of keyframes based upon the results of the image processing techniques. According to an embodiment of the present invention, the six most dissimilar keyframes in each group may be selected to be drawn on the keyframe canvas. It should be apparent that various other techniques known to those skilled in the art may also be used to perform the selection of video keyframes.","Keyframes from the groups of keyframes identified in step  are then drawn on the keyframe canvas in their corresponding segments (step ). Various different formats may be used for drawing the selected keyframes in a particular segment. For example, as shown in , for each segment, the selected keyframes may be laid out left-to-right and top-to-bottom in rows of 3 frames. Various other formats known to those skilled in the art may also be used to draw the keyframes on the keyframe canvas. The size of each individual keyframe drawn on the keyframe canvas depends on the height (segmentHeight) of the segment in which the keyframe is drawn and the number of keyframes to be drawn in the segment. As previously stated, the height of a segment depends on the value of pps. Accordingly, the size of each individual keyframe drawn on the keyframe canvas also depends on the value of pps.","Server  then determines a height (or length) of thumbnail - that displays the video keyframes in GUI  (step ). According to the teachings of the present invention, the height of thumbnail - is set to be the same as the height of thumbnail - that displays text information (i.e., the height of thumbnail - is set to ThumbnailHeight).","Thumbnail - is then generated by scaling the keyframe canvas such that the height of thumbnail - is equal to ThumbnailHeight and thumbnail - fits entirely within the size constraints of second viewing area  (step ). Thumbnail -, which represents a scaled version of the keyframe canvas, is then displayed in second viewing area  of GUI  (step ). Thumbnail - is displayed in GUI  next to thumbnail image - and is temporally aligned or synchronized with thumbnail - (as shown in ). Accordingly, the top of thumbnail - is aligned with the top of thumbnail -.","Multipliers are calculated for thumbnail - for converting pixel locations in thumbnail - to seconds and for converting seconds to pixel locations in thumbnail - (step ). Since thumbnail - is the same length as thumbnail - and is aligned with thumbnail -, multipliers \u201ctpix_m\u201d and \u201ctsec_m\u201d calculated for thumbnail - can also be used for thumbnail -. These multipliers may then be used to convert pixels to seconds and seconds to pixels in thumbnail -.","According to the method displayed in , the size of each individual video keyframe displayed in thumbnail - depends, in addition to other criteria, on the length of thumbnail - and on the length of the video information. Assuming that the length of thumbnail - is fixed, the height of each individual video keyframe displayed in thumbnail - is inversely proportional to the length of the video information. Accordingly, as the length of the video information increases, the size of each keyframe displayed in thumbnail - decreases. As a result, for longer multimedia documents, the size of each keyframe may become so small that the video keyframes displayed in thumbnail - are no longer recognizable by the user. To avoid this, various techniques may be used to display the video keyframes in thumbnail - in a manner that makes thumbnail - more readable and recognizable by the user.",{"@attributes":{"id":"p-0194","num":"0198"},"figref":["FIG. 16","FIG. 16","FIG. 16","FIG. 16"],"b":["1600","312","2","312","2","312","2","104","102","104","102","104","102","104","102","104"]},"As depicted in , steps , , , and  are the same as steps , , , and , depicted in  and explained above. After step , one or more groups whose video keyframes are to be drawn in the keyframe canvas are then selected from the groups determined in step  (step ). Various different techniques may be used to select the groups in step . According to one technique, the groups determined in step  are selected based upon a \u201cSkipCount\u201d value that is user-configurable. For example, if SkipCount is set to 4, then every fifth group (i.e., 4 groups are skipped) is selected in step . The value of SkipCount may be adjusted based upon the length of the multimedia information. According to an embodiment of the present invention, the value of SkipCount is directly proportional to the length of the multimedia information, i.e., SkipCount is set to a higher value for longer multimedia documents.","For each group selected in step , server  identifies one or more keyframes from the group to be drawn on the keyframe canvas (step ). As described above, various techniques may be used to select keyframes to be drawn on the keyframe canvas.","The keyframe canvas is then divided into a number of equal-sized row portions, where the number of row portions is equal to the number of groups selected in step  (step ). According to an embodiment of the present invention, the height of each row portion is approximately equal to the height of the keyframe canvas (\u201ckeyframeCanvasHeight\u201d) divided by the number of groups selected in step .","For each group selected in step , a row portion of the keyframe canvas is then identified for drawing one or more video keyframes from the group (step ). According to an embodiment of the present invention, row portions are associated with groups in chronological order. For example, the first row is associated with a group with the earliest start time, the second row is associated with a group with the second earliest start time, and so on.","For each group selected in step , one or more keyframes from the group (identified in step ) are then drawn on the keyframe canvas in the row portion determined for the group in step  (step ). The sizes of the selected keyframes for each group are scaled to fit the row portion of the keyframe canvas. According to an embodiment of the present invention, the height of each row portion is more than the heights of the selected keyframes, and height of the selected keyframes is increased to fit the row portion. This increases the size of the selected keyframes and makes them more visible when drawn on the keyframe canvas. In this manner, keyframes from the groups selected in step  are drawn on the keyframe canvas.","The keyframe canvas is then scaled to form thumbnail - that is displayed in second viewing area  according to steps , , and . Since the height of the keyframes drawn on the keyframe canvas is increased according to an embodiment of the present invention, as described above, the keyframes are also more recognizable when displayed in thumbnail -. Multipliers are then calculated according to step . Steps , , , and  are similar to steps , , , and , depicted in  and explained above. As described above, by selecting a subset of the groups, the number of keyframes to be drawn on the keyframe canvas and displayed in thumbnail - is reduced. This is turn increases the height of each individual video keyframe displayed in thumbnail - thus making them more recognizable when displayed.",{"@attributes":{"id":"p-0201","num":"0205"},"figref":["FIG. 17","FIG. 17","FIG. 17"],"b":["1700","314","314","306","322","322","308","310","104","102","104","102","104","102","104","102","104"]},"As depicted in , server  first determines a height (in pixels) of each panel (\u201cPanelHeight\u201d) to be displayed in third viewing area  of GUI  (step ). The value of PanelHeight depends on the height (or length) of third viewing area . Since the panels are to be aligned to each other, the height of each panel is set to PanelHeight. According to an embodiment of the present invention, PanelHeight is set to the same value as ThumbnailHeight. However, in alternative embodiments of the present invention, the value of PanelHeight may be different from the value of ThumbnailHeight.","A section of the text canvas (generated in the flowchart depicted in ) equal to PanelHeight is then identified (step ). The section of the text canvas identified in step  is characterized by vertical pixel coordinate (P) marking the starting pixel location of the section, and a vertical pixel coordinate (P) marking the ending pixel location of the section.","Time values corresponding to the boundaries of the section of the text canvas identified in step  (marked by pixel locations Pand P) are then determined (step ). The multiplier sec_m is used to calculate the corresponding time values. A time t(in seconds) corresponding to pixel location Pis calculated as follows:\n\n\n\nA time t(in seconds) corresponding to pixel location Pis calculated as follows:\n\n\n","A section of the keyframe canvas corresponding to the selected section of the text canvas is then identified (step ). Since the height of the keyframe canvas is the same as the height of the keyframe canvas, the selected section of the keyframe canvas also lies between pixels locations Pand Pin the keyframe canvas corresponding to times tand t.","The portion of the text canvas identified in step  is displayed in panel - in third viewing area  (step ). The portion of the keyframe canvas identified in step  is displayed in panel - in third viewing area  (step ).","A panel viewing area lens  is displayed covering a section of third viewing area  (step ). Panel viewing area lens  is displayed such that it emphasizes or covers a section of panel - panel and - displayed in third viewing area  between times tand twhere (t\u2266t<t\u2266t). The top edge of panel viewing area lens  corresponds to time tand the bottom edge of panel viewing area lens  corresponds to time t. The height of panel viewing area lens  (expressed in pixels) is equal to: (Vertical pixel location in the text canvas corresponding to t)\u2212(Vertical pixel location in the text canvas corresponding to t). The width of panel viewing area lens  is approximately equal to the width of third viewing area  (as shown in ).","A portion of thumbnail - corresponding to the section of text canvas displayed in panel - and a portion of thumbnail - corresponding to the section of keyframe canvas displayed in panel - are then determined (step ). The portion of thumbnail - corresponding to the section of the text canvas displayed in panel - is characterized by vertical pixel coordinate (TN) marking the starting pixel location of the thumbnail portion, and a vertical pixel coordinate (TN) marking the ending pixel location of the thumbnail portion. The multiplier tpix_m is used to determine pixel locations TNand TNas follows:\n\n\n\n\n\nSince thumbnails - and - are of the same length and are temporally aligned to one another, the portion of thumbnail - corresponding to the sections of keyframe canvas displayed in panel - also lies between pixel locations TNand TNon thumbnail -.\n","Thumbnail viewing area lens  is then displayed covering portions of thumbnails - and - corresponding to the section of text canvas displayed in panel - and the section of keyframe canvas displayed in panel - (step ). Thumbnail viewing area lens  is displayed covering portions of thumbnails - and - between pixels locations TNand TNof the thumbnails. The height of thumbnail viewing area lens  in pixels is equal to (TN\u2212TN). The width of thumbnail viewing area lens  is approximately equal to the width of second viewing area  (as shown in ).","A portion of second viewing area  corresponding to the section of third viewing area  emphasized by panel viewing area lens  is then determined (step ). In step , server  determines a portion of thumbnail - and a portion of thumbnail - corresponding to the time period between tand t. The portion of thumbnail - corresponding to the time window between tand tis characterized by vertical pixel coordinate (TNSub) corresponding to time tand marking the starting vertical pixel of the thumbnail portion, and a vertical pixel coordinate (TNSub) corresponding to time tand marking the ending vertical pixel location of the thumbnail portion. Multiplier tpix_m is used to determine pixel locations TNSuband TNSubas follows:\n\n\n\n\n\nSince thumbnails - and - are of the same length and are temporally aligned to one another, the portion of thumbnail - corresponding to the time period between tand talso lies between pixel locations TNSuband TNSubon thumbnail -.\n","Sub-lens  is then displayed covering portions of thumbnails - and - corresponding to the time window between tand t(i.e., corresponding to the portion of third viewing area  emphasized by panel viewing area lens ) (step ). Sub-lens  is displayed covering portions of thumbnails - and - between pixels locations TNSuband TNSub. The height of sub-lens  in pixels is equal to (TNSub\u2212TNSub). The width of sub-lens  is approximately equal to the width of second viewing area  (as shown in ).","Multimedia information corresponding to the portion of third viewing area  emphasized by panel viewing area lens  is displayed in fourth viewing area  (step ). For example, video information starting at time tis played back in area - of fourth viewing area  in GUI . In alternative embodiments, the starting time of the video playback may be set to any time between and including tand t. Text information corresponding to the time window between tand tis displayed in area - of fourth viewing area .","The multimedia information may then be analyzed and the results of the analysis are displayed in fifth viewing area  (step ). For example, the text information extracted from the multimedia information may be analyzed to identify words that occur in the text information and the frequency of individual words. The words and their frequency may be printed in fifth viewing area  (e.g., information printed in area  of fifth viewing area  as shown in ). As previously described, information extracted from the multimedia information may be stored in data structures accessible to server . For example, text information and video keyframes information extracted from the multimedia information may be stored in one or more data structures accessible to server . Server  may use the information stored in these data structures to analyze the multimedia information.","Multimedia Information Navigation","As previously described, a user of the present invention may navigate and scroll through the multimedia information stored by a multimedia document and displayed in GUI  using thumbnail viewing area lens  and panel viewing area lens . For example, the user can change the location of thumbnail viewing area lens  by moving thumbnail viewing area lens  along the length of second viewing area . In response to a change in the position of thumbnail viewing area lens  from a first location in second viewing area  to a second location along second viewing area , the multimedia information displayed in third viewing area  is automatically updated such that the multimedia information displayed in third viewing area  continues to correspond to the area of second viewing area  emphasized by thumbnail viewing area lens  in the second location.","Likewise, the user can change the location of panel viewing area lens  by moving panel viewing area lens  along the length of third viewing area . In response to a change in the location of panel viewing area lens , the position of sub-lens  and also possibly thumbnail viewing area lens  are updated to continue to correspond to new location of panel viewing area lens . The information displayed in fourth viewing area  is also updated to correspond to the new location of panel viewing area lens .",{"@attributes":{"id":"p-0217","num":"0221"},"figref":["FIG. 18","FIG. 18","FIG. 18"],"b":["1800","306","314","104","102","104","102","104","102","104","102","104"]},"As depicted in , the method is initiated when server  detects a change in the position of thumbnail viewing area lens  from a first position to a second position over second viewing area  (step ). Server  then determines a portion of second viewing area  emphasized by thumbnail viewing area lens  in the second position (step ). As part of step , server  determines pixel locations (TNand TN) in thumbnail - corresponding to the edges of thumbnail viewing area lens  in the second position. TNmarks the starting vertical pixel location in thumbnail -, and TNmarks the ending vertical pixel location in thumbnail -. Since thumbnails - and - are of the same length and are temporally aligned to one another, the portion of thumbnail - corresponding to second position of thumbnail viewing area lens  also lies between pixel locations TNand TN.","Server  then determines time values corresponding to the second position of thumbnail viewing area lens  (step ). A time value tis determined corresponding to pixel location TNand a time value tis determined corresponding to pixel location TN. The multiplier tsec_m is used to determine the time values as follows:\n\n\n\n\n","Server  then determines pixel locations in the text canvas and the keyframe canvas corresponding to the time values determined in step  (step ). A pixel location Pin the text canvas is calculated based upon time t, and a pixel location Pin the text canvas is calculated based upon time t. The multiplier pix_m is used to determine the locations as follows:\n\n\n\n\n\nSince the text canvas and the keyframe canvas are of the same length, time values tand tcorrespond to pixel locations Pand Pin the keyframe canvas.\n","A section of the text canvas between pixel locations Pand Pis displayed in panel - (step ). The section of the text canvas displayed in panel - corresponds to the portion of thumbnail - emphasized by thumbnail viewing area lens  in the second position.","A section of the keyframe canvas between pixel locations Pand Pis displayed in panel - (step ). The section of the keyframe canvas displayed in panel - corresponds to the portion of thumbnail - emphasized by thumbnail viewing area lens  in the second position.","When thumbnail viewing area lens  is moved from the first position to the second position, sub-lens  also moves along with thumbnail viewing area lens . Server  then determines a portion of second viewing area  emphasized by sub-lens  in the second position (step ). As part of step , server  determines pixel locations (TNSuband TNSub) in thumbnail - corresponding to the edges of sub-lens  in the second position. TNSubmarks the starting vertical pixel location in thumbnail -, and TNSubmarks the ending vertical pixel location of sub-lens  in thumbnail -. Since thumbnails - and - are of the same length and are temporally aligned to one another, the portion of thumbnail - corresponding to second position of sub-lens  also lies between pixel locations TNSuband TNSub.","Server  then determines time values corresponding to the second position of sub-lens  (step ). A time value tis determined corresponding to pixel location TNSuband a time value tis determined corresponding to pixel location TNSub. The multiplier tsec_m is used to determine the time values as follows:\n\n\n\n\n","Server  then determines pixel locations in the text canvas and the keyframe canvas corresponding to the time values determined in step  (step ). A pixel location PSubin the text canvas is calculated based upon time t, and a pixel location PSubin the text canvas is calculated based upon time t. The multiplier pix_m is used to determine the locations as follows:\n\n\n\n\n\nSince the text canvas and the keyframe canvas are of the same length, time values tand tcorrespond to pixel locations PSuband PSubin the keyframe canvas.\n","Panel viewing area lens  is drawn over third viewing area  covering a portion of third viewing area  between pixels location PSuband PSub(step ). The multimedia information displayed in fourth viewing area  is then updated to correspond to the new position of panel viewing area lens  (step ).",{"@attributes":{"id":"p-0227","num":"0231"},"figref":["FIG. 19","FIG. 19","FIG. 19"],"b":["1900","308","314","316","322","104","102","104","102","104","102","104","102","104"]},"As depicted in , the method is initiated when server . detects a change in the position of panel viewing area lens  from a first position to a second position over third viewing area  (step ). Server  then determines time values corresponding to the second position of panel viewing area lens  (step ). In step , server  determines the pixel locations of the top and bottom edges of panel viewing area lens  in the second position. Multiplier sec_m is then used to covert the pixel locations to time values. A time value tis determined corresponding to top edge of panel viewing area lens  in the second position, and a time value tis determined corresponding to bottom edge of panel viewing area lens .\n\n=(Pixel location of top edge of panel viewing area lens 322)*\n\n=(Pixel location of bottom edge of panel viewing area lens 322)*\n","Server  then determines pixel locations in second viewing area  corresponding to the time values determined in step  (step ). A pixel location TNSubin a thumbnail (either - or - since they aligned and of the same length) in second viewing area  is calculated based upon time t, and a pixel location TNSUbin the thumbnail is calculated based upon time t. The multiplier tpix_m is used to determine the locations as follows:\n\n\n\n\n","Sub-lens  is then updated to emphasize a portion of thumbnails  in second viewing area  between pixel locations determined in step  (step ). As part of step , the position of thumbnail viewing area lens  may also be updated if pixels positions TNSubor TNSublie beyond the boundaries of thumbnail viewing area lens  when panel viewing area lens  was in the first position. For example, if a user uses panel viewing area lens  to scroll third viewing area  beyond the PanelHeight, then the position of thumbnail viewing area lens  is updated accordingly. If the second position of panel viewing area lens  lies within PanelHeight, then only sub-lens  is moved to correspond to the second position of panel viewing area lens  and thumbnail viewing area lens  is not moved.","As described above, panel viewing area lens  may be used to scroll the information displayed in third viewing area . For example, a user may move panel viewing area lens  to the bottom of third viewing area  and cause the contents of third viewing area  to be automatically scrolled upwards. Likewise, the user may move panel viewing area lens  to the top of third viewing area  and cause the contents of third viewing area  to be automatically scrolled downwards. The positions of thumbnail viewing area lens  and sub-lens  are updated as scrolling occurs.","Multimedia information corresponding to the second position of panel viewing area lens  is then displayed in fourth viewing area  (step ). For example, video information corresponding to the second position of panel viewing area lens  is displayed in area - of fourth viewing area  and text information corresponding to the second position of panel viewing area lens  is displayed in area - of third viewing area .","According to an embodiment of the present invention, in step , server  selects a time \u201ct\u201d having a value equal to either tor tor some time value between tand t. Time \u201ct\u201d may be referred to as the \u201clocation time\u201d. The location time may be user-configurable. According to an embodiment of the present invention, the location time is set to t. The location time is then used as the starting time for playing back video information in area - of fourth viewing area .","According to an embodiment of the present invention, GUI  may operate in two modes: a \u201cfull update\u201d mode and a \u201cpartial update\u201d mode. The user of the GUI may select the operation mode of the GUI.","When GUI  is operating in \u201cfull update\u201d mode, the positions of thumbnail viewing area lens  and panel viewing area lens  are automatically updated to reflect the position of the video played back in area - of fourth viewing area . Accordingly, in \u201cfull update\u201d mode, thumbnail viewing area lens  and panel viewing area lens  keep up or reflect the position of the video played in fourth viewing area . The video may be played forwards or backwards using the controls depicted in area  of fourth viewing area , and the positions of thumbnail viewing area lens  and panel viewing area lens  change accordingly. The multimedia information displayed in panels  in third viewing area  is also automatically updated (shifted upwards) to correspond to the position of thumbnail viewing area lens  and reflect the current position of the video.","When GUI  is operating in \u201cpartial update\u201d mode, the positions of thumbnail viewing area lens  and panel viewing area lens  are not updated to reflect the position of the video played back in area - of fourth viewing area . In this mode, the positions of thumbnail viewing area lens  and panel viewing area lens  remain static as the video is played in area - of fourth viewing area . Since the position of thumbnail viewing area lens  does not change, the multimedia information displayed in third viewing area  is also not updated. In this mode, a \u201clocation pointer\u201d may be displayed in second viewing area  and third viewing area  to reflect the current position of the video played back in area - of fourth viewing area . The position of the location pointer is continuously updated to reflect the position of the video.","Ranges","According to an embodiment, the present invention provides techniques for selecting or specifying portions of the multimedia information displayed in the GUI. Each portion is referred to as a \u201crange.\u201d A range may be manually specified by a user of the present invention or may alternatively be automatically selected by the present invention based upon range criteria provided by the user of the invention.","A range refers to a portion of the multimedia information between a start time (R) and an end time (R). Accordingly, each range is characterized by an Rand a Rthat define the time boundaries of the range. A range comprises or identifies a portion of the multimedia information occurring between times Rand Rassociated with the range.",{"@attributes":{"id":"p-0240","num":"0244"},"figref":["FIG. 20A","FIG. 20A"],"b":["2000","2000"]},"As depicted in , GUI  provides various features (buttons, tabs, etc.) that may be used by the user to either manually specify one or more ranges or to configure GUI  to automatically generate ranges. In the embodiment depicted in , the user can manually specify a range by selecting \u201cNew\u201d button . After selecting button , the user can specify a range by selecting a portion of a thumbnail displayed in second viewing area . One or more ranges may be specified by selecting various portions of the thumbnail. For example, in , six ranges -, -, -, -, -, and - have been displayed. One or more of these ranges may be manually specified by the user by selecting or marking portions of thumbnail -.","In alternative embodiments, instead of selecting a portion of a thumbnail, a user can also specify a range by clicking on a location within a thumbnail. A range is then automatically generated by adding a pre-specified buffer time before and after the current clicked location. In this manner, a range can be specified by a single click. Multiple ranges may be specified using this technique.","In , each specified range is indicated by a bar displayed over thumbnail -. An identifier or label may also be associated with each range to uniquely identify the range. In , each range is identified by a number associated with the range and displayed in the upper left corner of the range. The numbers act as labels for the ranges. Accordingly, information stored for a range may include the start time (R) for the range, the end time (R) for the range, and a label or identifier identifying the range. Information identifying a multimedia document storing information corresponding to a range may also be stored for a range.","Each range specified by selecting a portion of thumbnail - is bounded by a top edge (R) and a bottom edge (R). The Rand Rtimes for a range may be determined from the pixel locations of Rand Ras follows:\n\n\n\n\n","It should be apparent that various other techniques may also be used for specifying a range. For example, in alternative embodiments of the present invention, a user may specify a range by providing the start time (R) and end time (R) for the range.","In GUI  depicted , information related to the ranges displayed is GUI  is displayed in area . The information displayed for each range in area  includes a label or identifier  identifying the range, a start time (R)  of the range, an end time (R)  of the range, a time span  of the range, and a set of video keyframes  extracted from the portion of the multimedia information associated with the range. The time span for a ranges is calculated by determining the difference between the end time Rand the start time associated with the range (i.e., time span for a range=R\u2212R). In the embodiment depicted in , the first, last, and middle keyframe extracted from the multimedia information corresponding to each range are displayed. Various other techniques may also be used for selecting keyframes to be displayed for a range. The information depicted in  is not meant to limit the scope of the present invention. Various other types of information for a range may also be displayed in alternative embodiments of the present invention.","According to the teachings of the present invention, various operations may be performed on the ranges displayed in GUI . A user can edit a range by changing the Rand Rtimes associated with the range. Editing a range may change the time span (i.e., the value of (R\u2212R)) of the range. In GUI  depicted in , the user can modify or edit a displayed range by selecting \u201cEdit\u201d button . After selecting \u201cEdit\u201d button , the user can edit a particular range by dragging the top edge and\/or the bottom edge of the bar representing the range. A change in the position of top edge modifies the start time (R) of the range, and a change in the position of the bottom edge modifies the end time (R) of the range.","The user can also edit a range by selecting a range in area  and then selecting \u201cEdit\u201d button . In this scenario, selecting \u201cEdit\u201d button  causes a dialog box to be displayed to the user (e.g., dialog box  depicted in ). The user can then change the Rand Rvalues associated with the selected range by entering the values in fields  and , respectively. The time span of the selected range is displayed in area  of the dialog box.","The user can also move the location of a displayed range by changing the position of the displayed range along thumbnail -. Moving a range changes the Rand Rvalues associated with the range but maintains the time span of the range. In GUI , the user can move a range by first selecting \u201cMove\u201d button  and then selecting and moving a range. As described above, the time span for a range may be edited by selecting \u201cEdit\u201d button and then dragging an edge of the bar representing the range.","The user can remove or delete a previously specified range. In GUI  depicted in , the user can delete a displayed range by selecting \u201cRemove\u201d button  and then selecting the range that is to be deleted. Selection of \u201cClear\u201d button  deletes all the ranges that have been specified for the multimedia information displayed in GUI .","As indicated above, each range refers to a portion of the multimedia information occurring between times Rand Rassociated with the range. The multimedia information corresponding to a range may be output to the user by selecting \u201cPlay\u201d button . After selecting \u201cPlay\u201d button , the user may select a particular range displayed in GUI  whose multimedia information is to be output to the user. The portion of the multimedia information corresponding to the selected range is then output to the user. Various different techniques known to those skilled in the art may be used to output the multimedia information to the user. According to an embodiment of the present invention, video information corresponding to multimedia information associated with a selected range is played back to the user in area . Text information corresponding to the selected range may be displayed in area . The positions of thumbnail viewing area lens  and panel viewing area lens , and the information displayed in third viewing area  are automatically updated to correspond to the selected range whose information is output to the user in area .","The user can also select a range in area  and then play information corresponding to the selected range by selecting \u201cPlay\u201d button . Multimedia information corresponding to the selected range is then displayed in area .","The user may also instruct GUI  to sequentially output information associated with all the ranges specified for the multimedia information displayed by GUI  by selecting \u201cPreview\u201d button . Upon selecting \u201cPreview\u201d button , multimedia information corresponding to the displayed ranges is output to the user in sequential order. For example, if six ranges have been displayed as depicted in , multimedia information corresponding to the range identified by label \u201c1\u201d may be output first, followed by multimedia information corresponding to the range identified by label \u201c2\u201d, followed by multimedia information corresponding to the range identified by label \u201c3\u201d, and so on until multimedia information corresponding to all six ranges has been output to the user. The order in which the ranges are output to the user may be user-configurable.","Multimedia information associated with a range may also be saved to memory. For example, in the embodiment depicted in , the user may select \u201cSave\u201d button  and then select one or more ranges that are to be saved. Multimedia information corresponding to the ranges selected by the user to be saved is then saved to memory (e.g., a hard disk, a storage unit, a floppy disk, etc.)","Various other operations may also be performed on a range. For example, according to an embodiment of the present invention, multimedia information corresponding to one or more ranges may be printed on a paper medium. Details describing techniques for printing multimedia information on a paper medium are discussed in U.S. application Ser. No. 10\/001,895, filed Nov. 19, 2001, the entire contents of which are herein incorporated by reference for all purposes.","Multimedia information associated with a range may also be communicated to a user-specified recipient. For example, a user may select a particular range and request communication of multimedia information corresponding to the range to a user-specified recipient. The multimedia information corresponding to the range is then communicated to the recipient. Various different communication techniques known to those skilled in the art may be used to communicate the range information to the recipient including faxing, electronic mail, wireless communication, and other communication techniques.","Multimedia information corresponding to a range may also be provided as input to another application program such as a search program, a browser, a graphics application, a MIDI application, or the like. The user may select a particular range and then identify an application to which the information is to be provided. In response to the user's selection, multimedia information corresponding to the range is then provided as input to the application.","As previously stated, ranges may be specified manually by a user or may be selected automatically by the present invention. The automatic selection of ranges may be performed by software modules executing on server , hardware modules coupled to server , or combinations thereof.  is a simplified high-level flowchart  depicting a method of automatically creating ranges according to an embodiment of the present invention. The method depicted in  may be performed by server , by client , or by server  and client  in combination. For example, the method may be executed by software modules executing on server  or on client , by hardware modules coupled to server  or to client , or combinations thereof. In the embodiment described below, the method is performed by server . The method depicted in  is merely illustrative of an embodiment incorporating the present invention and does not limit the scope of the invention as recited in the claims. One of ordinary skill in the art would recognize other variations, modifications, and alternatives.","As depicted in , the method is initiated when server  receives criteria for creating ranges (step ). The user of the present invention may specify the criteria via GUI . For example, in GUI  depicted in , area  displays various options that can be selected by the user to specify criteria for automatic creation of ranges. In GUI  depicted in , the user may select either \u201cTopics\u201d or \u201cWords\u201d as the range criteria. If the user selects \u201cTopics\u201d, then information related to topics of interest to the user (displayed in area ) is identified as the range creation criteria. If the user selects \u201cWords\u201d, then one or more words selected by the user in area  of GUI  are identified as criteria for automatically creating ranges. In alternative embodiments, the criteria for automatically creating ranges may be stored in a memory location accessible to server . For example, the criteria information may be stored in a file accessible to server . Various other types of criteria may also be specified according to the teachings of the present invention.","The multimedia information stored in the multimedia document is then analyzed to identify locations (referred to as \u201chits\u201d) in the multimedia information that satisfy the criteria received in step  (step ). For example, if the user has specified that one or more words selected by the user in area  are to be used as the range creation criteria, then the locations of the selected words are identified in the multimedia information. Likewise, if the user has specified topics of interest as the range creation criteria, then server  analyzes the multimedia information to identify locations in the multimedia information that are relevant to the topics of interest specified by the user. As described above, server  may analyze the multimedia information to identify locations of words or phrases associated with the topics of interest specified by the user. Information related to the topics of interest may be stored in a user profile file that is accessible to server . It should be apparent that various other techniques known to those skilled in the art may also be used to identify locations in the multimedia information that satisfy the range criteria received in step .","One or more ranges are then created based upon the locations of the hits identified in step  (step ). Various different techniques may be used to form ranges based upon locations of the hits. According to one technique, one or more ranges are created based upon the times associated with the hits. Hits may be grouped into ranges based on the proximity of the hits to each other. One or more ranges created based upon the locations of the hits may be combined to form larger ranges.","The ranges created in step  are then displayed to the user using GUI  (step ). Various different techniques may be used to display the ranges to the user. In , each range is indicated by a bar displayed over thumbnail -.",{"@attributes":{"id":"p-0263","num":"0267"},"figref":["FIG. 22","FIG. 22","FIG. 21","FIG. 22","FIG. 22"],"b":["2200","2106","104","102","104","102","104","102","104","102","104"]},"As depicted in , the method is initiated by determining a time associated the first hit in the multimedia information (step ). The first hit in the multimedia information corresponds to a hit with the earliest time associated with it (i.e., a hit that occurs before other hits in the multimedia information). A new range is then created to include the first hit such that Rfor the new range is set to the time of occurrence of the first hit, and Rfor the new range is set to some time value after the time of occurrence of the first hit (step ). According to an embodiment of the present invention, Ris set to the time of occurrence of the hit plus 5 seconds.","Server  then determines if there are any additional hits in the multimedia information (step ). Processing ends if there are no additional hits in the multimedia information. The ranges created for the multimedia information may then be displayed to the user according to step  depicted in . If it is determined in step  that additional hits exist in the multimedia information, then the time associated with the next hit is determined (step ).","Server  then determines if the time gap between the end time of the range including the previous hit and the time determined in step  exceeds a threshold value (step ). Accordingly, in step  server  determines if:","(Time determined in step )\u2212(Rof range including previous hit)>GapBetweenHits wherein, GapBetweenHits represents the threshold time value. The threshold value is user configurable. According to an embodiment of the present invention, GapBetweenHits is set to 60 seconds.","If it is determined in step  that the time gap between the end time of the range including the previous hit and the time determined in step  exceeds the threshold value, then a new range is created to include the next hit such that Rfor the new range is set to the time determined in step , and Rfor the new range is set to some time value after the time determined in step  (step ). According to an embodiment of the present invention, Ris set to the time of occurrence of the hit plus 5 seconds. Processing then continues with step .","If it is determined in step  that the time gap between the end time of the range including the previous hit and the time determined in step  does not exceed the threshold value, then the range including the previous hit is extended by changing the end time Rof the range to the time determined in step  (step ). Processing then continues with step .","According to the method depicted in , a single range is created for hits in the multimedia information that occur within a threshold value (\u201cGapBetweenHits\u201d) from the previous range. At the end of the method depicted in , one or more ranges are automatically created based upon the range criteria.","According to an embodiment of the present invention, after forming one or more ranges based upon the times associated with the hits (e.g., according to flowchart  depicted in ), one or more ranges created based upon the locations of the hits may be combined with other ranges to form larger ranges. According to an embodiment of the present invention, a small range is identified and combined with a neighboring range if the time gap between the small range and the neighboring range is within a user-configurable time period threshold. If there are two neighboring time ranges that are within the time period threshold, then the small range is combined with the neighboring range that is closest to the small range. The neighboring ranges do not need to be small ranges. Combination of smaller ranges to form larger ranges is based upon the premise that a larger range is more useful to the user than multiple small ranges.",{"@attributes":{"id":"p-0272","num":"0276"},"figref":["FIG. 23","FIG. 23","FIG. 21","FIG. 22","FIG. 23","FIG. 23"],"b":["2300","2106","2200","104","102","104","102","104","102","104","102","104"]},"In order to describe the processing performed in , it is assumed that \u201cN\u201d ranges (N\u22671) have been created for the multimedia information displayed by the GUI. The ranges may have been created according to the processing depicted in flowchart  in . Each range R, where (1\u2266i\u2266N), in the set of \u201cN\u201d ranges has a start time Rand an end time Rassociated with it. For a range R, the neighbors of the range include range Rand range R, where Rof range Roccurs before Rof range Rand Rof range Roccurs before Rof range R. Range Ris referred to as a range that occurs before range R. Range Ris referred to as a range that occurs after range R.","As depicted in , the method is initiated by initializing a variable \u201ci\u201d to 1 (step ). A range Ris then selected (step ). During the first pass through flowchart , the first range (i.e., the range having the earliest Rtime) in the set of \u201cN\u201d ranges is selected. Subsequent ranges are selected in subsequent passes.","Server  then determines if range Rselected in step  qualifies as a small range. According to an embodiment of the present invention, a threshold value \u201cSmallRangeSize\u201d is defined and a range is considered a small range if the time span of the range is less than or equal to threshold value SmallRangeSize. Accordingly, in order to determine if range Rqualifies as a small range, the time span of range Rselected in step  is compared to threshold time value \u201cSmallRangeSize\u201d (step ). The value of SmallRangeSize may be user-configurable. According to an embodiment of the present invention, SmallRangeSize is set to 8 seconds.","If it is determined in step  that the range Rselected in step  does not qualify as a small range (i.e., the time span (R\u2212R) of range Ris greater than the threshold value SmallRangeSize), then the range is not a candidate for combination with another range. The value of variable \u201ci\u201d is then incremented by one (step ) to facilitate selection of the next range in the set of \u201cN\u201d ranges. Accordingly, according to the teachings of the present invention depicted in , only ranges that qualify as small ranges are eligible for combination with other neighboring ranges.","After step , server  determines if all the ranges in the set of \u201cN\u201d ranges have been processed. This is done by determining if the value of \u201ci\u201d is greater than the value of \u201cN\u201d (step ). If the value of \u201ci\u201d is greater than \u201cN\u201d, it indicates that all the ranges in the set of ranges for the multimedia information have been processed and processing of flowchart  ends. If it is determined in step  that \u201ci\u201d is less than or equal to \u201cN\u201d, then it indicates that the set of \u201cN\u201d ranges comprises at least one range that has not been processed according to flowchart . Processing then continues with step  wherein the next range Ris selected.","If it is determined in step  that range Rselected in step  qualifies as a small range (i.e., the time span (R\u2212R) of range Ris less than or equal to the threshold value SmallRangeSize), the present invention then performs processing to identify a range that is a neighbor of range R(i.e., a range that occurs immediately before or after range Rselected in step ) with which range Rcan be combined. In order to identify such a range, server  initializes variables to facilitate selection of ranges that are neighbors of range Rselected in step  (step ). A variable \u201cj\u201d is set to the value (i+1) and a variable \u201ck\u201d is set to the value \u201c(i\u22121)\u201d. A variable \u201cj\u201d is used to refer to a range that is a neighbor of range Rand occurs after range R, and a variable \u201ck\u201d is used to refer to a range that is a neighbor of range Rand occurs before range R.  depicts a simplified diagram showing the relationship between ranges R, R, and R. As shown in , range Roccurs after range R(i.e., Rof Roccurs after Rof R) and before range R(i.e., Rof Roccurs before Rof R).","Server  then determines if the set of \u201cN\u201d ranges created for the multimedia information includes a range that is a neighbor of range Rselected in step  and occurs before range R, and a range that is a neighbor of range Rand occurs after range R. This is done by determining the values of variables \u201cj\u201d and \u201ck\u201d. If the value of \u201cj\u201d is greater than \u201cN\u201d, it indicates that the range Rselected in step  is the last range in the set of \u201cN\u201d ranges created for the multimedia information implying that there is no range that occurs after range R. If the value of \u201ck\u201d is equal to zero, it indicates that the range Rselected in step  is the first range in the set of \u201cN\u201d ranges created for the multimedia information implying that there is no range that occurs before range R.","Accordingly, server  determines if range Rhas a neighboring range that occurs before Rand a neighboring range that occurs after R. This is done by determining if the value of \u201cj\u201d is less than \u201cN\u201d and if the value of \u201ck\u201d is not equal to zero (step ). If the condition in step  is satisfied, then it indicates that the set of \u201cN\u201d ranges comprises a range that is a neighbor of range Rselected in step  and occurs before range R, and a range that is a neighbor of range Rand occurs after range R. In this case, processing continues with step . If the condition in step  is not satisfied, then it indicates that range Rselected in step  is either the first range in the set of \u201cN\u201d ranges implying that there is no range that occurs before range R, and\/or that range Rselected in step  is the last range in the set of \u201cN\u201d ranges implying that there is no range that occurs after range R. In this case, processing continues with step .","If the condition in step  is determined to be true, server  then determines time gaps between ranges Rand Rand between ranges Rand R(step ). The time gap (denoted by G) between ranges Rand Ris calculated by determining the time between Rof range Rand Rof R, (see ) i.e.,\n\n=(of )\u2212(of )\n\nThe time gap (denoted by G) between ranges Rand Ris calculated by determining the time between Rof range Rand Rof R, (see ) i.e.,\n\n=(of )\u2212(of )\n","According to the teachings of the present invention, a small range is combined with a neighboring range only if the gap between the small range and the neighboring range is less than or equal to a threshold gap value. The threshold gap value is user configurable. Accordingly, server  then determines the sizes of the time gaps to determine if range Rcan be combined with one of its neighboring ranges.","Server  then determines which time gap is larger by comparing the values of time gap Gand time gap G(step ). If it is determined in step  that Gis greater that G, it indicates that range Rselected in step  is closer to range Rthan to range R, and processing continues with step . Alternatively, if it is determined in step  that Gis not greater that G, it indicates that the time gap between range Rselected in step  and range Ris equal to or less than the time gap between ranges Rand R. In this case processing continues with step .","If it is determined in step  that Gis not greater than G, server  then determines if the time gap (G) between range Rand range Ris less than or equal to a threshold gap value \u201cGapThreshold\u201d (step ). The value of GapThreshold is user configurable. According to an embodiment of the present invention, GapThreshold is set to 90 seconds. It should be apparent that various other values may also be used for Gap Threshold.","If it is determined in step  that the time gap (G) between range Rand range Ris less than or equal to threshold gap value GapThreshold (i.e., G\u2266GapThreshold), then ranges Rand Rare combined to form a single range (step ). The process of combining ranges Rand Rinvolves changing the end time of range Rto the end time of range R(i.e., Rof Ris set to Rof R) and deleting range R. Processing then continues with step  wherein the value of variable \u201ci\u201d is incremented by one.","If it is determined in step  that time gap Gis greater than GapThreshold (i.e., G>GapThreshold), it indicates that both ranges Rand Rare outside the threshold gap value and as a result range Rcannot be combined with either range Ror R. In this scenario, processing continues with step  wherein the value of variable \u201ci\u201d is incremented by one.","Referring back to step , if it is determined that Gis greater than G, server  then determines if the time gap (G) between ranges Rand Ris less than or equal to the threshold gap value \u201cGapThreshold\u201d (step ). As indicated above, the value of GapThreshold is user configurable. According to an embodiment of the present invention, GapThreshold is set to 90 seconds. It should be apparent that various other values may also be used for GapThreshold.","If it is determined in step  that the time gap (G) between ranges Rand Ris less than or equal to threshold gap value GapThreshold (i.e., G\u2266GapThreshold), then ranges Rand Rare combined to form a single range (step ). The process of combining ranges Rand Rinvolves changing the start time of range Rto the start time of range R(i.e., Rof Ris set to Rof R) and deleting range R. Processing then continues with step  wherein the value of variable \u201ci\u201d is incremented by one.","If it is determined in step  that time gap Gis greater than GapThreshold (i.e., G>GapThreshold), it indicates that both ranges Rand Rare outside the threshold gap value and as a result range Rcannot be combined with either range Ror R. In this scenario, processing continues with step  wherein the value of variable \u201ci\u201d is incremented by one.","If server  determines that the condition in step  is not satisfied, server  then determines if the value of \u201ck\u201d is equal to zero (step ). If the value of \u201ck\u201d is equal to zero, it indicates that the range Rselected in step  is the first range in the set of \u201cN\u201d ranges created for the multimedia information which implies that there is no range in the set of \u201cN\u201d ranges that occurs before range R. In this scenario, server  then determines if the value of variable \u201cj\u201d is greater than \u201cN\u201d (step ). If the value of \u201cj\u201d is also greater than \u201cN\u201d, it indicates that the range Rselected in step  is not only the first range but also the last range in the set of \u201cN\u201d ranges created for the multimedia information which implies that there is no range in the set of ranges that comes after range R. If it is determined in step  that \u201ck\u201d is equal to zero and that \u201cj\u201d>N in step , it indicates that the set of ranges for the multimedia information comprises only one range (i.e., N=1). Processing depicted in flowchart  is then ended since no ranges can be combined.","If it is determined in step  that \u201ck\u201d is equal to zero and that \u201cj\u201d is not greater than \u201cN\u201d in step , it indicates that the range Rselected in step  represents the first range in the set of \u201cN\u201d ranges created for the multimedia information, and that the set of ranges includes at least one range Rthat is a neighbor of range Rand occurs after range R. In this case, the time gap Gbetween range Rand range Ris determined (step ). As indicated above, time gap Gis calculated by determining the time between Rof range Rand Rof R, i.e.,\n\n=(of )\u2212(of )\n\nProcessing then continues with step  as described above.\n","If it is determined in step  that \u201ck\u201d is not equal to zero, it indicates that the range Rselected in step  represents the last range in the set of \u201cN\u201d ranges created for the multimedia information, and that the set of ranges includes at least one range Rthat is a neighbor of range Rand occurs before range R. In this case, the time gap Gbetween range Rand range Ris determined (step ). As indicated above, time gap Gis calculated by determining the time gap between Rof range Rand Rof R, i.e.,\n\n=(of )\u2212(of )\n\nProcessing then continues with step  as described above.\n",{"@attributes":{"id":"p-0293","num":"0297"},"figref":["FIG. 25A","FIG. 24","FIG. 25B","FIG. 24"],"sub":["i ","k ","i ","j "]},"As indicated above, the processing depicted in  may be performed after one or more ranges have been created according to the times associated with the hits according to flowchart  depicted in . According to an embodiment of the present invention, after the ranges have been combined according to flowchart  depicted in , the ranges may then be displayed to the user in GUI  according to step  in .","According to an alternative embodiment of the present invention, after combining ranges according to flowchart  depicted in , a buffer time is added to the start time and end time of each range. A user may configure the amount of time (BufferStart) to be added to the start time of each range and the amount of time (BufferEnd) to be added to the end time of each range. The buffer times are added to a range so that a range does not start immediately on a first hit in the range and stop immediately at the last hit in the range. The buffer time provides a lead-in and a trailing-off for the information contained in the range and thus provides a better context for the range.","A buffer is provided at the start of a range by changing the Rtime of the range as follows:\n\nof range=(of range before adding buffer)\u2212BufferStart\n\nA buffer is provided at the end of a range by changing the Rtime of the range as follows:\n\nof range=(of range before adding buffer)+BufferEnd\n",{"@attributes":{"id":"p-0297","num":"0301"},"figref":["FIG. 26","FIG. 26","FIG. 23"],"b":["2000","2602","2008","1","2006","2","2006","3","2006","2","2300"]},"Displaying Multimedia Information from Multiple Multimedia Documents","The embodiments of the present invention described above display representations of information that has been recorded (or captured) along a common timeline. The recorded information may include information of different types such as audio information, video information, closed-caption (CC) text information, slides information, whiteboard information, etc. The different types of information may have been captured by one or more capture devices.","As described above, a multimedia document may provide a repository for storing the recorded or captured information. The multimedia document may be a file that stores the recorded information comprising information of multiple types. The multimedia document may be a file that includes references to one or more other files that store the recorded information. The referenced files may store information of one or more types. The multimedia document may also be a location where the recorded information of one or more types is stored. For example, the multimedia document may be a directory that stores files comprising information that has been captured or recorded during a common timeline. According to an embodiment of the present invention, each file in the directory may store information of a particular type, i.e., each file may store a particular stream of information. Accordingly, for recorded information that comprises information of multiple types (e.g., a first type, a second type, etc.), the information of the various types may be stored in a single file, the information for each type may be stored in a separate file, and the like.","Since the different types of information have been captured along a common timeline, the representations of the information can be displayed in a manner such that the representations when displayed by the GUI are temporally aligned with each other. For example, interface  depicted in  displays multimedia information stored by a television broadcast recording multimedia document. The different types of information stored in the broadcast recording include video information, audio information, and possibly closed-caption (CC) text information. The video information, audio information, and CC text information are all captured along the same (or common) timeline possibly by different capture devices. For example, the audio information may have been captured using an audio information capture device (e.g., a microphone) and the video information may have been captured by a video information capture device (e.g., a video camera). The audio and video information might also have been captured by a single information capture device.","As described above, interface  displays text information that is a representation of the audio or CC text information included in the broadcast recording (or a text representation of some other type of information included in the multimedia information). Interface  also displays video keyframes extracted from the video information included in the broadcast recording. The displayed video keyframes are a representation of the video information stored in the multimedia document. Since the audio and video information are captured along the same timeline, the representations of the information can be displayed such that they are temporally aligned or synchronized with each other. For example, as described above, thumbnail images - and - are aligned such that the text information (which may represent a transcript of the audio information or the CC text information or a text representation of some other type of information included in the multimedia information) in thumbnail image - and video keyframes displayed in thumbnail - that occur at a particular point of time are displayed approximately close to each other along the same horizontal axis. This enables a user to determine various types of information in the television broadcast recording occurring approximately concurrently by simply scanning the thumbnail images in the horizontal axis. Likewise, panels - and - are temporally aligned or synchronized with each other such that representations of the various types of information occurring concurrently in the television broadcast recording are displayed approximately close to each other.","Embodiments of the present invention can also display recorded multimedia information that may be stored in multiple multimedia documents. The multimedia information in the multiple multimedia documents may have been captured along different timelines. For example, embodiments of the present invention can display representations of multimedia information from a television news broadcast captured or recorded during a first timeline (e.g., a morning newscast) and from another television news broadcast captured during a second timeline (e.g., an evening newscast) that is different from the first timeline. Accordingly, embodiments of the present invention can display multimedia information stored in one or more multimedia documents that may store multimedia information captured along different timelines. Each multimedia document may comprise information of different types such as audio information, video information, CC text information, whiteboard information, slides information, and the like.","The multiple multimedia documents whose information is displayed may also include documents that store information captured along the same timeline. For example, the multiple multimedia documents may include a first television program recording from a first channel captured during a first timeline and a second television program recording from a second channel captured during the same timeline (i.e., the first timeline) as the first television program recording. Embodiments of the present invention can accordingly display representations of information from multiple multimedia documents that store information that may have been captured along the same or different timelines.",{"@attributes":{"id":"p-0305","num":"0309"},"figref":"FIG. 27","b":["2700","2700"]},"As depicted in , interface  comprises a toolbar  including several user-selectable buttons. The buttons include a button  for loading multimedia documents for display, a button  for removing one or more previously loaded multimedia documents, a button  for printing multimedia information from one or more loaded multimedia documents on a paper medium, a button  for configuring user preferences, and other buttons that allow a user to perform actions, configure, customize, or control the manner in which information from one or more multimedia documents is displayed. Additional features of interface  are described below in more detail.","In order to load one or more multimedia documents to be displayed, the user selects load button .  depicts a simplified window  that is displayed when the user selects load button  according to an embodiment of the present invention. Window  facilitates selection of one or more multimedia documents to be loaded and displayed according to the teachings of the present invention. As depicted in , information identifying one or more multimedia documents that are available to be loaded is displayed in box  of window . Each multimedia document may be identified by an identifier (e.g., a filename, a location identifier such as a directory name). In the embodiment depicted in , each multimedia document is identified by a five digit code identifier. The user may select one or more multimedia documents to be loaded by highlighting the identifiers corresponding to the multimedia documents in box  and then selecting \u201cAdd\u201d button . The highlighted identifiers for the multimedia documents are then moved from box  and displayed in box  that displays multimedia documents selected for loading. A previously selected multimedia document can be deselected by highlighting the identifier for the multimedia document in box  and then selecting \u201cRemove\u201d button .","Information related to the multimedia document corresponding to a highlighted identifier (highlighted either in box  or ) is displayed in information area . In the embodiment depicted in , the displayed information includes information  indicating the duration of the multimedia document, information  indicating the date on which the information in the multimedia document was captured or recorded, information  indicating the time of the recording, information  identifying the television channel from which the information was recorded, and information  indicating the type of recording. Other descriptive information that is available for the multimedia document (e.g., name of the TV program) might be displayed in description area .","The user can select \u201cLoad\u201d button  to load and display contents of multimedia documents identified by the identifiers displayed in box . As shown in , three multimedia documents have been selected and will be loaded upon selection of \u201cLoad\u201d button . The selected multimedia documents may store multimedia information captured along the same or different timelines. Each selected multimedia document may comprise information of one or more types (e.g., audio information, video information, CC text information, whiteboard information, slides information, etc.). The types of information stored by one multimedia document may be different from the types of information stored by another selected multimedia document. The user can cancel the load operation by selecting \u201cCancel\u201d button .","Other techniques may also be used for selecting and loading a multimedia document. For example, according to one technique, a user may scan a particular identifier (e.g., a barcode). The multimedia document (or portion of information stored by the multimedia document) corresponding to the scanned barcode may be selected and loaded.",{"@attributes":{"id":"p-0311","num":"0315"},"figref":"FIG. 29A","b":["2900","2700"]},"As depicted in , contents of three multimedia documents have been loaded and displayed. For each multimedia document, representations of information of different types stored by the multimedia document are displayed in a thumbar corresponding to the multimedia document. A video window is also displayed for each multimedia document. The thumbar for each multimedia document includes one or more thumbnail images displaying representations of the various types of information included in the multimedia document.","For example, in the , a thumbar  displays representations of information stored by a first multimedia document, a thumbar  displays representations of information stored by a second multimedia document, and a thumbar  displays representations of information stored by a third multimedia document. A video window  is displayed for the first multimedia document, a video window  is displayed for the second multimedia document, and a video window  is displayed for the third multimedia document. In the embodiment depicted in , the first, second, and third multimedia documents are recordings of television programs and each comprise audio information, video information, and possibly CC text information. This is however not intended to limit the scope of the present invention. A multimedia document displayed according to the teachings of the present invention may include different types of information.","Each thumbar displayed in  includes one or more thumbnail images. Each thumbnail image displays a representation of a type of information stored in the multimedia document. Since the three multimedia documents loaded in interface  comprise audio, video, and possibly CC text information, thumbars , , and  each include a thumbnail image displaying text information that is a representation of the audio information or CC text information (or a text representation of some other type of information included in the multimedia information) from the corresponding multimedia document and a thumbnail image displaying video keyframes representing the video information in the corresponding multimedia document. For example, thumbar  includes a thumbnail image  that displays text information representing the audio information (or CC text information) from the first multimedia document, and a thumbnail image  displaying video keyframes extracted from the video information of the first multimedia document. Thumbar  includes a thumbnail image  that displays text information representing the audio information (or CC text information, or a text representation of some other type of information included in the multimedia information) from the second multimedia document, and a thumbnail image  displaying video keyframes extracted from the video information of the second multimedia document. Thumbar  includes a thumbnail image  that displays text information representing the audio information (or CC text information, or a text representation of some other type of information included in the multimedia information) from the third multimedia document, and a thumbnail image  displaying video keyframes extracted from the video information of the third multimedia document. Techniques for generating and displaying the thumbnail images have been previously described. Each thumbar is like the second viewing area depicted in .","The thumbnail images in a thumbar are aligned such that representations of information that occurs temporally concurrently in the multimedia document are displayed approximately close to each other along the same horizontal axis. Each thumbar represents information captured according to a common timeline. However, the timeline corresponding to one thumbar may be different from the timeline corresponding to another thumbar.","A lens (\u201cthumbnail viewing area lens\u201d) is displayed for each thumbar covering or emphasizing a portion of the thumbar. As depicted in , a thumbnail viewing area lens  covers an area of thumbar , a thumbnail viewing area lens  covers an area of thumbar , and a thumbnail viewing area lens  covers an area of thumbar . The thumbnail viewing area lenses are initially positioned at the top of the thumbars (i.e., at the start of the multimedia documents) as depicted in . As described above with respect to , each thumbnail viewing lens can be moved along the corresponding thumbar and can be used to navigate and scroll through the contents of the multimedia document displayed in the thumbar. Techniques for displaying a thumbnail viewing area lens and techniques for using the thumbnail viewing area lens to navigate and scroll through the contents of each multimedia document have been previously described. Each thumbnail viewing area lens may or may not comprise a sublens such as sublens  depicted in .","Descriptive information related to each multimedia document may also be displayed in the thumbar corresponding to the multimedia document. The information may include information such as information indicating the duration of the multimedia document, information indicating the date on which the information in the multimedia document was captured or recorded, information indicating the time of the recording, information identifying the television channel or program from which the information was recorded, information indicating the type of recording, etc. As depicted in , descriptive information  for each multimedia document is displayed along a side of the corresponding thumbar.","For each multimedia document, the video information may be played back in a video window corresponding to the multimedia document. The audio information accompanying the video information may also be out via an audio output device. For example, video information from the first multimedia document may be played back in video window , video information from the second multimedia document may be played back in video window , and video information from the third multimedia document may be played back in video window . A control bar is provided with each video window for controlling playback of information in the associated video window. For example, the playback of video information in video window  may be controlled using controls provided by control bar , the playback of video information in video window  may be controlled using controls provided by control bar , and the playback of video information in video window  may be controlled using controls provided by control bar .","The contents of video information displayed in a video window for a multimedia document also depends on the position of the thumbnail viewing area lens over the thumbar corresponding to the multimedia document. For example, contents of the video information displayed in video window  depend upon the position of thumbnail viewing area lens  over thumbar . As previously described, each thumbnail viewing area lens is characterized by a top edge corresponding to time tand a bottom edge corresponding to a time t. The playback of the video information in the video window is started at time tor tor some time in between times tand t. As the thumbnail viewing area lens is repositioned over a thumbar, the video played back in the corresponding video window may change such that the video starts playing from time tor time tcorresponding to the present position of thumbnail viewing area lens over the thumbar, or some time in between tand t. It should be noted that the thumbnail viewing area lenses covering portions of the different thumbars can be repositioned along the thumbars independent of each other.","Each video window may also display information related to the multimedia document whose video information contents are displayed in the video window. The information may include for example, information identifying the television program for the recording, information identifying the time in the multimedia document corresponding to the currently played back content, etc.","According to an embodiment of the present invention, a list of words  found in all the loaded multimedia documents (i.e., common words) is also displayed in an area of interface . The list of words  includes words that are found in information of one or more types contained by the loaded multimedia documents. For example, the list of words displayed in  includes words that were found in the information from first multimedia document, the second multimedia document, and the third multimedia document. According to an embodiment of the present invention, text representations of information contained by the loaded multimedia documents are searched to find the common words. The text information may represent the CC text information, a transcription of the audio information, or a text representation of some other type of information stored in the multimedia documents. According to another embodiment of the present invention, the list of words may also include words determined from the video information contained by the multimedia documents. For example, the video keyframes extracted from the video information may also be searched to find the common words. The keyframes may be searched for the words. The number of occurrences of the words in the multimedia documents is also shown.",{"@attributes":{"id":"p-0322","num":"0326"},"figref":["FIG. 29B","FIG. 29A","FIG. 29B"],"b":["2900","2926","2928","2930","2904","2908","2912","2942","2942","2942"],"sub":["1 ","2 "]},"According to an embodiment of the present invention, the user can specify criteria and the contents of the multimedia documents that are loaded and displayed in the user interface may be searched to find locations within the multimedia documents that satisfy the user-specified criteria. Sections or locations of the multimedia document that satisfy the user specified criteria may be highlighted and displayed in interface . According to an embodiment of the present invention, the user-specified criteria may include user-specified words or phrases, search queries comprising one or more terms, topics of interest, etc.","In interface  depicted in , a user may enter a word or phrase in input area  and request that the contents of the multimedia documents be searched for the user-specified word or phrase by selecting \u201cFind\u201d button . The word or phrase to be searched may also be selected from the common list of words . In , the user has specified word \u201cStewart\u201d.","The contents of the multimedia documents are then searched to identify locations and occurrences of the user-specified word or phrase. According to an embodiment of the present invention, text representations of information stored by the multimedia documents are searched to find locations of the user-specified word or phrase. Video keyframes may also be searched for the word or phrase. All occurrences (\u201chits\u201d)  of the user-specified word or phrase in the various thumbars (i.e., in the thumbnail images in the thumbars) are highlighted as shown in . Various different techniques may be used for highlighting the hits in the multimedia documents. For example, the individual hits may be highlighted. Ranges may also be determined based upon the hits (as describe above) and the ranges may be highlighted. Other techniques such as marks (see  described below) may also be used to mark the approximate locations of the hits. According to an embodiment of the present invention, as shown in , colored rectangles may be drawn around lines in the thumbnail images to highlight lines that contain the search word or phrase. Video keyframes displayed in the thumbars that contain the search word or phrase may also be highlighted by drawing colored boxes around the video keyframes. Various other types of techniques may also be used. For example, if a multimedia document comprises slides information, then the slides displayed in the thumbars that contain the search word or phrase may be highlighted. The total number of occurrences  of the word or phrase in the various multimedia documents is also displayed. For example, in , the word \u201cStewart\u201d occurs 31 times in the three multimedia documents.","The user may also form search queries comprising multiple terms (e.g., multiple words and\/or phrases). As shown in , the words or phrases that are included in a search query are displayed in area . The user can add a word or phrase to the search query by typing the word or phrase in input area  (or by selecting a word from common list of words ) and selecting \u201cAdd\u201d button . The word or phrase is then added to the search query and displayed in area . In , the word \u201cStewart\u201d has been added to the search query. The user can delete or remove a word or phrase from the search query by selecting the word or phrase in area  and selecting \u201cDel\u201d button . The user can reset or clear the search query using \u201cReset\u201d button .","The user can also specify Boolean connectors for connecting the terms in a search query. For example, in the embodiment depicted in , the words or phrases in a search query may be ANDed or ORed together based upon the selection of radio buttons . If the words are ORed together, then all locations of the words or phrases in the search query in the various multimedia documents are found and highlighted. If the words are ANDed together, then only those portions of the multimedia documents are highlighted as being relevant that contain all the words or phrases in the search query within a close proximity. The proximity measure may be user configurable. According to an embodiment of the present invention, the proximity measure corresponds to a number of words. For example, locations of search query words or phrases in the multimedia document are highlighted if they occur within a certain number o words of each other. Proximity can also be based upon time. In this embodiment, the locations of search query words or phrases in the multimedia document are highlighted if they occur within a specific length of time.","In , the locations of the hits are shown by marks  displayed in the thumbars. Each mark  identifies a line in the text information printed in the thumbnails that contains the search query terms.","In the embodiment depicted in , ranges have been formed based on the location of the hits. Techniques for forming ranges based on locations of hits has been previously described (see , B, , , , , A, B, and , and the associated description). The locations of the ranges are displayed using colored rectangles . Each rectangle identifies a range. The rectangular boxes representing the ranges thus identify portions of the multimedia documents that satisfy or are relevant to the user-specified criteria (e.g., words, phrases, topics of interest, etc.) that are used for searching the multimedia documents.","In embodiments of the present invention wherein contents of only one multimedia document are displayed (e.g., in ), a range is identified by a start time (R) and an end time (R) that define the boundaries of the range, as previously described. In embodiments of the present invention wherein information from multiple multimedia documents is displayed, a range is defined by a start time (R), an end time (R), and an identifier identifying the multimedia document in which the range is present. Further, as described above, identifiers (e.g., a text code, numbers, etc.) may be used to identify each range. The range identifier for a range may be displayed in the rectangular box corresponding to the range or in some other location on the user interface.","Each thumbar in  also includes a relevance indicator  that indicates the degree of relevance (or a relevancy score) of the multimedia document whose contents are displayed in the thumbar to the user-specified search criteria (e.g., user-specified word or phrase, search query, topics of interest, etc.). Techniques for determining the relevancy score or degree of relevance have been described above. According to an embodiment of the present invention, the degree of relevance for a thumbar is based upon the frequency of hits in the multimedia document whose contents are displayed in the thumbar. In the relevance indicators depicted in , the degree of relevance of a multimedia document to the user-specified criteria is indicated by the number of bars displayed in the relevance indicators. Accordingly, the first and third multimedia documents whose contents are displayed in thumbars  and  are more relevant (indicated by four bars in their respective relevance indicators) to the current user-specified criteria (i.e., search query including the word \u201cStewart\u201d) than the second multimedia document displayed in thumbar  (only one bar in the relevance indicator). Various other techniques (e.g., relevance scores, bar graphs, different colors, etc.) may also be used to indicate the degree of relevance of the multimedia documents.","As previously described, various operations may be performed on ranges. The operations performed on a range may include printing a representation of the contents of the range on a paper document, saving the contents of the range, communicating the contents of the range, etc. Ranges can also be annotated or highlighted or grouped into sets. Ranges in a set of ranges can also be ranked or sorted according to some criteria that may be user-configurable. For example, ranges may be ranked based upon the relevance of each range to the user specified search criteria. According to an embodiment of the present invention, a range with higher number of hits may be ranked higher than a range with a lower number of hits. Other techniques may also be used to rank and\/or sort ranges.","The user may also select one or more ranges displayed by the user interface and perform operations on the selected ranges. According to an embodiment of the present invention, the user may select a range by clicking on a rectangle representing the range using an input device such as a mouse. In , range  has been selected by the user. The rectangle representing range  may be highlighted (e.g., in a color different from the color of the rectangles representing the other ranges) to indicate that it has been selected. In the embodiment depicted in , a web page  is generated for the user-selected range and displayed in a window  of user interface . Web page  comprises text information  that represents the audio information or CC text information (or a text representation of some other type of information included in the multimedia information) for the selected range (i.e., text information that represents the audio information, CC text information, or other information occurring between times Rand Rof the selected range) and one or more video keyframes or images  extracted from the video information corresponding to the selected range (i.e., video keyframes extract from video information occurring between times Rand Rof the selected range). According to an embodiment of the present invention, each image  in web page  is a hypertext link and when selected starts playback of video information from a time associated with the image.","As shown in , a barcode  may be printed for each image. The barcode may represent a time associated with the image and scanning the barcode using a barcode reader or scanner may cause playback of the video information from a time associated with the image and represented by the barcode. The playback may be displayed in the video window of a multimedia document corresponding to the selected range. Information  identifying the multimedia document from which the range is selected is also displayed on web page . Each barcode  may also identify a start time and end time for a range. Scanning such a barcode using a barcode reader or scanner may cause playback of information corresponding to the range. Barcode  may also represent a label or identifier that identifies a range. Upon scanning such a barcode, the range identifier represented by the scanned barcode may be used to determine the start and end times of the range and information corresponding to the range may then be played back.","If the user has identified user-specified criteria (e.g., a word or phrase, a topic of interest, a search query, etc.) for searching the multimedia documents, then occurrences of the user-specified criteria in web page  are highlighted. For example, in , the user has specified a search query containing the word \u201cStewart\u201d, and accordingly, all occurrences  of the word \u201cStewart\u201d in web page  are highlighted (e.g., bolded).","A hypertext link  labeled \u201cComplete Set\u201d is also included in web page . Selection of \u201cComplete Set\u201d link  causes generation and display of a web page that is based upon the contents of the various ranges depicted on the thumbars across all the multimedia documents that are displayed in interface .","In alternative embodiments, other types of documents besides web pages may be generated and displayed for the ranges. According to an embodiment of the present invention, a printable representation of the selected range or ranges may be generated and displayed. Further details related to generation and display of such a printable representation of multimedia information are described in U.S. application Ser. No. 10\/001,895 filed Nov. 19, 2001, the entire contents of which are herein incorporated by reference for all purposes.",{"@attributes":{"id":"p-0338","num":"0342"},"figref":"FIG. 29F","b":["2900","2980","2971","2964"]},"As previously described, the user-specified criteria for searching the multimedia documents may also include topics of interest. Accordingly, according to an embodiment of the present invention, the contents of the one or more multimedia documents may be searched to identify portions of the multimedia documents that are relevant to topics of interest that may be specified by a user.",{"@attributes":{"id":"p-0340","num":"0344"},"figref":["FIG. 29G","FIG. 29G"],"b":["2900","2981","2983","2971","2982"]},"According to an embodiment of the present invention, a particular style or color may be associated with each topic of interest. For example, a first color may be associated with topic of interest \u201cAirlines\u201d, a second color may be associated with topic of interest \u201cmstewart\u201d, and a third topic of interest may be associated with the topic of interest \u201cbaseball\u201d. Portions of the multimedia document that are deemed to be relevant to a particular topic of interest may be highlighted using the style or color associated with the particular topic of interest. This enables the user to easily determine the portions of the multimedia documents that are relevant to a particular topic of interest.",{"@attributes":{"id":"p-0342","num":"0346"},"figref":["FIG. 29H","FIG. 29H"],"b":"2981"},"In , the playback of video information for a particular multimedia document has been moved from the video window corresponding to the multimedia document to a larger video window. As shown in , the video playback of the third multimedia document has been moved from video window  to larger video window . A mode displaying the larger video window  is activated by selecting \u201cVideo\u201d tab . The switch of the display from video window  to window  may be performed by selecting a control provided by control bar . A control bar  comprising controls for controlling playback of the video information in window  is also displayed below larger video window . Moving the playback of video information from the smaller video window  (or  or ) to larger video window  makes it easier for the user to view the video information playback. The video playback can be switched back to small window  from window  by selecting control from control bar  or by selecting control from control bar .","Text information  (e.g., CC text information, transcript of audio information, or a text representation of some other type of information included in the multimedia information) corresponding to the video playback is also displayed below larger video window . Text information  scrolls along with the video playback. Each word in text information  is searchable such that a user can click on a word to see how many times the selected word occurs in the contents of the multimedia documents and the locations where the word occurs. It should be noted that, as with the video playback in the smaller video window, the contents of the video played back in larger video window  are affected by the position of thumbnail viewing area lens of the thumbar that displays a representation of the contents of the multimedia document whose video information is played back in larger video window .","As previously described, a user may also manually define ranges for one or more multimedia documents. Techniques for manually defining ranges for a multimedia document have been previously described. In the embodiment depicted in , a button  is provided that when selected initiates a mode of operation in which manual ranges can be defined by the user. According to an embodiment of the present invention, selection of button  invokes a window like window  depicted in . In addition to the information depicted in , the window also includes an entry field allowing the user to enter information identifying the multimedia document, from the multimedia documents loaded by the user interface, for which a range is to be defined. The user can also specify the start and end times for the range. In an alternative embodiment, selection of button  initiates a mode wherein the user can manually specify a range by clicking on a portion of one of the thumbars depicted in interface  using an input device such as a mouse. Clicking a portion of a thumbar causes the display of a rectangular box representing the range. The user can manipulate the top and bottom edges of the rectangular box to configure the start time (R) and end time (R) for the range. The rectangular box itself can be moved along the thumbar.","According to an embodiment of the present invention, rectangular boxes representing ranges that are automatically generated (e.g., ranges generated based upon hits for user-specified criteria) and rectangular boxes representing manual ranges specified by a user may be displayed at the same time by interface . In order to differentiate between the manually generated and automatically generated ranges, different colors or styles may be used to display rectangular boxes that represent automatic ranges and boxes that represent manual ranges.",{"@attributes":{"id":"p-0347","num":"0351"},"figref":["FIG. 29K","FIG. 29K"],"b":["2900","2990","2900","2904","2908","2912","2984","2900"]},{"@attributes":{"id":"p-0348","num":"0352"},"figref":["FIG. 30A","FIG. 30A"],"b":["3000","3002","3004","3006","3008","3010","3012","3014","3016"]},"In addition, user interface  includes a number of web pages -, -, -, etc. that are generated for the various ranges displayed on the thumbars by rectangular boxes. According to an embodiment of the present invention, a web page is generated for each range displayed in the thumbars. The web pages (referred to as the \u201cpalette\u201d view) shown in  are generated and displayed by selecting \u201cPalette\u201d button . Accordingly, the palette of web pages includes web pages generated for the various ranges. The palette of web pages may be displayed in the form of a scrollable list as shown in . The user can add notes to the palette, annotate information to the web pages in the palette, and annotate parts of the multimedia documents with supplemental information. The ranges themselves may also be annotated. For example, annotation may be added to a range by adding a comment to the web page that displays information for the range.","In the embodiment depicted in , each web page  for a particular range comprises text information that represents the audio information, CC text information, or a text representation of some other type of information included for the particular range (i.e., text information that represents the transcribed audio information or CC text information occurring between times Rand Rof the particular range). The web page also includes one or more video keyframes or images extracted from the video information corresponding to the particular range. The images and the text information may be temporally synchronized or aligned. The images in the web page may be hypertext links and when selected start playback of video information from a time associated with the selected the image.","A barcode may be printed and associated with each image printed in a web page. The barcode may represent a time associated with the image and scanning the barcode using a barcode reader or scanner may cause playback of the video information from a time associated with the image and represented by the barcode.","For each range, information identifying the range and information identifying the multimedia document from which the range is selected may also displayed on the web page corresponding to the range. For example, in the embodiment depicted in , the start and end times  for each range are displayed on the web pages. Identifiers  identifying the ranges are also displayed. Since the multimedia documents in  correspond to television video recordings, each web page corresponding to a range also displays an icon  associated with the TV network that broadcast the information for the range.","Occurrences of user-specified criteria in each web page are highlighted. For example, in the embodiment depicted in , the search query includes terms \u201cStewart\u201d, \u201cImclone\u201d, and \u201cFaksal\u201d, and occurrences of these terms in the web pages are highlighted. Various different techniques may be used for highlighting the terms such as bolding, use of colors, different styles, use of balloons, boxes, etc. As previously described, the search query terms may also be highlighted in the representations displayed in the thumbars.","A lens  is displayed emphasizing or covering an area of a web page corresponding to a currently selected range. For example, in the embodiment depicted in , a range has been selected in thumbar , and lens  is displayed covering a portion of web page - corresponding to the selected range. The portion of web page - covered by lens  is displayed in a larger window . The user can change the positions of lens  along the length of web page -. The portion of the web page displayed in window  is changed such that it continues to correspond to the portion of web page - covered by lens . In this manner, the user may use lens  to navigate the contents of the selected web page. The user may also use the scrollbar provided by window  to scroll through the web page displayed in window . The position of lens  over web page - is changed such that it continues to correspond to the portion of web page displayed in window .","A user may chose another range by clicking on that range (i.e., by clicking on the rectangle representing the range) in the thumbars using an input device such as a mouse. In response, the position of lens  is changed such that it is displayed over a web page in the palette view corresponding to the newly selected range. The portion of the web page in the palette view covered by lens  is then displayed in window . For example, as depicted in , a different range  has been selected by the user. The user may select this range by clicking on a rectangular box corresponding to the range in thumbar . In response, lens  is drawn covering a portion of web page - corresponding to range . The portion of web page - covered or emphasized by lens  is displayed in window .","According to an embodiment of the present invention, a use may also select a range by selecting a web page corresponding to the range from the palette of web pages. The user may select a web page by clicking on that web page using an input device such as a mouse. In response, lens  is displayed on the selected web page. The portion of the web page in the palette view covered by lens  is displayed in window . The rectangular box representing the range corresponding to the newly selected web page is also highlighted to indicate selection of the range.","As described above, embodiments of the present invention can display representations of information stored by one or more multimedia documents that may have been recorded during the same or different timelines. The user can specify criteria such as words, phrases, search queries including multiple terms, topics of interest, etc., and portions of the multimedia documents that are relevant to or contain the user-specified criteria are highlighted using markers, boxes representing ranges, etc. Embodiments of the present invention can accordingly be used to compare the contents of multiple multimedia documents.","For example, the recordings of three different television programs such as \u201cClosing Bell\u201d, \u201c60 Minute II\u201d, and \u201cBusiness Center\u201d may be displayed as depicted in , and searched for user-specified criteria (e.g., words. phrases, terms in the search query, topics of interest, etc.). Portions of the three programs that are relevant to or match the user-specified criteria are highlighted. Such an ability to search across multiple multimedia documents is not provided by conventional tools. Further, based upon the results of the searches displayed by the interface, the user can easily determine the relevance of the television programs to the user criteria. Embodiments of the present invention thus can be used to analyze the contents of the multimedia documents with respect to each other. The visualization of the search results is often useful for obtaining a feel for the contents of the multimedia documents.","As another example, if the user is interested in the Imclone\/Martha Stewart scandal, the user can form a search query including the terms \u201cStewart\u201d, \u201cImclone\u201d, and \u201cWaksal\u201d (or other words related to the scandal) and portions of the representations of the multimedia documents that are displayed by the user interface and that contain the search query terms are highlighted using markers, colors, etc. Ranges may also be formed based upon the search hits and depicted on the interface using colored boxes to highlight the relevant sections. By viewing the portions of the multimedia documents highlighted in the interface, the user can easily determine how much information related to the scandal is contained in the multimedia documents and the locations in the multimedia documents of the relevant information. The user can also determine the distribution of the relevant information in the multimedia documents. The multimedia documents can also be compared to each other with regards to the search query. Embodiments of the present invention thus provide a valuable tool for a user who wants to analyze multiple multimedia documents.","The analysis and review of multiple multimedia documents is further facilitated by generating and displaying web pages corresponding to the ranges (that may be automatically generated or manually specified) displayed in the interface. The web pages generated for the ranges allow the user to extract, organize, and gather the relevant portions of the multiple multimedia documents.","Embodiments of the present invention also provide the user the ability to simultaneously watch a collection of multimedia documents. For example, the user can watch the contents of multiple video recordings or video clips. Various controls are provided for controlling the playback of the multimedia information. Portions of the multimedia documents played back by the user may be highlighted. The user can accordingly easily determine portions of the multimedia documents that the user has already viewed and portions that have not been viewed.","As previously described, several operations can be performed using ranges. These operations include, for example, printing a representation of the contents of a range on a paper document, saving the contents of a range, communicating the contents of a range, annotating a range, etc. Ranges may also be grouped (e.g., grouped into sets) and operations performed on the groups. For example, ranges in a set of ranges can also be ranked or sorted based upon some criteria that might be user-configurable. For example, ranges may be ranked based upon the relevance of each range to the user specified search criteria. According to an embodiment of the present invention, a range with higher number of hits may be ranked higher than a range with a lower number of hits. Other techniques may also be used to rank and\/or sort ranges.","Printing Multimedia Information","As previously indicated, multimedia information from one or more multimedia documents displayed by the user interfaces described above may be printed on a paper medium to produce a multimedia paper document. Accordingly, a multimedia paper document may be generated for the one or more multimedia documents. The term \u201cpaper\u201d or \u201cpaper medium\u201d may refer to any tangible medium on which information can be printed, written, drawn, imprinted, embossed, etc.","According to an embodiment of the present invention, for each multimedia document, a printable representation is generated for the recorded information stored by the multimedia document. Since the recorded information may store information of different types such as audio information, video information, closed-caption (CC) text information, slides information, whiteboard information, etc., according to an embodiment of the present invention, the printable representation of the recorded information may comprise printable representations of one or more types of information. The printable representation for the recorded information, which may include printable representations for one or more types of information that make up the recorded information, can be printed on a paper medium to generate a multimedia paper document. Various different techniques may be used for generating a printable representation for the multimedia information. Examples of techniques for generating a printable representation and printing the printable representation on a paper medium to produce a multimedia paper document are described in U.S. patent application Ser. No. 10\/001,895, filed Nov. 19, 2001, the entire contents of which are herein incorporated by reference for all purposes.","The printable representation can then be printed on a paper medium. The term \u201cprinting\u201d includes printing, writing, drawing, imprinting, embossing, and the like. According to an embodiment of the present invention, the printable representation is communicated to a paper document output device (such as a printer, copier, etc.) that is configured to print the printable version on a paper medium to generate a paper document. Various different techniques may be used for printing the printable representation on a paper medium. According to an embodiment of the present invention, the printing is performed according to the techniques described in U.S. patent application Ser. No. 10\/001,895, filed Nov. 19, 2001, the entire contents of which are herein incorporated by reference for all purposes.","In other embodiments of the present invention, instead of generating a multimedia paper document for the entire contents of the multimedia documents, a multimedia paper document may be generated only for the ranges displayed in the graphical user interface. In this embodiment, a printable representation is generated for multimedia information corresponding to the ranges, and the printable representation is then printed on a paper medium. Since multimedia information corresponding to a range may comprise information of one or more types, the printable representation of the multimedia information corresponding to the range may comprise printable representations of one or more types. Various different techniques may be used for generating a printable representation for the multimedia information corresponding to the ranges. For example, the described in U.S. patent application Ser. No. 10\/001,895, filed Nov. 19, 2001, may be used.",{"@attributes":{"id":"p-0368","num":"0372"},"figref":["FIG. 31","FIG. 31"],"b":["3100","3100","3100"]},"As depicted in , a user can specify that only information corresponding to the ranges is to be printed by selecting checkbox . If checkbox  is not selected it implies that all the contents of the one or more multimedia documents that have been loaded are to be printed. The user can indicate that information corresponding to all the displayed ranges is to be printed by selecting checkbox . Alternatively, the user can specifically identify the ranges to be printed by entering the range identifiers in input boxes . For example, if the ranges are identified by numbers assigned to the ranges, then the user can enter the numbers corresponding to the ranges to be printed in boxes . If the range identifiers are serially numbers, a list of ranges may be specified.","Selection of \u201cPrint\u201d button  initiates printing of the contents of the ranges or contents of the loaded multimedia documents. User interface  can be canceled by selecting \u201cCancel\u201d button .","Several options are provided for controlling the manner in which information corresponding to the ranges or information from the loaded multimedia documents is printed. For example, a format style may be selected for printing the information on the paper medium. In the embodiment depicted in , the user can select from one of three different styles  by selecting a checkbox corresponding to the style. , B, and C depict pages printed according to the three styles selectable from interface  depicted in  according to an embodiment of the present invention.  depicts a page printed according to Style 1.  depicts a page printed according to Style 2.  depicts a page printed according to Style 3. Various other styles may also be provided in alternative embodiments of the present invention.","The user can also select different styles  for printing keyframes extracted from video information. For example, in the embodiment depicted in , the user can select a style wherein one keyframe is printed for each barcode (or alternatively, one barcode will be printed for each printed keyframe) or multiple (e.g., 4) keyframes are be printed for each barcode.  depict pages printed according to the two keyframes styles selectable from interface  depicted in .  depicts a page wherein one keyframe is printed per barcode.  depicts a page wherein four keyframes are printed per barcode. Other styles may also be provided in other embodiments of the present invention.","A list of printers  (or any other paper document output device that can generate a print representation of multimedia information, e.g., a copier, a facsimile machine, etc.) is displayed. The user can select one or more printers from list  to print the multimedia information on a paper medium. The user can select a specific copier for performing the printing (or copying) by selecting \u201cSend to Copier\u201d checkbox  and identifying the copier to be used in box .","According to an embodiment of the present invention, the printable representation of multimedia information corresponding to the selected ranges or to the multimedia documents can be stored in memory. For example, the printable representation can be stored as a PDF file. A name for the file can be specified in entry box .","According to an embodiment of the present invention depicted in , the user has the option of indicating whether sections of the printable representation that comprise words or phrases that satisfy or match user-specified criteria are to be highlighted when the printable representation is printed on the paper medium. The user may activate this option by selecting checkbox . When this option has been selected, words or phrases in the multimedia information corresponding to the multimedia documents or the selected ranges that are relevant to the topics of interest, or match words or phrases specified by the user or search query terms are highlighted when printed on paper. Various different techniques may be used for highlighting the word or phrases on paper.","A text marker that relates the barcodes to the printed text information may be printed by selecting checkbox .","As described above, the user can specify that only information corresponding to ranges is to be printed by selecting checkbox . If desired, for each range, the user can specify a buffer time period to be added to the start and end of the range in entry box . For example, if a buffer time period of 5 seconds is specified, for each range, information corresponding to 5 seconds before the range start and corresponding to 5 seconds after the range end is printed along with the information corresponding to the range.","Embodiments of the present invention can also print a cover sheet for the printed information (either for information corresponding to ranges of information corresponding to multimedia document contents). The user can specify that a coversheet should be printed in addition to printing the contents of the ranges or multimedia documents by selecting checkbox . The cover sheet may provide a synopsis or summary of the printed contents of the multimedia documents or ranges.","Various different techniques may be used for printing a coversheet. Different styles  for the coversheet may be selected. , B, and C depict examples of coversheets that may be printed according to an embodiment of the present invention. Examples of techniques for generating and printing coversheets are described in U.S. patent application Ser. No. 10\/001,895, filed Nov. 19, 2001, the entire contents of which are herein incorporated by reference for all purposes. Examples of different coversheets are also described in U.S. patent application Ser. No. 10\/001,895, filed Nov. 19, 2001.","The coversheets may be used for several different purposes. As previously indicated, a coversheet provides a synopsis or summary of the printed contents of the multimedia documents or ranges. The coversheet may also provide a summary of information stored on a storage device. For example, for multimedia information stored on a CD, a coversheet may be generated based upon the contents of the CD that summarizes what contents of the CD. For example, as shown in , a coversheet is generated and used as a cover for a jewel case that may store the CD. In the embodiment depicted in , the barcodes printed on the CD may be used to access or index into the multimedia information stored on the CD. Techniques for using the barcodes printed on the coversheet to access the multimedia information are described in U.S. patent application Ser. No. 10\/001,895, filed Nov. 19, 2001. Various other uses of coversheets are also envisioned within the scope of the present invention.","A user may also elect to print only the coversheet and not the contents of the ranges or the multimedia documents by selecting checkbox  in . This is useful for example when a cover sheet is to be generated for providing an index into information stored on a storage device.","The coversheets depicted in , B, and C each display a limited number of keyframes sampled (e.g., sampled uniformly every N seconds) from the multimedia information for which the coversheet is generated. The sampling interval may be specified by the user. For example, in the embodiment depicted in , the user can enter the sampling interval in entry box .","The user is also provided the ability to control the quality of the printed image. In the embodiment depicted in , the user can select one of three options .",{"@attributes":{"id":"p-0384","num":"0388"},"figref":["FIGS. 35A","FIGS. 325"],"b":["35","35","35","35","35","35","35","35"]},"The document depicted in , B, C, D, and E is printed for ranges selected from three multimedia documents. The three multimedia documents are television program recordings, namely, \u201cMoney and Markets\u201d program captured from the CNN\/fn channel (Channel ), \u201cClosing Bell\u201d program captured from CNBC channel (Channel ) and \u201cStreet Sweep\u201d program also captured from the CNN\/fn channel (Channel ).","As depicted in , the contents for the ranges of the three recorded programs are printed sequentially. The contents of ranges from the \u201cMoney and Markets\u201d program recording multimedia document are printed on the pages depicted in , the contents of ranges from the \u201cClosing Bell\u201d program recording multimedia document are printed on the pages depicted in , and the contents of ranges from the \u201cStreet Sweep\u201d program recording multimedia document are printed on the page depicted in .","Information  identifying the multimedia documents from which the ranges are selected is printed as shown in , C, and E. In the embodiment depicted in , the information identifying each multimedia document includes the name of the television program, information identifying the channel from which the program was recorded, the duration of the recording, and the date and time of the recording. Other types of information related to the multimedia documents may also be printed.","The start of each range is indicated by a bar . Accordingly, contents of two ranges have been printed from the \u201cMoney and Markets\u201d multimedia document, contents of four ranges have been printed from the \u201cClosing Bell\u201d multimedia document, and contents of three ranges have been printed from the \u201cStreet Sweep\u201d multimedia document. Information  related to the range is also printed in each bar . In the embodiment depicted in , the information related to the ranges includes, an identifier for the range, a start time (R) and end time (R) for the range, and the span of the range. Other types of information related to each range may also be printed.","The information printed for each range includes text information  and one or more images . The text information is a printable representation of the audio information (or CC text, or a text representation of some other type of information included in the multimedia information) corresponding to the range. Occurrences of words or phrases occurring in the printed text information that are relevant to topics of interest, or match user-specified words or phrases or search criteria are highlighted. For example, for the embodiment depicted in , the user has defined a search query containing terms \u201cStewart\u201d, \u201cImclone\u201d, and \u201cWaksal\u201d. Accordingly, all occurrences of these search query terms are highlighted (using underlining) in the printed text sections for the various ranges. Various different techniques may also be used to highlight the words such as bolded text, different fonts or sizes, italicized text, etc.","Images  printed for each range represent images that are extracted from the video information for the range. Several different techniques may be used for extracting video keyframes from the video information of the range and for identifying the keyframes to be printed. Examples of these techniques are described above and in U.S. patent application Ser. No. 10\/001,895, filed Nov. 19, 2001. Various different styles may be used for printing the information. For example, the user may chose from styles  and  depicted in .","Barcodes  are also printed for each range. In the embodiment depicted in , a barcode  is printed for each image  and is placed below the image. Various different styles may be used for printing the barcodes. For example, in the embodiment depicted in , two different styles  are provided for printing barcodes, namely, a first style in which one barcode is printed per keyframe (as shown in ) and a second style in which a barcode is printed for every four keyframes.","According to an embodiment of the present invention depicted in , each barcode printed below an image represents a time associated with the image. Barcodes  provide a mechanism for the reader of the paper document to access multimedia information using the paper document. According to an embodiment of the present invention, scanning of a barcode using a device such as a scanner, barcode reader, etc. initiates playback of multimedia information from the multimedia document corresponding to the barcode from the time represented by the barcode. The playback may occur on any output device. For example, the information may be played back in a window of the previously described GUIs displayed on a computer screen.","Each barcode  may also identify a start time and end time for a range. Scanning such a barcode using a barcode reader or scanner may cause playback of information corresponding to the range. Each barcode  may also represent a label or identifier that identifies a range. In this embodiment, upon scanning such a barcode, the range identifier represented by the scanned barcode may be used to determine the start and end times of the range and information corresponding to the range may then be played back.","The document depicted in  thus provides a paper interface for accessing stored multimedia information. Further information related to using a paper interface for accessing multimedia information is discussed in U.S. patent application Ser. No. 10\/001,895, filed Nov. 19, 2001. Other user-selectable identifiers such as watermarks, glyphs, text identifiers, etc. may also be used in place of barcodes in alternative embodiments of the present invention. The user-selectable identifiers might be printed in a manner that does not reduce or affect the overall readability of the paper document.","A set of barcodes  are also printed at the bottom of each paper page of the paper document depicted in . Barcodes  allow a user to initiate and control playback of multimedia information using the paper document. According to an embodiment of the present invention, each barcode corresponds to a command for controlling playback of multimedia information. Five control barcodes  are printed in the embodiment shown in . Control barcode - allows the user to playback or pause the playback. For example, a user may scan a barcode  and then scan barcode - to initiate playback of information from the time represented by scanned barcode . The user may rescan barcode - to pause the playback. The playback can be fast forwarded by selecting barcode -. The user may perform a rewind operation by selecting barcode -. The playback can be performed in an enhanced mode by selecting barcode -. Enhanced mode is an alternative GUI that provides additional viewing controls and information (e.g., a specialized timeline may be displayed, controls may be provided such that on screen buttons on a PDA may be used to navigate the information that is played back). Further details related to enhanced mode display are described in U.S. application Ser. No. 10\/174, 522, filed Jun. 17, 2002, the entire contents of which are incorporated herein for all purposes. Specific modes of operation can be entered into by selecting barcode -. Barcodes for various other operations may also be provided in alternative embodiments of the present invention. Information related to barcodes for controlling playback of information is discussed in U.S. patent application Ser. No. 10\/001,895, filed Nov. 19, 2001.","Although specific embodiments of the invention have been described, various modifications, alterations, alternative constructions, and equivalents are also encompassed within the scope of the invention. The described invention is not restricted to operation within certain specific data processing environments, but is free to operate within a plurality of data processing environments. Additionally, although the present invention has been described using a particular series of transactions and steps, it should be apparent to those skilled in the art that the scope of the present invention is not limited to the described series of transactions and steps. For example, the processing for generating a GUI according to the teachings of the present invention may be performed by server , by client , by another computer, or by the various computer systems in association.","Further, while the present invention has been described using a particular combination of hardware and software, it should be recognized that other combinations of hardware and software are also within the scope of the present invention. The present invention may be implemented only in hardware, or only in software, or using combinations thereof.","The specification and drawings are, accordingly, to be regarded in an illustrative rather than a restrictive sense. It will, however, be evident that additions, subtractions, deletions, and other modifications and changes may be made thereunto without departing from the broader spirit and scope of the invention as set forth in the claims."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0019","num":"0023"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0020","num":"0024"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0021","num":"0025"},"figref":"FIG. 3","b":"300"},{"@attributes":{"id":"p-0022","num":"0026"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0023","num":"0027"},"figref":"FIGS. 5A","b":["5","5"]},{"@attributes":{"id":"p-0024","num":"0028"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0025","num":"0029"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0026","num":"0030"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0027","num":"0031"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0028","num":"0032"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0029","num":"0033"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0030","num":"0034"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0031","num":"0035"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0032","num":"0036"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0033","num":"0037"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0034","num":"0038"},"figref":"FIG. 16","b":["312","2"]},{"@attributes":{"id":"p-0035","num":"0039"},"figref":"FIG. 17","b":["314","314","306","322","322","308","310"]},{"@attributes":{"id":"p-0036","num":"0040"},"figref":"FIG. 18","b":["306","314"]},{"@attributes":{"id":"p-0037","num":"0041"},"figref":"FIG. 19","b":["308","314","316","322"]},{"@attributes":{"id":"p-0038","num":"0042"},"figref":"FIG. 20A"},{"@attributes":{"id":"p-0039","num":"0043"},"figref":"FIG. 20B"},{"@attributes":{"id":"p-0040","num":"0044"},"figref":"FIG. 21"},{"@attributes":{"id":"p-0041","num":"0045"},"figref":"FIG. 22"},{"@attributes":{"id":"p-0042","num":"0046"},"figref":"FIG. 23"},{"@attributes":{"id":"p-0043","num":"0047"},"figref":"FIG. 24"},{"@attributes":{"id":"p-0044","num":"0048"},"figref":["FIG. 25A","FIG. 24"],"sub":["i ","k "]},{"@attributes":{"id":"p-0045","num":"0049"},"figref":["FIG. 25B","FIG. 24"],"sub":["i ","j "]},{"@attributes":{"id":"p-0046","num":"0050"},"figref":"FIG. 26"},{"@attributes":{"id":"p-0047","num":"0051"},"figref":"FIG. 27"},{"@attributes":{"id":"p-0048","num":"0052"},"figref":"FIG. 28"},{"@attributes":{"id":"p-0049","num":"0053"},"figref":"FIGS. 29A","b":["29","29","29","29","29","29","29","29","29","29"]},{"@attributes":{"id":"p-0050","num":"0054"},"figref":"FIGS. 30A and 30B"},{"@attributes":{"id":"p-0051","num":"0055"},"figref":"FIG. 31"},{"@attributes":{"id":"p-0052","num":"0056"},"figref":["FIGS. 32A","FIG. 31"],"b":["32","32"]},{"@attributes":{"id":"p-0053","num":"0057"},"figref":["FIGS. 33A and 33B","FIG. 31"],"b":"31"},{"@attributes":{"id":"p-0054","num":"0058"},"figref":"FIGS. 34A","b":["34","34"]},{"@attributes":{"id":"p-0055","num":"0059"},"figref":"FIGS. 35A","b":["35","35","35","35"]}]},"DETDESC":[{},{}]}
