---
title: Parallel migration of data between systems
abstract: A system and method for migrating data, in parallel, from a source database system into a target database system includes storing data in groups (e.g., clusters) in the source database system. The groups of data are transferred, in parallel, to respective temporary staging tables in the target database system. The data in the temporary staging tables are then inserted, in parallel, into a target table in the target database system to complete the migration.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07548898&OS=07548898&RS=07548898
owner: Teradata US, Inc.
number: 07548898
owner_city: Miamisburg
owner_country: US
publication_date: 20011129
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS REFERENCE TO RELATED APPLICATION","TECHNICAL FIELD","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["This is a continuation-in-part of U.S. Ser. No. 09\/796,145, filed Feb. 28, 2001.","The invention relates to parallel migration of data between systems.","A database is a collection of stored data that is logically related and that is accessible by one or more users. A popular type of database system is the relational database management system, which includes relational tables made up of rows and columns. Each row represents an occurrence of an entity defined by the table, with an entity being a person, place, or thing about which the table contains information.","Administrators of database systems often archive contents of the systems for various reasons. For example, archiving and restoring data are steps that occur in migrating data from one database system (the source system) to another database system (the target system).","The archive and restore procedure traditionally involves transferring data from the source database system to a storage medium such as a tape or disk. Normally, if large amounts of data (e.g., gigabytes or terabytes of data) are involved, conventional systems archive the data to tape. The archived data is then loaded from the tape onto the target database system.","The data from the source database system is backed up (archived) to the tape or disk and, and via manual operator intervention, the tape or disk is then exported from the source system and imported into the target database system. The data from the source database system, which is contained on the tape or disk, can then be restored to the target database system.","For very large database systems, higher data migration transfer speeds can be obtained by executing, concurrently and in parallel, as many of these archive\/export\/import\/restore activities as can be supported by both systems. When transferring data between complex database systems, such as TERADATA\u00ae systems from NCR Corporation, the configurations of the source and target systems also place a constraint on parallelism of the data transfer. Some TERADATA\u00ae database systems include a plurality of nodes and access module processors (AMPs) for managing concurrent access of data. If the numbers of nodes and\/or AMPs are different in the source and target database systems, then distribution of contents of the tables across the nodes and\/or AMPs can be different. This may require that portions of the tables be transferred in sequence (back-to-back), which reduces parallelism and efficiency of data transfer.","Consequently, migrating large amounts of data from one system to another can take a relatively long period of time.","In general, improved data migration operations are provided between systems. For example, a database system comprises a storage subsystem to store a plurality of temporary staging tables and a target table, and an access management subsystem adapted to receive, in parallel, groups of data from a source system for storage in corresponding temporary staging tables. The access management subsystem is adapted to further insert data, in parallel, from the temporary staging tables into the target table.","Other or alternative features will become apparent from the following description, from the drawings, and from the claims.","In the following description, numerous details are set forth to provide an understanding of the present invention. However, it will be understood by those skilled in the art that the present invention may be practiced without these details and that numerous variations or modifications from the described embodiments are possible.",{"@attributes":{"id":"p-0020","num":"0019"},"figref":["FIG. 1","FIG. 1"],"b":["12","14","16","16","12","18","12","14","20","14","12","14","18","20"]},"The storage modules  or  in respective database systems  or  are implemented as hard disk drives, disk arrays, tape drives, or other magnetic, optical, or other type of media. The designation of \u201csource database system\u201d and \u201ctarget database system\u201d can be switched if migration of data is from the database system  to the database system .","In accordance with one embodiment of the invention, data migration occurs over the data network  between the source and target database systems. Alternatively, the data migration involves archiving data from the source database system  to an intermediate storage system  (e.g., tape drives, disk drives, etc.), followed by a restore of data from the intermediate storage system  to the target database system . Note that the intermediate storage system  is not used in some embodiments where data is transferred by a concurrent archive and restore process, as further described below. The database system  includes plural nodes . Each node  includes one or plural access module processors (AMPs) . AMPs are software components executable in each node  for managing access to data contained in respective storage modules . The target database system  also includes AMPs . Although not shown, the AMPs  are also executable on respective nodes.","Each AMP  or  includes an access or database manager that creates, modifies, or deletes definitions of tables; inserts, deletes, or modifies rows within the tables; retrieves information from definitions and tables; and locks databases and tables. To access data stored in relational tables in the storage modules  and  in respective source and target database systems  and , queries are submitted to respective parsing engines  and . In the source database system , the parsing engine  is executable in a node . Although only one parsing engine is shown, the database system  in an alternative embodiment includes plural parsing engines , which are also executable in nodes . The parsing engine(s)  and the AMPs  are interconnected by an interconnect layer .","Similarly, the parsing engine  is executable in a node  in the target database system . Alternatively, the target database system  can also include multiple parsing engines. The parsing engine  and the AMPs  are interconnected by an interconnect layer .","Upon receipt of a query, the parsing engine  or  interprets the query, checks the query for proper syntax, and sends out executable actions to be performed by the AMPs  or . In one embodiment, queries that are processed by the parsing engine  include queries according to a standard database query language, such as Structured Query Language (SQL). One version of SQL is the SQL-92 Standard, while another version of SQL is the SQL-99 Standard (also referred to as the SQL-3 Standard). Also connected to the network  is a parallel job manager . Generally, the parallel job manager  is responsible for managing various tasks to be performed in plural nodes or by plural AMPs in the source and target database systems  and . The parallel job manager  is also responsible for the management of parallel data migration from the source database system  to the target database system . The parallel job manager  manages the creation of any necessary tables (such as temporary staging tables, discussed below) for performing parallel migration. The parallel job manager  also schedules tasks (e.g., launch software routines) in the source and target database systems to perform data transfers during the migration. In one embodiment, the various tasks performed by the parallel job manager  are performed by creating scripts. In addition, the parallel job manager  also generates the appropriate migration plan (e.g., script generation, runtime workload distribution, job launch, and so forth) and executes parallel steps in the correct sequence.","The parallel job manager  in one arrangement is a software module executable in a system separate from the database systems  and , such as a client terminal  coupled to the network  (shown in ). Alternatively, the parallel job manager  is executable in one or both of the database systems  and .","As shown in , the client terminal  also includes a processor  on which the parallel job manager , scripts, and other software routines associated with the parallel job manager  are executable. The processor  is coupled to a storage , which stores a configuration file  editable by an operator and accessible by the parallel job manager  to perform data migration. The configuration file  contains control information for data migration between the source and target database systems  and . The client terminal  also includes a display  and a user interface  (e.g., mouse, keyboard, graphical user interface, etc.).",{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 2","b":["12","14","100","12","100"]},"In one embodiment, the source table  is stored by clusters  of AMPs . A cluster  is a group of AMPs that act as a single fallback unit. Fallback is used for protecting data within each cluster. Each row of a table stored by a first AMP is also copied to another AMP in the same cluster. If the first AMP fails, then the system can access the fallback row in the second AMP so that database system operations can continue despite failure of the first AMP. Generally, AMPs in each cluster are from different nodes. Thus, if one node in the database system fails, then data access can continue by using fallback in each of multiple clusters.","Although reference is made to clusters in the described examples, the invention is not to be limited in scope to migration techniques that involve clustering. In other embodiments, other groupings of data can be used.","Definitions about the source table  are stored in a data dictionary . The data dictionary includes one or more system tables containing definitions of all objects in the database system. The information contained in the data dictionary  is \u201cdata about data\u201d or \u201cmetadata.\u201d Examples of information contained in the data dictionary  include information pertaining to characteristics of each table in the database, including column names, the data type for each column, the primary key, and indexes defined on each table. Other information includes information pertaining to whether fallback is used, the owner of the table, the creator of the table, associated privileges per user, journaling definitions for rollback\/rollforward-type operations, and other information. Archive\/restore module instances  are launched (under control of the parallel job manager ) in the source database system . Each archive\/restore module instance  controls the archiving of data from the source table  and the restore of the data to the target database system . The number of archive\/restore module instances  that are launched is specified in the configuration file .","The specified number of archive\/restore module instances  is based on the software\/hardware arrangements of the source and target database systems. For example, the number of network ports present on the nodes of the source and target database systems can control the number of network connections between the source and target database systems, and thus the possible number of concurrent data transfer streams between the source and target database systems. For example, if there are 40 separate network connections between the source and database systems, then 40 archive\/restore module instances  can be executed to concurrently transfer migration data across the 40 network connections.","When migrating data from the source table  in the source database system  to the target database system , each archive\/restore module instance  transfers (archives and restores) data from a cluster  to a respective one of plural temporary staging tables (referred to as \u201cTempTable\u201d)  in the target database system . The temporary staging tables  are used to temporarily store the migration data as the migration data is being transferred from the source database system  to the target database system . Once the data from the source table  has been transferred to the temporary staging tables , a merge request is issued by the parallel job manager  to the target database system  to write data from the temporary staging tables  into a target table .","Each staging table  is used to temporarily store data from a respective cluster  in the source database system . In one embodiment, there is a predefined relationship of clusters  to the staging tables . For example, there can be a one-to-one relationship between one cluster and one staging table. Alternatively, multiple clusters can be assigned to one staging table. Thus, for example, clusters  and  are assigned to a first temporary and unique staging table, clusters  and  are assigned to a second temporary and unique staging table, and so forth.","The staging tables  are useful when the configurations of the source database system  and target database system  are different. For example, the source database system  may have more or less AMPs than the target database system . As a result of the different configurations, distribution of a table across the AMPs will differ. For example, if a database system has M AMPs, then a table is distributed as evenly as possible across the M AMPs. If another database system has N AMPs (M N), then the same table is distributed across N AMPs. The distributions of the table across different numbers of AMPs are thus different.","Conventionally, in some database systems, differing configurations of the source and target database systems prevent the parallel migration of clusters of data. In some conventional systems, the migration of the clusters is performed in sequence (instead of in parallel), resulting in use of only one data stream (e.g., network connection) from source to target system even though many parallel data streams (e.g., network connections) are available, which slows down the migration process. To enable the parallel migration of data via optimal usage of all available data streams (e.g., network connections) despite different configurations of the source and target database systems, the temporary staging tables  are used as buffers to directly receive the data from respective sets (one or more) of clusters . In other words, data is migrated (in parallel) from sets of one or plural clusters to respective staging tables . Data in the staging tables  is next inserted, in parallel, into the target table , which takes advantage of the parallelism of the target database system .","In some cases, once the data has been migrated from the source system to the temporary staging tables, all the rows are already properly distributed to the AMPs that they are assigned to. A re-distribution of these rows in the final merge phase is therefore not required. When all rows from the temporary staging tables are inserted into a corresponding portion of the target table, each AMP is executing the insert in parallel and the rows the AMP is inserting are already properly distributed. As a result, no redistribution takes place in the final merge phase from the temporary staging tables to the target table. Such an operation is referred to as \u201can AMP local operation\u201d in which each AMP is doing the insert in parallel, taking advantage of the parallel architecture of the database system, which is a TERADATA\u00ae parallel system in one example embodiment. In this arrangement, each temporary staging table has an uneven distribution of the rows in which only those AMPs that are assigned the rows are populated but other AMPs may have significantly less or zero rows. However, the rows in all the temporary staging tables are evenly distributed and cover all AMPs so when the insert operation is executed all AMPs are isolated and working in parallel to do the insert from the temporary staging tables to the target table.","As shown in , staging tables TempTable  are distributed across the storage modules  in the target database system . The target table  is also distributed across the storage modules . Note that the number of staging tables  is not necessarily the same as the number of storage modules . The number of staging tables  created depends on the number of concurrent archive\/restore module instances  running in the source database system . The number of storage modules  depends on the number of AMPs  in the target database system . In addition, data dictionary definitions  are created in the target database system . The data dictionary definitions  are either stored in one storage module  or distributed across the storage modules . In creating the staging tables , the parallel job manager  first copies the data dictionary definition in the data dictionary  from the source database system  to the target database system . Using the copied definitions to create the staging tables , the characteristics of the staging tables  are defined identically as those of the source table . Thus, the staging tables  have the same columns, primary key, indexes, fallback or non-fallback definitions, and so forth, as the source table . This allows the concurrent transfer of data directly from each set of one or more clusters  of the source table  into a corresponding staging table . Other source table  definitions, such as information pertaining to owner, creator, privileges per user, and other information, are not necessarily associated with the staging tables .","In another arrangement, as shown in , each staging table  is used to receive data of multiple clusters. Thus, as an example, the source table  is divided into sets of two clusters each. Thus, a first set  includes clusters  and , a second set  includes clusters  and , and so forth. Parallel archive\/restore module instances  are executable in the source database system  to transfer data from each set  to a respective staging table . The parallel merge phase is then performed to merge the data in the staging tables  into the target table . Note that the parallel archive\/restore module instances  are executed in the target database system  in an alternative embodiment.","The source tables in the source database system  to migrate to respective target tables in the target database system  are specified in the configuration file , which is accessible by the parallel job manager . For example, as shown in , the source database system  includes two or more source tables _, _N that have been selected for migration. To perform the data migration, a first set of archive\/restore module instances _, and a second set of archive\/restore module instances _N are launched in the source database system .","Also, respective sets of staging tables _, _N are created in the target database system , with each set corresponding to a respective one of the source tables _, _N. The staging tables _ are defined according to data dictionary definitions _, which are copied definitions for the source table _. Similarly, the staging tables _N are defined according to data dictionary definitions _N, which are copied definitions for the source table _N.","Each set of archive\/restore module instances (one of _, _N) then transfers the data from respective clusters of source table (one of _, _N) to the corresponding set of staging tables (one of _, _N). Subsequently, data from the staging tables are merged into the target table (one of _, _N).","In another embodiment, instead of launching multiple sets of archive\/restore module instances _, _N to perform the migration of multiple source tables, only a single set of archive\/restore module instances are launched. In this embodiment, each archive\/restore module instance is assigned a unique cluster (or set of clusters) for all source tables. Also, one archive\/restore module instance is assigned per network connection. The archive\/restore module instances are run in parallel to perform the migration, with the migration of the different tables performed back-to-back (or in sequence). In other words, the archive\/restore module instances migrate, in parallel, clusters of a first table, followed by a migration of clusters of a second table, and so forth. One benefit offered by this embodiment is that the archive\/restore module instances do not have to contend for network connection bandwidth with other archive\/restore module instances.","In accordance with some embodiments of the invention, the archive\/restore module instances  shown in  are part of a concurrent archive and restore mechanism in the source database system . Generally, the concurrent archive and restore mechanism involves the concurrent execution of an archive process and a restore process, with a relatively fast transfer medium defined between the archive and restore processes. Each pair of an archive process and restore process makes up one of the archive\/restore instances . The archive process includes an archive utility module , and the restore process includes a restore utility module , as shown in .","Each node  in the source database system  includes a gateway  (designated as the local gateway). The gateway  generally manages communications between a utility or application, such as the archive utility module , and the database software (including the one or more AMPs ). In one embodiment, the gateway  establishes and manages sessions (in response to a number of sessions specified by a user) during which the one or more AMPs  perform database access operations for the utility or application. A directive, such as one issued by the parallel job manager , can indicate if all or a subset of AMPs  are selected for communication with the utility or application in each node .","The archive utility module  issues archive requests to the AMPs  through a call level interface (CLI) application programming interface (API) . The archive utility module  includes an input\/output (I\/O) layer that is capable of communicating with a transfer medium .","In one embodiment, the node  runs a UNIX operating system (OS) . Alternatively, other types of operating systems can be employed in the node . In an embodiment in which the operating system is a UNIX operating system, the archive utility module  is a UNIX process, as are other software components in the node . The node  also includes the restore utility module , which contains an I\/O layer for communicating with the transfer medium .","In one embodiment, the transfer medium  is a UNIX pipe, which is a file type defined in a UNIX system. A pipe allows the transfer of data between UNIX processes in a first-in-first-out (FIFO) manner. There are currently two kinds of UNIX pipes: a named pipe and an un-named pipe. A named pipe and an un-named pipe are similar except for the manner in which they are initialized and how processes can access the pipe. A writer process (such as the archive utility module ) writes into one end of a pipe and a reader process (such as the restore utility module ) reads from the other end of the pipe. There can be greater than one writer and reader of a pipe. In the following description, it is assumed that the operating system  is a UNIX operating system and that the archive and restore utility modules  and  are UNIX processes. In other types of systems, other types of operating systems and processes, threads, or execution entities can be employed.","In another embodiment, the transfer medium  includes a buffer, such as a buffer allocated in a memory  of the node . In yet another embodiment, the transfer medium  includes a shared memory accessible by plural processes.","The archive utility module  converts data retrieved from the storage module  into archive blocks of data, which are then written through its I\/O layer to the pipe . The restore utility module  receives the blocks of data from the pipe  through its I\/O layer. In one embodiment, the archive utility module  and restore utility module  are different instantiations of the same software code. Different input strings are provided during different instantiations of the software code to cause one instance to behave as an archive process while another instance behaves as a restore process.","The restore utility module  outputs the restored data through a CLI , a network interface , and the data network  to the target database system . The network interface  includes various layers to enable communications over the network . For example, the layers include physical and data link layers, which can be in the form of a network adapter (e.g., an Ethernet adapter). Also, in one example, the layers include an Internet Protocol (IP) and Transmission Control Protocol (TCP) or User Datagram Protocol (UDP) stack. One version of IP is described in Request for Comments (RFC) 791, entitled \u201cInternet Protocol,\u201d dated September 1981; and another version is described in RFC 2460, entitled \u201cInternet Protocol, Version 6 (IPv6) Specification\u201d dated December 1998. TCP is described in RFC 793, entitled \u201cTransmission Control Protocol,\u201d dated September 1981; and UDP is described in RFC 768, entitled \u201cUser Datagram Protocol,\u201d dated August 1980. TCP and UDP are transport layers for managing connections between network elements over an IP network.","In the target database system , each node  also includes a network interface  that is coupled to the data network . The network interface  includes the same or similar layers as the network interface . In addition, a gateway  (designated as the remote gateway) resides in the node . The remote gateway  provides functions that are similar to those of the local gateway  in the source database system . The remote gateway  receives restored data from the restore utility module  through the network interface . The remote gateway  then provides the data to the AMP , which writes the data into the storage modules .","An operating system  also resides in the node . In one example, the operating system  is a UNIX operating system, although other types of operating systems can be employed in further embodiments. The various software components of the node  are executable on a control unit , which is coupled to a memory  for storing data and instructions. Similarly, in the node  of the source database system , software components are executable on a control unit , which is coupled to the memory .","By transferring data through a pipe created or defined by the archive utility  and managed by the operating system , high data transfer rates can be accomplished between the archive and restore utility modules  and . This is due to the fact that the pipe is defined in the main memory of the node. Consequently, data transfers to a disk or other relatively slow storage device can be avoided. Another benefit offered by the pipe  is that the archive and restore utility modules  and  can be run concurrently (with one writing archive data into the pipe  and the other reading the archive data from the pipe  for output through the network interface  to the target database system ). The archive and restore utilities are run concurrently as separate processes or threads to enable the concurrency of execution","The parallel job manager  manages the parallel archive and restore mechanism described above. The parallel job manager  divides the archive and restore job into separate portions for execution by the plural archive and restore modules to balance the workload.",{"@attributes":{"id":"p-0056","num":"0055"},"figref":"FIG. 7","b":["12","14","100","17","170","12","100","17"]},"Restore module instances  are executable in the target database system  to restore data from respective segments of the intermediate storage system into respective staging tables . Note, that in this arrangement, the archiving of data is first performed into the intermediate storage system , followed by the restoring of data from the intermediate storage system  into the staging tables . This is compared with the concurrent archive and restore mechanism used in the embodiment of , in which the archive and restore processes can concurrently proceed.","After data has been restored into the staging tables , the rows of the staging tables  are inserted into the target table .",{"@attributes":{"id":"p-0059","num":"0058"},"figref":["FIG. 8","FIG. 2"],"b":["12","14","42","304","123","120","123","123","42","302"]},"Definitions in the data dictionary  in the source database system  are then copied (at ) from the source database system  to the target database system . Using the data dictionary definitions ( in ), the staging tables  are created (at ) by using SQL CREATE TABLE statements submitted by the parallel job manager . Once the staging tables  are created in the target database system , the parallel job manager  launches (at ) the parallel archive\/restore module instances  in the source database system . The archive\/restore module instances  archive (at ) the data from the clusters of the source table . Simultaneously (if the concurrent archive and restore mechanism is used), the archived data is restored to the staging tables  in the target database system . Once all data has been transferred into the staging tables , the parallel job manager  issues (at ) an SQL INSERT statement to select all rows from the staging tables  to insert into the target table . An example INSERT statement is as follows:","INSERT INTO TARGETTABLE","SELECT * FROM TEMPTABLE(S)","The insert operation is performed in parallel by the AMPs  () in the target database system . Note that, in one embodiment, a single \u201cmulti-statement\u201d SQL INSERT\/SELECT statement is generated. This provides that the target table is considered an empty table when the merge operation starts, which causes the database system to optimize this SQL statement such that there will not be additional processing and disk I\/O overhead associated with non-empty tables, which may occur if separate multiple SQL INSERT\/SELECT statements (one per temporary staging table) are used. The additional processing and overhead includes transient journaling, in which housekeeping information is kept during the operation to allow the transaction to be backed out (rolled back) in the event the transaction does not complete successfully. By doing the merge with a single SQL statement, the database system knows that the table at the start of the operation is empty, so no such housekeeping data is needed or kept. If the transaction fails, the table is still left empty with no rollbacks required.",{"@attributes":{"id":"p-0064","num":"0063"},"figref":["FIG. 9","FIG. 6"]},"An archive operation is started in response to a directive, such as from the parallel job manager  (). In response to the archive directive, the archive utility module is instantiated followed by instantiation of the restore utility module. The archive utility module opens (at ) a pipe, which as discussed above is used for the transfer of data between the archive utility module and the restore utility module. In creating a pipe in the UNIX operating system, according to one example, a file descriptor for reading from the pipe and another file descriptor for writing to the pipe are created. The file descriptors enable the archive utility and restore utility modules to write to and read from, respectively, the pipe.","After the pipe has been created, the archive utility module sends (at ) an archive request, in a defined session, to the source AMP. Although a single AMP is described in this example, it is noted that plural AMPs may be involved. The request contains a table identifier to identify the source table that is to be archived. Upon receiving the archive request, the source AMP recognizes the database access operation as an archive operation. The source AMP then reads (at ) data from the source table and collects the data into parcels, with each parcel varying in size, up to a predetermined maximum size.","The archive data parcels (including data, table definitions, and other information) are transferred (at ) from the source AMP to the archive utility module. The archive utility module then writes (at ) a length indicator to the pipe. The length indicator contains a value that indicates the amount of archive data that is to be transferred to the restore utility module. The parcels are encapsulated in datablocks and transferred through the pipe. In one example, a length indicator is sent before each datablock so that the restore utility module will know how much data is in the next datablock. The length indicator can also specify an end-of-data indication to terminate the data transfer. Once the restore utility module is instantiated, it continuously monitors the pipe for data from the archive utility module. When the restore utility module detects (at ) the length indicator (which has a header with a special flag), the restore utility module knows that archive datablocks are going to be coming over the pipe. The archive utility module writes (at ) datablocks to the pipe, with the restore utility module reading the datablocks (at ) from the pipe. The restore utility unblocks and unpacks the received datablocks into parcels for communication to the target access module processor.","In one embodiment, writing and reading is done in a \u201cstreaming\u201d fashion, with the archive utility continuously writing to the pipe (as long as the pipe has not filled up), and the restore utility module continuously reading from the pipe. More generally, the pipe is one example of a transfer medium that communicates data in a stream, with the archive module writing data to one end of the stream and the restore module reading from another end of the stream. In some embodiments, the transfer medium is implemented with high-speed, volatile storage devices (such as integrated circuit or semiconductor memory devices), which are typically used for the main memory of most computer systems.","Both the archive utility module and the restore utility modules are active concurrently in performing the archive and restore operation. The terms \u201ccontinuously\u201d or \u201cconcurrently\u201d as used here does not require that the archive and restore utility modules must both be writing and reading, respectively, at exactly the same time to and from the pipe. The archive and restore utility modules can actually access the pipe or other transfer medium in a time-shared manner. The significant aspect of some embodiments is that the archive and restore utility modules are both active to enhance data transfer efficiency.","The restore utility module then transfers (at ) the parcels received from the pipe to the target AMP. Next, the target AMP writes (at ) the rows contained in each parcel to the respective staging table in the target database system. When the archive operation is complete, the archive utility writes an end-of-data indicator to the pipe, which is subsequently read by the restore utility. Both archive and restore utilities then shut down and terminate.","The various systems discussed each includes various software routines or modules (such as the parallel job manager , AMPs, and others). Such software routines or modules are executable on corresponding control units or processors. Each control unit or processor includes a microprocessor, a microcontroller, a processor module (including one or more microprocessors or microcontrollers), or other control or computing devices. As used here, a \u201ccontroller\u201d refers to a hardware component, software component, or a combination of the two. \u201cController\u201d can also refer to plural components (software, hardware, or a combination thereof).","The storage devices referred to in this discussion include one or more machine-readable storage media for storing data and instructions. The storage media include different forms of memory including semiconductor memory devices such as dynamic or static random access memories (DRAMs or SRAMs), erasable and programmable read-only memories (EPROMs), electrically erasable and programmable read-only memories (EEPROMs) and flash memories; magnetic disks such as fixed, floppy and removable disks; other magnetic media including tape; and optical media such as compact disks (CDs) or digital video disks (DVDs). Instructions that make up the various software routines or modules in the various systems are stored in respective storage devices. The instructions when executed by a respective control unit cause the corresponding system to perform programmed acts.","The instructions of the software routines or modules are loaded or transported to each system in one of many different ways. For example, code segments including instructions stored on floppy disks, CD or DVD media, a hard disk, or transported through a network interface card, modem, or other interface device are loaded into the system and executed as corresponding software routines or modules. In the loading or transport process, data signals that are embodied in carrier waves (transmitted over telephone lines, network lines, wireless links, cables, and the like) communicate the code segments, including instructions, to the system. Such carrier waves are in the form of electrical, optical, acoustical, electromagnetic, or other types of signals.","While the invention has been disclosed with respect to a limited number of embodiments, those skilled in the art will appreciate numerous modifications and variations therefrom. It is intended that the appended claims cover such modifications and variations as fall within the true spirit and scope of the invention."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIGS. 2-4"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 9"}]},"DETDESC":[{},{}]}
