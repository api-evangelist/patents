---
title: System and method for customized I/O fencing for preventing data corruption in computer system clusters
abstract: Systems, methods, apparatus and software can implement a flexible I/O fence mechanism framework allowing clustered computer systems to conveniently use one or more I/O fencing techniques. Various different fencing techniques can be used, and fencing mechanism can be customized.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07590737&OS=07590737&RS=07590737
owner: Symantec Operating Corporation
number: 07590737
owner_city: Cupertino
owner_country: US
publication_date: 20040716
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION"],"p":["The present invention relates to the field of fault tolerance in distributed computer systems and, more particularly, to preventing data corruption on a shared resource of a computer system cluster.","Distributed computing systems are an increasingly important part of research, governmental, and enterprise computing systems. Among the advantages of such computing systems are their ability to handle a variety of different computing scenarios including large computational problems, high volume data processing situations, and high availability situations. For applications that require the computer system to be highly available, e.g., the ability to maintain the system while still providing services to system users, a cluster of computer systems is a useful implementation of the distributed computing model. In the most general sense, a cluster is a distributed computer system that works together as a single entity to cooperatively provide processing power and mass storage resources. With a cluster, the processing load of the computer system is typically spread over more than one computer, thereby eliminating single points of failure. Consequently, programs executing on the cluster can continue to function despite a problem with one computer in the cluster. In another example, one or more computers of the cluster can be ready for use in the event that another computer in the cluster fails. While each computer in a cluster typically executes an independent instance of an operating system, additional clustering software is executed on each computer in the cluster to facilitate communication and desired cluster behavior.",{"@attributes":{"id":"p-0004","num":"0003"},"figref":["FIG. 1","FIG. 6","FIG. 1"],"b":["100","110","120","100","110","120","110","120","150","150","140","110","120","150","110","120","130","130","110","120","100","130","130"]},"Other elements of cluster  include storage area network (SAN) , SAN switch , and storage devices such as tape library  (typically including one or more tape drives), a group of disk drives  (i.e., \u201cjust a bunch of disks\u201d or \u201cJBOD\u201d), and intelligent storage array . These devices are examples of the type of storage used in cluster . Other storage schemes include the use of shared direct-attached storage (DAS) over shared SCSI buses. SAN  can be implemented using a variety of different technologies including fibre channel arbitrated loop (FCAL), fibre channel switched fabric, IP networks (e.g., iSCSI), Infiniband, etc.","SAN switch  and storage devices , , and  are examples of shared resources. The most common shared resource in a cluster is some form of shared data resource, such as one or more disk drives. Using a shared data resource gives different nodes in the cluster access to the same data, a feature that is critical for most cluster applications. Although a disk device is perhaps the most common example of both a shared resource and a shared data resource, a variety of other types of devices will be well known to those having ordinary skill in the art. Moreover, although servers  and  are shown connected to storage array storage devices through SAN switch  and SAN , this need not be the case. Shared resources can be directly connected to some or all of the nodes in a cluster, and a cluster need not include a SAN. Alternatively, servers  and  can be connected to multiple SANs. Additionally, SAN switch  can be replaced with a SAN router or a SAN hub.","One known problem among computer system clusters occurs when one or more of the nodes of the cluster erroneously believes that other node(s) are either not functioning properly or have left the cluster. This \u201csplit-brain\u201d condition results in the effective partitioning of the cluster into two or more subclusters. Causes of the split-brain condition include failure of the communication channels between nodes, e.g., failure of private network , and the processing load on one node causing an excessive delay in the normal sequence of communication among nodes, e.g., one node fails to transmit its heartbeat signal for an excessive period of time. For example, if cluster  is configured for failover operation with an application program operating on server A  and server B  existing in the cluster to takeover for server A should it fail, then complete failure of private network  would lead server B to conclude that server A has failed. Server B then begins operation even though server A has not in fact failed. Thus, the potential exists that the two servers might attempt to write data to the same portion of one of the storage devices thereby causing data corruption. The solution is to ensure that one of the nodes cannot access the shared resource, i.e., to \u201cfence off\u201d the node from the resource.","Cluster partitioning can take a variety of other forms and have a variety of detrimental effects. For example, a node might attempt to reenter a cluster after the node has already been successfully excluded from the cluster. Thus, the reentering node might encounter a cluster environment setup to exclude the node and interpret that instead as a partition event. Additionally, cluster partitioning can be problematic even though there is no shared resource among the cluster nodes, so called \u201cshared nothing\u201d clusters. For example, if one node of a cluster is supposed to be the node interacting with a client device and another node detects a cluster partition, the client device could ultimately communicate with the wrong node thereby leading to some manner of error.","Many existing solutions to the split-brain problem focus on a single technique or mechanism for determining which nodes should remain in a cluster and how to protect shared data subsequent to a cluster partition event. One example of such a solution can be found in the pending U.S. patent application Ser. No. 10\/105,771, entitled \u201cSystem and Method for Preventing Data Corruption in Computer System Clusters,\u201d naming Bob Schatz and Oleg Kiselev as inventors, and filed on Mar. 25, 2002 (\u201cthe '771 application\u201d) which is hereby incorporated by reference herein in its entirety.","While techniques such as those described in the '771 application adequately address split-brain problems, they may suffer some other deficiency that makes them less desirable. For example, fencing techniques that make use of SCSI-3 persistent reservation commands (such as those described in the '771 application) can require the use of specialized hardware such as SCSI-3 compliant devices. This requirement may impose certain cost or flexibility restrictions that make the particular technique less desirable. Moreover, some cluster implementations may benefit from the use of multiple different fence mechanisms, rather than a single fence mechanism.","Accordingly, it is desirable to have a generalized I\/O fencing framework for providing and using one or more scalable, flexible, and robust I\/O fencing schemes for handling cluster partition conditions.","It has been discovered that systems, methods, apparatus and software can implement a flexible I\/O fence mechanism framework allowing clustered computer systems to conveniently use one or more I\/O fencing techniques. Various different fencing techniques can be used, and fencing mechanism can be customized.","Accordingly, one aspect of the present invention provides a method. At least one of a plurality of input\/output (I\/O) fence mechanisms is selected. The at least one of a plurality of I\/O fence mechanisms uses at least one coordinator resource. Computer system cluster (including a plurality of nodes) partitioning is detected. An attempt is made to gain control of the at least one coordinator resource using the at least one of a plurality of I\/O fence mechanisms. At least one of the plurality of nodes is removed from the computer system cluster in response to the attempting to gain control.","In another aspect of the present invention, a system includes a plurality of fence mechanism modules, a fence mechanism manager configured to invoke at least one of the plurality of fence mechanism modules, and a fence driver in communication with the fence mechanism manager. Each fence mechanism module configured to implement at least one input\/output (I\/O) fence mechanism. The at least one of the plurality of fence mechanism modules uses at least one coordinator resource. The fence driver is configured to receive an indication that a distributed computer system has partitioned into at least two subclusters and to cause at least one of the fence mechanism manager and the at least one of the plurality of fence mechanism modules to attempt to control of the at least one coordinator resource.","Still another aspect of the present invention provides a computer readable medium comprising program instructions executable on a processor, the computer readable medium being at least one of an electronic storage medium, a magnetic storage medium, an optical storage medium, and a communications medium conveying signals encoding the instructions. The program instructions are operable to implement each of: selecting at least one of a plurality of input\/output (I\/O) fence mechanisms, wherein the at least one of a plurality of I\/O fence mechanisms uses at least one coordinator resource; detecting when a computer system cluster including a plurality of nodes is partitioned; attempting to gain control of the at least one coordinator resource using the at least one of a plurality of I\/O fence mechanisms; and removing at least one of the plurality of nodes from the computer system cluster in response to the attempting to gain control.","Yet another aspect of the present invention provides an apparatus including: a means for selecting at least one of a plurality of means for performing input\/output fencing, wherein the at least one of a plurality of means for performing I\/O fencing uses at least one coordinator means; a means for detecting when a computer system cluster including a plurality of nodes is partitioned; a means for attempting to gain control of the at least one coordinator means using the at least one of a plurality of means for performing I\/O fencing; and a means for removing at least one of the plurality of nodes from the computer system cluster.","The foregoing is a summary and thus contains, by necessity, simplifications, generalizations and omissions of detail; consequently, those skilled in the art will appreciate that the summary is illustrative only and is not intended to be in any way limiting. As will also be apparent to one of skill in the art, the operations disclosed herein may be implemented in a number of ways including implementation in hardware, i.e. ASICs and special purpose electronic circuits, and such changes and modifications may be made without departing from this invention and its broader aspects. Other aspects, inventive features, and advantages of the present invention, as defined solely by the claims, will become apparent in the non-limiting detailed description set forth below.","The following sets forth a detailed description of at least the best contemplated mode for carrying out the one or more devices and\/or processes described herein. The description is intended to be illustrative and should not be taken to be limiting.",{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 2"},"Memory  is a representative memory space of a cluster node. In general, the software components are divided into those components operating at the kernel level, and those operating at the user level, as is well known in the art. Kernel level components include some basic software components supporting cluster operation. For example, low latency transport  provides high-speed kernel-to-kernel communications and monitors network connections between nodes. Node membership and messaging  is a kernel component that maintains and monitors node membership in the cluster. Node membership and messaging  can also provide a messaging mechanism to other software components, e.g., file system , process membership , volume manager , and cluster server software . Alternatively, the functions performed by node membership and messaging  could be performed by software operating at the user level. SCSI pass through  is an example of a hardware driving component for sending SCSI commands from other components to SCSI devices, such as the data disks in devices  and  and certain coordinator resources used by I\/O fence mechanisms that are designed to receive SCSI commands, i.e., SCSI-2 and SCSI-3 compliant devices.","Fence driver  operates in conjunction with fence daemon  and various fence mechanisms  and  to prevent ejected cluster nodes, i.e., nodes that have lost cluster membership because of some connectivity failure, from accessing shared storage resources, causing data corruption, or otherwise disrupting expected cluster behavior (e.g., shared-nothing cluster operation). For example, fence driver  receives node membership information either directly or indirectly from node membership and messaging component . Once node membership and messaging  learns that communication with another cluster node has been lost, i.e., that the cluster has partitioned, it can directly inform fence driver , which in turn begins taking action to protect shared resources. Alternately, node membership and messaging  informs other kernel components such as file system , process memberships , or volume manager , that communication with another cluster node has been lost. Subsequent communication between the informed kernel component(s) and fence driver  is the mechanism by which partition event information is passed to fence driver . For example, upon receiving cluster membership change information from component , a component such as process membership  can query fence driver  to determine whether it is safe, e.g., the split-brain condition has been resolved, to process membership change information. Fence driver  will not typically allow process membership  to proceed until fencing operations are sufficiently complete. File system kernel component  provides and\/or supports additional (beyond typical operating system file system capabilities) file system features including for example: quick-recovery, journaling, backup without data lock out, and online file system resizing. Process membership component  monitors and controls the membership of processes in the node and ensures that information on current member processes remains the same on all cluster nodes. Volume manager  enables physical resources configured in the cluster to be managed as logical devices or volumes.","Those having ordinary skill in the art will readily recognize that a variety of different additional kernel components can be (and typically are) utilized by a cluster node. Many of the components described above as well as some of the user level components described below are part of one or more of the VERITAS Volume Manager\u2122, VERITAS File System\u2122, and VERITAS Cluster Server\u2122 products provided by VERITAS Software Corporation.","Fence driver , fence daemon , and fence mechanisms  and , can operate in conjunction with fence configuration software , fence administration software , and fence data . For example, fence configuration software  can be used by a system operator to specify and initialize information stored in a coordinator resource, e.g., which nodes are part of the cluster, as well as to configure fence driver  and fence daemon . For those devices to which an attached volume maps, volume manager  can issue to SCSI pass through  commands for reading and displaying keys, registering with devices, making a reservation with a device, removing registrations made by other devices, reading reservations, and removing registrations. Together with fence driver , fence daemon , and fence mechanisms  and , components , , and  provide core functionality for the I\/O fencing services used to prevent data corruption. Note that one or more of components , , , , , and  can be included within other components, and\/or several components can be combined.","The user level also typically includes software components such as the previously mentioned cluster server software  and application software , e.g., database administration systems (DBMS), file servers, application servers, web servers, backup and restore software, customer relationship management software, and the like.","Fencing components use one or more coordinator resources as part of the I\/O fencing operation. The use of coordinator resources enables the fencing components to resolve split-brain conditions occurring in cluster computer systems. In general, when a split-brain condition occurs, a designated node in each subcluster, e.g., the lowest numbered node, \u201craces\u201d to gain control of the coordinator resource(s). The winning node remains in the cluster, and fence components are used to fence losing nodes off from the shared data storage and\/or remove the nodes from the cluster. This can be accomplished by causing nodes to lose their membership in the cluster, e.g., as reflected in membership records stored in a coordinator resource. The nodes remove themselves from the cluster (\u201ccommit suicide\u201d) upon recognizing that they have lost membership. In still other examples, nodes are forcibly removed from the cluster by, for example, cycling their power. Ejected systems cannot write to the data disks, and therefore the potential for corrupt data is greatly reduced or eliminated.","In the generalized I\/O fence framework illustrated in , much of the functionality for implementing various I\/O fence techniques is contained in each of the fence mechanisms  and . In general, each fence mechanism is a separate module, e.g., a separate executable, script, DLL, etc., that is used in conjunction with the other fence components illustrated to implement a particular I\/O fencing scheme. Numerous different I\/O fence mechanisms can be implemented, some of which are discussed in greater detail below with respect to .","Fence mechanisms can be differentiated by the type of coordinator resource or resources used to determine which node or nodes remain in the cluster. Examples of the different coordinator resources used by fence mechanisms include: SCSI-3 compliant devices where SCSI-3 persistent reservation and registration commands are used to register with and gain control of the devices in order to determine the node or nodes that will remain in the cluster; SCSI-2 compliant devices where SCSI-2 reservation and registration commands are used to register with and gain control of the devices in order to determine the node or nodes that will remain in the cluster; remote access power distribution units that provide power to cluster nodes and can be used to cycle the power or turn nodes off; node hardware and\/or software that supports a standard for receiving remote management commands such as the Intelligent Platform Management Interface (IPMI) standard; virtual devices such as specifically defined volumes for use in a manner similar to SCSI devices; management processors or computer systems that are coupled to the cluster but are not nodes of the cluster and provide some cluster management functionality (e.g., the blade management processor of a blade server system); storage area network devices (e.g., switches, routers, etc) that can provide SAN zoning functionality to \u201czone out\u201d node access to certain storage elements; arbitrator processes operating on a computer system, typically remote from the cluster nodes, control or ownership of which can confer success in a race for the coordinator resource; and contact-response systems where a person or program is contacted (e.g., via e-mail or telephone) and a designated response indicates control of the resource. Numerous other examples of coordinator resources and corresponding fence mechanisms will be known to those having ordinary skill in the art.","Thus, the coordinator resource or resources can include a variety of physical devices, logical devices, processes, and combinations thereof. One of the advantages of the framework illustrated in  is that it can, in general, accommodate any type of fence mechanism. Functionality specific to the particular I\/O fencing technique is concentrated in a corresponding I\/O fence mechanism (, ). Fence daemon  manages the various fence mechanisms, invokes them as necessary, provides supporting functionality, and interacts with fence driver  to, for example, get information about cluster membership and cluster partition conditions. For example, when node membership and messaging  receives information indicating that the node has lost contact with another node, it will inform fence driver  which in turn communicates with fence daemon . Fence daemon  operates in association with an appropriate fence mechanism to perform the corresponding fencing operation.","While various fence mechanisms will tend to differ based on differences among the coordinator resources used, fence driver , fence daemon  and any fence mechanism will typically operate in conjunction with each other to provide one or more of the following functionalities: the ability to generate and compare mechanism comparison information to ensure that the there are no critical differences in the instance\/version of the fence mechanism used by the nodes of a cluster; the ability to join a particular node to a cluster; the ability to race for the coordinator resource(s) so that only one node can win the race; the ability to fence off data resources that must be protected from possible data corruption; the ability to unjoin a node from a cluster under certain circumstances (typically when the node gracefully and successfully leaves the cluster); and the ability to exit a node from the cluster (either in a un-graceful, error-driven situation or in an unjoin situation where there is a related error condition).","In some cases, the manner in which one or more of these functionalities is implemented will be very specific to the fencing technique used by the fence mechanism. For example, the race operation for SCSI-2 or SCSI-3 devices includes the issuing of various SCSI commands to try to gain control of the coordinator resource(s), while the race for control of power distribution units supplying power to nodes of the cluster might simply entail issuing \u201cpower off\u201d commands. In other examples, different fence mechanisms may share the same or similar implementation of a particular functionality. Additionally, the type of fence mechanism and\/or simple implementation details my dictate how the different system components perform different parts of the needed tasks. In one embodiment, communication between fence daemon  and fence driver  is performed according to an application programming interface (API). Such communication typically includes instructions to perform certain tasks, e.g., begin race for coordinator resource, and messages about operation status, e.g., race success\/failure. Similarly, communication between fence daemon  and fence mechanisms can be via API calls or using other techniques well known to those having skill in the art.",{"@attributes":{"id":"p-0038","num":"0037"},"figref":["FIG. 3","FIG. 2","FIG. 2"],"b":["300","245","305","255","260","265"]},"Next, a fence driver is initialized on the node in question as shown in step . Driver initialization typically includes such operations as configuring various other components, e.g., a fence daemon, confirming that the selected fence mechanism is compatible with the node and the cluster configuration, establishing contact with the coordinator resource(s), and the like. Still other initialization tasks can be performed at this time. In , it is determined whether a kernel copy of the mechanism comparison information is present on the node. In general, the kernel copy of the mechanism comparison information is associated with a particular instance of the fence driver on a given node. If, for example, an instance of the fence daemon and\/or an associated fence mechanism is running, subsequently terminates, and a new instance is created (all while the same instance of the fence driver is running) the potential exists that there is a mismatch between previous mechanism comparison information and new mechanism comparison information. If there is a kernel copy, operation transitions to  where the kernel copy of the mechanism comparison information is compared with the copy generated in step . A difference in the copies is indicative of an error condition . Once operation proceeds to , a variety of other tasks can be performed. For example, a message can be sent to an administrator or another program\/process, the driver can be unconfigured\/unloaded, and execution of related programs (e.g., a fence daemon) can be suspended. In still other examples, steps can be taken to make sure that the corresponding node has not and will not join the cluster. If the kernel mechanism comparison information is the same as that generated in , operation transitions to . If there is no kernel mechanism comparison information as determined in , a copy is stored\/retained as shown in step .","Next, the kernel mechanism comparison information is compared with remote mechanism comparison information (). Such remote mechanism comparison information can include copies of mechanism comparison information generated by other nodes in the cluster, and sent to the node in question, remote mechanism comparison information retained in a remote location (i.e., not on the node), information from the coordinator resource, and the like. If the remote mechanism comparison information does not compare favorably with the kernel mechanism comparison information, operation transitions to error condition  as described above. If the information does compare favorably, the node then proceeds to join the cluster (). The precise manner in which a node joins a cluster is beyond the scope of the present application. However, the cluster join operation may vary depending on the particular fence mechanism in use. To that end, some or all of the operation can be performed by fence daemon  and\/or a particular fence mechanism ( and ). In still other examples, some or all of the required steps to join a cluster may be sufficiently common across multiple fence mechanisms that they can be without the use of components like fence mechanisms  and . In step , it is determined whether the join operation was a success. In one example, the join operation is performed by some combination of fence daemon  and a particular fence mechanism. After completing the join attempt, the fence daemon passes a corresponding message back to the fence driver. If the join operation succeeded, the node operates normally as cluster node (). If the join operation failed, operation transitions to error condition .",{"@attributes":{"id":"p-0041","num":"0040"},"figref":["FIG. 4","FIG. 3"],"b":["400","400","350","405","410","410"]},"In , the racer node races for the coordinator resource designated by the fence mechanism in use. Next,  determines whether the node has successfully gained control of the coordinator resource. If not, operation transitions to  where a timeout determination is made. For example, a given node's authorization to act as the racer may be limited by a requirement that it must succeed within a given time period. Such a feature is particularly useful where there are multiple nodes in a subcluster and it may be desirable to allow another node to operate as the racer after a certain time period. In another example, the timeout period is used to facilitate implementation of multiple fence mechanisms in the same cluster system. Thus, if a timeout period has occurred, operation can proceed to  where it is determined whether another fence mechanism is available. For example, a cluster can be configured to use a primary fence mechanism, e.g., SCSI-3 devices, and a secondary fence mechanism, e.g., power distribution units, in the event that the first mechanism fails to work properly. Alternatively, multiple fence mechanisms can be used in parallel (not illustrated). If there is another fence mechanism to use, operation transitions to  where that mechanism is selected. Operation then returns to  to proceed using the alternate mechanism. If there are no further mechanisms to be used, an error condition occurs (). In many implementations, only one fence mechanism will be available, and there will be no need to perform operations like those illustrated at  and . Moreover, for a given race mechanism, there will typically be only one racer node in a given subcluster and if that racer node fails to win the race for any reason, all nodes in that subcluster will be ejected. In still other examples, expiration of a timeout time period can cause an error condition to occur, and subsequent steps like those discussed above with respect to step  of  can be performed.","If the node succeeds in gaining control of the coordinator resource, operation transitions to  where the winning node informs other nodes of its success. Those nodes may take subsequent action based on such a \u201crace success\u201d message. Next, the winning node takes any additional steps necessary to fence of data and protect it from corruption. With some fence mechanisms, winning the race for the coordinator device may accomplish this data fencing task. For example, where the coordinator resource is some device or facility used to change a nodes power state, winning the race may necessarily cause other nodes to be reset or shut down, which in turn accomplishes the desired data protection of . In other examples, there may be no fencing operation needed, e.g., a cluster operating as a shared-nothing cluster, and there are no shared data resources to fence off. In still other examples, both racing and fencing operations will be necessarily separate. Further examples of these operations will be discussed below in the context of several different fencing techniques. In step , a determination is made whether the data fencing has been successful. This might include confirming that any nodes from \u201closing\u201d subclusters are unregistered from a coordinator device, ensuring that only nodes in the winning subcluster have access to the data, and the like. Although not illustrated, this process can also include steps similar to those illustrated at , , and . Thus, there may be a timeout associated with the data fencing step (e.g., when this process is explicitly separate from the race process) and there can also be multiple data fencing mechanisms to try. If the fence has failed, an error condition has occurred () and subsequent steps can be performed as described above. If the fence has succeeded that the remaining nodes can resume normal operation and the process ends ().","The flow charts of  illustrate some of the many operational examples of the I\/O fencing tools and techniques disclosed in the present application. Those having ordinary skill in the art will readily recognize that certain steps or operations illustrated in  can be eliminated or taken in an alternate order. Moreover, the methods described in  are typically implemented as one or more software programs for a computer system and are encoded in a computer readable medium as instructions executable on one or more processors. The computer readable medium can be any one of an electronic storage medium, a magnetic storage medium, an optical storage medium, and a communications medium conveying signals encoding the instructions. Separate instances of these programs can be executed on separate computer systems in keeping with the multi-process methods described above. Thus, although certain steps have been described as being performed by certain devices, software programs, processes, or entities, this need not be the case and a variety of alternative implementations will be understood by those having ordinary skill in the art. Additionally, the software components described can be implemented in a variety of different ways. For example, those components described as operating in a kernel or a user memory space are not necessarily limited to those implementations.","Those having ordinary skill in the art will readily recognize that the techniques described above can be utilized in a variety of coordinator resources, different storage devices, and cluster systems with variations in, for example, the number of nodes, the type of cluster operation (failover, parallel, etc.), the number and type of shared data resources, the number of paths between nodes and shared data resources, and the number and type of coordinator resources. Similarly, the techniques described above can be used in a variety of hardware architectures that might not represent conventional cluster configurations, such as a storage area network appliances, switches, routers, etc.",{"@attributes":{"id":"p-0046","num":"0045"},"figref":["FIG. 5A","FIG. 5A","FIGS. 3 and 4"],"b":["500","502","504","506","508","502","504","510","514","512","512","512","512","512"]},"The following registration steps can be performed before, during, or after a cluster join operation such as . A node (e.g., node A ) registers with the designated coordinator disks () using the PERSISTENT OUT-REGISTER command. Alternately, registration can occur manually when a system administrator configures the system. For example, a system administrator starts an application on node A using appropriate cluster server software. Alternately, this step can be performed as part of an automatic start-up process. Node A then registers with the data disks  in storage array  by issuing a PERSISTENT OUT\u2014REGISTER command on the path between the node and storage array  using a registration key, e.g., \u201cK.\u201d Node A  checks if other nodes are registered with any of the data disks  using the PERSISTENT IN-READ KEYS command.","Next, node A  prevents data disks  from accepting I\/Os from other nodes by issuing a PERSISTENT OUT-RESERVE with a \u201cwrite exclusive-registrants only\u201d (WXRO) reservation. This means that data disks  will only accept write requests from a registered node. A node which is not registered and attempting to write to the disk will receive a SCSI RESERVATION CONFLICT error. Any cluster applications can now proceed normally. For example, a database contained one or more of data disks  is started on node A. Node A reads and writes to the data disks normally, and cluster operation proceed ().","As described above with respect to step , some event may cause a cluster partition. For example, private network  has failed and the two nodes lose contact with each other. On one side of the partition is node A, and on the other side node B. One or more of the fence components (driver, daemon, and fence mechanism) on node A and node B race for control () of coordinator disks  since these nodes are the lowest node IDs (in this case the only IDs) in their respective subclusters. This is performed by node A attempting to unregister node B from the coordinator disks using the PERSISTENT OUT-PREEMPT AND ABORT command while node B is attempting to do the same to node A. The respective fence components determine () if their node was successful in gaining control of a majority of the coordinator disks .","One subcluster will win and the other subcluster will receive an error that it is not registered with the coordinator disk(s). In the case where node A wins the race, it continues operation with the I\/O fence in place. Had node A failed to gain control of a majority of coordinator disks , it would eject itself from the cluster. Similarly, if node B determines that it gained control of the majority of coordinator disks , it ensures that node A is unregistered from data disks . To perform that task, node B sends PERSISTENT OUT-REGISTER commands to each data disk  using node B's key, e.g., \u201cK\u201d. In general, the commands to each data disk are sent in parallel. This task is performed because node B has not previously registered with data disks . Node B can then issue PERSISTENT IN-READ KEYS commands to determine the keys registered on data disks . If any of the data disks have a key not belonging to node B, e.g., not K, node B can then issue PERSISTENT OUT-PREEMPT and ABORT commands to the appropriate data disks with a victim key value corresponding to that read in the previous command. In this example, node B finds that node A (key K) is still registered, and accordingly unregisters node A. Thus, node B takes over operation with an I\/O fence in place. At this point additional action can be taken by node B. For example, node B can prevent data disks  from accepting I\/Os from other nodes by issuing a PERSISTENT OUT-RESERVE with a \u201cwrite exclusive-registrants only\u201d (WXRO) reservation.","This example illustrates some of the specific operations performed by fence components in order to implement a SCSI-3 based fencing technique. As will be understood by those having skill in the art, the specific operations needed to implement a particular fencing technique will largely depend on the type of coordinator resource(s) used.  illustrate a number of other fencing techniques that can be implemented using the above-described fence components.",{"@attributes":{"id":"p-0052","num":"0051"},"figref":"FIG. 5B","b":["515","517","519","525","517","519","521","523","517","519","529","527","527","517","519","527","527","527","528"]},"For example, nodes responding to a partition event will seek to gain control or ownership of the coordinator process as part of a fence operation. Depending on the implantation, one or more coordinator processes can be used. Moreover, cluster operation can include various coordinator process registration steps much like those described above in the context of the SCSI-3 fence mechanism. In one embodiment, the coordinator process is maintained by a cluster management system that enables the management of multiple clusters and their applications from a single console. An example of such cluster management software is the VERITAS Global Cluster Manager\u2122 product provided by VERITAS Software Corporation.",{"@attributes":{"id":"p-0054","num":"0053"},"figref":"FIG. 5C","b":["530","534","536","534","536","532","532","538","544","534","536","540","542","538"]},"In , power distribution units  and  serve as the coordinator resources used by another fence mechanism. Two node cluster  includes nodes  and  which are coupled to shared storage  typically implemented as one or more storage devices such as JBODs, disk arrays, and the like. Power distribution units  and  are coupled to respective nodes to provide remote power control of the nodes. Power distribution units  and  are typically coupled to some other communications bus , e.g., a LAN\/WAN , dedicated serial connection, or the like, so as to receive remote control commands. In a typical example, power distribution units  and  provide one or more of: individual electrical outlet control, power on and off delays, current monitoring, alarm thresholds, and network management. Individual outlet control allows users to turn outlets on, turn outlets off, or cycle power to equipment, which can reboot nodes. Power delays allow users to determine the order in which equipment is powered up or down. Users can typically access, configure, and control power distribution units through web, SNMP, or telnet interfaces. In some examples, power distribution units are built into the servers operating as nodes, or are part of other related equipment such as an uninterruptible power supply (UPS).","Nodes  and  use an established communication link between them (private network ) to exchange heartbeat information. Private network failure () gives rise to a split-brain condition or cluster partition event. When this occurs, the fence components operating on each node race to control power distribution units. Thus, node A would attempt to control power distribution unit  associated with node B so as to turn off or reset that server. This attempt typically takes the form of sending one or more commands to power distribution unit  via bus\/network . Similarly, node B races to control power distribution unit . This illustrates an example of a well known technique often referred to as STONITH (shoot the other node in the head). With STONITH fencing, forcing errant cluster nodes to reset, allows the nodes to attempt to rejoin the cluster and acquire resources in a normal fashion or recognize that they cannot rejoin. In some cases, ejected nodes are simply shutdown rather than rebooted. With this type of fencing technique, the fence mechanism is specifically designed to send necessary commands to power distribution units.","A related STONITH fencing technique is illustrated in . Two node cluster  includes nodes  and  which are coupled to shared storage  typically implemented as one or more storage devices such as JBODs, disk arrays, and the like. Nodes  and  use an established communication link between them (private network ) to exchange heartbeat information. Private network failure () gives rise to a split-brain condition or cluster partition event. Nodes  and  also use some other communications link, e.g., a LAN\/WAN . Nodes  and  support some type of node management standard that allows management commands to be received remotely, e.g., via network . In this example, nodes  and  support the Intelligent Platform Management Interface (IPMI) standard as described in, for example, 2.0, \u00a92004 Intel Corporation, Hewlett-Packard Company, NEC Corporation, and Dell Computer Corporation (http:\/\/www.intel.com\/design\/servers\/ipmi\/).","IPMI standardizes the implementation of management functions in servers and telecom equipment. IPMI compatible devices include basic IPMI building blocks, such as management microcontrollers, sensors, chips sets and firmware. The IPMI specification defines standardized, abstracted interfaces to a platform management subsystem. IPMI includes the definition of interfaces for extending platform management between boards within a main chassis, and between multiple chassis. Platform management encompasses both monitoring and control functions that are built in to the platform hardware and primarily used for the purpose of monitoring the health of the system hardware. This typically includes monitoring elements such as system temperatures, voltages, fans, power supplies, bus errors, system physical security, etc. It also includes automatic and manually driven recovery capabilities such as local or remote system resets and power on\/off operations. These capabilities can be used to implement a STONITH-type fencing mechanism.","IPMI uses message-based interfaces for the different interfaces to the platform management subsystem such as IPMB, serial\/modem, LAN, ICMB, PCI Management Bus, and the system software-side \u201cSystem Interface\u201d to a baseboard management controller (BMC). To illustrate this functionality, nodes  and  include BMCs  and , respectively. IPMI messages share the same fields in the message \u2018payload\u2019\u2014regardless of the interface (transport) over which they are transferred. IPMI messaging uses a request\/response protocol where request messages are commonly referred to as commands. The use of a request\/response protocol facilitates the transfer of IPMI messages over different transports. It also facilitates multi-master operation on busses like the IPMB and ICMB, allowing messages to be interleaved and multiple management controllers to directly intercommunicate on the bus. For example, the IPMI LAN interface adds formatting for sending IPMI messages as LAN packets.","When a split-brain condition is detected, the fence mechanisms operating on each of the nodes send appropriate IPMI commands in an effort to control the IPMI-related coordinator resource(s) present on the other nodes. Thus, node A would attempt to control BMC  associated with node B so as to turn off, reset, or otherwise prevent that server from accessing shared storage. This attempt typically takes the form of sending one or more commands to BMC  via bus\/network . Similarly, node B races to control BMC . Forcing errant cluster nodes to reset, allows the nodes to attempt to rejoin the cluster and acquire resources in a normal fashion or recognize that they cannot rejoin. In some cases, ejected nodes are simply shutdown rather than rebooted.",{"@attributes":{"id":"p-0061","num":"0060"},"figref":["FIG. 5F","FIG. 5F"],"b":["575","577","579","585","585","587","575"]},"One feature often provided by devices such as SAN switch  is zoning. In general, zoning allows segmentation of a node by physical port, name, or address. The goal is to restrict accessibility of storage devices to servers, effectively subdividing the SAN into a set of private subnetworks. Within each of these subnetworks, cooperative sharing of storage resources is still implemented. Zoning techniques typically operate by blocking access to ranges of device\/port addresses, e.g., Fibre Channel addresses. Thus, SAN switch  might maintain tables of Fibre Channel port addresses that are permitted to intercommunicate. If a port attempts to establish communication with a port address not in its zone, switch  blocks that communication. The software and\/or devices in control of SAN zoning can thus serve as coordinator resources, control of which allow for I\/O fencing and data protection needed when split-brain conditions occur. Fabric fencing provides data integrity while allowing a more ordered and graceful resolution of problems. It does this by focusing on control of server access to shared resources, as opposed to a server's ability to operate. With a SAN zoning fence mechanism, the cluster protects shared resources by using access control mechanisms within the SAN fabric or within the storage devices to control precisely which servers have access to which resources. The servers themselves can remain up even while they are excluded from accessing shared resources.","In one example, Nodes  and  use an established communication link between them (private network ) to exchange heartbeat information. Private network failure () gives rise to a split-brain condition or cluster partition event. Detection of the split-brain condition causes fence components on each node to attempt to control a coordinator resource in charge of SAN zoning, e.g., SAN switch . In other implementations, the nodes race to control software that in turn manages one or more SAN devices that provide zoning. An example of such software is VERITAS CommandCentral\u2122 Storage, which integrates storage resource management, performance and policy management, storage provisioning and zoning capabilities to ensure that a particular storage infrastructure runs efficiently. The wining node is the node that succeeds in having the other node zoned out of access to shared storage, thereby protecting the storage. Similar techniques such as LUN masking and LUN mapping can also be used as part of the I\/O fencing technique.","As illustrated by , numerous different I\/O fencing techniques can be implemented. The generalized fence mechanism framework described above, provides a convenient platform for implementing one or more of these (or other) techniques in a cluster environment. The generalized fence framework provides flexibility and expandability in ways that prior art I\/O fence mechanisms have not.",{"@attributes":{"id":"p-0065","num":"0064"},"figref":"FIG. 6","b":["600","600","600","610","620","605","610","620","255","220","620","610"]},"Those having ordinary skill in the art will readily recognize that the techniques and methods discussed below can be implemented in software using a variety of computer languages, including, for example, traditional computer languages such as assembly language, Pascal, and C; object oriented languages such as C++, C#, and Java; and scripting languages such as Perl and Tcl\/Tk. Additionally, software  and  can be provided to the computer system via a variety of computer readable media including electronic media (e.g., flash memory), magnetic storage media (e.g., hard disk , a floppy disk, etc.), optical storage media (e.g., CD-ROM ), and communications media conveying signals encoding the instructions (e.g., via a network coupled to network interface ).","Computer system  also includes devices such as keyboard & mouse , SCSI interface , network interface , graphics & display , hard disk , and CD-ROM , all of which are coupled to processor  by communications bus . It will be apparent to those having ordinary skill in the art that computer system  can also include numerous elements not shown in the figure, such as additional storage devices, communications devices, input devices, and output devices, as illustrated by the ellipsis shown. An example of such an additional computer system device is a fibre channel interface.","Although the present invention has been described with respect to a specific preferred embodiment thereof, various changes and modifications may be suggested to one skilled in the art and it is intended that the present invention encompass such changes and modifications that fall within the scope of the appended claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["A more complete understanding of the present invention and advantages thereof may be acquired by referring to the following description and the accompanying drawings, in which like reference numbers indicate like features.",{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIGS. 5A-5F"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 6"}]},"DETDESC":[{},{}]}
