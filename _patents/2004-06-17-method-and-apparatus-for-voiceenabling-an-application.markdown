---
title: Method and apparatus for voice-enabling an application
abstract: A method of voice-enabling an application for command and control and content navigation can include the application dynamically generating a markup language fragment specifying a command and control and content navigation grammar for the application, instantiating an interpreter from a voice library, and providing the markup language fragment to the interpreter. The method also can include the interpreter processing a speech input using the command and control and content navigation grammar specified by the markup language fragment and providing an event to the application indicating an instruction representative of the speech input.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08768711&OS=08768711&RS=08768711
owner: Nuance Communications, Inc.
number: 08768711
owner_city: Burlington
owner_country: US
publication_date: 20040617
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION"],"p":["1. Field of the Invention","The present invention relates to command, control, and content navigation with respect to multimodal applications.","2. Description of the Related Art","Visual browsers are complex application programs that can render graphic markup languages such as Hypertext Markup Language (HTML) or Extensible HTML (XHTML). As such, visual browsers lack the ability to process audible input and\/or output. Still, visual browsers enjoy a significant user base.","Voice browsers are the audio counterparts of visual browsers. More particularly, voice browsers can render voice markup languages such as Voice Extensible Markup Language (VXML), thereby allowing users to interact with the voice browser using speech. Voice browsers, however, are unable to process or render graphic markup languages.","Recent developments in Web-based applications have led to the development of multimodal interfaces. Multimodal interfaces allow users to access multimodal content, or content having both graphical and audible queues. Through a multimodal interface, the user can choose to interact or access content using graphic input such as a keyboard or pointer entry, using an audible queue such as a speech input, or using a combination of both. For example, one variety of multimodal interface is a multimodal browser that can render XHTML and Voice markup language, also referred to as X+V markup language.","To provide both graphic and voice functionality, developers are left with the option of developing a new multimodal browser\/application or, alternatively, redesigning an existing visual browser\/application to provide voice functionality. The complexity of visual browsers, and browsers in general, however, makes such efforts both time consuming and costly.","Further complicating the process of voice-enabling an application program, operations such as rendering content, command and control, and content navigation typically are distinct functions. Voice-enabling content refers to generating or playing an audible rendition of an electronic document such as a markup language document. Command and control pertains to graphical user interface (GUI) features such as commands that are accessible through menus and dialog boxes of an application. Content navigation pertains to the ability of a user to select hyperlinks presented within a rendered electronic document using voice, thereby causing a browser, for example, to load the document represented by the hyperlink. Thus, to speech enable an application program, efforts not only must be directed to voice-enabling the content, but also to voice-enabling command and control and content navigation functions of the application program.","The inventive arrangements disclosed herein provide a solution for speech enabling an application program for performing command and control and content navigation. In one embodiment, a library of voice markup language functions is provided. Through the voice library, an interpreter can be instantiated and passed a markup language fragment. The markup language fragment can specify a grammar that can be used to process received user spoken utterances.","One aspect of the present invention can include a method of voice-enabling an application for command and control and content navigation. The method can include the application dynamically generating a markup language fragment specifying a command and control and content navigation grammar for the application. The application can instantiate an interpreter from a voice library and provide the markup language fragment to the interpreter.","The interpreter can process a speech input using the command and control and content navigation grammar specified by the markup language fragment. An event can be provided from the interpreter to the application that indicates an instruction representative of the speech input.","Another aspect of the present invention can include a machine readable storage being programmed to cause a machine to perform the various steps described herein.",{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 1","b":["100","100","102","105","115","100","140","145","115","105"]},"The computer system  can be a server for hosting one or more applications such as voice browsers, interactive voice response systems, voice servers, or the like. For example, in one embodiment, the application  can be a visual browser, or other application, that is to be voice or speech enabled. Accordingly, the application  can function as a multimodal browser once an interpreter  is instantiated. In another embodiment, the application  can be a voice server. In that case, the interpreter  can function as, or form, a voice browser. It should be appreciated, however, that the application  and the voice library  need not be located within the same information processing system. For example, each can be located within one or more different information processing systems communicatively linked via a suitable communications network. In one embodiment, the application  can be disposed within a user computing machine while the voice library  is disposed in a network computing machine that is remote from the user machine.","The voice library  can include modules configured to perform the various functions described herein. In one embodiment, the voice library  can include a function for instantiating an interpreter , for example upon request of the application . The voice library  can include a library application programming interface (API)  through which the application  and the voice library  can communicate. As such, the library API  provides the application  with access to the functions of the voice library .","The application  can call a function in the voice library  via the library API  to instantiate the interpreter . In one embodiment, the interpreter  can function as a voice markup language interpreter. The interpreter  can be configured to parse and render any of a variety of voice markup languages such as Voice Extensible Markup Language (VXML) or any subset thereof.","For example, in another embodiment, the interpreter  can be configured to render the subset of VXML used by the Extensible Hypertext Markup Language (XHTML) and Voice markup language, commonly referred to as X+V markup language. In this manner, the interpreter  can function in a complementary fashion with the application  to provide multimodal browsing. More particularly, the interpreter  can provide functions such as command and control and content navigation (C3N). As noted, command and control refers to the manipulation of an application through the use of application commands, for example those typically accessible via one or more menu constructs of the application. Content navigation refers to the ability to select hyperlinks presented or otherwise specified by a rendered markup language document, such that the content referenced by the hyperlink can be retrieved and presented through the browser or application.","The application  further can invoke another function referred to as \u201caddLink\u201d. The addLink function of the voice library  can pass a VXML fragment , generated by the application , to the interpreter . The VXML fragment  can specify one or more link elements, containing C3N grammars, with which speech inputs to the application  can be matched. That is, the interpreter  can match speech inputs received from the application  with the C3N grammar(s). Upon detecting a match, the interpreter  can generate one or more events that are sent back to the application .","In cases where multiple devices may be enabled for multimodal interaction, a configuration file  can be included. The configuration file  can specify one or more different devices such as a telephone, mobile phone, home security system, dashboard audio\/communication system, computer system, portable computer system, or the like. Within the configuration file , each device can be assigned an identifier that uniquely identifies that device. In one embodiment, prior to registering the VXML fragment  with the interpreter , the application  can access the configuration file  to obtain the identity of the device being used. Identifying information for the device can be substituted in a command object of the C3N grammar specified by the VXML fragment .","In consequence, the interpreter  can parse the command object and include that identifier in its response, i.e. an event, that is sent back to the application . In this manner, information passing to the application  from the interpreter  can be designated as corresponding to a particular device. Accordingly, the application  can service and interact with more than one device concurrently. Further, this functionality permits a user to personalize his or her particular device with a name unique to that device.","As noted, the system  can include speech resources such as the ASR engine  configured to convert speech to text and the TTS engine  for generating synthetic voice from text. Notably, an audio playback system (not shown) can be included for playing recorded portions of audio if so desired. The interpreter  can manipulate the speech resources through the speech services API . This allows the interpreter  to be implemented independently of the speech resources, thereby facilitating the use of speech resources from different vendors.","While the application  and the interpreter  can function in a cooperative manner, the ASR engine  and the TTS engine  need not be part of the same system. That is, in one embodiment, the processing resources can execute in one, or more other computer systems. Such computer systems can be proximate to, or remotely located from the computer system . For example, the speech resources can be provided as individual services that are accessible to the interpreter  and application  via a communications network , which can include, but is not limited to, a local area network, a wide area network, the public switched telephone network, a wireless or mobile communications network, the Internet, and\/or the like. Still, in another embodiment, the resources can be located within a same computer system as the application  and\/or the interpreter .","In operation, one or more instances of the interpreter  are created through function calls from the application  to the voice library . Once created, the application  can access the speech resources via the interpreter . That is, the interpreter  can render voice markup languages and access the ASR engine  and the TTS engine . Accordingly, voice services can be provided to a user accessing the computer system  via a telephone  or a computer system  over another communications network . C3N grammars can be provided or specified by the application  to the interpreter  through the passing of the VXML fragment . Information indicating matches to user speech, such as application  commands or content navigation commands, i.e. selections of hyperlinks, can be passed back to the application  from the interpreter  as one or more events.","The application program  can be synchronized with the interpreter  through events and state change information, i.e. through the addition of XML event listeners and state listeners. Events and state changes are propagated from the interpreter  to the application  through these event listeners. The application  uses the library API  for adding event and state change listeners to the interpreter . A listener is an object oriented programming technique for implementing a callback function. Using a state change event allows API's to function properly as some API's may fail if the interpreter  is in the wrong state. Accordingly, the application  can wait until the interpreter  is in the correct state, using the state change listener, before calling those API's that are sensitive to the internal state of the interpreter .",{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 2","b":["200","200"]},"The method can begin in step , where the commands and controls of an application, and links specified by an electronic document rendered by the application can be identified. More particularly, the application itself can determine which commands and controls, for example those accessible via a menu construct, are available for user selection. Such a determination can depend upon the state in which the application is operating. For example, an application commonly makes particular menu items or functions available to users depending upon the state in which the application operates at any given time. As the instant invention can speech enable aspects of an application such as command and control and content navigation, those menu commands that are available or are active within the application program can be identified.","Additionally, links specified by an electronic document, such as those selectable within a page generated by rendering a markup language document, also can be identified. For example, Hypertext Markup Language (HTML) links can be identified within a HTML or other markup language document. Thus, any links, such as hyperlinks, that are displayed or are associated with displayable objects, can be identified.","In step , the identified commands, controls, and links can be added to one or more C3N grammars. In step , a VXML or other markup language fragment can be generated by the application. The VXML fragment can specify the C3N grammars to be used to recognize user specified commands pertaining to command and control and content navigation for the application.","Below is an example of a VXML fragment specifying a C3N grammar. As noted, the VXML fragment can be generated by the application and provided to, and registered with, an interpreter for use in processing a user speech input.",{"@attributes":{"id":"p-0034","num":"0033"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"<vxml:link eventexpr=\u201capplication.lastresult$.interpretation.c3n\u201d>"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"<vxml:grammar>"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"<![CDATA["]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"175pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"#JSGF V1.0;"]},{"entry":[{},"grammar c3nfinal;"]},{"entry":[{},"public <c3nfinal> = <c3n> {$.c3n=$c3n;};"]},{"entry":[{},"<c3n> = browser (<comand> {$ = \u201ccomand.\u201d+"]},{"entry":[{},"$command} |"]},{"entry":[{},"<link> {$=\u201clink.\u201d+$link});"]},{"entry":[{},"<command> = <history> {$ = $history} | <file> {$=$file};"]},{"entry":[{},"<history> = [go] back {$ = \u201cback\u201d} | forward;"]},{"entry":[{},"<link> = [\u201cgo to\u201d | \u201cjump to\u201d ] <documentlinks>"]},{"entry":[{},"{$=$documentlinks};"]},{"entry":[{},"<documentlnks> = Glossary {$=\u201cglossary.html\u201d}"]},{"entry":[{},"| Contents {$=\u201ccontents.html\u201d}"]},{"entry":[{},"| Next Page {$=\u201cchapter3.html\u201d}"]},{"entry":[{},"| Previous Page {$=\u201cchapter1.html\u201d};"]},{"entry":[{},"]]>"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"<\/vxml:grammar>"]},{"entry":[{},"<catch event = \u201ccommand link\u201d>"]},{"entry":[{},"<value expr=\u201cwindow.c3nEvent (application.lastresult$.intrepre-"]},{"entry":[{},"tation.c3n)\u201d\/>"]},{"entry":[{},"<\/catch>"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"<\/vxml:link>"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"In step , the application can instruct the voice library to instantiate an interpreter. For example, the interpreter instance can be created using a factory design pattern or a constructor. The application then can send the VXML fragment to the interpreter for use in recognizing received user speech in step . Accordingly, the VXML fragment can be registered with the interpreter.","In step , a speech input can be received by the application from a user. In step , the application can pass the speech input to the interpreter for analysis. The interpreter, using the C3N grammar specified by the VXML fragment as well as the speech resources, can resolve the speech input to an appropriate command, control, or instruction for content navigation, i.e. selecting a hyperlink. More particularly, the speech input can be converted to a textual representation of the speech input. While the textual representation can be a translation of what the user said, it also can include, or be converted to, other characters which when read by the application can cause the user intended action to occur.","The semantic interpretation builds an event string when a user's utterance is matched with an entry in the specified C3N grammar. By including the interpretation in the \u201ceventexpr\u201d attribute of the <link>, that string can then be raised as a user defined event. The <link> also can contain a <catch> element that processes the events generated by the semantic interpretation. In one embodiment, within a catch element, a Document Object Model (DOM) function \u201cwindow.c3nEvent( ) can be executed, thereby passing the event string to the application.","As an example, and with reference to the VXML link already provided, if the user utters \u201cbrowser, go back\u201d the event expression attribute of the <link> containing \u201capplication.lastresult$.interpretation.c3n\u201d resolves to the string \u201ccommand.back\u201d. If the user utters \u201cbrowser, go to next page\u201d, the expression attribute of the <link> resolves to \u201clink.chapter3.html\u201d.","In step , the processing results from the interpreter can be provided back to the application. The event then can be raised in the application. The application can interpret the event in step . The application can interpret the event, for example a string, according to an event hierarchy established by the C3N grammar contained in the VXML fragment. In one embodiment, strings that start with \u201ccommand.\u201d can be interpreted as menu commands, while strings starting with \u201clink.\u201d can be interpreted as content navigation. In another embodiment, a DOM API called by the catch handler can interpret the event string.","While the method  has been descriptive of a single interpreter, it should be appreciated that multiple instances of the interpreter can be created and run. Accordingly, in another embodiment, a pool of one or more interpreter instances can be created by the application program. A threading policy can be established in the application program to facilitate the asynchronous operation of each of the interpreter instances.","Further, it should be appreciated that the process described herein can occur concurrently, or substantially concurrently, with content rendering. That is, the application can render an electronic document while speech inputs pertaining to C3N are received and processed. In illustration, when running a flight information application, i.e. executing one or more electronic documents providing such functionality, which asks a user for destination, date, and seating class, the present invention allows a user speech command of \u201cbrowser go back\u201d to be recognized, thereby causing the browser to display the previous page.","The present invention can be realized in hardware, software, or a combination of hardware and software. The present invention can be realized in a centralized fashion in one computer system or in a distributed fashion where different elements are spread across several interconnected computer systems. Any kind of computer system or other apparatus adapted for carrying out the methods described herein is suited. A typical combination of hardware and software can be a general-purpose computer system with a computer program that, when being loaded and executed, controls the computer system such that it carries out the methods described herein.","The present invention also can be embedded in a computer program product, which comprises all the features enabling the implementation of the methods described herein, and which when loaded in a computer system is able to carry out these methods. Computer program in the present context means any expression, in any language, code or notation, of a set of instructions intended to cause a system having an information processing capability to perform a particular function either directly or after either or both of the following: a) conversion to another language, code or notation; b) reproduction in a different material form.","This invention can be embodied in other forms without departing from the spirit or essential attributes thereof. Accordingly, reference should be made to the following claims, rather than to the foregoing specification, as indicating the scope of the invention."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["There are shown in the drawings, embodiments that are presently preferred; it being understood, however, that the invention is not limited to the precise arrangements and instrumentalities shown.",{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 2"}]},"DETDESC":[{},{}]}
