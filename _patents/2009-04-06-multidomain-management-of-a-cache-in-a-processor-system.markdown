---
title: Multi-domain management of a cache in a processor system
abstract: A system and method are provided for managing cache memory in a computer system. A cache controller portions a cache memory into a plurality of partitions, where each partition includes a plurality of physical cache addresses. Then, the method accepts a memory access message from the processor. The memory access message includes an address in physical memory and a domain identification (ID). A determination is made if the address in physical memory is cacheable. If cacheable, the domain ID is cross-referenced to a cache partition identified by partition bits. An index is derived from the physical memory address, and a partition index is created by combining the partition bits with the index. A processor is granted access (read or write) to an address in cache defined by partition index.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08176282&OS=08176282&RS=08176282
owner: Applied Micro Circuits Corporation
number: 08176282
owner_city: San Diego
owner_country: US
publication_date: 20090406
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["RELATED APPLICATIONS","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION","Functional Description"],"p":["This application is a Continuation-in-Part of a pending application entitled, USING DOMAINS FOR PHYSICAL ADDRESS MANAGEMENT IN A MULTIPROCESSOR SYSTEM, invented by Daniel Bouvier, filed Mar. 11, 2009, Ser. No. 12\/402,345, which is incorporated herein by reference.","1. Field of the Invention","This invention generally relates to computer memory management and, more particularly, to a means for managing cache memory in a computer system.","2. Description of the Related Art","Physical memory, as used herein, is the actual memory device(s) (DRAM, SRAM, FLASH, etc.) where data is stored. In processing, two types of information are generally stored in this memory\u2014data and instructions. Data is the working set of constants and variables that software acts upon. Instructions are the list of commands or operations that are to be carried out on the data. Access to the memory is done through an address. Each location in memory has a unique address. The address of the actual physical devices is referred to as the physical address, or sometimes, real address.","In the early days of microprocessors, the address generated by software (SW) to access a memory location, was always a physical address. The working area that contained both the instructions and the data was called the working set. In the early days one, and only one, program would execute at a time on the computer, so operations were simple. Later, the notion of operating systems and applications was introduced. This meant that more than one SW program was resident on the computer and the processor could switch back and forth between these programs. Since multiple programs had access to all of physical memory, it was possible for a bug or mistake in one program to corrupt the working set of another program. For example, if a first program made a mistake with an address pointer calculation, the wrong address might be written, perhaps overwriting the instructions for a second program. When the second program sequentially stepped through its instruction list and landed on a corrupted instruction, the computer would crash.","To get around this problem, the notion of virtual memory addressing was introduced. Each application is given a virtual address space to work within. The memory management unit and address translation mechanism permit the virtual address space to be translated to the actual physical memory where the storage of data and instructions actually exists. Alternately stated, software executes in what is called virtual address space. Each application, as well as the operating system (OS), \u201clive\u201d in their own virtual address map. However, the processor must ultimately use physical addresses in memory. So, an association has to be made between the virtual address space and the physical address space. The OS does this association and makes assignments to the individual applications using a memory allocation software routine.","When the system first boots, the OS builds an overall physical address map of the system. Memory is mapped in chunks called pages. A page table is built in memory by the OS with entries for each page called page table entries (PTE). Each page table entry includes the virtual page number, the associated physical page number, and any additional attribute bits related to the virtual or physical address. For example, each virtual address also includes the a Process Number associating a particular application with its physical address space.","A programmer writes their program with a specific address map in mind for the data structures to be accessed. Physical address cannot be used in the program because the programmer cannot know in advance if the addresses they might select are available or being used by another program. The memory management unit acts as a translation mechanism between the virtual address space where the program is executing and the actual physical address space where the instructions and data actually reside. As an example, when both application A and application B want to write address 0x0001 4000, a translation might be made such that the actual physical location for A is 0x00F0  and for B is 0x000C 1000. This assignment of virtual to physical address translation is made by the Operating System in what is called a memory allocation routine or MALLOC.","But, if more than one operating system is being used in the system, it becomes possible for a first OS to assign the same physical address space to a first SW application, as a second OS might assign to a second application. In this circumstance, a Hypervisor and Virtualization become necessary. Now, a second level of address management software must run on the microprocessor, which assigns virtual address spaces and associated physical address translations to the individual OSs. The current art for cross-referencing virtual and physical addresses requires adding \u201cextra\u201d bits to the virtual side of the address, essentially expanding the virtual address. This expansion of the virtual address requires running some additional code (e.g., the Hypervisor). The advantage of this approach is that multiple OSs can then coexist on the same processor core. However, this approach does require an additional software layer (Hypervisor) to be active to manage that assignment on the virtual side.","It is not possible to use a Hypervisor if the system is using multiple heterogeneous asymmetric processors. Symmetric multiprocessing (SMP) is a system of computer architecture where two or more identical processors are connected to a single shared main (physical) memory. Further, each processor participating in the SMP system must coordinate together to manage memory. SMP systems permit any processor to work on any task no matter where the data for that task is located in memory. SMP systems can move tasks between processors to balance the workload efficiently. Asymmetric multiprocessing (AMP) refers to a system whereby multiple processors independently run operating systems with no awareness of each other. In this case there is no memory management coordination between the operating systems. Heterogeneous processors in this context are processors that have different programming models especially where memory management is concerned. Given the incompatibilities in memory management mechanisms between processors in a heterogeneous asymmetric multiprocessor, it is generally not pragmatic to use a Hypervisor.","Modern general purpose Harvard architecture processors typically include a multi-level cache hierarchy. The cache memory subsystem aids in delivering of commonly used instructions or data to the execution unit with the lowest latency possible. The average access latency is a key component to the execution performance of a software application.","The access time of a cache is based on the physical constraints of the access time of the SRAM arrays and logic associated with the cache controller. A larger cache has a physically larger array and, thus, the access latency due to lookup overhead and wire delays increases. Therefore, a processor typically has a moderately small first level cache (L1) in order to provide the best trade off in access latency vs. cache hit ratios. Subsequently, a second level cache (L2) is responsible for reducing cache miss penalty by caching a larger portion of the working set. This is done by providing a much larger cache array size, and comes with a penalty of longer access latency.",{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 1"},"Certain applications require deterministic behavior as part of their operating characteristics. For example, real-time or deadline based computing often found in embedded applications requires a certain amount of computation be completed within a predetermined time period. Given a cache shared by multiple concurrent software processes, and further by multiple processors, the access latency for a thread is not guaranteed to be consistent due to the varied interactions of the other threads.","One solution has been to allow software configurable partitioning of the shared cache based on each physical processor that is sharing the cache. Such partitioning is implemented as part of the cache allocation scheme of the cache controller. For a two-CPU system, software running on CPU A is allocated use of space A in the cache, while CPU B is allocated space B. Such partitioning is very coarse and does not allow for inter-processor behaviors, especially where larger numbers of cores exist. Further, it does not address the specific behaviors and needs of different software operating on the same processor core.","The reduction in performance and access determinism is primarily due to two factors\u2014the first is cache line replacement. This is the case where two or more threads are concurrently sharing a common cache. As these threads interact with the cache they compete for the limited resource, thus, randomly replacing cache elements that the other is potentially using, now or in the near future. In this circumstance, a change of code in one thread may adversely impact the performance of another thread.","The second item that impacts cache access latency is blocking. Blocking is the condition whereby two processors are accessing a common cache tag in order to examine if the desired cache element is currently resident in the cache. Since coherency must be maintained, one and only one access to a particular cache address can occur at a time.",{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 2"},"It would be advantageous if a mechanism existed that permitted a cache memory to be efficiently partitioned and\/or shared between multiple processors, multiple OSs, or multiple applications.","Disclosed herein is a means to enable combined subsystems on a multicore processor with configurable partitioning. For example, some cores might be dedicated to a symmetric multiprocessing (SMP) shared memory domain, while others may work in separate partitions as individual single core subsystems. The invention provides a mechanism whereby cache can be partitioned into 2 or more domains (cache partitions). Each domain describes an amount of space that is eligible for cache allocation by that domain. Software threads and the memory working space used by them can thereby be associated with a specific domain and therefore take advantage of that particular portion of the cache. In this way some threads may be allowed full access to the cache while others may be allowed access to only a portion of the cache.","Accordingly, a method is provided for managing cache memory in a computer system. A cache controller portions a cache memory into a plurality of partitions, where each partition includes a plurality of physical cache addresses. Then, the method accepts a memory access message from the processor. The memory access message includes an address in physical memory and a domain identification (ID). A determination is made if the address in physical memory is cacheable. If cacheable, the domain ID is cross-referenced to a cache partition identified by partition bits. An index is derived from the physical memory address, and a partition index is created by combining the partition bits with the index. A processor is granted access (read or write) to an address in cache defined by partition index.","If a first memory access message, having a first address in physical memory and a first domain ID, is accepted in sequence with a second memory access message, having a second address in physical memory and a second domain ID, the first and second domain IDs may be cross-referenced to a shared first cache partition. Then, the processor (or processors) associated with the first and second memory access messages is sequentially granted access to the first and second cache addresses in the first cache partition.","Additional details of the above-described method, and a computer system for managing cache memory, are provided in more detail below.",{"@attributes":{"id":"p-0041","num":"0040"},"figref":"FIG. 3","b":["300","302","304","302","300"]},"A cache memory  is portioned into a plurality of partitions , where each partition includes a plurality of physical cache addresses. Shown are partitions through , where n is a variable not limited to any particular value. The cache memory  may be comprised of a single physical memory device, or a plurality of physical devices. A cache controller  has an interface on line  to accept the memory access message from the processor . The cache controller  determines if the address in physical memory is cacheable, and if cacheable, cross-references the domain ID to a cache partition  identified by partition bits . The cache controller may be enabled using state machine and combinational logic. Alternately, portions of the cache controller may be enabled as a software application comprised of processor instructions stored in memory, which are executed by a processor.","The cache controller  derives an index  from the physical memory address, and creates a partition index  by combining the partition bits with the index. In this manner, the cache controller  grants the processor  access to an address in cache  defined by partition index . In one aspect, a partition list  is used cross-referencing domain IDs to partition bits.","As defined herein, \u201caccess\u201d means either the reading of a cache line from cache (tag hit), or the writing of data into cache, if it is determined that the data is not already resident in the cache (tag miss). Partition bits through are shown, where p is a variable not limited to any particular value. In one aspect of the system, the partition bits are the most significant bits of the partition index. Thus, the cache partitions are identified using the partition bits, and where 2partitions are possible. Also shown are a domain mapping unit (DMU)  and physical memory . The partitioning of cache is analogous to the creation of domains in physical memory, and details of the DMU are provided in the above-referenced parent application. The memory access message is initially passed to the DMU  on line , before being passed to main physical memory  on line , or to the cache controller on line ","In one aspect, the cache controller  accepts a memory access message with an address in physical memory and a domain ID that is not recognized. The domain ID may not be recognized because the domain ID source has not been granted access to cache, or because of a machine error. If the domain ID is not recognized, the cache controller may ignore the memory access message, cross-reference the domain ID to a default cache partition, or originate a processor machine check error message.","In another aspect, the cache controller  sequentially accepts a first memory access message with a first address in physical memory and a first domain ID, and a second memory access message with a second address in physical memory and a second domain ID. The term \u201csequential\u201d merely indicates that the two memory access messages need not be received simultaneously. Neither must the sequential messages be consecutive messages. For example, the first memory access message may originate from the processor , and the second memory access message may originate from a second processor (not shown). Alternately, as described in more detail below, the two memory access messages may originate from the same processor running different software applications or operating systems (OSs), where different applications or OSs are associated with the messages. Regardless of the source, the cache controller  may cross-reference the first and second domain IDs to a shared first cache partition (e.g., partition ). If that is the case, the cache controller grants access to the first and second cache addresses in the first cache partition. However, it should be understood that not all cache partitions are necessarily shared.",{"@attributes":{"id":"p-0047","num":"0046"},"figref":"FIG. 4","b":["400","400","400"],"i":["a ","m"]},"Also shown is OS , which allocates a unique portion of virtual memory to each application. OS  is enabled as computer readable code. For convenience, the OS is shown as a module, but it should be understood that an OS is a type of software application made up of instructions stored in a memory that are operated upon by a processor. These instructions may be referenced using virtual addressing. However, the OS instructions are actually stored in a physical memory. More explicitly, an OS is responsible for the management of software applications, the coordination of activities, and the sharing of computer resources. By using the OS as an interface to the hardware, an application is relieved of management details, making the applications easier to write. Applications access OS services through application programming interfaces (APIs) or system calls. By invoking these interfaces, the application can request a service from the operating system, pass parameters, and receive the results of the operation.","The processor  includes a memory map unit (MMU)  with a page table map  of virtual memory addresses for each application cross-referenced to addresses in the physical memory, and the domain ID associated with each address in physical memory. Each application  is assigned a unique domain ID.","An MMU is a computer hardware component responsible for handling accesses to memory requested by a processor. For example, the MMU  may be embedded in a load store unit of a processor, which connects the processor to the memory system. An MMU typically divides the virtual address space into pages, each having a size which is a power of 2, usually a few kilobytes. The bottom n bits of the address (the offset within a page) are left unchanged. The upper address bits are the (virtual) page number. The MMU normally translates virtual page numbers to physical page numbers via an associative cache called a Translation Lookaside Buffer (TLB), which is also referred to as page table map . The data found in such a data structure is typically called a page table entry (PTEs), and the data structure itself is typically called a page table. The physical page number is typically combined with the page offset to give the complete physical address.","Each TLB entry typically carries descriptors that tell the system hardware how to treat memory accesses. Some example descriptors include address space cacheability, globally visibility, write through vs. write back, process number, and the allocation policy to be followed. Globally visible refers to an address space that is shared by other processors and associated caches, thus requiring a mechanism to maintain coherency. As described in more detail below, the use of cache partitions resolves performance issues associated with the use of global visibility. In addition to the table entry including the virtual page number and associated physical page number, the above-mentioned descriptors may be added as additional attribute bits related to a virtual or physical address.","More explicitly, additional attribute bits are added to the physical side of the PTEs, called domain bits (domain ID). During typical operation, domain mappings are made during system initialization. Each OS is assigned to one or more domains and given those numbers (domain IDs). If multiple OSs coexist sharing the same physical address space, then the domain mechanism is used to make sure that one OS doesn't allocate or use a non-assigned physical address space.","During runtime, the OS or an application calculates and generates address pointers within their allocated virtual memory space. When a load or store instruction occurs, the virtual memory address lookup occurs in the processor's MMU. This lookup results returns the associated physical address and the additional attribute (domain) bits.","The cache controller  sequentially accepts memory access messages associated with each application , cross-references domain IDs to cache partitions, and grants each application access to addresses in a corresponding cache partition. That is, upon receiving a memory access message, the cache controller uses that domain IDs to derive partition bits and create partition indexes, so that each application can access a unique partition in cache memory. However, as noted in the explanation of , it is possible for some, or all applications to share a cache partition.","In one aspect, the OS  is assigned addresses in a physical memory domain at initialization, builds the page table map, and loads the page table map into the MMU . Likewise, the cache controller  builds a partition list  to cross-reference domain IDs to partition bits at initialization.",{"@attributes":{"id":"p-0056","num":"0055"},"figref":"FIG. 5","b":["300","302","302","302","302","404","302","302","404","406","306","308","302","302","308"],"i":["a ","r","a ","b","a ","a ","b ","b. "]},"Alternately, processors through may be homogeneous processors. As above, each processor  includes an MMU . Each processor  receives a request to access virtual memory addresses from a corresponding OS , and accesses its corresponding MMU to recover cross-referenced addresses in physical memory and associated domain IDs. For example, processor receives a request from OS , and accesses MMU . Each process  sends the addresses in physical memory and domain IDs to the cache controller  as memory access messages. The cache memory  includes a unique cache partition associated with each homogeneous processor.","In another aspect, the MMUs  for the first processor and the second processor include a map of virtual memory addresses cross-referenced to addresses in the physical memory identified with a shared first domain ID, not shared with the third processor . If the cache controller  receives a memory access message with an address in physical memory and the first domain ID from the first processor, the cache controller determines if the first domain ID is shared with another processor, and since it is shared, sending a notification (snoop) message to the second processor , but not the third processor , that data in a shared cache partition is being accessed.",{"@attributes":{"id":"p-0059","num":"0058"},"figref":"FIG. 6","b":["300","302","402","402","600","402","302","404","406","302","402","404","302","306"],"i":["a ","s"]},{"@attributes":{"id":"p-0060","num":"0059"},"figref":"FIG. 7","b":["300","402","700","302","302","302","302","404","702","302","302","302","404","404","302","700","702","402","404","302","310","306"],"i":["a ","r","a ","r ","x ","z"]},{"@attributes":{"id":"p-0061","num":"0060"},"figref":"FIG. 8","b":["0","1","1","2"]},"Additional attributes, i.e. a domain ID, indicates the cache domain with which an address space is to be associated. For each access to a particular address space, the domain attribute is used to designate the specific treatment and allocation policy to carry out within the cache. The attribute is presented to the cache controller along with the address to be accessed. Only the portion associated with the configurable domain will be accessed in the cache.",{"@attributes":{"id":"p-0063","num":"0062"},"figref":"FIG. 9","b":"2"},{"@attributes":{"id":"p-0064","num":"0063"},"figref":"FIG. 10","b":["1000","1002","1004","1005","1006","1008","1009","1010","1012"]},"The cache controller may be software programmable, with settings to adjust the allocation policy for each domain, which allows allocation to more or less of the total capacity of the cache. The application of domain attributes and a software configurable allocation policy may span across a multibank cache. Attributes further indicate how a bank or set of banks is partitioned and enables steering to the appropriate bank. The application of domain attributes across multiple independent processes and operating systems permit independent subsystems to be configured in a shared cache.","The above-described cache controller permits the simultaneous use of a large unified cache, while allowing portions of the cache to be set aside and associated with a specific subsystem. The cache controller also enables combined subsystems on a multicore processor with configurable partitioning. For example some cores may be dedicated to an SMP shared memory domain, while others may be working in separate partitions as individual single core subsystems.","Thus, in one aspect, cache partitioning permits the configuration of multiple hardware managed coherence domains. Take the case of a quad processor core device. In order to make the device perform as 2 dual core subsystems, there must be a way for the software to instruct the hardware of physical memory regions associated with a domain. In this way, data accesses for one pair of processors do not interfere with transaction traffic for the other pair. In addition to providing physical address protection, performance advantages are also incurred. First, software may be used to control which portions of the cache are to be allocated to certain applications. Second, the overhead for maintaining hardware is removed. Cache coherency is created by more intelligently steering snoop traffic to only the processors that are part of the particular coherence domain. A coherence domain is defined as an address space where two or more processors share data. In the prior art, all processors are either \u201csnooped\u201d or they're not, based on a single \u201cglobal coherence\u201d attribute in the TLB for a particular memory page. The invention described herein is much more efficient, as only the processors sharing the same cache partition or physical address domain are snooped.","For example, a conventional cache management system with a 4-core device may have a dedicated level 1 (L1) cache for each core. Cores  and  are configured in an SMP cluster so they have an address space that is denoted with a common Domain ID. Cores  and  are unrelated and doing a completely different task. A common coherence controller is responsible for monitoring messages from each processor and making sure that the L1 caches remain consistent. The job of the coherence controller is to make sure that memory stays consistent when multiple processors are modifying memory locations. Consider that processor  has loaded an address into its L1 cache. Then, processor  modifies the data at the address. Subsequently, processor  wishes to do the same. It is the responsibility of the coherence controller to monitor the memory request messages for each core and to query the other core to make sure its cache stays current. So, upon receiving the request from processor , the coherence controller first queries processor  (snoop), checks whether it has a copy, and instructs it to invalidate that copy or write it back to memory. Once that is done, processor  is free to continue with its operation. However, the coherence controller has no idea which processors are participating in sharing of the memory location. So all processors must be snooped. This snooping has performance implications in higher core count systems, as snooping degrades performance.","However, the present invention cache controller takes domain ID into account. When it receives a message with a domain ID, it cross-references the domain ID to see which other processors are part of a shared domain. It then only queries (snoops) those processors that are in the shared domain, leaving the other processors alone and able to go on about their business.",{"@attributes":{"id":"p-0070","num":"0069"},"figref":"FIG. 11","b":["18","6","19","35"]},{"@attributes":{"id":"p-0071","num":"0070"},"figref":"FIG. 12","b":["18","6","19","35"]},{"@attributes":{"id":"p-0072","num":"0071"},"figref":"FIG. 13","b":["35","1","2","18","6","19","34"]},{"@attributes":{"id":"p-0073","num":"0072"},"figref":["FIG. 14","FIG. 12"]},{"@attributes":{"id":"p-0074","num":"0073"},"figref":"FIG. 15","b":"1500"},"In Step  a cache controller portions a cache memory into a plurality of partitions, where each partition includes a plurality of physical cache addresses. Step  accepts a memory access message from the processor. The memory access message includes an address in physical memory and a domain identification (ID). Step  determines if the address in physical memory is cacheable. If cacheable, Step  cross-references the domain ID to a cache partition identified by partition bits. Step  derives an index from the physical memory address. Step  creates a partition index by combining the partition bits with the index. Step  grants a processor access to an address in cache defined by partition index.","In one aspect, determining if the address in physical memory is cacheable (Step ) includes failing to recognize a domain ID associated with an accepted memory access message. Then, Step  performs one of the following operations: ignoring the memory access message, cross-referencing the domain ID to a default cache partition, or originating a processor machine check error message.","In another aspect, accepting the memory access message in Step  includes sequentially accepting a first memory access message with a first address in physical memory and a first domain ID, as well as a second memory access message with a second address in physical memory and a second domain ID. Cross-referencing the domain ID to the cache partition identified by partition bits in Step  includes cross-referencing the first and second domain IDs to a shared first cache partition. Then, granting the processor access to the address in cache defined by partition index in Step  includes granting access to the first and second cache addresses in the first cache partition.","In one aspect, Step generates an MMU with a page table map of virtual memory addresses for a plurality of applications, which are enabled as computer readable code, cross-referenced to addresses in the physical memory. Step associates a domain ID with each address in physical memory, where each application is assigned a unique domain ID. Then, accepting the memory access message in Step  includes sequentially accepting memory access messages associated with each application, and cross-referencing the domain ID to the cache partition in Step  includes cross-referencing domain IDs associated with each to application to a corresponding cache partition.","In another aspect, Step generates a plurality of MMUs for a corresponding plurality of processors. At least two of the processors are heterogeneous processors having different MMU algorithms for associating virtual addresses to addresses in physical memory. Each MMU includes a table map of virtual memory addresses cross-referenced to addresses in physical memory. Step associates a domain ID with each heterogeneous processor. Then, cross-referencing the domain ID to the cache partition in Step  includes cross-referencing the domain IDs of each heterogeneous processor to a unique cache partition.","In a different aspect, Step generates a MMU with a page table map of virtual memory addresses for a plurality of OSs enabled as computer readable code cross-referenced to addresses in the physical memory. Then Step associates a domain ID with each address in physical memory, where each OS is assigned a unique domain ID. As shown in , this operation may involve the use of a hypervisor.","In Step a plurality of MMUs are generated for a corresponding plurality of processors, including at least two homogeneous processors. Each processor and MMU are associated with a corresponding OS, and each MMU includes a table map of virtual memory addresses cross-referenced to addresses in the physical memory. Step associates a unique domain ID with each OS, and Step  cross-references the domain IDs of each OS to a unique cache partition.","In one aspect, Step  assigns an OS, enabled as computer readable code, addresses in a physical memory domain at initialization. Then, portioning the cache memory into a plurality of partitions in Step  includes building a partition list cross-referencing domain IDs to partition bits at initialization.","In another aspect, Step generates a plurality of MMUs for a corresponding plurality of processors, including a first subsystem with a plurality of symmetric processors, and a second subsystem with a plurality of asymmetric processors. Each MMU includes a table map of virtual memory addresses cross-referenced to addresses in the physical memory. Step associates a unique domain ID to each processor. Accepting the memory access message in Step  includes sequentially accepting memory access messages from processors in the first and second subsystems, and Step  cross-references the domain IDs of each processor in the first and second subsystems to a unique cache partition.","In one aspect Step generates a plurality of MMUs for a corresponding plurality of processors, including a first MMU for a first processor, a second MMU for a second processor, and a third MMU for a third processor. Each MMU includes a table map of virtual memory addresses cross-referenced to addresses in the physical memory. Step associates a shared first domain ID to the first and second processors, but not the third processor. Accepting the memory access message in Step  includes accepting a memory access message from the first processor with the first domain ID, and cross-referencing the domain ID to the cache partition in Step  includes substeps. Step determines if the first domain ID is shared with another processor, and since it is shared, Step sends a notification (snoop) message to the second processor, but not the third processor, that data in a shared cache partition is being accessed.","A system and method have been provided for managing cache memory in a computer system. Examples of particular message structures, processor, and hardware units have been presented to illustrate the invention. However, the invention is not limited to merely these examples. Other variations and embodiments of the invention will occur to those skilled in the art."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 15"}]},"DETDESC":[{},{}]}
