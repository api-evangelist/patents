---
title: Acoustic data transmission based on groups of audio receivers
abstract: Techniques are disclosed for acoustic data reception. Playback of modified audio content is acoustically detected by a first receiving entity and a second receiving entity operatively connected to the first receiving entity via a network. The modified audio content is generated by encoding specified data into specified audio content such that the modified audio content satisfies a predefined signal constraint characterizing imperceptibility of any differences between playback of the specified audio content and playback of the modified audio content. The specified data is collaboratively decoded based on the acoustically detected playback of the modified audio content and by the first receiving entity and the second receiving entity using the network and according to a predefined diversity scheme.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09318116&OS=09318116&RS=09318116
owner: Disney Enterprises, Inc.
number: 09318116
owner_city: Burbank
owner_country: US
publication_date: 20131213
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["This application claims benefit of U.S. provisional patent application Ser. No. 61\/737,723, filed on Dec. 14, 2012, which provisional patent application is herein incorporated by reference in its entirety.","At least in some contexts, acoustic data transmission may be used to communicate arbitrary information via sound from an output device, such as a loudspeaker system, to an input device, such as a device equipped with a microphone. Today, many locations are configured with loudspeaker systems, and furthermore, mobile devices equipped with microphones are prevalent. Some acoustic data transmission systems, such as those used for underwater communication, generate specific, audible sounds based on the message desired to be transmitted.","Embodiments presented in this disclosure provide a method, computer-readable medium, and system to perform an operation for acoustic data reception. The operation includes acoustically detecting playback of modified audio content by a first receiving entity. The modified audio content includes perceptible audio content and data encoded as imperceptible audio content. The operation also includes acoustically detecting playback of the modified audio content by a second receiving entity operatively connected to the first receiving entity via a network. The operation also includes decoding the data based on the acoustically detected playback of the modified audio content. The data is collaboratively decoded by the first receiving entity and the second receiving entity using the network and according to a diversity scheme. The operation also includes performing an action by the first receiving entity based on the collaboratively decoded data.","Embodiments presented in this disclosure generally provide techniques for acoustic data transmission. In various contexts, a dedicated network infrastructure is not practical or affordable, but portable devices such as smartphones may nevertheless be popular and prevalent. Collaborative audio transmission allows dissemination of content in such settings to foster new approaches to interactive and enriched story telling or audience engagement. Accordingly, one embodiment provides an audio transmitter application and one or more audio receiver applications. The audio transmitter application is configured to receive audio content, and data (such as text) to embed into the audio content. Depending on the embodiment, the text may include lyrics of the audio content, information about video content associated with the audio content, information about physical or virtual goods pertaining to the audio content or the video content, etc. The audio transmitter application is further configured to generate modified audio content by encoding the text into the audio content.","At least in some embodiments, the text may be encoded such that the text is imperceptible to the human ear during playback of the modified audio content. Additionally or alternatively, the text may be encoded such that any differences between playback of the audio content and playback of the modified audio content are imperceptible to the human ear. At least in some embodiments, imperceptibility to the human ear is measured based on a predefined, objective standard, e.g., the modified audio content satisfying predefined signal characteristics defined based on typical human hearing attributes. In an alternative embodiment, the text is encoded such that any differences between playback of the audio content and playback of the modified audio content are only perceptible to a predefined degree to the human ear. The degree of perceptibility may be tailored to suit the needs of a particular case, such as to manage a tradeoff between amount of text desired to be encoded and imperceptibility of the encoded text.","In one embodiment, the audio transmitter application is further configured to acoustically transmit the text to at least one of the audio receiver applications, such as via a movie theater sound system. The audio receiver application is configured to acoustically detect playback of the modified audio content, such as via a microphone module of a portable device. The audio receiver application is further configured to decode the text from the acoustically detected playback of the modified audio content. The audio receiver application can then output the decoded text, such as via a display screen and\/or internal speakers of the portable device. Further, in some embodiments, to reduce bit error rates, multiple instances of the audio receiver application may collaborate in terms of each instance acoustically detecting playback of the modified audio content and then collaboratively decoding the text therefrom, such as based on a predefined diversity scheme (i.e., as opposed to merely determining a best-performing audio receiver application and relaying its results to the other instances of the audio receiver application). Doing so may improve reliability of the transmission link without having to increase redundancy in data transmission, thereby supporting higher effective data rates. By acoustically transmitting data using the techniques described herein, information of interest may be delivered to users more seamlessly, thus providing the users with a more immersive multimedia experience at least in some cases, relative to alternative approaches that do not involve acoustic data transmission or collaborative decoding techniques.",{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 1","b":["100","100","102","104","102","106","104","108","102","110","112","110","112","104","112"]},"In one embodiment, once the audio transmitter application  receives the audio content  and the text, the audio transmitter application  generates modified audio content  by encoding the text into the audio content , such as based on a predefined set of acoustic text encoding rules. In one embodiment, the set of acoustic text encoding rules receive, as an input parameter, a desired degree of imperceptibility associated with text to be encoded. As described above, the text may be encoded such that the text is imperceptible to the human ear during playback of the modified audio content. Additionally or alternatively, the text may be encoded such that any differences between playback of the audio content and playback of the modified audio content are imperceptible to the human ear.","In one embodiment, the audio transmitter application  then acoustically transmits the modified audio content  to the audio receiver application , by playing the modified audio content  having encoded data , which may be encoded text. The audio receiver application  acoustically detects the playback of the modified audio content. The audio receiver application  then decodes the text from the acoustically detected playback of the modified audio content. The audio receiver application  then outputs decoded data , such as decoded text. Accordingly, the data  is acoustically transmitted from the audio transmitter application  to the audio receiver application , providing the users with a more immersive multimedia experience at least in some cases.",{"@attributes":{"id":"p-0031","num":"0030"},"figref":["FIG. 2","FIG. 1"],"b":["200","110","110","202","204","202","110","110","204","110","204","110","204","204","204"]},"In one embodiment, the encoder module  of the audio transmitter application  generates the modified audio content  based on the audio content  and the data . As shown, the modified audio content  includes modified acoustic data  having the encoded data . The modified audio content  further includes unmodified metadata , which remains unchanged relative to the metadata . In alternative embodiments, however, the modified audio content  includes metadata reflecting one or more changes as a result of the acoustic data  being modified.","In one embodiment, the modified acoustic data  may represent a modified set of sound waves compared to the acoustic data . For example, sound waves may be added to or removed from the set, and properties of sound waves, such as frequency or amplitude, may be modified. As described above, the encoded data  may be encoded such that the encoded data  is imperceptible to the human ear during playback of the modified audio content . Additionally or alternatively, the encoded data  may be encoded such that any differences between playback of the audio content  and playback of the modified audio content  are imperceptible to the human ear. The encoded data  may then be played back and detected by the audio receiver application . The data may then be decoded by the decoder module  of the audio receiver application  and output in order to provide users with a more immersive multimedia experience at least in some cases. At least in some embodiments, the data encoding may be performed in real-time relative to playback of the modified audio content. Additionally or alternatively, the data decoding may occur in real-time relative to detecting the playback of the modified audio content.","As described above, in one embodiment, the audio transmitter application  generates modified audio content by encoding the data into the audio content, such as based on a predefined set of acoustic text encoding rules. The predefined set of acoustic text encoding rules may apply information hiding techniques to embed the data into the audio content. Information hiding techniques allow data to be embedded into different types of media files such as images, videos or audio signals. Accordingly, these media files, also referred to as cover files, serve as carriers for hidden messages. In contrast to cryptography, information hiding is a form of security through obscurity. This means that a secret message is only safely hidden in a cover file, as long as the algorithm used to extract the message is not exposed, while on the other hand, a cryptography scheme is to remain secure even if its mechanisms are exposed.","The field of information hiding may generally be divided into steganography and watermarking. Watermarking is typically applied to encrypt any kind of data to protect it from unauthorized access, while steganography is typically used for imperceptible communication. Watermarking and steganography are employed to send hidden messages such that no one, apart from the sender and the intended recipient, suspects the existence of the message. Watermarking was developed for copyright protection, and a digital watermark has a purpose of identifying the ownership of digital content such as images, videos or audio files. In contrast to steganography, where the focus lies on imperceptibility of the embedded message, robustness against unauthorized removal of digital watermarks is perhaps the most important property of a watermarking scheme.","Information hiding methods applied to sound signals may also be referred to as audio hiding. One example of audio hiding is digital, audio watermarks that can be embedded into soundtracks. These watermarks may be used for copyright protection or content recognition. Audio hiding can also be used for data transmission over acoustic channels, in which case the audio signal with the embedded message may be played back with a loudspeaker at the transmitter and recorded with a microphone at the receiver. One example is encoding information pertaining to a movie into the movie soundtrack. In this example, the hidden message is imperceptible merely from listening to playback of the movie soundtrack, while a computing device with a microphone can decode the embedded information using the techniques disclosed herein. This use of audio hiding may be referred to herein as acoustic data transmission. At least in some embodiments, acoustic data transmission can rely on the same information hiding techniques used in audio watermarking.","In some embodiments, even when using the same or similar audio hiding techniques for the applications of watermarking and acoustic data transmission, these applications may have different requirements. For instance, while it is generally desirable for watermarking techniques to be robust against signal processing distortions and malicious attacks, watermarking techniques do not necessarily need to support high bit rates, e.g., bit rates at or higher than a predefined threshold rate, because he amount of data to be embedded is typically low. In acoustic data transmission, robustness of the signal is desired given digital-to-analog\/analog-to-digital conversion as well as interferences and noise that may occur during transmission over an acoustic channel. In addition, a high bit rate is also desired in order to transmit as much information as possible and minimizing or reducing any audible decrease in sound quality. At least in some embodiments, there is a tradeoff between robustness, bit rate and audio quality in that embedding redundant data improves robustness at the cost of supporting only a lower bit rate. It is thus desired to increase bandwidth and robustness while keeping the embedded messages imperceptible and while maintaining sound quality.","Some examples of audio hiding techniques include low-bit coding, echo hiding, spread spectrum hiding, and phase coding. In low-bit coding, the least significant bit (LSB) of each sample is replaced with the message bit to be embedded. Doing so allows for high data rates at least compared to other audio hiding techniques. For instance, a mono audio signal sampled at 44.1 kHz, i.e., 44100 samples per second, can contain 44100 hidden bits per second. On the other hand, low-bit coding may not be robust against many kinds of signal processing. Thus, low-bit coding is inapplicable in the audio watermarking domain, because low-bit-encoded watermarks would readily be destroyed by filtering, resampling, and other audio processing transformations. For instance, when transmitting an audio file with low-bit-encoded information over an acoustic channel, the hidden message is likely no longer decodable at the receiver, because many of the least significant bits may have been altered in the process. Further, attackers can remove watermarks merely be altering the least significant bit of each sample. Accordingly, low-bit coding may not necessarily be suitable for acoustic data transmission. Despite its limitations, however, low bit coding may be used in some embodiments, depending upon the particular context.","Echo hiding exploits the inability of the human auditory system (HAS) to distinguish artificially introduced echoes in an audio signal, from those echoes that a room itself might naturally introduce owing to the acoustics of the room. Accordingly, the time offset between the original signal and an embedded echo can be used to encode a desired message. For instance, an early echo could encode a bit with the value of 1, whereas an echo with a longer delay could encode a bit with the value of 0. By carefully managing the echo offsets and amplitudes, the embedded information becomes nearly imperceptible. Even when an audio file carrying an echo-encoded message might sound slightly different from the corresponding, original audio, the echoes are still not perceived as distortions by the HAS. Echo hiding is more robust against signal processing operations than low-bit coding and is less perceptible than low-bit coding. However, the relatively low data rate supported by echo hiding, often around 50 bits per second, does not necessarily render it suitable for acoustic data transmission.","Spread spectrum hiding applies a spread spectrum technique to communicate data over an acoustic channel, including auditory masking to hide desired information. Messages are embedded in the frequency domain as noise in the form of pseudorandom sequences that depend on the properties of the carrier signal. Models of the HAS are used to intelligently spread the message in the frequency domain so that it is covered by features of the host signal and the hidden information becomes almost inaudible. Furthermore, spread spectrum coding provides robustness against noise and distortions and may offer a bit rate of about 200 bits per second in case of acoustic data transmission.","In phase coding, the phases of audio signals are altered in specific ways to embed hidden messages. In a respect similar to echo hiding and spread spectrum hiding, phase coding attempts to exploit knowledge about the properties of the HAS to render the hidden information inaudible. The phases in audio signals may be manipulated because the human ear is not sensitive to phase changes and can typically only sense relative but not absolute phases. In some embodiments, a data hiding algorithm may be used to transmit audio signals with hidden messages over a distance of one to three meters and with an error rate of about ten percent and a bit rate of several hundred kilobits per second. In addition, the distortions introduced to the host signal are virtually imperceptible. Accordingly, at least some embodiments herein describe a data transmission system that is based on phase coding techniques.","To embed data into an audio signal using phase coding, the signal is transformed to the frequency domain. In this domain, the amplitude and the phase of the signal can be altered to encode information. It may not necessarily be desirable to transform the audio signal as a whole, e.g., using a Fourier transform. In the case of a song or a movie soundtrack, computing a single, large Fourier transform leads to a frequency representation of the signal with a fine-grained frequency resolution. However, the time resolution would be low, because the resulting spectrum represents the signal over the whole duration of the audio signal. Thus, there is only a single sample of the frequency spectrum on the time axis. Accordingly, a Time-Frequency Representation (TFR) of the signal is needed to change the phases at different frequencies and times. TFRs are used to analyze the sinusoidal frequency and phase content of local sections of a signal as it changes over time. To compute a TFR, the signal has to be partitioned into equally sized blocks of samples and transformed block by block to the frequency domain. For each block, the TFR yields a set of sub-bands (frequency bins) that describes the corresponding frequency spectrum. The size of a block influences the time and frequency resolution of the TFR. Large blocks lead to a high frequency resolution at the expense of a low time resolution. Small blocks cause the opposite: a low frequency but a high time resolution. The optimal block size is a compromise between the two and highly depends on the specific application of a TFR. A common example of a TFR is the spectrogram of an audio signal. Spectrograms visualize the frequency spectrum of sound over time and are used in speech processing, seismology and many other scientific areas.","In one embodiment, a TFR is computed by a Short-Time Fourier Transform (STFT), as follows:",{"@attributes":{"id":"p-0044","num":"0043"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"msub":{"mi":["X","m"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03c9"}},{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mrow":{"mo":"-","mi":"\u221e"}},"mi":"\u221e"},"mo":"\u2062","mrow":{"mrow":[{"mi":"x","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},{"mi":"w","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["n","mR"],"mo":"-"}}}],"mo":["\u2062","\u2062"],"msup":{"mi":"\u2147","mrow":{"mrow":{"mo":"-","mi":"j"},"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mi":["\u03c9","n"]}}}}],"mo":["=","\u2062"],"mi":{}}}},{"mtd":{"mrow":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"msub":{"mi":["DTFT","\u03c9"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"x","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},{"msub":{"mi":"S","msub":{"mi":"HIFT","mrow":{"mi":["mR","n"],"mo":","}}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"w"}}],"mo":"\u00b7"}}}},"mo":","}}}]}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"1"}}}]}}}},"br":{},"ul":{"@attributes":{"id":"ul0001","list-style":"none"},"li":{"@attributes":{"id":"ul0001-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0002","list-style":"none"},"li":["\u03c9=Angular frequency in radians per second","x(n)=n-th sample of the input signal, n\u03b5","w(n)=Length M window function, typically M\u03b5[32, 64, . . . , 4096]","R=Hop-size in samples","j=imaginary unit (\u221a{square root over (\u22121)})"]}}}},{"@attributes":{"id":"p-0045","num":"0049"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":[{"msub":{"mi":["DTFT","\u03c9"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}},{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mrow":{"mo":"-","mi":"\u221e"}},"mi":"\u221e"},"mo":"\u2062","mrow":{"mrow":{"mi":"x","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},"mo":"\u2062","msup":{"mi":"\u2147","mrow":{"mrow":{"mo":"-","mi":"j\u03c9"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"n"}}}}],"mo":"="},"mo":","}}},"ul":{"@attributes":{"id":"ul0003","list-style":"none"},"li":{"@attributes":{"id":"ul0003-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0004","list-style":"none"},"li":["\u2003the Discrete Time Fourier Transform","S(x)=x(n\u2212\u0394), \u0394\u03b5, the shift operator","X(\u03c9)=DTFT of windowed data centered about time mR.\n\nIf the window w(n) has a Constant OverLap-Add (COLA) property at hop-size R, i.e., if\n"]}}}},{"@attributes":{"id":"p-0046","num":"0053"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mrow":{"mo":"-","mi":"\u221e"}},"mi":"\u221e"},"mo":"\u2062","mrow":{"mi":"w","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["n","mR"],"mo":"-"}}}},"mo":"=","mn":"1"},{"mo":"\u2200","mi":"n"},{"mi":"m","mo":"\u2208","mrow":{"mi":"\u2124","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"mo":["(",")"],"mrow":{"mi":"w","mo":"\u2208","mrow":{"mi":["C","O","L"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mrow":{"mi":"A","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"R"}}}}}}}],"mo":[",",",",","]}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"2"}}}]}}}},"br":{}},{"@attributes":{"id":"p-0047","num":"0054"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"m","mo":"=","mrow":{"mo":"-","mi":"\u221e"}},"mi":"\u221e"},"mo":"\u2062","mrow":{"msub":{"mi":["X","m"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03c9"}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"m","mo":"=","mrow":{"mo":"-","mi":"\u221e"}},"mi":"\u221e"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mrow":{"mo":"-","mi":"\u221e"}},"mi":"\u221e"},"mo":"\u2062","mrow":{"mrow":[{"mi":"x","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},{"mi":"w","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"n","mo":"-","mrow":{"mi":["m","R"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}}}],"mo":["\u2062","\u2062"],"msup":{"mi":"\u2147","mrow":{"mrow":{"mo":"-","mi":"j"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"wn"}}}}}],"mo":["\u2062","\u2062","\u2062"],"mover":{"mo":"=","mi":"\u0394"},"mi":{}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mrow":{"mo":"-","mi":"\u221e"}},"mi":"\u221e"},"mo":"\u2062","mrow":{"mrow":{"mi":"x","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},"mo":["\u2062","\u2062"],"msup":{"mi":"\u2147","mrow":{"mrow":{"mo":"-","mi":"j"},"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mi":["w","n"]}},"munder":{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"m","mo":"=","mrow":{"mo":"-","mi":"\u221e"}},"mi":"\u221e"},"mo":"\u2062","mrow":{"mi":"w","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"n","mo":"-","mrow":{"mi":["m","R"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}}}},"munder":{"mi":"\ufe38","mrow":{"mrow":[{"mn":"1","mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":["if","w"]},{"mi":"COLA","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"R"}}],"mo":"\u2208"}}}}}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mrow":{"mo":"-","mi":"\u221e"}},"mi":"\u221e"},"mo":"\u2062","mrow":{"mrow":{"mi":"x","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},"mo":"\u2062","msup":{"mi":"\u2147","mrow":{"mrow":{"mo":"-","mi":"j"},"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mi":["w","n"]}}}}}}},{"mtd":{"mrow":{"mover":{"mo":"=","mi":"\u0394"},"mo":["\u2062","\u2062"],"mi":{},"mrow":{"mrow":[{"msub":{"mi":["DTFT","\u03c9"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}},{"msub":{"mi":["X","\u03c9"]},"mo":"."}],"mo":"="}}}}]}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"3"}}}]}}}},"br":{}},{"@attributes":{"id":"p-0048","num":"0055"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"mi":"x","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},{"msub":{"mi":["IDTFT","\u03c9"]},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"m","mo":"=","mrow":{"mo":"-","mi":"\u221e"}},"mi":"\u221e"},"mo":"\u2062","mrow":{"msub":{"mi":["X","m"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03c9"}}}}}],"mo":["=","\u2062"],"mi":{}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"m","mo":"=","mrow":{"mo":"-","mi":"\u221e"}},"mi":"\u221e"},"mo":"\u2062","mrow":{"msub":{"mi":["IDTFT","\u03c9"]},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"msub":{"mi":["X","m"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03c9"}}}}}}}},{"mtd":{"mrow":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"m","mo":"=","mrow":{"mo":"-","mi":"\u221e"}},"mi":"\u221e"},"mo":"\u2062","mrow":{"msub":{"mi":["x","m"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}}}},"mo":","}}}]},"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mtext":{}},{"mtext":{}}],"mi":"where","mrow":{"mrow":[{"msub":{"mi":["IDTFT","\u03c9"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"X"}},{"mrow":[{"mfrac":{"mn":"1","mrow":{"mn":"2","mo":"\u2062","mi":"\u03c0"}},"mo":"\u2062","mrow":{"msubsup":{"mo":"\u222b","mrow":{"mo":"-","mi":"\u03c0"},"mi":"\u03c0"},"mo":"\u2062","mrow":{"mrow":[{"mi":"X","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03c9"}},{"mo":"\u2146","mi":"\u03c9"}],"mo":["\u2062","\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.2em","height":"0.2ex"}}},"msup":{"mi":"\u2147","mrow":{"mi":["j\u03c9","n"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}}}},{"mrow":{"mi":"x","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},"mo":"."}],"mo":"="}],"mo":"="}}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"4"}}}]}}}}},"Thus, after applying the inverse STFT, the resulting blocks can be overlap-added to reconstruct the original signal in the time domain. At least in some embodiments, the STFT can be used to perform spectral manipulation of an audio signal in the frequency domain. In practical implementations of the STFT, the DTFT is replaced by a Discrete Fourier Transform (DFT) or a Fast Fourier Transform (FFT). In contrast to the DTFT, which operates on sampled signals defined over all integers n\u03b5, the DFT operates on sampled signals of length N. Therefore, unlike the DTFT, the DFT is a function of discrete frequency \u03c9=2\u03c0f=2\u03c0k\/N, k\u03b5[0, N\u22121] and can be defined as:",{"@attributes":{"id":"p-0050","num":"0057"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["X","m"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["\u03c9","k"]}}},{"munderover":{"mo":"\u2211","mrow":[{"mi":"n","mo":"=","mn":"0"},{"mi":"N","mo":"-","mn":"1"}]},"mo":"\u2062","mrow":{"mrow":{"mi":"x","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},"mo":"\u2062","msup":{"mi":"\u2147","mrow":{"mrow":{"mo":"-","mi":"j"},"mo":["\u2062","\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["\u03c9","k"]},"mi":"n"}}}}],"mo":["\u2062","\u2062"],"mover":{"mo":"=","mi":"\u0394"}}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"5"}}}]}}}},"br":{}},{"@attributes":{"id":"p-0051","num":"0058"},"figref":"FIG. 3","b":["300","304","304","306"],"sub":["1 ","2 ","1","2 ","1","2 ","2 ","1 ","1"]},{"@attributes":{"id":"p-0052","num":"0059"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"sin","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"c","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}},{"mfrac":{"mrow":[{"mi":"sin","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["\u03c0","x"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}},{"mi":["\u03c0","x"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}]},"mo":"."}],"mo":"="}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"6"}}}]}}}},"br":[{},{},{}],"b":"302","in-line-formulae":[{},{}],"img":[{"@attributes":{"id":"CUSTOM-CHARACTER-00004","he":"3.13mm","wi":"2.46mm","file":"US09318116-20160419-P00002.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00005","he":"3.13mm","wi":"2.46mm","file":"US09318116-20160419-P00002.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00006","he":"3.13mm","wi":"2.46mm","file":"US09318116-20160419-P00002.TIF","alt":"custom character","img-content":"character","img-format":"tif"}}],"i":["x","t","y","t","x","t","y","t"]},{"@attributes":{"id":"p-0053","num":"0060"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":[{"mrow":[{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":"*"}},{"mo":["[","]"],"mi":"n"}],"mo":"\u2061"},{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mrow":{"mo":"-","mi":"\u221e"}},"mi":"\u221e"},"mo":"\u2062","mrow":{"mrow":[{"mi":"x","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"m"}},{"mi":"y","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["n","m"],"mo":"-"}}}],"mo":"\u2062"}}],"mo":["\u2062","\u2062"],"mover":{"mo":"=","mi":"\u0394"}},{"mi":"n","mo":"\u2208","mrow":{"mi":"\u2124","mo":"."}}],"mo":","}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"8"}}}]}}}}},"Hence, the Fourier transform of the product between a rectangular window and a sine wave is equal to the convolution between the corresponding sin c function and the corresponding Dirac pulse. Referring again to , this convolution shifts the peak of the sin c function to the location of the Dirac pulse. In the case of sine wave s, this has the effect that all the zero-crossings of the sin c function are aligned perfectly with the discrete sampling grid of the DFT. Therefore, the DFT spectrum of the windowed signal of scontains a single peak at frequency f, while all the other frequency bins contain zeros. On the other hand, windowing and transforming syields a different result. The zero-crossings of the sin c function are not aligned with the sampling grid. For this reason, the DFT spectrum has spectral leakage, meaning that it contains frequencies that are not present in the signal of sbut that are artifacts caused by the rectangular window. These artifacts can also be explained in reference to the two signals in the time domain. A signal that is transformed using a DFT is assumed to be periodic. Periodic repetition of the windowed version of sresults in an infinite sine wave with frequency f. However, windowing and periodically repeating sdoes not necessarily lead to a sine wave.",{"@attributes":{"id":"p-0055","num":"0062"},"figref":"FIG. 4","b":["400","400"],"sub":"2 "},"In one embodiment, the Modulated Lapped Transform (MLT) uses a fifty percent overlap (R=M\/2) to avoid blocking artifacts. In contrast to the STFT, the MLT is critically sampled. Transforming a block of length M therefore leads to only M\/2 spectral coefficients. Thus, a single block can never be perfectly reconstructed. However, the MLT applies Time Domain Aliasing Cancellation (TDAC) to enable perfect reconstruction of the entire signal. When overlap-adding the blocks resulting from an inverse MLT, TDAC causes the errors introduced by the transform to cancel out. The basis functions of the MLT are defined as:",{"@attributes":{"id":"p-0057","num":"0064"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"mrow":[{"msub":{"mi":["p","MLT"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["n","k"],"mo":","}}},{"mrow":[{"mi":"w","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},{"mi":"cos","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"mi":"n","mo":"+","mfrac":{"mrow":{"mi":"M","mo":"+","mn":"1"},"mn":"2"}}},{"mo":["(",")"],"mrow":{"mi":"k","mo":"+","mfrac":{"mn":["1","2"]}}}],"mo":["\u2062","\u2062"],"mfrac":{"mi":["\u03c0","M"]}}}}],"mo":["\u2062","\u2062"],"msqrt":{"mfrac":{"mn":"2","mi":"M"}}}],"mo":"="},"mo":[",","\u2062","\u2062"],"mstyle":[{"mtext":{}},{"mspace":{"@attributes":{"width":"4.4em","height":"4.4ex"}}}],"mi":"where"}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"9"}}}]},{"mtd":[{"mrow":{"mstyle":{"mspace":{"@attributes":{"width":"4.4em","height":"4.4ex"}}},"mo":"\u2062","mrow":{"mrow":[{"mrow":[{"mi":"w","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},{"mo":"-","mrow":{"mi":"sin","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":"n","mo":"+","mfrac":{"mn":["1","2"]}}},"mo":"\u2062","mfrac":{"mi":"\u03c0","mrow":{"mn":"2","mo":"\u2062","mi":"M"}}}}}}],"mo":"="},{"mo":["(",")"],"mrow":{"msup":{"mi":"w","mn":"2"},"mo":"\u2208","mrow":{"mi":"COLA","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"M"}}}}],"mo":[",",","]}}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"10"}}}]}]}}},"br":{},"sup":"2"},"In one embodiment, introducing a synthesis window is useful because the synthesis window tapers distortions introduced by modifications of the frequency content off to zero. This property makes the MLT a suitable transform for a phase coding application according to embodiments disclosed herein. However, two problems arise. First, the MLT does not contain phase information, and second, spectral modifications unbalance the time domain aliasing components, which results in un-canceled time domain aliasing in the reconstructed signal, despite TDAC.","In one embodiment, the Modulated Complex Lapped Transform (MCLT) incorporates phase content and does not suffer from time domain aliasing, because it is oversampled. The MCLT can be viewed as an MLT extended with additional sine-modulated functions, which results in a 2\u00d7 oversampling in the frequency domain. In other words, the MCLT computes a complex frequency component for each real-valued input sample. The MCLT is therefore similar to an STFT with fifty percent overlap. However, the basis functions of the MCLT are different and may provide fewer artifacts than the STFT (or other basic DFT filter banks) when transforming audio signals, especially in case of manipulations in the frequency domain. Accordingly, at least some embodiments are described herein in conjunction with using MCLT rather than STFT to compute time-frequency representations. The basis functions of the MCLT are given by:",{"@attributes":{"id":"p-0060","num":"0067"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["p","MCLT"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["n","k"],"mo":","}}},{"mrow":[{"mi":"w","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},{"mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"mo":"-","mrow":{"mi":"j","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"n","mo":"+","mfrac":{"mrow":{"mi":"M","mo":"+","mn":"1"},"mn":"2"}}}}},{"mo":["(",")"],"mrow":{"mi":"k","mo":"+","mfrac":{"mn":["1","2"]}}}],"mo":["\u2062","\u2062"],"mfrac":{"mi":["\u03c0","M"]}}}},"mo":"."}],"mo":["\u2062","\u2062"],"msqrt":{"mfrac":{"mn":"2","mi":"M"}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"11"}}}]}}}},"br":{}},{"@attributes":{"id":"p-0061","num":"0068"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["X","m"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"k"}},{"munderover":{"mo":"\u2211","mrow":[{"mi":"n","mo":"=","mn":"0"},{"mrow":{"mn":"2","mo":"\u2062","mi":"M"},"mo":"-","mn":"1"}]},"mo":"\u2062","mrow":{"mrow":[{"msub":{"mi":["x","m"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},{"mrow":{"msub":{"mi":["p","MCLT"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["n","k"],"mo":","}}},"mo":"."}],"mo":"\u2062"}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"12"}}}]}}}},"br":{},"sub":["m","m"]},{"@attributes":{"id":"p-0062","num":"0069"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":[{"msub":{"mi":["y","m"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},{"munderover":{"mo":"\u2211","mrow":[{"mi":"k","mo":"=","mn":"0"},{"mi":"M","mo":"-","mn":"1"}]},"mo":"\u2062","mrow":{"mrow":[{"msub":{"mi":["X","m"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"k"}},{"msub":{"mi":["p","MCLT"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["n","k"],"mo":","}}}],"mo":"\u2062"}}],"mo":"="},"mo":","}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"13"}}}]}}}},"br":{},"sub":"m"},"As discussed above, the MCLT provides fewer artifacts than the STFT where spectral modifications are performed in the frequency domain. To find out if this holds true for the type of frequency domain manipulations involved in phase coding, the two transforms were compared using the acoustic data transmission system described herein. A pseudorandom bit sequence was embedded into a carrier file. Then, the carrier was played back over a high quality speaker and recorded with a professional studio microphone at a one-meter distance from the speaker. After this transmission over the acoustic channel, the data was extracted again and compared to the originally embedded bit sequence in terms of Bit Error Rate (BER), the percentage of erroneously transmitted bits. This experiment was repeated five times for each type of transform.",{"@attributes":{"id":"p-0064","num":"0071"},"figref":"FIG. 5","b":["500","502","504","506","500","500"]},"One embodiment provides a data hiding technique for acoustic data transmission based on phase coding. Because the data hiding technique is to be configured for at least movie theater and home entertainment applications, specific requirements are involved. Movie theaters tend to have acoustic properties such as good frequency balance, sound projection and favorable reverberation times. The sound systems used in cinemas most often follow high quality standards. Thus, movie theaters offer beneficial conditions for acoustic data transmission. Another factor to consider is imperceptibility. For at least movie theater and home entertainment applications, it is desirable that the hidden information contained in a signal remains inaudible to the human ear. It may be possible to achieve an improvement in sound quality by decreasing the bandwidth. In the cinema communication application, that a content provider has control over the signal, e.g., the movie soundtrack, can be exploited and adapted in favor of imperceptibility of the audio hiding techniques being used.",{"@attributes":{"id":"p-0066","num":"0073"},"figref":"FIG. 6","b":["600","602","604","612","614","606","606","602","608","608","604"]},"In the data extraction stage , a receiver in a synchronization sub-stage synchronizes to an input signal coming from the microphone . Next, the audio is divided into blocks in a Block Partition sub-stage and again transformed to the frequency domain in an MCLT sub-stage. Then, the hidden bits are extracted from the phase content in a phase decoding sub-stage. Redundancy decoding and the previously embedded error detection codes are used to identify successfully transmitted bits in a Packet Decoding sub-stage. These bits then form the extracted data, which should be identical to the input data. At least in some embodiments, the data extraction process executes in real-time. Accordingly, the input of each module depends on the output of its predecessor and is to finish within a given time period, so that the next chunk of input data may be successfully processed.","In one embodiment, phase coding is used to embed the data to be transmitted into the phase content of the audio file that acts as the carrier signal. As discussed above, the MCLT (Equation 12) is used to get a time-frequency representation of an audio signal. Transforming a block of 2M real-valued audio samples results in M complex-valued MCLT coefficients. These coefficients represent the amplitude and phase at M sub-bands, which are equally distributed over the frequency spectrum. After altering the phase content to resemble the data to be hidden, each MCLT block is transformed back to the time domain applying the IMCLT defined in Equation 13 and overlap-adding the resulting blocks. This results in an audio file which sounds nearly the same as the unedited carrier signal. The embedded data is inaudible to the extent that the human ear cannot distinguish the two signals. The processed audio file can then be played back over a speaker and recorded with a microphone at a receiver, e.g., a smartphone. The receiver transforms the incoming audio signal block by block into the MCLT domain and decodes the embedded data after synchronization, which is described in further detail below.","In one embodiment, data is only embedded in a certain range of the spectrum. The range is a parameter that may be selected arbitrarily but has to be known at the receiver. In some embodiments, the acoustic data transmission system may optionally allow the frequency range to change dynamically over time. Frequencies between one and ten Hertz may be used, because such frequencies are likely to be present in music or movie soundtracks and thus provide a better link quality. This is because embedding data is facilitated where there is energy in the selected spectrum; embedding bits into silence may not be feasible. The frequency range determines the number of sub-bands M. The number of sub-bands then defines how many bits per audio block can be embedded. Before encoding, each bit is translated to a spreading code consisting of multiple symbols to decrease the probability of transmission errors. The length K of the spreading codes, described in further detail below, also has an influence on the maximum number of embedded bits per block. In general, the number of bits per block is equal to M\/K and the number of sub-bands M is chosen to be a multiple of the code length K. The number of bits per block multiplied by the number of blocks per second, determines the bit rate of the acoustic data transmission system. The bit rate is approximately 500 bits per second for typical parameters.","In one embodiment, spreading codes are used in spread-spectrum radio communication to allow multiple channels to operate on the same spectrum. They can be defined as finite sequences of code symbols from an alphabet given as the set C:\n\n{()}(0),(1), . . . ,(1)},\u2003\u2003(Equation 14)\n\nwhere K is the code length and c(k)\u03b5C. Note that function notation, e.g., c(k) instead of c, is used for sequences to improve readability of formulas with complicated indexing. In one embodiment, spreading codes provide redundancy that reduces the probability of bit errors in the acoustic data transmission technique. Instead of embedding one data bit per frequency, it may be more reliable to add redundancy by spreading a single bit over different, adjacent frequencies. The longer the spreading codes are, the more redundancy is added and the lower the channel bandwidth becomes. For instance, for a code length of K=4 and the alphabet of code symbols C={\u22121, 1}, the following mapping from bits to binary spreading codes may be used:\n\n1()={1,\u22121,1,\u22121}\n\n0()={\u22121,1,\u22121,1}\u2003\u2003(Equation 15)\n","In one embodiment, before data is embedded, the data is translated bit by bit to a binary spreading code sequence using a mapping such as shown in Equation 15. The resulting sequence is a concatenation of the codes c(k) and c(k) and thus solely consists of the two spreading code symbols 1 and \u22121. Hence, the resulting sequence is another finite sequence of code symbols given by:\n\n{()}(0),(1), . . . ,(1)},\u2003\u2003(Equation 16)\n\nwhere s(n)\u03b5C={\u22121, 1}, N is the total length of the sequence, and N\/K is the number of spreading codes c(k) and c(k) it contains. Further, s(n) can be modulated symbol by symbol into the phase content of the carrier signal.\n","In the theoretical scenario of an error-free channel, the result of the extraction process at the receiver is the exact same sequence of spreading code symbols that was embedded before transmission. However, an analog channel from a speaker to a microphone in practice contains certain amounts of noise and thus may not necessarily be error free. Accordingly, the resulting extracted symbols may be slightly different. The extracted symbols may be defined as:\n\n{()}(0),(1), . . . ,(1)},\u2003\u2003(Equation 17)\n\nwhere r(n)\u03b5[0, 1]. The only difference compared to Equation 16 is that r(n) can contain arbitrary values in the range [\u22121, 1], instead of only 1 and \u22121. These values can be interpreted as follows: the closer a value is to \u22121 or 1, respectively, the more likely the value represents the corresponding spreading code symbol. If a value is 0, the value tends neither to the \u22121 nor to the 1 symbol. To decode the hidden data, a sequence of extracted symbols r(n) is mapped back to 0 and 1 bits. To that end, equally sized sub-sequences of length K are cross-correlated with c(k), the spreading code for the 1-bit. K is the code length, which is the number of symbols in c(k) and c(k). The result is a sequence of correlation coefficients \u03c1(n) which is defined as:\n",{"@attributes":{"id":"p-0073","num":"0080"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"\u03c1","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},{"mfrac":{"mn":"1","mi":"K"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"k","mo":"=","mn":"0"},{"mi":"K","mo":"-","mn":"1"}]},"mo":"\u2062","mrow":{"mrow":[{"mi":"r","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["n","K"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mo":"+","mi":"k"}}},{"msub":{"mi":["c","one"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"k"}}],"mo":"\u2062"}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"18"}}}]}}}},"br":{},"sub":["zero","one","zero"]},{"@attributes":{"id":"p-0074","num":"0081"},"figref":"FIG. 7","b":["700","702","704","706","702"]},"For simplicity of notation in the following equations, it is assumed that all of the sub-bands of an MCLT block are used for data embedding. However, often in practice, only a limited range of sub-bands is used, because embedding data over the whole frequency spectrum introduces audible distortions in the audio file. The following equations can be translated to use only a certain range of sub-bands by adapting the indexing for the variable m. Absolute phase coding is defined as follows:\n\n(),\u2003\u2003(Equation 19)\n\nwhere\n\n","In one embodiment, by computing the absolute value of an MCLT coefficient, its phase is shifted to 0. Negating the absolute value (if s(n)=\u22121) causes a phase shift of \u03c0. Therefore, the resulting coefficients have a phase of either 0 or \u03c0. In both cases, their imaginary parts have vanished. To embed data represented as a sequence s(n) of concatenated spreading codes c(k) and c(k), start with the first sub-band of the first transformed audio block, then continue with the second sub-band, and so forth. Once all sub-bands of an MCLT block have been processed, continue with the first sub-band of the next block, which procedure is illustrated in .",{"@attributes":{"id":"p-0077","num":"0089"},"figref":"FIG. 8","b":["800","800","802","804","806"],"sub":"i,m"},{"@attributes":{"id":"p-0078","num":"0090"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":{"mrow":[{"mi":"r","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["i","M"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mo":"+","mi":"m"}}},{"mrow":[{"mo":"-","mn":"2"},{"mo":["(",")"],"mrow":{"mfrac":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mi":"angle","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"X","mrow":{"mi":["i","m"],"mo":","}}}}},"mi":"\u03c0"},"mo":"-","mn":"0.5"}}],"mo":"\u2062"}],"mo":"="},"mo":[",","\u2062"],"mstyle":{"mtext":{}},"mi":"where"},{"mrow":[{"mi":"angle","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}},{"mi":"atan","mo":["\u2062","\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"2","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"Im","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}},{"mi":"Re","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}],"mo":","}}}],"mo":"="},{"mrow":[{"mi":"atan","mo":["\u2062","\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"2","mrow":{"mo":["(",")"],"mrow":{"mi":["y","x"],"mo":","}}},{"mo":"{","mtable":{"mtr":[{"mtd":[{"mrow":{"mi":"arctan","mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mi":["y","x"]}}}},{"mrow":{"mi":"x","mo":">","mn":"0"}}]},{"mtd":[{"mrow":{"mrow":{"mi":"arctan","mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mi":["y","x"]}}},"mo":"+","mi":"\u03c0"}},{"mrow":{"mrow":[{"mi":"y","mo":"\u2264","mn":"0"},{"mi":"x","mo":"<","mn":"0"}],"mo":","}}]},{"mtd":[{"mrow":{"mrow":{"mi":"arctan","mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mi":["y","x"]}}},"mo":"-","mi":"\u03c0"}},{"mrow":{"mrow":[{"mi":"y","mo":"<","mn":"0"},{"mi":"x","mo":"<","mn":"0"}],"mo":","}}]},{"mtd":[{"mrow":{"mo":"+","mfrac":{"mi":"\u03c0","mn":"2"}}},{"mrow":{"mrow":[{"mi":"y","mo":">","mn":"0"},{"mi":"x","mo":"=","mn":"0"}],"mo":","}}]},{"mtd":[{"mrow":{"mo":"-","mfrac":{"mi":"\u03c0","mn":"2"}}},{"mrow":{"mrow":[{"mi":"y","mo":"<","mn":"0"},{"mi":"x","mo":"=","mn":"0"}],"mo":","}}]},{"mtd":[{"mi":"undefined"},{"mrow":{"mrow":[{"mi":"y","mo":"=","mn":"0"},{"mi":"x","mo":"=","mn":"0."}],"mo":","}}]}]}}],"mo":"="}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mtext":{}},{"mtext":{}}]}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"20"}}}]}}}},"br":{},"b":"802","figref":"FIG. 8"},"In one embodiment, to decode the phases, the phases are mapped into the range of spreading codes [\u22121, 1]. This is achieved by dividing the phases in the range of [0, \u03c0] by \u03c0, subtracting 0.5 and multiplying by \u22122. The minus sign comes from the fact that the spreading code symbol 1 is embedded with a phase of 0, whereas \u03c0 is used for the symbol \u22121. Therefore, the range is to be inverted. Applying the above steps leads to extracted codes in the form of Equation 17, which can be decoded as described in Equation 18. The phases seen at the receiver depend on the distance between the speaker and the microphone. Because different frequencies have different wavelengths, the phases at the receiver may reflect different shifts depending on the sub-band. Furthermore, the signal that arrives at the receiver often has a time offset that is smaller than a single sample. This cannot be feasibly avoided, because the digital-to-analog converter at the transmitter may not necessarily be in sync with the analog-to-digital converter at the receiver. It is therefore infeasible to decode data embedded using absolute phase coding as soon as it is transmitted over an acoustic channel.","One solution separately apples a k-means clustering algorithm to each sub-band to classify the coefficients into two groups: one cluster for the spreading code symbol 1 (phases closer to 0) and another one for \u22121 (phases closer to \u03c0). Predefined code sequences are periodically transmitted in order for the clustering to operate correctly. Phase shifts can be large, so that a phase close to 0 can be received as a phase close to \u03c0 at the receiver, and vice versa. A receiver that starts listening to the signal can therefore only correctly decode the data, after the receiver has learned which cluster actually belongs to which spreading code symbol. By transmitting a predefined sequence known to the receiver, clusters can be successfully mapped to spreading code symbols. These predefined sequences are sent in lieu of data bits. Hence, doing so decreases the overall transmission bandwidth of the acoustic data transmission system, and to address this problem, alternative techniques disclosed herein may be used.","At least in some embodiments, in sub-band relative phase coding, desired information is not incorporated into the phases directly but into the differences between phases of two adjacent MCLT sub-bands, also referred to as Differential Phase-Shift Keying (DPSK) in digital communication. Similar to Equation 19, the corresponding formula is given by:",{"@attributes":{"id":"p-0082","num":"0094"},"maths":{"@attributes":{"id":"MATH-US-00015","num":"00015"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msubsup":{"mi":["X","\u2032"],"mrow":{"mi":["i","m"],"mo":","}},"mo":"=","mrow":{"mfrac":{"msub":{"mi":"X","mrow":{"mi":"i","mo":",","mrow":{"mi":"m","mo":"-","mn":"1"}}},"mrow":{"mo":["\uf603","\uf604"],"msub":{"mi":"X","mrow":{"mi":"i","mo":",","mrow":{"mi":"m","mo":"-","mn":"1"}}}}},"mo":["\u2062","\u2062"],"mrow":[{"mo":["\uf603","\uf604"],"msub":{"mi":"X","mrow":{"mi":["i","m"],"mo":","}}},{"mi":"s","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"i","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"M","mo":"-","mn":"1"}}},"mo":"+","mi":"m"}}}]}},"mo":","}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"21"}}}]}}}},"br":{},"ul":{"@attributes":{"id":"ul0007","list-style":"none"},"li":{"@attributes":{"id":"ul0007-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0008","list-style":"none"},"li":["i=0, 1, . . . , N\u22121 with N=number of MCLT blocks","m=1, 2, . . . , M\u22121 with M=number of subbands per block","X=Original phase at m-th subband of i-th MCLT block","X\u2032=Modified phase at m-th subband of i-th MCLT block","s(n)\u03b5{\u22121, 1}=Sequence of code symbols representing the data do be embedded.\n\nNote that the first (0-th) sub-band of each MCLT block is used as a reference only and thus remains unchanged with respect to the original signal. Accordingly, m starts at index 1 instead of 0. M sub-bands can only encode M\u22121 differences, but such is not necessarily a disadvantage because to compensate, the frequency range used for data embedding can just be extended to incorporate an additional sub-band. The first symbol is encoded in the phase difference between the reference and the second sub-band (m=2). This is achieved by modifying the phase of the second sub-band to differ by an angle of either 0 or \u03c0 from the phase of the first sub-band. The second symbol is encoded in the phase difference between the second and the third sub-band, and so forth as illustrated in sub-band relative phase coding ,  of .\n"]}}}},"In one embodiment, for each block, the phase of the first sub-band implicitly defines a reference axis on which all the other phases are to lie. Then, the phases are rotated to either the positive or the negative side of this axis, depending on the symbol to be embedded and the phase of the previous sub-band. As with in absolute coding, the magnitudes of the MCLT coefficients are not altered.","In one embodiment, extraction of the phases at the receiver is performed in a similar fashion as in absolute coding described above, with the formula given by:",{"@attributes":{"id":"p-0085","num":"0102"},"maths":{"@attributes":{"id":"MATH-US-00016","num":"00016"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":{"mrow":[{"mi":"r","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"i","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"M","mo":"-","mn":"1"}}},{"mo":["(",")"],"mrow":{"mi":"m","mo":"-","mn":"1"}}],"mo":"+"}}},{"mrow":[{"mo":"-","mn":"2"},{"mo":["(",")"],"mrow":{"mfrac":{"mrow":{"mi":"min","mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mo":["\uf603","\uf604"],"msub":{"mi":"d","mrow":{"mi":["i","m"],"mo":","}}},{"mrow":[{"mn":"2","mo":"\u2062","mi":"\u03c0"},{"mo":["\uf603","\uf604"],"msub":{"mi":"d","mrow":{"mi":["i","m"],"mo":","}}}],"mo":"-"}],"mo":","}}},"mi":"\u03c0"},"mo":"-","mn":"0.5"}}],"mo":"\u2062"}],"mo":"="},"mo":[",","\u2062","\u2062"],"mstyle":[{"mtext":{}},{"mspace":{"@attributes":{"width":"4.4em","height":"4.4ex"}}}],"mi":"where"},{"msub":{"mi":"d","mrow":{"mi":["i","m"],"mo":","}},"mo":"=","mrow":{"mrow":[{"mi":"angle","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"X","mrow":{"mi":["i","m"],"mo":","}}}},{"mi":"angle","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"X","mrow":{"mi":"i","mo":",","mrow":{"mi":"m","mo":"-","mn":"1"}}}}}],"mo":"-"}}],"mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mtext":{}},{"mspace":{"@attributes":{"width":"4.4em","height":"4.4ex"}}}]}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"22"}}}]}}}},"br":{},"sub":"i,m "},"Encoding the message into differences between phases helps address the problem of phase shifts described above in conjunction with absolute phase coding. However, as mentioned above, different phase shifts occur on different sub-bands. Therefore the phase shifts affect the difference between the phases of two neighboring sub-bands and might still introduce errors. Because neighboring sub-bands experience similar phase shifts, sub-band relative phase coding is expected to outperform than absolute phase coding but still may not necessarily address the problem completely. A disadvantage of encoding the data in the phase differences is that a single, incorrectly received phase value causes two incorrectly decoded spreading code symbols. Both differences from the erroneous phase Xto its neighboring phases Xand X(previous and next sub-band) become incorrect. A burst of n incorrectly received phases results in n+1 incorrectly decoded spreading code symbols. In comparison, when using absolute phase coding with the k-means clustering technique, a sequence of n incorrectly received phases only leads to n incorrectly decoded spreading code symbols.","At least in some embodiments, block relative coding is similar to sub-band relative coding in that desired data is encoded in the differences between phases. However, instead of considering the phase difference between two adjacent MCLT coefficients, the difference between the phases of two corresponding MCLT components of two adjacent audio blocks is instead analyzed, as shown in sub-band relative phase coding  and block relative phase coding  of . Block relative coding is defined as follows:",{"@attributes":{"id":"p-0088","num":"0105"},"maths":{"@attributes":{"id":"MATH-US-00017","num":"00017"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msubsup":{"mi":["X","\u2032"],"mrow":{"mi":["i","m"],"mo":","}},"mo":"=","mrow":{"mfrac":{"msub":{"mi":"X","mrow":{"mrow":{"mi":"i","mo":"-","mn":"1"},"mo":",","mi":"m"}},"mrow":{"mo":["\uf603","\uf604"],"msub":{"mi":"X","mrow":{"mrow":{"mi":"i","mo":"-","mn":"1"},"mo":",","mi":"m"}}}},"mo":["\u2062","\u2062"],"mrow":[{"mo":["\uf603","\uf604"],"msub":{"mi":"X","mrow":{"mi":["i","m"],"mo":","}}},{"mi":"s","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":"i","mo":"-","mn":"1"}},"mo":"\u2062","mi":"M"},"mo":"+","mi":"m"}}}]}},"mo":","}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"23"}}}]}}}},"br":{},"ul":{"@attributes":{"id":"ul0009","list-style":"none"},"li":{"@attributes":{"id":"ul0009-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0010","list-style":"none"},"li":["i=1, 2, . . . , N\u22121 with N=number of MCLT blocks","m=0, 1, . . . , M\u22121 with M=number of subbands per block","X=Original phase at m-th subband of i-th MCLT block","X\u2032=Modified phase at m-th subband of i-th MCLT block","s(n)\u03b5{\u22121, 1}=Sequence of code symbols representing the data do be embedded.\n\nA representation of block relative coding  in the complex plane is shown in . The first MCLT block acts as a reference and does not contain any hidden data. Thus, i starts at index 1 in Equation 23. The magnitudes of the MCLT coefficients remain unchanged.\n"]}}}},"In one embodiment, extraction operates similarly as laid out in Equation 22 but with the distinction that the phase differences dare computed between blocks instead of sub-bands. Accordingly, the corresponding formula is given by:",{"@attributes":{"id":"p-0090","num":"0112"},"maths":{"@attributes":{"id":"MATH-US-00018","num":"00018"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":[{"mi":"r","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":"i","mo":"-","mn":"1"}},"mo":"\u2062","mi":"M"},"mo":"+","mi":"m"}}},{"mrow":[{"mo":"-","mn":"2"},{"mo":["(",")"],"mrow":{"mfrac":{"mrow":{"mi":"min","mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mo":["\uf603","\uf604"],"msub":{"mi":"d","mrow":{"mi":["i","m"],"mo":","}}},{"mrow":[{"mn":"2","mo":"\u2062","mi":"\u03c0"},{"mo":["\uf603","\uf604"],"msub":{"mi":"d","mrow":{"mi":["i","m"],"mo":","}}}],"mo":"-"}],"mo":","}}},"mi":"\u03c0"},"mo":"-","mn":"0.5"}}],"mo":"\u2062"}],"mo":"="},{"mrow":[{"mi":"where","mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mtext":{}},{"mspace":{"@attributes":{"width":"4.4em","height":"4.4ex"}}}],"msub":{"mi":"d","mrow":{"mi":["i","m"],"mo":","}}},{"mrow":[{"mi":"angle","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"X","mrow":{"mi":["i","m"],"mo":","}}}},{"mrow":{"mi":"angle","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"X","mrow":{"mrow":{"mi":"i","mo":"-","mn":"1"},"mo":",","mi":"m"}}}},"mo":"."}],"mo":"-"}],"mo":"="}],"mo":[",","\u2062","\u2062"],"mstyle":[{"mtext":{}},{"mspace":{"@attributes":{"width":"4.4em","height":"4.4ex"}}}]}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"24"}}}]}}}}},"As discussed above, phase shifts are not necessarily constant over the frequency spectrum. However, the phase shifts are constant over time as long as the acoustic environment or the position of the microphone and the speakers does not change. Accordingly, block relative phase coding can perform well even in the presence of phase shifts and can operate even with a moving receiver. If the motion is slow or small, the extracted phase differences will be correct because only small phase shifts are introduced by such movement. On the other hand, fast motion may often lead to decoding errors, but as soon as the movement ends, the phase differences are correctly received once again. A single, incorrectly received phase may still cause two incorrectly decoded spreading code symbols, however. The differences between an erroneously received phase and its adjacent phases Xand Xfrom the previous and next block would both be incorrect. But because spreading codes are disposed across sub-bands and not across blocks as shown in block relative coding  of , a single, incorrectly received phase will not lead to a decoding error. The correlation between the received code and the spreading code is still strong enough to decode the corresponding bit.","After embedding the data and transforming the MCLT blocks back to the time domain, the transformed blocks are overlap-added to generate the final audio signal for transmission. In some embodiments, there may be significant interference among frequency responses of neighboring sub-bands in the MCLT domain. Furthermore, the overlap-add operation mixes the phase content of adjacent blocks. The phase content will no longer be the same, when transforming a synthesized audio file to the MCLT basis again. However, in the case where the phases of only every other block and sub-band are changed, these interferences can be canceled. A correction coefficient \u03bcis computed based on the previous and next audio block and the neighboring sub-bands of the current coefficient X. Further, \u03bcis defined as follows:",{"@attributes":{"id":"p-0093","num":"0115"},"maths":{"@attributes":{"id":"MATH-US-00019","num":"00019"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msub":{"mi":"\u03bc","mrow":{"mi":["i","m"],"mo":","}},"mo":"=","mrow":{"mn":"2","mo":"\u2062","mrow":{"mi":"j","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"msubsup":{"mi":["z","T"],"mrow":{"mrow":{"mo":"-","mn":"1"},"mo":",","mi":"m"}},"mo":"\u2062","msub":{"mi":"X","mrow":{"mrow":{"mo":"-","mi":"i"},"mo":"-","mn":"1"}}},{"mfrac":{"mn":["1","4"]},"mo":"\u2062","msub":{"mi":"X","mrow":{"mi":"i","mo":",","mrow":{"mi":"m","mo":"-","mn":"1"}}}},{"mfrac":{"mn":["1","4"]},"mo":"\u2062","msub":{"mi":"X","mrow":{"mi":"i","mo":",","mrow":{"mi":"m","mo":"+","mn":"1"}}}},{"msubsup":{"mi":["z","T"],"mrow":{"mn":"1","mo":",","mi":"m"}},"mo":"\u2062","msub":{"mi":"X","mrow":{"mi":"i","mo":"+","mn":"1"}}}],"mo":["+","-","+"]}}}}},"mo":","}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"25"}}}]}}}},"br":{},"ul":{"@attributes":{"id":"ul0011","list-style":"none"},"li":{"@attributes":{"id":"ul0011-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0012","list-style":"none"},"li":["X, X=MCLT coefficient vectors of previous and next MCLT block, respectively","X, X=MCLT coefficients of previous and next subband, respectively"]}}}},{"@attributes":{"id":"p-0094","num":"0118"},"maths":{"@attributes":{"id":"MATH-US-00020","num":"00020"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msub":{"mi":"z","mrow":{"mi":["i","m"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"l"}},{"mo":"{","mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"mrow":{"mo":["(",")"],"mrow":{"mo":"-","mi":"i"}},"mo":"\u2062","mfrac":{"msup":{"mrow":[{"mo":["(",")"],"mrow":{"mo":"-","mn":"1"}},{"mi":["l","k"],"mo":"+"}]},"mrow":{"mn":"2","mo":["\u2062","\u2062"],"mrow":[{"mi":"\u03c0","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mn":"2","mo":"\u2062","mi":"k"},"mo":"-","mn":"1"}}},{"mo":["(",")"],"mrow":{"mrow":{"mn":"2","mo":"\u2062","mi":"k"},"mo":"+","mn":"1"}}]}}},"mo":","}},{"mrow":{"mrow":[{"mi":"if","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mi":["l","m"],"mo":"-"}}},{"mn":"2","mo":"\u2062","mi":"k"}],"mo":"="}}]},{"mtd":[{"mrow":{"mfrac":{"msup":{"mrow":{"mo":["(",")"],"mrow":{"mo":"-","mn":"1"}},"mi":"l"},"mn":"8"},"mo":","}},{"mrow":{"mrow":{"mi":["else","if"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mi":["l","m"],"mo":"-"}}},"mo":"=","mn":"1"}}]},{"mtd":[{"mrow":{"mn":"0","mo":","}},{"mrow":{"mi":"otherwise","mo":"."}}]}]}}],"mo":"="}}}},"Subtracting \u03bcfrom the altered phase X\u2032 compensates in advance for the interferences to be introduced by neighboring sub-bands and the overlap-add operation. At least in some embodiments, this cancellation technique requires that the adjacent blocks and sub-bands cannot be modified. Because information can only be embedded into every other block and sub-band, four times less information can be transmitted as without the cancellation techniques. This can be compensated for by using a wider frequency range in which data is embedded. A wider frequency range with manipulated phases improves the audibility of the embedded data. However, the effect is not particularly strong, because then only every other sub-band and audio block contains embedded data.",{"@attributes":{"id":"p-0096","num":"0120"},"figref":["FIG. 9","FIG. 9","FIG. 9"],"b":["900","902","904"]},"In one embodiment, when recording an audio signal with an embedded message, the receiver first finds the correct partition into blocks in order to next decode the data. If the audio block partition is incorrect, the resulting MCLT blocks will not contain the phases representing the original message. This is due to a time shift causing a phase shift in the frequency domain, which necessitates synchronization.","At least in some embodiments, the size of the blocks is a parameter of the phase coding algorithm that remains constant. Accordingly, if a receiver identifies the correct offset at which a new audio block starts, the receiver can then partition the audio correctly and synchronize to the embedded signal. To find the correct offset, the synchronization algorithm attempts to decode a single block of the signal at each possible offset. Because interference cancellation is used, bits are only encoded into every other block. Thus, for a block length of N, there are 2N offsets to be tested. To determine the correct offset, the synchronization algorithm decodes a single block for each offset k, resulting in a sequence of correlation coefficients \u03c1(n) with length M as defined in Equation 18. For synchronization, the sum of the absolute values of this sequence is desired, which yields a measure of signal strength S(k) as a function of the offset k and given by:",{"@attributes":{"id":"p-0099","num":"0123"},"maths":{"@attributes":{"id":"MATH-US-00021","num":"00021"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"S","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"k"}},{"mfrac":{"mn":"1","mi":"M"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"m","mo":"=","mn":"0"},{"mi":"M","mo":"-","mn":"1"}]},"mo":"\u2062","mrow":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"msub":{"mi":["\u03c1","k"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"m"}}},"mo":"."}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"26"}}}]}}}}},"At least in some embodiments, the above sum is computed for every offset k=0, 1, 2, . . . , 2N. The maximum signal strength S(q) then indicates the optimal offset q, because at the optimal offset q, \u03c1(n) mostly consists of values close to the spreading code symbols 1 or \u22121, which leads to a high signal strength S(q). This is not the case at a sub-optimal offset r, where \u03c1(n) mainly contains values close to 0, causing S(r) to be small. The closer k is to the optimal offset, the higher the signal strength, as can be seen when plotting S(k) for k=0, 1, 2, . . . , 2N.",{"@attributes":{"id":"p-0101","num":"0125"},"figref":"FIG. 10","b":"1000"},"In one embodiment, for an error free channel, the optimal offset computed by the synchronization procedure described above yields the correct segmentation of the signal into blocks. However, in practice, fairly quiet audio blocks exist, for which the signal strength is low for every offset, and the maximum does not necessarily yield the optimal offset. Thus, the synchronization algorithm may require the maximum signal strength to be higher than a predefined threshold \u03c4. If it is not, the synchronization algorithm is repeated with the next block and continues to repeat, until a maximum signal strength that exceeds \u03c4is found. To increase the probability of determining the correct offset, the synchronization algorithm is executed L\u22121 times more for all the offsets k that exceeded \u03c4in the first run. Then, the average signal strength of the overall L runs is computed for each offset k\u03b5{0, 1, . . . , 2N|S(k)>\u03c4}:",{"@attributes":{"id":"p-0103","num":"0127"},"maths":{"@attributes":{"id":"MATH-US-00022","num":"00022"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msub":{"mover":{"mi":"S","mo":"^"},"mi":"k"},"mo":"=","mrow":{"mfrac":{"mn":"1","mi":"L"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"l","mo":"=","mn":"0"},{"mi":"L","mo":"-","mn":"1"}]},"mo":"\u2062","mrow":{"mfrac":{"mn":"1","mi":"M"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"m","mo":"=","mn":"0"},{"mi":"M","mo":"-","mn":"1"}]},"mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mrow":{"msub":{"mi":["\u03c1","k"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"m"}}}}}}}},"mo":","}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"27"}}}]}}}},"br":{},"img":{"@attributes":{"id":"CUSTOM-CHARACTER-00009","he":"3.13mm","wi":"2.46mm","file":"US09318116-20160419-P00001.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},"sub":"k"},{"@attributes":{"id":"p-0104","num":"0128"},"maths":{"@attributes":{"id":"MATH-US-00023","num":"00023"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"munder":{"mrow":{"mi":["arg","max"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mi":"k"},"mo":"\u2062","mrow":{"msub":{"mover":{"mrow":{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mo":"\u2062","mi":"S"},"mo":"^"},"mi":"k"},"mo":"."}}}}},"In one embodiment, L is typically a value between 3 and 20, depending on the block size. Because more spreading code symbols can be encoded in the phase content of a large block, the signal strength may be computed with improved accuracy. Hence, not as many iterations of the synchronization algorithm are needed as in the case of blocks with fewer spreading codes.","At least in some embodiments, the acoustic data transmission system does not necessarily require dedicated synchronization codes to be embedded in the audio signal. This is an advantage in that more data bits can be embedded instead. Further, synchronization is feasible at any time, as compared to the case where specific synchronization codes are used, in which a receiver may be required to wait until the next occurrence of such codes in the signal before the receiver can synchronize and begin decoding the embedded data.","In one embodiment, block selection can cause the receiver to attempt synchronization on a part of the audio signal that does not contain any embedded data. If this occurs in the first run of the synchronization algorithm, the signal strength will not exceed the threshold for any of the offsets, and synchronization is restarted using the next block. On the other hand, if it does not occur in the first run, the maximum \u015cresulting from all L iterations will not necessarily represent the optimal offset. However, the synchronization procedure may just be repeated until arg max\u015csurpasses a second predefined threshold \u03c4. Both thresholds \u03c4and \u03c4depend on the requirements for a specific application of the acoustic data transmission system.","At least in some embodiment, the reliability of the acoustic channel between a speaker and a microphone strongly depends on the audio content the information is embedded in. It is not feasible to transmit and receive data over such a channel, if the carrier signal is not loud enough or has a limited spectrum. Block selection attempts to improve the link by embedding data only into parts of the audio signal that are expected to work well as a carrier. The audio blocks are analyzed and classified into two groups: strong blocks and weak blocks. The classification is based on properties of the frequency representation of an audio block. An example of such a property is the number of sub-bands for which the magnitude exceeds a predefined threshold. Further, sequences of strong blocks shorter than a given minimum length are also deemed as being weak blocks. Data is only embedded in sequences of strong blocks of a certain length. Such a sequence could be identified at the receiver by a starting delimiter and ending delimiter embedded into its first and last block, respectively.","However, identification of strong blocks at the receiver is not necessarily required. The receiver could alternatively attempt to decode every block and throw away corrupted data. Still alternatively, the receiver could measure the signal strength as defined in Equation 26, from time to time and switch into a duty-cycling mode if the signal strength remains low for a predefined, prolonged period of time. In duty-cycling mode, the receiver periodically decodes just a few blocks each time, checks signal strength, and switches back to normal reception mode if a block contains non-corrupted data. Corruption of data may be determined via cyclic redundancy codes, described in further detail below.","In one embodiment, repetition codes and cyclic redundancy check codes are used to increase reliability of the data link. In combination with spreading codes, doing so provides a form of forward error correction (FEC). Before the data to be embedded is mapped to spreading code symbols and encoded into the phase content of the audio signal, the data is divided into packets of the same size, and redundancy is added to each of these packets.","In one embodiment, fixed packet size P is first defined, which is a multiple of the number of bits that can be embedded in a single block. As discussed earlier, the number is given by M\/K, with M being the number of sub-bands per MCLT block used for data embedding, and K being the spreading code length. By setting P=M\/K, each packet fits into a single MCLT block. This has the advantage that no delimiters are needed to mark the beginning and the end of a packet. However, larger packet sizes may be required by certain techniques, such as the diversity techniques discussed in further detail below. In one embodiment, instead of embedding delimiter symbols, thereby using up valuable bandwidth that could otherwise have been used for data bits, an alternative solution may be used, in which P is chosen as a multiple of M\/K. Thus, the beginning of a packet is also the start of a MCLT block.","In one embodiment, the first MCLT block of a packet can be marked by using different spreading codes c\u2032(k) and c\u2032for embedding data. If the receiver finds these codes during synchronization, then the receiver knows that the current block marks the beginning of a packet. In some embodiments, the synchronization algorithm disclosed herein may be adapted to not only synchronize to the correct block offset but also to find the beginning of a packet. Rather than calculating the signal strength for each block and the single spreading code c(k), both signal strength measures for both spreading codes c(k) and c\u2032are computed, and the maximum of the two resulting values is used. To this end, the two spreading codes should be complementary sequences; in other words, their cross-correlation should be low. Because the packet size P is constant, the receiver knows where packets start and end, once the receiver has performed synchronization using the adapted algorithm.","In one embodiment, a redundancy factor R for repetition coding may be selected. In cases where redundancy is added to each packet, R is chosen as a divisor of the packet size P. Instead of embedding a sequence of bits only once into the audio signal, the sequence of bits is embedded R times consecutively. When choosing R as a divisor of the number of bits encoded per MCLT block, the repetitions of a single bit are encoded in the same sub-bands of different blocks. Then, if the channel is noisy in a certain frequency range, it would not be feasible to decode the data, despite the added redundancy. This can be addressed by carefully selecting R so that it does not align with the block partitioning. An alternative solution involves reordering the spreading codes before encoding with a predefined mapping, and mapping the spreading codes back to their initial order after extracting the phase values at the receiver.","In one embodiment, CRC codes may also be used to detect transmission errors. CRC codes are a form of checksum based on the remainder of a division of a bit sequence by a so-called generator polynomial over the finite field GF(2). The CRC code is computed before repetition coding of a sequence of bits and appended to the end of the sequence. Thus, the CRC code is also repetition-coded to make sure it is transmitted correctly.",{"@attributes":{"id":"p-0115","num":"0139"},"figref":"FIG. 11","b":["1100","1102","1104"]},{"@attributes":{"id":"p-0116","num":"0140"},"maths":{"@attributes":{"id":"MATH-US-00024","num":"00024"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":[{"mover":{"mi":"\u03c1","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},{"mfrac":{"mn":"1","mi":"R"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"m","mo":"=","mn":"0"},{"mi":"R","mo":"-","mn":"1"}]},"mo":"\u2062","mrow":{"mi":"\u03c1","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["m","R"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mo":"+","mi":"n"}}}}}],"mo":"="},{"mrow":{"mi":["where","n"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},"mo":"=","mn":"0"},{"mi":["P","R"],"mo":"\/"},{"mi":"m","mo":"\u2208","mrow":{"mi":"\u2124","mo":"."}}],"mo":[",","\u2062",",",",","\u2062",",",","],"mstyle":[{"mtext":{}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mn":"1","mi":"\u2026"}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"28"}}}]}}}},"br":{}},"Accordingly, reliability of the acoustic data transmission link may be improved using the techniques disclosed herein. In some embodiments, however, feedback from the receiver to the transmitter is required in order to achieve a reliable channel with a negligible probability of transmission errors. For instance, such feedback may be in the form of positive\/negative acknowledgments (ACK\/NACK) in an acoustic data transmission system between smartphones, for smartphones that are equipped with both speakers and microphones. However, at least some techniques are not necessarily suited for such a system, such as techniques in which phase coding is processed offline and therefore does not support dynamically encoding data into audio in real-time. However, in alternative embodiments, the acoustic data transmission system may be configured with real-time phase encoding based on the techniques disclosed herein.","In one embodiment, the acoustic data transmission system may be implemented in any predefined programming language such as matrix laboratory (MATLAB). The acoustic data transmission system contains both the data embedding and extraction algorithms and can therefore act as both the transmitter and as the receiver. A data embedding module accepts bit sequences from vectors consisting of zeros as input data. Arbitrary audio files in the common uncompressed .wav format may be used as carriers. The acoustic data transmission system implements all three phase coding methods disclosed above for both embedding and extraction of data. Interference cancellation, block selection, and repetition coding and CRC codes are also supported, and phase coding is performed in the MCLT domain. In addition, the acoustic data transmission system contains a script that embeds information, extracts the information, and compares the result to the initial input. This can be used to evaluate the three phase coding methods against each other for different parameter configurations.","In one embodiment, as an alternative to playing back the phase-coded audio signal and recording it with a microphone, the acoustic data transmission system supports simulating an acoustic channel. The characteristics of a channel may be described by its impulse response, which is measurable. When viewing at the audio channel as a linear time invariant filter, its effects may be simulated by convolving its impulse response with the signal to be transmitted over the channel. The acoustic data transmission system uses convolution defined in Equation 8 to simulate an acoustic channel given its impulse response.",{"@attributes":{"id":"p-0120","num":"0144"},"figref":"FIG. 12","b":["1200","1202","1204","1206","1208"]},"In one embodiment, acoustic data transmission techniques may be combined with space diversity techniques to further increase the reliability of the data link. In space diversity, multiple receivers and\/or transmitters are used to improve the quality and reliability of wireless communication links. This concept may also be referred to as antenna diversity in radio communications. The usage of multiple antennas at both the transmitter and the receiver is termed Multiple-Input Multiple-Output (MIMO), which is an important part of modern wireless communication standards, e.g., any form of wireless fidelity (Wi-Fi) such as Institute of Electrical and Electronics Engineers (IEEE) 802.11n. In some embodiments, the acoustic data transmission system may be configured to include multiple antennas or receivers but only a single transmitter. This configuration may be referred to as Single-Input Multiple-Output (SIMO). The operating mode or operating configuration of the acoustic data transmission system may be MIMO or SIMO, depending on the needs of a particular case. In some embodiments, any operating mode supporting multiple transmitters may be used in conjunction with the embodiments disclosed herein.","In one embodiment, because of reflections that may often occur in indoor environments, an emitted signal may take multiple different paths to the receiver. Reflections lead to phase shifts and other distortions that can cause destructive interference at the receiver (fading). Interference makes the signal more difficult or impossible to decode. Because the type and degree of interference is location- and time-dependent, multiple physically separated receivers allow the same signal to be observed under different conditions. Often, if one receiver experiences a large amount of destructive interference, one of the other receivers has sufficient signal quality. By combining the received signals from all the receivers according to techniques disclosed herein, link quality and reliability can be drastically improved at least in some cases.","At least in some embodiments, the link quality of the acoustic data transmission system may at times suffer from interferences caused by reflections of the sound waves from walls and objects. Another scenario is that some of the receivers may not be well-positioned or may be exposed to noise. Smartphones may be located inside of a pocket or purse or even next to a rustling popcorn bag in the cinema. In one embodiment, spatial diversity may be applied to alleviate these scenarios. Assuming that L receivers are connected to each other in a network with reliable communication links, a predefined diversity scheme allows each receiver to decode the data independently, and the resulting bit sequences may be combined via majority voting. This means that for each bit, the number of receivers that decoded a  as well as the amount of receivers which vote for a \u22121 are counted. The higher number of votes then determines the value of a bit in the combined sequence. Formally this can be expressed as:",{"@attributes":{"id":"p-0124","num":"0148"},"maths":{"@attributes":{"id":"MATH-US-00025","num":"00025"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mover":{"mi":"b","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},{"mo":"{","mtable":{"mtr":[{"mtd":[{"mrow":{"mn":"1","mo":","}},{"mrow":{"mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"l","mo":"=","mn":"0"},{"mi":"L","mo":"-","mn":"1"}]},"mo":"\u2062","mrow":{"msub":{"mi":["b","l"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}}},"mo":"\u2265","mn":"0"}}]},{"mtd":[{"mrow":{"mrow":{"mo":"-","mn":"1"},"mo":","}},{"mrow":{"mrow":{"mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"l","mo":"=","mn":"0"},{"mi":"L","mo":"-","mn":"1"}]},"mo":"\u2062","mrow":{"msub":{"mi":["b","l"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}}},"mo":"<","mn":"0"},"mo":","}}]}]}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"29"}}}]}}}},"br":{},"sub":"1"},"Note that in Equation 29, \u22121 is used to represent the 0-bit to simplify notation. The number of receivers L should be odd. Otherwise, especially with a low number of receivers, a vote may often end up as a draw. In that case, no majority decision can be made, and by convention of the above formula, the corresponding bit is assumed to be a 1. Another issue with this type of diversity is that receivers experiencing poor signal quality may have the same impact on the final bit sequence as receivers that experience good signal quality.","In one embodiment, the predefined space diversity scheme may be improved to keep track of how many times each receiver voted correctly (the same as the majority vote). The improved space diversity scheme may be referred to as an adaptive space diversity scheme. The number of correct votes may be translated into a weight w\u03b5[0, 1], which determines how much impact the votes from a receiver is to have in the future. These weights change with time, and the more correct votes a receiver makes, the higher its weight becomes, and vice versa. The number of previous votes taken into account to update the weights influences how fast the diversity scheme adapts to changes in signal quality (sliding window). This leads to the following new formula:",{"@attributes":{"id":"p-0127","num":"0151"},"maths":{"@attributes":{"id":"MATH-US-00026","num":"00026"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mover":{"mi":"b","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},{"mo":"{","mtable":{"mtr":[{"mtd":[{"mrow":{"mn":"1","mo":","}},{"mrow":{"mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"l","mo":"=","mn":"0"},{"mi":"L","mo":"-","mn":"1"}]},"mo":"\u2062","mrow":{"msub":{"mi":["w","l"]},"mo":"\u2062","mrow":{"msub":{"mi":["b","l"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}}}},"mo":"\u2265","mn":"0"}}]},{"mtd":[{"mrow":{"mrow":{"mo":"-","mn":"1"},"mo":","}},{"mrow":{"mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"l","mo":"=","mn":"0"},{"mi":"L","mo":"-","mn":"1"}]},"mo":"\u2062","mrow":{"msub":{"mi":["w","l"]},"mo":"\u2062","mrow":{"msub":{"mi":["b","l"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}}}},"mo":"<","mn":"0."}}]}]}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"30"}}}]}}}},"br":{}},"As described above in conjunction with Equation 18, phase decoding results in a sequence of correlation coefficients in some embodiments. These correlation coefficients give a measure for the correctness of the corresponding bits, which can be used for diversity purposes. Such a space diversity scheme may be referred to as a space diversity scheme with spreading code correlation sequences. Instead of combining the bit sequences b(n) from each receiver, their correlation sequences \u03c1(n) are combined:",{"@attributes":{"id":"p-0129","num":"0153"},"maths":{"@attributes":{"id":"MATH-US-00027","num":"00027"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":[{"mover":{"mi":"\u03c1","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},{"mfrac":{"mn":"1","mi":"L"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"l","mo":"=","mn":"0"},{"mi":"L","mo":"-","mn":"1"}]},"mo":"\u2062","mrow":{"msub":{"mi":["\u03c1","l"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}}}}],"mo":"="},"mo":","}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"31"}}}]}}}},"br":{},"ul":{"@attributes":{"id":"ul0013","list-style":"none"},"li":{"@attributes":{"id":"ul0013-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0014","list-style":"none"},"li":["L=The total number of receivers","b(n)\u03b5{\u22121, 1}=The bit sequence of length N decoded at receiver l"]}}}},{"@attributes":{"id":"p-0130","num":"0156"},"maths":{"@attributes":{"id":"MATH-US-00028","num":"00028"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["\u03c1","l"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},{"mfrac":{"mn":"1","mi":"K"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"k","mo":"=","mn":"0"},{"mi":"K","mo":"-","mn":"1"}]},"mo":"\u2062","mrow":{"mrow":[{"msub":{"mi":["b","l"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["nK","k"],"mo":"+"}}},{"msub":{"mi":["c","one"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"k"}}],"mo":"\u2062"}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mrow":{"mi":["from","Equation"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mn":"18"}}}]}}}},"ul":{"@attributes":{"id":"ul0015","list-style":"none"},"li":{"@attributes":{"id":"ul0015-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0016","list-style":"none"},"li":"{circumflex over (\u03c1)}(n)=The combined spreading code correlation sequence."}}}},"Note that Equation 31 is essentially the same as the formula for combining repetition codes (Equation 28). Equation 31 computes a weighted sum of the signals received at the different, physically separated receivers. Bits with high corresponding correlation coefficients are likely to be decoded correctly and therefore have a stronger influence on the combined sequence than the bits with a low corresponding correlation coefficient. As discussed above, the sign of {circumflex over (\u03c1)}(n) determines if the corresponding bit is a \u22121 (logical 0) or a 1 (logical 1).",{"@attributes":{"id":"p-0132","num":"0159"},"figref":"FIG. 13","b":["1300","1302","1304","1308","1306"]},"In one embodiment, if packets with CRC codes are used, the space diversity scheme may be further optimized. Before combining the packets, their CRC is computed. If a cyclic redundancy check was successful, the corresponding packet can be assumed to be valid. The packet is thus used as the result instead of computing the combined result of all packets as discussed above. Accordingly, at least some embodiments herein are described with reference to an acoustic data transmission system that implements space diversity with spreading code correlation sequences and CRC optimization.",{"@attributes":{"id":"p-0134","num":"0161"},"figref":"FIG. 14","b":["1400","1400","1404","1402","1406","1408"]},"In one embodiment, the receiver application has a graphical user interface (GUI) that includes a button to start\/stop reception and an area where the received data is displayed. In a particular embodiment, the receiver application is configured to decode two different data formats transmitted through an acoustic channel: simple sequences of bits and packets with American Standard Code for Information Interchange (ASCII) characters. The encoder is configured to use repetition codes with a redundancy factor of three and CRC codes for the ASCII packets. When setting the rest of the encoding parameters to their default values, a data rate of about 160 bits per second is achieved. Accordingly, in the case of 8-bit ASCII encoding, 20 characters per second can be transferred. When transmitting arbitrary bit sequences without redundancy coding and CRC codes, the default encoding parameters permit a data rate of 520 bits per second, in which case the data rate is equal to the bit rate of the system, because no redundant bits are transmitted.","In one embodiment, communication between different receivers for diversity purposes is achieved through a Wi-Fi ad-hoc network hosted by one of the receivers. Creating the network programmatically may not necessarily be officially supported by the operating systems of the smartphones. However, automatic peer-to-peer networks may still be possible using Bluetooth on the operating system. In some embodiments, the receiver application is extended to support Bluetooth in addition to Wi-Fi. And although communication over a Wi-Fi hotspot or cellular network is also contemplated, in such scenarios the information does not need to be embedded in the audio signal but can directly be transmitted via a Wi-Fi or cellular connection. The amount of information exchanged between receivers over the ad-hoc network is fairly low. For each bit received over the acoustic channel, a 32-bit floating point number is sent to the diversity server which, in turn, sends back a single bit containing the diversity result. Thus, for an acoustic data transmission bit rate of 520 bits per second, the resulting Wi-Fi traffic amounts to 33*520=17,160 bits per second, which is approximately 20 kilobits per second.","In one embodiment, the integrated phase decoding algorithm uses a Fast Modulated Complex Lapped Transform (FMCLT) implementation. That vector instructions are used to compute the transform and phase manipulations has a positive impact on performance of the receiver application, thus conserving battery of the smartphone on which the receiver application is executing. In a particular experiment in which the receiver application executing on a smartphone constantly received data from a server over Wi-Fi but did not communicate its results back to the server, the battery level of the smartphone dropped down to fifty percent after five-and-a-half hours, and battery consumption may be higher with diversity over Wi-Fi.",{"@attributes":{"id":"p-0138","num":"0165"},"figref":"FIG. 15","b":["1500","1500","1506","1502","1504"]},{"@attributes":{"id":"p-0139","num":"0166"},"figref":["FIG. 16","FIG. 15"],"b":["1600","1602","1604","1606","1502","1502","1502","1502"]},"In one embodiment, the ReceiverView allows a user to start and stop the receiver by tapping a button at the top of the screen. The current state of the receiver application is displayed as a text label next to the button. When the user launches the receiver application, the receiver application begins in the Connecting state. Once the connection to the server is established, the current state changes to the Syncing state. As soon as the receiver application is synchronized to an input audio signal with embedded data, the current state switches to the Receiving state. In the Receiving state, the received data (bits or ASCII text) is displayed to the user along with the current packet loss rate and signal strength. In the case of ASCII text, successfully received character sequences are denoted with a font background of green, while invalid characters are denoted with a font background of red. When the user indicates to stops reception, the current state of the receiver application switches to the Stopped state.",{"@attributes":{"id":"p-0141","num":"0168"},"figref":"FIG. 17","b":"1700"},"In one embodiment, the model components  contain the application logic. The model components  may be divided into four parts: the Settings singleton, the data extraction logic, the diversity client and the diversity server. In some embodiments, the model components  also include classes with logging features used for performance evaluation. The Settings class is a singleton that contains the configurable parameters of the receiver application and shown in . Given that many of these settings are used across all the modules of the system, the settings may preferably be stored in a central place that is accessible from anywhere in the receiver application. The initial configuration is loaded from a settings property list file when the receiver application is launched. The settings can then be changed by the user in the SettingsView. The current configuration is saved in the property list file before the receiver application terminates.","In one embodiment, in terms of the data extraction logic, as soon as the user activates the start button in the ReceiverView, the MessageExtractor constantly receives new audio blocks from an AudioSource, which is normally an AudioRecorder. The AudioPlayer component is only used for testing and supports reading blocks directly from an audio file stored on the device instead of recording a signal through the microphone. Once the MessageExtractor receives a new audio block, the MessageExtractor passes the new audio block on to the PhaseDecoder. The PhaseDecoder synchronizes to the signal hidden in the recorded audio and extracts the embedded data using block relative phase decoding described above. The PhaseDecoder then sends the result back to the MessageExtractor, which in turn forwards the result to the MainController.","In one embodiment, the MainController then forwards the extracted data to the DiversityClient, which in turn sends the extracted data to the DiversityServer. At least in some embodiments, the MessageExtractor does not directly communicate with the DiversityClient in order to keep data extraction logic separate from client networking code. Results received from the server are passed back to the MainController.","In one embodiment, the DiversityServer class is only used on the particular device that acts as a server in the ad-hoc network of receivers. Thus, at least in some embodiments, the DiversityServer is kept separate from the rest of the model and only made visible to the MainController. Each receiver in the network sends extracted packets through its DiversityClient to the DiversityServer. The server reads the decoded sequence of each ConnectedClient and combines them using the diversity scheme from Equation 31 with the CRC optimization discussed above. The ConnectedClient class is the server-side counterpart of the DiversityClient class. At least in some embodiments, the clients are synchronized to each other to avoid incorrect results stemming from combining decoded packets representing different blocks of the recorded audio. The DiversityServer operates in predefined rounds to keep the clients synchronized. A round starts as soon as a ConnectedClient delivers a new packet and ends after a timer with a predefined duration has expired. The timer is started at the beginning of the round. As long as the timer is running, the other ConnectedClients have time to deliver new packets also. This timer is preferable at least in some cases, because the receivers receive the embedded data with a certain delay depending on their distance from the speaker system and the speed of sound. In addition, the Wi-Fi link may also introduce a short delay.","In one embodiment, after the timer has expired, the delivered packets are combined, and the result is sent over the network to all of the ConnectedClients. Clients that could not deliver a packet in time are considered out of sync. Unsynchronized clients keep receiving results from the server, but their packets are not included in the diversity computations. The DiversityServer keeps track of how many packets the clients are behind by and disconnects them if they are not able to catch up within a certain number of rounds. ConnectedClients may often get out of sync due to a temporarily unreliable Wi-Fi connection. Once the Wi-Fi connection operates reliably again, the ConnectedClients may catch up without issue, because Wi-Fi may provide much higher data rates than the acoustic data transmission system. At least in some embodiments, the key factor for synchronization of the clients is not the bandwidth but rather the delay of the network connection. To tolerate a higher network delay, the sync block interval parameter may be increased, which leads to a larger packet size. The bigger the packets, the longer it takes to extract them from the audio signal, and therefore the more time the DiversityServer has to combine them before the next set of packets is to be handled. At least in some embodiments, no dedicated server device is needed. The smartphone that runs the DiversityServer can simultaneously act as a receiver with a DiversityClient.","In one embodiment, the controller components  include four controller classes, including one for each of its views and a main controller. The main controller, referred to as MainController, inherits from the TabBarController class, which is part of an application programming interface (API) associated with the operating system of the smartphone. When the receiver application is launched, the receiver application instantiates the three view controllers and hosts them in a tab bar. This causes a navigation bar to be displayed in each view at the bottom of the screen. When the user switches between views, the MainController receives an event and can display the requested view. The MainController also instantiates the MessageExtractor, the DiversityClient and the DiversityServer and manages interactions between these modules. The view controllers receive events from their corresponding views and handle them in order to keep the views up to date. Events that cause a change of the application state are passed on to the MainController, which is configured to update the model accordingly. One exception, however, is the SettingsViewController. Whenever the SettingsView is loaded, the SettingsViewController instructs the MainController to stop recording and message extraction. Then the SettingsViewController updates the settings according to user input in the Settings singleton.",{"@attributes":{"id":"p-0148","num":"0175"},"figref":"FIG. 18","b":["1800","1800","1802","102","1804","102"]},"At step , the audio transmitter application  acoustically transmits the text to a receiving entity such as the audio receiver application , by playing the modified audio content. At least in some embodiments, the audio receiver application  is configured to, at step , acoustically detect playback of the modified audio content. The audio receiver application  is also configured to, at step , decode the data based on the acoustically detected playback of the modified audio content. At least in some embodiments, multiple instances of the audio receiver application  collaboratively decode the data using a network and according to a predefined diversity scheme. In a particular embodiment, the network is an ad-hoc Wi-Fi or Bluetooth network, and the predefined diversity scheme is adaptive space diversity optionally with spreading code correlation sequences, CRC optimization, or both. Further, at least in some embodiments, the decoded data is identical to the data received by the audio transmitter application  at step . The audio receiver application  is also configured to, at step , perform a predefined action on the decoded data. Examples of predefined actions include outputting the decoded data and generating an output based on the decoded data, where the generated output does not include the decoded data.",{"@attributes":{"id":"p-0150","num":"0177"},"figref":["FIG. 19","FIG. 1"],"b":["1900","1900","100","1900","1952","1950","1952","1903","1950","1903","1903","1903"]},"In one embodiment, the client systems  may include existing computer systems, e.g., smartphones and other cellular phones, desktop computers, server computers, laptop computers, tablet computers, gaming consoles, hand-held or portable devices and the like. The client systems  illustrated in , however, are merely examples of computer systems in which embodiments disclosed herein may be used. Embodiments disclosed herein may be implemented differently, regardless of whether the computer systems are complex multi-user computing systems, such as a cluster of individual computers connected by a high-speed network, single-user workstations, or network appliances lacking non-volatile storage. Moreover, it is explicitly contemplated that embodiments disclosed herein may be implemented using any device or computer system capable of performing the functions described herein.","As shown, each client system  and server system  includes, without limitation, a processor , which obtains instructions and data via a bus  from a memory  and storage . The processor  is a programmable logic device that performs instruction, logic, and mathematical processing, and may be representative of one or more CPUs. The memory  is any memory sufficiently large to hold the necessary programs and data structures. The memory  could be one or a combination of memory devices, including Random Access Memory, nonvolatile or backup memory (e.g., programmable or Flash memories, read-only memories, etc.).","As shown, the memory  includes an operating system (\u201cOS\u201d) . Operating system  is software used for managing the operation of the client system  or the server system . Examples of the OS  include UNIX, versions of the Microsoft Windows\u00ae operating system and distributions of the Linux\u00ae operating system. Additional examples of the OS  include custom operating systems for smartphones and gaming consoles, including the custom operating systems for systems such as the Microsoft Xbox 360\u00ae, Nintendo WHO and Sony PlayStation\u00ae 3. As shown, the memory of the client system  further includes the audio receiver application  having the decoder module , according to embodiments described above. The memory of the server system  further includes the audio transmitter application  having the encoder module , according to embodiments described above.","In one embodiment, the storage  is representative of hard-disk drives, flash memory devices, optical media and the like. Generally, the storage  stores application programs and data for use by the client systems . In addition, the memory  and the storage  may be considered to include memory physically located elsewhere; for example, on another computer coupled to the client system  or to the server system  via the bus . The client systems  and the server systems  include network interfaces for operably connecting to one another via a network, such as the network . As shown, the storage of the server system  includes the audio content , video content  that is associated with the audio content , the data , and modified audio content  having encoded text, according to embodiments described above. The storage of the client system  includes the decoded text , according to embodiments described above.","In one embodiment, the server systems  and the client systems  are each coupled to a display device . The display devices  may include output devices such as cellular phone displays, movie theater displays, monitors, touch screen displays, and so on. For example, in a particular embodiment, the display devices of the client systems  may include smartphone displays used to visually convey the decoded data , while the display devices of the server system  may include a cinema display used to play back the video content . At least in some embodiments, the video content  is not played back via the display devices of the client systems , and the data  is not conveyed via the display devices of the server system . In one embodiment, where the text includes song lyrics of the audio content being played back, the audio receiver application  is configured to convey each syllable or word of the song lyrics, concurrently with the respective syllable or word is being played back in the audio content, thereby synchronizing visual display of the song lyrics of the audio content with aural playback of the audio content.","In some embodiments, each display  may provide a touch sensitive surface allowing the user to interact with the decoded data , such as to select a coupon or other incentive, a physical good or a virtual good pertaining to the audio content  or the video content . The audio input device  may represent any device configured to produce sound, such as a loudspeaker. The audio input device  may represent any device configured to detect sound, such as a microphone. In a particular embodiment, in acoustically playing back the modified audio content , the loudspeaker of a movie theater produces sound that is detected by the microphone of a cellular phone, and the cellular phone decodes text from the detected sound and outputs the decoded text via a display screen of the cellular phone, thus providing cellular phone users with a more immersive multimedia experience in the movie theater at least in some cases.","In some embodiments, each client system  or server system  may include other input devices such as keypads, keyboards, mice, controllers, and so on. Such input devices may include a set of buttons, switches or other physical device mechanisms for controlling the client system  or the server system . For example, such input devices could include a set of directional buttons used to select a coupon or other incentive, a physical good or a virtual good presented on the display device . Additionally or alternatively, such input devices may also include one or more sensors such as cameras, GPS modules, accelerometers, light sensors, etc.","In the preceding, reference is made to embodiments presented in this disclosure. However, the scope of the present disclosure is not limited to specific described embodiments. Instead, any combination of the following features and elements, whether related to different embodiments or not, is contemplated to implement and practice contemplated embodiments. Furthermore, although embodiments disclosed herein may achieve advantages over other possible solutions or over the prior art, whether or not a particular advantage is achieved by a given embodiment is not limiting of the scope of the present disclosure. Thus, the preceding aspects, features, embodiments and advantages are merely illustrative and are not considered elements or limitations of the appended claims except where explicitly recited in a claim(s). Likewise, reference to \u201cthe invention\u201d shall not be construed as a generalization of any inventive subject matter disclosed herein and shall not be considered to be an element or limitation of the appended claims except where explicitly recited in a claim(s).","Aspects presented in this disclosure may be embodied as a system, method or computer program product. Accordingly, aspects disclosed herein may take the form of an entirely hardware embodiment, an entirely software embodiment (including firmware, resident software, micro-code, etc.) or an embodiment combining software and hardware aspects that may all generally be referred to herein as a \u201ccircuit,\u201d \u201cmodule\u201d or \u201csystem.\u201d Furthermore, aspects disclosed herein may take the form of a computer program product embodied in one or more computer readable medium(s) having computer readable program code embodied thereon.","Any combination of one or more computer readable medium(s) may be utilized. The computer readable medium may be a computer readable signal medium or a computer readable storage medium. A computer readable storage medium may be, for example, but not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, or device, or any suitable combination of the foregoing. More specific examples (a non-exhaustive list) of the computer readable storage medium would include the following: an electrical connection having one or more wires, a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), an optical fiber, a portable compact disc read-only memory (CD-ROM), an optical storage device, a magnetic storage device, or any suitable combination of the foregoing. In the context of this disclosure, a computer readable storage medium may be any tangible medium that can contain, or store a program for use by or in connection with an instruction execution system, apparatus, or device.","A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein, for example, in baseband or as part of a carrier wave. Such a propagated signal may take any of a variety of forms, including, but not limited to, electro-magnetic, optical, or any suitable combination thereof. A computer readable signal medium may be any computer readable medium that is not a computer readable storage medium and that can communicate, propagate, or transport a program for use by or in connection with an instruction execution system, apparatus, or device.","Program code embodied on a computer readable medium may be transmitted using any appropriate medium, including but not limited to wireless, wireline, optical fiber cable, RF, etc., or any suitable combination of the foregoing.","Computer program code for carrying out operations for aspects disclosed herein may be written in any combination of one or more programming languages, including an object oriented programming language such as Java, Smalltalk, C++ or the like and conventional procedural programming languages, such as the \u201cC\u201d programming language or similar programming languages. The program code may execute entirely on the computer of a user, partly on the computer of the user, as a stand-alone software package, partly on the computer of the user and partly on a remote computer, or entirely on the remote computer or server. In the latter scenario, the remote computer may be connected to the computer of the user via any type of network, including a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).","Aspects presented in this disclosure are described above with reference to flowchart illustrations or block diagrams of methods, apparatus (systems) and computer program products according to embodiments disclosed herein. It will be understood that each block of the flowchart illustrations or block diagrams, and combinations of blocks in the flowchart illustrations or block diagrams, can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute via the processor of the computer or other programmable data processing apparatus, create means for implementing the functions\/acts specified in the flowchart or block diagram block or blocks.","These computer program instructions may also be stored in a computer readable medium that can direct a computer, other programmable data processing apparatus, or other devices to function in a particular manner, such that the instructions stored in the computer readable medium produce an article of manufacture including instructions which implement the function\/act specified in the flowchart or block diagram block or blocks.","The computer program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other devices to cause a series of operational steps to be performed on the computer, other programmable apparatus or other devices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions\/acts specified in the flowchart or block diagram block or blocks.","Embodiments disclosed herein may be provided to end users through a cloud computing infrastructure. Cloud computing generally refers to the provision of scalable computing resources as a service over a network. More formally, cloud computing may be defined as a computing capability that provides an abstraction between the computing resource and its underlying technical architecture (e.g., servers, storage, networks), enabling convenient, on-demand network access to a shared pool of configurable computing resources that can be rapidly provisioned and released with minimal management effort or service provider interaction. Thus, cloud computing allows a user to access virtual computing resources (e.g., storage, data, applications, and even complete virtualized computing systems) in \u201cthe cloud,\u201d without regard for the underlying physical systems (or locations of those systems) used to provide the computing resources.","Typically, cloud computing resources are provided to a user on a pay-per-use basis, where users are charged only for the computing resources actually used (e.g. an amount of storage space consumed by a user or a number of virtualized systems instantiated by the user). A user can access any of the resources that reside in the cloud at any time, and from anywhere across the Internet. In context of the present disclosure, the audio content  or the data  may be stored in the cloud, while the audio transmitter application  or the audio receiver application  may additionally execute in the cloud, thereby improving availability of the audio content, the text, or the modified audio content at least in some cases.","The flowchart and block diagrams in the Figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods and computer program products according to various embodiments disclosed herein. In this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of code, which comprises one or more executable instructions for implementing the specified logical function(s). In some alternative implementations, the functions noted in the block may occur out of the order noted in the figures. For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse order, depending upon the functionality involved. Each block of the block diagrams or flowchart illustration, and combinations of blocks in the block diagrams or flowchart illustration, can be implemented by special-purpose hardware-based systems that perform the specified functions or acts, or combinations of special purpose hardware and computer instructions.","While the foregoing is directed to embodiments presented in this disclosure, other and further embodiments may be devised without departing from the basic scope of contemplated embodiments. That is, although specific embodiments and numerous specific details are set forth to provide a more thorough understanding of the present disclosure, persons skilled in the art, however, will understand that various modifications and changes may be made thereto without departing from the broader spirit and scope of the disclosure. The foregoing description and drawings are, accordingly, to be regarded in an illustrative rather than a restrictive sense."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["So that the manner in which the above recited features presented in this disclosure can be understood in detail, a more particular description, briefly summarized above, may be had by reference to embodiments, some of which are illustrated in the appended drawings. It is to be noted, however, that the appended drawings illustrate only typical embodiments presented in this disclosure and are therefore not to be considered limiting of its scope, for the disclosure may admit to other equally effective embodiments.",{"@attributes":{"id":"p-0006","num":"0005"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0007","num":"0006"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 18"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 19"}]},"DETDESC":[{},{}]}
