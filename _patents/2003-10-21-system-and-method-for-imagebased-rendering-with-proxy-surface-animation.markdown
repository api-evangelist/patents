---
title: System and method for image-based rendering with proxy surface animation
abstract: Methods and systems for animating with proxy surfaces are provided. A method for animating includes preprocessing an object to form proxy surfaces of part(s) and/or joint(s), and rendering the proxy surfaces to be animated. In an embodiment, preprocessing includes dividing an object to be animated into parts that can move independently without changing shape, forming a proxy surface for each of the parts corresponding to an initial viewing direction, and obtaining a set of view textures for each of the proxy surfaces. Each part proxy surface is then rendered at a new viewing direction. The new viewing direction is function of an object transformation, part transformation, and an initial viewing direction. The object is then animated by repeating the rendering steps. In another embodiment, the object to be animated is divided into parts and at least one joint that can change shape.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=06924805&OS=06924805&RS=06924805
owner: Silicon Graphics, Inc.
number: 06924805
owner_city: Mountain View
owner_country: US
publication_date: 20031021
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION","EXAMPLE ARCHITECTURE OF THE INVENTION","EXAMPLE SYSTEM EMBODIMENT OF THE PRESENT INVENTION","EXAMPLE METHOD EMBODIMENTS OF THE PRESENT INVENTION","EXAMPLE SYSTEM EMBODIMENT OF THE PRESENT INVENTION","CONCLUSION"],"p":["This application is a Continuation-in-Part of U.S. application Ser. No. 10\/197,845, filed Jul. 19, 2002, and this application is also a Continuation-in-Part of U.S. application Ser. No. 10\/197,822, filed Jul. 19, 2002, now U.S. Pat. No. 6,831,642; both of which are incorporated herein by reference in their entirety.","1. Field of the Invention","The present invention relates to computer graphics including animation.","2. Related Art","A wide variety of applications rely on computer graphics to generate images. An image is made up of an array of picture elements (pixels) and can be displayed on a display unit, such as, a monitor, screen, or cathode ray tube. Images often include graphical representations of objects. Many different types of computing devices with graphics capabilities are used to generate images. Such computing devices use graphics processing. A combination of software, firmware and\/or hardware is used to implement graphics processing. For example, graphics processing including rendering can be carried out in a graphics card, graphics subsystem, graphics processor, graphics or rendering pipeline, and\/or a graphics application programming interface (API) and libraries, such as OPENGL.","In computer graphics, the geometry of an object is represented by primitives. Common primitives are points, lines, and polygons such as triangles or quads. Each primitive is made up of points called vertices. Each vertex includes information relating to the object and its graphical display as an image. For example, vertex information can include, but is not limited to, vertex coordinates, texture coordinates, an alpha value, and a depth value. Vertex coordinates define the location of a respective vertex in a three-dimensional coordinate space (x,y,z), such as, an object coordinate space or a world coordinate space. In this way, vertex coordinates define the geometry of an object surface. Texture coordinates map to a texture (such as, an image in a two-dimensional texture space (s,t) or a three-dimension texture space (r,s,t)). One way this image (also called a texture) is used in graphics processing is to provide additional surface detail to the object. An alpha value is used to represent transparency information. The depth value is used to define depth information.","Increasing demands are being made upon graphics processing, particularly in the area of animation. These demands include realism, speed and cost. Realistic images are desired to be rendered at real-time, interactive rates while avoiding burdensome graphics processing or hardware requirements. One way to achieve more realism is to use complex objects. Complex objects, however, can involve a large number of primitives. For example, hundred, thousands, or millions of triangles may be needed to represent complex objects. This increases the number of calculations and other processing work required to fully render the object. This complexity is transferred to animation, which basically consists of repetitive object rendering from different camera or viewpoint directions.","Billboard image-based rendering algorithms have been used as an alternative to full rendering of objects. In such a billboard approach object data is rendered as if it were mapped or tiled on a flat plane or \u201cbillboard.\u201d The billboard approach, however, sacrifices realism and geometry detail. Accordingly, its usefulness is limited to objects viewed from afar unless hundreds of pre-computed texture images are used exceeding the limits of texture hardware. The billboard approach also generally requires many pre-computed texture images. Further, using billboards for an animated object is unreasonable because each frame of the animation would require all the view textures needed for a static object. Not only this would result in a extremely large number of textures that would need to be stored but also would require the animation sequence to be known at the preprocessing step. Others have been using a single view direction, reducing the number of textures, but making it impossible to view the object from more than a single view direction.","One approach to accommodating these demands is to approximate geometric information by forming an object proxy. Conventional approaches to forming an object proxy, however, have been limited. For example, Buehler et al. describes use of geometric proxies based on a progressive hull technique described by Sander et al. See, Buehler et al., \u201cUnstructured Lumigraph Rendering,\u201d SIGGRAPH 2001, Los Angeles, Calif., August 2001 (pages 1-8); and Sander et al., \u201cSilhouette Clipping,\u201d SIGGRAPH 2000, New Orleans, La., July 2000 (pages 1-8). The progressive hull approach described in the Buehler and Sander articles has significant deficiencies. At the very least, the approach does not apply generally to all types of object shapes. This progressive hull approach cannot handle concave proxies, or objects that have parts close to each other or touching each other. In addition, this approach is not well suited for mixing image-based rendering (ibr) objects with regular scenes. Image based rendering, and thus animation, needs to be improved with a method and system for forming object proxies that can be used with all types of object shapes. Animation at an intermediate level of rendering quality in between full object rendering and a billboard rendering is needed.","Methods and systems for animating proxy surfaces of parts and\/or joints of an object are provided. The present invention overcomes each of the above-identified problems and has additional advantages as described herein. An improved animation solution is provided in which a single, relatively small set of view textures is precomputed and during the rendering these textures are mapped on a proxy surface, a simplified version of the complex object. The animation is achieved by animating the proxy surface itself and selecting the appropriate view textures for each animated part. This provides an efficient approach to real-time rendering of complex objects that are animated. In this way, the present invention leverages the formation of object proxies to provide an intermediate level of rendering quality. This intermediate level of rendering quality is then leveraged to produce realistic animation at real-time interactive rates while avoiding burdensome graphics processing and hardware requirements.","In an embodiment, a method for animating includes preprocessing and rendering stages that operate on parts of an object. The preprocessing stage forms proxy surfaces for parts of the object. Such formation can include dividing an object to be animated into parts. Preferably, the parts can move independently without changing shape in the animation. For each part, a proxy surface is formed at an initial viewing direction, and a set of view textures is obtained. The rendering stage renders the object based on the proxy surfaces. Such rendering can include determining a part transformation with respect to the object, and rendering the proxy surfaces of each part at a new viewing direction. The new viewing direction is determined as a function of an object transformation, the determined part transformation, and an initial selected viewing direction (Ds). In one implementation, a new viewing direction (Dr) is determined as function of an object transformation expressed as a matrix (M), a part transformation expressed as a matrix (M), and an initial selected viewing direction (Ds). The object is then animated by repeating the rendering for different viewing directions.","In another embodiment, a method for animating includes pre-processing and rendering stages that operate on part(s) and\/or joint(s) of an object. The preprocessing stage forms proxy surfaces for parts and joints of the object. Preferably, the parts can move independently without changing shape in the animation. Joints generally connect parts and can change shape during animation. For each part and each joint, a corresponding proxy surface is formed at an initial viewing direction, and a set of view textures is obtained. The rendering stage renders the object based on the proxy surfaces. The rendering stage can include selecting a viewing direction (Ds) and determining an object transformation. Parts are then rendered by determining a part transformation with respect to the object, and rendering the proxy surfaces of each part at a new viewing direction.","Since joints can change shape, individual primitives of a joint are rendered during the rendering stage. For each primitive of joint, rendering steps include determining a joint primitive transformation with respect to the object, and rendering the proxy surfaces of each part at a new viewing direction. The new viewing direction is determined as function of an object transformation, the determined joint primitive transformation, and an initial selected viewing direction (Ds). In one implementation, a new viewing direction (Dr) is determined as function of an object transformation expressed as a matrix (M), a joint primitive transformation expressed as a matrix (M), and an initial selected viewing direction (Ds). The object defined by parts and joints is then animated by repeating the rendering for different viewing directions.","Methods of the present invention can be implemented as control logic in software, firmware, hardware or any combination thereof. These methods can be implemented to run on any computing or processing device having graphics processing capability including, but not limited to, a workstation or personal computer having graphics hardware.","In an embodiment, a system for animating objects includes a preprocessor having a proxy former and a view texture former, and an animator coupled to the preprocessor. The animator has a transformation matrix calculator, a view direction calculator, and an image based renderer. The animator receives object proxies and view textures, from the preprocessor and outputs an animation of the object made up of parts and\/or joints.","Further embodiments, features, and advantages of the present inventions, as well as the structure and operation of the various embodiments of the present invention, are described in detail below with reference to the accompanying drawings.","The present invention leverages two co-pending, commonly-owned applications to disclose a method and system for image based rendering with proxy surface animation. Section I presents terminology used to describe the present invention. Section II summarizes formation of object proxies in accordance with co-pending U.S. application Ser. No. 10\/197,822, filed Jul. 19, 2002. Section III summarizes image based rendering using object proxies in accordance with co-pending U.S. application Ser. No. 10\/197,845, filed Jul. 19, 2002.","Section IV describes image-based rendering with proxy surface animation according to embodiments of the present invention. First, an architecture in which the present invention may be implemented is described. Second, a system in which the present invention may be implemented is described. Next, two embodiments of a method for proxy surface animation are illustrated and described according to the present invention. Finally, a system according to the present invention is described.","I. Terminology","The following terms are defined so that they may be used to describe embodiments of the present invention. As used herein:","\u201cPixel\u201d means a data structure, which is used to represent a picture element. Any type of pixel format can be used.","\u201cReal-time\u201d or \u201cInteractive Rate\u201d refers to a rate at which successive display images can be redrawn without undue delay upon a user or application. This can include, but is not limited to, a nominal rate of between 30-60 frames\/second. In some example embodiments, such as some flight simulators or some interactive computer games, an interactive rate may be approximately 10 frames\/second. In some examples, real-time can be one update per second. These examples are illustrative of real-time rates; in general, smaller or larger rates may be considered \u201creal-time\u201d depending upon a particular use or application.","\u201cTexture\u201d refers to image data or other type of data that can be mapped to an object to provide additional surface detail or other effects. In computer graphics applications, texture is often a data structure including but not limited to an array of texels. A texel can include, but is not limited to, a color value or an intensity value. These texel values are used in rendering to determine a value for a pixel. As used herein, the term \u201ctexture\u201d includes, for example, texture maps, bump maps, and gloss maps.","\u201cTexture sample\u201d refers to a sample selected from a texture map or texture. The sample can represent one texel value or can be formed from two or more texel values blended together. Different weighting factors can be used for each texel blended together to form a texel. The terms \u201ctexel\u201d and \u201ctexture sample\u201d are sometimes used interchangeably.","\u201cTexture unit\u201d refers to graphics hardware, firmware, and\/or software that can be used to obtain a texture sample (e.g., a point sample or a filtered texture sample) from a texture. A texture unit can in some embodiments obtain multiple texture samples from multiple textures.","II. Method and System for Forming an Object Proxy","An object proxy approximates the geometry of an object. A method and system for forming an object proxy is disclosed in co-pending U.S. application Ser. No. 10\/197,822, filed Jul. 19, 2002, which is hereby incorporated by reference in its entirety. The method includes forming a volume that encompasses the object, forming an isosurface within the volume, adjusting the isosurface relative to a surface of the object, and pruning the isosurface to obtain the object proxy. The formed volume is made up of voxels that encompass the object. The isosurface forming step includes determining sets of vertices for the voxels such that the sets of vertices define polygons representing the isosurface at a first distance from the surface of the object.","According to a feature of the method, the isosurface adjusting step includes adjusting at least some of the vertices of the isosurface until each vertex is at or near a second distance from the surface of the object. In an example, each vertex is iteratively advanced by an amount based on a first distance, a second distance, and direction information.","According to a further feature, the isosurface pruning step includes pruning the isosurface based on a distance condition and an intersection condition. In an example, a list of vertices of the isosurface is iteratively pruned. This pruning includes selecting a vertex, and determining whether the selected vertex can be eliminated from the isosurface based on a distance condition and an intersection condition. When a selected vertex can be eliminated, the vertex is eliminated and new edges are created using remaining vertices.","Another feature of the method includes a step of iteratively moving vertices in the initial object proxy closer to the surface of the object. In one example, after pruning to obtain an initial object proxy, the distance of a respective vertex to the object surface is computed. Each vertex of the initial object proxy is advanced toward the object surface until each vertex is on or near the object surface with no polygons deeper than a predetermined distance or until a predetermined number of iterations has been reached.","According to further feature, object proxy formation parameters can be user-specified and\/or predetermined. These object proxy formation parameters include: a grid resolution (N1\u00d7N2\u00d7N3) representing the resolution of voxels in a volume, a first distance value (D1) representing the first distance between an initial isosurface and the object surface, a second distance value (D2) representing the distance between an initial object proxy obtained after pruning and the object surface, a third distance value (D3) representing the maximum allowed distance between a vertice in the isosurface and the object surface during pruning, and a fourth distance value (D4) representing a maximum depth for vertices from the object surface.","The apparatus for forming an object proxy that approximates the geometry of an object includes an isosurface former that forms an isosurface within a volume encompassing an object, and an isosurface shaper that adjusts the isosurface relative to the surface of the object and prunes the isosurface to obtain the object proxy.","Another advantage of the above summarized method is that it allows object formation parameters to be user-specified (or set by default) so that the degree in which the isosurface shape approximates the original object surface can be adjusted depending upon a particular application or need. Forming object proxies according to method summarized above also make it possible both to render more image-based objects in a single computer generated scene and to render these image-based objects using fewer views\/textures than conventional image-based rendering and animation schemes.","Forming an object proxy has a general application in that it can be used to form object proxies for all types of object shapes. Approximating object geometries reduces the processing work required at run-time. For example, the object proxy can have less polygons and associated vertices than the original object. In this way, rendering and animation of the object based on the object proxy can be carried out for display or animation in real-time at an interactive rate even on cheaper graphics hardware. Compared to a general mesh simplification algorithm, object proxies formed according to the method summarized above, and disclosed in U.S. application Ser. No. 10\/197,822, filed Jul. 19, 2002, are assured to have no or minimal interaction with an object, which is especially advantageous in hardware-accelerated image-based rendering and animation.","III. Method and System for Image Based Rendering with Object Proxies","A method for image based rendering is disclosed in co-pending U.S. application Ser. No. 10\/197,845, filed Jul. 19, 2002, and is hereby incorporated by reference. The method includes the steps of: forming a set of view textures corresponding to a set of viewing directions; selecting a viewing direction for rendering; selecting at least two view textures from the formed set based on the selected viewing direction; and rendering the object proxy at the selected viewing direction. The rendering step includes applying texture from the selected view textures onto the selected object proxy. This rendering can include, but is not limited to, image-based rendering. Multiple object proxies can be used. One or more objects can be rendered in a single scene or frame for display or animation, or other applications.","According to one feature, a pre-computed view texture is a texture according to the invention that contains view information for an associated pre-computed object proxy. Preferably, a view texture according to the invention comprises a pre-rendered full view image of an object of interest and pre-rendered extra texture patches associated with partially obstructed or fully obstructed polygons of the object proxy. This view information is used during rendering to draw the object of interest in a scene. A pre-computed object proxy is a simplification of an object boundary or contour.","The view texture set forming step includes: calculating texture coordinates for the object proxy based on the level of obstruction at different portions of the object proxy and texture packing data; and drawing portions of the object based on the level of obstruction data for the object proxy and based on the texture packing data to obtain a view texture at the selected viewing direction.","In one example, this drawing step includes: (i) drawing the portions of the object that correspond to unobstructed front-facing portions of the object proxy with additional displacement based on corresponding texture packing data; (ii) drawing the portions of the object that correspond to at least partially obstructed front-facing portions of the object proxy with additional displacement based on corresponding texture packing data and removal of any intervening object portions; and (iii) drawing the portions of the object that correspond to back-facing portions of the object proxy that are visible from nearby viewing directions with additional displacement based on corresponding texture packing data and removal of intervening object portions, whereby a view texture having texture patches can be obtained.","A view texture set forming step includes: (i) selecting a particular viewing direction from a set of viewing directions; (ii) determining level of obstruction data for each polygon of an object proxy; (iii) storing the level of obstruction data; (iv) for each polygon, determining whether the polygon is visible from the selected particular viewing direction and whether texture should be applied from a neighboring viewing direction; (v) determining texture packing data; and (vi) storing the texture packing data. The view texture set forming step further includes (vii) adjusting texture coordinates of each polygon for the selected particular viewing direction based on said determining step (iv) and said texture packing data.","Any type of texture packing technique and data can be used. For example, the texture packing data can comprise shift and rotation data relative to a viewing direction which is used in a texture packing algorithm to create texture patches.","The method can also include storing the object proxy together with a set of texture coordinates for each viewing direction of the set of viewing directions. Alternatively, the method can include forming groups of viewing directions, and storing a modified object proxy together with a set of texture coordinates for each group of viewing directions.","A system for rendering an object proxy includes a view texture former and a renderer. The view texture former forms a set of view textures corresponding to a set of viewing directions. The renderer renders the object proxy at a viewing direction with texture applied from view textures selected from the set of view textures.","In one example, the view texture former includes an object proxy texture coordinate calculator and a drawer. The object proxy texture coordinate calculator calculates texture coordinates for the object proxy based on the level of obstruction at different portions of the object proxy and based on texture packing data. The drawer draws portions of the object based on the level of obstruction data for the object proxy and based on the texture packing data to obtain a view texture at a respective viewing direction. Further, in one example implementation a drawer is provided that draws the portions of the object that correspond to unobstructed front-facing portions of the object proxy with additional displacement based on corresponding texture packing data, draws the portions of the object that correspond to at least partially obstructed front-facing portions of the object proxy with additional displacement based on corresponding texture packing data and removal of any intervening object portions, and draws the portions of the object that correspond to back-facing portions of the object proxy that are visible from nearby viewing directions with additional displacement based on corresponding texture packing data and removal of intervening object portions, whereby a view texture having texture patches can be obtained.","The method provides level of detail rendering and a method for creating a level of detail rendering node. In an embodiment, a level of detail rendering node is created by combining three node components. A first node component includes an object and at least one texture. This first node component is used to draw an image of the object at a first level of detail having high resolution. Typically, the first node component is used to draw images of the object that are close to a viewer of a scene. A second node component includes an object proxy and a plurality of view textures. The object proxy approximates the geometry of the object. The second node component is used to draw the image of the object at a second level of detail having intermediate resolution. A third node component includes a billboard with several small views. The third node component is used to draw the image of the object at a third level of detail having low resolution. Typically, the third node component is used to draw images of the object that are far from the viewer. During rendering, the first node component, the second node component, or the node component of the level of detail node is selected and used to draw the image of the object based on an input distance. Preferably, the input distance is the distance between the image of the object and the viewer.","Rendering based on the use of object proxies as disclosed in co-pending U.S. application Ser. No. 10\/197,845, filed Jul. 19, 2002, and summarized above, has advantages. First, such rendering with the use of object proxies is faster and less expensive than fully rendering objects based on object geometry data. This is especially true for complex objects with large numbers of polygons. Additionally, rendering with the use of object proxies provides more surface detail and realism than conventional approaches that render to a billboard. In this way, rendering of one or more objects based on their object proxies can be carried out for display or animation in real-time at an interactive rate even on cheaper graphics hardware.","IV. Image Based Rendering with Proxy Surface Animation","The present invention leverages the above summarized methods for (i) forming object proxies and (ii) image based rendering with object proxies in order to achieve realistic animation, rendered in real-time, at increased speed and reduced cost.",{"@attributes":{"id":"p-0054","num":"0053"},"figref":"FIG. 1","b":["100","100"]},"Architecture  includes six overlapping layers. Layer  represents a high level software application program. Layer  represents a three-dimensional (3D) graphics software tool kit, such as OPENGL PERFORMER. Layer  represents a graphics application programming interface (API), which can include but is not limited to OPENGL, available from Silicon Graphics, Incorporated. Layer  represents system support such as operating system and\/or windowing system support. Layer  represents firmware. Finally, layer  represents hardware, including graphics hardware. Hardware  can be any hardware or graphics hardware including, but not limited to, a computer graphics processor (single chip or multiple chip), a specially designed computer, an interactive graphics machine, a gaming platform, a low end game system, a game console, a network architecture, et cetera. Some or all of the layers - of architecture  will be available in most commercially available computers.","As will be apparent to a person skilled in the relevant art after reading the description of the invention herein, various features of the invention can be implemented in any one of the layers - of architecture , or in any combination of layers - of architecture .",{"@attributes":{"id":"p-0057","num":"0056"},"figref":"FIG. 2","b":["200","200","210","220","270","200"]},"Host system  comprises an application program , a hardware interface or graphics API , a processor , and a memory . Application program  can be any program requiring the rendering of a computer image. The computer code of application program  is executed by processor . Application program  assesses the features of graphics subsystem  and display  through hardware interface or graphics API . Memory  stores information used by application program .","Graphics subsystem  comprises a vertex operation module , a rasterizer , a texture memory , and a frame buffer . Texture memory  can store one or more textures or images, such as texture . Texture memory  is connected to a texture unit  by a bus (not shown). Rasterizer  comprises a pixel operation module , a texture unit  and a blending unit . Texture unit  and blending unit  can be implemented separately or together as part of a graphics processor. The operation of these features of graphics system  would be known to a person skilled in the relevant art given the description herein.","In embodiments of the present invention, texture unit  can obtain multiple point samples or multiple filtered texture samples from textures and\/or images stored in texture memory . Blending unit  blends texels and\/or pixel values according to weighting values to produce a single texel or pixel. The output of texture unit  and\/or blending unit  is stored in frame buffer . Display  can be used to display images stored in frame buffer .","The embodiment of the invention shown in  has a multipass graphics pipeline. It is capable of operating on each pixel of an image (object) during each pass that the image makes through the graphics pipeline. For each pixel of the image, during each pass that the image makes through the graphics pipeline, texture unit  can obtain at least one texture sample from the textures and\/or data stored in texture memory . Although the embodiment of the invention shown in  has a multipass graphics pipeline, it is noted here that other embodiments of the invention do not have a multipass graphics pipeline. As described below, method embodiments of the invention can be implemented using systems that do not have a multipass graphics pipeline.",{"@attributes":{"id":"p-0062","num":"0061"},"figref":["FIGS. 3A and 3B","FIG. 3A"],"b":["300","300","3"]},"In an embodiment, preprocessing stage A operates as follows. An object to be animated is divided into parts. Parts can be generally defined as portions of an object of interest that can move independently with respect to the object without changing shape. Next, a proxy surface is formed for each part. For each proxy surface, a set of view textures is obtained. The method then proceeds to the animation stage B. As illustrated in , method  comprises steps ,  and . Each of these steps will now be described in further detail with reference to the flowchart of FIG. A and an example object in .","In step , the object to be animated is divided into parts that can move independently of each other. For example, referring to , an object  representing a person divided into two parts that can move independently, such as the head P, and the remainder of the body P. As shown in , the head P is able to move\u2014e.g., rotate\u2014independently of the remainder of the body P.","In step , a proxy surface is formed for each part P, P. As noted above, an object proxy is a simplification of an object boundary that is useful for hardware-accelerated image-based rendering. Step  leverages the object proxy forming method summarized above in Section II, and disclosed in further detail in co-pending U.S. application Ser. No. 10\/197,822, filed Jul. 19, 2002, which has been incorporated by reference in its entirety. The invention, however, is not limited to this embodiment. Other methods can also be used to form object proxies that can be used in accordance with the present invention.","In step , a set of view textures is obtained for each proxy surface. The formation of view textures is summarized above in Section III above and disclosed in further detail in co-pending U.S. application Ser. No. 10\/197,845, filed Jul. 19, 2002. View textures are formed using a set of viewing directions. A viewing direction is the direction from which the object will appear to be seen by a viewer after it is drawn or rendered. The direction at which a particular view texture is initially calculated (Di) becomes the baseline for the animation stage B, which is discussed in more detail below. Once the view textures have been obtained for each proxy surface, the method for image based rendering with proxy surface animation proceeds to the animation stage B.","As shown in , a viewing direction (Ds) is selected (step ). The particular view direction selected will depend on the circumstances of the desired animation. This is because the animation fundamentally consists of repeatedly rendering the object to be animated from an incrementally changing set of view directions. Thus, any detected change in the desired geometry of an object will result in a new viewing direction. Whether or not a new view is selected thus depends on the current animation application program and whether the object of interest in a scene is in motion.","In step , the transformation of the entire object to be animated (\u201cobject transformation\u201d) is determined with respect to the position at which the view textures were determined. In other words, the entire object at the selected view direction Ds is quantitatively compared to the entire object at the direction (Di) at which the view textures were initially calculated. This comparison is used to determine an appropriate transformation. The transformation can include rotation, translation and\/or scaling, and combined into a transformation matrix (M). M thus represents the degree to which the entire object has moved with respect to its original position. For example, the woman shown in  could be standing on an escalator. While the two parts described above\u2014head (P) and remainder of body (P)\u2014may be static with respect to each other, the entire body may have changed positions within the scene. Matrix M represents this change.","In step , the transformation of each part (\u201cpart transformation\u201d)\u2014the part being represented by a proxy\u2014is determined with respect to the entire object at the selected viewing direction. As with step , the part transformation is done for rotation, translation and\/or scaling, and combined into a part transformation matrix (M). M thus represents the degree to which the individual parts have moved with respect to the object at its original position. Using the preceding example of the woman  of  standing on an escalator, the animation may show her rotating her head as she passes an attractive graphics programmer or patent attorney on the opposite escalator. This would cause her head (P) to move with respect to the rest of the body (P). For P, matrix M would represent this change.","In step , a new viewing direction for rendering (Dr) is calculated. The new rendering viewing direction (Dr) is a function of the selected viewing direction (Ds), the object transformation matrix (M), and the part transformation matrix (M). In one implementation, the new rendering viewing direction (Dr) is equal to the selected viewing direction (Ds) multiplied by an inverse of matrix (M) and an inverse of the matrix (M). In other words: Dr=Ds\u00d7M\u00d7M. Each part is rendered at the new viewing direction Dr. According to step , rendering steps  through  are repeated for animation as long as the object of interest in the scene remains in motion.","The above described embodiment works well for animation of an object whose parts behave independently\u2014such as the turning of a head on a body. Another example might be the wheels on a car, which can also move independently of the car. A more complicated animation scenario arises where there is more dependency between the individual parts of an object and the object itself\u2014for example at a joint. Where an arm is moved up and down, certain portions of the shoulder joint will be displaced at different times and at different rate than other portion of the joint, depending on the particulars of the movement.","Recall from above that parts can be generally defined as portions of an object of interest that can move independently with respect to the object without changing shape. A joint can be generally defined as a portion of the object of interest that connects two or more parts, where the joint changes shape as the parts to which it is connected are displaced. For instance, a joint may be stretched or compressed as the parts to which it is connected are displaced. An embodiment of the invention, described next, provides a method for animation of proxy surfaces that are connected by a joint.",{"@attributes":{"id":"p-0073","num":"0072"},"figref":["FIGS. 5A and 5B","FIG. 5A"],"b":["500","500","5"]},"In an embodiment, preprocessing stage A operates as follows. An object to be animated is divided into parts and joints, as described above. Next, a proxy surface is formed for each part and for each joint. For each proxy surface, a set of view textures is obtained. The method then proceeds to the animation stage B where part and joint transformations are determined, a new viewing direction is calculated, and the proxy surfaces a rendered. As illustrated in , method  comprises steps ,  and . Each of these steps will now be described in further detail with reference to the flowchart of FIG. A and the example illustrated in .","In step , the object to be animated is divided into at least two parts that can move independently with respect to the object without changing shape. This step is similar to step  described above. Next, in step , joints are defined that connect at least two parts of the object. The joints, as described above, are capable of changing shape when the parts connected to it are displaced. Each joint is comprised of a plurality of triangles, the number of triangles depending on the complexity of the joint.","For example, referring to , an object  representing a person divided into two parts that can move independently, such as an arm P and body P. As shown in , the arm P is able to move\u2014e.g., be raised\u2014independently of the remainder of the body P. In addition to the two parts, a joint J is defined. Joint J connects the arm P and the body P. Joint J represents a shoulder joint. It will be appreciated that to achieve realistic animation, the shoulder joint would be stretched or compressed depending on the motion characteristics of the arm as well as the direction from the motion of the arm is observed.","In step , a proxy surface is formed for each part and for each joint. As noted above, an object proxy is a simplification of an object boundary that is useful for hardware-accelerated image-based rendering. Step  leverages the object proxy forming method summarized above in Section II, and disclosed in co-pending U.S. application Ser. No. 10\/197,822, filed Jul. 19, 2002, which has been incorporated by reference in its entirety. The invention, however, is not limited to this embodiment. Other methods can also be used to form object proxies that can be used in accordance with the present invention.","In step , a set of view textures is obtained for each proxy surface. This step is essentially the same as step  above. The formation of view textures is summarized above in Section III above and disclosed in further detail in co-pending U.S. application Ser. No. 10\/197,845, filed Jul. 19, 2002. View textures are formed using a set of viewing directions. As used herein, viewing direction means the direction from which the object will appear to be seen by a viewer after it is drawn or rendered. The direction at which a particular view texture is initially calculated (Di) becomes the baseline for the animation stage B, which is discussed in more detail below. Once the view textures have been obtained for each proxy surface, the method for image based rendering with proxy surface animation proceeds to the animation stage B, illustrated in FIG. B.","Animation stage B differs from animation stage B only in the fact that the transformation of a joint is determined differently. In step , a viewing direction (Ds) is selected. In step , a transformation matrix M is determined. Therefore, steps  and  are similar to steps  and . These steps  and  determine the transformation of the entire object for use in determining the transformation of the individual parts and joints represented in a first transformation matrix M.","According to step , a decision is made as to whether a part or a joint is to be rendered. If a part is to be rendered, then steps ,  and  of  proceed the same way as corresponding steps ,  and  in FIG. B. Steps  thus results in a part transformation represented by a second matrix M, step  calculates a second viewing direction for the part, and step  renders the part using the view textures and the second viewing direction.","However, if a joint is to be rendered, a slightly different procedure is followed. This is because, as described above, a joint is comprised of a plurality of triangles and may be stretched or compressed depending on the motion characteristics of the parts to which the joint connects.  shows an example of a joint J when arm P is outstretched from body P. In , joint J changes its shape compared to joint J shown in FIG. A. For a joint, according to step , the transformation is determined for each of the plurality of triangles of the proxy surface representing the joint with respect to the object. This joint transformation is represented by a third matrix (M). According to step , matrix M is used to calculate a third viewing direction. Finally, according to step , the third viewing direction is used to render the proxy surface representing the joint on a triangle by triangle basis. It will be appreciated that by rendering the joint on a triangle by triangle basis, a more realistic animation can be produced for the joint area.","According to step , animation method B is repeated as long as the object of interest in the scene remains in motion, as determined by the particular application. In sum, the method described in  is similar to the method described in . The primary difference is the identification and treatment of the portions of the object of interest that join two separate parts\u2014joints. Therefore, the embodiment described with respect to , still gains the benefits of proxy surface animation while providing for more realistic animation for complex motion characteristics of joints.",{"@attributes":{"id":"p-0083","num":"0082"},"figref":"FIG. 7","b":["700","300","500","700","718","720","750","720","750","716","716","718","718","705","710","760"]},"Preprocessor  includes a proxy former  and a view texture former . View textures  and proxies  are passed to animator . Animator  includes a transformation matrix calculator , a view direction calculator , and an image based renderer .","The formation of object proxies by proxy former  and the formation of view textures by view texture former  are summarized above and disclosed in disclosed in further detail in co-pending U.S. application Ser. No. 10\/197,822, filed Jul. 19, 2002, which has been incorporated by reference in its entirety. The transformation of part and joint proxies as proxy surface animation proceeds according to the above-described methods is carried out in the transformation matrix calculator . View direction calculator  calculates view directions according to the above-described methods. Vertices that form the part and joint proxies, texture coordinates for each vertex, and blending weights are all outputs of image based rendering as disclosed in detail in co-pending U.S. application Ser. No. 10\/197,845, filed Jul. 19, 2002, which has been incorporated by reference in its entirety.","The present invention has been described above with the aid of functional building blocks and method steps illustrating the performance of specified functions and relationships thereof. The boundaries of these functional building blocks and method steps have been arbitrarily defined herein for the convenience of the description. Alternate boundaries can be defined so long as the specified functions and relationships thereof are appropriately performed. Any such alternate boundaries are thus within the scope and spirit of the claimed invention. One skilled in the art will recognize that these functional building blocks can be implemented by discrete components, application specific integrated circuits, processors executing appropriate software and the like or any combination thereof.","Various embodiments of the present invention have been described above, which are capable of being implemented on an interactive graphics machine. It should be understood that these embodiments have been presented by way of example only, and not limitation. It will be understood by those skilled in the relevant art that various changes in form and details of the embodiments described above may be made without departing from the spirit and scope of the present invention as defined in the claims. Thus, the breadth and scope of the present invention should not be limited by any of the above-described exemplary embodiments, but should be defined only in accordance with the following claims and their equivalents."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE FIGURES","p":["The present invention is described with reference to the accompanying figures. In the figures, like reference numbers indicate identical or functionally similar elements. Additionally, the leftmost digit or digits of a reference number identify the figure in which the reference number first appears. The accompanying figures, which are incorporated herein and form part of the specification, illustrate the present invention and, together with the description, further serve to explain the principles of the invention and to enable a person skilled in the relevant art to make and use the invention.",{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIGS. 3A and 3B"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":["FIGS. 4A and 4B","FIGS. 3A and 3B"]},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIGS. 5A and 5B"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":["FIGS. 6A and 6B","FIGS. 5A and 5B"]},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 7"}]},"DETDESC":[{},{}]}
