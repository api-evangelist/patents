---
title: Spatial noise suppression for a microphone array
abstract: A microphone array having at least three microphones provides a captured signal. Spatial noise suppression estimates a desired signal from a captured signal using spatio-temporal distribution of the speech and the noise. In particular, spatial information indicative of at least two quantities of direction are used. A first quantity is based on a first combination of the signals from the at least three microphones, a second quantity is based on a second combination of the signals of the at least three microphones.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07565288&OS=07565288&RS=07565288
owner: Microsoft Corporation
number: 07565288
owner_city: Redmond
owner_country: US
publication_date: 20051222
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["The discussion below is merely provided for general background information and is not intended to be used as an aid in determining the scope of the claimed subject matter.","Small computing devices such as personal digital assistants (PDA), devices and portable phones are used with ever increasing frequency by people in their day-to-day activities. With the increase in processing power now available for microprocessors used to run these devices, the functionality of these devices is increasing, and in some cases, merging. For instance, many portable phones now can be used to access and browse the Internet as well as can be used to store personal information such as addresses, phone numbers and the like. Likewise, PDAs and other forms of computing devices are being designed to function as a telephone.","In many instances, mobile phones, PDAs and the like are increasingly being used in situations that require hands-free communication, which generally places the microphone assembly in a less than optimal position when in use. For instance, the microphone assembly can be incorporated in the housing of the phone or PDA. However, if the user is operating the device in a hands-free mode, the device is usually spaced significantly away from and not directly in front of the user's mouth. Environment or ambient noise can be significant relative to the user's speech in this less than optimal position. Stated another way, a low signal-to-noise ratio (SNR) is present for the captured speech. In view that mobile devices are commonly used in noisy environments, a low SNR is clearly undesirable.","To address this problem, at least in part, mobile phones and other devices can also be operated using a headset worn by the user. The headset includes a microphone and is connected either by wire or wirelessly to the device. For reasons of comfort, convenience and style, most users prefer headset designs that are compact and lightweight. Typically, these designs require the microphone to be located at some distance from the user's mouth, for example, alongside the user's head. This positioning again is suboptimal, and when compared to a well-placed, close-talking microphone, again yields a significant decrease in the SNR of the captured speech signal when compared to an optimal position.","One way to improve sound capture performance, with or without a headset, is to capture the speech signal using multiple microphones configured as an array. Microphone array processing improves the SNR by spatially filtering the sound field, in essence pointing the array toward the signal of interest, which improves overall directivity. However, noise reduction of the signal after the microphone array is still necessary and has had limited success with current signal processing algorithms.","This Summary and Abstract are provided to introduce some concepts in a simplified form that are further described below in the Detailed Description. This Summary and Abstract are not intended to identify key features or essential features of the claimed subject matter, nor are they intended to be used as an aid in determining the scope of the claimed subject matter. In addition, the description herein provided and the claimed subject matter should not be interpreted as being directed to addressing any of the short-comings discussed in the Background.","A microphone array having at least three microphones provides a captured signal. Spatial noise suppression estimates a desired signal such as clean speech from the captured signal using spatio-temporal distribution of the speech and the noise. In particular, spatial information indicative of two quantities of direction is used. A first quantity is based on a first combination of the signals from the at least three microphones, while a second quantity is based on a second combination of the signals of the at least three microphones. The desired signal is obtained based on stored signal and noise variance models in the multi-dimensional space defined by the first and second quantities.","In one embodiment, the signal and noise variance models are updated so as to adapt to changes in the noise present in the captured signals. A speech activity detector is used to identify frames having speech (or some other desired signal in the captured signal). The signal and noise variance models are updated with respect to the two dimensional space defined by the first and second quantities and based upon the presence of speech in the captured signal. In particular, the signal variance model is updated if speech is present in the captured signal, whereas the noise variance model is updated if speech is not present in the captured signal.","One concept herein described provides spatial noise suppression for a microphone array. Generally, spatial noise reduction is obtained using a suppression rule that exploits the spatio-temporal distribution of noise and speech with respect to multiple dimensions.","However, before describing further aspects, it may be useful to first describe exemplary computing devices or environments that can implement the description provided below.",{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 1","b":["100","100","100","100"]},"In addition to the examples herein provided, other well known computing systems, environments, and\/or configurations may be suitable for use with concepts herein described. Such systems include, but are not limited to, personal computers, server computers, hand-held or laptop devices, multiprocessor systems, microprocessor-based systems, set top boxes, programmable consumer electronics, network PCs, minicomputers, mainframe computers, distributed computing environments that include any of the above systems or devices, and the like.","The concepts herein described may be embodied in the general context of computer-executable instructions, such as program modules, being executed by a computer. Generally, program modules include routines, programs, objects, components, data structures, etc. that perform particular tasks or implement particular abstract data types. Those skilled in the art can implement the description and\/or figures herein as computer-executable instructions, which can be embodied on any form of computer readable media discussed below.","The concepts herein described may also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment, program modules may be located in both locale and remote computer storage media including memory storage devices.","With reference to , an exemplary system includes a general purpose computing device in the form of a computer . Components of computer  may include, but are not limited to, a processing unit , a system memory , and a system bus  that couples various system components including the system memory to the processing unit . The system bus  may be any of several types of bus structures including a memory bus or memory controller, a peripheral bus, and a locale bus using any of a variety of bus architectures. By way of example, and not limitation, such architectures include Industry Standard Architecture (ISA) bus, Micro Channel Architecture (MCA) bus, Enhanced ISA (EISA) bus, Video Electronics Standards Association (VESA) locale bus, and Peripheral Component Interconnect (PCI) bus also known as Mezzanine bus.","Computer  typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by computer  and includes both volatile and nonvolatile media, removable and non-removable media. By way of example, and not limitation, computer readable media may comprise computer storage media and communication media. Computer storage media includes both volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information such as computer readable instructions, data structures, program modules or other data. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by computer . Communication media typically embodies computer readable instructions, data structures, program modules or other data in a modulated data signal such as a carrier WAV or other transport mechanism and includes any information delivery media. The term \u201cmodulated data signal\u201d means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media includes wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, FR, infrared and other wireless media. Combinations of any of the above should also be included within the scope of computer readable media.","The system memory  includes computer storage media in the form of volatile and\/or nonvolatile memory such as read only memory (ROM)  and random access memory (RAM) . A basic input\/output system  (BIOS), containing the basic routines that help to transfer information between elements within computer , such as during start-up, is typically stored in ROM . RAM  typically contains data and\/or program modules that are immediately accessible to and\/or presently being operated on by processing unit . By way o example, and not limitation,  illustrates operating system , application programs , other program modules , and program data .","The computer  may also include other removable\/non-removable volatile\/nonvolatile computer storage media. By way of example only,  illustrates a hard disk drive  that reads from or writes to non-removable, nonvolatile magnetic media, a magnetic disk drive  that reads from or writes to a removable, nonvolatile magnetic disk , and an optical disk drive  that reads from or writes to a removable, nonvolatile optical disk  such as a CD ROM or other optical media. Other removable\/non-removable, volatile\/nonvolatile computer storage media that can be used in the exemplary operating environment include, but are not limited to, magnetic tape cassettes, flash memory cards, digital versatile disks, digital video tape, solid state RAM, solid state ROM, and the like. The hard disk drive  is typically connected to the system bus  through a non-removable memory interface such as interface , and magnetic disk drive  and optical disk drive  are typically connected to the system bus  by a removable memory interface, such as interface .","The drives and their associated computer storage media discussed above and illustrated in , provide storage of computer readable instructions, data structures, program modules and other data for the computer . In , for example, hard disk drive  is illustrated as storing operating system , application programs , other program modules , and program data . Note that these components can either be the same as or different from operating system , application programs , other program modules , and program data . Operating system , application programs , other program modules , and program data  are given different numbers here to illustrate that, at a minimum, they are different copies.","A user may enter commands and information into the computer  through input devices such as a keyboard , a microphone (herein an array) , and a pointing device , such as a mouse, trackball or touch pad. Other input devices (not shown) may include a joystick, game pad, satellite dish, scanner, or the like. These and other input devices are often connected to the processing unit  through a user input interface  that is coupled to the system bus, but may be connected by other interface and bus structures, such as a parallel port, game port or a universal serial bus (USB). A monitor  or other type of display device is also connected to the system bus  via an interface, such as a video interface . In addition to the monitor, computers may also include other peripheral output devices such as speakers  and printer , which may be connected through an output peripheral interface .","The computer  may operate in a networked environment using logical connections to one or more remote computers, such as a remote computer . The remote computer  may be a personal computer, a hand-held device, a server, a router, a network PC, a peer device or other common network node, and typically includes many or all of the elements described above relative to the computer . The logical connections depicted in  include a locale area network (LAN)  and a wide area network (WAN) , but may also include other networks. Such networking environments are commonplace in offices, enterprise-wide computer networks, intranets and the Internet.","When used in a LAN networking environment, the computer  is connected to the LAN  through a network interface or adapter . When used in a WAN networking environment, the computer  typically includes a modem  or other means for establishing communications over the WAN , such as the Internet. The modem , which may be internal or external, may be connected to the system bus  via the user-input interface , or other appropriate mechanism. In a networked environment, program modules depicted relative to the computer , or portions thereof, may be stored in the remote memory storage device. By way of example, and not limitation,  illustrates remote application programs  as residing on remote computer . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.","It should be noted that the concepts herein described can be carried out on a computer system such as that described with respect to . However, other suitable systems include a server, a computer devoted to message handling, or on a distributed system in which different portions of the concepts are carried out on different parts of the distributed computing system.",{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 2","b":["200","200","202","204","206","208","210"]},"Memory  is implemented as non-volatile electronic memory such as random access memory (RAM) with a battery back-up module (not shown) such that information stored in memory  is not lost when the general power to mobile device  is shut down. A portion of memory  is preferably allocated as addressable memory for program execution, while another portion of memory  is preferably used for storage, such as to simulate storage on a disk drive.","Memory  includes an operating system , application programs  as well as an object store . During operation, operating system  is preferably executed by processor  from memory . Operating system  is designed for mobile devices, and implements database features that can be utilized by applications  through a set of exposed application programming interfaces and methods. The objects in object store  are maintained by applications  and operating system , at least partially in response to calls to the exposed application programming interfaces and methods.","Communication interface  represents numerous devices and technologies that allow mobile device  to send and receive information. The devices include wired and wireless modems, satellite receivers and broadcast tuners to name a few. Mobile device  can also be directly connected to a computer to exchange data therewith. In such cases, communication interface  can be an infrared transceiver or a serial or parallel communication connection, all of which are capable of transmitting streaming information.","Input\/output components  include a variety of input devices such as a touch-sensitive screen, buttons, rollers, as well as a variety of output devices including an audio generator, a vibrating device, and a display. The devices listed above are by way of example and need not all be present on mobile device .","However, in particular, device  includes an array microphone assembly , and in one embodiment, an optional analog-to-digital (A\/D) converter , noise reduction modules described below and an optional recognition program stored in memory . By way of example, in response to audible information, instructions or commands from a user of device  generated speech signals are digitized by A\/D converter . Noise reduction modules process the digitized speech signals to obtain an estimate of clean speech. A speech recognition program executed on device  or remotely can perform normalization and\/or feature extraction functions on the clean speech signals to obtain intermediate speech recognition results. Using communication interface , speech data can be transmitted to a remote recognition server, not shown, wherein the results of which are provided back to device . Alternatively, recognition can be performed on device . Computer  processes speech input from microphone array  in a similar manner to that described above.",{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 3","b":["300","302","163","232","304","306","308","310","310","312"]},"At this point it should be noted, that in one embodiment, the modules  (modules , ,  and ) can operate as a computer process entirely within a microphone array computing device, with the microphone array  receiving raw audio inputs from its various microphones, and then providing a processed audio output at . In this embodiment, the microphone array computing device includes an integral computer processor and support modules (similar to the computing elements of ), which provides for the processing techniques described herein. However, microphone arrays with integral computer processing capabilities tend to be significantly more expensive than would be the case if all or some of the computer processing capabilities could be external to the microphone array . Therefore in another embodiment, the microphone array  only includes microphones, preamplifiers, A\/D converters, and some means of connectivity to an external computing device, such as, for example, the computing devices described above. In yet another embodiment, only some of the modules  form part of the microphone array computing device.","When the microphone array  contains only some of the modules  or simply contains sufficient components to receive audio signals from the plurality of microphones forming the array and provide those signals to an external computing device which then performs the remaining processes, device drivers or device description files can be used. Device drivers or device description files contain data defining the operational characteristics of the microphone array, such as gain, sensitivity, array geometry, etc., and can be separately provided for the microphone array , so that the modules residing within the external computing device can be adjusted automatically for that specific microphone array.","In one embodiment, beamformer module  employs a time-invariant or fixed beamformer approach. In this manner, the desired beam is designed off-line, incorporated in beamformer module  and used to process signals in real time. However, although this time-invariant beamformer will be discussed below, it should be understood that this is but one exemplary embodiment and that other beamformer approaches can be used. In particular, the type of beamformer herein described should not be used to limit the scope or applicability of the spatial noise reduction module  described below.","Generally, the microphone array  can be considered as having M microphones with known positions. The microphones or sensors sample the sound field at locations p=(x,y,z) where m={1, . . . , M} is the microphone index. Each of the m sensors has a known directivity pattern U(\u0192,c), where f is the frequency band index and c represents the location of the sound source in either a radial or a rectangular coordinate system. The microphone directivity pattern is a complex function, providing the spatio-temporal transfer function of the channel. For an ideal omni-directional microphone, U(\u0192,c) is constant for all frequencies and source locations. A microphone array can have microphones of different types, so U(\u0192,c) can vary as a function of m.","As is known to those skilled in the art, a sound signal originating at a particular location, c, relative to a microphone array is affected by a number of factors. For example, given a sound signal, S(f), originating at point c, the signal actually captured by each microphone can be defined by Equation (1), as illustrated below:\n\n(\u0192,)=(\u0192,)(\u0192)(\u0192,)(\u0192)\u2003\u2003Eq. 1\n\nwhere D(\u0192,c) represents the delay and the decay due to the distance between the source and the microphone. This is expressed as\n",{"@attributes":{"id":"p-0044","num":"0043"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["D","m"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","c"],"mo":","}}},{"mrow":{"msub":{"mi":["F","m"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","c"],"mo":","}}},"mo":"\u2062","mfrac":{"msup":{"mi":"\u2147","mrow":{"mrow":[{"mo":"-","mi":"j2\u03c0"},{"mo":["\uf605","\uf606"],"mrow":{"mi":"c","mo":"-","msub":{"mi":["p","m"]}}}],"mo":["\u2062","\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"fv"}},"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"c","mo":"-","msub":{"mi":["p","m"]}}}}}],"mo":"="}},{"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"2"}}]}}}},"br":{},"sub":["m","m","m"]},"The exemplary beamformer design described herein operates in a digital domain rather than directly on the analog signals received directly by the microphone array. Therefore, any audio signals captured by the microphone array are first digitized using conventional A\/D conversion techniques. To avoid unnecessary aliasing effects, the audio signal is processed into frames longer than two times the period of the lowest frequency in a modulated complex lapped transform (MCLT) work band.","The beamformer herein described uses the modulated complex lapped transform (MCLT) in the beam design because of the advantages of the MCLT for integration with other audio processing components, such as audio compression modules. However, the techniques described herein are easily adaptable for use with other frequency-domain decompositions, such as the FFT or FFT-based filter banks, for example.","Assuming that the audio signal is processed in frames longer than twice the period of the lowest frequency in the frequency band of interest, the signals from all sensors are combined using a filter-and-sum beamformer as:",{"@attributes":{"id":"p-0048","num":"0047"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"Y","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}},{"munderover":{"mo":"\u2211","mrow":{"mi":"m","mo":"=","mn":"1"},"mi":"M"},"mo":"\u2062","mrow":{"mrow":[{"msub":{"mi":["W","m"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}},{"msub":{"mi":["X","m"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}}],"mo":"\u2062"}}],"mo":"="}},{"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"3"}}]}}}},"br":{},"sub":["m","m"],"figref":"FIG. 4"},"The matrix W is computed using the known methodology described by I. Tashev, H. Malvar, in \u201cA New Beamformer Design Algorithm for Microphone Arrays,\u201d published by ICASSP 2005, Philadelphia, Mar. 2005, or U.S. Patent Application US 2005\/0195988, published Sept. 8, 2005. In order to do so, the filter F(\u0192,c) in Eq. (2) must be determined. Its value can be estimated theoretically using a physical model, or measured directly by using a close-talking microphone as reference.","However, it should be noted again the beamformer herein described is but an exemplary type, wherein other types can be employed.","In any beamformer design, there is a tradeoff between ambient noise reduction and the instrumental noise gain. In one embodiment, more significant ambient noise reduction was utilized at the expense of increased instrumental noise gain. However, this additional noise is stationary and it can easily be removed using stationary noise suppression module . Besides removing the stationary part of the ambient noise remaining after the time-invariant beamformer, the stationary noise suppression module  reduces the instrumental noise from the microphones and preamplifiers.","Stationary noise suppression modules are known to those skilled in the art. In one embodiment, stationary noise suppression module  can use a gain-based noise suppression algorithm with MMSE power estimation and a suppression rule similar to that described by P. J. Wolfe and S. J. Godsill, in \u201cSimple alternatives to the Ephraim and Malah suppression rule for speech enhancement,\u201d published in the Proceedings of the IEEE Workshop on Statistical Signal Processing, pages 496-499, 2001. However, it should be understood that this is but one exemplary embodiment and that other stationary noise suppression modules can be used. In particular, the type of stationary noise suppression module herein described should not be used to limit the scope or applicability of the spatial noise reduction module  described below.","The output of the stationary noise suppression module  is then processed by spatial noise suppression module . Operation of module  can be explained as follows. For each frequency bin f the stationary noise suppressor output Y(\u0192)R(\u0192).exp(j\u03b8(\u0192)) consists of signal S(\u0192)A(\u0192).exp(j\u03b1(\u0192)) and noise D(\u0192). If it is assumed that they are uncorrelated, then Y(\u0192)S(\u0192)+D(\u0192).","Given an array of microphones, the instantaneous direction-of-arrival (IDOA) information for a particular frequency bin can be found based on the phase differences of non-repetitive pairs of input signals. In particular, for M microphones (where M equals at least three) these phase differences form an M\u22121 dimensional space, spanning all potential IDOA. In one embodiment as illustrated in , the microphone array  consists of three microphones (M=3), in which case two phase differences quantities \u03b4(\u0192) (between microphones  and ) and \u03b4(\u0192) (between microphones  and ) exist, thereby forming a two-dimensional space. In this space each physical point from the real space has a corresponding point. However, the opposite is not correct, i.e. there are points in this two-dimensional space without corresponding points in the real space.","As appreciated by those skilled in the art, the technique described herein can be extended to more than three microphones. Generally, if an IDOA vector is defined in this space as",{"@attributes":{"id":"p-0056","num":"0055"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"mrow":[{"mi":"\u0394","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mo":["(",")"],"mi":"f"}},{"mo":["[","]"],"mrow":{"mrow":[{"msub":{"mi":"\u03b4","mn":"1"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}},{"msub":{"mi":"\u03b4","mn":"2"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}},{"msub":{"mi":"\u03b4","mrow":{"mi":"M","mo":"-","mn":"1"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}}],"mo":[",",",","\u2062",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}}],"mo":["\u2062","\u2062"],"mover":{"mo":"=","mi":"\u0394"}},"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}},"mi":"where"}},{"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"4"}}]},{"mtd":[{"mrow":{"mrow":[{"mrow":[{"msub":{"mi":"\u03b4","mrow":{"mi":"j","mo":"-","mn":"1"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}},{"mrow":[{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},"mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"X","mn":"1"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}}}},{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},"mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["X","j"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}}}}],"mo":"-"}],"mo":"="},{"mi":"j","mo":"=","mrow":{"mo":["{","}"],"mrow":{"mn":"2","mo":[",","\u2062",","],"mi":["\u2026","M"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}}}],"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}}}},{"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"5"}}]}]}}},"br":{}},{"@attributes":{"id":"p-0057","num":"0056"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":[{"msub":{"mi":["\u03bb","Y"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","\u0394"],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}}},{"mi":"E","mo":"\u2061","mrow":{"mo":["[","]"],"msup":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mi":"Y","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","\u0394"],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}}}},"mn":"2"}}}],"mo":["\u2062","\u2062"],"mover":{"mo":"=","mi":"\u0394"}},{"mrow":[{"msub":{"mi":["\u03bb","D"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","\u0394"],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}}},{"mi":"E","mo":"\u2061","mrow":{"mo":["[","]"],"msup":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mi":"D","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","\u0394"],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}}}},"mn":"2"}}}],"mo":["\u2062","\u2062"],"mover":{"mo":"=","mi":"\u0394"}}],"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}}}},{"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"6"}}]}}}},"br":{}},{"@attributes":{"id":"p-0058","num":"0057"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"mi":"\u03be","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","\u0394"],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}}},"mo":["\u2062","\u2062"],"mover":{"mo":"=","mi":"\u0394"},"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":{"mi":"\u03b2","mo":"\u2062","mfrac":{"mrow":[{"mrow":[{"msub":{"mi":["\u03bb","Y"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","\u0394"],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}}},{"msub":{"mi":["\u03bb","D"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","\u0394"],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}}}],"mo":"-"},{"msub":{"mi":["\u03bb","D"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","\u0394"],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}}}]}},"mo":"+"}}},{"mtd":{"mrow":{"mrow":[{"mrow":[{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","mi":"\u03b2"}},{"mi":"max","mo":["\u2062","[","]"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mn":"0","mo":",","mrow":{"mi":"\u03b3","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","\u0394"],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"\u2758"}}}}}}],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mi":"\u03b2","mo":"\u2208","mrow":{"mo":["[",")"],"mrow":{"mn":["0","1"],"mo":","}}}],"mo":","}}}]}}},{"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"7"}}]},{"mtd":[{"mrow":{"mrow":{"mi":"\u03b3","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","\u0394"],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}}},"mo":["\u2062","\u2062"],"mover":{"mo":"=","mi":"\u0394"},"mfrac":{"mrow":[{"mo":"\uf603","msup":{"mrow":{"mi":"Y","mo":["(","\uf604"],"mrow":{"mi":["f","\u0394"],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}},"mn":"2"}},{"msub":{"mi":["\u03bb","D"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","\u0394"],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}}}]}}},{"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"8"}}]}]}}},"br":{}},{"@attributes":{"id":"p-0059","num":"0058"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"H","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","\u0394"],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}}},"mo":"=","msqrt":{"mrow":{"mfrac":{"mrow":[{"mi":"\u03be","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","\u0394"],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}}},{"mn":"1","mo":"+","mrow":{"mi":"\u03be","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","\u0394"],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}}}}]},"mo":"\u2062","mrow":{"mo":["(",")"],"mfrac":{"mrow":[{"mn":"1","mo":"+","mrow":{"mi":"\u03d1","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","\u0394"],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}}}},{"mi":"\u03b3","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","\u0394"],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}}}]}}}}}},{"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"9"}}]}}}},"br":{}},{"@attributes":{"id":"p-0060","num":"0059"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"\u03d1","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","\u0394"],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}}},{"mfrac":{"mrow":[{"mi":"\u03be","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","\u0394"],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}}},{"mn":"1","mo":"+","mrow":{"mi":"\u03be","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","\u0394"],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}}}}]},"mo":"\u2062","mrow":{"mrow":{"mi":"\u03b3","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","\u0394"],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}}},"mo":"."}}],"mo":["\u2062","\u2062"],"mover":{"mo":"=","mi":"\u0394"}}},{"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"10"}}]}}}},"br":[{},{},{}],"sub":["1","M"],"in-line-formulae":[{},{}],"i":["A","H","Y"]},{"@attributes":{"id":"p-0061","num":"0060"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"S","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}},{"mrow":[{"mi":"A","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}},{"mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"j\u03b8","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}}}},"mo":"."}],"mo":"\u00b7"}],"mo":["\u2062","\u2062"],"mover":{"mo":"=","mi":"\u0394"}}}}},"Note that this is a gain-based estimator and accordingly the phase of the beamformer output signal is directly applied.","Method  provided in  illustrates steps for updating the noise and input signal variance models \u03bband \u03bbof spatial noise reduction module , which will be described with respect to a microphone array having three microphones. Method  is performed for each frame of audio signal. At step , \u03b4(\u0192) (phase difference between of non-repetitive input signals of microphones  and ) and \u03b4(\u0192) (phase difference between of non-repetitive input signals of microphones  and ) are computed (herein obtained from IDOA estimator module ).","At step , a determination is made as to whether the frame has a desired signal relative to noise therein. In the embodiment described, the desired signal is speech activity from the user, for example, whether the user of the headset having the microphone array is speaking. (However, in another embodiment, the desired signal could take any number of forms.)","At step , in the exemplary embodiment herein described, each audio frame is classified as having speech from the user therein or just having noise. In , a speech activity detector is illustrated at  and can comprise a physical sensor such as a sensor that detects the presence of vibrations in the bones of the user, which are present when the user speaks, but not significantly present when only noise is present. In another embodiment, the speech activity detector  can comprise another module of modules . For instance, the speech activity detector  may determine that speech activity exists when energy above a selected threshold is present. As appreciated by those skilled in the art, numerous types of modules and\/or sensors can be used to perform the function of detecting the presence of the desired signal.","At step , based on whether the user is speaking during a given frame, the signal or noise spatial variance \u03bband \u03bbas provided by Eq. 6 is calculated for each frequency bin and used in the corresponding signal or noise model at the dimensional space computed at step .","In practical realizations of the proposed spatial noise reduction algorithm implemented by module , the (M\u22121)-dimensional space of the phase differences is mathematically discrete or discretized. Empirically, it has been found that using 10 bins to cover the range [\u2212\u03c0, +\u03c0] provided adequate precision and results in a resolution of the differences in the phases of 36\u00b0. This converts \u03bband \u03bbto square matrices for each frequency bin. In addition to updating the current cell in \u03bband \u03bb, the averaging operator \u0395[ ]can perform \u201caging\u201d of the values in the other matrix cells.","In one embodiment, to increase the adaptation speed of the spatial noise suppressor, the signal and noise variance matrices \u03bband \u03bbare computed for a limited number of equally spaced frequency subbands. The values for the remaining frequency bins can then be computed using a linear interpolation or nearest neighbor technique. Also in another embodiment, the computed value for a frequency bin can be duplicated or used for other frequencies having the same dimensional space position. In this manner, the signal and noise variance matrices \u03bband \u03bbcan adapt quicker, for example, for moving noise.","By way of example, the variance matrices for the subband around 1000 Hz are shown in . Note that the vertical axis is different in each plot. These variances were measured under 75 dB SPL ambient cocktail-party noise.  clearly show that the signal from the speaker is concentrated in certain area\u2014direction 0\u00b0. The uncorrelated instrumental noise is spread evenly in the whole angular space, while the correlated ambient noise is concentrated around the DOA trace 0\u2212\u03c0\/2\u2212\u03c0. Due to the beamformer, the variance decreases as it goes farther from the focus point at 0\u00b0.","Method  in  illustrates the steps for estimating the clean speech signal based on the signal and noise variances described above, which can include the adaptation described with respect to . At step , an estimation of clean speech is obtained based on the a priori spatial SNR \u03be(\u0192|\u0394) and the a posteriori spatial SNR \u03b3(\u0192,\u0394). Commonly, this would include using appropriate code that embodies Equations 7-11. However, for purposes of understanding this can be obtained by explicitly computing the a priori spatial SNR \u03be(\u0192|\u0394) and the a posteriori spatial SNR \u03b3(\u0192,\u0394). based on Eq. 7 and 8 at step , and using equations 9-11, to obtain an estimation of the clean speech signal therefrom.","Although the subject matter has been described in language directed to specific environments, structural features and\/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not limited to the environments, specific features or acts described above as has been held by the courts. Rather, the environments, specific features and acts described above are disclosed as example forms of implementing the claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIGS. 6A and 6B"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 7"}]},"DETDESC":[{},{}]}
