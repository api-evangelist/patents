---
title: Device and method for generating a representation of a subject's attention level
abstract: A device and method for generating a representation of a subject's attention level. The device measures brain signals from the subject; extracts temporal features from the brain signals; classifies the extracted temporal features using a classifier to give a score X; extracts spectral-spatial features from the brain signals; selects spectral-spatial features containing discriminative information between concentration and non-concentration states from the set of extracted spectral-spatial features; classifies the selected spectral-spatial features using a classifier to give a score X; combines the scores Xand Xto give a single score; and presents the score to the subject.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09149719&OS=09149719&RS=09149719
owner: AGENCY FOR SCIENCE, TECHNOLOGY AND RESEARCH
number: 09149719
owner_city: Singapore
owner_country: SG
publication_date: 20090914
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD OF INVENTION","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["The present invention relates broadly to a device and method for generating a representation of a subject's attention level, and to a computer storage medium having stored thereon computer code means for instructing a computing device to execute a method of generating a representation of a subject's attention level.","Attention Deficit Hyperactivity Disorder (ADHD) is a common behavioural disorder in children, characterised by symptoms such as inattention and hyperactivity. Today's management of ADHD often leads to the use of pharmacological stimulant medication. Parents, however, may be concerned about potential unknown side effects of these medications which include headache, stomach pain, sleeplessness, poor appetite, physical growth retardation, etc.","Recently, there has been a growing interest in treatment of ADHD based on psychosocial aspects. Although important, such treatment has, to date, been shown to be less superior compared to pharmacological treatments.","At the same time, it has been noted that advanced technologies such as brain-computer interface (BCI) can be used to improve the treatment of ADHD based on psychological aspects. BCI provides a direct communication pathway between a human brain and an external device. It relies on bio-signals such as electroencephalogram (EEG) and thus is a low cost and non-invasive interface. Various studies have demonstrated the efficacy of neurofeedback (NFB) or EEG biofeedback in the treatment of ADHD.","U.S. Pat. No. 6,402,520 describes regulating theta and beta wave activity (more specifically, to decrease theta wave activity and increase beta wave activity) as measured based on respective average millivolt activity. However, a clear and direct correlation between the measured average millivolt theta and beta activities and attention has not been established. Thus, the users involved in the training approaches in that document only learn to control their measured average millivolt theta and beta activities, which are not a direct measure of attention. As a result, while the users may develop a mechanism for controlling the measured average millivolt theta and beta activities, this does not directly correlate with achieving higher attention levels.","A need therefore exists to provide device and method for generating a representation of a subject's attention level that seek to address at least one of the above problems.","In accordance with a first aspect of the present invention, there is provided a device for generating a representation of a subject's attention level comprising:","means for measuring brain signals from the subject;","means for extracting temporal features from the brain signals;","means for classifying the extracted temporal features using a classifier to give a score x;","means for extracting spectral-spatial features from the brain signals;","means for selecting spectral-spatial features containing discriminative information between concentration and non-concentration states from the set of extracted spectral-spatial features;","means for classifying the selected spectral-spatial features using a classifier to give a score x;","means for combining the scores xand xto give a single score; and","means for presenting said score to the subject.","The means for presenting may present said score in the form of a game.","The means for presenting said score may adaptively adjust at least one control parameter of the game based on said score.","The adjusting of said one parameter may comprise:","presenting said game using a relationship between said score and said one control parameter over a first period of time;","determining a representative value for the score of the subject over the first period of time;","adjusting the relationship based on said representative value; and","presenting said game using the modified relationship between said score and said one control parameter over a second period of time.","The adjusting of the relationship may be such that a level of difficulty of the game in the second period of time is proportional to the representative value over the first period.","The extracting of the temporal features from the brain signals may comprise:","computing statistics of brain waveforms in each of a plurality of electrode channels; and","concatenating the statistics into a joint feature vector.","The statistics of the brain waveforms may be standard deviations.","The extracting of the spectral-spatial features of the brain signals may comprise:","extracting respective brain signal components in discrete frequency windows using filter banks to obtain spectral features of brain signals; and","applying a CSP algorithm to each of the spectral features using a CSP array to obtain the spectral-spatial features of brain signals.","The selecting of the spectral-spatial features containing discriminative information between concentration and non-concentration states from the set of extracted spectral-spatial features may comprise selecting spectral-spatial features based on the mutual dependence of the features with respect to the concentration and non-concentration states.","The combining of the scores xand xto give a single score may comprise:","normalizing the scores xand xaccording to an equation (x\u2212m)\/s, wherein mand sare the mean and standard deviation of outputs from the classifiers using training samples to give xand xrespectively;","assigning weights wand wto normalized scores xand xrespectively; and","combining the scores xand xaccording to an equation x*w+x*wto give a single score.","The weights wand wmay be calculated according to the equation w=(y)where yis a classification accuracy in classifying the extracted temporal features if i=1 and in classifying the extracted spectral-spatial features if i=2 and p (p>0) controls the power of win the calculation of the single score.","The classifier may comprise one or more of a group consisting of a Linear Discriminant Analysis classifier, Neural Networks, Support Vector Machines, Fuzzy Inference System, Tree-based classifiers, Fuzzy Type 2 and Relevance Vector Machine.","The device may use training data to generate parameters for classifying the extracted temporal features using a classifier, for extracting spectral-spatial features from brain signals, for selecting spectral-spatial features containing discriminative information between the concentration and non-concentration states from the set of extracted spectral-spatial features and for classifying the selected spectral-spatial features using a classifier.","The parameters may comprise one or more of a group consisting of projection matrices of common spatial patterns (CSPs) for the CSP algorithm, parameters for selecting spectral-spatial features based on mutual information and a model for the classifiers.","The use of training data to generate parameters may comprise:","collecting training data from subjects performing a set of tasks; and","determining said parameters via machine learning methods.","In accordance with a second aspect of the present invention, there is provided a method for generating a representation of a subject's attention level, the method comprising the steps of:","measuring brain signals from the subject;","extracting temporal features from the brain signals;","classifying the extracted temporal features using a classifier to give a score x;","extracting spectral-spatial features from the brain signals;","selecting spectral-spatial features containing discriminative information between concentration and non-concentration states from the set of extracted spectral-spatial features;","classifying the selected spectral-spatial features using a classifier to give a score x;","combining the scores xand xto give a single score; and","presenting said score to the subject.","The presenting of said score to the subject may comprise presenting said score in the form of a game.","The presenting of said score to the subject may comprise adaptively adjusting at least one control parameter of the game based on said score.","The adjusting of said one parameter may comprise:","presenting said game using a relationship between said score and said one control parameter over a first period of time;","determining a representative value for the score of the subject over the first period of time;","adjusting the relationship based on said representative value; and","presenting said game using the modified relationship between said score and said one control parameter over a second period of time.","The adjusting of the relationship may be such that a level of difficulty of the game in the second period of time is proportional to the representative value over the first period.","The extracting of the temporal features from the brain signals may comprise:","computing statistics of brain waveforms in each of a plurality of electrode channels; and","concatenating the statistics into a joint feature vector.","The statistics of the brain waveforms may be standard deviations.","The extracting of the spectral-spatial features of the brain signals may comprise:","extracting respective brain signal components in discrete frequency windows using filter banks to obtain spectral features of brain signals; and","applying a CSP algorithm to each of the spectral features using a CSP array to obtain the spectral-spatial features of brain signals.","The selecting of the spectral-spatial features containing discriminative information between concentration and non-concentration states from the set of extracted spectral-spatial features may comprise selecting spectral-spatial features based on the mutual dependence of the features with respect to the concentration and non-concentration states.","The combining of the scores xand xto give a single score may comprise:","normalizing the scores xand xaccording to an equation (x\u2212m)\/s, wherein mand sare the mean and standard deviation) of outputs from the classifiers using training samples to give xand xrespectively;","assigning weights wand wto normalized scores xand xrespectively; and","combining the scores xand xaccording to an equation x*w+x*wto give a single score.","The weights wand wmay be calculated according to the equation w=(y)where yis a classification accuracy in classifying the extracted temporal features if i=1 and in classifying the extracted spectral-spatial features if i=2 and p (p>0) controls the power of win the calculation of the single score.","The classifier may comprise one or more of a group consisting of a Linear Discriminant Analysis classifier, Neural Networks, Support Vector Machines, Fuzzy Inference System, Tree-based classifiers, Fuzzy Type 2 and Relevance Vector Machine.","The method may further comprise using training data to generate parameters for classifying the extracted temporal features using a classifier, for extracting spectral-spatial features from brain signals, for selecting spectral-spatial features containing discriminative information between the concentration and non-concentration states from the set of extracted spectral-spatial features and for classifying the selected spectral-spatial features using a classifier.","The parameters may comprise one or more of a group consisting of projection matrices of CSPs for the CSP algorithm, parameters for selecting spectral-spatial features based on mutual information and a model for the classifiers.","The use of training data to generate parameters may comprise:","collecting training data from subjects performing a set of tasks; and","determining said parameters via machine learning methods.","In accordance with a third aspect of the present invention, there is provided a computer storage medium having stored thereon computer code means for instructing a computing device to execute a method of generating a representation of a subject's attention level, the method comprising the steps of:","measuring brain signals from the subject;","extracting temporal features from the brain signals;","classifying the extracted temporal features using a classifier to give a score x;","extracting spectral-spatial features from the brain signals;","selecting spectral-spatial features containing discriminative information between concentration and non-concentration states from the set of extracted spectral-spatial features;","classifying the selected spectral-spatial features using a classifier to give a score x;","combining the scores xand xto give a single score; and","presenting said score to the subject.","Example embodiments of the present invention provide a BCI-based system that seeks to improve ADHD treatment, e.g. improving attention of a user. The BCI technology of the example embodiments has been implemented in the form of computer games. Players can control, using their degree\/level of attention and the BCI setup, various parameters of games, e.g. fish jump, racing car with speed, puzzle parts, etc.",{"@attributes":{"id":"p-0099","num":"0098"},"figref":"FIG. 1","b":["100","100","102","102","102","106","104","106","112","102"],"i":["a","b ","n "]},"In the example embodiment, the client  initiates the start of a session, e.g. by requesting the server  to provide a game. The client  then downloads the game from the server . A game program  provided by the server  to the client  comprises, inter alia, algorithm  used for the EEG processing, analysis and classification, attention model  and the game  embedded therewithin. Various types of game can be developed for use in the example embodiment. While playing the game the client  can save and send information (i.e. data ) back to the server  to update the respective player's profile stored on the player manager . In addition, the device  of the example embodiment is advantageously capable of multi-user operation. Furthermore, while playing the game, the user (i.e. player) can be shown his\/her level of attention, e.g. as visual feedback. This can advantageously help the user perform better.","Some portions of the description which follows are explicitly or implicitly presented in terms of algorithms and functional or symbolic representations of operations on data within a computer memory. These algorithmic descriptions and functional or symbolic representations are the means used by those skilled in the data processing arts to convey most effectively the substance of their work to others skilled in the art. An algorithm is here, and generally, conceived to be a self-consistent sequence of steps leading to a desired result. The steps are those requiring physical manipulations of physical quantities, such as electrical, magnetic or optical signals capable of being stored, transferred, combined, compared, and otherwise manipulated.","Unless specifically, stated otherwise, and as apparent from the following, it will be appreciated that throughout the present specification, discussions utilizing terms such as \u201cscanning\u201d, \u201ccalculating\u201d, \u201cdetermining\u201d, \u201creplacing\u201d, \u201cgenerating\u201d, \u201cinitializing\u201d, \u201coutputting\u201d, \u201cconcatenating\u201d, \u201cextracting\u201d, \u201cclassifying\u201d, \u201cadjusting\u201d or the like, refer to the action and processes of a computer system, or similar electronic device, that manipulates and transforms data represented as physical quantities within the computer system into other data similarly represented as physical quantities within the computer system or other information storage, transmission or display devices.","The present specification also discloses apparatus for performing the operations of the methods. Such apparatus may be specially constructed for the required purposes, or may comprise a general purpose computer or other device selectively activated or reconfigured by a computer program stored in the computer. The algorithms and displays presented herein are not inherently related to any particular computer or other apparatus. Various general purpose machines may be used with programs in accordance with the teachings herein. Alternatively, the construction of more specialized apparatus to perform the required method steps may be appropriate. The structure of a conventional general purpose computer will appear from the description below.","In addition, the present specification also implicitly discloses a computer program, in that it would be apparent to the person skilled in the art that the individual steps of the method described herein may be put into effect by computer code. The computer program is not intended to be limited to any particular programming language and implementation thereof. It will be appreciated that a variety of programming languages and coding thereof may be used to implement the teachings of the disclosure contained herein. Moreover, the computer program is not intended to be limited to any particular control flow. There are many other variants of the computer program, which can use different control flows without departing from the spirit or scope of the invention.","Furthermore, one or more of the steps of the computer program may be performed in parallel rather than sequentially. Such a computer program may be stored on any computer readable medium. The computer readable medium may include storage devices such as magnetic or optical disks, memory chips, or other storage devices suitable for interfacing with a general purpose computer. The computer readable medium may also include a hard-wired medium such as exemplified in the Internet system, or wireless medium such as exemplified in the GSM mobile telephone system. The computer program when loaded and executed on such a general-purpose computer effectively results in an apparatus that implements the steps of the preferred method.",{"@attributes":{"id":"p-0106","num":"0105"},"figref":["FIG. 2","FIG. 1"],"b":["204","206","210","212","214","202"]},"In the example embodiment, the physiological data acquisition module  obtains bio-signals from the subject  and provides the results to the attention detection module . The attention detection results are in turn provided to the adaptive control module . The interface control module  provides a link between the adaptive control module  and the presentation module .","Physiological Data Acquisition","In the example embodiment, the physiological data acquisition module  is used to record physiological bio-signals from the subject . The bio-signals include the electroencephalogram (EEG), electrooculogram (EoG), electromyogram (EMG). Different sensor montages can be used for data acquisition in the example embodiment. For example, for stimulation-dependent applications (e.g. P300 based games), EEG signals are collected from a central-parietal region of the scalp. For voluntary attention control, EEG signals are collected from a pre-frontal area.","In the example embodiment, the EEG signals are first passed through a filter bank to be broken down to sub-bands, where the number of bands N is variable depending on the tasks.","Let X(t) be the multi-channel EEG\/EMG\/EOG signal, it is filtered to generate N sub-band signals:\n\n()=((),),1, . . . ,\u2003\u2003(1)\n","where A, Bare the coefficients of bandpass filters.","The filters in the example embodiment can be Infinite Impulse Response (IIR) or Finite Impulse Response (FIR) filters. In addition, by excluding the lower frequency filter (0-4 Hz) in the example embodiment, artifacts are advantageously removed and detection accuracy improved in the later stage.","The filtered signals are then sent to spatial filters corresponding to each frequency band. The spatial filters in the example embodiment are based on Common Spatial Pattern (CSP). The CSP filters are trained by joint maximization\/minimization of the variances for two classes involved. The spatially filtered signal for iband, Y(), is given as\n\n()=()\u2003\u2003(2)\n","where W is the CSP projection matrix.","The rows of W are the stationary spatial filters and the columns of Ware the common spatial patterns. The spatially filtered signal given in Equation (2) maximizes the differences in the variance of the two classes of EEG measurements. However, the variances of only a small number m of the spatial filtered signal are used in the example embodiment as features for classification. The m first and last rows of Y(t) form a feature vector O",{"@attributes":{"id":"p-0117","num":"0116"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"msubsup":{"mi":["O","i","t"]},"mo":"=","mrow":{"mi":"log","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"var","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["Y","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}}},{"munderover":{"mo":"\u2211","mrow":[{"mi":"i","mo":"=","mn":"1"},{"mn":"2","mo":"\u2062","mi":"m"}]},"mo":"\u2062","mrow":{"mi":"var","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["Y","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}}}}],"mo":"\/"}}}}},{"mrow":{"mo":["(",")"],"mn":"3"}}]},{"mtd":[{"mrow":{"mrow":[{"mi":"var","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["Y","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}}},{"munderover":{"mo":"\u2211","mrow":[{"mi":"\u03c4","mo":"=","msub":{"mi":"t","mn":"1"}},{"mi":"t","mo":"=","mrow":{"msub":[{"mi":"t","mn":"1"},{"mi":"t","mn":"2"}],"mo":"+"}}]},"mo":"\u2062","mrow":{"msup":{"mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["Y","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["t","\u03c4"],"mo":"+"}}},"mo":"-","msub":{"mover":{"mi":["Y","_"]},"mi":"i"}}},"mi":"T"},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["Y","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["t","\u03c4"],"mo":"+"}}},"mo":"-","msub":{"mover":{"mi":["Y","_"]},"mi":"i"}}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}]}}}},"where tand tare the starting and ending times of a moving window respectively.","The feature vectors for classification are formed by concatenating all CSP features from sub-bands as follows\n\n]\u2003\u2003(5)\n","In the method and system of the example embodiment, these feature vectors are used for attention detection. Based on specific tasks, different features are selected and finally a classification is applied.","Attention Detection","Generally, in the attention detection module , advanced pattern recognition methods are used in the example embodiment to process the incoming EEG signal and classify it into attention\/non-attention states with a quantifiable score to indicate the level of attention of the subject .","In addition, the attention detection of the example embodiment is capable of dealing with two types of scenarios. In one scenario, the subject  is presented with stimuli and the EEG is recorded so as to monitor the anticipatory aspect of attention, to classify signals which relate to the task presented. This is named reactive\/dependent attention in the example embodiment. In another scenario, the subject  voluntarily directs his\/her attention to the auditory and\/or visual stimuli, and this is named self-paced attention. Thus, the attention detection of the example embodiment preferably provides a useful means to tackle specific aspects of attention such as spatial, verbal, and object orientation.","In the example embodiment, to use attention detection in ADHD training session, a score Srepresenting the level of attention is obtained from the attention detection module , i.e.:\n\n(,\u039b)\u2003\u2003(6)\n","where F denotes function, Ois obtainable from Equation (5) and \u039bdenotes model parameters for attention detection, which is built upon the EEG\/EMG\/EOG data collected during a calibration period when the subject is asked to perform attention\/relaxation tasks.","In the following, an example implementation of obtaining the score Sfrom the brain signals is described in detail and named \u201cHybrid EEG Model\u201d.",{"@attributes":{"id":"p-0127","num":"0126"},"figref":"FIG. 3","b":["300","302"],"sup":"th"},"In step , windowing and pre-processing are performed. Step  selects electrode channels of interest and segments the incoming data stream into chunks using a running windowing mechanism. The window size and shift step are determined using training data. Step  also removes noise and artifacts through filtering.","In step , temporal feature extraction is performed. Step  computes statistics such as the standard deviation of the windowed and pre-processed EEG waveforms in each channel. The statistics are then concatenated into a joint feature vector. The feature vector is then input to step . In step , a classifier, such as the Linear Discriminant Analysis (LDA), is implemented to produce a score, for example x, indicating the likelihood of the hypothesis whereby the hypothesis is that the subject is in a state of concentration i.e. with focused attention. Other classifiers that can be used include Neural Networks (NNs), Support Vector Machines (SVM), Fuzzy Inference System (FIS), Tree-based classifiers etc., and their variants such as the Fuzzy Type 2 and the Relevance Vector Machine (RVM). Steps  and  form the temporal feature extraction module in the method .","In step , an array of band pass filters i.e. filter banks is implemented on the windowed and pre-processed EEG. Each filter bank is centred at a particular frequency, sampled at a fixed interval and is used to extract the EEG component in each discrete frequency window. For example, the fixed interval may be 4 Hz for the frequency range of the EEG from 4 Hz to 36 Hz. In one example, the filter bank is a digital filter with a low order and a linear phase. Such a filter bank can be a Finite Impulse Response (FIR) filter or an Infinite Impulse Response (IIR) filter. In a preferred embodiment, the filter bank is a low-order bandpass Chebyshev Type II filter with a pass-band width of 4 Hz. MATLAB (MathWorks Inc.) tools can be used to design and implement the filter banks. At the output of the filter banks, an EEG component is obtained for each filter bank with each component further containing separate components from each of the selected electrode channels.","In step , a common spatial pattern (CSP) array is implemented. Step  applies the CSP algorithm to each EEG component obtained in step  to emphasize the difference in spatial distributions of the energy between the two classes, the concentration and the non-concentration classes corresponding to the brain states during which the subject is concentrating and not concentrating respectively. The CSP algorithm is detailed in Equation (7) whereby for the jEEG component, a CSP feature cf(j) is extracted according to Equation (7). In Equation (7), Wis a matrix comprising of the first Iand the last Irows of W, whereby Iand Iare normalized for data processing efficiency and the ratio between Iand Iis kept constant. Furthermore, Eis a m\u00d7n data matrix of the jEEG component whereby m is the number of selected electrode channels and n is the number of samples in the EEG component in one channel. The relationship between W and the covariance matrices of the EEG components is given by Equation (8) in which \u03a3and \u03a3are the covariance matrices of the EEG components corresponding to two different classes of brain signals (i.e. different brain states), I is the identity matrix and D is a diagonal matrix.",{"@attributes":{"id":"p-0132","num":"0131"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":[{"mi":"cf","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"j"}},{"mi":"diag","mo":["(",")"],"mrow":{"msub":{"mi":["W","l"]},"mo":["\u2062","\u2062"],"mfrac":{"mrow":[{"msub":{"mi":["E","j"]},"mo":"\u2062","msubsup":{"mi":["E","j","T"]}},{"mi":"trace","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["E","j"]},"mo":"\u2062","msubsup":{"mi":["E","j","T"]}}}}]},"msubsup":{"mi":["W","l","T"]}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"7"}}]},{"mtd":[{"mrow":{"mrow":[{"mrow":{"mi":"W","mo":["\u2062","\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":[{"mi":"\u03a3","mrow":{"mo":["(",")"],"mn":"1"}},{"mi":["W","T"]}]},"mo":"=","mi":"D"},{"mrow":[{"mi":"W","mo":["\u2062","\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":[{"mi":"\u03a3","mrow":{"mo":["(",")"],"mn":"2"}},{"mi":["W","T"]}]},{"mi":["I","D"],"mo":"-"}],"mo":"="}],"mo":","}},{"mrow":{"mo":["(",")"],"mn":"8"}}]}]}}}},"The spatial filtering parameters i.e. spatial patterns such as the matrix W are learnt from the examples of the two classes via a subject dependent model training approach which would be elaborated later. The CSP array produces an array of spectral-spatial features, each representing the energy of the EEG component projected onto a particular spatial pattern. Such an array of features is usually over-redundant since not every spectral-spatial feature is associated with the concentration or non-concentration state in the brain. Preferably, the unnecessary (i.e. redundant) features are removed.","In step , a mutual information feature selection is implemented to remove the unnecessary features. Step  selects a set of features that contains the discriminative information between the concentration and the non-concentration states. This set is determined through a model training procedure via a subject dependent model training approach which would be elaborated later. At the end of step , a feature vector is obtained and is input into step .","In step , a classifier such as the LDA is implemented. Using the feature vector input from step , a score, for example x, is produced by the classifier. This score indicates the likelihood of the hypothesis whereby the hypothesis is that the subject is in a state of concentration i.e. with focused attention. Steps - form the spectral-spatial feature extraction module of the method .","Step  implements the fusion of the results from the temporal feature extraction module and the spectral-spatial feature extraction module to obtain a single output. In step , the continuous outputs of the classifiers in the temporal feature extraction module and the spectral-spatial feature extraction module are normalized. In one example, if an output is the score x, the normalized output xwill be (x\u2212m)\/swhereby mand sare respectively the mean and standard deviation of the outputs obtained using the training samples. Two normalized outputs xand xfrom the temporal feature module and the spectral-spatial module respectively are hence obtained. In one example, these two normalized outputs xand xare combined according to Equation (9) using weights wand wwhereby weights wand wcorrespond to xand xrespectively and reflect the individual performance of each of the modules. However, the normalized outputs xand xcan also be combined using non-linear methods such as a non-linear weighted regression. Weights wand ware calculated according to the formula w=(y)where yis the classification accuracy of the module alone and is obtained via training samples, and p (p>0) controls the power of the accuracy's weight in the combination. In one example, p is set to 1.\n\n\u2003\u2003(9)\n","In step , a quantitative output Sis generated.","Because of the large cross-subject variances in EEG patterns, a subject-dependent model training approach is used in the embodiments of the present invention to obtain the parameters and models for the method .","In the subject-dependent model training approach in the example embodiments, training data collection sessions are implemented to collect a subject's EEGs during navigated sessions.  illustrates a data collection protocol  for the subject-dependent model training approach according to an embodiment of the present invention. The protocol consists of 4 different tasks to be performed by the subject. In task , a subject is required to read a technical paper hence, in this task, the subject is in a state of concentration with his or her eyes opened. In task , the subject is required to relax and look around hence, in this task, the subject is not in a state of concentration and has his or her eyes opened. In task , the subject is required to perform mental arithmetic for example, taking 400 minus 7 repeatedly, hence, in this task, the subject is in a state of concentration with his or her eyes closed. In task , the subject is required to have his or her body and mind in a resting state with his or her eyes closed, hence in this task, the subject is not in a state of concentration with his or her eyes closed. The ideal level of attention for each of these tasks is plotted in  as line  whereby the ideal level of attention is high when the subject is required to be in a state of concentration and is low when the subject is required to be not in a state of concentration. In one example, the subject is required to take part in a few sessions, each session involving an array of alternate tasks.","Furthermore, in the subject-dependent training approach in the example embodiments, groups of parameters are determined via machine learning methods. An example of a machine learning method is the automation parameter optimization which is an iterative approach. Further details of the machine learning methods are given below. In one example, three groups of parameters are generated.","Firstly, projection matrices of CSPs for the CSP algorithm in the spectral-spatial feature extraction module (See ) are obtained. The learning of these projection matrices are carried out using the CSP method that jointly diagonalizes the two covariance matrices of the two classes i.e. the concentration class and the non-concentration class.","In one example, the CSP method includes the following steps.","In step 1, the normalized spatial covariance \u03a3 of the EEG measurements is computed according to Equation (10). In Equation (10), E is an N\u00d7T matrix representing the raw EEG measurement data of a single trial, N is the number of channels, T is the number of measurement samples per channel, \u2032 denotes the transpose operator and trace(\u2022) denotes the operation that sums the diagonal elements.",{"@attributes":{"id":"p-0144","num":"0143"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"\u03a3","mo":"=","mfrac":{"msup":{"mi":["EE","\u2032"]},"mrow":{"mi":"trace","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":["EE","\u2032"]}}}}}},{"mrow":{"mo":["(",")"],"mn":"10"}}]}}}}},"In step 2, the composite spatial covariance \u03a3is computed according to Equation (11). In Equation (11), the spatial covariance of one distribution is taken to be the average over the trials of each class and d\u03b5{1, 2} is the class index.\n\n\u03a3= + \u2003\u2003(11)\n","In step 3, the whitening transformation matrix P is computed according to Equation (12). In Equation (12), I is the identity matrix.\n\n\u2003\u2003(12)\n","In step 4, the whitened spatial covariance of the two classes is computed according to Equation (13). In Equation (13), Eand Eshare common eigenvectors B as shown in Equation (14) where I is the identity matrix and is the diagonal matrix of eigenvalues.\n\n\u03a3\u2032 and \u03a3\u2003\u2003(13)\n\n\u03a3\u2032 and \u03a3()\u2003\u2003(14)\n","In step 5, the CSP projection matrix W is computed according to Equation (15). In Equation (15), the rows of W are the stationary spatial filters and the columns of Ware the common spatial patterns.\n\n\u2003\u2003(15)\n","The spatial filtered signal Z of a single trial EEG E is given according to Equation (16).\n\n\u2003\u2003(16)\n","Equation (16) is equivalent to equation (2).","The spatial filtered signal Z given in Equation (16) maximizes the difference in the variance of the two classes of EEG measurements. In general, the variances of only a small number m of the spatial filtered signals are used as features for classification The signals Z, p\u03b5{1 . . . 2m} that maximize the difference in the variance of the two classes of EEG are associated with the largest eigenvalues \u03bb and (I\u2212\u03bb). In one example, these signals are used to form the feature vector Xgiven in Equation (17) whereby feature vectors Xare inputs to the classifier.",{"@attributes":{"id":"p-0152","num":"0151"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["X","p"]},"mo":"=","mrow":{"mi":"log","mo":["(",")"],"mrow":{"mrow":[{"mi":"var","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["Z","p"]}}},{"munderover":{"mo":"\u2211","mrow":[{"mi":"i","mo":"=","mn":"1"},{"mn":"2","mo":"\u2062","mi":"m"}]},"mo":"\u2062","mrow":{"mi":"var","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["Z","p"]}}}}],"mo":"\/"}}}},{"mrow":{"mo":["(",")"],"mn":"17"}}]}}}}},"Equation (17) is equivalent to Equation (3).","Secondly, a set of parameters for mutual information feature selection in the spectral-spatial feature selection module is determined. The mutual information feature selection method is based on mutual information which indicates the mutual dependence of the features with respect to the classes. Further details of the mutual information feature selection process are as follows.","Taking into consideration a vector variable X for example, CSP features as obtained in Equation (17) and its corresponding class label Y, the mutual information between the two random variables X and Y is given by Equation (18). In Equation (18), H(X) denotes the entropy of the feature variable X and H(Y|X) represents the conditional entropy of class label variable Y given feature variable X. The entropy and the conditional entropy are given respectively in Equation (19) and Equation (20).",{"@attributes":{"id":"p-0156","num":"0155"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":[{"mi":"I","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["X","Y"],"mo":";"}}},{"mrow":[{"mi":"H","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"X"}},{"mi":"H","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["Y","X"],"mo":"|"}}}],"mo":"-"}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"18"}}]},{"mtd":[{"mrow":{"mrow":[{"mi":"H","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"X"}},{"mo":"-","mrow":{"msub":{"mo":"\u222b","mrow":{"mi":["x","X"],"mo":"\u2208"}},"mo":"\u2062","mrow":{"mrow":[{"mi":"xp","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}},{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}},{"mo":"\u2146","mi":"x"}],"mo":["\u2062","\u2062","\u2062","\u2062"],"msub":{"mi":"log","mn":"2"},"mstyle":{"mspace":{"@attributes":{"width":"0.2em","height":"0.2ex"}}}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"19"}}]},{"mtd":[{"mrow":{"mrow":[{"mi":"H","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["Y","X"],"mo":"|"}}},{"mo":"-","mrow":{"msub":{"mo":"\u222b","mrow":{"mi":["x","X"],"mo":"\u2208"}},"mo":"\u2062","mrow":{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}},{"munder":{"mo":"\u2211","mrow":{"mi":["y","Y"],"mo":"\u2208"}},"mo":"\u2062","mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["y","x"],"mo":"|"}}},{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["y","x"],"mo":"|"}}},{"mo":"\u2146","mi":"x"}],"mo":["\u2062","\u2062","\u2062","\u2062"],"msub":{"mi":"log","mn":"2"},"mstyle":{"mspace":{"@attributes":{"width":"0.2em","height":"0.2ex"}}}}}],"mo":"\u2062"}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"20"}}]}]}}}},"In one example, the mutual information feature selection process includes the following steps.","In step 1, a candidate set of d features is initialized as F={f, f, . . . , f} and a select feature set is initialized as a null set Fopt=\u00f8.","In step 2, for each feature fin the candidate set, a tentative feature vector F=Fopt\u222a{fk} is formed. Next, Fand the Na\u00efve Bayesian Parzen Window are used to predict the class label Y. The mutual information of the predicted class label and the true label i.e. I(Y; Y) is then computed.","In step 3, the feature fwhich maximizes I(Y; Y) is then selected.","In step 4, if F=\u00f8 and the gain in the mutual information is less than a preset threshold \u03b4 i.e. I(Y;Y)\u2212I<\u03b4, the process is terminated. Otherwise, in step 5, I=I(Y;Y).","In step 6, the candidate set is updated by F\u2192F\\{f} whereas the select feature set is updated by Fopt\u2192Fopt\u222aS{f}.","In step 7, if the candidate set is empty, the process is terminated. Otherwise, the process is repeated from step 2.","In the example embodiments, a feature refers to a CSP feature from a filter bank and can take on different values at different instances. The mutual information feature selection process in the example embodiments as described above is applied to the training set with labelled samples. After the feature selection process is completed, the select set of features includes the CSP features determined as \u201cimportant\u201d or characteristic for concentration detection based on their mutuality amongst the labeled samples. This set of features is used during the feature selection process when processing unlabelled data for concentration detection.","Thirdly, models for the classifiers in the method  are obtained by the traditional Fisher linear discriminant method, using labelled training data samples. In one example, the labelled training data samples have positive labels if they are recorded from the concentration tasks and negative labels if they are recorded from the non-concentration tasks.","In the example embodiments, the set of parameters obtained from the subject dependent training approach can be used to recreate a model for concentration detection using a computer program. In one example, a setup\/configuration file is created whereby this file includes the projection vector and the bias of the classifiers, projection matrices of each CSP filter, the bands to be selected for the filter banks, and the weights to be used for combining the outputs from the temporal feature extraction module and the spectral-spatial feature extraction module.",{"@attributes":{"id":"p-0167","num":"0166"},"figref":["FIG. 5","FIG. 3","FIG. 3"],"b":["502","504","506","508","510","512","300","514","516","300"]},"In , training EEGs are acquired from the subjects when they are performing the required tasks during the training data collection sessions implemented in the subject-dependent training approach in the example embodiments. Machine learning techniques are then implemented in using the training EEGs in the feature extraction training unit , feature selection training unit  and the modelling unit  in . This would obtain the required parameters and model for the feature extraction unit , feature selection unit  and the classification units  and  for the online processing of real-time EEGs.","In , in one example, the feature extraction unit  implements steps  and  in  whereas the feature extraction unit  implements the step . In addition, the feature selection unit  implements the step . Furthermore, the classification units,  and , implement steps  and  in  respectively whereas the post-processing unit  implements steps  and  in .","The advantages conferred by the embodiments of the present invention can include:","Firstly, the method for concentration detection in the example embodiments provides an accurate quantitative measure of the subject's attention or concentration level that is not provided by any of the prior arts. The method in the example embodiments is subject-specific and uses optimized parameters. On the other hand, the prior art methods are based on spectral features alone, with their output typically based on the average of a large set of results and a comparison performed within a narrow range to detect concentration. For example, the range can be extending from the mean minus the standard deviation to the mean plus the standard deviation of the results. Hence, the method in the example embodiments is more accurate. Furthermore, in the example embodiments of the present invention, an accurate score can be obtained continuously and this is important in (near) real-time situations when a fast and accurate score is necessary.","Secondly, the hybrid model approach implemented in the example embodiments of the present invention takes all dimensions of the EEG into consideration. Specifically, these dimensions are the temporal, spatial and spectral information of the EEG which are then combined to give a single result. On the other hand, prior arts only concentrate on the spectral information of the EEG and hence provide a less detailed picture of the subject's EEG characteristics as compared to the embodiments of the present invention. In addition, in the example embodiments, the windowing approach allows the method of concentration detection to adjust the time resolution by changing the time segmentation window size to the best window size. This allows different window sizes to be chosen under different circumstances. For example, when a long term score is desired, the EEG recording session is preferably long whereas in a real-time situation, the EEG recording segment is preferably short.","Thirdly, the method in the example embodiments of the present invention allows the creation of the best model for each subject. The method can also be used to create models based on a small cohort and thus, investigate group-specific issues for example, a group of ADHD boys. Furthermore, using a large database, the method can also be useful in investigating generalization issues for example population based medical studies.","Fourthly, in the example embodiments, automatic selection and combination of features is achieved as the parameters and models for the method are automatically obtained from subject-specific modelling. This can improve the performance of the concentration detection method in the example embodiments. The mutual information feature selection in the example embodiments provides a novel way to create subject-specific modelling for example, for individualized healthcare. Furthermore, the use of the subject-specific model in the example embodiments achieves a higher accuracy and the machine learning methods used to create the subject-specific models allow the method in the example embodiments to be more flexible.","Fifthly, in the example embodiments, the metric used in the overall performance evaluation is based on receiver operating characteristics (ROC) analysis. In the example embodiments, performance curves plotting the False Positive Rate (FPR) against the False Negative Rate are used to analyze the ROC. This metric (ROC) shows objectively the true performance of the method in the example embodiments using a simple curve. It will also allow one to determine the best model to be used for each subject and also to choose a model that will fit the sensitivity and specificity requirements along the ROC curve, while taking note of the trade-off between the sensitivity and specificity.","Furthermore, the method in the example embodiments can be implemented in the form of a software tool for example, as add-ons to EEG systems or as internet-based web services. The method can also be embedded into a PDA-like medical device. Even with only a low-cost EEG acquired at a low sampling rate and from a few EEG sensors on the forehead, the method in the example embodiments is still able to provide robust attention or concentration detection and scoring. Thus, the method in the example embodiments can be implemented in a simple and handy system with only forehead sensors.","Hence, the example embodiments of the present invention can provide a continuous, quantitative, accurate and robust scoring mechanism for subject attention or concentration level since the example embodiments are based on features extracted and further selected using a multi-domain (spatial, spectral and temporal) analysis of the EEG and classified using machine learning. In addition, the example embodiments of the present invention provide a system to capture subject-specific EEG characteristics into a computational model and an automated parameter selection process that can find the best parameters and model. Furthermore, the example embodiments of the present invention provide a post-processing fusion scheme that improves performance by a multi-scale approach.","To further illustrate the advantages of the example embodiments of the present invention, an experimental study involving 5 participating subjects (all male and healthy) was carried out. The EEGs from these subjects are recorded from a standard 10\/20 EEG system (NeuroScan NuAmps) with 15 channels and from frontal channels (Fp1\/Fp2).","Table 1 shows the results achieved by a method for concentration detection according to an embodiment of the present invention and by the prior art method in Monastra and Lubar [Monastra and Lubar, 2000\u2014US06097980\u2014Quantitative electroencephalographic (QEEG) process and apparatus for assessing attention deficit hyperactivity disorder; V. J. Monastra, S. Lynn, M. Linden, J. F. Lubar, J. Gruzelier, and T. J. LaVaque, \u201cElectroencephalographic Biofeedback in the Treatment of Attention-Deficit\/Hyperactivity Disorder,\u201d Applied Psychophysiology and Biofeedback, vol. 30, no. 2, pp. 95-114, June 2005.] In Table 1, the row, corresponding to \u201cTheta\/beta (prior-art)\u201d shows the mean accuracy obtained by the method according to the prior art, the row corresponding to \u201cWaveform only\u201d shows the mean accuracy obtained from the temporal feature extraction module alone the row corresponding to \u201cSpectrum only\u201d shows the mean accuracy obtained from the spectral-spatial feature extraction module alone and the row corresponding to \u201cHybrid technique\u201d shows the mean accuracy obtained from the method in the example embodiments. Furthermore, the results in Table 1 are in percentage, expressed in the form \u201cmean\u00b1standard deviation\u201d and are obtained via a 2\u00d72 fold cross-validation method. From Table 1, it can be seen that the mean accuracy of the method in the example embodiments is significantly better than that of the prior art method. More specifically, the overall performance improvement (absolute value) of the method in the example embodiments over the prior art method is 14.8%. Thus, these results demonstrate the ability of the method in the example embodiments to create an optimized subject-specific model that outperforms the prior art method.",{"@attributes":{"id":"p-0180","num":"0179"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"28pt","align":"center"}}],"thead":{"row":[{"entry":[{},"TABLE 1"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"6","align":"center","rowsep":"1"}}]},{"entry":[{},"Subject","Subject","Subject","Subject","Subject",{}]},{"entry":[{},"1","2","3","4","5","Average"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"6","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"1","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Theta\/beta","57.5 \u00b1 2.7","57.5 \u00b1 3.5","66.7 \u00b1 10.9","56.9 \u00b1 9.7","57.5 \u00b1 2.2","59.2"]},{"entry":"(prior-art)"},{"entry":["Waveform only","60.2 \u00b1 3.8","78.8 \u00b1 5.3","69.8 \u00b1 4.7","76.3 \u00b1 5.3","72.8 \u00b1 6.2","71.6"]},{"entry":["Spectrum only","64.4 \u00b1 4.0","87.9 \u00b1 6.2","72.8 \u00b1 3.2","76.3 \u00b1 0.0","59.6 \u00b1 8.9","72.2"]},{"entry":["Hybrid","62.8 \u00b1 4.4","83.8 \u00b1 3.5","76.0 \u00b1 1.0","76.3 \u00b1 1.7","71.3 \u00b1 5.3","74.0"]},{"entry":"technique"},{"entry":["Improvement","5.3","26.3","9.3","19.4","13.8","14.8"]},{"entry":{"@attributes":{"namest":"1","nameend":"7","align":"center","rowsep":"1"}}}]}}]}}},"Table 2 shows further results achieved by a method for concentration detection according to an embodiment of the present invention and by the prior art method in Monastra and Lubar. In Table 2, for each subject, the row corresponding to \u201cTheta\/beta (prior-art)\u201d shows the equal error rate (EER) obtained by the method according to the prior art, the row corresponding to \u201cWaveform only\u201d shows the EER obtained from the temporal feature extraction module alone, the row corresponding to \u201cSpectrum only\u201d shows the EER obtained from the spectral-spatial feature extraction module alone and the row corresponding to \u201cHybrid technique\u201d shows the EER obtained from the method in the example embodiments. The EER is the rate at which the false positive rate and the false negative rate are equal. Furthermore, the results in Table 2 are in percentage, expressed in the form \u201cmean\u00b1standard deviation\u201d and are obtained via a 2\u00d72 fold cross-validation method. For each subject, the best performance by each of the methods is tabulated in Table 2. The relative error reduction rate is calculated according to Equation (21). It can be seen from Table 2 that the overall error rate reduction is 42.5% indicating that the method in the example embodiments performs significantly better than the prior art method. Furthermore, Table 2 also shows that even the performance of the temporal feature extraction module alone (\u201cWaveform only\u201d) or the spectral-spatial feature extraction module alone (\u201cSpectral only\u201d) in the example embodiments is better than the prior art method. This illustrates that the subject dependent training approach can significantly improve the performance of the methods.",{"@attributes":{"id":"p-0182","num":"0181"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"center"}}],"thead":{"row":[{"entry":[{},"TABLE 2"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"6","align":"center","rowsep":"1"}}]},{"entry":[{},"Subject","Subject","Subject","Subject","Subject",{}]},{"entry":[{},"1","2","3","4","5","Average"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"6","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"5","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"6","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"7","colwidth":"35pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Theta\/beta","42.7","44.1","30.6","39.3","38.7","39.1"]},{"entry":"(prior-art)"},{"entry":["Waveform only","39.2","17.9","27.5","17.8","33.9","27.3"]},{"entry":["Spectrum only","37.9","8.2","21.9","25.1","30.6","24.7"]},{"entry":["Hybrid","35.0","7.3","21.9","20.8","27.7","22.5"]},{"entry":"technique"},{"entry":["Improvement","18","83.4","28.4","47.0","29.7","42.5"]},{"entry":"(Relative Error"},{"entry":"Reduction Rate)"},{"entry":{"@attributes":{"namest":"1","nameend":"7","align":"center","rowsep":"1"}}}]}}]}}},{"@attributes":{"id":"p-0183","num":"0182"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":["Relative","Error","Reduction","Rate"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]},"mo":"=","mfrac":{"mrow":{"msub":[{"mi":"EER","mrow":{"mi":["prior","art"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}},{"mi":["EER","hybrid"]}],"mo":"-"},"msub":{"mi":"EER","mrow":{"mi":["prior","art"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}}}}},{"mrow":{"mo":["(",")"],"mn":"21"}}]}}}}},"It should be appreciated that, as the system of the example embodiment makes use of direct attention detection to control e.g. a game which then provides feedback cues to the subject , it is actually a feedforward instead of feedback system. In other words, the game only provides a visual representation of the concentration level. Other forms of representation can also be used, e.g. a simple bar indicator.","This is a major distinction between the system of the example embodiment and prior art approaches which purely rely on feedback signals and in which the connection between attention and feedback training program is implicit. In the system of the example embodiment, the control of the game is advantageously through direct attention control. That is, the subject  only needs to concentrate and does not need to follow any feedback cues, and the result provided (i.e. representation of concentration level) is direct. Thus, the subject  knows explicitly that he is controlling if he so wishes.","Adaptive Control","The adaptive control module  of the example embodiment receives the results of the attention detection module  together with the information from the presentation module , and adjusts automatically and adaptively the level of difficulty of the attention training games. In addition, the rules embedded in the adaptive control module  are programmable.","In the example embodiment, Hierarchical Temporal Control is used. First, a probabilistic model is learnt from the attention score Sby fitting it with a Gaussian distribution, i.e.",{"@attributes":{"id":"p-0189","num":"0188"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["S","a","t"]}}},{"mfrac":{"mn":"1","msup":{"mrow":[{"mo":["(",")"],"mrow":{"mn":"2","mo":"\u2062","msup":{"mi":"\u03c0\u03c3","mn":"2"}}},{"mn":["1","2"],"mo":"\/"}]}},"mo":["\u2062","\u2062"],"mi":"exp","mrow":{"mo":["{","}"],"mrow":{"mfrac":{"mn":"1","mrow":{"mn":"2","mo":"\u2062","msup":{"mi":"\u03c3","mn":"2"}}},"mo":"\u2062","msup":{"mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["S","a","t"]},"mo":"-","mi":"\u03bc"}},"mn":"2"}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"22"}}]}}}}},"The value from Equation (22) is then converted to a fixed range between 0-100 as follows:",{"@attributes":{"id":"p-0191","num":"0190"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":["Q","t"]},"mo":"=","mfrac":{"mn":"100","mrow":{"mn":"1","mo":"+","mrow":{"mi":"exp","mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mo":"-","mi":"\u03b2"},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["S","a","t"]}}}],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}}}}}},{"mrow":{"mo":["(",")"],"mn":"23"}}]}}}}},"where \u03b2 is a predetermined constant.","Qis advantageously the attention score which directly associates with the performance of a game control. In the example embodiment, a threshold R is set such that the game control (e.g. speed of a car) is proportional to the value of \u0394 where",{"@attributes":{"id":"p-0194","num":"0193"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"\u0394","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"mo":"{","mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"msup":{"mi":["Q","t"]},"mo":"-","mi":"R"},"mo":";"}},{"mrow":{"mrow":{"mi":"if","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"mo":["(",")"],"mrow":{"msup":{"mi":["Q","t"]},"mo":"-","mi":"R"}}},"mo":">","mn":"0"}}]},{"mtd":[{"mrow":{"mn":"0","mo":";"}},{"mrow":{"mrow":{"mi":"if","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"mo":["(",")"],"mrow":{"msup":{"mi":["Q","t"]},"mo":"-","mi":"R"}}},"mo":"<","mn":"0"}}]}]}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"24"}}]}}}}},"For example, in the case of racing car game, the speed of the car is proportional to \u0394. The speed of the car is set as:\n\n()=\u03b1\u0394()\u2003\u2003(25)\n","where \u03b1 is a constant which can be set initially and is subsequently adjustable e.g. from a configuration file. The distance D moved by the car over a time T is as follows:\n\n()\u03b1()\u2003\u2003(26)\n","where  is an average value of Qover time T.","With the quantifiable attention score and the speed control, the adaptive control module  of the example embodiment is able to update the threshold R based on a performance indicator obtained from a specific subject  when he plays the game in his first trial. In the example embodiment, the time for a subject  to perform a specific task (e.g. controlling a racing car around a racing course on the screen) is defined as T, and the total distance of the course is defined as D\u2032. The adaptive control module  is advantageously able to adjust the difficulty level based on the following equation:",{"@attributes":{"id":"p-0199","num":"0198"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":["R","\u2032"]},"mo":"=","mrow":{"mover":{"mi":["Q","_"]},"mo":"-","mfrac":{"msup":{"mi":["D","\u2032"]},"mrow":{"mi":"\u03b1","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["T","g"]}}}}}},{"mrow":{"mo":["(",")"],"mn":"27"}}]}}}}},"Thus, the system of the example embodiment adjusts the level of difficulty (to make it more difficult or easier) from a temporal perspective. It should also be appreciated that the system of the example embodiment is preferably based on an asynchronous BCI mode, i.e. the subject  does not need to follow any cue in order to control the game.","Based on the above, the adaptive control module  works by first setting a default level of difficulty. The subject  then plays the game at the default level of difficulty over a first period of time. During a subsequent delay time (i.e. learning phase), the relevant parameters such as Qand T are estimated. These provide a direct indication of the subject's concentration level. The adaptive control module  then changes the level of difficulty based on the estimated parameters.","The delay time in the example embodiment ranges from about 1 second to 19 seconds depending on the type of game and\/or presentation. The time delay can also be automatically and adaptively adjusted by the system based on statistics of the subject's respective game profile.","Subsequently, during the training session, the parameters are continuously calculated and the difficulty level is adjusted accordingly. In the example embodiment, different thresholds R may be set for respective difficulty levels. For example, the level of difficulty is increased if the player's attention level is high (e.g. time taken to complete the racing course is below a lower limit). Similarly, the level of difficulty is decreased if the player's attention level is low (e.g. time taken to complete the racing course is above an upper limit). Various other schemes for adjusting the level of difficulty can be implemented depending on e.g. whether the game is intended to be more or less challenging. The session ends after an allocated period has passed, and the relevant session information such as highest score\/level, duration at the highest level, distribution of score, etc. is stored in the system.","Interface Control","The interface control module  of the example embodiment comprises a typical BCI setup which comprises a multi-channel EEG cap, amplifiers and application programming interface (API) code for allowing the client  to record and store the physiological bio-signals (EEG, EoG, EMG, etc.).","Presentation","The presentation module  comprises the game part, which is the implementation of the game on the client  (), and allows interaction with the subject. Various game examples have been developed for use with the system of the example embodiment. It will be appreciated that the games are run on the client while the client is being connected to the interface control module  to record the subject's physiological bio-signals. In the example embodiment, the interface and game parameters are fully configurable using e.g. a graphic user interface (GUI). A Stimulus generator is also provided in the presentation module , allowing the creation of additional auditory and visual disturbances during the training phase. Such disturbances may advantageously influence the subject's performance and thus his\/her EEG\/EoG\/EMG.",{"@attributes":{"id":"p-0208","num":"0207"},"figref":["FIG. 6","FIG. 6","FIG. 1"],"b":["102","602","110","604","602","202","110","606"]},{"@attributes":{"id":"p-0209","num":"0208"},"figref":"FIG. 7","b":["700","700","702","704","706","708","710","712","714","716"],"sub":["1","2","1 ","2 "]},"The method and system of the example embodiment described above advantageously provide an attention feedforward training protocol for attention training. In addition, the attention training game has an adjustable level of difficulty under time constraints. Further, the training system is preferably asynchronous, i.e. initiated by the user, not the computer. Advantageously, the degree\/level of difficulty is adaptively and automatically adjusted by information derived from physiological bio-signals (e.g. EEG, EOG, EMG, etc.).","The method and system of the example embodiment also provide rules\/strategy for programmable and automated adaptation of level of difficulty (e.g. by the Graphical User Interface (GUI) and Game). Furthermore, the parameters of the GUI\/Game preferably allow the creation of a learning curve (e.g. level of difficulty over time related to training). Also, the presentation of the stimuli in a multiple sequence with additional disturbance can be created during the training. These disturbances can be auditory or\/and visual, and advantageously allowing active control of the level of difficulty. The training game can also be tailored to cater for various aspects of attention (e.g. spatial, verbal, object orientation) and to provide measures of adaptive learning.","The method and system of the example embodiment can be implemented on a computer system , schematically shown in . It may be implemented as software, such as a computer program being executed within the computer system , and instructing the computer system  to conduct the method of the example embodiment.","The computer system  comprises a computer module , input modules such as a keyboard  and mouse  and a plurality of output devices such as a display , and printer .","The computer module  is connected to a computer network  via a suitable transceiver device , to enable access to e.g. the Internet or other network systems such as Local Area Network (LAN) or Wide Area Network (WAN).","The computer module  in the example includes a processor , a Random Access Memory (RAM)  and a Read Only Memory (ROM) . The computer module  also includes a number of Input\/Output (I\/O) interfaces, for example I\/O interface  to the display , and I\/O interface  to the keyboard .","The components of the computer module  typically communicate via an interconnected bus  and in a manner known to the person skilled in the relevant art.","The application program is typically supplied to the user of the computer system  encoded on a data storage medium such as a CD-ROM or flash memory carrier and read utilising a corresponding data storage medium drive of a data storage device . The application program is read and controlled in its execution by the processor . Intermediate storage of program data maybe accomplished using RAM .","Although the present invention has been mainly described with respect to ADHD treatment, it can also find application for other mental disorders, such as depression, degraded working memory, Mild Cognitive Disorder (MCI), Alzheimer Disease (AD), etc. In addition, while the implementation of the present invention has been described as a system, it can also take the form of a software program with CDROM\/DVD on a computer, a web-service over the internet, or a cartridge for a console, etc.","It will be appreciated by a person skilled in the art that numerous variations and\/or modifications may be made to the present invention as shown in the specific embodiments without departing from the spirit or scope of the invention as broadly described. The present embodiments are, therefore, to be considered in all respects to be illustrative and not restrictive."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["Embodiments of the invention will be better understood and readily apparent to one of ordinary skill in the art from the following written description, by way of example only, and in conjunction with the drawings, in which:",{"@attributes":{"id":"p-0090","num":"0089"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0091","num":"0090"},"figref":["FIG. 2","FIG. 1"]},{"@attributes":{"id":"p-0092","num":"0091"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0093","num":"0092"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0094","num":"0093"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0095","num":"0094"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0096","num":"0095"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0097","num":"0096"},"figref":"FIG. 8"}]},"DETDESC":[{},{}]}
