---
title: Method and apparatus for object tracking via hyperspectral imagery
abstract: A computer-implemented method for tracking a small sample size user-identified object comprising extracting a plurality of blocks of pixels from a first frame of a plurality of frames of a scene detected by a hyperspectral (HS) sensor, comparing a reference sample of the object with the plurality of blocks to generate a first attribute set corresponding to contrasting HS response values of the reference sample and HS response values of each block of the plurality of blocks, comparing a test sample of a portion of the first frame to each block of the plurality of blocks to generate a second attribute set corresponding to contrasting HS response values of the test samples and HS response values of each block of the plurality of blocks and determining if the object exists in two or more of the frames by comparing the first HS attribute set with the second HS attribute set.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08611603&OS=08611603&RS=08611603
owner: The United States of America as represented by the Secretary of the Army
number: 08611603
owner_city: Washington
owner_country: US
publication_date: 20120214
---

{"@attributes":{"id":"description"},"GOVINT":[{},{}],"heading":["GOVERNMENT INTEREST","FIELD OF INVENTION","BACKGROUND OF THE INVENTION","BRIEF SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION"],"p":["Governmental Interest\u2014The invention described herein may be manufactured, used and licensed by or for the U.S. Government.","Embodiments of the present invention generally relate to tracking objects in imagery and, more particularly, to a method and apparatus for motion-invariant object tracking via hyperspectral imagery.","Often, objects of interest, such as motor vehicles, persons or other entities are required to be tracked by remote sensing. Sensors are mounted on platforms surveying the entities which move relative to the sensor platforms enabling kinematic motion tracking, i.e., tracking based on the positional movement of the entities. The sensors create discrete images of a scene and the images are represented as a sequence of frames. In conventional kinematic approaches, entity motion is easily blocked by structures such as buildings, trees, tunnels and the like in a sensor's viewing area. Continuity of entity motion is lost across the sensed sequence of frames when an entity goes out of view of the sensor and one unique object is mistaken for two distinct objects when the entity comes back into view of the sensor.","For example, in a scenario where an aerial sensor is capturing frames of a motor vehicle travelling on a highway, kinematic tracking allows the vehicle to be tracked accurately until the vehicle becomes occluded, e.g., enters a tunnel. Once the vehicle emerges from the tunnel, the aerial sensor has no way of correlating the vehicle to the previous view of the same vehicle, and will therefore classify the vehicle exiting the tunnel as new entity. This problem is worsened if several vehicles are entering and emerging from the tunnel. The aerial camera effectively loses tracking for all vehicles when they enter and exit a tunnel, or vehicles are obstructed from view by any structure for a few frames. The loss of tracking across the sequence of frames results in incomplete tracking of entities, and important targets are lost. The loss of tracking the motion is more pronounced when a vehicle is obscured for long periods of time. For shorter periods of occlusion, a kinematic motion tracker may be able to compensate for small frame to frame losses, but for long periods of occlusion in a sequence of frames, the kinematic motion tracker is not able to correlate two seemingly distinct entities sensed at different periods of the tracking as being the same entity.","In order to surmount these difficulties imposed by kinematic motion tracking, hyperspectral (HS) based remote sensing is often used as a substitute technology. HS imaging collects and processes the wavelength of responses of incident surfaces being exposed to a plurality of regions of the electromagnetic spectrum. HS imaging divides the spectrum into many more bands than visible light. Vehicles, persons and other entities often leave a hyperspectral \u201cfingerprint\u201d known as spectral profiles or spectral signatures due to their paint material, clothing material and the like. Due to the number of HS bands, algorithms are available to identify nearly any material type. A sensor capable of HS imaging captures several frames of spectral profiles and compares the profiles in each frame to track an entity without relying on motion. For each frame, the HS sensor senses multiple HS bands per pixel, creating a three-dimensional HS data cube for processing and analysis. These cubes can be compared with cubes for other frames to perform tracking. In contrast to kinematic tracking, HS imaging can \u201cpick up\u201d tracks of an entity hours after the entity was initially obstructed and correlate the two tracks to one object.","However, often in HS sensing, entities are represented by only a few pixels out of the entire frame relative to the number of spectral bands sensed by the HS sensor, i.e., the entity has a \u201crare sample size\u201d and thus is difficult to track because statistical methods have not proved reliable on rare sample sizes. For example, a vehicle is made of metallic red paint, but that paint appears in only seven pixels out of a five megapixel image. Conventional statistical hypothesis tests, which are used to perform HS tracking; cannot be implemented on such a small sample of pixels relative to the number of HS bands sensed.","Therefore, there is a need in the at for a method and apparatus for tracking entities of small sample size via hyperspectral imaging.","Embodiments of the present invention relate to a computer-implemented method for tracking a small sample size user-identified object comprising extracting a plurality of blocks of pixels from a first frame of a plurality of frames of a scene detected by a hyperspectral (HS) sensor, comparing a reference sample of the object with the plurality of blocks to generate a first attribute set corresponding to contrasting HS response values of the reference sample and HS response values of each block of the plurality of blocks, comparing a test sample of a portion of the first frame to each block of the plurality of blocks to generate a second attribute set corresponding to contrasting HS response values of the test samples and HS response values of each block of the plurality of blocks and determining if the object exists in two or more of the frames by comparing the first HS attribute set with the second HS attribute set.","Another embodiment of the invention is directed to an apparatus comprising a selection module for extracting a plurality of blocks of pixels from a first frame of a plurality of frames of a scene detected by a hyperspectral (HS) sensor and a comparator module for comparing a reference sample of the object with the plurality of blocks to generate a first attribute set corresponding to contrasting HS response values of the reference sample and HS response values of each block of the plurality of blocks, comparing a test sample of a portion of the first frame to each block of the plurality of blocks to generate a second attribute set corresponding to contrasting HS response values of the test samples and HS response values of each block of the plurality of blocks and determining if the object exists in two or more of the frames by comparing the first HS attribute set with the second HS attribute set.","Embodiments of the present invention comprise a method and apparatus for tracking entities of small sample size via hyperspectral (HS) imaging. An HS sensor captures several spectral \u201cimage\u201d frames of a scene over a period of time, capturing several HS band values for each pixel in the spectral image frame. An external user looks at a few of the frames and determines that a particular object is of interest and should be tracked by selecting the object on a display using a cursor, voice command, or the like. The identified object is known as the reference sample and is represented by its own set of hyperspectral profiles for all or some of the pixels in the object. A window, the same size as the reference sample (e.g., five pixels by seven pixels by two hundred HS bands), is established for iterating through every pixel of all subsequently sensed frames, where the window denotes a test sample represented by another set of hyperspectral profiles for each iteration for each sensed frame. Blocks of pixels (identical in size to the reference sample and test window, e.g., five pixels by seven pixels by two hundred HS bands) are extracted from the current frame randomly and the reference sample is compared to each block using a discriminant metric, i.e. a Spectral Angle Mapper (discussed below), which measures the angle difference between two vectors (in this case, two \u201cmean\u201d HS profiles), to generate a large sample, low dimensional attribute set. The blocks are randomly selected because randomness will provide the statistical independence required by a hypothesis test used in an embodiment of the present invention. The blocks are also compared to the test sample to generate a second large sample, low dimensional attribute set. These two attribute sets are then compared to determine if the entity represented by the reference sample is present in the window represented by the test sample. When this is repeated for all possible spatial locations in a current frame by positioning the window to test different pixels for each frame sensed by the HS sensor, an entity is trackable via its distinct large sample, low dimensional attribute set tied to the entity's hyperspectral profile.",{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 1","b":["100","100","102","104","106","102","106","102","107","107","104","106","107","108","109","111","107"]},"The memory  stores non-transient processor-executable instructions and\/or data that may be executed by and\/or used by the processor . These processor-executable instructions may comprise firmware, software, and the like, or some combination thereof. Modules having processor-executable instructions that are stored in the memory  comprise the selection module , the block comparator , the windowing module , and the attribute comparator . Further, the memory  stores the frames  sensed by the sensor  and the samples  comprising the reference sample  and the test samples . In an exemplary embodiment, the memory  may include one or more of the following: random access memory, read only memory, magneto-resistive read\/write memory, optical read\/write memory, cache memory, magnetic read\/write memory, and the like, as well as signal-bearing media, not including non-transitory signals such as carrier waves and the like.","The selection module  extracts blocks of pixels from each frame in the plurality of sensed frames , where each pixel contains a plurality of hyperspectral responses. The blocks of pixels are selected randomly by selection module  to ensure statistical independence is achieved from the contrast between the comparison of blocks and the reference sample discussed below. The selection module  is configured to select a predetermined amount of blocks from each frame and is adjustable for effectiveness.","The block comparator  compares the reference sample  with the extracted blocks from frames , by comparing the hyperspectral responses to generate a contrast attribute set. The attribute set represents the difference between hyperspectral response values in the blocks and the reference sample. The windowing module  moves a small test window over the current frame of the plurality of sensed frames defining a test sample for each area in the frame and the block comparator  compares each test sample with the extracted blocks in the same manner to generate a second contrast attribute set. The attribute comparator  compares the output of the block comparator  for the reference sample with the output of the block comparator  for the test sample to determine whether these attribute sets matching, thereby indicating that the reference object exists in the current location of the test window.",{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 2","b":["200","203","202","204","203","203","203","203","203","110","210","204","203"],"sub":"1 . . . N "},"A user  observing the frames  selects an object of interest with n pixels in it from the chosen frame as a reference sample , where n is significantly smaller than B, the number of hyperspectral bands sensed by the sensor . The user  determines that this object may be of special interest and should be tracked across the frames . The user  can select this object using a simple mouse cursor, voice command for marking a portion of the frame, and the like. In other embodiments, the user  can highlight a portion of a frame using touch sensors to signify the object of interest. In yet another embodiment, the apparatus  is pre-configured by the user to automatically identify (using a pre-stored HS library) and track specific types of objects as the objects of interest. The dimension of the user selected reference sample is n\u00d7B, or the number of pixels used to represent an entity by the number of hyperspectral bands captured by the sensor . Note that this dimension should not be confused with the dimension of the data cube, as known to those of ordinary skill in the art of HS imaging, represented in this context only by B; n represents the number of spectra in the HS sample\u2014or sample size\u2014and B represents the dimension of the HS sample). The chosen number of blocks  is referred to as N and is predetermined or selected by the user  in an exemplary embodiment. The dimension of each of the blocks  is also n\u00d7B to ensure mathematical (linear algebra) compliance between the reference sample and the blocks.","The windowing module  generates multiple test samples by moving a window across the each location in the current frame from the frames . Once there is a selected reference sample , a set of blocks , and a test sample , the block comparator module  compares, iteratively, the reference sample  hyperspectral band values with the hyperspectral band values of the blocks . During this process, the reference sample  is compared to each block where each comparison results in a contrast value and the entire set of contrast values is a reference attribute set  of dimensions N\u00d7m, where m is the number of decomposed regions of the spectrum available from the sensor  and is significantly smaller than N. In one embodiment, m=3 representing a decomposition among the regions V, NIR, and SWIR. In other embodiments the number of decomposed portions of the spectrums is selected by the user . An attribute set represents the compartmentalized contrasts between a HS sample [reference or test] and the random blocks of data, where compartmentalization is done according to the sensor's operating regions of the spectrum.","The block comparator  then compares the test sample hyperspectral band values with the hyperspectral band values of the blocks . In an exemplary embodiment, N=200, although other configurations are possible. The comparison of the test samples with the blocks by the block comparator  will generate a test attribute set  of dimensions N\u00d7m (where N now also represents the number of contrasting attributes [a new sample size] and m now also represents the dimension of the attributes, where N is much greater than m), similar to the reference attribute set . Directly comparing the reference sample  sized n\u00d7B (where, for example, n is 14 and B is 112) with the test samples would not result in useful results as the number of pixels n is very low relative to B (dimension of the data), such that proper statistical measures cannot be determined to yield a reliable autonomous decision. For the tracking problem described, the use of Large Sample Theory is by far the most suitable to develop a hypothesis test, where a reliable autonomous decision can be made, but Large Sample Theory cannot be utilized with a low sample size such as n pixels. The sample size n is not only small but significantly smaller than B; ideally, for Large Sample Theory to be used, n must be an order of magnitude greater than B. With the above outlined flow, the reference attribute set and the test attribute set can be compared using Large Sample Theory because the problem is mapped from a small sample case (n much smaller in magnitude than dimension B) to a large sample case (N much greater in magnitude than dimension m). The attribute comparator  will compare the reference attribute set  and the test attribute set  to determine whether a particular test window location in the frames  contains the object represented by the reference sample  as selected by the user . The windowing module  then advances the window, by systematically moving the window by a pixel-length step within the current frame, to choose a new test sample within the current frame, and continues the comparison on a block by block basis until the windowing module  determines that the entire image frame has been processed, i.e., until each possible test sample within the current frame observed by the sliding window is compared to the randomly selected blocks . In accordance with an exemplary embodiment of the present invention, the attribute sets  and  are discriminant object features, i.e., distinct contrast values indirectly representing the reference object  and contrast values indirectly representing the object currently observed by the sliding window , which can be exploited by a Large Sample Theory based hypothesis test (as N is much greater than m) to yield a reliable autonomous decision based on a known family of distributions controlling the hypothesis test's output results.",{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 3","sub":["n\u00d7B ","n\u00d7B ","n\u00d7B","n\u00d7B","n\u00d7B","N\u00d7m","n\u00d7B","n\u00d7B","n\u00d7B","N\u00d7m","N\u00d7m","N\u00d7m"],"b":["304","306","304","306","302","307","308","307","308"],"sup":["(reference) ","(1)","(N)","(reference) ","(1)","(N) ","(test) ","(test) ","(reference) ","(test) ","2 ","2 "],"i":["Testing Statistical Hypotheses\u201d","Testing Statistical Hypotheses\u201d"]},"The individual contrast between Xand the ksample Zmay not be unique, relative to the contrast between Zand some other sample (e.g., X), but the object uniqueness, or distinctness, is achieved by collecting these contrasts between Xand each sample in the series {Z}. In this context, the contrast is attained by computing mean averages of the available HS samples and applying a linear or non-linear metric between the resulting mean averages, where the new attribute set is finally generated for a particular entity by first capitalizing on the entity's sample original data space distinctness, using all of the bands (contrasts between spectral averages), and then on the fact that a sufficiently large number of contrasts will draw a better picture on the distinctness of the particular object in the new attribute space. The samples in the new attribute space (Yor Y) are statistically independent, since the contrasts are computed between a given HS sample (Xor X) and a series {Z}, of randomly selected blocks of data.","Given that an initial cue X, a HS sample, is available as a reference set from image frame f, where n<<B, and another HS sample Xis also available as a testing set taken from another image, frame f+k, where r indexes a particular spatial location in frame f+k. These samples are rearranged as\n\n]\n\n],\u2003\u2003(1)\n\nwhere x\u03b5R, x\u03b5R, B is the number of frequency bands, i=(0,1), j=(1, . . . , n), and the operator (\u2022)means transposed. Xand Xmust be transformed to a feature space where their representations, namely Yand Y, respectively, have a large sample size N relative to m number of components within any vector in the new space, i.e., (N>>m). The vectors within Tshould also be statistically independent, similarly for Y.\n","The independence requirement can be handled by autonomously and randomly selecting N blocks of data Z(b=1, . . . , N) from frame f+k, such that the mean estimate using each available HS sample is decomposed into m distinct regions of the spectrum; and then applying a contrasting method, in this case the Spectral Angle Mapper (SAM) as described in \u201cHyperspectral imagery: clutter adaptation in anomaly detection,\u201d IEEE Trans. Information Theory, August 2000, 46, no. 5, pp. 1855-1871, S. M. Schweizer and J. M. F. Moura as a function of m. In mathematical language, this approach using Xand X, separately, against Z(b=1, . . . , N) yields",{"@attributes":{"id":"p-0030","num":"0029"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msubsup":{"mi":"Y","mrow":[{"mi":["N","m"],"mo":"\u00d7"},{"mo":["(",")"],"mi":"f"}]},"mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msup":{"mi":"y","mrow":{"mrow":[{"mo":["(",")"],"mi":"f"},{"mo":["(",")"],"mn":"1"}],"mo":"\u2062"}}}},{"mtd":{"msup":{"mi":"y","mrow":{"mrow":[{"mo":["(",")"],"mi":"f"},{"mo":["(",")"],"mn":"1"}],"mo":"\u2062"}}}},{"mtd":{"mi":"\u22ee"}},{"mtd":{"msup":{"mi":"y","mrow":{"mrow":[{"mo":["(",")"],"mi":"f"},{"mo":["(",")"],"mi":"N"}],"mo":"\u2062"}}}}]}}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"2"}}]}}}},"br":{},"sup":"(f)(b) "},{"@attributes":{"id":"p-0031","num":"0030"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"msup":{"mi":"y","mrow":{"mrow":[{"mo":["(",")"],"mi":"f"},{"mo":["(",")"],"mi":"b"}],"mo":"\u2062"}},"mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msubsup":{"mi":["y","V"],"mrow":{"mrow":[{"mo":["(",")"],"mi":"f"},{"mo":["(",")"],"mi":"b"}],"mo":"\u2062"}}}},{"mtd":{"msubsup":{"mi":["y","NIR"],"mrow":{"mrow":[{"mo":["(",")"],"mi":"f"},{"mo":["(",")"],"mi":"b"}],"mo":"\u2062"}}}},{"mtd":{"msubsup":{"mi":["y","SWIR"],"mrow":{"mrow":[{"mo":["(",")"],"mi":"f"},{"mo":["(",")"],"mi":"b"}],"mo":"\u2062"}}}},{"mtd":{"msubsup":{"mi":["y","MWIR"],"mrow":{"mrow":[{"mo":["(",")"],"mi":"f"},{"mo":["(",")"],"mi":"b"}],"mo":"\u2062"}}}},{"mtd":{"msub":{"mi":["y","LWIR"]}}}]}}},"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}},"mi":"and"}},{"mrow":{"mo":["(",")"],"mn":"3"}}]},{"mtd":[{"mrow":{"mrow":{"msubsup":{"mi":"Y","mrow":[{"mi":["N","m"],"mo":"\u00d7"},{"mrow":[{"mo":["(",")"],"mrow":{"mi":["f","k"],"mo":"+"}},{"mo":["(",")"],"mi":"r"}],"mo":"\u2062"}]},"mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msup":{"mi":"y","mrow":{"mrow":[{"mo":["(",")"],"mrow":{"mi":["f","k"],"mo":"+"}},{"mo":["(",")"],"mi":"r"},{"mo":["(",")"],"mn":"1"}],"mo":["\u2062","\u2062"]}}}},{"mtd":{"msup":{"mi":"y","mrow":{"mrow":[{"mo":["(",")"],"mrow":{"mi":["f","k"],"mo":"+"}},{"mo":["(",")"],"mi":"r"},{"mo":["(",")"],"mn":"2"}],"mo":["\u2062","\u2062"]}}}},{"mtd":{"mi":"\u22ee"}},{"mtd":{"msup":{"mi":"y","mrow":{"mrow":[{"mo":["(",")"],"mrow":{"mi":["f","k"],"mo":"+"}},{"mo":["(",")"],"mi":"r"},{"mo":["(",")"],"mi":"N"}],"mo":["\u2062","\u2062"]}}}}]}}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}]}}},"br":{},"sup":"(f+k)(r)(b)"},{"@attributes":{"id":"p-0032","num":"0031"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msup":[{"mi":"y","mrow":{"mrow":[{"mo":["(",")"],"mrow":{"mi":["f","k"],"mo":"+"}},{"mo":["(",")"],"mi":"r"},{"mo":["(",")"],"mi":"b"}],"mo":["\u2062","\u2062"]}},{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msubsup":{"mi":["y","V"],"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"mi":["f","k"],"mo":"+"}},{"mo":["(",")"],"mi":"r"},{"mo":["(",")"],"mi":"b"}],"mo":["\u2062","\u2062"]}}}},{"mtd":{"msubsup":{"mi":["y","NIR"],"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"mi":["f","k"],"mo":"+"}},{"mo":["(",")"],"mi":"r"},{"mo":["(",")"],"mi":"b"}],"mo":["\u2062","\u2062"]}}}},{"mtd":{"msubsup":{"mi":["y","SWIR"],"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"mi":["f","k"],"mo":"+"}},{"mo":["(",")"],"mi":"r"},{"mo":["(",")"],"mi":"b"}],"mo":["\u2062","\u2062"]}}}},{"mtd":{"msubsup":{"mi":["y","MWIR"],"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"mi":["f","k"],"mo":"+"}},{"mo":["(",")"],"mi":"r"},{"mo":["(",")"],"mi":"b"}],"mo":["\u2062","\u2062"]}}}},{"mtd":{"msubsup":{"mi":["y","LWIR"],"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"mi":["f","k"],"mo":"+"}},{"mo":["(",")"],"mi":"r"},{"mo":["(",")"],"mi":"b"}],"mo":["\u2062","\u2062"]}}}}]}},"mi":"t"}],"mo":"="},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"5"}}]}}}},"br":{}},{"@attributes":{"id":"p-0033","num":"0032"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msubsup":{"mi":["y","V"],"mrow":{"mo":["(",")"],"mi":"b"}},"mo":"=","mrow":{"mi":"arc","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"cos","mo":["(",")"],"mfrac":{"mrow":[{"msubsup":[{"mover":{"mi":"\u03bc","mo":"^"},"mi":["V","T"]},{"mover":{"mi":"\u03bc","mo":"^"},"mi":"V","mrow":{"mo":["(",")"],"mi":"b"}}],"mo":"\u2062"},{"mrow":[{"mo":["\uf605","\uf606"],"msub":{"mover":{"mi":"\u03bc","mo":"^"},"mi":"V"}},{"mo":["\uf605","\uf606"],"msubsup":{"mover":{"mi":"\u03bc","mo":"^"},"mi":"V","mrow":{"mo":["(",")"],"mi":"b"}}}],"mo":"\u2062"}]}}}},{"msubsup":{"mi":["y","NIR"],"mrow":{"mo":["(",")"],"mi":"b"}},"mo":"=","mrow":{"mi":"arc","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"cos","mo":["(",")"],"mfrac":{"mrow":[{"msubsup":[{"mover":{"mi":"\u03bc","mo":"^"},"mi":["NIR","T"]},{"mover":{"mi":"\u03bc","mo":"^"},"mi":"NIR","mrow":{"mo":["(",")"],"mi":"b"}}],"mo":"\u2062"},{"mrow":[{"mo":["\uf605","\uf606"],"msub":{"mover":{"mi":"\u03bc","mo":"^"},"mi":"NIR"}},{"mo":["\uf605","\uf606"],"msubsup":{"mover":{"mi":"\u03bc","mo":"^"},"mi":"NIR","mrow":{"mo":["(",")"],"mi":"b"}}}],"mo":"\u2062"}]}}}},{"msubsup":{"mi":["y","SWIR"],"mrow":{"mo":["(",")"],"mi":"b"}},"mo":"=","mrow":{"mi":"arc","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"cos","mo":["(",")"],"mfrac":{"mrow":[{"msubsup":[{"mover":{"mi":"\u03bc","mo":"^"},"mi":["SWIR","T"]},{"mover":{"mi":"\u03bc","mo":"^"},"mi":"SWIR","mrow":{"mo":["(",")"],"mi":"b"}}],"mo":"\u2062"},{"mrow":[{"mo":["\uf605","\uf606"],"msub":{"mover":{"mi":"\u03bc","mo":"^"},"mi":"SWIR"}},{"mo":["\uf605","\uf606"],"msubsup":{"mover":{"mi":"\u03bc","mo":"^"},"mi":"SWIR","mrow":{"mo":["(",")"],"mi":"b"}}}],"mo":"\u2062"}]}}}},{"msubsup":{"mi":["y","MWIR"],"mrow":{"mo":["(",")"],"mi":"b"}},"mo":"=","mrow":{"mi":"arc","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"cos","mo":["(",")"],"mfrac":{"mrow":[{"msubsup":[{"mover":{"mi":"\u03bc","mo":"^"},"mi":["MWIR","T"]},{"mover":{"mi":"\u03bc","mo":"^"},"mi":"MWIR","mrow":{"mo":["(",")"],"mi":"b"}}],"mo":"\u2062"},{"mrow":[{"mo":["\uf605","\uf606"],"msub":{"mover":{"mi":"\u03bc","mo":"^"},"mi":"MWIR"}},{"mo":["\uf605","\uf606"],"msubsup":{"mover":{"mi":"\u03bc","mo":"^"},"mi":"MWIR","mrow":{"mo":["(",")"],"mi":"b"}}}],"mo":"\u2062"}]}}}},{"msubsup":{"mi":["y","LWIR"],"mrow":{"mo":["(",")"],"mi":"b"}},"mo":"=","mrow":{"mi":"arc","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"cos","mo":["(",")"],"mfrac":{"mrow":[{"msubsup":[{"mover":{"mi":"\u03bc","mo":"^"},"mi":["V","T"]},{"mover":{"mi":"\u03bc","mo":"^"},"mi":"V","mrow":{"mo":["(",")"],"mi":"b"}}],"mo":"\u2062"},{"mrow":[{"mo":["\uf605","\uf606"],"msub":{"mover":{"mi":"\u03bc","mo":"^"},"mi":"LWIR"}},{"mo":["\uf605","\uf606"],"msubsup":{"mover":{"mi":"\u03bc","mo":"^"},"mi":"LWIR","mrow":{"mo":["(",")"],"mi":"b"}}}],"mo":"\u2062"}]}}}}],"mo":[",","\u2062",",","\u2062",",","\u2062",",","\u2062",","],"mstyle":[{"mtext":{}},{"mtext":{}},{"mtext":{}},{"mtext":{}}]}},{"mrow":{"mo":["(",")"],"mn":"6"}}]}}}},"br":{},"sub":["V","NIR","SWIR","MWIR","LWIR","X","V","NIR","SWIR","MWIR","LWIR","Z","V","NIR","SWIR","MWIR","LWIR","X ","n\u00d7B","n\u00d7B","Z","n\u00d7B","V","NIR","SWIR","MWIR","LWIR","V","NIR","SWIR","MWIR","LWIR","1","2","3","4","5"],"sup":["(b)","(b)","(b)","(b)","(b)","t","B","(b)","(b)","(b)","(b)","(b)","(b)","t","B","(f) ","(f+k)(r)","(b) ","(f+k)(b) ","B",{"sub2":"1"},"B",{"sub2":"2"},"B",{"sub2":"3"},"B",{"sub2":"4"},"B",{"sub2":"5"},"(b)","B",{"sub2":"1"},"(b)","B",{"sub2":"2"},"(b)","B",{"sub2":"3"},"(b)","B",{"sub2":"4"},"(b)","B",{"sub2":"5"}]},"By setting m=5, it is assumed that measurements are being made in all 5 regions of the spectrum. In exemplary embodiments m will likely be set to m=1 (MWIR or LWIR), m=2 (V and NIR), or m=3 (V, NIR, and SWIR). Notice in (6) that the mean averages of X, X, and Z(b=1, . . . , N) are decomposed into the wavelengths that correspond to specific regions of the spectrum (e.g., V, NIR, SWIR). The sample size of Zdoes not necessarily need to be equal to n, since results are reduced as shown in (6) by contrasting Xand X, separately, against each Z. Finally, in equations (2) and (4) that the sample size N can be freely set to be significantly larger than m. Equipped with a large sample size statistical problem, a strong hypothesis test can now be utilized to function as a binary classifier, as shown next.","Using (2) and (4), let",{"@attributes":{"id":"p-0036","num":"0035"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"msubsup":{"mi":"\u0394","mrow":[{"mi":["N","m"],"mo":"\u00d7"},{"mrow":[{"mo":["(",")"],"mrow":{"mi":"f","mo":",","mrow":{"mi":["f","k"],"mo":"+"}}},{"mo":["(",")"],"mi":"r"}],"mo":"\u2062"}]},"mo":["=","\u2062"],"mi":{},"mrow":{"msubsup":[{"mi":"Y","mrow":[{"mi":["N","m"],"mo":"\u00d7"},{"mo":["(",")"],"mi":"f"}]},{"mi":"Y","mrow":[{"mi":["N","m"],"mo":"\u00d7"},{"mrow":[{"mo":["(",")"],"mrow":{"mi":["f","k"],"mo":"+"}},{"mo":["(",")"],"mi":"r"}],"mo":"\u2062"}]}],"mo":"-"}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msubsup":{"mi":"\u0394","mrow":[{"mi":["I","m"],"mo":"\u00d7"},{"mrow":[{"mo":["(",")"],"mrow":{"mi":"f","mo":",","mrow":{"mi":["f","k"],"mo":"+"}}},{"mo":["(",")"],"mi":"r"},{"mo":["(",")"],"mn":"1"}],"mo":["\u2062","\u2062"]}]}}},{"mtd":{"msubsup":{"mi":"\u0394","mrow":[{"mi":["I","m"],"mo":"\u00d7"},{"mrow":[{"mo":["(",")"],"mrow":{"mi":"f","mo":",","mrow":{"mi":["f","k"],"mo":"+"}}},{"mo":["(",")"],"mi":"r"},{"mo":["(",")"],"mn":"2"}],"mo":["\u2062","\u2062"]}]}}},{"mtd":{"mi":"\u22ee"}},{"mtd":{"msubsup":{"mi":"\u0394","mrow":[{"mi":["I","m"],"mo":"\u00d7"},{"mrow":[{"mo":["(",")"],"mrow":{"mi":"f","mo":",","mrow":{"mi":["f","k"],"mo":"+"}}},{"mo":["(",")"],"mi":"r"},{"mo":["(",")"],"mi":"N"}],"mo":["\u2062","\u2062"]}]}}}]}}}}}]}},{"mrow":{"mo":["(",")"],"mn":"7"}}]}}}},"br":[{},{},{},{},{}],"in-line-formulae":[{},{},{},{},{},{}],"sub":["1\u00d7m","1\u00d7m","1\u00d7m","1\u00d7m","b=1","1\u00d7m","m\u00d7m","N\u00d7m","1\u00d7m","N\u00d7m","1\u00d7m"],"sup":["(f\u00b7f+k)(r)(b)","(f)(b)","(f+k)(r)(b)","(f,f+k)(r)","\u22121","N","(f,f+k)(r)(b)","(f,f+k)(r)","\u22121","(f,f+k)(r)","(f,f+k)(r)","t","(f,f+k)(r)","(f,f+k)(r)"],"i":["=y","\u2212y","b=",", . . . , N","=N","N\u2212"],"o":["\u0394","\u0394","\u0394"]},"Equation (9) represents a normalized sum of independent random variables, where one can utilize a \u03b1\u2212level test of H:\u03b4=0versus H:\u03b4\u22600, rejecting Hif the observed",{"@attributes":{"id":"p-0038","num":"0037"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msup":{"mi":"T","msup":{"mn":"2","mrow":{"mrow":[{"mo":["(",")"],"mrow":{"mi":"f","mo":",","mrow":{"mi":["f","k"],"mo":"+"}}},{"mo":["(",")"],"mi":"r"}],"mo":"\u2062"}}},"mo":"=","mrow":{"mrow":[{"mrow":{"mi":"N","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mover":{"mi":"\u0394","mo":"\u21c0"},"mrow":[{"mi":["I","m"],"mo":"\u00d7"},{"mrow":[{"mo":["(",")"],"mrow":{"mi":"f","mo":",","mrow":{"mi":["f","k"],"mo":"+"}}},{"mo":["(",")"],"mi":"r"}],"mo":"\u2062"}]},"mo":"-","msub":{"mi":"\u03b4","mrow":{"mi":["I","m"],"mo":"\u00d7"}}}}},"mo":["\u2062","\u2062"],"msup":[{"mrow":[{"mo":["(",")"],"msubsup":{"mi":"\u03a3","mrow":[{"mi":["m","m"],"mo":"\u00d7"},{"mrow":[{"mo":["(",")"],"mrow":{"mi":"f","mo":",","mrow":{"mi":["f","k"],"mo":"+"}}},{"mo":["(",")"],"mi":"r"}],"mo":"\u2062"}]}},{"mo":"-","mn":"1"}]},{"mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mover":{"mi":["\u0394","_"]},"mrow":[{"mi":["I","m"],"mo":"\u00d7"},{"mrow":[{"mo":["(",")"],"mrow":{"mi":"f","mo":",","mrow":{"mi":["f","k"],"mo":"+"}}},{"mo":["(",")"],"mi":"r"}],"mo":"\u2062"}]},"mo":"-","msub":{"mi":"\u03b4","mrow":{"mi":["I","m"],"mo":"\u00d7"}}}},"mi":"t"}]},{"mfrac":{"mrow":[{"mrow":{"mo":["(",")"],"mrow":{"mi":"N","mo":"-","mn":"1"}},"mo":"\u2062","mi":"m"},{"mi":["N","m"],"mo":"-"}]},"mo":"\u2062","mrow":{"msub":{"mi":"F","mrow":{"mi":"m","mo":",","mrow":{"mi":["N","m"],"mo":"-"}}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03b1"}}}],"mo":">"}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"11"}}]}}}},"br":{},"sub":["m,N\u2212m","1\u00d7m","1\u00d7m ","n\u00d7B"],"o":"\u0394","sup":["(f,f+k)(r) ","(f) "]},{"@attributes":{"id":"p-0039","num":"0038"},"figref":["FIG. 4","FIG. 4"]},{"@attributes":{"id":"p-0040","num":"0039"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mfrac":{"mrow":[{"mrow":{"mo":["(",")"],"mrow":{"mi":"N","mo":"-","mn":"1"}},"mo":"\u2062","mi":"m"},{"mi":["N","m"],"mo":"-"}]},"mo":"\u2062","mrow":{"msub":{"mi":"F","mrow":{"mi":"m","mo":",","mrow":{"mi":["N","m"],"mo":"-"}}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03b1"}}}}},"br":{}},{"@attributes":{"id":"p-0041","num":"0040"},"figref":"FIG. 5","b":["500","500","200","100","102","502","504"]},"At step  a reference sample is selected by a user viewing the received frame from the sensor  of . The user identifies an object of interest by viewing several images from a sensor and concluding that a particular vehicle, object, person, or the like, should be tracked across multiple frames captured by sensor . The user selects the object of interest as a reference sample, which has the same dimensions as each block in the sensed frame. In exemplary embodiments, the user selection is performed using a mouse cursor over an image frame, voice command, touch sensor, or the like. The method then proceeds to step , where a predetermined number (N) of blocks are extracted from a frame sensed by the hyperspectral sensor  of . The blocks are selected at random, where each pixel within each block has a vector of hyperspectral values for each hyperspectral band that sensor  detects. In step , the reference sample is compared to each of the N blocks extracted from the current frame. The comparison is a contrast between the HS sample mean of the reference sample and the HS sample mean of each randomly selected block of data, generating a contrast-value for the reference sample against each block and, therefore, an attribute set consisting of N contrast values. As the contrasting task is compartmentalized by the sensor's operational regions of the spectrum (e.g., m=2 for V, NIR), the comparison results in a low dimensional (m), having a large sample size (N), reference attribute set.","The method proceeds to step  where a window with the same dimensions as the reference sample is established over the current frame. The window iterates over the entire sensed frame so that every pixel (allowing overlap of the sliding window) is covered, such that, during each iteration, the window highlights a test sample. At step , the content of the window is selected as a test sample. At step , the test sample is also compared to each of the same N blocks of the current frame to generate a test attribute set of the same dimensions (m) and sample size (N) as the reference attribute set. From top to bottom and left to right, the test starts by taking a cue (a reference sample X) from frame  (, element ) of the target, where (using the notation in Section 3) n=6, B=112. The Independent and Indirectly Generated Attributes (IIGA) method, as described earlier, is first applied to the same frame (i.e. f=1, k=0), fixing m=2 (V,NIR), where N=200 (two orders of magnitude above m) blocks of data are autonomously and randomly selected Z(b=1, . . . , N). All of the spatial locations in frame  [r=1, . . . , R, for R=(320)(256)\u2212n] are tested, i.e., X|against X, indirectly, through contrasts against Z(b=1, . . . , N) in order to generate a 2 dimensional output surface using (11). The testing process is repeated for successive frames, using the initially cued target sample Xas the reference sample, and keeping all of the other parameters constant, in order to generate two additional output surfaces from those frames. As an example, X|was tested against X, indirectly, through contrasts against Z(b=1, . . . , N). The output surfaces' pixel values are the output T, as shown in (11), and the interpretation of Trelative to the hypotheses Hand Hand the pseudo color used to display the output surfaces is shown in , for a probability of error \u03b1=0.05, N=200, and m=2, where the cluster of white pixels in a given output surface represent the likely spatial location in the frame where the object of interest is detected.","The method then proceeds to step  to determine whether all pixels in the current frame have been compared against all N blocks. If all pixels in the frame have not been compared, the method returns to step  and iterates the window. If all pixels have been compared, the test attribute set has been fully populated and the method moves to step . At step , the test attribute set is compared to the reference attribute set, where there comparison using equation (11), described above, results in a contrast value. If the contrast value is lower than a cutoff threshold as shown in , the test and reference samples are automatically determined to be a match. At step , if all frames sensed by the sensor  of  have not been compared, then the method returns to step  to compare the same reference sample with test samples from the next frame. If all frames have been compared, the method ends at step .","The foregoing description, for purpose of explanation, has been described with reference to specific embodiments. However, the illustrative discussions above are not intended to be exhaustive or to limit the invention to the precise forms disclosed. Many modifications and variations are possible in view of the above teachings. The embodiments were chosen and described in order to best explain the principles of the present disclosure and its practical applications, to thereby enable others skilled in the art to best utilize the invention and various embodiments with various modifications as may be suited to the particular use contemplated.","Various elements, devices, modules and circuits are described above in associated with their respective functions. These elements, devices, modules and circuits are considered means for performing their respective functions as described herein. While the foregoing is directed to embodiments of the present invention, other and further embodiments of the invention may be devised without departing from the basic scope, thereof, and the scope thereof is determined by the claims that follow."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["So that the manner in which the above recited features of the present invention can be understood in detail, a more particular description of the invention, briefly summarized above, may be had by reference to embodiments, some of which are illustrated in the appended drawings. It is to be noted, however, that the appended drawings illustrate only typical embodiments of this invention and are therefore not to be considered limiting of its scope, for the invention may admit to other equally effective embodiments.",{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 5"}]},"DETDESC":[{},{}]}
