---
title: Scheduler and resource manager for coprocessor-based heterogeneous clusters
abstract: A system and method for scheduling client-server applications onto heterogeneous clusters includes storing at least one client request of at least one application in a pending request list on a computer readable storage medium. A priority metric is computed for each application, where the computed priority metric is applied to each client request belonging to that application. The priority metric is determined based on estimated performance of the client request and load on the pending request list. The at least one client request of the at least one application is scheduled based on the priority metric onto one or more heterogeneous resources.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08984519&OS=08984519&RS=08984519
owner: NEC Laboratories America, Inc.
number: 08984519
owner_city: Princeton
owner_country: US
publication_date: 20111013
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["RELATED APPLICATION INFORMATION","BACKGROUND","SUMMARY","DETAILED DESCRIPTION OF PREFERRED EMBODIMENTS","Pseudocode Example 1"],"p":["This application claims priority to provisional application Ser. No. 61\/414,454 filed on Nov. 17, 2010 and provisional application Ser. No. 61\/483,950 filed on May 9, 2011, both applications incorporated herein by reference.","1. Technical Field","The present invention relates to coprocessor-based heterogeneous clusters, and more particularly, to a scheduler and resource manager for coprocessor-based heterogeneous clusters.","2. Description of the Related Art","Coprocessor-based heterogeneous clusters are those whose nodes have one or more coprocessors as accelerators. Coprocessors may include, e.g., graphics processing units (GPUs). Such heterogeneous clusters are increasingly being deployed to accelerate non-graphical compute kernels, such as large scientific and compute-intensive jobs. At the same time, client-server applications are becoming more computationally intensive. In a client-server application, an important metric is response time, or the latency per request. Latency per request can be improved by using, e.g., GPUs.","For better utilization, multiple client-server applications should be able to concurrently run and share heterogeneous clusters (i.e., the cluster should support multi-tenancy). Further, the response times of the heterogeneous clusters for processing client requests should be largely immune to load variations and unpredictable load spikes. Thus, any practical heterogeneous cluster infrastructure should be able to handle multi-tenancy and varying load while delivering an acceptable response time for as many client requests as possible. For a heterogeneous cluster to handle client-server applications with load spikes, a scheduler that enables dynamic sharing of coprocessor-based heterogeneous resources is necessary.","A system of a scheduler for scheduling client-server applications onto heterogeneous clusters includes a pending request list configured to store at least one client request of at least one application on a computer readable storage medium. A priority metric module is configured to compute a priority metric for each application and the computed priority metric is applied to each client request belonging to that application. The priority metric is determined based on estimated performance of the client request and load on the pending request list. The scheduler is configured to schedule the at least one client request of the at least one application based on the priority metric onto one or more heterogeneous resources.","A system of a scheduler for scheduling client-server applications onto heterogeneous clusters includes a performance estimator module configured to dynamically model performance of at least one client request of a new application on the heterogeneous resources. A pending request list is configured to store at least one client request of at least one application on a computer readable storage medium. A priority metric module is configured to compute a priority metric for each application and the computed priority metric is applied to each client request belonging to that application. The priority metric is determined based on estimated performance of the client request and load on the pending request list. The scheduler is configured to pack more than one client requests belonging to an application together and schedule at least one client request of the at least one application based on the priority onto one or more heterogeneous resources.","A method for scheduling client-server applications onto heterogeneous clusters includes storing at least one client request of at least one application in a pending request list on a computer readable storage medium. A priority metric is computed for each application and the computed priority metric is applied to each client request belonging to that application. The priority metric is determined based on estimated performance of the client request and load on the pending request list. The at least one client request of the at least one application is scheduled based on the priority metric onto one or more heterogeneous resources.","These and other features and advantages will become apparent from the following detailed description of illustrative embodiments thereof, which is to be read in connection with the accompanying drawings.","In accordance with the present principles, systems and methods are provided for the scheduling and resource managing of coprocessor-based heterogeneous resources. In one embodiment, coprocessor-based heterogeneous resources include graphics processing units (GPUs). Advantageously, GPUs provide for a faster resource while being able to be easily added to or removed from existing systems with low setup and operational costs. A middleware framework is provided, which includes a cluster manager and several worker nodes, to enable efficient sharing of coprocessor-based heterogeneous cluster resources. Each worker node includes at least one central processing unit (CPU) and at least one GPU. The cluster manager, which executes the cluster-level scheduler, receives client requests and schedules them on available worker nodes. Each worker node includes a node-level dispatcher that intercepts and dispatches the scheduled tasks to local resources (e.g., CPUs and\/or GPUs) as directed by the cluster manager. The term \u201ctasks\u201d will be used synonymously with the term \u201cclient requests\u201d throughout this application. After a client request is processed by the worker nodes, the cluster manager may consolidate the results from the individual worker nodes before sending them back to the clients. The present principles enable the efficient sharing of heterogeneous cluster resources while delivering acceptable client request response times despite load spikes.","In one embodiment, scheduling is based on a priority metric such that the tasks of the application with the highest priority metric are selected first for immediate scheduling. The priority metric may be computed based on the actual achieved performance for recently processed tasks, the desired performance, the number of unprocessed requests, the number and types of resources allocated, and the average processing time needed to process each queue item. Alternatively, the priority metric may be computed based on the request's slack, the expected processing time, and the load on each application. In another embodiment, client requests belonging to the selected application are packed together before being dispatched to the worker nodes. In yet another embodiment, a dynamic data collection for building CPU\/GPU performance models is performed for each new application to find suitable resources for that application and optimize performance by estimating performance of a task of an application on the heterogeneous resources.","Embodiments described herein may be entirely hardware, entirely software or including both hardware and software elements. In a preferred embodiment, the present invention is implemented in software, which includes but is not limited to firmware, resident software, microcode, etc.","Embodiments may include a computer program product accessible from a computer-usable or computer-readable medium providing program code for use by or in connection with a computer or any instruction execution system. A computer-usable or computer readable medium may include any apparatus that stores, communicates, propagates, or transports the program for use by or in connection with the instruction execution system, apparatus, or device. The medium can be magnetic, optical, electronic, electromagnetic, infrared, or semiconductor system (or apparatus or device) or a propagation medium. The medium may include a computer-readable storage medium such as a semiconductor or solid state memory, magnetic tape, a removable computer diskette, a random access memory (RAM), a read-only memory (ROM), a rigid magnetic disk and an optical disk, etc.","A data processing system suitable for storing and\/or executing program code may include at least one processor coupled directly or indirectly to memory elements through a system bus. The memory elements can include local memory employed during actual execution of the program code, bulk storage, and cache memories which provide temporary storage of at least some program code to reduce the number of times code is retrieved from bulk storage during execution. Input\/output or I\/O devices (including but not limited to keyboards, displays, pointing devices, etc.) may be coupled to the system either directly or through intervening I\/O controllers.","Network adapters may also be coupled to the system to enable the data processing system to become coupled to other data processing systems or remote printers or storage devices through intervening private or public networks. Modems, cable modem and Ethernet cards are just a few of the currently available types of network adapters.","Referring now to the drawings in which like numerals represent the same or similar elements and initially to , a high-level system architecture of scheduler  is illustratively depicted in accordance with one embodiment. Scheduler  provides scheduling of application tasks onto heterogeneous resources to allow each task to achieve its desired Quality of Service (QoS). In the present application, the QoS refers to client request response time. Each application  specifies an acceptable response time for the user requests. All applications  have a client interface  and a server portion. The server portion is mapped to specific cluster nodes and is expected to be running with scheduler . It is assumed that applications  already have optimized CPU and GPU implementations available in the form of runtime libraries with well-defined interfaces. This enables scheduler  to intercept calls at runtime and dispatch them to the heterogeneous resources.","Client interface  receives requests from multiple concurrently running applications , and sends them to cluster manager  to schedule them on worker nodes . Scheduler  consists of two distinct portions: cluster-level scheduler  and node-level dispatcher . Cluster-level scheduler  is run on cluster manager  to manage worker nodes . Cluster manager  may be a dedicated general-purpose multicore server node. Worker nodes  may be back-end servers.","Worker nodes  include heterogeneous computational resources comprising, e.g., conventional multicores and Compute Unified Device Architecture (CUDA) enabled GPUs. Advantageously, GPUs can be easily added to or removed from existing systems with low setup and operational costs. Cluster manager  and worker nodes  may be inter-connected using a standard interconnection network (e.g., gigabit Ethernet).","Referring now to , a system of a scheduler and resource manager for coprocessor-based heterogeneous resources  is illustratively depicted in accordance with one embodiment. To use the system of a scheduler and resource manager for coprocessor-based heterogeneous resources , each application  initially registers itself with cluster-level scheduler  and sends notifications to cluster-level scheduler  each time it receives a client request. New requests are stored in pending request list . Application  waits until priority metric module  of cluster-level scheduler  indicates that the request can be processed before dispatching the requests. Once requests have completed processing on worker nodes , application  informs cluster-level scheduler , which updates history table  and resource map . In doing so, applications  may utilize two threads: one to notify cluster-level scheduler  of new requests and the other to ask if requests can be issued and inform the cluster-level scheduler  of completion.","Each application may provide information through an Application Programming Interface (API). First, applications  register with cluster-level scheduler  using the newAppRegistration( ) API and specifies a number of arguments. For example, applications  may specify its expected response time for each client request, the average number of client requests it expects to receive each second, the set of cluster nodes onto which its static data has been mapped, and how many nodes each request will require for processing. Optionally, applications  may also specify how many requests it can consolidate together. For each new client request, applications  notify cluster-level scheduler  using the newRequestNotification( ) API, specifying the size of the request as an argument. Cluster-level scheduler  then provides the go-ahead to applications  for issuing pending requests using the canIssueRequests( ) API and provides a unique identifier for this set of requests as an argument. Applications  then issue the requests and informs cluster-level scheduler  of its completion using the API requestComplete( ) specifying the unique identifier. The APIs and arguments of the above-described application programming interface are illustratively depicted in Table 1. It is noted that the APIs and arguments used by cluster-level scheduler  are application specific. The APIs and arguments discussed herein are illustrative and not meant to be limiting.",{"@attributes":{"id":"p-0029","num":"0028"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"266pt","align":"center"}},"thead":{"row":{"entry":"TABLE 1"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Example list of APIs and arguments"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"98pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"112pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["API","ARGUMENTS","DESCRIPTION"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":["void newAppRegistration( )",{},"Application registers with scheduler"]},{"entry":[{},"float","Expected latency (ms) for each"]},{"entry":[{},"response_time","client request"]},{"entry":[{},"float","Average number of requests"]},{"entry":[{},"average_load","expected every second"]},{"entry":[{},"int * nodelist","Possible cluster nodes on which a"]},{"entry":[{},{},"client request could be processed"]},{"entry":[{},"int nodelist_size","Size of above nodelist"]},{"entry":[{},"int num_nodes","Number of nodes necessary to"]},{"entry":[{},{},"process a client request"]},{"entry":[{},"int consolidate","Number of requests that can be"]},{"entry":[{},{},"consolidated by application"]},{"entry":["void newRequestNotification( )",{},"Application notifies scheduler of"]},{"entry":[{},{},"the arrival of a new request"]},{"entry":[{},"int size","Size of data sent by request"]},{"entry":["bool canIssueRequests( )",{},"Application asks scheduler if"]},{"entry":[{},{},"requests can be issued"]},{"entry":[{},"int * num_reqs","Number of consecutive requests"]},{"entry":[{},{},"that can be consolidated and issued"]},{"entry":[{},"int * id","Unique scheduler ID for this set of"]},{"entry":[{},{},"requests"]},{"entry":[{},"int * nodes","Which cluster nodes to use"]},{"entry":[{},"int * resources","Which resources to use within each"]},{"entry":[{},{},"cluster node"]},{"entry":["void requestComplete( )",{},"Application informs scheduler that"]},{"entry":[{},{},"issued requests have completed"]},{"entry":[{},{},"processing"]},{"entry":[{},"int id","Scheduler ID pertaining to the"]},{"entry":[{},{},"completed requests"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}]}}},"Continuing to refer to , the architecture of cluster-level scheduler  may include the following components: pending request list , resource map , history table , performance estimator , priority metric module  and a load balancer, which is not shown in . Cluster-level scheduler  decides which application  should issue requests, how many requests application  should consolidate, which worker node  the request should be sent to, and which processing resource (e.g., CPU  or GPU ) in worker node  should process the requests. As described above, cluster-level scheduler  communicates with applications  using an API.","Cluster-level scheduler  receives incoming application requests from applications  and stores them in pending request list . Pending request list  stores additional information pertaining to the pending requests, such as the application that received the request, the time at which the request was received, the deadline by which the request should complete, and the size of the request data. The pending request list  may be used to determine the input load on each application  and to keep track of the average task arrival rate for each application . The average task arrival rate is used to determine if a particular application  is experiencing a load spike.","History table  stores the details of recently completed tasks of each application . Each entry of history table  may include executed user requests, resources allocated, and the actual time taken by the allocated resources to execute the user requests. History table  is updated each time an application task is completed.","The information stored in history table  is used to dynamically build a linear performance model using performance estimator . The goal is to estimate performance on the CPU  or GPU  so that the requests can be issued with minimal QoS failures. After collecting request sizes and corresponding execution times in history table , performance estimator  fits the data into a linear model to obtain CPU or GPU performance estimations based on request sizes. The model is dependent on the exact type of CPU or GPU. For different generations of CPUs and GPUs, a model can be developed for each specific kind. In addition, existing analytical models can also be used to estimate the execution time of an application on available resources.","Resource map  includes the current resource allocation information for each application, including a map of the applications that are being executed on each particular resource. This information is used by cluster-level scheduler  to make scheduling decisions. Resource map  is also used to determine the load on each worker and to balance the load across the entire cluster.","Cluster-level scheduler  may also include a load balancer, which is not shown in . It is assumed that the server components of applications  and any static application data are pre-mapped to the cluster nodes. Client requests can be processed by a subset of these nodes and application  tells cluster-level scheduler  how many nodes are requested to process the request (through the API). When cluster-level scheduler  directs an application  to issue requests, cluster-level scheduler  provides a list of the least loaded cluster nodes. The application  is expected to issue its requests to these nodes and thus maintain overall load balancing.","Priority metric module  generates a priority metric (PM) to adjust the allocated resources for each application  so that tasks may be completed within an acceptable QoS. The goal of priority metric module  is to indicate which of the applications  is most critical and which resources (e.g., CPU  or GPU ) should process that request. The PM is computed for each application rather than for each individual task. The PM of each application is then applied for all tasks belonging to that application. An illustrative priority metric is provided for in equations (1) and (2), in accordance with one embodiment, as follows:",{"@attributes":{"id":"p-0037","num":"0036"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"msub":{"mi":["PM","app"]},"mo":"=","mrow":{"mi":["ratio","between","actual","and","required","QoS","queue","items","per","allocated","resources","processing","time","per","queue","item"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u00d7","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u00d7","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]}}},{"mrow":{"mo":["(",")"],"mn":"1"}}]},{"mtd":[{"mrow":{"mstyle":{"mspace":{"@attributes":{"width":"4.4em","height":"4.4ex"}}},"mo":"\u2062","mrow":{"msub":{"mi":["PM","app"]},"mo":"=","mrow":{"mfrac":[{"mi":["AQoS","QoS"]},{"mi":["NR","RA"]}],"mo":["\u00d7","\u00d7"],"mi":"T"}}}},{"mrow":{"mo":["(",")"],"mn":"2"}}]}]}}},"br":{},"b":"205"},"An application lagging behind in meeting the QoS or experiencing a spike in the user requests results in a higher PM value by priority metric module . A higher PM value leads to an increase in priority and more computational resources are allocated to enable it to meet the QoS.","In another embodiment, the PM is a function of three dynamic parameters: the request's slack, the request's expected processing time, and the load on each application. The heterogeneous cluster has r types of resources in each node, labeled Rthrough R. For example, if a node has 1 CPU and 1 GPU, r is 2. Additionally, it is assumed that all nodes to which an application is mapped are identical.","The application itself is responsible for actual request consolidation, but scheduler  indicates how many requests can be consolidated. To do this, scheduler  is aware of the maximum number of requests MAXthat application A can consolidate. So if A is the most critical application, scheduler  simply directs it to consolidate the minimum of MAXor nrequests, where nis the number of requests in the pending request list for application A.","The request's slack represents how long the request can be pending before it is to be processed to satisfy response time constraints. The slack for request k of application A on resource R is provided in equation (3).\n\nSlack\u2212()\u2003\u2003(3)\n\nwhere DLis the deadline for request k of application A, CT is the current time, and EPTis the estimated processing time of request k of application A on resource R.\n","In one embodiment, the requesting task's estimated processing time may be computed by performance estimator  by dynamically building a linear performance model based on its historical performance in history table . Alternatively, existing analytical models can also be used to estimate processing time of a task. Initially, in the absence of historical information, EPTis assumed to be zero. Resource R is either the CPU  or GPU . If the system has different types of CPUs and GPUs, then each type would be a resource since it would result in a different estimated processing time. A zero slack indicates the request should be issued immediately, while a negative slack indicates the request is overdue.","Given the slack, urgency of request k of application A on resource R is provided in equation (4). Urgency increases exponentially as the slack nears zero.\n\n=2\u2003\u2003(4).\n","To account for load spikes, the load for each application A is calculated using the average number of pending requests in the queue (n) and the average number of requests expected every second (navg) specified at the time of application registration. Application registration is discussed above with respect to the API. Load is provided for in equation (5).\n\navg\u2003\u2003(5).\n","The urgency of issuing the requests of application A on R is the product of the urgency of issuing the first pending request of A and the load of A. This is provided in ).",{"@attributes":{"id":"p-0046","num":"0045"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":"U","mrow":{"mi":["A","R"],"mo":","}},"mo":"=","mrow":{"mo":"{","mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"msub":[{"mi":"U","mrow":{"mn":"1","mo":[",",","],"mi":["A","R"]}},{"mi":["L","A"]}],"mo":"\u00d7"},"mo":","}},{"mrow":{"mi":["if","R","is","available"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]}}]},{"mtd":[{"mrow":{"mrow":{"mo":"-","mi":"\u221e"},"mo":","}},{"mrow":{"mi":"otherwise","mo":"."}}]}]}}}},{"mrow":{"mo":["(",")"],"mn":"6"}}]}}}}},"The overall urgency for issuing A's request is the minimum urgency across all available resources R. The urgency for issuing A's request is provided in equation (7), where r is the number of different types of resources in each cluster node.\n\n=min(||)\u2003\u2003(7).\n","Given the urgency for all applications, scheduler  will request application A to consolidate and issue q requests to resource R such that application A has the highest urgency U, q is the minimum of MAXand n, and R is the resource which, when scheduled on which application A, has minimum urgency. It is noted that if the request falls behind in meeting its deadline, its urgency sharply increases (equation (4)). If an application experiences a load spike, its urgency sharply increases (equation (6)). Request issuance is predicated on resource availability (equation (6)). The resource with the best chance of achieving the deadline is chosen since that resource will have the lowest urgency (equation (7)). Pseudocode example 1 shows one illustrative approach to implement the priority metric, in accordance with one embodiment.",{"@attributes":{"id":"p-0049","num":"0048"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Input: appList, reqList, resList, DL, EPT"]},{"entry":[{},"Output: app, q, R"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"for all A in appList do"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"175pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"for all k in reqList do"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"161pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"slack= calculateSlack(DL,EPT)"]},{"entry":[{},"U= calculateReqUrgency(slack)"]},{"entry":[{},"if k \u2267 MAXthen"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"147pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"break"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"161pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"end if"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"175pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"end for"]},{"entry":[{},"n= getAppReqCount(A)"]},{"entry":[{},"navg= getAvgAppReqCount(A)"]},{"entry":[{},"L= n\/ navg"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"end for"]},{"entry":[{},"for all A in appList do"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"175pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"k = getEarliestRequest(A)"]},{"entry":[{},"for all R in resList do"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"161pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"if res Available (R) then"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"147pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"U= UL"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"161pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"else"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"147pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"U= \u2212\u221e"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"161pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"end if"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"175pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"end for"]},{"entry":[{},"U= getMinimum(U,resList)"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"end for"]},{"entry":[{},"\/* Select application app to issue q requests to resource R *\/"]},{"entry":[{},"app = getAppHighestUrgency( )"]},{"entry":[{},"R = getResWithLowestUrgency(app)"]},{"entry":[{},"q = MIN(MAXapp,napp)"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}}},"Tasks of applications  are sorted based on their priority metric, with the application that has the highest priority metric given the highest priority and selected for immediate scheduling. Cluster-level scheduler  then checks history table  for the recently completed QoS for the selected application  to determine if the application  is currently providing an acceptable QoS. If the application  is adequately providing an acceptable QoS and the number of user requests in the pending request list  is less than the normal input load for that application , then no change is made to the resources allocated for that application . However, if history table  shows that recent tasks for that application  have not had an acceptable QoS, or if the number of the user requests in pending request list  is more than the normal input load, then more resources are allocated for that application .","Once scheduling of the application  with the highest priority metric is completed, cluster-level scheduler  selects the next application  with the highest priority metric and schedules the requests for that application  in the same manner. The scheduling iteration is repeated until all applications have been scheduled.","If no free resources are available, then resources are made available by de-allocating the resources from the application  with the minimum priority metric and re-allocating those resources to the particular application  with the higher priority metric that needs more resources to provide an acceptable QoS. It is noted that if the application  with the minimum priority metric needs more resources, then the number of resources in the heterogeneous cluster are not sufficient to provide an acceptable QoS for all the hosted applications . Cluster-level scheduler  prioritizes applications  based on the priority metric and the application  with the highest priority metric is selected. Therefore, the resources that have already been allocated will not have to be reduced for any application , even if it exceeds in providing an acceptable QoS. All other applications  with lesser priority metrics than the selected application's priority metric should also exceed in providing their respective performance threshold. Additionally, although the user request within the application  may be serviced in first in, first out order, the user requests across different applications  may be served out-of-order based on the priority metric and performance of each application .","Node-level dispatchers  include call interception module  and dispatcher  and run on each worker node . Node-level dispatchers  are responsible for receiving an issued request and directing it to the correct resource (e.g., CPU  and\/or GPU ) as specified by cluster-level scheduler . It is assumed that parallelizable kernels in the applications  have both CPU and GPU implementations available as dynamically loadable libraries. Node-level dispatchers  uses call interception module  to intercept the call to the kernel. At runtime, dispatcher  directs the call to CPU  and\/or GPU . If a task is executed on more than one computational resource, then the partial results produced by each resource are merged locally by cluster manager . The merged results are sent back to the client interface server.","Each application task is run until completion on the specified resources. Cluster manager  maintains a thread pool where one thread executes the specified application  on the given heterogeneous resource. Cluster manager  keeps track of the resources that are currently being allocated to a particular application. This allows the cluster manager  to initialize the application  on the allocated resources and warm the resource memory with the application data. This may be needed if the specific resource was previously vacant or was allocated to a different application . If the same resources are being used for executing the next application tasks, then all application data that may be needed is already in its memory and there is no need to copy the application data to the resource memory.","The scheduler and resource manager for coprocessor-based heterogeneous resources include at least three main threads running concurrently to provide scheduling. The performance collector thread dynamically builds CPU\/GPU performance models of tasks of a new application on different resources to find suitable resources and optimize performance of the heterogeneous resources. The scheduler thread performs the core of the scheduling and dispatching. The receiver thread receives results and updates history table  and resource map . Additionally, there is a listener thread that adds incoming client requests to pending request list .","Referring now to , a block\/flow diagram of scheduler thread  is illustratively depicted in accordance with one embodiment. Scheduler thread  performs the core of the scheduling and dispatching. This allows the middleware framework to intelligently manage and utilize the heterogeneous resources. Initially, in block , scheduler thread  will get the number and size of items in pending request list  of . Pending request list  stores requests from each application . The stored metadata may include the application id, the item arrival time, size of the user request data, application to node mapping, and a pointer to the actual data of the request. In block , scheduler thread  will determine if pending request list  is empty. If pending request list  is empty, scheduler thread  will continue to check for the number and size of items until there is at least one item in pending request list .","If pending request list  is not empty, a priority metric is computed for each application  in pending request list  in block . The priority metric allows scheduler thread  to reorder requests across different applications so that the requests achieve an acceptable QoS. In one embodiment, the PM is determined based on the actual achieved performance for recently processed requests, desired performance, the number of unprocessed requests in pending request list , the number and types of resources allocated, and the average processing time required to process each queue item. In another embodiment, the PM is a function of the request's slack, the request's expected processing time, and the load on each application . Tasks are arranged using the PM, with the task with the highest PM given the highest priority.","In block , the application  with the highest priority metric is selected for immediate scheduling. Tasks of the selected application  are packed together in block , which may result in improved performance. Scheduler thread  may pack as many pending requests together as possible. In block , resources are allocated based on QoS and availability. To allocate resources, cluster-level scheduler  consults history table  and resource map  of . Resource map  of  is updated in block  to reflect the resource allocation. In block , the packed tasks are dispatched to the allocated resources. Cluster-level scheduler  gets the number and type of resources where the request should be executed from resource map  and sends the request to node-level dispatchers , which will dispatch the tasks on CPU  and\/or GPU  as indicated by cluster-level scheduler . Once the task has been dispatched, scheduler thread  repeats this process by selecting the next application  with the highest PM. This process is repeated until all tasks have been dispatched.","Referring now to , a block\/flow diagram of the performance collector thread  is illustratively depicted in accordance with one embodiment. The performance collector thread uses performance estimator  of  to dynamically build CPU\/GPU performance models dynamically. In block  of , performance collector thread  checks to determine whether an application  is newly registered with cluster manager . The performance of new applications  on the different resources is unknown. Performance collector thread  performs a learning phase when executing the initial user request of a new application  to find suitable resources for the new application  and optimize performance. If the application  is not new, performance collector thread  will continue to check for new applications .","In block , if an application  is new, performance collector thread  requests resources from resource map  of . Resources may include CPU and\/or GPU resources. The performance of the application tasks will be evaluated on these resources. In block , tasks are dispatched to the respective CPU or GPU resource. Dispatching may be performed by dispatcher  of . In block , performance is measured. Node-level dispatcher  measures the performance of the task on the particular resource and sends the results to cluster-level scheduler . In block , history table  of  is updated. Cluster-level scheduler  may update history table  with application, resource, and performance information. This information may be used for the scheduling of future requests. In block , if there are not enough history table  entries, the performance collector thread will request another CPU or GPU resource from resource map  of  and the process will be repeated. A single entry, for example, in history table  may be sufficient. If there are enough history table  entries, the performance collector thread will end for that application. In another embodiment, each application task is executed on all available resources on a particular node.","Referring now to , a block\/flow diagram of the receiver thread  is illustratively depicted in accordance with one embodiment. In block , receiver thread  checks worker nodes  to determine if results are ready. If no results are ready, receiver thread  will continue to check until there are worker node  results that are ready. If results are ready, receiver thread  will update the cluster-level scheduler . In block , history table  is updated to reflect that actual performance of the tasks on the resources. Actual performance may be the actual QoS. In block , resource map  is updated. Since worker node  has completed its scheduled tasks, resource map  is updated to reflect that the worker node  is available.","Having described preferred embodiments of a scheduler and resource manager for coprocessor-based heterogeneous clusters (which are intended to be illustrative and not limiting), it is noted that modifications and variations can be made by persons skilled in the art in light of the above teachings. It is therefore to be understood that changes may be made in the particular embodiments disclosed which are within the scope of the invention as outlined by the appended claims. Having thus described aspects of the invention, with the details and particularity required by the patent laws, what is claimed and desired protected by Letters Patent is set forth in the appended claims."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF DRAWINGS","p":["The disclosure will provide details in the following description of preferred embodiments with reference to the following figures wherein:",{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 5"}]},"DETDESC":[{},{}]}
