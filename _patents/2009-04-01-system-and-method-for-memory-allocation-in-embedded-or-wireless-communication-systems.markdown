---
title: System and method for memory allocation in embedded or wireless communication systems
abstract: Systems and methods for an improved memory allocation service in embedded or wireless devices. Memory is allocated using a combination of container memory items and referencing memory items.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08321651&OS=08321651&RS=08321651
owner: QUALCOMM Incorporated
number: 08321651
owner_city: San Diego
owner_country: US
publication_date: 20090401
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","BACKGROUND","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT"],"p":["This application claims the benefit of U.S. Provisional Patent Application No. 61\/041,878, filed Apr. 2, 2008, entitled MEMORY ALLOCATION SCHEME FOR EMBEDDED OR WIRELESS COMMUNICATION SYSTEMS. This application is incorporated by reference in its entirety.","1. Field of the Invention","This field relates generally to memory allocation, and more specifically, to memory allocation in embedded or wireless communication systems","2. Description of the Related Art","Memory allocation services make use of pools of memory items. In some cases, memory items are sorted into different pools based on the size of the memory item. For example, one pool might consist of many small memory items while another pool might consist of relatively few large memory items. In response to memory allocation requests, an appropriate data item may be selected from a particular pool and returned to the requesting entity. This system results in significant wasted memory and processor resources. For example, with small memory items, the ratio of header to payload is high resulting in inefficient use of memory. Further, when entire memory items are allocated, significant data portions of any particular data item may go unused. Also, in these allocation services, no statistical multiplexing is available. Processor resources are also consumed due to the chaining of multiple small memory items.","In wireless communication systems, packets are often segmented into small fixed size \u201cradio link\u201d packets, for example 40 bytes, to ensure reliable radio transmission. In order to use memory efficiently, one approach is to create a large pool of relatively small memory items, each holding a 40 byte block, which eventually may be chained together at the higher layers to form larger data blocks (e.g. 1500 byte IP packets). One disadvantage of this service is that some space may be wasted because the memory items may have to be cache row aligned (32 or 64 byte), which may not fit the small radio link packet size. Also, different technologies may share the same memory item pool to reduce to overall memory, in which case the memory item payload size must be chosen to fit the largest radio link packet size which can further increase wastage.","In one embodiment, a system provides for memory management is provided. The system comprising a processor and a memory management service which is executable on the processor. The memory management service configurable to generate first memory items, wherein each of the first memory items comprises a header and a payload, the payload configurable to store a plurality of independently allocatable memory chunks, the header of the first memory items referencing allocatable space in the payload, generate second memory items, wherein each of the second memory items comprises a header referencing one or more memory chunks in the payload of the first memory items, and return a reference from either the header of a first or second memory item responsive to a memory allocation request.","In another embodiment, a method for memory management is provided. The method comprises allocating first memory items, the first memory items each comprising a first header and a payload, the payload comprising chunks of independently allocatable memory, the first header comprising a reference to unallocated memory in the payload allocating second memory items, the second memory items each comprising a second header, the second header comprising a reference to an allocated chunk of memory in the payload of a first memory item, receiving requests for memory allocation, and responding to requests for memory allocation by returning a reference from the header of a first or second memory item.","In another embodiment, a memory management system is provided. The system comprises means for allocating first memory items, the first memory items each comprising a first header and a payload, the payload comprising chunks of independently allocatable memory, the first header comprising a reference to unallocated memory in the payload, means for allocating second memory items, the second memory items each comprising a second header, the second header comprising a reference to an allocated chunk of memory in the payload of a first memory item, means for receiving requests for memory allocation, and means for responding to requests for memory allocation by returning a reference from the header of a first or second memory item.","In another embodiment, a computer readable medium encoded with computer instructions is provided. The instruction, when executed, cause a processor to allocate first memory items, the first memory items each comprising a first header and a payload, the payload comprising chunks of independently allocatable memory, the first header comprising a reference to unallocated memory in the payload, allocate second memory items, the second memory items each comprising a second header, the second header comprising a reference to an allocated chunk of memory in the payload of a first memory item, receive requests for memory allocation, and respond to requests for memory allocation by returning a reference from the header of a first or second memory item.","The following detailed description is directed to certain specific embodiments of the invention. However, the invention can be embodied in a multitude of different ways. It should be apparent that the aspects herein may be embodied in a wide variety of forms and that any specific structure, function, or both being disclosed herein is merely representative. Based on the teachings herein one skilled in the art should appreciate that an aspect disclosed herein may be implemented independently of any other aspects and that two or more of these aspects may be combined in various ways. For example, an apparatus may be implemented or a method may be practiced using any number of the aspects set forth herein. In addition, such an apparatus may be implemented or such a method may be practiced using other structure, functionality, or structure and functionality in addition to or other than one or more of the aspects set forth herein.","Mobile devices typically make use of memory allocation services in order to operate. Methods and devices are described herein to decrease the total amount of memory required by various subsystems in a mobile device and to decrease the processing resources consumed by the mobile device. Set forth below are some architectures that may be used in conjunction with the described methods and devices.",{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 1","b":["102","102","202","204","206","106","102","210","212","214","106","216","106","210"]},"The network interface  may include any suitable antenna (not shown), a receiver , and a transmitter  so that the exemplary device  can communicate with one or more devices over the wireless link . Optionally, the network interface  may also have processing capabilities to reduce processing requirements of the processor .","Optionally, the device  may include a second network interface  that communicates over the network  via a link . For example, the device  may provide connectivity to the other network  (e.g., a wide area network such as the Internet) via a wired or wireless communication link. Accordingly, the device  may enable other devices  (e.g., a Wi-Fi station) to access the other network . In addition, it should be appreciated that one or more of the devices  may be portable or, in some cases, relatively non-portable. The second network interface  may transmit and receive RF signals according to the IEEE 802.11 standard, including IEEE 802.11(a), (b), or (g), the BLUETOOTH standard, and\/or CDMA, GSM, AMPS or other known signals that are used to communicate within a wireless cell phone network. In addition, the second network interface  may comprise any suitable wired network interface such as Ethernet (IEEE 802.3), USB, or MDDI.","The device  may include a battery  to provide power to one or more components of the device . The device  may comprise at least one of a phone, smartphone, Personal Digital Assistant (PDA), Ultra-Mobile Personal Computer (UMPC), Mobile Internet Device (MID), or any other mobile device. In particular, the teachings herein may be incorporated into (e.g., implemented within or performed by) a variety of the devices .","The components described herein may be implemented in a variety of ways. Referring to , the device or apparatus  is represented as a series of interrelated functional blocks that may represent functions implemented by, for example the processor , software, some combination thereof, or in some other manner as taught herein. For example, the processor  may facilitate user input via the input devices . Further, the transmitter  may comprise a processor for transmitting that provides various functionalities relating to transmitting information to another device . The receiver  may comprises a processor for receiving that provides various functionality relating to receiving information from another device  as taught herein.","Processor  is also in communication with memory allocation service . In one embodiment, memory allocation service  runs on processor . memory allocation service  responds to requests for memory from one or more subsystems operating in device . The method and operation of memory allocation service  will be described in greater detail here below.","As noted above,  illustrates that in some aspects these components may be implemented via appropriate processor components. These processor components may in some aspects be implemented, at least in part, using structure as taught herein. In some aspects, a processor may be adapted to implement a portion or all of the functionality of one or more of these components. In some aspects one or more of the components represented by dashed boxes are optional.","In one or more exemplary embodiments, the functions described may be implemented in hardware, software, firmware, or any combination thereof. If implemented in software, the functions may be stored on or transmitted over as one or more instructions or code on a computer-readable medium. Computer-readable media includes both computer storage media and communication media including any medium that facilitates transfer of a computer program from one place to another. A storage media may be any available media that can be accessed by a general purpose or special purpose computer. By way of example, and not limitation, such computer-readable media can comprise RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium that can be used to carry or store desired program code means in the form of instructions or data structures and that can be accessed by a general-purpose or special-purpose computer, or a general-purpose or special-purpose processor. Also, any connection is properly termed a computer-readable medium. For example, if the software is transmitted from a website, server, or other remote source using a coaxial cable, fiber optic cable, twisted pair, digital subscriber line (DSL), or wireless technologies such as infrared, radio, and microwave, then the coaxial cable, fiber optic cable, twisted pair, DSL, or wireless technologies such as infrared, radio, and microwave are included in the definition of medium. Disk and disc, as used herein, includes compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), floppy disk and blu-ray disc where disks usually reproduce data magnetically, while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer-readable media.","In accordance with one aspect of the memory allocation service, small memory blocks are packed into a larger fixed size units (called \u201cDSM items\u201d). Each of the smaller blocks is kept track of using \u201cDUP\u201d (duplicate) items, which point to the payload within a DSM. Since the DSM items are of limited size, and all blocks within the DSM can be assumed to have similar limited lifetime, it is not necessary to deal with fragmentation of free memory within each DSM. The benefit of this is that memory utilization can be greatly reduced since only little memory within each DSM is wasted, but at a fraction of the complexity of other memory allocation techniques with high packing efficiency. Another way of looking at this is that this is a hybrid of fixed and flexible size allocation services, where overall the memory is allocated in fixed size blocks, but within the blocks, flexible size allocations are allowed because the flexible allocations are expected to have short lifetime.","In one aspect, the fundamental memory block is called a DSM item. In is comprised of a header and a payload section. When a block of data has to be stored in memory, a new DSM item is allocated, and the data is copied to the payload section of the DSM. Additionally, the headers are updated to reflect the start and length of the data. If the length of the data exceeds the size of the payload section, multiple DSM items may be chained together into a linked list.","Further, a special type of DSM item is defined, denoted as DUP (\u201cduplicate\u201d). A DUP uses the same header structure as a regular DSM, but has no payload section of its own. Instead, it can point to a data section elsewhere, for example inside a normal DSM. In order to track how many DUPs point to data in the same DSM item, there is a header field denoted \u201cref_count\u201d, which counts the number of references to this DSM item and which is set to one when the original DSM is first allocated. Every time a new DUP is created pointing inside the DSM item, the ref_count of the DSM item is increased. Likewise, when the DUP is freed up, the ref_count of the original DSM item is decreased. This way, the allocation algorithm can know when the DSM item can actually be freed, which is only when no other DUPs point to the payload section of this packet, which is when the ref_count has gone back to zero.","The DUPs can be used to split packets, to reorder data, to remove headers, etc., without having to touch the original data. All this can be achieved just by manipulating the DSM and DUP headers.","According to another aspect, techniques to allow packing of data into the same DSM item to improve memory efficiency can address some of the disadvantages noted previously. According to this aspect, a pool of large DSM items is defined. A large DSM size would allow data from multiple radio link packets to be concatenated into the same DSM item.",{"@attributes":{"id":"p-0033","num":"0032"},"figref":["FIG. 2","FIG. 2"],"b":["102","202","102","610","610","610","605","605"]},"An expanded view of memory item  is also provided in . As seen in the expanded view, these container type items  comprise multiple sections. A first section of the memory item may be referred to as the header . The header  may contain information relating to the remaining portions of the memory item  as well as other information. A second section of the memory item  may be referred to as the payload. The payload itself may have several allocated portions or blocks of memory . In addition the payload may have unallocated or unused space . In one embodiment, the payload comprises approximately 1500 bytes, about the size needed to store an IP packet.","The header  of the memory item  may comprise information such as a reference to the unallocated space  in the memory item . In one example, this reference takes the form of a pointer indicating the start address of the unallocated space  in the memory item . This reference to the unallocated memory  in data item  may also be referred to as \u201cdata_ptr.\u201d The header may also contain a field indicating the size of allocated memory . This quantity of allocated space may be referred to as a \u201cused\u201d space in the payload. The header may also contain a field indicating an identifier identifying the memory pool  associated with the memory item . This identifier may be referred to as a \u201cpool_id.\u201d The header may also contain a field indicating the number of references to the memory item . For example, as described below, the memory item  may be referenced by other memory items. For example, each of the allocated chunks  of the payload is referenced by a different memory item. Further, the header  of the memory item  references the unallocated portion  of the payload. A count of these references may be used, inter alia, for the purpose of determining when the memory item  may be freed up and returned to the pool  to be subsequently reallocated. This reference count may be referred to as \u201creferences\u201d. The header  may also contain one or more references to other memory items. For example, if a requested chunk of memory is too big to fit in a single memory item , the references in the header may be used to indicate one or more additional memory items that may be chained together to meet the request. Depending on the type of memory items being referenced, these references may be referred to as \u201cpkt_ptr\u201d or \u201cdup_ptr.\u201d For example, pkt_ptr may reference zero or more DSM or container items while dup_ptr may reference zero or more DUP items. In addition to the identified fields, the header  may comprise additional fields such as user defined field or fields used for testing purposes. In addition, the header  may omit one or more of the identified fields.",{"@attributes":{"id":"p-0036","num":"0035"},"figref":["FIG. 2","FIG. 2","FIG. 2"],"b":["630","630","615","610","615","630","630","620","610","630","640","605","640","650","640","650","620","610"]},{"@attributes":{"id":"p-0037","num":"0036"},"figref":["FIG. 3","FIG. 2"],"b":["715","730","735","725","720","715"]},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIGS. 4A and 4B","b":["780","780","202","780"]},"Method  begins when a memory allocation service receives a memory request at step . As part of the memory request, the requesting subsystem may specify a requested size of memory. The requesting subsystem may also supply a subsystem ID so that the memory allocating service can select an appropriate DSM item for that subsystem. After receiving the memory request, the memory allocation service determines if the requested size is less that the size of the unallocated memory in a current DSM container item as shown in decision step . As discussed with respect to , DSM items may have unallocated space in the payload section. Decision step  may be performed by comparing the size of the unallocated portion to the requested size. Alternatively, since the total size of the payload is known, the requested amount may be compared to the size of the allocated portion of the payload. In another embodiment, rather than using the requested size of memory for the comparison, the memory allocation system may increase the size of the request so that the augmented request is aligned with the cache boundaries. For instance, an actual request may be increased so that the resulting request is a multiple of a particular cache row size such as 16, 32, 64, or 128 bytes. Regardless, if the requested memory size, or its augmented aligned version, fits in the unallocated portion, the method proceeds to step .","Continuing to decision step , the memory allocation system determines if the requested size plus the size of the previously allocated portion of the current DSM item is greater than a first threshold. For example, if a DSM payload is approximately 1500 bytes long, the threshold might be 1200 bytes. Accordingly, the memory allocation system would determine if the size of the already allocated memory plus the size of the requested memory exceeds the 1200 byte limit. In one embodiment, the threshold may be selected as a certain percentage of the size of the payload of a DSM item. For example, the threshold may be approximately 70%-80% of the size of the payload. In another embodiment, the threshold size may be chosen to reflect the size characteristics of common request such as requests for memory to store IP packets. In another embodiment, rather than comparing the size of allocated memory to a threshold, the size of remaining unallocated memory can be compared to a threshold. In this embodiment, the service would calculate the portion of the payload which would remain unallocated if the requested or adjusted memory size were allocated. The comparison of the remaining unallocated memory to this threshold could then be used in a manner similar to the previously discussed comparison of the allocated memory to a threshold.","Performing the decision step , if the identified sum is greater than the threshold, the method proceeds to step . Proceeding at step , the memory allocation system returns a reference to the requesting subsystem. In one embodiment, the returned reference is the data_ptr from the header of the current DSM item. Advantageously, by returning the reference from the header of the DSM item, the memory allocation system saves the overhead and resources involved in creating a DUP item and returning the reference from the DUP header. Further, allocating the remainder of the DSM payload rather than just the requested size allows the memory allocation service to avoid small blocks of data at the end of payloads which would otherwise result in chaining if used.","Returning to step decision , if the requested memory size plus the already allocated memory size is not greater than the threshold, the method proceeds to step . In step  the memory allocation service creates a DUP item, allocates the requested memory, and returns the reference in the DUP to the requesting subsystem. Advantageously, over multiple iterations, this method allows the memory allocation service to pack multiple allocations into a single DSM item. This packing allows for efficient use of memory resources and provides gain in both memory use and processing use.","Returning to step , if the requested memory size is greater than the unallocated space in the current DSM item, the method continues on to decision step  of . In step decision , the memory allocation service determines if the requested size is greater than a second threshold. This second threshold, like the first may be set as a fixed number of bytes. For example, if the size of the DSM item payload is 1600 bytes, the second threshold may be 1200 bytes. Alternatively, the second threshold may also be determined as a percentage of the size of the payload. For example, the second threshold may be 65%-75% of the payload size. In another embodiment, the threshold may be dynamically determined. For example, the threshold may be set equal to the total amount of allocated memory in the DSM item. In this embodiment the requested size would exceed the threshold when the requested size exceeds the allocated space in the DSM. Regardless of how the threshold is determined, if the requested size is greater than this threshold, the memory allocation service allocates a new DSM item and returns the data_ptr from the new DSM item header to the requesting subsystem as shown in step . Similar to step , this process of returning a reference from the header of the new DSM item saves the overhead of creating a new DUP item. The requesting subsystem is given more than it asked for, an entire DSM item, and this allocation allows subsequent allocation requests to be satisfied with unallocated memory remaining in the current DSM item.","Returning to step , if the requested size is less than this second threshold, the method proceeds to step . In step  the DSM item header reference to the unallocated portion of the DSM payload is deleted. In effect, this frees up the remainder of the DSM item. In addition, a new DSM item is created. A block of memory in the new DSM item is allocated and a corresponding DUP created. The reference in the new DUP to the new DSM item is then returned to the requesting subsystem.","For the purpose of explanation, examples of the foregoing method are illustrated in . In CASE 1, a current DSM item  is illustrated. The current DSM has both an allocated portion  and an unallocated portion . A requested packet size  is illustrated as is the first threshold . As shown in the figure, the size of the allocated portion  plus the size of the requested portion  is less that the first threshold . Accordingly, as shown, a chunk of the DSM item , is allocated and a new DUP item is created and its reference returned to the calling subsystem. The new DUP item includes a reference to into DSM  that points to the start of the newly allocated data.","In CASE 2, a current DSM  is shown in conjunction with another requested packet . As illustrated in the figure, the size of the allocated portion of DSM  plus the size of the requested packet exceeds the first threshold, however, the sum does not exceed the total payload length. Accordingly, the reference to the unallocated portion of the DSM item  is returned to requesting subsystem. The result is that in addition to getting the requested memory size or even a slightly larger row adjusted allocation, the requesting subsystem will get all the remaining unallocated space  in the DSM . Again, returning the reference from the DSM item header saves the overhead of creating a DUP and eliminates the chaining potential created by the stub  that would have otherwise been left.","In CASE 3, a current DSM  is illustrated with another requested memory block . As illustrated, the requested packet is too large to fit in the unallocated portion of the current DSM. In addition, the requested size is less than the second threshold . Accordingly, the reference in the header of the current DSM  to the unallocated portions of the payload is deleted. A new DSM  is created. A block of the memory from that DSM is allocated and a new DUP item is created referencing the allocated block in the new DSM. The reference from this new DUP item is then returned to the requesting subsystem. Again, it will be appreciated that while the second threshold  is illustrated as being static, the threshold may be determined dynamically such as by setting it equal to the size of the allocated memory in the original.","In CASE 4, a current DSM  is illustrated along with a requested memory block . As with CASE 3, the requested size is too big to fit in the unallocated portion of DSM item . In addition, the requested block size  is greater than the second threshold . The result is that the current DSM  is preserved in its current state. Meanwhile, a new DSM  is created and the reference to the unallocated portion of the payload in the new DSM header is returned.",{"@attributes":{"id":"p-0049","num":"0048"},"figref":["FIG. 6","FIGS. 4A and 4B","FIG. 6","FIG. 6","FIG. 6","FIG. 6"],"b":["1080","1080","202","102"]},"current_dsm_item_ptr[ ]: this is a set of pointers to the current DSM item for each subsystem.","aligned_pkt_size: the is the size of the requested block of memory, adjusted to align with cache rows of sizes such as 16, 32, 64, or 128.","pkt_ptr: pointer to the allocated memory that is going to be returned.","DSMI_MEM_THRESHOLD_TO_FREE_DSM: the first threshold for determining if a DUP or DSM reference should be returned. If after allocating a block of memory, the remaining unallocated memory would be less that this threshold, the DSM reference is returned, effectively allocating all remaining unallocated memory in the DSM item.","DSMI_THRESHOLD_FOR_COMPLETE_ITEM_ALLOCATION: the second threshold for determining whether a DUP or DSM reference should be returned. If the requested packet size is greater that this threshold, a new DSM item is created and the reference from the header of the new DSM is returned. The current DSM is retained.","The method illustrated in  begins with the function call shown at 1085. As a parameter to the request, the allocation service receives the requested packet size \u201cpkt_size\u201d. In addition, in some embodiments, another parameter, the subsystem ID would also be passed to the function call. Proceeding to step  the method locks the particular pool of DSM items to avoid concurrent access issues. After locking the pool, the method proceeds to decision step . At decision step  the service determines if the pkt_ptr is null. If no memory has been allocated for the request, the pointer may be null. However, if the pointer has a non NULL value, the service proceeds to unlock the pool  and return the pointer . However, if the pointer is null, the service proceeds to decision step .","Continuing at decision step , the service determines if the current_dsm_item_ptr is null. If previous iterations have resulted in no current DSM item existing, the service proceeds to create one  and exit if such creation fails , , . Once a valid current DSM is confirmed to exist or is created, the method proceeds to decision step . At decision step  the service determines if the aligned requested size is smaller than the size of the unallocated space in the current DSM. If it is, the service then determines if the unallocated space that would remain after allocating the requested memory in the DSM is smaller than a first threshold at step . If so, the service creates a DUP and allocates the aligned requested memory in the current DSM , unlocks the pool , and returns the DUP pointer . If not, the service creates an entirely new DSM , unlocks the pool , and returns the pointer from the header of the new DSM .","Returning to step , if the requested size is greater than the remaining unallocated space, the method proceeds to decision step . At step , the service determines if the requested size is greater than or equal to the second threshold. If so, the service creates an entirely new DSM , unlocks the pool , and returns the pointer from the header of the new DSM . If not, the service frees the remaining memory in the current DSM and proceeds to start the method over again at step . The result is that a new DSM is created, a DUP referencing the body of the new DSM is created and the DUP pointer is returned to the requesting subsystem.","Additional features and embodiments of the present memory allocation service are also presented herein:","Multiple DSM Pointers for Different Users","In order to handle cases where lifetime of data blocks is expected to differ substantially for different users, the algorithm may maintain multiple DSM pointers, and only pack data blocks with similar lifetime into the same DSM. For example, a user specific ID could be passed along with the allocation request, such that only data from this user would be packed into the same DSM item. Requests from other users would be packed into other DSMs. Here \u201cuser\u201d can be, set of tasks or sub-tasks or layers or functions, or any other combination of these to distinguish one set of users profile from other.","Support for Adding Data in Front (or Back) of a DSM","In order to efficiently handle the addition of data in front of an existing DSM or DUP item (i.e. addition of protocol headers) without having to allocate a new DSM item and perform chaining, it is proposed to allow allocating a DSM with free space in the front. The free space is kept track of by an \u201coffset\u201d header. This way, a DUP or DSM can be allocated with a given offset, and later if more data needs to be inserted in the front of the DSM, this can be done without a new DSM allocation followed by chaining. Similar service could be allowed for adding data in the end of a DSM.","Multiple Pools","The service allows for multiple DSM pool to allow reservation of a given number of items to a certain user. This way that user will not have to worry that suddenly there are no more free DSM items because another user had allocated them all. Also, this provides better debug ability of memory overrun issues (where one user overrides the boundaries of an allocation), because the issue is restricted to the codebase of this user only.","Quota Service as Alternative to Multiple Pools","Another way of achieving the same is to have one pool, but each user has a max quota of DSM items (bytes) which he can allocate. Once he has reached his quota, he must free some items in order to allocate new items, to avoid memory starvation of other users. One benefit this has over the multiple pools approach is that sharing can still be achieved. For example if there are 4 users with their own pool of X items, it would require 4X items of memory. However since it is very unlikely that all 4 users would allocate all items at the same time, the quota service could allow to reduce the total memory to say 3X, assuming that if one user takes all his memory=X, the 3 remaining users would be able to share the remaining memory=2X.","Multiple DSM Pointers for Different Sizes","In this aspect, the DSM allocation may be partitioned based on the memory space requested (as opposed to using a subsystem ID above). Instead of having conventional way of multiple size pools shared across multiple layers\/subsystems to satisfy needs of all the tasks\/profiles, with this proposal there can be a provision to support multiple DSM pointers from which only specific size blocks can be allocated per DSM pointer. For example, we can have DSM pointer for 128, 256, 512, 768, 1024, 1536 byte block sizes and so on.","Whenever any layer\/modules requests for the specific size memory, algorithm can determine the best fit DSM pointer and provide the dup if requested size is available. If requested size is not available then release DSM pointer (if not NULL) for that size and allocate a big item from global pool of big items (e.g., items greater than a predetermined number of bytes, in one example items greater than or equal to 768 bytes) and then DUP the best fit block size and return the DUP to the caller.","The present allocation also has several distinct advantages over the prior art. Embodiments of the present service reduces memory footprint. The use of fewer pools allows for better statistical multiplexing. The service facilitates better packing efficiency. There is less overhead for DSM items, even with the added overhead for DUPs. CPU processing overhead (MIPS) is reduced due to less chaining. The service allows for using larger DSM items avoiding long packet chains (especially at larger packet sizes) thereby reducing the number of DSM allocation\/free operations. Embodiments simplify the maintenance associated with a more conventional approach involving multiple memory pools. The service may be implemented as having a central API for handling this avoids having to implement such functionality per subsystem, thereby duplicating code. The need for separate pool sizes and item sizes per target is avoided. Without the techniques outlined herein, one would need to implement code separately in many places (e.g., various or multiple layers in the stack, such as, packet services module (PS), high speed USB (HS-USB) module, etc.), which is more prone to errors, etc. It is Possible to define DUP pool in high speed memory for further performance improvements (e.g., cache benefits), for example, by defining the DUP pool in a low latency memory (e.g., internal random access memory (IRAM). Reduced checks in DSM can be obtained if only one pool is used with this application programming interface (API). Currently DSM performs some checks based on pool ID (identification) which is passed as a parameter."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIGS. 4A and 4B"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 6"}]},"DETDESC":[{},{}]}
