---
title: Methods, systems, and computer readable media for synthesizing sounds using estimated material parameters
abstract: The subject matter described herein includes methods, systems, and computer readable media for synthesizing sounds using estimated material parameters. According to one aspect, a method for sound synthesis using estimated material parameters is provided. The method includes at a computing platform including a memory and a processor, estimating, by the processor and based on a recorded audio sample of an impact on a physical object, one or more material parameters of a physical object for use in synthesizing a sound associated with a virtual object having material properties similar to the physical object. The method further includes storing the one or more material parameters in memory. The method further includes using the estimated material parameters to generate a synthesize sound for the virtual object.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09401684&OS=09401684&RS=09401684
owner: The University of North Carolina at Chapel Hill
number: 09401684
owner_city: Chapel Hill
owner_country: US
publication_date: 20130531
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["PRIORITY CLAIM","GOVERNMENT INTEREST","TECHNICAL FIELD","BACKGROUND","SUMMARY","DETAILED DESCRIPTION","1. Introduction","2. Related Work","3. Background","4. Feature Extraction","5. Parameter Estimation","6. Residual Compensation","7. Results and Analysis","Example","A Complicated Scenario","8. Perceptual Study","9. Conclusions and Future Work","REFERENCES","APPENDIX","A. Transferability of Rayleigh Damping Parameters","B. Comparison with Previous Feature Extraction Methods","C. Constants and Functions"],"p":["This application claims the benefit of U.S. Provisional Patent Application Ser. No. 61\/653,827, filed May 31, 2012, the disclosure of which is incorporated herein by reference in its entirety.","This invention was made with government support under Grant No. U.S. Pat. No. 0,917,040 awarded by the National Science Foundation and Grant No. W911 NF-06-1-0355 awarded by the Army Research Office. The government has certain rights in the invention.","The subject matter described herein relates to sound synthesis. More particularly, the subject matter described herein relates to synthesizing sounds using estimated material parameters.","Linear modal synthesis methods have been used to generate sounds for rigid bodies. One of the key challenges in widely adopting such techniques is the lack of automatic determination of satisfactory material parameters that recreate realistic audio quality of sounding\u2032 materials. Accordingly, there exists a long felt need for methods, systems, and computer readable media for synthesizing sounds using estimated material parameters.","The subject matter described herein includes methods, systems, and, computer readable media for synthesizing sounds using estimated material parameters. According to one aspect, a method for sound synthesis using estimated material parameters is provided. The method includes at a computing platform including a memory and a processor, estimating, by the processor and based on a recorded audio sample of an impact on a physical object, one or more material parameters of a physical object for use in synthesizing a sound associated with a virtual object having material properties similar to the physical object. The method further includes storing the one or more material parameters in memory. The method further includes using the estimated material parameters to generate a synthesize sound for the virtual object.","The subject matter described herein may be implemented in hardware, software, firmware, or any combination thereof. As such, the terms \u201cfunction\u201d \u201cnode\u201d or \u201cmodule\u201d as used herein refer to hardware, which may also include software and\/or firmware components, for implementing the feature being described. In one exemplary implementation, the subject matter described herein may be implemented using a computer readable medium having stored thereon computer executable instructions that when executed by the processor of a computer control the computer to perform steps. Exemplary computer readable media suitable for implementing the subject matter described herein include non-transitory computer-readable media, such as disk memory devices, chip memory devices, programmable logic devices, and application specific integrated circuits. In addition, a computer readable medium that implements the subject matter described herein may be located on a single device or computing platform or may be distributed across multiple devices or computing platforms.","The subject matter described herein includes a method that uses pre-recorded audio clips to estimate material parameters that capture the inherent quality of recorded sounding materials. The method extracts perceptually salient features from audio examples. Based on psychoacoustic principles, we design a parameter estimation algorithm using an optimization framework and these salient features to guide the search of the best material parameters for modal synthesis. We also present a method that compensates for the differences between the real-world recording and sound synthesized using solely linear modal synthesis models to create the final synthesized audio. The resulting audio generated from this sound synthesis pipeline well preserves the same sense of material as a recorded audio example. Moreover, both the estimated material parameters and the residual compensation naturally transfer to virtual objects of different sizes and shapes, while the synthesized sounds vary accordingly. A perceptual study shows the results of this system compares well with real-world recordings in terms of material perception.","Sound plays a prominent role in a virtual environment. Recent progress has been made on sound synthesis models that automatically produce sounds for various types of objects and phenomena. However, it remains a demanding task to add high-quality sounds to a visual simulation that attempts to depict its real-world counterpart. Firstly, there is the difficulty for digitally synthesized sounds to emulate real sounds as closely as possible. Lack of true-to-life sound effects would cause a visual representation to lose its believability. Secondly, sound should be closely synchronized with the graphical rendering in order to contribute to creation of a compelling virtual world. Noticeable disparity between the dynamic audio and visual components could lead to a poor virtual experience for users.","The traditional sound effect production for video games, animation, and movies is a laborious practice. Talented Foley artists are normally employed to record a large number of sound samples in advance and manually edit and synchronize the recorded sounds to a visual scene. This approach generally achieves satisfactory results. However, it is labor-intensive and cannot be applied to all interactive applications. It is still challenging, if not infeasible, to produce sound effects that precisely capture complex interactions that cannot be predicted in advance.","On the other hand, modal synthesis methods are often used for simulating sounds in real-time applications. This approach generally does not depend on any pre-recorded audio samples to produce sounds triggered by all types of interactions, so it does not require manually synchronizing the audio and visual events. The produced sounds are capable of reflecting the rich variations of interactions and also the geometry of the sounding objects. Although this approach is not as demanding during run time, setting up good initial parameters for the virtual sounding materials in modal analysis is a time-consuming and non-intuitive process. When faced with a complicated scene consisting of many different sounding materials, the parameter selection procedure can quickly become prohibitively expensive and tedious.","Although tables of material parameters for stiffness and mass density are widely available, directly looking up these parameters in physics handbooks does not offer as intuitive, direct control as using a recorded audio example. In fact, sound designers often record their own audio to obtain the desired sound effects. The subject matter described herein includes a new data-driven sound synthesis technique that preserves the realism and quality of audio recordings, while exploiting all the advantages of physically-based modal synthesis. We introduce a computational framework that takes a single example audio recording and estimates the intrinsic material parameters (such as stiffness, damping coefficients, and mass density) that can be directly used in modal analysis. However, as described below in the Conclusions and Future Work section, using multiple audio recordings is also within the scope of the subject matter described herein.","As a result, for objects with different geometries and run-time interactions, different sets of modes are generated or excited differently, and different sounds are produced. However, if the material properties are the same, they should all sound like coming from the same material. For example, a plastic plate being hit, a plastic ball being dropped, and a plastic box sliding on the floor generate different sounds, but they all sound like \u2018plastic\u2019, as they have the same material properties. Therefore, if we can deduce the material properties from a recorded sound and transfer them to different objects with rich interactions, the intrinsic quality of the original sounding material is preserved. Our method can also compensate the differences between the example audio and the modal-synthesized sound. Both the material parameters and the residual compensation are capable of being transferred to virtual objects of varying sizes and shapes and capture all forms of interactions.  shows an example of our framework. From one recorded impact sound (a), we estimated material parameters, which can be directly applied to various geometries (c), (d), and (e) to generate audio effects that automatically reflect the shape variation while still preserve the same sense of material.",{"@attributes":{"id":"p-0035","num":"0034"},"figref":["FIG. 2()","FIG. 2()"],"b":["200","202","204"]},"Feature extraction module  may reside in memory  and be executed by processor  to perform feature extraction: Feature extraction may include, given a recorded impact audio clip, initial extraction from the audio o clip of high-level features, namely, a set of damped sinusoids with constant frequencies, dampings, and initial amplitudes (Sec. 4). These features are then used to facilitate estimation of the material parameters (Sec. 5), and guide the residual compensation process (Sec. 6).","Parameter estimation module  may reside in memory  and be executed by processor  to perform parameter estimation: In one example of parameter estimation, due to the constraints of the sound synthesis model, we assume a limited input from just one recording and it is challenging to estimate the material parameters from one audio sample. To do so, a virtual object of the same size and shape as the real-world object used in recording the example audio is created.","Each time an estimated set of parameters is applied to the virtual object for a given impact, the generated sound, as well as the feature information of the resonance modes, are compared with the real world example sound and extracted features respectively using a difference metric. This metric is designed based on psychoacoustic principles, and aimed at measuring both the audio material re-semblance of two objects and the perceptual similarity between two sound clips. The optimal set of material parameters is thereby determined by minimizing this perceptually-inspired metric function (see Sec. 5). These parameters are readily transferable to other virtual objects of various geometries undergoing rich interactions, and the synthesized sounds preserve the intrinsic quality of the original sounding material.","Residual compensation module  may reside in memory  and be executed by processor  to perform residual compensation. Residual compensation accounts for the residual, i.e. the approximated differences between the real-world audio recording and the modal synthesis sound with the estimated parameters. First, the residual is computed using the extracted features, the example recording, and the synthesized audio. Then at run-time, the residual is transferred to various virtual objects. The transfer of residual is guided by the transfer of modes, and naturally reflects the geometry and run-time interaction variation (see Sec. 6).","Once material parameters are estimated for a real world object using the system illustrated in , the parameters may be stored in a memory and accessed by a sound synthesizer module to synthesize sounds for virtual objects having the same or similar properties as the real world object. For example, as illustrated in , a computer  may include a processor  and a memory . Computer  may be a personal computer, a laptop computer, a tablet computer, a mobile device, a gaming console, a graphics card or any other platform with a processor and on which it is desirable to generate synthesized sounds for virtual objects. A synthesizer module  may be embodied or stored in memory . Memory  may also store material parameters  estimated based on a recorded audio sample of an impact on a physical object having material properties similar to a virtual object (see Sec. 5). For example, the virtual object may be an object in a game, such as a car door, the recorded sound may be the sound of the door closing, and the material properties may include properties of the door. Synthesizer module  may be configured to retrieve one or more of the estimated material parameters  associated with a virtual object and utilize the retrieved material parameters to synthesize a sound associated with the virtual object. For example, synthesizer module  may synthesize the sound of a virtual car door closing where the virtual card door is larger than the real car door for which the material properties were calculated and stored using the estimated material parameters stored for the real car door.","According to another aspect of the subject matter described herein, synthesizer module  may be configured to augment the sound to account for residual differences between the set of damped sinusoids and the recorded audio sample, as described below in Sec. 6. Synthesizer module  may also be configured to determine the residual differences between the set of damped sinusoids and the recorded audio sample by computing a represented sound that corresponds to a portion of the recorded audio sample represented by the set of damped sinusoids and subtracting a power spectrogram of the represented sound from a power spectrogram of the recorded audio sample, as also described below in Sec. 6. When the virtual object differs in geometry from the real object for which the audio was recorded, synthesizer module  may be configured to generate a transform function for transforming a power spectrogram based on the set of damped sinusoids to a power spectrogram that accounts for the differing geometry and apply the transform function to a power spectrogram of the residual differences, as described below in Sec. 6.","Exemplary contributions of the present subject matter are summarized below:","a feature-guided parameter estimation framework to determine the optimal material parameters that can be used in existing modal sound synthesis applications;","an effective residual compensation method that accounts for the difference between the real-world recording and the modal-synthesized sound;","a general framework for synthesizing rigid-body sounds that closely resemble recorded example materials; and","automatic transfer of material parameters and residual compensation to different geometries and runtime dynamics, producing realistic sounds that vary accordingly.","In the last couple of decades, there has been strong interest in digital sound synthesis in both computer music and computer graphics communities due to the needs for auditory display in virtual environment applications. The traditional practice of Foley sounds is still widely adopted by sound designers for applications like video games and movies. Real sound effects are recorded and edited to match a visual display. More recently, granular synthesis became a popular technique to create sounds with computers or other digital synthesizers. Short grains of sounds are manipulated to form a sequence of audio signals that sound like a particular object or event. Roads [2004] gave an excellent review on the theories and implementation of generating sounds with this approach. Picard et al. [2009] proposed techniques to mix sound grains according to events in a physics engine.","Physically-Based Sound Synthesis: Another approach for simulating sound sources is using physically-based simulation to synthesize realistic sounds that automatically synchronize with the visual rendering. Generating sounds of interesting natural phenomena like fluid dynamics and aerodynamics have been pro-posed [Dobashi et al. 2003; 2004; Zheng and James 2009; Moss et al. 2010; Chadwick and James 2011]. The ubiquitous rigid-body sounds play a vital role in all types of virtual environments, and the synthesis of rigid body sounds using estimated material properties is described in detail herein. O'Brien et al. [2001] proposed simulating rigid bodies with deformable body models that approximates solid objects' small-scale vibration leading to variation in air pressure, which propagates sounds to human ears. This approach accurately captures surface vibration and wave propagation once sounds are emitted from objects. However, it is far from being efficient enough to handle interactive applications. Adrien [1991] introduced modal synthesis to digital sound generation. For real-time applications, linear modal sound synthesis has been widely adopted to synthesize rigid-body sounds [van den Doel and Pai 1998; O'Brien et al. 2002; Raghuvanshi and Lin 2006; James et al. 2006; Zheng and James 2010]. This method acquires a modal model (i.e. a bank of damped sinusoidal waves) using modal analysis and generates sounds at runtime based on excitation to this modal model. Moreover, sounds of complex interaction can be achieved with modal synthesis. Van den Doel et al. [2001] presented parametric models to approximate contact forces as ex-citation to modal models to generate impact, sliding, and rolling sounds. Ren et al. [2010] proposed including normal map information to simulate sliding sounds that reflect contact surface de-tails. More recently, Zheng and James [2011] created highly realistic contact sounds with linear modal synthesis by enabling non-rigid sound phenomena and modeling vibrational contact damping. Moreover, the standard modal synthesis can be accelerated with techniques proposed by [Raghuvanshi and Lin 2006; Bonneel et al. 2008], which make synthesizing a large number of sounding objects feasible at interactive rates.","The use of linear modal synthesis is not limited to creating simple rigid-body sounds. Chadwick et al. [2009] used modal analysis to compute linear mode basis, and added nonlinear coupling of those modes to efficiently approximate the rich thin-shell sounds. Zheng and James [2010] extended linear modal synthesis to handle complex fracture phenomena by precomputing modal models for ellipsoidal sound proxies.","However, few previous sound synthesis works addressed the issue of how to determine material parameters used in modal analysis to more easily recreate realistic sounds.","Parameter Acquisition: Spring-mass [Raghuvanshi and Lin 20061 and finite element [O'Brien et al. 2002] representations have been used to calculate the modal model of arbitrary shapes. Challenges lie in how to choose the material parameters used in these representations. Pai et al. [2001] and Corbett et al. [2007] directly acquires a modal model by estimating modal parameters (i.e. amplitudes, frequencies, and dampings) from measured impact sound data. A robotic device is used to apply impulses on a real object at a large number of sample points, and the resulting impact sounds are analyzed for modal parameter estimation. This method is capable of constructing a virtual sounding object that accurately recreates the audible resonance of its measured real-world counterpart. However, each new virtual geometry would require a new measuring process performed on a real object that has exactly the same shape, and it can become prohibitively expensive with an increasing number of objects in a scene. This approach generally extracts hundreds of parameters for one object from many audio clips, while the goal of our technique instead is to estimate the few parameters that best represent one material of a sounding object from only one audio clip.","To the best of our knowledge, the only other research work that attempts to estimate sound parameters from one recorded clip is by Lloyd et al. [2011]. Pre-recorded real-world impact sounds are utilized to find peak and long-standing resonance frequencies, and the amplitude envelopes are then tracked for those frequencies. They proposed using the tracked time-varying envelope as the amplitude for the modal model, instead of the standard damped sinusoidal waves in conventional modal synthesis. Richer and more realistic audio is produced this way. Their data-driven approach estimates the modal parameters instead of material parameters. Similar to the method proposed by Pai et al. [2001], these are per-mode parameters and not transferable to another object with corresponding variation. At runtime, they randomize the gains of all tracked modes to generate an illusion of variation when hitting different locations on the object. Therefore, the produced sounds do not necessarily vary correctly or consistently with hit points. Their adopted resonance modes plus residual resynthesis model is very similar to that of SoundSeed Impact [Audiokinetic 2011], which is a sound synthesis tool widely used in the game industry. Both of these works extract and track resonance modes and modify them with signal processing techniques during synthesis. None of them attempts to fit the extracted per-mode data to a modal sound synthesis model, i.e. estimating the higher-level material parameters.","In computer music and acoustic communities, researchers proposed methods to calibrate physically-based virtual musical instruments. For example, V\u00e4lim\u00e4ki et al. [1996; 1997] proposed a physical model for simulating plucked string instruments. They presented a parameter calibration framework that detects pitches and damping rates from recorded instrument sounds with signal processing techniques. However, their framework only fits parameters for strings and resonance bodies in guitars, and it cannot be easily extended to extract parameters of a general rigid-body sound synthesis model. Trebian and Oliveira [2009] presented a sound synthesis method with linear digital filters. They estimated the parameters for recursive filters based on pre-recorded audio and re-synthesized sounds in real time with digital audio processing techniques. This approach is not designed to capture rich physical phenomena that are automatically coupled with varying object interactions. The relationship between the perception of sounding objects and their sizes, shapes, and material properties have been investigated with experiments, among which Lakatos et al. [1997] and Fontana [2003] presented results and studied human's capability to tell materials, sizes, and shapes of objects based on their sounds.","Modal Plus Residual Models: The sound synthesis model with a deterministic signal plus a stochastic residual was introduced to spectral synthesis by Serra and Smith [1990]. This approach analyzes an input audio and divides it into a deterministic part, which are time-variant sinusoids, and a stochastic part, which is obtained by spectral subtraction of the deterministic sinusoids from the original audio. In the resynthesis process, both parts can be modified to create various sound effects as suggested by Cook [1996; 1997; 2002] and Lloyd et al. [2011]. Methods for tracking the amplitudes of the sinusoids in audio dates back to Quateri and McAulay [1985], while more recent work [Serra and Smith III 1990; Serra 1997; Lloyd et al. 2011] also proposes effective methods for this purpose. All of these works directly construct the modal sounds with the extracted features, while our modal component is synthesized with the estimated material parameters. Therefore, although we adopt the same concept of modal plus residual synthesis for our framework, we face different constraints due to the new objective in material parameter estimation, and render these existing works not applicable to the problem addressed herein. Later, we will describe our feature extraction (Sec. 4) and residual compensation (Sec. 6) methods that are suitable for material parameter estimation.","Modal Sound Synthesis: The standard linear modal synthesis technique [Shabana 1997] is frequently used for modeling of dynamic deformation and physically-based sound synthesis. We adopt tetrahedral finite element models to represent any given geometry [O'Brien et al. 2002]. The displacements, x\u03b5R, in such a system can be calculated with the following linear deformation equation:\n\n\u2003\u2003(1)\n","where M, C, and K respectively represent the mass, damping and stiffness matrices. For small levels of damping, it is reasonable to approximate the damping matrix with Rayleigh damping, i.e. representing damping matrix as a linear combination of mass matrix and stiffness matrix: C=\u03b1M+\u03b2K. This is a well-established practice and has been adopted by many modal synthesis related works in both graphics and acoustics communities. After solving the generalized eigenvalue problem\n\n\u2003\u2003(2)\n\nthe system can be decoupled into the following form:\n\n+(\u03b1+\u03b2\u039b)\u2003\u2003(3)\n\nwhere \u039b is a diagonal matrix, containing the eigenvalues of Eqn. 2; U is the eigenvector matrix, and transforms x to the decoupled deformation bases q with x=Uq.\n","The solution to this decoupled system, Eqn. 3, is a bank of modes, i.e. damped sinusoidal waves. The i'th mode looks like:\n\nsin(2\u03c0\u0192)\u2003\u2003(4)\n\nwhere \u0192is the frequency of the mode, dis the damping coefficient, ais the excited amplitude, and \u03b8is the initial phase.\n","The frequency, damping, and amplitude together define the feature of \u03c6 mode i:\n\n\u03c6=(\u0192)\u2003\u2003(5)\n\nand will be used throughout the rest of this description. We ignore \u03b8in Eqn. 4 because it can be safely assumed as zero in our estimation process, where the object is initially at rest and struck at t=0. \u0192 and \u03c9 are used interchangeably to represent frequency, where \u03c9=2\u03c0\u0192.\n","Material properties: The values in Eqn. 4 depend on the material properties, the geometry, and the run-time interactions: aand depend on the run-time excitation of the object, while \u0192and ddepend on the geometry and the material properties as shown below. Solving Eqn. 3, we get",{"@attributes":{"id":"p-0060","num":"0059"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["d","i"]},"mo":"=","mrow":{"mfrac":{"mn":["1","2"]},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mi":"\u03b1","mo":"+","mrow":{"mi":"\u03b2","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["\u03bb","i"]}}}}}}},{"mrow":{"mo":["(",")"],"mn":"6"}}]}}}}},{"@attributes":{"id":"p-0061","num":"0060"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["f","i"]},"mo":"=","mrow":{"mfrac":{"mn":"1","mrow":{"mn":"2","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"\u03c0"}},"mo":"\u2062","msqrt":{"mrow":{"msub":{"mi":["\u03bb","i"]},"mo":"-","msup":{"mrow":{"mo":["(",")"],"mfrac":{"mrow":{"mi":"\u03b1","mo":"+","mrow":{"mi":"\u03b2","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["\u03bb","i"]}}},"mn":"2"}},"mn":"2"}}}}}},{"mrow":{"mo":["(",")"],"mn":"7"}}]}}}}},"It is feasible to assume the Rayleigh damping coefficients, \u03b1 and \u03b2, are shape-invariant material parameters. They can be transferred to another object if it bears no drastic shape or size change. Empirical experiments were carried out to verify this, and the results are included in Appendix A. The eigenvalues Xi's are calculated from M and K and determined by the geometry and tetrahedralization as well as the material properties: in our tetrahedral finite element model, M and K depend on mass density \u03c1. Young's modulus E, and Poisson's ratio \u03bd, if we assume the material is isotropic and homogeneous.","Constraint for modes: We observe modes in the adopted linear modal synthesis model have to obey some constraint due to its formulation. Because of the Rayleigh damping model we adopted, all estimated modes lie on a circle in the (\u03c9, d)-space, characterized by \u03b1 and \u03b2. This can be shown as follows. Rearranging Eqn. 6 and Eqn. 7 as",{"@attributes":{"id":"p-0064","num":"0063"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msubsup":{"mi":["\u03c9","i"],"mn":"2"},"mo":"+","msup":{"mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["d","i"]},"mo":"-","mfrac":{"mn":"1","mi":"\u03b2"}}},"mn":"2"}},"mo":"=","msup":{"mrow":{"mo":["(",")"],"mrow":{"mfrac":{"mn":"1","mi":"\u03b2"},"mo":"\u2062","msqrt":{"mrow":{"mn":"1","mo":"-","mrow":{"mi":["\u03b1","\u03b2"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}}}},"mn":"2"}}},{"mrow":{"mo":["(",")"],"mn":"8"}}]}}}},"br":{},"sub":["i","i","c","c","c"],"sup":["2","2","2 "]},"An example impact sound can be represented by high-level features collectively.","We first analyze and decompose a given example audio clip into a set of features, which will later be used in the subsequent phases of our pipeline, namely the parameter estimation and residual compensation parts. Next we present the detail of our feature extraction algorithm.","Multi-level power spectrogram representation: As shown in Eqn. 5, the feature of a mode is defined as its frequency, damping, and amplitude. In order to analyze the example audio and extract these feature values, we use a time-varying frequency representation called power spectrogram. A power spectrogram P for a time domain signal s[n], is obtained by first breaking it up into overlapping frames, and then performing windowing and Fourier transform on each frame:",{"@attributes":{"id":"p-0068","num":"0067"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mi":["m","\u03c9"],"mo":","}}},"mo":"=","msup":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"munderover":{"mo":"\u2211","mi":"n","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":[{"mi":"s","mo":"\u2061","mrow":{"mo":["[","]"],"mi":"n"}},{"mi":"w","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mi":["n","m"],"mo":"-"}}}],"mo":["\u2062","\u2062"],"msup":{"mi":"\u2147","mrow":{"mrow":{"mo":"-","mi":"j"},"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mi":["\u03c9","n"]}}}}},"mn":"2"}}},{"mrow":{"mo":["(",")"],"mn":"9"}}]}}}},"br":{}},"When computing the power spectrogram for a given sound clip, one can choose the resolutions of the time or frequency axes by adjusting the length of the window w. Choosing the resolution in one dimension, however, automatically determines the resolution in the other dimension. A high frequency resolution results in a low temporal resolution, and vice versa.","To fully accommodate the range of frequency and damping for all the modes of an example audio, we compute multiple levels of power spectrograms, with each level doubling the frequency resolution of the previous one and halving the temporal resolution. Therefore, for each mode to be extracted, a suitable level of power spectrogram can be chosen first, depending on the time and frequency characteristics of the mode.","Global-to-Local Scheme:","After computing a set of multi-level power spectrograms for a recorded example audio, we globally search through all levels for peaks (local maxima) along the frequency axis. These peaks indicate the frequencies where potential modes are located, some of which may appear in multiple levels. At this step the knowledge of frequency is limited by the frequency resolution of the level of power spectrogram. For example, in the level where the window size is 512 points, the frequency resolution is as coarse as 86 Hz. A more accurate estimate of the frequency as well as the damping value is obtained by performing a local shape fitting around the peak.","The power spectrogram of a damped sinusoid has a \u2018hill\u2019 shape, similar to the surface shown in the left hand graph in . The actual shape contains information of the damped sinusoid: the position and height of the peak are respectively determined by the frequency and amplitude, while the slope along the time axis and the width along the frequency axis are determined by the damping value. For a potential mode, a damped sinusoid with the initial guess of (f, d, a) is synthesized and added to the sound clip consisting of all the modes collected so far. The power spectrogram of the resulting sound clip is computed (shown as the hill shape in ), and compared locally with that of the recorded audio (the leftmost hill shape in ). An optimizer then searches in the continuous (f, d, a)-space to minimize the difference and acquire an accurate estimate of the frequency, damping, and amplitude of the mode at question.  illustrates this process.","The local shape fittings for all potential modes are performed in a greedy manner. Among all peaks in all levels, the algorithm starts with the one having the highest average power spectral density. If the shape fitting error computed is above a predefined threshold, we conclude that this level of power spectrogram is not sufficient in capturing the feature characteristics and thereby discard the result; otherwise the feature of the mode is collected. In other words, the most suitable time-frequency resolution (level) for a mode with a particular frequency is not predetermined, but dynamically searched for. Similar approaches have been proposed to analyze the sinusoids in an audio clip in a multi-resolution manner (e.g. Levine et al. [1998], where the time-frequency regions' power spectrogram resolution is predetermined).","We have tested the accuracy of our feature extraction with 100 synthetic sinusoids with frequencies and damping values randomly drawn from [0, 22050.0] (Hz) and [0.1, 1000](s) respectively. The average relative error is 0.040% for frequencies and 0.53% for damping values, which are accurate enough for our framework.","Comparison with Existing Methods:","The SMS method [Serra and Smith III 1990] is also capable of estimating information of modes. From a power spectrogram, it tracks the amplitude envelope of each peak over time, and a similar method is adopted by Lloyd et al. [2011]. Unlike our algorithm, which fits the entire local hill shape, they only track a single peak value per time frame. In the case where the mode's damping is high or the signal's background is noisy, this method yields high error.","Another feature extraction technique was proposed by Pai et al. [2001] and Corbett et al. [2007]. The method is known for its ability to separate modes within one frequency bin. In our frame-work, however, the features are only used to guide the subsequent parameter estimation process, which is not affected much by replacing two nearly duplicate features with one. Our method also offers some advantages and achieves higher accuracy in some cases compared with theirs. First, our proposed greedy approach is able to reduce the interference caused by high energy neighboring modes. Secondly, these earlier methods use a fixed frequency-time resolution that is not necessarily the most suitable for extracting all modes, while our method selects the appropriate resolution dynamically. Detailed comparisons and data can be found in Appendix B.","Using the extracted features (Sec. 4) and psychoacoustic principles (as described in this section), we introduce a parameter estimation algorithm based on an optimization framework for sound synthesis.","5.1 An Optimization Framework","We now describe the optimization work flow for estimating material parameters for sound synthesis. In the rest of this description, all data related to the example audio recordings are called reference data; all data related to the virtual object (which are used to estimate the material parameters) are called estimated data, and are denoted with a tilde, e.g. {tilde over (\u0192)}.","Reference Sound and Features:","The reference sound is the example recorded audio, which can be expressed as a time domain signal s[n]. The reference features \u03a6={\u03c6}={(\u0192, d, a)} are the features extracted from the reference sound, as described in Sec. 4.","Estimated Sound and Features:","In order to compute the estimated sound {tilde over (s)}[n] and estimated features {tilde over (\u03a6)}={{tilde over (\u03c6)}}={({tilde over (\u0192)}, {tilde over (d)}, \u00e3)} we first create a virtual object that is roughly the same size and geometry as the real-world object whose impact sound was recorded. We then tetrahedralize it and calculate its mass matrix M and stiffness matrix K. As mentioned in Sec. 3, we assume the material is isotropic and homogeneous. Therefore, the initial M and K can be found using the finite element method, by assuming some initial values for the Young's modulus, mass density, and Poisson's ratio, E, \u03c1, and \u03bd. The assumed eigenvalues \u03bb's can thereby be computed. For computational efficiency, we make a further simplification that the Poisson's ratio is held as constant. Then the eigenvalue \u03bbfor general E and \u03c1 is just a multiple of \u03bb:",{"@attributes":{"id":"p-0085","num":"0084"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["\u03bb","i"]},"mo":"=","mrow":{"mfrac":{"mi":"\u03b3","msub":{"mi":"\u03b3","mn":"0"}},"mo":"\u2062","msubsup":{"mi":["\u03bb","i"],"mn":"0"}}}},{"mrow":{"mo":["(",")"],"mn":"10"}}]}}}},"br":{},"sub":["0","0","0 "]},"We then apply a unit impulse on the virtual object at a point corresponding to the actual impact point in the example recording, which gives an excitation pattern of the eigenvalues as Eqn. 4. We denote the excitation amplitude of mode j as a. The superscript 0 notes that it is the response of a unit impulse; if the impulse is not unit, then the excitation amplitude is just scaled by a factor \u03c3,\n\n\u2003\u2003(11)\n\nCombining Eqn. 6, Eqn. 7, Eqn. 10, and Eqn. 11, we obtain a mapping from an assumed eigenvalue and its excitation (\u03bb, a) to an estimated mode with frequency {tilde over (\u0192)}, damping {tilde over (d)}, and amplitude \u00e3:\n",{"@attributes":{"id":"p-0087","num":"0086"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["\u03bb","j"],"mn":"0"},{"mi":["a","j"],"mn":"0"}],"mo":","}},{"mo":["(",")"],"mrow":{"msub":[{"mover":{"mi":"f","mo":"~"},"mi":"j"},{"mover":{"mi":"d","mo":"~"},"mi":"j"},{"mover":{"mi":"a","mo":"~"},"mi":"j"}],"mo":[",",","]}}],"mo":["\u2062","\u2062"],"mover":{"mo":"\u2192","mrow":{"mo":["{","}"],"mrow":{"mi":["\u03b1","\u03b2","\u03b3","\u03c3"],"mo":[",",",",","]}}}}},{"mrow":{"mo":["(",")"],"mn":"12"}}]}}}}},"The estimated sound {tilde over (s)}[n], is thereby generated by mixing all the estimated modes,",{"@attributes":{"id":"p-0089","num":"0088"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mover":{"mi":"s","mo":"~"},"mo":"\u2061","mrow":{"mo":["[","]"],"mi":"n"}},{"munderover":{"mo":"\u2211","mi":"j","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mo":["(",")"],"mrow":{"msub":{"mover":{"mi":"a","mo":"~"},"mi":"j"},"mo":["\u2062","\u2062"],"msup":{"mi":"\u2147","mrow":{"mo":"-","mrow":{"msub":{"mover":{"mi":"d","mo":"~"},"mi":"j"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"n","mo":"\/","msub":{"mi":["F","s"]}}}}}},"mrow":{"mi":"sin","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"2","mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mi":"\u03c0","mrow":{"msub":{"mover":{"mi":"f","mo":"~"},"mi":"j"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"n","mo":"\/","msub":{"mi":["F","s"]}}}}}}}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"13"}}]}}}},"br":{},"sub":"s "},"Difference Metric:","The estimated sound {tilde over (s)}[n] and features {tilde over (\u03a6)} can then be compared against the reference sound s[n] and features \u03a6, and a difference metric can be computed. If such difference metric function is denoted by \u03c0, the problem of parameter estimation becomes finding",{"@attributes":{"id":"p-0092","num":"0091"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mo":["{","}"],"mrow":{"mi":["\u03b1","\u03b2","\u03b3","\u03c3"],"mo":[",",",",","]}},"mo":"=","mtable":{"mtr":[{"mtd":{"mrow":{"mi":["arg","min","\u03a0"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]}}},{"mtd":{"mrow":{"mo":["{","}"],"mrow":{"mi":["\u03b1","\u03b2","\u03b3","\u03c3"],"mo":[",",",",","]}}}}]}}},{"mrow":{"mo":["(",")"],"mn":"14"}}]}}}}},"An optimization process is used to find such parameter set. The most challenging part of our work is to find a suitable metric function that can truly reflect what we view as the difference. Next we discuss the details about the metric design in Sec. 5.2 and the optimization process in Sec. 5.3.","5.2 Metric","Given an impact sound of a real-world object, the goal is to find a set of material parameters such that when they are applied to a virtual object of the same size and shape, the synthesized sounds have the similar auditory perception as the original recorded sounding object. By further varying the size, geometry, and the impact points of the virtual object, the intrinsic \u2018audio signature\u2019 of each material for the synthesized sound clips should closely resemble that of the original recording. These are the two criteria guiding the estimation of material parameters based on an example audio clip:","(1) the perceptual similarity of two sound clips;","(2) the audio material resemblance of two generic objects.","The perceptual similarity of sound clips can be evaluated by an \u2018image domain metric\u2019 quantified using the power spectrogram; while the audio material resemblance is best measured by a \u2018feature do-main metric\u2019\u2014both will be defined below,","Image Domain Metric:","Given a reference sound s[n] and an estimated sound {tilde over (s)}[n], their power spectrograms are computed using Eqn. 9 and denoted as two 2D images: I=P[m,w], \u0128={tilde over (P)}[m,w]. An image domain metric can then be expressed as\n\n\u03a0()\u2003\u2003(15)\n","Our goal is to find an estimated image \u0128 that minimizes a given image domain metric. This process is equivalent to image registration in computer vision and medical imaging.","Feature Domain Metric:","A feature \u00d8=\u0192, d, ais essentially a three dimensional point. As established in Sec. 3, the set of features of a sounding object is closely related to the material properties of that object. Therefore a metric defined in the feature space is useful in measuring the audio material resemblance of two objects. In other words, a good estimate of material parameters should map the eigenvalues of the virtual object to similar modes as that of the real object. A feature domain metric can be written as\n\n\u03a0(\u03a6,{tilde over (\u03a6)})\u2003\u2003(16)\n\nand the process of finding the minimum can be viewed as a point set matching problem in computer vision.\n","Hybrid Metric:","Both the auditory perceptual similarity and audio material resemblance would need to be considered for a generalized framework, in order to extract and transfer material parameters for modal sound synthesis using a recorded example to guide the automatic selection of material parameters. Therefore, we propose a \u2018hybrid\u2019 metric that takes into account both:\n\n\u03a0(,{tilde over (\u03a6)})\u2003\u2003(17)\n\nNext, we provide details on how we design and compute these metrics.\n\n5.2.1 Image Domain Metric\n","Given two power spectrogram images I and \u0128, a naive metric can be defined as their squared difference: \u03a0(I,\u0128)=\u03a3(P[m,\u03c9]\u2212{tilde over (P)}[m,\u03c9]). There are, however, several problems with this metric. The frequency resolution is uniform across the spectrum, and the intensity is uniformly weighted. As humans, however, we distinguish lower frequencies better than the higher frequencies, and mid-frequency signals appear louder than extremely low or high frequencies [Zwicker and Fastl 1999]. Therefore, directly taking squared difference of power spectrograms overemphasizes the frequency differences in the high-frequency components and the intensity differences near both ends of the audible frequency range. It is necessary to apply both frequency and intensity transformations before computing the image domain metric. We design these transformations based on psychoacoustic principles [Zwicker and Fastl 1999].","Frequency Transformation:","Studies in psychoacoustics suggested that humans have a limited capacity to discriminate between nearby frequencies, i.e. a frequency \u0192is not distinguishable from \u0192if \u0192is within \u0192\u00b1\u0394\u0192. The indistinguishable range \u0394\u0192 is itself a function of frequency, for example, the higher the frequency, the larger the indistinguishable range. To factor out this variation in \u0394\u0192 a different frequency representation, called critical-band rate z, has been introduced in psychoacoustics. The unit for z is Bark, and it has the advantage that while \u0394\u0192 is a function of \u0192 (measured in Hz), it is constant when measured in Barks. Therefore, by transforming the frequency dimension of a power spectrogram from \u0192 to z, we obtain an image that is weighted according to human's perceptual frequency differences. shows the relationship between critical-band rate z and frequency \u0192, z=Z(\u0192).","Intensity Transformation:","Sound can be described as the variation of pressure, p(t), and human auditory system has a high dynamical range, from 10Pa (threshold of hearing) to 10Pa (threshold of pain). In order to cope with such a broad range, the sound pressure level is normally used. For a sound with pressure p, its sound pressure level L, in decibel (abbreviated to dB-SPL) is defined as\n\n=20 log(),\u2003\u2003(18)\n\nwhere pis a standard reference pressure. While Lis just a physical value, loudness L is a perceptual value, which measures human sensation of sound intensity. In between, loudness level Lrelates the physical value to human sensation. Loudness level of a sound is defined as the sound pressure level of a 1-kHz tone that is perceived as loud as the sound. Its unit is phon, and is calibrated such that a sound with loudness level of 40 phon is as loud as a 1-kHz tone at 40 dB-SPL. Finally, loudness L is computed from loudness level. Its unit is sone, and is defined such that a sound of 40 phon is 1 sone; a sound twice as loud is 2 sone, and so on.\n",{"@attributes":{"id":"p-0109","num":"0108"},"figref":"FIG. 4","sub":["p","N ","p","N ","p ","p"]},"There are other psychoacoustic factors that can affect the human sensation of sound intensity. For example, van den Doel et al. [van den Doel and Pai 2002; van den Doel et al. 2004] considered the \u2018masking\u2019 effect, which describes the change of audible threshold in the presence of multiple stimuli, or modes in this case. However, they did not handle the loudness transform above the audible threshold, which is critical in our perceptual metric. Similar to the work by van den Doel and Pai [1998], we have ignored the masking effect.","Psychoacoustic Metric:","After transforming the frequency \u0192 (or equivalently, \u03c9) to the critical-band rate z and mapping the intensity to loudness, we obtain a transformed image T(I)=T(I)[m,z]. Different representations of a sound signal are shown in . Then we can define a psychoacoustic image domain metric as",{"@attributes":{"id":"p-0113","num":"0112"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["\u03a0","psycho"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"I","mo":",","mover":{"mi":"I","mo":"~"}}}},{"munderover":{"mo":"\u2211","mrow":{"mi":["m","z"],"mo":","},"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mrow":[{"mi":"T","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"I"}},{"mo":["[","]"],"mrow":{"mi":["m","z"],"mo":","}}],"mo":"\u2061"},{"mrow":[{"mi":"T","mo":"\u2061","mrow":{"mo":["(",")"],"mover":{"mi":"I","mo":"~"}}},{"mo":["[","]"],"mrow":{"mi":["m","z"],"mo":","}}],"mo":"\u2061"}],"mo":"-"}},"mn":"2"}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"19"}}]}}}},"br":[{},{}]},"As shown in Eqn. 8, in the (\u03c9,d)-space, modes under the assumption of Rayleigh damping lie on a circle determined by damping parameters \u03b1 and \u03b2, while features extracted from example recordings can be anywhere. Therefore, it is challenging to find a good match between the reference features \u03a6 and estimated features {tilde over (\u03a6)}.  shows a typical matching in the (\u0192,d)-space. Next we present a feature domain metric that evaluates such a match.","In order to compute the feature domain metric, we first transform the frequency and damping of feature points to another different 2D space. Namely, from (\u0192,d) to (x, y), where x=X(\u0192) and y=Y(d) encode the frequency and damping information respectively. With suitable transformations, the Euclidean distance defined in the transformed space can be more useful and meaningful for representing the perceptual difference. The distance between two feature points is thus written as\n\n(\u03c6,{tilde over (\u03c6)})=\u2225((\u0192),())\u2212(({tilde over (\u0192)}),())\u2225\u2003\u2003(20)\n\nFrequency and damping are key factors in determining material agreement, while amplitude indicates relative importance of modes. That is why we measure the distance between two feature points in the 2D (\u0192,d)-space and use amplitude to weigh that distance\n","For frequency, as described in Sec. 5.2.1 we know that the frequency resolution of human is constant when expressed as critical-band rate and measured in Barks: \u0394\u0192(\u0192)\u221d\u0394z. Therefore it is a suitable frequency transformation\n\n(\u0192)=(\u0192)\u2003\u2003(21)\n\nwhere cis some constant coefficient.\n","For damping, although human can roughly sense that one mode damps faster than another, directly taking the difference in damping value d is not feasible. This is due to the fact that humans cannot distinguish between extremely short bursts [Zwicker and Fastl 1999]. For a damped sinusoid, the inverse of the damping value, 1\/d, is proportional to its duration, and equals to how long before the signal decays to eof its initial amplitude. While distance measured in damping values overemphasizes the difference between signals with high d values (corresponding to short bursts), distance measured in durations does not. Therefore",{"@attributes":{"id":"p-0118","num":"0117"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"Y","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"d"}},{"msub":{"mi":["c","d"]},"mo":"\u2062","mfrac":{"mn":"1","mi":"d"}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"22"}}]}}}},"br":{},"sub":"d ","figref":["FIG. 6()","FIG. 6()"]},"Having defined the transformed space, we then look for matching the reference and estimated feature points in this space. Our matching problem belongs to the category where there is no known correspondence, i.e. no prior knowledge about which point in one set should be matched to which point in another. Furthermore, because there may be several estimated feature points in the neighborhood of a reference point or vice versa, the matching is not necessarily a one-to-one relationship. There is also no guarantee that an exact matching exist, because (1) the recorded material may not obey the Rayleigh damping model, (2) the discretization of the virtual object and the assumed hit point may not give the exact eigenvalues and excitation pattern of the real object. Therefore we are merely looking for a partial, approximate matching.","The simplest point-based matching algorithm that solves problems in this category (i.e. partial, approximate matching without known correspondence) is Iterative Closest Points. It does not work well, however, when there is a significant number of feature points that cannot be matched [Besl and McKay 1992], which is possibly the case in our problem. Therefore, we define a metric, Match Ratio Product, that meets our need and is discussed next.","For a reference feature point set \u03a6, we define a match ratio that measures how well they are matched by an estimated feature point set {tilde over (\u03a6)}. This set-to-set match ratio, defined as",{"@attributes":{"id":"p-0122","num":"0121"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"R","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"\u03a6","mo":",","mover":{"mi":"\u03a6","mo":"~"}}}},"mo":"=","mfrac":{"mrow":[{"munderover":{"mo":"\u2211","mi":"i","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":["\u03c9","i"]},"mo":"\u2062","mrow":{"mi":"R","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["\u03d5","i"]},"mo":",","mover":{"mi":"\u03a6","mo":"~"}}}}}},{"munderover":{"mo":"\u2211","mi":"i","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["\u03c9","i"]}}]}}},{"mrow":{"mo":["(",")"],"mn":"23"}}]}}}},"br":{}},{"@attributes":{"id":"p-0123","num":"0122"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"R","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["\u03d5","i"]},"mo":",","mover":{"mi":"\u03a6","mo":"~"}}}},"mo":"=","mfrac":{"mrow":[{"munderover":{"mo":"\u2211","mi":"j","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":["u","ij"]},"mo":"\u2062","mrow":{"mi":"k","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["\u03d5","i"]},{"mover":{"mi":"\u03d5","mo":"~"},"mi":"j"}],"mo":","}}}}},{"munderover":{"mo":"\u2211","mi":"j","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["u","ij"]}}]}}},{"mrow":{"mo":["(",")"],"mn":"24"}}]}}}},"br":{},"sub":["1","j","i","j","i ","i ","ij "]},{"@attributes":{"id":"p-0124","num":"0123"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mover":{"mi":"R","mo":"~"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"\u03a6","mo":",","mover":{"mi":"\u03a6","mo":"~"}}}},"mo":"=","mfrac":{"mrow":[{"munderover":{"mo":"\u2211","mi":"j","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mover":{"mi":"\u03c9","mo":"~"},"mi":"j"},"mo":"\u2062","mrow":{"mi":"R","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mover":{"mi":"\u03d5","mo":"~"},"mi":"j"},"mo":",","mi":"\u03a6"}}}}},{"munderover":{"mo":"\u2211","mi":"i","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mover":{"mi":"\u03c9","mo":"~"},"mi":"j"}}]}}},{"mrow":{"mo":["(",")"],"mn":"25"}}]}}}}},"The match ratios for the reference and the estimated feature point sets are then combined to form the Match Ratio Product (MRP), which measures how well the reference and estimated feature point sets match with each other,\n\n\u03a0(\u03a6,{tilde over (\u03a6)})=\u2212\u2003\u2003(26)\n\nThe negative sign is to comply with the minimization framework. Multiplying the two ratios penalizes the extreme case where either one of them is close to zero (indicating poor matching).\n","The normalization processes in Eqn. 23 and Eqn. 25 are necessary. Notice that the denominator in Eqn. 25 is related to the number of estimated feature points inside the audible range, \u00d1(in fact \u03a3{tilde over (\u03c9)}=\u00d1if all {tilde over (\u03c9)}=1). Depending on the set of parameters, \u00d1can vary from a few to thousands. Factoring \u00d1prevents the optimizer from blindly introducing more modes into the audible range, which may increase the absolute number of matched feature points, but may not necessarily increase the match ratios. Such averaging techniques have also been employed to improve the robustness and discrimination power of point-based object matching methods [Dubuisson and Jain 1994; Gope and Kehtarnavaz 2007].","In practice, the weights \u03c9's and if u's, can be assigned according to the relative energy or perceptual importance of the modes. The point-to-point match score k(\u03c6,{tilde over (\u03c6)}), can also be tailored to meet different needs. The constants and function forms used in this section are listed in Appendix C.","5.2.3 Hybrid Metric","Finally, we combine the strengths from both image and feature domain metrics by defining the following hybrid metric:",{"@attributes":{"id":"p-0129","num":"0128"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["\u03a0","hybrid"]},"mo":"=","mfrac":{"msub":{"mi":["\u03a0","psycho"]},"mrow":{"mo":["\uf603","\uf604"],"msub":{"mi":["\u03a0","MRP"]}}}}},{"mrow":{"mo":["(",")"],"mn":"27"}}]}}}},"br":[{},{},{}]},"First, as elaborated by Eqn. 8 in Sec. 3, the estimated modes are constrained by a circle in the (\u03c9,d)-space. Secondly, although there are many reference modes, they are not evenly excited by a given impact\u2014we observe that usually the energy is mostly concentrated in a few dominant ones. Therefore, a good estimate of \u03b1 and \u03b2 must define a circle that passes through the neighborhood of these dominant reference feature points. We also observe that in order to yield a low metric value, there must be at least one dominant estimated mode at the frequency of the most dominant reference mode.","We thereby generate our starting points by first drawing two dominant reference feature points from a total of Nof them, and find the circle passing through these two points. This circle is potentially a \u2018good\u2019 circle, from which we can deduce a starting estimate of \u03b1 and \u03b2 using Eqn. 8. We then collect a set of eigenvalues and amplitudes (defined in Sec. 5.1) {(\u03bb,a)}, such that there does not exist any (\u03bb,a) that simultaneously satisfies \u03bb<\u03bband a>a. It can be verified that the estimated modes mapped from this set always includes the one with the highest energy, for any mapping parameters {\u03b1,\u03b2,\u03b3,\u03c3} used in Eqn. 12. Each (\u03bb,a) in this set is then mapped and aligned to the frequency of the most dominant reference feature point, and its amplitude is adjusted to be identical as the latter. This step gives a starting estimate of \u03b3 and \u03c3. Each set of {\u03b1,\u03b2,\u03b3,\u03c3} computed in this manner is a starting point, and may lead to a different local minimum. We choose the set which results in the lowest metric value to be our estimated parameters. Although there is no guarantee that a global minimum will be met, we find that the results produced with this strategy are satisfactory in our experiments, as discussed in Sec. 7.","With the optimization proposed in Sec. 5, a set of parameters that describe the material of a given sounding object can be estimated, and the produced sound bears a close resemblance of the material used in the given example audio. However, linear modal synthesis alone is not capable of synthesizing sounds that are as rich and realistic as many real-world recordings. Firstly, during the short period of contact, not all energy is transformed into stable vibration that can be represented with a small number of damped sinusoids, or modes. The stochastic and transient nature of the non-modal components makes sounds in nature rich and varying. Secondly, as discussed in Sec. 3, not all features can be captured due to the constraints for modes in the synthesis model. In this section we present a method to account for the residual, which approximates the difference between the real-world recordings and the modal synthesis sounds. In addition, we propose a technique for transferring the residual with geometry and interaction variation. With the residual computation and transfer algorithms introduced below, more realistic sounds that automatically vary with geometries and hitting points can be generated with a small computation overhead.","6.1 Residual Computation","In this section we discuss how to compute the residual from the recorded sound and the synthesized modal sound generated with the estimated parameters.","Previous works have also looked into capturing the difference between a source audio and its modal component [Serra and Smith III 1990; Serra 1997; Lloyd et al. 2011]. In these works, the modal part is directly tracked from the original audio, so the residual can be calculated by a straightforward subtraction of the power spectrograms. The synthesized modal sound in our framework, however, is generated solely from the estimated material parameters. Although it preserves the intrinsic quality of the recorded material, in general the modes in our synthesized sounds are not perfectly aligned with the recorded audio. An example is shown in  and . It is due to the constraints in our sound synthesis model and discrepancy between the discretized virtual geometries and the real-world sounding objects. As a result, direct subtraction does not work in this case to generate a reasonable residual. Instead, we first compute an intermediate data, called the represented sound. It corresponds to the part in the recorded sound that is captured, or represented, by our synthesized sound. This represented sound () can be directly subtracted from the recorded sound to compute the residual ().","The computation of the represented sound is based on the following observations. Consider a feature (described by \u03c6) extracted from the recorded audio. If it is perfectly captured by the estimated modes, then it should not be included in the residual and should be completely subtracted from the recorded sound. If it is not captured at all, it should not be subtracted from the recorded sound, and if it is approximated by an estimated mode, it should be partially subtracted. Since features closely represent the original audio, they can be directly subtracted from the recorded sound.","The point-to-set match ratio R(\u03c6,{tilde over (\u03a6)}) proposed in Sec. 5.2 essentially measures how well a reference feature \u03c6is represented (matched) by all the estimated modes. This match ratio can be conveniently used to determine how much of the corresponding feature should be subtracted from the recording.","The represented sound is therefore obtained by adding up all the reference features that are respectively weighted by the match ratio of the estimated modes. And the power spectrogram of the residual is obtained by subtracting the power spectrogram of the represented sound from that of the recorded sound.  illustrates the residual computation process.","6.2 Residual Transfer","Residual of one particular instance (i.e. one geometry and one hit point) can be obtained through the above described residual computation method. However, when synthesizing sounds for a different geometry undergoing different interaction with other rigid bodies, the residual audio needs to vary accordingly. Lloyd et al. [2011] proposed applying a random dip filter on the residual to provide variation. While this offers an attractive solution for quickly generating modified residual sound, it does not transfer accordingly with the geometry change or the dynamics of the sounding object.","6.2.1 Exemplary Algorithm","As discussed in previous sections, modes transfer naturally with geometries in the modal analysis process, and they respond to excitations at runtime in a physical manner. In other words, the modal component of the synthesized sounds already provides transferability of sounds due to varying geometries and dynamics. Hence, we compute the transferred residual under the guidance of modes as follows.","Given a source geometry and impact point, we know how to transform its modal sound to a target geometry and impact points. Equivalently, we can describe such transformation as acting on the power spectrograms, transforming the modal power spectrogram of the source, Pto that of the target, P:",{"@attributes":{"id":"p-0141","num":"0140"},"maths":{"@attributes":{"id":"MATH-US-00015","num":"00015"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msubsup":[{"mi":["P","modal","s"]},{"mi":["P","modal","t"]}],"mo":["\u2062","\u2062"],"mover":{"mo":"\u2192","mi":"H"}}},{"mrow":{"mo":["(",")"],"mn":"28"}}]}}}},"br":{}},{"@attributes":{"id":"p-0142","num":"0141"},"maths":{"@attributes":{"id":"MATH-US-00016","num":"00016"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msubsup":[{"mi":["P","residual","s"]},{"mi":["P","residual","t"]}],"mo":["\u2062","\u2062"],"mover":{"mo":"\u2192","mi":"H"}}},{"mrow":{"mo":["(",")"],"mn":"29"}}]}}}},"br":{}},"More specifically, H can be decomposed into per-mode transform functions, H, which transforms the power spectrogram of a source mode \u03c6=(\u0192,d,a) to a target mode \u03c6=(\u0192,d,a). H, can further be described as a series of operations on the source power spectrogram P: (1) the center frequency is shifted from \u0192to \u0192; (2) the time dimension is stretched according to the ratio between dand d; (3) the height (intensity) is scaled pixel-by-pixel to match PThe per-mode transform is performed in the neighborhood of \u0192, namely between \u00bd(\u0192+\u0192) and \u00bd(\u0192+\u0192), to that of \u0192namely between \u00bd(\u0192+\u0192) and \u00bd(\u0192+\u0192).","The per-mode transform is performed for all pairs of source and target modes, and the local residual power spectrograms are \u2018stitched\u2019 together to form the complete P. Finally, the time-domain signal of the residual is reconstructed from P, using an iterative inverse STFT algorithm by Griffin and Lim [2003]. Algorithm 1 shows the complete feature-guided residual transfer algorithm. With this scheme, the transform of the residual power spectrogram is completely guided by the appropriate transform of modes. The resulting residual changes consistently with the modal sound. Since the modes transform with the geometry and dynamics in a physical manner, the transferred residual also faithfully reflects this variation.",{"@attributes":{"id":"p-0145","num":"0144"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Algorithm 1: Residual Transformation at Runtime"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"196pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Input:","source modes \u03a6= {\u03c6}, target modes \u03a6= {\u03c6}, and"]},{"entry":[{},"source residual audio s[n]"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"Output: target residual audio s[n]"},{"entry":"\u03a8 \u2190 DetermineModePairs(\u03a6, \u03a6)"},{"entry":"foreach mode pair (\u03c6, \u03c6) \u2208 \u03a8 do"},{"entry":"\u2002|\u2003P\u2032 \u2190 ShiftSpectrogram( P, \u0394frequency)"},{"entry":"\u2002|\u2003P\u2033 \u2190 StretchSpectrogram( P\u2032, damping_ratio)"},{"entry":"\u2002|\u2003A \u2190 FindPixelScale(P, P\u2033)"},{"entry":"\u2002|\u2003P\u2032 \u2190 ShiftSpectrogram(P, \u0394frequency)"},{"entry":"\u2002|\u2003P\u2033 \u2190 StretchSpectrogram(P\u2032, damping_ratio)"},{"entry":"\u2002|\u2003P\u2033 \u2190 MultiplyPixelScale(P\u2033, A)"},{"entry":"\u2002|\u2003(\u03c9, \u03c9) \u2190 FindFrequencyRange(\u03c6, \u03c6)"},{"entry":"\u2002|\u2003P[m, \u03c9, . . . , \u03c9] \u2190 P\u2033 [m, \u03c9, . . . , \u03c9]"},{"entry":"end"},{"entry":"s[n] \u2190 IterativeInverseSTFT(P)"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}},"br":[{},{}],"sub":"dominant "},"The most computation costly part of residual transfer is the iterative inverse STFT process. We are able to obtain acceptable time-domain reconstruction from the power spectrogram when we limit the iteration of inverse SIFT to 10. Hardware acceleration is used in our implementation to ensure fast STFT computation. More specifically, CUFFT, a CUDA implementation of Fast Fourier Transform, is adopted for parallelized inverse STFT operations. Also note that residual transfer computation only happens when there is a contact event, the obtained time-domain residual signal can be used until the next event.","On an NVIDIA GTX 480 graphics card, if the contact events arrive at intervals around 1\/30s, the residual transfer in the current implementation can be successfully evaluated in time.","Parameter Estimation:","Before working on real-world recordings, we design an experiment to evaluate the effectiveness of our parameter estimation with synthetic sound clips. A virtual object with known material parameters {\u03b1,\u03b2,\u03b3,\u03c3} and geometry is struck, and a sound clip is synthesized by mixing the excited modes. The sound clip is entered to the parameter estimation pipeline to test if the same parameters are recovered. Three sets of parameters are tested and the results are shown in .","This experiment demonstrates that if the material follows the Rayleigh damping model, the proposed framework is capable of estimating the material parameters with high accuracy. Below we will see that real materials do not follow the Rayleigh damping model exactly, but the presented framework is still capable of finding the closest Rayleigh damping material that approximates the given material.","We estimate the material parameters from various real-world audio recordings: a wood plate, a plastic plate, a metal plate, a porcelain plate, and a glass bowl. For each recording, the parameters are estimated using a virtual object that is of the same size and shape as the one used to record the audio clips. When the virtual object is hit at the same location as the real-world object, it produces a sound similar to the recorded audio, as shown in  and the supplementary video.",{"@attributes":{"id":"p-0152","num":"0151"},"figref":"FIG. 11"},"Transferred Parameters and Residual:","The parameters estimated can be transferred to virtual objects with different sizes and shapes. Using these material parameters, a different set of resonance modes can be computed for each of these different objects. The sound synthesized with these modes preserves the intrinsic material quality of the example recording, while naturally reflect the variation in virtual object's size, shape, and interactions in the virtual environment.",{"@attributes":{"id":"p-0155","num":"0154"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE I"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Estimated parameters"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"161pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"7pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"Parameters",{}]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"49pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Material","\u03b1","\u03b2","\u03b3","\u03c3"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}},{"entry":["Wood","2.1364e+0","3.0828e\u22126","6.6625e+5","3.3276e\u22126"]},{"entry":["Plastic","5.2627e+1","8.7753e\u22127","8.9008e+4","2.2050e\u22126"]},{"entry":["Metal","6.3035e+0","2.1160e\u22128","4.5935e+5","9.2624e\u22126"]},{"entry":["Glass","1.8301e+1","1.4342e\u22127","2.0282e+5","1.1336e\u22126"]},{"entry":["Porcelain","3.7388e\u22122","8.4142e\u22128","3.7068e+5","4.3800e\u22127"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}},{"entry":"Refer to Sec. 3 and Sec. 5 for the definition and estimation of these parameters."}]}}]}}},"Moreover, taking the difference between the recording of the example real object and the synthesized sound from its virtual counterpart, the residual is computed. This residual can also be transferred to other virtual objects, using methods described in Sec. 6.",{"@attributes":{"id":"p-0157","num":"0156"},"figref":"FIG. 12"},"Comparison with Real Recordings:",{"@attributes":{"id":"p-0159","num":"0158"},"figref":"FIG. 13"},"We applied the estimated parameters for various virtual objects in a scenario where complex interactions take place, as shown in  and the accompanying video.","Performance:","Table II shows the timing for our system running on a single core of a 2.80 GHz Intel Xeon X5560 machine. It should be noted that the parameter estimation is an offline process: it needs to be run only once per material, and the result can be stored in a database for future reuse.","For each material in column one, multiple starting points are generated first as described in Sec. 5.3, and the numbers of starting points are shown in column two. From each of these starting points, the optimization process runs for an average number of iterations (column three) until convergence. The average time taken for the process to converge is shown in column four. The convergence is defined as when both the step size and the difference in metric value are lower than their respective tolerance values, \u0394, and \u0394. The numbers reported in Table II are measured with \u0394=1e-4 and \u0394=1e-8.",{"@attributes":{"id":"p-0164","num":"0163"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE II"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Offline Computation for Material Parameter Estimation"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"56pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},"#starting","average","average"]},{"entry":[{},"Material","points","#iteration","time (s)"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"63pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"56pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Wood","60","1011","46.5"]},{"entry":[{},"Plastic","210","904","49.4"]},{"entry":[{},"Metal","50","1679","393.5"]},{"entry":[{},"Porcelain","80","1451","131.3"]},{"entry":[{},"Glass","190","1156","68.9"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]}]}}]}}},"To assess the effectiveness of our parameter estimation algorithm, we designed an experiment to evaluate the auditory perception of the synthesized sounds of five different materials. Each subject is presented with a series of 24 audio clips with no visual image or graphical animation. Among them, 8 are audio recordings of sound generated from hitting a real-world object, and 16 are synthesized using the techniques described herein. For each audio clip, the subject is asked to identify among a set of 5 choices (wood, plastic, metal, porcelain, and glass), from which the sound came. A total of 53 subjects (35 women and 18 men), from age of 22 to 71, participated in this study. The 8 real objects are: a wood plate, a plastic plate, a metal plate, a porcelain plate, and four glass bowls with different sizes. The 16 virtual objects are: three different shapes (a plate, a stick, and a bunny) for each of these four materials: wood, plastic, metal, and porcelain, plus four glass bowls with different sizes.","We show the cumulative recognition rates of the sounding materials in two separate matrices: Table III presents the recognition rates of sounds from real-world materials, and Table IV reflects the recognition rates of sounds from synthesized virtual materials. The numbers are normalized with the number of subjects answering the questions. For example, Row 3 of Table III means that for a given real-world sound recorded from hitting a metal object, none of the subjects thought it came from wood or plastic, 66.1% of them thought it came from metal, 9.7% of them thought it came from porcelain and 24.2% of them thought it came from glass. Correspondingly, Row 3 of Table IV shows that for a sound synthesized with our estimated parameters for metal, the percentage of subjects thinking that it came from wood, plastic, metal, porcelain or glass respectively.","We found that the successful recognition rate of virtual materials using our synthesized sounds compares favorably to the recognition rate of real materials using recorded sounds. The difference of the recognition rates (recorded minus synthesized) is close to zero for most of the materials, with 95% confidence intervals shown in Table V. A confidence interval covering zero means that the difference in recognition rate is not statistically significant. If both endpoints of a confidence interval are positive, the recognition rate of the real material is significantly higher than that of the virtual material; if both endpoints are negative, the recognition rate of the real material is significantly lower.","In general, for both recorded and synthesized sounds, several subjects have reported difficulty in reliably differentiating between wooden and dull plastic materials and between glass and porcelain. On the other hand, some of the subjects suggested that we remove redundant audio clips, which are in fact distinct sound clips of recordings generated from hitting real materials and their synthesized counterparts.",{"@attributes":{"id":"p-0169","num":"0168"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE III"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Material Recognition Rate Matrix: Recorded Sounds"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"175pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"7pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"Recognized Material",{}]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"6"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Recorded","Wood","Plastic","Metal","Porcelain","Glass"]},{"entry":["Material","(%)","(%)","(%)","(%)","(%)"]},{"entry":{"@attributes":{"namest":"1","nameend":"6","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"6"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"49pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"5","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Wood","50.7","47.9","0.0","0.0","1.4"]},{"entry":["Plastic","37.5","37.5","6.3","0.0","18.8"]},{"entry":["Metal","0.0","0.0","66.1","9.7","24.2"]},{"entry":["Porcelain","0.0","0.0","1.2","15.1","83.7"]},{"entry":["Glass","1.7","1.7","1.7","21.6","73.3"]},{"entry":{"@attributes":{"namest":"1","nameend":"6","align":"center","rowsep":"1"}}}]}}]}}},{"@attributes":{"id":"p-0170","num":"0169"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE IV"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Material Recognition Rate Matrix:"},{"entry":"Synthesized Sounds Using Our Method"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"168pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"Recognized Material"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"6"},"colspec":[{"@attributes":{"colname":"1","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"28pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Synthesized","Wood","Plastic","Metal","Porcelain","Glass"]},{"entry":["Material","(%)","(%)","(%)","(%)","(%)"]},{"entry":{"@attributes":{"namest":"1","nameend":"6","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"6"},"colspec":[{"@attributes":{"colname":"1","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"5","colwidth":"42pt","align":"char","char":"."}},{"@attributes":{"colname":"6","colwidth":"28pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Wood","52.8","43.5","0.0","0.0","3.7"]},{"entry":["Plastic","43.0","52.7","0.0","2.2","2.2"]},{"entry":["Metal","1.8","1.8","69.6","15.2","11.7"]},{"entry":["Porcelain","0.0","1.1","7.4","29.8","61.7"]},{"entry":["Glass","3.3","3.3","3.8","40.4","49.2"]},{"entry":{"@attributes":{"namest":"1","nameend":"6","align":"center","rowsep":"1"}}}]}}]}}},{"@attributes":{"id":"p-0171","num":"0170"},"tables":{"@attributes":{"id":"TABLE-US-00006","num":"00006"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE V"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"95% Confidence Interval of Difference in Recognition Rates"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"42pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Wood(%)","Plastic(%)","Metal(%)","Porcelain(%)","Glass (%)"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}},{"entry":["(\u221217.1; 12.9)","(\u221244.7; 14.3)","(\u221218.2; 11.3)","(\u221227.7; \u22121.6)","(12.6; 35.6)"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}}]}}},"The subject matter described herein includes a data-driven, physically-based sound synthesis algorithm using an example audio clip from real-world recordings. By exploiting psychoacoustic principles and feature identification using linear modal analysis, we are able to estimate the appropriate material parameters that capture the intrinsic audio properties of the original materials and transfer them to virtual objects of different sizes, shape, geometry and pair-wise interaction. We also propose an effective residual computation technique to compensate for linear approximation of modal synthesis.","Although our experiments show successful results in estimating the material parameters and computing the residuals, it has some limitations. Our model assumes linear deformation and Rayleigh damping. While offering computational efficiency, these models cannot always capture all sound phenomena that real world materials demonstrate. Therefore, it is practically impossible for the modal synthesis sounds generated with our estimated material parameters to sound exactly the same as the real-world recording. Our feature extraction and parameter estimation depend on the assumption that the modes do not couple with one another. Although it holds for the objects in our experiments, it may fail when recording from objects of other shapes, e.g. thin shells where nonlinear models would be more appropriate [Chadwick et al. 2009].","We also assume that the recorded material is homogeneous and isotropic. For example, wood is highly anisotropic when measured along or across the direction of growth. The anisotropy greatly affects the sound quality and is an important factor in making high-precision musical instruments.","Despite these limitations, our proposed framework is general, allowing future research to further improve and use different individual components. For example, the difference metric now considers the psychoacoustic factors and material resemblance through power spectrogram comparison and feature matching. It is possible that more factors can be taken into account, or a more suitable representation, as well as a different similarity measurement of sounds can be found.","The optimization process approximates the global optimum by searching through all \u2018good\u2019 starting points. With a deeper investigation of the parameter space and more experiments, the performance may be possibly improved by designing a more efficient scheme to navigate the parameter space, such as starting-point clustering, early pruning, or a different optimization procedure can be adopted.","Our residual computation compensates the difference between the real recording and the synthesized sound, and we proposed a method to transfer it to different objects. While our transferring technique is consistent with the other components of the sound (i.e. the modes) and produces plausible results, it is not the only way\u2014much due to the fact that the origin and nature of residual is unknown.","The subject matter described herein includes a system that can quickly estimate the optimal material parameters and compute the residual merely based on a single recording with a non-expensive and simple setup. However, where a small number of recordings of the same material are given as input, machine learning techniques can be used to determine the set of parameters with maximum likelihood, and it could be an area worth exploring. Finally, we would like to extend this framework to other non-rigid objects and fluids, and possibly nonlinear modal synthesis models as well.","In summary, data-driven approaches have proven useful in areas in computer graphics, including rendering, lighting, character animation, and dynamics simulation. With promising results that are transferable to virtual objects of different geometries, sizes, and interactions, this work is the first rigorous treatment of the problem on automatically determining the material parameters for physically-based sound synthesis using a single sound recording, and it offers a new direction for combining example-guided and modal-based approaches.","The disclosure of each of the following references is incorporated herein by reference in its entirety.",{"@attributes":{"id":"p-0181","num":"0000"},"ul":{"@attributes":{"id":"ul0001","list-style":"none"},"li":["ADMEN, J.-M. 1991. Representations of musical signals. MIT Press, Cambridge, Mass., USA, Chapter The missing link: modal synthesis, 269-298.","AUDIOKINETIC. 2011. Wwise SoundSeed Impact. http:\/\/www.audiokinetic.com\/en\/products\/wwise-add-ons\/soundseed\/introduction.","BESL, P. J. AND MCKAY, N. D. 1992. A method for registration of 3-D shapes. 239-256.","BONNEEL, N., DRETTAKIS, G., TSINGOS, N., VIALTIO-DELMON, I., AND JAMES, D. 2008. Fast modal sounds with scalable frequency-domain synthesis. () 27, 3, 24.","CHADWICK, J. N., AN, S. S., AND JAMES, D. L. 2009. Harmonic shells: a practical nonlinear sound model for near-rigid thin shells. In 09: 2009 . ACM, New York, N.Y., USA, 1-10.","CHADWICK, J. N. AND JAMES, D. L. 2011. Animating fire with sound. In (). Vol. 30. ACM, 84.","COOK, P. R. 1996. Physically informed sonic modeling (PhISM): percussive synthesis. In 1996 . The International Computer Music Association, 228-231.","COOK, P. R. 1997. Physically informed sonic modeling (phism): Synthesis of percussive sounds. 21, 3, 38-49.","COOK, P. R. 2002. . A. K. Peters, Ltd., Natick, Mass., USA.","CORBETT, R., VAN DEN DOEL, K., LLOYD, J. E., AND HEIDRICH, W. 2007. Timbrefields: 3d interactive sound models for real-time audio. 16, 6, 643-654.","DOBASHI, Y., YAMAMOTO, T., AND NISFIITA, T. 2003. Real-time rendering of aerodynamic sound using sound textures based on computational fluid dynamics. 22, 3, 732-740.","DOBASHI, Y., YAMAMOTO, T., AND NISHITA, T. 2004. Synthesizing sound from turbulent field using sound textures for interactive fluid simulation. In . Vol. 23. Wiley Online Library, 539-545.","DUBUISSON, M. P. AND JAIN, A. K. 1994. A modified hausdorff distance for object matching. In 1202 . Vol. 1. IEEE Comput. Soc. Press, 566-568.","FONTANA, F. 2003. . Mondo Estremo.","GOPE, C. AND KEHTARNAVAZ, N. 2007. Affine invariant comparison of point-sets using convex hulls and hausdorff distances. 40, 1,309-320.","GRIFFIN, D. AND LIM, J. 2003. Signal estimation from modified short-time Fourier transform. 32, 2,236-243.","ISO. 2003. ISO 226: 2003: -. International Organization for Standardization.","JAMES, D., BARBIG, J., AND PAT, D. 2006. Precomputed acoustic transfer: output-sensitive, accurate sound generation for geometrically complex vibration sources. In 2006 . ACM, 995.","KLATZKY, R. L., PAI, D. K., AND KROTKOV, E. P. 2000. Perception of material from contact sounds. 9, 399-410.","KROTKOV, E., KLATZKY, R., AND ZUMEL, N. 1996. Analysis and synthesis of the sounds of impact based on shape-invariant properties of materials. In 1996 (96) -7270. ICPR '96 IEEE Computer Society, Washington, D.C., USA, 115-.","LAGARIAS, J. C., REEDS, J. A., WRIGHT, M. H., AND WRIGHT, P. E. 1999. Convergence properties of the Nelder-Mead simplex method in low dimensions. 9, 1,112-147.","LAKATOS, S., MOADAMS, S., AND CAUSSE, R. 1997. The representation of auditory source characteristics: Simple geometric form. , & 59, 8, 1180-1190.","LEVINE, S. N VERMA, T. S., AND SMITH, J. O. 1998. Multiresolution sinusoidal modeling for wideband audio with modifications. In 1998. 1998 . Vol. 6. IEEE, 3585-3588 vol. 6.","LLOYD, D. B., RAGHUVANSHI, N., AND GOVINDARAJU, N. K. 2011. Sound Synthesis for Impact Sounds in Video Games. In 3","MORCHEN, F., ULTSCH, A., THIES, M., AND LOHKEN, I. 2006. Modeling timbre distance with temporal statistics from polyphonic music. 14, 1 (January), 81-90.","Moss, W., YEH, H., HONG, J., LIN, M., AND MANOCHA, D. 2010. Sounding Liquids: Automatic Sound Synthesis from Fluid Simulation. ().","O'BRIEN, J. F., COOK, P. R., AND ESSL, G. 2001. Synthesizing sounds from physically based motion. In 2001. ACM Press, 529-536.","O'BRIEN, J. F., SHEN, C., AND GATCHALIAN, C. M. 2002. Synthesizing sounds from rigid-body simulations. In 2002 . ACM Press, 175-181.","OPPENHEIM, A. V., SCHAFER, R. W., AND BUCK, J. R. 1989-. Vol. 1999. Prentice hall Englewood Cliffs, N.J.","PAT, D. K., DOF.L., K. V. D., JAMES, D. L., LANG, J., LLOYD, J. E., RICHMOND, J. L., AND YAU, S. H. 2001. Scanning physical interaction behavior of 3d objects. In 28. SIGGRAPH '01. ACM, New York, N.Y., USA, 87-96.","PAMPALK, E., RAUBER, A., AND MERKL, D. 2002. Content-based organization and visualization of music archives. In . ACM, 570-579.","PICARD, C., TSINGOS, N., AND FAURE, F. 2009. Retargetting example sounds to interactive physics-driven animations. In 35-","QUATIERI, T. AND McAHLAY, R. 1985. Speech transformations based on a sinusoidal representation. In 85. Vol. 10. 489-492.","RAGHUVANSHI, N. AND LIN, M. 2006. Symphony: Real-time physically-based sound synthesis. In 3","REN, Z., YEII, H., AND LIN, M. 2010. Synthesizing contact sounds between textured models. In (), 2010 139-146.","ROADS, C. 2004. . The MIT Press.","SERRA, X. 1989. A system for sound Analysis\/Transformation\/Synthesis based on a deterministic plus stochastic decomposition. Ph.D. thesis.","SERRA, X. 1997. Musical sound modeling with sinusoids plus noise. 497-510.","SERRA, X. AND SMITH III, J. 1990. Spectral modeling synthesis: A sound analysis\/synthesis system based on a deterministic plus stochastic decomposition. 14, 4, 12-24.","SHABANA, A. 1997. . Springer Verlag.","STEIGLITZ, K. AND McBRIDE, L. 1965. A technique for the identification of linear systems. 10, 4, 461-464.","TREBIEN, F. AND OLIVEIRA, M. 2009. Realistic real-time sound resynthesis and processing for interactive virtual worlds. 25, 469-477.","VALIMAKI, V., HUOPANIEMI, J., KARIALAINEN, M., AND JANOSY, Z. 1996. Physical modeling of plucked string instruments with application to real-time sound synthesis. 44, 5, 331-353.","VALIMAKI, V. AND TOLONEN, T. 1997. Development and calibration of a guitar synthesizer. -. VAN DEN DOEL, K., KNOTT, D., AND PAI, D. K. 2004. Interactive simulation of complex audiovisual scenes. 13, 99-111.","VAN DEN DOEL, K., KRY, P., AND PAT, D. 2001. FoleyAutomatic: physically-based sound effects for interactive simulation and animation. In 28. ACM New York, N.Y., USA, 537-544.","VAN DEN DOEL, K. AND PAT, D. K. 1998. The sounds of physical shapes. 7, 382-395.","VAN DEN DOEL, K. AND PAT, D. K. 2002. Measurements of perceptual quality of contact sound models. In (2002. 345-349.","WANG, S., SEKEY, A., AND GERSHO, A. 1992. An objective measure for predicting subjective quality of speech coders. 10, 5 (June), 819-829.","WILDES, R. AND RICHARDS, W. 1988. Recovering material properties from sound. 356-363.","ZHENG, C. AND JAMES, D. L. 2009. Harmonic fluids. 09: 2009 . ACM, New York, N.Y., USA, 1-12. ZHENG, C. AND JAMES, D. L. 2010. Rigid-body fracture sound with precomputed soundbanks. 29, 69:1-69:13.","ZHENG, C. AND JAMES, D. L. 2011. Toward high-quality modal contact sound. (2011) 30, 4 (August).","ZWICKER, E. AND FASTL, H. 1999. Psychoacoustics: Facts and models, 2nd updated edition ed. Vol. 254. Springer New York,"]}},"The concept of internal friction describes the relationship between frequency and damping for an object and is regarded as a shape-invariant property [Klatzky et al. 2000; Krotkov et al. 1996; Wildes and Richards 1988]. People have used empirical data to model this frequency-damping relationship. Some work fits a constant angle of friction model to the measured data, while some fits a quadratic curve. Rayleigh damping is among one of the known empirical models that model the frequency-damping relationship with a special class of curve and can be considered a specific type of internal friction.","In order to reexamine the applicability of the Rayleigh damping empirical model in its transferability across different geometry and shapes, we performed some experiments to verify if the Rayleigh damping parameters can be considered shape-invariant. We recorded impact audio for real objects of various shapes for four materials. Then the resonance modes of those objects were extracted with our feature extraction algorithm. If the modes of different objects for the same material all fall on one Rayleigh damping curve, we believe it is reasonable to assume that the Rayleigh damping parameters can indeed be transferred across different shapes. The results of our experiments are shown in . The real objects used in this experiment are pictured in the left column, i.e. (a) glass bowls, (b) china set, (c) graphite tiles, and (d) wooden blocks. In the right column, i.e. feature plots, the objects' resonance modes are visualized as lines topped with geometric shapes in the frequency-damping space, and the height of each line indicates the modes' relative energy. We show the most prominent modes, i.e. modes with highest energy levels. Different objects' resonance modes are color coded differently. For example in (a), the largest glass bowl is coded \u2018GREEN\u2019, and its resonances modes are shown as the green lines in that material's feature plot. However, because color drawings are not permitted in PCT patent applications, each color is mapped to a geometric shape. The left hand column illustrates the mappings between colors and shapes: green corresponds to a square, blue corresponds to a circle, red corresponds to a triangle, and magenta corresponds to a star. The black curve shown on the frequency-damping plane in each feature plot is determined by one set of Rayleigh damping parameters. For various shaped objects with the same material, their resonance modes appear to lie closely to the same black curve. This indicates the same Rayleigh damping parameters can be used to describe the frequency-damping relationship for all those different geometries with the same material.","Furthermore, our user study results provide validation on the assumptions we made about Raleigh-damping model, offering insights consistent with our core hypothesis. We played synthesized sounds with our estimated material parameters applied to various shapes (plate, bunny, stick, and bowl), and the material detection rate for each individual shape does not vary much from one another. For example, the material detection rate for the metallic bunny is almost the same as that for the metallic plate. Our perceptual studies indicate that it is feasible to transfer Rayleigh damping parameters across different shapes and geometry for sound synthesis applications.","B.1 Comparison with SMS","The Spectral Modeling Synthesis (SMS) method [Serra and Smith III 1990] detects a peak also in the power spectrogram, tracks the one peak point over time, and forms an amplitude envelope. One can certainly use this amplitude envelope to infer the damping value, for example, by linear regression of the logarithmic amplitude values (which is the approach adopted by V\u00e4lim\u00e4ki et al. [1996]). There are, however, several disadvantages of this approach.","First of all, tracking only the peak point over time implies that the frequency estimation is only accurate to the width of the frequency bins of power spectrogram. For example, for a window size of 512 samples, the width of a frequency bin is about 86 Hz, direct frequency peak tracking has frequency resolution as coarse as 86 Hz.","Serra and Smith pointed out this problem [Serra and Smith III 1990], and proposes to improve the accuracy by taking the two neighboring frequency bins around the peak and performing a 3-point curve fitting to find the real peak [Serra 1989]. Our method takes a further step: instead of 3 points per time frame, we use all points within a rectangular region. The region extends as far as possible in both frequency and time axes until (a) the amplitude falls under a threshold to the peak amplitude, or (b) a local minimum in amplitude is reached. We then use an optimizer to find a damped sinusoid whose power spectrogram best matches the shape of the input data in the region of interest. An example is shown in , where the solid surface is the power spectrogram of the input sound clip, and the overlay mesh is the power spectrogram of the best fitted damped sinusoid.","Secondly, for linear regression to work well, there must be at least two points (the more the better) along the time axis, before the signal falls to the level of background noise. For high damping values, there will be only a few data points along the time axis. On the other hand, we know that the damping value is also reflected in the width of the hill, so when there are not enough points along the time axis, there are more points along the frequency axis with significant heights-which will help determining the damping value in our surface fitting method.","Taking more points into account makes it less sensitive to noise. In , we simulated a noisy case where white noise with signal-to-noise ratio (SNR)=8 dB is added to a damped sinusoid with damping value 240, and use (a) our local surface fitting method and (b) SMS with linear regression to infer the damping value. In this particular example, due to the high damping value and high noise level, only 4 points participate in linear regression, while 24 points are considered in our method. Our shape fitting is less sensitive to irregularities than the fitted line in SMS. The average damping error versus damping value for both methods are plotted in and , where SNR=20 dB and 8 dB respectively.","Mathematically, the 2D power spectrogram contains as much information as the original time domain signal (except for the windowing effect and the loss of phase). Using only a 1D sequence inevitably discards a portion of all available information (as in SMS), and in some cases (e.g. high damping values and high noise level) this portion is significant. Our surface matching method utilizes as much information as possible. Fitting a surface is indeed more costly than fitting a line, but it also achieves higher accuracy.","B.2 Comparison with a Phase Unwrapping Method","The \u2018phase unwrapping\u2019 technique proposed by Pai et al. [2001] and Corbett et al. [2007] is known for its ability to separate close modes within one frequency bin. Our method, however, works under a different assumption, and the ability to separate modes within a frequency bin has different impacts in our framework and theirs. In their framework, the extracted features {\u0192,d,a} are directly used in the sound synthesis stage and thus control the final audio quality. In our case, the features are only used to guide the subsequent parameter estimation process. In this process, two close modes will show up as near-duplicate points in the (f,d)-space. Because as pointed out by Pai et al. [2001], modes with close frequencies usually result from the shape symmetry of the sounding object, and their damping values should also be close. In the process of fitting material parameters, or more specifically, in computing the feature domain metric, replacing these near-duplicate points with one point does not affect the quality of the result much.","Secondly, despite of its ability to separate nearby modes, Corbett et al. [2007] also proposes to merge modes if their difference in frequency is not greater than human's audible frequency discrimination limit (2-4 Hz). Among the multiple levels of power spectrograms that we used, the finest frequency resolution (about 3 Hz) is in fact around this limit.","On the other hand, our proposed feature extraction algorithm offers some advantages and achieves higher accuracy compared with Pai et al. [2001] and Corbett et al. [2007] in some cases. When extracting the information of a mode, other modes within the same frequency bin (which are successfully resolved by the Steiglitz-McBride algorithm [Steiglitz and McBride 1965] underlying Pai et al. [2001] and Corbett et al. [2007]) are not the only source of interference. Other modes from several bins away also affect the values (complex or magnitude-only alike) in the current bin, known as the \u2018spillover effect\u2019. In order to minimize this effect, the greedy method proposed herein collects the modes with the largest average power spectral density first. Therefore, when examining a mode, the neighboring modes that have higher energy than the current one are already collected, and their influence removed. This can be demonstrated in . The original power spectrogram of a mode (\u0192,d,a) is shown in . The values at the frequency bin Fcontaining \u0192are plotted over time, shown as the curve connecting the circles in . In , the presence of another strong mode (\u0192,d,a) located 5 bins away changes the values at F, plotted as the curve connecting the x's in . The complex values of the STFT at Fare not shown, but they are similarly interfered. If these complex values at Fare directly fitted with the Steiglitz-McBride algorithm in the works by Pai et al. [2001] and Corbett et al. [2007], the estimated damping has a 20% error. The greedy approach in our multi-level algorithm removes the influence of the neighboring, mode first, resulting in a 1% damping error.","Based on our experimentations, we also found that the universal frequency-time resolution used in Pai et al. [2001] and Corbett et al. [2007] is not always most suitable for all modes. Our method uses a dynamic selection of frequency-time resolution to address this problem. For example, in the case of high damping values, under a fixed frequency-time resolution, there may only be a few points above noise level along the time axis, which will undermine the accuracy of the Steiglitz-McBride algorithm.  shows such an example, the damping value (150 s) is high but not unreasonable, as shown in the time domain signal , where a white noise with SNR=60 dB is added. The power spectrogram is shown in . We implemented the method in the paper by Corbett et al. [2007] using the suggested 46 ms window size (with N=4) and tested on the above case. The input to this method is the complex values at the peak frequency bin, whose magnitudes of the real and imaginary parts are shown in , and an error of 5.7% for damping is obtained. As a comparison, our algorithm automatically selects a 23 ms window size and fits the local shape in a 6\u00d75 region in the frequency-time space, yielding merely a 0.9% error for damping.","We provide here the actual values and forms used in our implementation for the constants and functions introduced in Sec. 5.2,","For the relationship between critical-band rate z (in Bark) and frequency (in Hz), we use\n\n(\u0192)=6 sin (\u0192\/600)\u2003\u2003(30)\n\nthat approximates the empirically determined curve shown in  [Wang et al. 1992].\n\nWe use c=5.0 and c=100.0 in Eqn. 21 and Eqn. 22.\n\nIn Eqn. 23, the weight wassociated to a reference feature point \u03c6is designed to be related to the energy of mode i. The energy can be found by integrating the power spectrogram of the damped sinusoid, and we made a modification such that the power spectrogram is transformed prior to integration. The image domain transformation introduced in Sec. 5.2.1, which better reflects the perceptual importance of a feature, is used.\n\nThe weight \u0169used in Eqn. 24 is \u0169=0 for k(\u03c6,{tilde over (\u03c6)})=0, and \u0169=1 for k(\u03c6,{tilde over (\u03c6)})>0 (uis defined similarly).\n\nFor the point-to-point match score k(\u03c6,{tilde over (\u03c6)}) in Eqn. 24, we use\n",{"@attributes":{"id":"p-0197","num":"0247"},"maths":{"@attributes":{"id":"MATH-US-00017","num":"00017"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"mi":"k","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["\u03d5","i"]},{"mover":{"mi":"\u03d5","mo":"~"},"mi":"j"}],"mo":","}}},{"mi":"k","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"D"}}],"mo":["=","\u2062"],"mi":{}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"mo":"{","mtable":{"mtr":[{"mtd":[{"mrow":{"mn":"1.0","mo":"-","mrow":{"mn":"0.5","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"D"}}},{"mrow":{"mrow":{"mi":["if","D"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},"mo":"\u2264","mn":"1.0"}}]},{"mtd":[{"mrow":{"mn":"0.5","mo":"\/","mi":"D"}},{"mrow":{"mrow":{"mi":"if","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"1.0"},"mo":["<","\u2264"],"mi":"D","mn":"5.0"}}]},{"mtd":[{"mn":"0"},{"mrow":{"mrow":{"mi":"if","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"5.0"},"mo":"<","mi":"D"}}]}]}}}}}]}},{"mrow":{"mo":["(",")"],"mn":"31"}}]}}}},"br":{},"sub":["i","j"]},"The subject matter described herein for sound synthesis using estimated material parameters can be used for a variety of applications, including video games and virtual reality applications. The example guided sound synthesis techniques described herein may also be used to estimate intrinsic parameters for liquid sound simulation or voice recognition and retargeting.","It will be understood that various details of the subject matter described herein may be changed without departing from the scope of the subject matter described herein. Furthermore, the foregoing description is for the purpose of illustration only, and not for the purpose of limitation."],"GOVINT":[{},{}],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The subject matter described herein will now be explained with reference to the accompanying drawings of which:",{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":["FIG. 2()","FIG. 2()"]},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 2()"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":["FIG. 3","FIG. 3"]},{"@attributes":{"id":"p-0013","num":"0012"},"figref":["FIG. 4","FIG. 4"],"sub":["N ","p "]},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":["FIG. 6","FIG. 6"],"b":["1","2","3"]},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 8","sub":["t","1","1","2","2","2"]},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 18"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 19"}]},"DETDESC":[{},{}]}
