---
title: Decision enhancement system for a vehicle safety restraint application
abstract: The disclosure describes systems and methods that pertain to interactions between a vehicle and an occupant within the vehicle. More specifically, various systems and methods for enhancing the decisions of automated vehicle applications (collectively “decision enhancement system”) are disclosed. In a safety restraint embodiment, a sensor is used to capture various sensor readings. Sensor readings are typically in the form of images. Occupant information, such as location attributes, motion attributes, and occupant category attributes can be obtained from the sensor readings. Such information can then used by the system to enhance the decisions made by various automated applications.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=06944527&OS=06944527&RS=06944527
owner: Eaton Corporation
number: 06944527
owner_city: Cleveland
owner_country: US
publication_date: 20031107
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","SUMMARY OF INVENTION","DETAILED DESCRIPTION"],"p":["The invention relates generally to systems and methods that pertain to interactions between a vehicle and an occupant within the vehicle. More specifically, the invention is a system or method for enhancing the decisions (collectively \u201cdecision enhancement system\u201d) made by automated vehicle applications, such as safety restraint applications.","Automobiles and other vehicles are increasingly utilizing a variety of automated technologies that involve a wide variety of different vehicle functions and provide vehicle occupants with a diverse range of benefits. Some of those functions are more central to the function of the vehicle, as a vehicle, than other functions. For example, certain applications may assist vehicle drivers to \u201cparallel-park\u201d the vehicle. Other automated applications focus on occupant safety. Safety restraint applications are one category of occupant safety applications. Airbag deployment mechanisms are a common example of a safety restraint application in a vehicle. Automated vehicle applications can also include more discretionary functions such as navigation assistance, and environmental controls, and even purely recreational options such as DVD players, Internet access, and satellite radio. Automated devices are an integral and useful part of modern vehicles. However, the automated devices embedded into vehicles need to do a better job of taking into account the context of the particular vehicle, and the person(s) or occupant(s) involved in using the particular vehicular user. In particular, such devices typically fail to fully address the interactions between the occupants within the vehicle and the internal environment of the vehicle. It would be desirable for automated applications within vehicles to apply more occupant-centric and context-based \u201cintelligence\u201d to enhance the functionality of automated applications within the vehicle.","One example of such omissions is in the field of safety restraint applications, such as airbag deployment mechanisms. Airbags provide a significant safety benefit for vehicle occupants in many different contexts. However, the deployment decisions made by such airbag deployment mechanisms could be enhanced if additional \u201cintelligence\u201d were applied to the process. For example, in several contexts, the deployment of the airbag is not desirable. The seat corresponding to the deploying airbag might be empty, rendering the deployment of the airbag an unnecessary hassle and expense. With respect to certain types of occupants, such as small children or infants, deployment of the airbag may be undesirable. Deployment of the airbag can also be undesirable if the occupant is too close to the deploying airbag, e.g. within an at-risk-zone. Thus, even with the context of a particular occupant, deployment of the airbag is desirable in some contexts (e.g. when the occupant is not within the at-risk-zone) while not desirable in other contexts (e.g. when the occupant is within the at-risk-zone). Automated vehicle applications such as safety restraint applications can benefit from \u201cenhanced\u201d decision-making that applies various forms of \u201cintelligence.\u201d","With respect to safety restraint applications, such as airbag deployment mechanisms, it is useful for automated applications to obtain information about vehicle occupants. With respect to airbag deployment mechanisms, the existing art typically relies on \u201cweight-based\u201d approaches that utilize devices such as accelerometers which can often be fooled by sudden movements by the occupant. Vehicle crashes and other traumatic events, the type of events for which safety applications are most needed, are precisely the type of context most likely to result in inaccurate conclusions by the automated system. Other existing deployment mechanisms rely on various \u201cbeam-based\u201d approaches to identify the location of an occupant. While \u201cbeam-based\u201d approaches do not suffer from all of the weaknesses of \u201cweight-based\u201d approaches, \u201cbeam-based\u201d approaches fail to distinguish between the outer extremities of the occupant, such as a flailing hand or stretched out leg, and the upper torso of the occupant. Moreover, \u201cbeam-based\u201d approaches are not able to distinguish between different types of occupants (e.g. categorize different types of occupants), such as adults versus infants in baby chairs. It may be desirable for vehicle safety applications (and other applications that would benefit from obtaining occupant information) to obtain both location information (including by derivation, velocity and acceleration) about the occupant as well as information relating to the characteristics of the occupant that are independent of location and motion, such as the \u201ctype\u201d of occupant, the estimated mass of the occupant, etc. It may be desirable for decision enhancement systems in vehicles to utilize an image of the occupant in obtaining contextual information about the occupant and the environment surrounding the occupant. Although the existing art does not teach or even suggest an \u201cimage-based\u201d approach to safety restraint applications, an \u201cimage-based\u201d approach can provide both location information as well as occupant categorization information.","Image processing can provide increasingly useful possibilities for enhancing the decision-making functionality or \u201cintelligence\u201d of automated applications in vehicles and other applications. The cost of image-based sensors including digitally based image-based sensors continues to drop. At the same time, their capabilities continue to increase. Unfortunately, the process of automatically interpreting images and otherwise harvesting images for information has not kept pace with developments in the sensor technology. Unlike the human mind, which is particularly adept at making accurate conclusions about a particular image, automated applications typically have a much harder time to correctly utilize the context of an image in accurately interpreting the characteristics of the image. For example, even a small child will understand that person pulling a sweater over their head is still a person. The fact that a face and head are temporarily not visible will not cause a human being to misinterpret the image. In contrast, an automated device looking for a face or head will likely conclude in that same context that no person is present. It would be desirable for decision enhancement systems to apply meaningful contextual information to the interpretation of images and other forms of sensor readings. One way to apply a meaningful context for image processing is to integrate past images and potentially other sensor readings into the process that evaluates the current or most recent sensor readings. Past determinations, including past determinations associated with probability values or some other form of confidence values can also be integrated into the decision making process. The use of Kalman filters can provide one potential means by which the past can be utilized to evaluate the present.","Another obstacle to effective information gathering from images and other forms of sensor readings is the challenge of segmenting the focus of the inquiry (e.g. the \u201csegmented image\u201d of the occupant) from the area in the image that surrounds the occupant (e.g. the \u201cambient image\u201d). Automated applications are not particularly effective at determining whether a particular pixel in an image is that of the occupant, the vehicle interior, or representative of something outside the vehicle that is visible through a window in the vehicle. It can be desirable for a decision enhancement system to apply different segmentation heuristics depending on different lighting conditions and other environmental and contextual attributes. It may also be desirable for a decision enhancement to utilize template or reference images of the vehicle without the occupant so that the system can compare ambient images that include the occupant with ambient images that do not include the occupant.","The varieties of occupant behavior and vehicle conditions can be voluminous, and each situation is in many respects unique. Such a divergent universe of situations and contexts can overwhelm vehicle applications and other automated devices. It would be a desirable approach to define various conditions, modes, or states that relate to information that is relevant to the particular context. For example, with respect to safety restraint applications, the occupant within the vehicle can be considered to be in a state of being at rest, in a state of normal human movement, or in a state of experiencing pre-crash breaking. It can be desirable for the decision enhancement system to associate a probability with each of the predefined conditions in making decisions or applying intelligence to a particular situation. For example, in the context of a deploying airbag, it can be desirable for the decision enhancement system to calculate probabilities that the occupant is in a state of pre-crash breaking, is asleep, or is riding normally in the vehicle.","Various cost-benefit tradeoffs preclude effective decision enhancement systems in vehicles. For example, standard video cameras do not typically capture images quickly enough for existing safety restraint applications to make timely deployment decisions. Conversely, specialized digital cameras can be too expensive to be implemented in various vehicles for such limited purposes. It would be desirable for the heuristics and other processing applied by the decision enhancement system to generate timely \u201cintelligence\u201d from images captured from standard video cameras. This can be accomplished by focusing on certain aspects of the image, as well as by generating future predictions based on past and present data. An approach that attempts to integrate general image processing techniques with context specific vehicle information can succeed where general image processing techniques would otherwise fail.","The solutions to the limitations discussed above and other limitations relating to automated vehicle applications are not adequately addressed in the existing art. Moreover, the existing art does not suggest solutions to the above referenced obstacles to decision enhancements. The \u201cgeneral purpose\u201d nature of image processing tools and the \u201cgeneral purpose\u201d goals of the persons developing those tools affirmatively teach away from the highly context-specific processing needed to effectively enhance the decision-making of automated vehicle safety restraint applications.","The invention relates generally to systems and methods that pertain to interactions between a vehicle and an occupant within the vehicle. More specifically, the invention is a system or method for enhancing the decisions (collectively \u201cdecision enhancement system\u201d) made by automated vehicle applications, such as safety restraint applications.","The decision enhancement system can obtain information from sensor readings such as video camera images that can assist safety restraint applications make better decisions. For example, the decision enhancement system can determine whether or not a vehicle occupant will be too close (e.g. within the at-risk-zone) to a deploying airbag such that it would be better for the airbag not to deploy.","An image capture subsystem can be used to capture images. An image processing subsystem can be used to obtain desirable information from the captured images. For example, information relating to the type of occupant, the location and motion attributes of the occupant, whether or not the occupant is within an at-risk-zone, the kinetic energy of the occupant, or some other relevant occupant-related information can be obtained from the image processing subsystem. In some embodiments, a power subsystem can be used to manage the power requirements of the system, and to provide an emergency source of backup power. A diagnostic subsystem can be used to monitor the status of the system, and to provide fault detection, as well as other system-focused heuristics in some embodiments of the system. Depending on the hardware used to implement the system, a communications subsystem might be utilized to communicate information between the various components.","The invention relates generally to systems and methods that pertain to interactions between a vehicle and an occupant within the vehicle. More specifically, the invention is a system or method for enhancing the decisions (collectively \u201cdecision enhancement system\u201d) made by automated vehicle applications, such as safety restraint applications. Automated vehicle systems can utilize information to make better decisions, benefiting vehicle occupants and their vehicles.","I. Introduction of Elements","A. Environmental View for a Decision Enhancement System",{"@attributes":{"id":"p-0058","num":"0057"},"figref":"FIG. 1","b":["100","102","100","102","102","100","100","104","106"]},"If an occupant  is present, the occupant  sits on a seat . In a preferred embodiment, a decision enhancement device (\u201cenhancement device\u201d or simply the \u201cdevice\u201d)  is located within the roof liner  of the vehicle , above the occupant  and in a position closer to a front windshield  than the occupant . The location of the decision enhancement device  can vary widely from embodiment to embodiment of the system . In many embodiments, there will be two or more enhancement devices . Examples of different decision enhancement device  components can include but is not limited to, a power supply component, an analysis component, a communications component, a sensor component, an illumination component, and a diagnosis component. These various components are described in greater detail below. In a safety restraint embodiment, the enhancement device  will typically include some type of image-based sensor component, and that component should be located in such a way as to capture useful occupant images.","The sensor component(s) of the decision enhancement device  in a safety restraint embodiment should preferably be placed in a slightly downward angle towards the occupant  in order to capture changes in the angle and position of the occupant's upper torso resulting from a forward or backward movement in the seat . There are many other potential locations for a sensor component that are well known in the art. Similarly, the analysis component(s) of the decision enhancement devices  could be located virtually anywhere in the vehicle . In a preferred embodiment, the analysis component(s) is located near the sensor component(s) to avoid sending sensor readings such as camera images through long wires.","A safety restraint controller  such as an airbag controller is shown in an instrument panel , although the safety restraint controller  could be located virtually anywhere in the vehicle . An airbag deployment mechanism  is shown in the instrument panel  in front of the occupant  and the seat , although the system  can function with the airbag deployment mechanism  in alternative locations.","B. High-level Component View",{"@attributes":{"id":"p-0063","num":"0062"},"figref":"FIG. 2","b":"112"},"1. Power Supply Component","A power supply component (\u201cpower component\u201d)  can be used to provide power to the system . In some embodiments, the system  can rely on the power supply of the vehicle . However, in safety-related embodiments, such as a safety restraint application, it is preferable for the system  and the underlying application to have the ability to draw power from an independent power source in a situation where the power source for the vehicle  is impaired.","2. Analysis Component","An analysis component  can be made up of one or more computers that perform the various heuristics used by the system . The computers can be any device or combination of devices capable of performing the application logic utilized by the decision enhancement system .","3. Communication Component","A communication component  can be responsible for all interactions between the various components, as well as interactions between the system  and the applications within the vehicle  that interface with the system  in order to receive the decision enhancement functionality of the system .","In a safety restraint embodiment, it is the communication component  that is typically responsible for communicating with the safety restraint controller  and the safety restraint deployment mechanism .","4. Sensor Component","A sensor component  is the mechanism through which information is obtained by the system . The sensor component  includes one or more sensors, and potentially various sub-components that assist in the functionality performed by the sensor component , such as computer devices to assist in certain image processing functions.","In a preferred safety restraint embodiment, the sensor component  includes a video camera configured to capture images that include the occupant  and the area in the vehicle  that surrounds the occupant . In some embodiments of the system , the video camera used by the system  can be a high-speed camera that captures between roughly 250 and 1000 images each second. Such a sensor can be particularly desirable if the system  is being relied upon to identify affirmative deployment situations, instead of merely modifying, impeding, or disabling situations where some other sensor (such as an accelerometer or some type of beam-based sensor) is the initial arbiter of whether deployment of the safety restraint is necessary.","However, the heuristics applied by the system  can negate the need for specialized sensors. The heuristics applied by the system  can predict future occupant  attributes by detecting trends from recent sensor readings and applying multiple-model probability-weighted processing. Moreover, the heuristics applied by the system  can in certain embodiments, focus on relatively small areas within the captured sensor readings, mitigating the need for high speed cameras. A standard off-the-shelf video camera typically captures images at a rate of 40 images per second.","In some embodiments of the system , the sensor operates at different speeds depending on the current status of the occupant . For example, in an At-Risk-Zone detection\/intrusion embodiment (an ARZ embodiment) of the system , the purpose of the decision enhancement system  is to determine whether or not the occupant  is too close (or will be too close by the time of deployment) to the deploying safety restraint  such that the deployment should be precluded. In an ARZ embodiment, a lower speed mode (between 6 and 12 frames a second, and preferably 8 frames per second) can be used before a crash or pre-crash determination is made that would other result in deployment of the safety restraint . A higher speed mode (between 25 and 45 frames per second) can then be used to determine if an ARZ intrusion should then preclude, disable, impede, or modify what would otherwise be a deployment decision by the safety restraint application.","5. Illumination Component","An illumination component  can be incorporated into the system  to aid in the functioning of the sensor component . In a preferred safety restraint application embodiment, the illumination component  is an infrared illuminator that operates at a wavelength between 800 nm and 960 nm. The wavelength of 880 nm may be particularly well suited for the goals of spectral sensitivity, minimizing occupant  distractions, and incorporating commercially available LED (light emitting diode) technologies.","Some embodiments that involve image based-sensors need not include the illumination component . Certain embodiments involving non-image-based sensors can include \u201cillumination\u201d components  that assist the sensor even though the \u201cillumination\u201d component  has nothing to do with visual light.","6. Diagnosis Component","A diagnosis component  can be incorporated into the system  to monitor the functioning of the system . This can be particularly desirable in embodiments of the system  that relate to safety. The diagnosis component  can be used to generate various status metrics, diagnostic metrics, fault detection indicators, and other internal control processing.","7. Combinations of Components","The various components in  can be combined into a wide variety of different components and component configurations used to make up the decision enhancement device. For example, an analysis component and a sensor component could be combined into a single \u201cbox\u201d or unit for the purposes of certain image processing functionality. A single embodiment of the system  could have multiple sensor components , but no diagnosis component.","The minimum requirements for the system  include at least one analysis component . In a minimalist embodiment, the system  can be configured to utilize sensor readings from a sensor that already exists within the vehicle , allowing the decision enhancement device  to \u201cpiggy-back\u201d off that sensor. In other embodiments, the system  will typically include at least one sensor component .","C. High-level Process Flow View",{"@attributes":{"id":"p-0078","num":"0077"},"figref":"FIG. 3","b":"100"},"An incoming image (\u201cambient image\u201d)  of a seat area  that includes both the occupant , or at least certain portions of the occupant , and some portions of the seat area  that surround the occupant .","The incoming ambient image  is captured by a sensor  such as a video or any other sensor capable of rapidly capturing a series of images. In some embodiments, the sensor  is part of the sensor component  that is part of the decision enhancement device . In other embodiments, the system  \u201cpiggy-backs\u201d off of a sensor  for some other automated application.","In the Figure, the seat area  includes the entire occupant . Under some circumstances and embodiments, only a portion of the occupant  image will be captured within the ambient image , particularly if the sensor  is positioned in a location where the lower extremities may not be viewable. The ambient image  is then sent to some type of computer device within the decision enhancement device , such as the analysis component  discussed above.","The internal processing of the decision enhancement device  is discussed in greater detail below. In a safety restraint embodiment of the system , two important categories of outputs are deployment information  and disablement information . Deployment information  and disablement information  relate to two different questions in the context of a safety restraint embodiment of the system . Deployment information  seeks to answer the question at to whether or not an event occurred such that the deployment of a safety restraint might be desirable. For example, disablement information  may address the questions as to whether or not a crash has occurred. In contrast, disablement information  assists the system  in determining whether or not in the context of a situation where deployment may be desirable (e.g. a crash is deemed to have occurred), should deployment of the safety restraint be disabled, precluded, impeded or otherwise constrained. For example, if the occupant  is too close to the deploying device, or of a particular occupant type classification, it might be desirable for the safety restraint not to deploy.","Deployment information  can include a crash determination, and attributes related to a crash determination such as a confidence value associated with a particular determination, and the basis for a particular crash determination. Disablement information  can include a disablement determination, and attributes related to a disablement determination such as a confidence value associate with a particular determination, and the basis for a particular disablement determination (e.g. such a determination could be based on an occupant type classification, an at-risk-zone determination, an impact assessment metric, or some other attribute). In a preferred embodiment, a deployment determination  (e.g. a decision to either activate or not activate the safety restraint mechanism  is made using both deployment information  and disablement information .","In some embodiments, the decision enhancement device  can be configured to provide such information to the safety restraint controller  so that the safety restrain controller  can make an \u201cinformed\u201d deployment determination  relating to the deployment mechanism  for the safety restraint application. In other embodiments, the decision enhancement device  can be empowered with full-decision making authority. In such an embodiment, the decision enhancement device  generates the deployment determination  that is implemented by the deployment mechanism .","The deployment determination  can include the timing of the deployment, the strength of the deployment (for example, an airbag could be deployed at half-strength), or any other potential course of action relating to the deployment of the safety restraint.","Deployment information  can include any information, and especially any occupant  attribute, that is useful in making an affirmative determination as to whether a crash has or is about to occur (for example, the occupant  could be in a state of pre-crash braking, as discussed below) such that the safety restraint mechanism  should be deployed so long as none of the disablement information  \u201cvetoes\u201d such a deployment determination .","Disablement information  can include any information that is useful in making determinations that the deployment of the safety restraint should be impeded, modified, precluded, or disabled on the basis of some \u201cveto\u201d factor. Examples of disablement conditions include an occupant  within a predefined At-Risk-Zone or an occupant  impact with the deploying restraint that is estimated to be to severe to for a desirable deployment.","Deployment information , disablement information , and deployment determinations  are discussed in greater detail below.","D. Processing-level Hierarchy",{"@attributes":{"id":"p-0090","num":"0089"},"figref":"FIG. 4","b":["100","100","100","100"]},"As disclosed in , the processing of the system  can include patch-level processing , region-level processing , image-level processing , and application-level processing . The fundamental building block of an image-based embodiment is a pixel. Images are made up of various pixels, with each pixel possession various values that reflect the corresponding portion of the image.","Patch-level processing , region-level processing , and image-level processing can involve performing operations on individual pixels. However an image-level process  performs functionality on the image as a whole, and a region-level process  performs operations on a region as a whole. Patch-level processing  can involve the modification of a single pixel value.","There is typically a relationship between the level of processing and the sequence in which processing is performed. Different embodiments of the system  can incorporate different sequences of processing, and different relationships between process level and processing sequence. In a typical embodiment, image-level processing  and application-level processing  will typically be performed at the end of the processing of the particular ambient image .","In the example in , processing is performed starting at the left side of the diagram, moving continuously to the right side of the diagram as the particular ambient image  is processed by the system . Thus, in the illustration, the system  begins with image-level processing  relating to the capture of the ambient image .","1. Initial Image-level Processing","The initial processing of the system  relates to process steps performed immediately after the capture of the ambient image . In many embodiments, initial image-level processing includes the comparing of the ambient image  to one or template images. This can be done to isolate the segmented image  of the occupant  (an image that does not include the area surrounding the occupant ) from the ambient image  (at image that does include the area adjacent to the occupant ). The segmentation process is described below.","2. Patch-level Processing.","Patch-level processing  includes processing that is performed on the basis of small neighborhoods of pixels referred to as patches . Patch-level processing  includes the performance of a potentially wide variety of patch analysis heuristics . A wide variety of different patch analysis heuristics  can be incorporated into the system  to organize and categorize the various pixels in the ambient image  into various regions  for region-level processing . Different embodiments may use different pixel characteristics or combinations of pixel characteristics to perform patch-level processing .","One example of a patch-level process is the ARZ heuristic described in greater detail below. The ARZ heuristic provides for dividing a detection window into a variety of patches.","3. Region-level Processing","A wide variety of different region analysis heuristics  can be used to determine which regions  belong to a particular region of interest, such as the ARZ detection window described below. Region-level processing  is especially important to the segmentation process described below. Region analysis heuristics  can be used to make the segmented image  available to image-level processing  performed by the system .","D. Subsequent Image-level Processing","The segmented image  can then be processed by a wide variety of potential image analysis heuristics  to identify a variety of image classifications  and image characteristics  that are used for application-level processing . The nature of the automated application should have an impact on the type of image characteristics  passed to the application.","1. Image Characteristics","The segmented image  (or some type of representation of the segmented image  such as a ellipse) is useful to the system  because certain image characteristics  can be obtained from the segmented image . Image characteristics  can include a wide variety of attribute types , such as color, height, width, luminosity, area, etc. and attribute values  that represent the particular trait of the segmented image  with respect to the particular attribute type . Examples of attribute values  corresponding to the attribute types of  of color, height, width, and luminosity can be blue, 20 pixels, 0.3 inches, and 80 Watts. Image characteristics  can include any attribute relating to the segmented image  or a representation of the segmented image , such as the ellipses discussed below. Image characteristics  also include derived image characteristics  that can include any attribute value  computed from two or more attribute values . For example, the area of the occupant  can be computed by multiplying height times width. Some derived imaged characteristics  can be based on mathematical and scientific relationships known in the art. Other derived image characteristics  may be utilize relationships that are useful to the system  that have no particular significance in the known arts. For example, a ratio of width to height to pixels could prove useful to an automated application of the system  without having a significance known in the mathematical or scientific arts.","Image characteristics  can also include statistical data relating to an image or a even a sequence of images. For example, the image characteristic  of image constancy can be used to assist in the process of whether a particular portion of the ambient image  should be included as part of the segmented image .","In a vehicle safety restraint embodiment of the system , the segmented image  of the vehicle occupant can include characteristics such as relative location with respect to an at-risk-zone within the vehicle, the location and shape of the upper torso, and\/or a classification as to the type of occupant.","In addition to being derived from the segmented image , expectations with respect to image characteristics  can be used to help determine the proper scope of the segmented image  within the ambient image . This \u201cboot strapping\u201d approach can be a useful way of applying some application-related context to the segmentation process implemented by the system .","2. Image Classification","In addition to various image characteristics , the segmented image  can also be categorized as belonging to one or more image classifications . For example, in a vehicle safety restraint embodiment, the segmented image  could be classified as an adult, a child, a rear facing child seat, etc. in order to determine whether an airbag should be precluded from deployment on the basis of the type of occupant. In addition to being derived from the segmented image , expectations with respect to image classification  can be used to help determine the proper boundaries of the segmented image  within the ambient image . This \u201cboot strapping\u201d process is a way of applying some application-related context to the segmentation process implemented by the system . Image classifications  can be generated in a probability-weighted fashion. The process of selectively combining image regions into the segmented image  can make distinctions based on those probability values.","E. Application-level Processing","In an embodiment of the system  invoked by a vehicle safety restraint application, image characteristics  and image classifications  can be used to preclude airbag deployments when it would not be desirable for those deployments to occur, invoke deployment of an airbag when it would be desirable for the deployment to occur, and to modify the deployment of the airbag when it would be desirable for the airbag to deploy, but in a modified fashion.","There are many different application-level processes that can be enhanced by the system . In a automated safety restraint embodiment, such processing can include a wide variety of affirmative deployment heuristics and disablement heuristics. Deployment and disablement processing is discussed in greater detail below.","In other embodiments of the system , application-level processing  can include any response or omission by an automated application to the image classification  and\/or image characteristics  provided to the application.","II. Capturing Occupant Characteristics for Decision Enhancement",{"@attributes":{"id":"p-0110","num":"0109"},"figref":"FIG. 5","b":"100"},"A. Segmentation","In a preferred embodiment of the system , the first step in capturing occupant characteristics  is identifying the segmented image  within the ambient image . The system  can invoke a wide variety of different segmentation heuristics. Segmentation heuristics can be invoked in combination with other segmentation processes or as stand-alone processes. Segmentation heuristics can be selectively invoked on the basis of the current environmental conditions within the vehicle . For example, a particular segmentation heuristic or sequence of segmentation heuristics can be invoked in relatively bright conditions while a different segmentation heuristic or sequence of segmentation heuristics can be invoked in relatively dark conditions.","Examples of segmentation heuristics are disclosed in the following patent applications:","\u201cIMAGE SEGMENTATION SYSTEM AND METHOD,\u201d Ser. No. 10\/023,787, filed on Dec. 17, 2001; \u201cMOTION-BASED IMAGE SEGMENTOR FOR OCCUPANT TRACKING,\u201d Ser. No. 10\/269,237, filed on Oct. 11, 2002; \u201cMOTION-BASED IMAGE SEGMENTOR FOR OCCUPANT TRACKING USING A HAUSDORF-DISTANCE HEURISTIC,\u201d Ser. No. 10\/269,357, filed on Oct. 11, 2002; \u201cSYSTEM OR METHOD FOR SELECTING CLASSIFIER ATTRIBUTE TYPES,\u201d Ser. No. 10\/375,946, filed on Feb. 28, 2003; \u201cSYSTEM OR METHOD FOR SEGMENTING IMAGES,\u201d Ser. No. 10\/619,035, filed on Jul. 14, 2003; and \u201cSYSTEM OR METHOD FOR IDENTIFYING A REGION-OF-INTEREST IN AN IMAGE,\u201d Ser. No. 10\/663,521, filed on Sep. 16, 2003, the contents of which are hereby incorporated by reference in their entirety.","The segmented image  is an input for a variety of different application-level processes  in automated safety restraint embodiments of the system . As discussed above and illustrated in , the segmented image  can be an input for generating occupant classifications  and for generating image characteristics  (which can also be referred to as \u201coccupant characteristics\u201d).","Different embodiments of the system  may include only a subset of the subsystems illustrated in FIG. .","B. Category Subsystem","A category subsystem  is a mechanism for classifying the segmented image  into one or more pre-defined classifications. The category subsystem  can generate an image-type classification . In some embodiments, the category subsystem  can set an image-type disablement flag  on the basis of the image-type classification . For example, if the occupant  is classified as an empty seat , the image-type disablement flag could be set to a value of \u201cyes\u201d or \u201cdisabled\u201d which would preclude the deployment of the safety restraint. In other embodiments, the system  is not authorized to definitively set any type of disablement flags, and the information included in the image-type classification  is merely passed on to the mechanism that is authorized to make the final deployment determination , such as the safety restraint controller .","The category subsystem  can perform a wide variety of categorization or classification heuristics. Examples of categorization or classification heuristics are disclosed in the following patent applications:","\u201cOCCUPANT LABELING FOR AIRBAG-RELATED APPLICATIONS,\u201d Ser. No. 10\/269,308, filed on Oct. 11, 2002; \u201cSYSTEM OR METHOD FOR SELECTING CLASSIFIER ATTRIBUTE TYPES,\u201d Ser. No. 10\/375,946, filed on Feb. 28, 2003; and \u201cSYSTEM OR METHOD FOR CLASSIFYING IMAGES,\u201d Ser. No. 10\/625,208, filed on Jul. 23, 2003, the contents of which are hereby incorporated by reference in their entirety.","C. Ellipse Fitting","For many non-categorization purposes, it can be useful to use some type of geometric shape to represent the occupant . In particular, motion and location processing can benefit from such representations. Given the purposes and contexts of automated safety restraint applications, the use of one or more ellipses  can be particularly effective.","An ellipse fitting subsystem  can generate one or more ellipses  from the segmented image  provided by the segmentation subsystem . The ellipse fitting subsystem  can perform a wide variety of ellipse fitting heuristics. The patent application titled \u201cOCCUPANT LABELING FOR AIRBAG-RELATED APPLICATIONS\u201d (Ser. No. 10\/269,308) that was filed on Oct. 11, 2002, the contents of which are incorporated herein in its entirety, discloses a number of difference ellipse fitting heuristics.",{"@attributes":{"id":"p-0124","num":"0123"},"figref":"FIG. 6","b":["250","106","252","106","106","106","252","252","100","100","208","100","106","120"]},{"@attributes":{"id":"p-0125","num":"0124"},"figref":"FIG. 7","b":["206","100"]},"A centroid  of the upper ellipse  can be identified by the system  for tracking and predicting location and motion characteristics of the occupant . It is known in the art how to identify the centroid  of an ellipse. Motion characteristics can include an x-coordinate (\u201cdistance\u201d)  of the centroid  (or other point within the representation) and a forward tilt angle (\u201c\u03b8\u201d) . Shape measurements include a y-coordinate (\u201cheight\u201d)  of the centroid  (or other point within the representation), a length of the major axis of the ellipse (\u201cmajor\u201d)  and a length of the minor axis of the ellipse (\u201cminor\u201d) . Alternative embodiments may utilize a wide variety of different occupant characteristics or ellipse attributes. Rate of change information and other mathematical derivations, such as velocity (single derivatives) and acceleration (double derivatives), are preferably captured for all shape and motion measurements, so in the preferred embodiment of the invention there are nine shape characteristics (height, height\u2032, height\u2033, major, major\u2032, major\u2033, minor, minor\u2032, and minor\u2033) and six motion characteristics (distance, distance\u2032, distance\u2033, \u03b8, \u03b8\u2032, and \u03b8\u2033). A sideways tilt angle \u03a6 is not shown because it is perpendicular to the image plane, and this the sideways title angle \u03a6 is derived, not measured, as discussed in greater detail below. Motion and shape characteristics are the types of image characteristics  that can be used to perform many different deployment and disablement heuristics. Alternative embodiments may incorporate a greater or lesser number of motion and shape characteristics.",{"@attributes":{"id":"p-0127","num":"0126"},"figref":"FIG. 8","b":["276","276","174"]},"In a preferred embodiment of the system , there are three shape states, a state of leaning left towards the driver (left) , a sate of sitting relatively upright (center) , and a state of leaning right away from the driver (right) . A three shape state embodiment is typically assigned three pre-defined tilt sideways tilt angles of \u2212\u03a6, 0, and \u03a6. In a preferred embodiment, \u03a6 is set at a value between 15 and 40 degrees, depending on the nature of the vehicle being used. Alternative embodiments may incorporate a different number of shape states, and a different range of sideways tilt angles .","D. Tracking and Predicting","Returning to , the ellipse  (or other geometric representation) and the information contained in the geometric representation are provided to a tracking and predicting subsystem . In many embodiments, the tracking and predicting subsystem  includes a shape tracking and predicting module  (\u201cshape tracker\u201d) for tracking and predicting shape characteristics, and a motion tracking a predicting module  (\u201cmotion tracker\u201d) for tracking and predicting motion characteristics.","In a preferred embodiment, a multiple-model probability-weighted Kalman filter is used to predict future characteristics by integrating current sensor readings with past predictions.","An academic paper entitled \u201cAn Introduction to the Kalman Filter\u201d by Greg Welch and Gary Bishop is attached and incorporated by reference. The general equation for the Kalman filter is shown in Equation 1:\n\n+Gain[\u2212\n\nIn a Kalman filter, \u201cGain\u201d represents the perceived accuracy of the most recent measurement. A Gain of 0 indicates such a poor measurement that it is of no value, and thus the new estimate Xis simply the value of the old estimate X.\n\n\u2003+0]\n\n+0 \n\n\u2003\u2003Equation 2:\n\nA Gain of 1 indicates such confidence in the most recent measurement Xthat the new prediction Xis simply the value of the most recent measurement X.\n\n+1]\n\n]\n\n\u2003\u2003Equation 3:\n","In a real world application, the Gain is virtually always greater than 0 and less than 1. The Gain thus determines to what degree a new measurement can change the previous aggregate estimate or prediction of the location of an object, in the case of the instant invention, the occupant  is the object being tracked. Both the shape tracker  and the motion tracker  are described in greater detail below, along with  respectively.","1. Shape Tracking and Predicting",{"@attributes":{"id":"p-0134","num":"0133"},"figref":"FIG. 9","b":"212"},"Referring also to , in some preferred embodiments of the system , the shape tracker and predictor module  tracks and predicts the major axis of the upper ellipse (\u201cmajor\u201d) , the minor axis of the upper ellipse (\u201cminor\u201d) , and the y-coordinate of the centroid (\u201cheight\u201d) . Returning to , each characteristic has a vector describing position, velocity, and acceleration information for the particular characteristic. The major vector is [major, major\u2032, major\u2033], with major\u2032 representing the rate of change in the major or velocity and major\u2033 representing the rate of change in major velocity or acceleration. Accordingly, the minor vector is [minor, minor\u2032, minor\u2033], and the height vector is [height, height\u2032, height\u2033]. Any other shape vectors will similarly have position, velocity, and acceleration components. The first step in the shape tracking and prediction process is an update of the shape prediction at .","a. Update Shape Prediction","An update shape prediction process is performed at . This process takes the last shape estimate and extrapolates that estimate into a future prediction using a transition matrix.\n\nUpdated Vector Prediction=Transition Matrix*Last Vector Estimate\u2003\u2003Equation 4:\n\nThe transition matrix applies Newtonian mechanics to the last vector estimate, projecting forward a prediction of where the occupant  will be on the basis of its past position, velocity, and acceleration. The last vector estimate is produced at  as described below. The process from  to , from  to , and from  to , loops back to . The process at  requires that an estimate be previously generated at , so processing at  and  is not invoked the first time through the repeating loop that is steps  through .\n","The following equation is then applied for all shape variables and for all shape states, where x is the shape variable, \u0394t represents change over time (velocity), and \u00bd\u0394trepresents acceleration. \n\n","In a preferred embodiment of the system , there are nine updated vector predictions at  because there are three shape states and three non-derived shape variables in the preferred embodiment, and 3\u00d73=9. The updated shape vector predictions are:\n\n","b. Update Covariance and Gain Matrices","After the shape predictions are updated for all variables and all states at , the shape prediction covariance matrices, shape gain matrices, and shape estimate covariance matrices must be updated at . The shape prediction covariance accounts for error in the prediction process. The gain, as described above, represents the weight that the most recent measurement is to receive and accounts for errors in the measurement segmentation process. The shape estimate covariance accounts for error in the estimation process.","The prediction covariance is updated first. The equation to be used to update each shape prediction covariance matrix is as follows:\n\nShape Prediction Covariance Matrix=[State Transition Matrix*Old Estimate Covariance Matrix*transpose(State Transition Matrix)]+System Noise\u2003\u2003Equation 6:\n\nThe state transition matrix is the matrix that embodies Newtonian mechanics used above to update the shape prediction. The old estimate covariance matrix is generated from the previous loop at . On the first loop from  through , step  is skipped. Taking the transpose of a matrix is simply the switching of rows with columns and columns with rows, and is known under the art. Thus, the transpose of the state transition matrix is the state transition matrix with the rows as columns and the columns as rows. System noise is a matrix of constants used to incorporate the idea of noise in the system. The constants used in the system noise matrix are set by the user of the invention, but the practice of selecting noise constants are known in the art.\n","The next matrix to be updated is the gain matrix. As discussed above, the gain represents the confidence of weight that a new measurement should be given. A gain of one indicates the most accurate of measurements, where past estimates may be ignored. A gain of zero indicates the least accurate of measurements, where the most recent measurement is to be ignored and the user of the invention is to rely solely on the past estimate instead. The role played by gain is evidenced in the basic Kalman filter equation of Equation 1.","\u2003+Gain[\u2212]","The gain is not simply one number because one gain exists for each combination of shape variable and shape state. The general equation for updating the gain is Equation 7:\n\nGain=Shape Prediction Covariance Matrix*transpose(Measure Matrix)*inv(Residue Covariance)\n\nThe shape covariance matrix is calculated above. The measure matrix is simply a way of isolating and extracting the position component of a shape vector while ignoring the velocity and acceleration components for the purposes of determining the gain. The transpose of the measure matrix is simply [1 0 0]. The reason for isolating the position component of a shape variable is because velocity and acceleration are actually derived components, only position can be measured by a snapshot. Gain is concerned with the weight that should be attributed to the actual measurement.\n","In the general representation of a Kalman filter, X=X+Gain[\u2212X+X], the residue represents the difference between the old estimate and the new measurement. There are entire matrices of residue covariances. The inverse of the residue covariance matrix is used to update the gain matrix. It is known in the art how to take the inverse of a matrix, which is a simple linear algebra process. The equation for residue covariance matrix is Equation 8:\n\nResidue Covariance=[Measurement Matrix*Prediction Covariance*transpose(Measurement Matrix)]+Measurement Noise\n\nThe measurement matrix is a simple matrix used to isolate the position component of a shape vector from the velocity and acceleration components. The prediction covariance is calculated above. The transpose of the measurement matrix is simply a one row matrix of [1 0 0] instead of a one column matrix with the same values. Measurement noise is a constant used to incorporate error associated with the sensor  and the segmentation heuristics performed by the segmentation subsystem .\n","The last matrix to be updated is the shape estimate covariance matrix, which represents estimation error. As estimations are based on current measurements and past predictions, the estimate error will generally be less substantial than prediction error. The equation for updating the shape estimation covariance matrix is Equation 9:","\u2003Shape Estimate Covariance Matrix=(Identity Matrix\u2212Gain Matrix*Measurement Matrix)*Shape Predictor Covariance Matrix","An identity matrix is known in the art, and consists merely of a diagonal line of 1's going from top left to bottom right, with zeros at every other location. The gain matrix is computed and described above. The measure matrix is also described above, and is used to isolate the position component of a shape vector from the velocity and acceleration components. The predictor covariance matrix is also computed and described above.","c. Update Shape Estimate","An update shape estimate process is invoked at . The first step in this process is to compute the residue.\n\nResidue=Measurement\u2212(Measurement Matrix*Prediction Covariance)\u2003\u2003Equation 10:\n\nThen the shape states themselves are updated.\n\nUpdated Shape Vector Estimate=Shape Vector Prediction+(Gain*Residue)\u2003\u2003Equation 11:\n\nWhen broken down into individual equations, the results are as follows:\n\n+Gain[\u2212(major at t\u2212)]\n\n+Gain[\u2212(major at t\u2212)]\n\n+Gain[\u2212(major at t\u2212)]\n\n+Gain[\u2212(minor at t\u2212)]\n\n+Gain[\u2212(minor at t\u2212)]\n\n+Gain[\u2212(minor at t\u2212)]\n\n+Gain[\u2212(height at t\u2212)]\n\n+Gain[\u2212(height at t\u2212)]\n\n+Gain[\u2212(height at t\u2212)]\n\nIn the preferred embodiment, C represents the state of center, L represents the state of leaning left towards the driver, and R represents the state of leaning right away from the driver. Different embodiments and different automated applications may utilize a wide variety of different shape states or shape conditions.\n","d. Generate Combined Shape Estimate","The last step in the repeating loop between steps  and steps  is a generate combined shape estimate step at . The first part of that process is to assign a probability to each shape vector estimate. The residue covariance is re-calculated, using the same formula as discussed above.\n\nCovariance Residue Matrix=[Measurement Matrix*Prediction Covariance Matrix*transpose(Measurement Matrix)]+Measurement Noise\u2003\u2003Equation 12:\n","Next, the actual likelihood for each shape vector is calculated. The system  determines which state the occupant is in by comparing the predicted values for the various states with the recent best estimate of what the current values for the shape variables actually are. \n\n\n\nThere is no offset in the preferred embodiment of the invention because it is assumed that offsets cancel each other out in the processing performed by the system . Sigma represents variance, and is defined in the implementation phase of the invention by a human developer. It is known in the art how to assign a useful value for sigma by looking at data.\n","The state with the highest likelihood determines the sideways tilt angle \u03a6. If the occupant  is in a centered state, the sideways tilt angle is 0 degrees. If the occupant  is tilting left, then the sideways tilt angle is \u2212\u03a6. If the occupant  is tilting towards the right, the sideways tilt angle is \u03a6. In the preferred embodiment of the invention, \u03a6 and \u2212\u03a6 are predefined on the basis of the type and model of vehicle using the system .","Next, state probabilities are updated from the likelihood generated above and the pre-defined Markovian mode probabilities discussed below.","\u2003\u2003\u2003Equation 14:\n\n\u2003\u2003Equation 15:\n\n\u2003\u2003Equation 16:\n\nThe equations for the updated mode probabilities are as follows, where L represents the likelihood of a particular mode as calculated above:\n\nProbability of mode Left=1*()+*()+*()]**()\u2003\u2003Equation 17:\n\nProbability of mode Right=1*()+*()+*()]**()\u2003\u2003Equation 18:\n\nProbability of mode Center=1\/[*()+*()+*()]**()\u2003\u2003Equation 19:\n","The combined shape estimate is ultimately calculated by using each of the above probabilities, in conjunction with the various shape vector estimates. \n\n\n\nX is any of the shape variables, including a velocity or acceleration derivation of a measure value.\n","The loop from  through  repeats continuously while the vehicle is in operation or while there is an occupant  in the seat . The process at  requires that an estimate be previously generated at , so processing at  and  is not invoked the first time through the repeating loop.","2. Motion Tracking and Predicting",{"@attributes":{"id":"p-0157","num":"0165"},"figref":"FIG. 10","b":["214","214","214","214","214"]},"The motion tracker and predictor  in  functions similarly in many respects, to the shape tracker and predictor  in FIG. . However, the motion tracker and predictor  tracks and predicts different characteristics and vectors than the shape tracker . In the preferred embodiment of the invention, the x-coordinate of the centroid  and the forward tilt angle \u03b8 (\u201c\u03b8\u201d) , and their corresponding velocities and accelerations (collectively \u201cmotion variables\u201d) are tracked and predicted. The x-coordinate of the centroid  is used to determine the distance between the occupant  and a location within the automobile such as the instrument panel , the safety restraint deployment mechanism , or some other location in the vehicle . In a preferred embodiment, the instrument panel  is the reference point since that is where the safety restraint is generally deployed from.","The x-coordinate vector includes a position component (x), a velocity component (x\u2032), and an acceleration component (x\u2033). The \u03b8 vector similarly includes a position component (\u03b8), a velocity component (\u03b8\u2032), and an acceleration component (\u03b8\u2033). Any other motion vectors will similarly have position, velocity, and acceleration components.","a. Update Motion Prediction","An update motion prediction process is performed at . This process takes the last motion estimate and extrapolates that estimate into a future prediction using a transition matrix as disclosed above in Equation 4:\n\nUpdated Vector Prediction=Transition Matrix*Last Vector Estimate\n\nThe transition matrix applies Newtonian mechanics to the last vector estimate, projecting forward a prediction of where the occupant  will be on the basis of its past position, velocity, and acceleration. The last vector estimate is produced at  as described below. The process from  to , from  to , and from  to , loops back to  on a potentially perpetual basis while the vehicle  is in operation. The process at  requires that an estimate be previously generated at , so processing at  and  is not invoked the first time through the repeating loop that is steps -.\n","As disclosed above with respect to shape variables, Equation 5 can then applied for all motion variables and for all motion modes: \n\n","In the preferred embodiment of the invention, there would be six updated vector predictions at  because there are three motion modes and two motion variables in the preferred embodiment, and 3\u00d72=6. The updated motion predictions are:\n\n","After the motion predictions are updated for all motion variables and all modes at , the motion prediction covariance matrices, motion gain matrices, and motion estimate covariance matrices must be updated at . The motion prediction covariance accounts for error in the prediction process. The gain, as described above, represents the weight that the most recent measurement is to receive and accounts for errors in the measurement and segmentation process. The motion estimate covariance accounts for error in the estimation process.","The prediction covariance is updated first. Equation 21 is used to update each motion prediction covariance matrix.\n\nMotion Prediction Covariance Matrix=State Transition Matrix*Old Estimate Covariance Matrix*transpose(State Transition Matrix)+System Noise\u2003\u2003Equation 21:\n\nThe state transition matrix is the matrix that embodies Newtonian mechanics used above to update the motion prediction. The old estimate covariance matrix is generated from the previous loop at . On the first loop from  through , steps  and  are skipped. Taking the transpose of a matrix is simply the switching of rows with columns and columns with rows, and is known under the art. Thus, the transpose of the state transition matrix is the state transition matrix with the rows as columns and the columns as rows. System noise is a matrix of constants used to incorporate the idea of noise in the system. The constants used in the system noise matrix are set by the user of the invention, but the practice of selecting such constants is known in the art.\n","The next matrix to be updated is the gain matrix. As discussed above, the gain represents the confidence of weight that a new measurement should be given. A gain of one indicates the most accurate of measurements, where past estimates may be ignored. A gain of zero indicates the least accurate of measurements, where the most recent measurement is to be ignored and the user of the invention is to rely on the past estimate instead. The role played by gain is evidenced in the basic Kalman filter equation in Equation 1 where\n\n+Gain[\u2212X]\n","The gain is not simply one number but an entire matrix because one gain exists for each combination of motion variable and motion mode. The general equation for updating the gain is Equation 22:\n\nGain=Motion Prediction Covariance Matrix*transpose(Measure Matrix)*inv(Residue Covariance)\n\nThe motion covariance matrix is calculated above. The measure matrix is simply a way of isolating and extracting the position component of a motion vector while ignoring the velocity and acceleration components for the purposes of determining the gain. The transpose of the measure matrix is simply [1 0 0]. The reason for isolating the position component of a motion variable is because velocity and acceleration are actually derived components. Position is the only component actually measured, and because gain is concerned with the weight that should be attributed to the actual measurement, derived variables should be isolated.\n","In the general representation of a Kalman filter, X=X+Gain[\u2212X+X], the residue represents the difference between the old estimate and the new measurement. There are entire matrices of residue covariances. The inverse of the residue covariance matrix is used to update the gain matrix. It is known in the art how to take the inverse of a matrix, which is a simple linear algebra process. The equation for residue covariance matrix is Equation 8 as disclosed above:\n\nResidue Covariance=[Measurement Matrix*Prediction Covariance*transpose(Measurement Matrix)]+Measurement Noise\n\nThe measurement matrix is a simple matrix used to isolate the position component of a motion vector from the velocity and acceleration components. The prediction covariance is calculated above. The transpose of the measurement matrix is simply a one row matrix of [1 0 0] instead of a one column matrix with the same values. Measurement noise is a constant used to incorporate error associated with the sensor  and the segmentation process .\n","The last matrix to be updated is the motion estimate covariance matrix, which represents estimation error. As estimations are based on current measurements and past predictions, the estimate error will generally be less substantial than the prediction error. The equation for updating the motion estimation covariance matrix is Equation 23:\n\nMotion Estimate Covariance Matrix=(Identity Matrix\u2212Gain Matrix*Measurement Matrix)*Motion Predictor Covariance Matrix\n","An identity matrix is known in the art, and consists merely of a diagonal line of 1's going from top left to bottom right, with zeros at every other location. The gain matrix is computed and described above. The measure matrix is also described above, and is used to isolate the position component of a motion vector from the velocity and acceleration components. The predictor covariance matrix is also computed and described above.","C. Update Motion Estimate","An update motion estimate process is invoked at  The first step in this process is to compute the residue using Equation 10 as disclosed above:\n\nResidue=Measurement\u2212(Measurement Matrix*Prediction Covariance)\n\nThen the motion states themselves are updated.\n\nMotion Vector Estimate=Motion Vector Prediction+(Gain*Residue)\u2003\u2003Equation 24:\n\nWhen broken down into individual equations, the results are as follows:\n\n+Gain[\u2212X(x-coordinate at t\u2212)]\n\n+Gain[\u2212X(x-coordinate at t\u2212)]\n\n+Gain[\u2212X(x-coordinate at t\u2212)]\n\n\u2003+Gain[(\u03b8 at t\u22121.)\n\n+Gain[(\u03b8 at t\u22121)\n\n+Gain[(\u03b8 at t\u22121)\n\nIn some preferred disablement embodiments, H represents the mode of human, C represents the mode of crash (or pre-crash braking), and S represents the mode of stationary. In some embodiments of the system , and especially those embodiments potentially responsible for making an affirmative deployment determination  in addition to various disablement determinations, it can be desirable to use a four mode model. In such an embodiment, the mode of crash and pre-crash braking are modes that are distinct from one another. For an example of a four-mode model, please see the application titled \u201cIMAGE PROCESSING SYSTEM FOR DETERMINING WHEN AN AIRBAG SHOULD BE DEPLOYED\u201d (Ser. No. 10\/052,152) that was filed on Jan. 17, 2002, the contents of which are incorporated herein in its entirety.\n","d. Generate Combined Motion Estimate","The last step in the repeating loop between steps  and steps  is a generate combined motion estimate step at . The first part of that process is to assign a probability to each motion vector estimate. The residue covariance is re-calculated, using Equation 25 as discussed above.\n\nCovariance Residue Matrix=[Measurement Matrix*Prediction Covariance Matrix*transpose(Measurement Matrix)]+Measurement Noise\n","Next, the actual likelihood for each motion vector is calculated. \n\n\n\nThere is no offset in a preferred embodiment of the invention because it can be assumed that offsets cancel each other out, and that system  processing can be zero-mean Gaussian signals. Sigma represents variance, is defined in the implementation phase of the invention by a human developer. It is known in the art how to assign a useful value for sigma by looking at data.\n","Next, mode probabilities are updated from the likelihood generated above and the pre-defined Markovian mode probabilities discussed below.\n\n\u2003\u2003Equation 27:\n\n\u2003\u2003Equation 28:\n\n\u2003\u2003Equation 29:\n\nThe equations for the updated mode probabilities are as follows, where L represents the likelihood of a particular mode as calculated above:\n\nProbability of mode Stationary=1*()+*()+*()]**()\u2003\u2003Equation 30:\n\nProbability of mode Human=1*()+*()+*()]**()\u2003\u2003Equation 31:\n\nProbability of mode Crash=1*()+*()+*()]**()\u2003\u2003Equation 32:\n","The combined motion estimate is ultimately calculated by using each of the above probabilities, in conjunction with the various motion vector estimates. \n\n\n\nX is any of the motion variables, including a velocity or acceleration derivation.\n","The loop from  through  repeats continuously while the vehicle  is in operation or while there is an occupant  in the seat .","3. Outputs from the Tracking and Predicting Subsystem","Returning to , the output from the tracking and predicting subsystem  are the occupant characteristics  (which can also be referred to as image characteristics), including attribute types  and their corresponding attribute values , as discussed above. Occupant characteristics  can be used to make crash determinations (e.g. whether an event has occurred that could potentially make deployment of the safety restraint desirable), as well as disablement determinations, such as whether the occupant  is too close to an At-Risk-Zone or whether the kinetic energy (or other impact assessment metric) would be too substantial for the deployment (or at least full strength deployment) of the safety restraint.","E. Crash Determination","A crash determination subsystem  can generate the output of a deployment flag  or a crash flag from the input of the various image characteristics  discussed above. In some embodiments, the impact of a crash flag  sent to crash (or pre-crash braking) is not \u201cbinding\u201d upon the safety restraint controller . In those embodiments, the safety restraint controller  may incorporate a wide variety of different crash determinations, and use those determinations in the aggregate to determine whether a deployment-invoking event has occurred. In other embodiments, the determinations of the crash determination subsystem  are binding upon the safety restraint controller . The crash determination subsystem  can generate crash determinations in a wide variety of different ways using a wide variety of different crash determination heuristics. Multiple heuristics can be combined to generate aggregated and probability-weighted conclusions. The patent application titled \u201cIMAGE PROCESSING SYSTEM FOR DETERMINING WHEN AN AIRBAG SHOULD BE DEPLOYED\u201d (Ser. No. 10\/052,152) was filed on Jan. 17, 2002 and is hereby incorporated by reference in its entirety, discloses various examples of crash determination heuristics.","1. Process-flow View of a Crash Determination",{"@attributes":{"id":"p-0182","num":"0196"},"figref":["FIG. 11","FIG. 11"],"b":["295","9","10"]},"Incoming ellipse parameters  or some other representation of the segmented image  is an input for computing residue values at  as discussed above. A past prediction (including a probability assigned to each state or mode in the various models) at  is also an input for computing the residue values .","At , gain matrices are calculated for each model and those gain matrices are used to estimate a new prediction for each model at . The residues at  and the estimates at  are then used to calculate likelihoods for each model at . This involves calculating a probability associated with each \u201ccondition\u201d such as \u201cmode\u201d and \u201cstate.\u201d","At , the system  compares the probability associated with the condition of crashing (or in some cases, pre-crash braking), to a predefined crash condition threshold. If the relevant probability exceeds the predefined crash condition threshold, a crash is deemed to have occurred, and the system  performs disablement processing at . If no crash is deemed to have occurred, a new ambient image  is captured, and the looping process of the tracking and predicting subsystem  continues.","In a preferred embodiment of the system , disablement process  such as the processing performed by the impact assessment subsystem  and the At-Risk-Zone detection subsystem  are not performed until after the crash determination subsystem  determines that a crash (or in some embodiments, pre-crash braking) has occurred.","In some embodiments of the system , the sensor  can operate at a relatively slow speed in order to utilize lower cost image processing electronics. Moreover, the system  can utilize a sensor  that operates at a relatively lower speed for crash detection while operating at a relatively higher speed for ARZ detection, as described in greater detail below.","2. Input-output View of Crash Determination",{"@attributes":{"id":"p-0188","num":"0202"},"figref":"FIG. 12","b":["220","190","210","298","226","296","298","226"]},"3. Probability-weighted Condition Models","a. Modeling Shape States","A preferred embodiment of the system  uses a multiple-model probability weighted implementation of a Kalman filter for all shape characteristics and motion characteristics. In a preferred embodiment, each shape characteristic has a separate Kalman filter equation for each shape state. Similarly, each motion characteristic has a separate Kalman filter equation for each motion mode. In a preferred embodiment of the invention, the occupant  has at least one shape state and at least one motion mode. There are certain predefined probabilities associated with a transition from one state to another state. These probabilities can best be illustrated through the use of Markov chains.",{"@attributes":{"id":"p-0191","num":"0205"},"figref":"FIG. 13","b":["106","300","302","304"]},"The probability of an occupant being in a particular state and then ending in a particular state can be identified by lines originating at a particular shape state with arrows pointing towards the subsequent shape state. For example, the probability of an occupant in center state remaining in center state Pis represented by the arrow at . The probability of moving from center to left Pis represented by the arrow  and the probability of moving from center to right Pis . The total probabilities resulting from an initial state of center  must add up to 1.\n\n=1.0\u2003\u2003Equation 34:\n\nFurthermore, all of the probabilities originating from any particular state must also add up to 1.0.\n","The arrow at  represents the probability that a left tilting occupant  will sit centered P, by the next interval of time. Similarly, the arrow at  represents the probability that a left tilting occupant will tilt right Pby the next interval of time, and the arrow at  represents the probability that a left tilting occupant will remain tilting to the left P. The sum of all possible probabilities originating from an initial tilt state of left must equal 1.\n\n=1.0\u2003\u2003Equation 35:\n","Lastly, the arrow at  represents the probability that a right tilting occupant will remain tilting to the right P, the arrow at  represents the probability that a right tilting occupant will enter a centered state P, and the arrow at  represents the probability that an occupant will tilt towards the left P. The sum of all possible probabilities originating from an initial tilt state of right equals 1.\n\n=1.0\u2003\u2003Equation 36:\n","As a practical matter, a preferred embodiment of the system  utilizes a standard commercially available video camera as the sensor . A typical video camera captures between 50 and 100 sensor readings each second. Even though the system  is preferably configured to perform crash detection heuristics in a low-speed mode (capturing between 5 and 15 images per second) and disablement heuristics in high-speed mode (capturing between 30 and 50 images per second), the speed of the video camera is sufficiently high such that it is essentially impossible for a left  leaning occupant to become a right  leaning occupant, or for a right  leaning occupant to become a left  leaning occupant, in a mere 1\/50 of a second. Thus, it is far more likely that a left  leaning occupant will first enter a center state  before becoming a right  leaning occupant, and similarly, it is far more realistic for a right  leaning occupant to become a centered  occupant before becoming a left  leaning occupant. Thus, in the preferred embodiment of, Pat  is always set at zero and Pat  will also always be set at zero. The three probability equations relating to shape state are thus as follows:\n\n=1.0\u2003\u2003Equation 37:\n\n=1.0\u2003\u2003Equation 38:\n\n=1.0\u2003\u2003Equation 39:\n","The values above are populated in a predefined manner based on empirical data, and generally useful assumptions about human behavior. In highly specialized contexts, additional assumptions can be made.","b. Modeling Motion Modes",{"@attributes":{"id":"p-0198","num":"0212"},"figref":"FIG. 14","b":["100","330","106","332","106","106","334","106","100"]},"The probability of an occupant  being in a particular motion mode and then ending in a motion mode can be identified by lines originating in the current mode with arrows pointing to the new mode. For example, the probability of an occupant in a stationary state remaining in stationary mode Pis represented by the arrow at . The probability of moving from stationary to human Pis represented by the arrow  and the probability of moving from stationary to crash Pis . The total probabilities resulting from an initial state of stationary  must add up to 1.\n\n=1.0\u2003\u2003Equation 40:\n","Similarly, the probability of human to human is Pat , the probability of human to stationary is Pat , and the probability of human to crash is Pat . The total probabilities resulting from an initial state of human  must add up to 1.\n\n=1.0\u2003\u2003Equation 41:\n","Lastly, the probability of going from crash to crash is Pat , crash to stationary is Pat , and crash to human is Pat . The total probabilities resulting from an initial state of crash  must add up to 1.","\u2003=1.0\u2003\u2003Equation 42:","As a practical matter, it is highly unlikely (but not impossible) for an occupant  to ever leave the state of crash at  once that state has been entered. Under most scenarios, a crash at  ends the trip for the occupant . Thus, in a preferred embodiment, Pis set to nearly zero and Pis also set to nearly zero. It is desirable that the system  allow some chance of leaving a crash mode  or else the system  may get stuck in a crash mode  in cases of momentary system  \u201cnoise\u201d conditions or some other unusual phenomenon. Alternative embodiments can set Pand Pto any desirable value, including zero, or a probability substantially greater than zero.","The transition probabilities associated with the various shape states and motion modes are used to generate a Kalman filter equation for each combination of characteristic and state\/mode\/condition. The results of those filters can then be aggregated in to one result, using the various probabilities to give the appropriate weight to each Kalman filter. All of the probabilities are predefined by the implementer of the system .","The Markov chain probabilities provide a means to weigh the various Kalman filters for each characteristic and for each state, mode, or other condition. The tracking and predicting subsystem system  incorporates the Markov chain probabilities in the form of the shape tracker and predictor  and the motion tracker and predictor .","C. Examples of Occupant Crash Determinations",{"@attributes":{"id":"p-0206","num":"0220"},"figref":"FIG. 15","b":["106","174","106","250","106","330","106","330","334","332"]},{"@attributes":{"id":"p-0207","num":"0221"},"figref":["FIG. 16","FIG. 15"],"b":["106","102","106","330","332"]},{"@attributes":{"id":"p-0208","num":"0222"},"figref":["FIG. 17","FIGS. 15 and 16","FIGS. 15 and 16","FIG. 17","FIGS. 15 and 16"],"b":["106","332","330","106"]},"III. Disablement Processing","As illustrated in , an indication of a \u201ccrash\u201d condition at  (e.g. a mode of either crash  or pre-crash breaking) results in the performance of various disablement processing  in the form of one or more disablement heuristics. As indicated in , two examples of disablement heuristics are an At-Risk-Zone detection heuristic (\u201cARZ heuristic\u201d) performed by an At-Risk-Zone Detection Subsystem (\u201cARZ subsystem\u201d)  and an impact assessment heuristic performed by an impact assessment subsystem . Both the impact assessment subsystem  and the ARZ subsystem  can generate disablement flags indicating that although a crash has occurred, it may not be desirable to deploy the safety restraint device. The ARZ subsystem  can set an At-Risk-Zone disablement flag  to a value of \u201cyes\u201d or \u201cdisable\u201d when the occupant  is predicted to be within the At-Risk-Zone at the time of the deployment. Similarly, the impact assessment subsystem  can set an impact assessment disablement flag  to a value of \u201cyes\u201d or \u201cdisable\u201d when the occupant  is predicted to impact the deploying safety restraint device with such a severe impact that the deployment would be undesirable.","A. Impact Assessment",{"@attributes":{"id":"p-0211","num":"0225"},"figref":"FIG. 18","b":["222","222","296","190"]},"The outputs of the impact assessment subsystem  can include an impact assessment metric. In a preferred embodiment, the impact assessment metric  is a kinetic energy numerical value relating to the point in time that the occupant  is estimated to impact into the deploying safety restraint. In alternative embodiments, momentum, or a weighted combination of kinetic energy and momentum can be used as the impact metric. Alternative embodiments can utilize any impact metric incorporating the characteristics of mass, velocity, or any of the other motion or shape variables, including any characteristics that could be derived from one or more motion and\/or shape variables. In some alternative embodiments, the impact assessment metric could be some arbitrary numerical construct useful for making impact assessments.","The impact assessment subsystem  uses the shape and motion variables above to generate the impact metric  representing the occupant  impact that an airbag, or other safety restraint device, needs to absorb.","If the impact assessment metric  exceeds an impact assessment threshold value, then the impact disablement flag  can be set to a value of \u201cyes\u201d or \u201cdisable.\u201d In some embodiments, the impact assessment threshold is a predefined value that applies to all occupants . In other embodiments, the impact assessment threshold is a \u201csliding scale\u201d ration that takes into consideration the characteristics of the occupant  in setting the threshold. For example, a larger person can have a larger impact assessment threshold than a smaller person.","In some embodiments of the system , the impact assessment can be associated with an impact assessment confidence value . Such a confidence value  can take into consideration the likely probabilities that the impact assessment metric  is a meaningful indicator as generated, in the particular context of the system .","Three types of occupant characteristics  are commonly useful in generating impact assessment metrics . Such characteristics  are typically derived from the images captured by the sensor , however, alternative embodiments may include additional sensors specifically designed to capture information for the impact assessment subsystem . The three typically useful attributes are mass, volume, and width.","1. Mass","As disclosed in Equation 43 below, mass is used to compute the impact metric. The density of a human occupant  is relatively constant across broad spectrum of potential human occupants . The average density of a human occupant  is known in the art as anthropomorphic data that can be obtained from NHTSA (National Highway Traffic Safety Administration) or the IIA (Insurance Institute of America). The mass of an occupant  is substantially a function of volume.\n\nMass=Volume*Density\u2003\u2003Equation 43:\n","In a preferred embodiment, the system  determines whether or not the occupant  is restrained by a seat belt. This is done in by comparing the velocity (x\u2032) of the occupant  with the rate of change in the forward tilt angle (\u03b8\u2032). If the occupant is restrained by a seat belt, the rate of change in the forward tilt angle should be roughly two times the velocity of the occupant . In contrast, for an unbelted occupant, the ratio of \u03b8\u2032\/x\u2032 will be roughly zero, because there will be an insignificant change in the forward tilt angle for an unbelted occupant. If an occupant  is restrained by a functional seatbelt, the mass of the occupant's  lower torso should not be included in the impact metric of the occupant  because the mass of the lower torso is restrained by a seal belt, and thus that particular portion of mass will not need to be constrained by the safety restraint deployment mechanism . If the occupant  is not restrained by a seatbelt, the mass of the lower torso needs to be included in the mass of the occupant . Across the broad spectrum of potential human occupants , the upper torso is consistently between 65% and 68% of the total mass of a human occupant . If the occupant  is not restrained by a seat belt in a preferred embodiment, the mass of both the occupant  (including the lower torso) is calculated by taking the mass of the upper torso and dividing that mass by a number between 0.65 and 0.68. A preferred embodiment does not require the direct calculation of the volume or mass of the lower ellipse .","The volume of an ellipsoid is well known in the art.\n\nVolume= 4\/3*\u03c0*major*minor*minor\u2003\u2003Equation 44:\n\nMajor is the major axis . Minor, is the minor axis . The 2-D ellipse is known to be a projection from a particular angle and therefore allows the system  to decide what the originating 3-D Ellipsoid should be. Shape characteristics tracked and predicted by the shape tracker and predictor module  can be incorporated into the translation of a 3-D ellipsoid from a 2-D ellipse. In a preferred embodiment, the \u201cwidth\u201d of the ellipsoid is capped at the width of the vehicle seat  in which the occupant  sits. The width of the vehicle seat  can be easily measured for any vehicle before the system  is used for a particular vehicle model or type.\n","Minoris derived from the major axis  and the minor axis . Anthropomorphic data from NHTSA or the Insurance Institute of America is used to create electronic \u201clook-up\u201d tables deriving the z-axis information from the major axis  and minor axis  values.",{"@attributes":{"id":"p-0221","num":"0235"},"figref":"FIGS. 19","i":["a","b","c "],"b":["19","19","112","22","360"]},"2. Velocity","Velocity is a motion characteristic derived from the differences in occupant  position as described by Newtonian mechanics and is described in greater detail above. The relevant measure of occupant  velocity is the moment of impact between the occupant  and the airbag (or other form of safety restraint device). The movement of the airbag towards the occupant  is preferably factored into this analysis in the preferred embodiment of the system .\n\n\u222bVelocity=\u222bVelocity\u2003\u2003Equation 45:\n\n3. Additional Alternative Variations and Embodiments\n","The underlying calculations of motion and shape variables can be updated very quickly using the outputted state transition matrix which allows the system  to predict the position and shape in advance, and at a rate more quickly than the rate in which the sensor  collects data. The impact metric prediction is thus similarly updated at a quicker rate than the rate at which the sensor  collects data. In alternative embodiments of the invention that classify the occupant  into different occupant types, each occupant type could have a distinct density. In a preferred embodiment, the impact assessment subsystem  is not invoked until after a crash condition is detected, or the probability of a crash condition is not lower that some predefined cautious threshold.","B. At-risk-zone Detection","Returning to , the At-Risk-Zone detection subsystem  is disclosed sending the At-Risk-Zone flag  to the safety restraint controller . Like all disablement flags, some disablement flags are \u201cmandatory\u201d in certain embodiments of the system , while other disablement flags in other system embodiments are \u201cdiscretionary\u201d or \u201coptional\u201d with final control residing within the safety restraint controller .","1. Input-Output View",{"@attributes":{"id":"p-0226","num":"0240"},"figref":"FIG. 20","b":["224","224","190","296","224","296"]},"The primary output of the ARZ detector subsystem  (which can also be referred to as a detection subsystem ) is an At-Risk-Zone determination . The outputs of the ARZ detector subsystem  can also include an At-Risk-Zone disablement flag  that can be set to a value of \u201cyes\u201d or \u201cdisable\u201d in order to indicate that at the time of deployment, the occupant  will be within the At-Risk-Zone. In some embodiments, the At-Risk-Zone assessment is associated with a confidence value  utilizing some type of probability value.","2. Process Flow View",{"@attributes":{"id":"p-0228","num":"0242"},"figref":"FIG. 21","b":"224"},"At , the input of the crash determination  is used to invoke the creation of a detector window for the At-Risk-Zone. As discussed above, in a preferred embodiment, the ARZ detection subsystem  is not invoked unless there is some reason to suspect that a crash or pre-crash breaking is about to occur. In alternative embodiments, ARZ processing can be performed without any crash determination , although this may result in the need for more expensive electronics within the decision enhancement device .","In a preferred embodiment, the ARZ is predefined, and takes into consideration the internal environment of the vehicle . In this process step, a window of interest is pre-defined to enclose the area around and including the At Risk Zone. The window is intentionally set slightly towards the occupant in front of the ARZ to support a significant correlation statistic.  illustrate an example of the detection window. The detection window is represented by the white rectangle to the part of the vehicle  in front of the occupant . Subsequent processing by the ARZ heuristic can ignore image pixels outside of the window of interest. Thus, only the portions of the ellipse (if any) that are within the window of interest require the system's attention with respect to ARZ processing. In , the occupant  is in a seated position that is a significant distance from the window of interest. In , the occupant  is much closer to the ARZ, but is still entirely outside the window of interest. In , a small portion of the occupant  is within the window of interest, and only that small portion is subject to subsequent processing for ARZ purposes in a preferred embodiment. In , a larger portion of the occupant  resides within the window of interest, with the occupant  moving closer to the window of interest and the ARZ as the position of the occupant  progresses from .","At , the sensor  (preferably a video camera) can be set from a low-speed mode (for crash detection) to a high speed mode for ARZ instrusion detection. Since the ARZ heuristics can ignore pixels outside of the window of interest, the ARZ heuristic can process incoming images at a faster frame rate. This can be beneficial to the system  because it reduces the latency with which the system  is capable of detecting an intrusion into the ARZ. In a typical embodiment, the \u201clow-speed\u201d mode of the video camera captures between approximately 5-15 (preferably 8) frames per second. In a typical embodiment, the \u201chigh-speed\u201d mode of the video camera captures between approximately 20-50 (preferably 30-40) frames per second.","At , the ARZ heuristic can divide the detector window (e.g. window of interest) into patches . In this step the incoming image in the region of the ARZ detector window is divided into N\u00d7M windows where M is the entire width of the detector window and N is some fraction of the total vertical extent of the window. The purpose of processing at  is to allow the ARZ heuristic to compute the correlation between the incoming ambient image  and a reference image in \u201cbands\u201d (which can also be referred to as \u201cstrings\u201d) which improves system  sensitivity. Reference images are images used by the system  for the purposes of comparing with ambient images  captured by the system . In a preferred embodiment, references images are captured using the same vehicle enterior  as the vehicle utilizing the system . In alternative embodiments, reference images may be captured after the system  is incorporated into a vehicle . For example, after the occupant  leaves the vehicle , a sensor reading of an empty seat  can be captured for future reference purposes.","The processing at  allows a positive detection to be made when only a portion of the ARZ detection window is filled as is the case in  where only the occupant's head is in the detection window and there is no change in the lower half of the window. In a preferred embodiment, the reference image is that of a empty occupant seat  corresponding to a similar vehicle  interior. Such a reference image can also be useful for segmentation and occupant-type classifying heuristics. In alternative embodiments, the reference image can be the image or sensor reading received immediately prior the current sensor reading.","At , the system  generates a correlation metric for each patch  with with respect to the reference image using one of a variety of correlation heuristics known in the art of statistics. This process step can include the performance of a simple no-offset correlation heuristic between the reference image and the incoming window of interest image.","At , a combined or aggregate correlation metric is calculated from the various patch-level correlation metrics generated at . The individual scores for each of the sub-patches in the ARZ detection window provide an individual correlation value. All of these values must then be combined in an optimal way to minimize false alarms and maximize the detection probability. In embodiments where the ARZ heuristic is only invoked after a crash determination  (or at least a greater than X % likelihood of being in a state of crash of pre-crash breaking), the likelihood of a false alarm is less than in a normal detection situation so the correlation threshold can be set lower to ensure a higher probability of detection.","At , the aggregate correlation metric from  is compared to a test threshold value that is typically pre-defined.","If the correlation metric exceeds the test threshold value, the system  can at  can set the ARZ disablement flag to a value of \u201cyes\u201d or \u201cdisable\u201d to indicate that the occupant  is believed to be within the ARZ. As discussed above, in some embodiments, the setting of the flag is \u201cbinding\u201d on the safety restraint controller , while in other embodiments, the safety restraint controller  can utilize the information to generate an independent conclusion. As discussed above, it is preferable that the detection window be defined so that it is slightly in front of the ARZ. The relative time of the initial excessive motion is known and the number of frames are known until the occupant has entered the ARZ Detection Window, and the relative distance from the initial point of the occupant  to the ARZ detection window is known from the Multiple Model Tracker, it is possible to estimate the speed of the occupant  and provide some predictive capability for the system  as well. In other words, since the occupant  has not yet entered the ARZ and the system  knows their speed, the system  can predict the time to entry and send an ARZ Intrusion Flag  in anticipation to the safety restraint controller . This allows the system  to remove some of the overall system latency in the entire vehicle due to vehicle bus (e.g. vehicle computer device(s)) latencies and the latency in the decision enhancement device  and the safety restraint controller . In other words, by setting the window of interest closer to the occupant  than the ARZ, the timing of decisions generated by the system  can compensate for a slower processing architecture incorporated into the system .","3. Subsystem-Level Views for ARZ Embodiments",{"@attributes":{"id":"p-0238","num":"0252"},"figref":"FIG. 26","b":"100"},"a. Sensor Subsystem","A sensor subsystem  can include the one or more sensors  used to capture sensor readings used by the tracking and predicting heuristics discussed above. In a preferred embodiment, there is only one sensor  supporting the functionality of the decision enhancement system . In a preferred embodiment, the sensor  is a standard video camera, and is used in a low-speed mode by a tracking subsystem  and is used in a high-speed mode by the ARZ detection subsystem (\u201cdetection subsystem\u201d ). The sensor subsystem  need not coincide with the physical boundaries of the sensor component  discussed above.","b. Tracking Subsystem","The tracking and predicting subsystem (\u201ctracking subsystem\u201d)  is discussed in detail above, and is illustrated in FIG. . The tracking subsystem  is responsible for tracking occupant characteristics , and preferably includes making future predictions of occupant characteristics . The tracking subsystem  processes occupant information in the context of various \u201cconditions\u201d such as the \u201cstates\u201d and \u201cmodes\u201d discussed above. Such conditions are preferably predetermined, and probability-weighted. When the tracking subsystem  determines that the occupant  is in a condition of crashing, pre-crash breaking, or is otherwise on the threshold of potentially requiring the deployment of the safety restraint mechanism  (collectively \u201cdeployment situation\u201d), the tracking subsystem  can initiate the processing performed by the detection subsystem . The tracking subsystem  can also initiate the switch in the sensor  from a low-speed mode to a high-speed mode.","c. Detection Subsystem","The detection subsystem  and the various detection heuristics are discussed in detail above. The detection subsystem  can be configured in a wide variety of different ways, with certain variables such as the location and size of the At-Risk-Zone being configured to best suit the particular vehicle  environment in which the decision enhancement system  is being utilized. Various iterations of the detection heuristics that can be performed by the detection subsystem  are disclosed in the patent application titled \u201cIMAGE PROCESSING SYSTEM FOR DYNAMIC SUPPRESSION OF AIRBAGS USING MULTIPLE MODEL LIKELIHOODS TO INFER THREE DIMENSIONAL INFORMATION\u201d (Ser. No. 09\/901,805) that was filed on Jul. 10, 2001, and is hereby incorporated by reference in its entirety.","d. Category Subsystem",{"@attributes":{"id":"p-0246","num":"0260"},"figref":"FIG. 27","b":["100","202","106","108","106","100"]},"e. Integrated Decision Making","Although various functions performed by the system  such as occupant tracking, crash determination, impact assessment, ARZ detection, and occupant-type classification, the system  can incorporate certain conclusions into the processing of other conclusions. For example, it may be desirable to take into consideration the probability of crash in determining whether the ARZ flag  should be set to a value of \u201cyes\u201d or \u201cdisable.\u201d If the system  is relatively unsure about whether a crash has occurred (e.g. the probability associated with a crash condition is just barely at the predefined threshold value), the \u201cborderline\u201d conclusion can be used to properly evaluated impact assessment, ARZ detection, and even occupant-type classification processing as well as the results of those processes.","4. Implementation Methodology",{"@attributes":{"id":"p-0249","num":"0263"},"figref":"FIG. 28","b":"100"},"At , the At-Risk-Zone is defined to correspond to a location of the deployment mechanism  within the vehicle . In some embodiments, this may also correspond to the location of the safety restraint controller .","At , the sensor  is configured for transmitting sensor readings to the decision enhancement device . The sensor configuration in a preferred embodiment is discussed below, in a component-level view of the system .","At , one or more computer components within the decision enhancement device  are programmed to filter out a window-of-interest. The window-of-interest should preferably be defined to be slightly in front to the ARZ so that the system  has sufficient time to react to a predicted ARZ intrusion.","At , one or more computer components with the decision enhancement device  are programmed to set at At-Risk-Zone flag if the component(s) determines that the occupant  would be within the At-Risk-Zone at the time of deployment. This determination can be \u201cbinding\u201d or merely \u201cdiscretionary\u201d with respect to the safety restraint controller .","At , the decision enhancement device  including the sensor  and other components is installed within the vehicle  in accordance with the contextual information leading up to the definition of the At-Risk-Zone within the vehicle .","IV. Component-based Views of the Decision Enhancement System","A. Component-based Subsystem-level Views",{"@attributes":{"id":"p-0256","num":"0270"},"figref":"FIG. 29"},"The decision enhancement system  can be composed of five primary subsystems: an image capture subsystem (ICS) ; an image processing subsystem (IPS) ; a power management subsystem (PMS) ; a communications subsystem (CS) ; and a status, diagnostics, control subsystem (diagnostic subsystem or simply SDCS) .","1. Image Capture Subsystem","The image capture subsystem  can include: a sensor module  for capturing sensor readings; an illumination module  to provide illuminazion within the vehicle  to enhance the quality of the sensor readings; and a thermal management module  to rake either manage or take into consideration the impact of heat on the sensor . The ICS  preferably uses a custom state-of-the-art complementary metal oxide semiconductor (CMOS) imager providing on-chip exposure control, psuedo-logrithmic response and histogram equalization to provide high-contrast, low-noise images for the IPS . The CMOS unager can provide for the electronic adding, subtracting, and scaling of a polarized signal (e.g. a \u201cdifference\u201d image).","The interior vehicle  environment can be one of the most difficult for image collection. The environment includes wide illumination levels, high clutter (shadows), and a wide temperature range This environment requires the imager to have wide dynamic range, low noise thermal noise, fast response to changing illumination, operation in dark conditions, and high contrast images. These characteristics are achieved in the system  by incorporating on-chip exposure control, pseudo-logarithmic response and histogram equalization. The imager can operate at modest frame rates (30-40 Hz) due to the predictive nature of the tracking and predicting heuristics. This is a significant advantage (lower data rates, less data, longer exposure time) over non-predictive systems would require frame rates up to 1000 hz to meet the ARZ intrusion timing requirements.","The large range of occupant positions, sizes and the limited system locations possibilities created a requirement for a very wide angle lens. A custom lens design was undertaken to generate a lens which has a 130 degree horizontal by 100 degree vertical field-of-view (FOV). This FOV is made slightly oversized to accommodate routine mounting uncertainties in the vehicle installation process. The lens has specific requirements for modulation transfer function (MTF) and image distortion important for forming high contrast images with the least amount of deformity over the wide spectral band of the system .","Operation in dark conditions typically requires the use of infrared illumination. The particular wavelength selected (880 nm) is a compromise in the tradeoff between matching the imager spectral sensitivity, minimizing distraction to the occupant, and using currently available LED (light emitting diode) technology. A key feature of the system  is the design of the illuminator. A preferred embodiment of the design incorporates a cylindrical shape in the vertical axis. In some embodiments, a distribution of LED's which directs more light to the extremes of the image is used. For example an LED configuration of 8-6-4-4-6-8 (with each number representing the number of LED's in a particular row) could provide more light at the outside extremities (8 LED's per row on the outer extremes) than for the center of the image (there would only be 4 LED's per row in the inner two rows). In a preferred embodiment, 5 rows of 4 LED's are used. The illuminator preferably incorporates a diffusing material which more evenly distributes the LED output while providing a larger apparent source size which is important for eye-safety. A requirement for the illuminator is that is must be safe for the occupant  by meeting eye and skin safe exposure standards. This requirement is met with this design through mechanical means (diffuser) and electrically via over-current protection and electromagnetic compatibility (EMC) immunity. Reliability can be improved through randomization of the electrical drive circuit thus preventing a large portion of the image from being darkened in the case of the failure of a group of LED's.","The ICS  includes the sensor component  discussed above. It can also include the illumination component  discussed above. A portion of the analysis component  discussed above is part of the ICS .","2. Image Processing Subsystem","The image processing subsystem  can include a head and torso tracking module  that provides the functionality of the tracking and predicting subsystem  discussed above. The image processing subsystem  can also include a deployment and disablement module  to house the deployment and disablement heuristics discussed above.","In a preferred embodiment, the IPS  is comprised of a digital signal processor (DSP) and local memory. The configuration of using a DSP coupled with local memory that is distinct from the analysis component  discussed above can be a desirable architecture for timely processing. The IPS  provides for object segmentation, classification, tracking, calibration, and image quality. The IPS  is also typically the interface to the communication subsystem .","The IPS executes the various imaging processing heuristics discussed above. The heuristics are initially stored in flash memory and loaded by the MCU (microcontroller unit) into the DSP during intialization. This boot method allows the system to be updated through the external communications bus providing the ability to accommodate upgrades and changes to occupant types (child and infant seats for example) or federal requirements. The IPS uses a pipelined dual processor\/internal dual-port random access memory CRAM) DSP coupled to external static random access memory (SRAM). This architecture allows for efficient processing with intermediate results and reference images stored in external memory.","3. Power Management Subsystem","The power management subsystem  provides incoming power conditioning, transient suppression, and power sequencing for starting and shutting down the system  and potentially one or more of the automated applications for the vehicle .","The power management subsystem  provides the interface to the vehicle power source, watchdog and reset function for the microcontroller unit (MCU) and reserve power during a loss of power situation. The vehicle interface includes the typical automotive requirements, under\/over voltage, reverse polarity, double voltage, load-dump, etc. The watchdog expects a timed reset from the MCU, lack of which causes the system  to reset. The reserve power maintains operation of the MCU and communications after power loss to allow for possible reception of a crash notification and subsequent recording of last transmitted classification and ARZ intrusion status.","The power management subsystem (PMS)  can include one or more power components  as discussed above.","4. Communications Subsystem","The communications subsystem (CS)  provides communication over a bus to the vehicle controller and uses the system microcontroller unit (MCU) resource. The communications subsystem (CS)  can include a vehicle  local area network (LAN) module  and a monitor module  for accessing the various components of the decision enhancement device  while they are installed in the vehicle .","Occupant characteristics  such as classification-type, ARZ intrusion status, impact assessment, other disablement information, and\/or deployment information can be communicated by the CS  to the safety restraint controller  through the MCU (part of the CS) to the vehicle controller area network (CAN) bus. A CAN is an information technology architecture comprised of independent, intelligent modules connected by a single high-speed cable, known as a bus, over which all the data in the system flows. While this protocol has some inherent and non-deterministic delay, the predictive nature of the ARZ intrusion heuristic accommodates the delay while meeting the NHTSA (\u201cNational Highway Transportation Safety Administration\u201d) airbag suppression delay specification. Tracking and predicting data can also transmitted at a lower rate over the bus. While the CAN bus is used in this implementation the communication of ARZ intrusion status is not limited to this technique. Any alternate transmission form providing verification feedback may be used. The CS  provides for transmission of unit identifier data, error conditions and reception of crash notification for recording of last transmitted classification and ARZ status.","5. Diagnostic Subsystem","The SDCS  provides for system  diagnostics, and controls the image and illuminator of the ICS . The functionality of the SDCS  includes monitoring: the accuracy of the sensor , the internal temperature within the various components of the enhancement device .","B. Hardware Functionality View",{"@attributes":{"id":"p-0273","num":"0287"},"figref":"FIG. 30","b":"100"},"1. Infrared Illuminator","An infrared illuminator  can be used to illuminate the interior area  to facilitate better image quality. In a preferred embodiment, the illuminator  should operate at a wavelength that balances the following goals: matching the imager spectral sensitivity; minimizing the distraction to the occupant ; and using commercially available \u201coff-the-shelf\u201d LED technology.","2. Filter","A filter  can be used to filter or regulate the power sent to the micro-controller unit .","3. Illuminator\/Control","An illuminating control  is the interface between the micro-controller (MCU)  and the illuminator .","4. Watchdog\/Reset Generator","A watchdog\/reset generator  is part of the SDCS , and is responsible for \u201cresetting\u201d the system  as discussed above.","5. Power Supply\/Power Monitor","A power supply\/power monitor  supports the functionality of the PMS  discussed above.","6. Serial Flash","A serial flash component  is the flash memory unit discussed above. It serves as a local memory unit for image processing purposes.","7. Image Sensor","An image sensor  is the electronic component that receives the image through a lens . The sensor readings from the image sensor  are sent to the DSP . The image sensor  is part of the sensor component  and ICS  that are discussed above.","8. Lens","The lens  is the \u201cwindow\u201d to the outside world for an image sensor . As discussed above, the lens  should have a horizontal field-of-view (FOV) between about 100 degrees and 160 degrees (preferably 130 degrees) and a vertical FOV between about 80 degrees and 120 degrees (preferably 100 degrees).","9. Imager Oscillator","An imager oscillator  produces electric oscillations for the image sensor .","10. SDRAM","An SDRAM  is a local memory unit used by the DSC .","11. Micro-Controller","The micro-controller  is the means for communicating with the vehicle , and other devices on the vehicle  such as the safety restraint controller  and deployment mechanism . The micro-controller  operates in conjunction with the Digital Signal Processor (DSP) .","12. Digital Signal Processor","The DSP , unlike a microprocessor, is designed to support high-speed, repetitive, numerically intensive tasks used by the IPS  to perform a variety of image processing functions. It is the DSP  that sets various disablement flags, and makes other application-level processing decisions as discussed above. The DSP  is part of the analysis component  discussed above.","13. SDM\/DASS Interface","An SDM\/DASS Interface  is part of the SDCS  responsible for monitoring the performance of the sensor .","14. LAN Interface","A LAN interface  is part of the CS  that facilitates communications between the system  and the computer network on the vehicle .","15. Level Shifters","A voltage level shifter  is enabled by the used to control the voltage for the micro-controller  between 5 and 7 volts.","16. Thermistor","A thermistor  is used to monitor the temperature surrounding the various components of the system . It is part of the SDCS  discussed above.","17. S\/W Diagnostic Testpoints","An S\/W diagnostic testpoints  and  refers to a part of the SDCS  used to confirm the proper processing of software used by the system  by \u201ctesting\u201d certain \u201creference points\u201d relating to the software processing. The testpoints  for the micro-controller  are distinct from the testpoints  for the DSP .","18. Crystal","A crystal oscillator  can be used to tune or synthesize digital output for communication by the CS  to the other vehicle applications, such as the safety restraint controller .","19. PLL Filter","A phase-locked loop filter (PLL filter ) is used to perform the gradient calculations of the Kalman filter.","C. One Example of a Hardware Configuration",{"@attributes":{"id":"p-0294","num":"0308"},"figref":"FIG. 31","b":["600","650","702","700","702","704","706"]},"The configuration in  is just one example of how the different components illustrated in  can be arranged. In other embodiments, all of the different components of  can possess their own distinct component units or boxes within the system . On the other side of the continuum, all of the components in  can be located within a single unit or box.","1. Power Supply\/MCU Box",{"@attributes":{"id":"p-0296","num":"0310"},"figref":"FIG. 32","i":"a ","b":["600","600","122","124","126","600","130","582"]},"2. Imager\/DSP Box",{"@attributes":{"id":"p-0297","num":"0311"},"figref":"FIG. 32","i":"b ","b":["650","112","650"]},"3. Component Examples",{"@attributes":{"id":"p-0298","num":"0312"},"figref":"FIG. 33","b":["100","730","722","100"]},"The example in  includes two housing components  and  and an imager circuit card  that includes tabs for configuring the imaging tool while it is assembled. Parts of the imaging tool can be focused and aligned by the movement of \u201ctabs\u201d that are accessible from outside the imaging tool. The tabs can resemble various linear adjustment mechanisms in other devices.","On the left side of the diagram is a lens assembly  that includes the various lenses incorporated into the imaging tool. The number and size of lenses can vary widely from embodiment to embodiment. A lens o-ring  is used to secure the position and alignment of the lens assembly . Some embodiments may not involve the use of o-rings , while other embodiments may incorporate multiple o-rings . A front housing component  and a rear housing component  are ultimately fastened together to keep the imaging tool in a fully aligned and focused position. In between the two housing components is an imager circuit board  with the imager  on the other side, hidden from view.",{"@attributes":{"id":"p-0301","num":"0315"},"figref":"FIG. 34","b":["736","738","740","742","744","746","736","748","720","750","720","736"]},{"@attributes":{"id":"p-0302","num":"0316"},"figref":["FIG. 35","FIGS. 33 and 34"],"b":["736","736","126"]},{"@attributes":{"id":"p-0303","num":"0317"},"figref":"FIG. 36","b":["128","760","760"]},"A power circuit board (PCB)  that actually holds the LED's (light emitting diodes) is also shown in the Figure. In a preferred embodiment, the PCB  is in an \u201cH\u201d shape that includes a flexible material in the middle of the \u201cH\u201d so that one side can be bent over the other.","A heat conducting bond ply tape  is used to attach the PCB  with the heat spreader . A separate piece of heat conducting bond ply type  is used to connect an illuminator heat spreader  (which serves just the LED's in contrast to the heat spreader  for the drive circuitry) to the LED's on the PCB . A surface  underneath the illuminator is what is visible to the occupant . The surface  is preferably configured to blend into the internal environment of the vehicle .",{"@attributes":{"id":"p-0306","num":"0320"},"figref":"FIGS. 37","b":["38","39","702"]},"D. Implementation of Hardware Configuration Process",{"@attributes":{"id":"p-0308","num":"0322"},"figref":"FIG. 40"},"At , the imager is configured to communicate with one or more analysis components .","At , the various image processing heuristics, including the tracking and predicting heuristics, the disablement heuristics, the deployment heuristics, and the segmentation heuristics.","At , a reference image is loaded onto the system . In some embodiments, this is stored on the local memory unit connected to the imager to facilitate quick processing.","At , the imager and analysis components are fixed within one or more casings that can then be installed into a vehicle.","V. Alternative Embodiments","While the invention has been specifically described in connection with certain specific embodiments thereof, it is to be understood that this is by way of illustration and not of limitation, and the scope of the appended claims should be construed as broadly as the prior art will permit. For example, the system  is not limited to particular types of vehicles , or particular types of automated applications.","VI. Related Applications","The following applications are hereby incorporated by reference in their entirety: \u201cIMAGE PROCESSING SYSTEM FOR DYNAMIC SUPPRESSION OF AIRBAGS USING MULTIPLE MODEL LIKELIHOODS TO INFER THREE DIMENSIONAL INFORMATION,\u201d Ser. No. 09\/901,805, filed on Jul. 10, 2001; \u201cIMAGE PROCESSING SYSTEM FOR ESTIMATING THE ENERGY TRANSFER OF AN OCCUPANT INTO AN AIRBAG,\u201d Ser. No. 10\/006,564, filed on Nov. 5, 2001; \u201cIMAGE SEGMENTATION SYSTEM AND METHOD,\u201d Ser. No. 10\/023,787, filed on Dec. 17, 2001; \u201cIMAGE PROCESSING SYSTEM FOR DETERMINING WHEN AN AIRBAG SHOULD BE DEPLOYED,\u201d Ser. No. 10\/052,152, filed on Jan. 17, 2002; \u201cMOTION-BASED IMAGE SEGMENTOR FOR OCCUPANT TRACKING,\u201d Ser. No. 10\/269,237, filed on Oct. 11, 2002; \u201cOCCUPANT LABELING FOR AIRBAG-RELATED APPLICATIONS,\u201d Ser. No. 10\/269,308, filed on Oct. 11, 2002; \u201cMOTION-BASED IMAGE SEGMENTOR FOR OCCUPANT TRACKING USING A HAUSDORF-DISTANCE HEURISTIC,\u201d Ser. No. 10\/269,357, filed on Oct. 11, 2002; \u201cSYSTEM OR METHOD FOR SELECTING CLASSIFIER ATTRIBUTE TYPES,\u201d Ser. No. 10\/375,946, filed on Feb. 28, 2003; \u201cSYSTEM AND METHOD FOR CONFIGURING AN IMAGING TOOL,\u201d Ser. No. 10\/457,625, filed on Jun. 9, 2003; \u201cSYSTEM OR METHOD FOR SEGMENTING IMAGES,\u201d Ser. No. 10\/619,035, filed on Jul. 14, 2003; \u201cSYSTEM OR METHOD FOR CLASSIFYING IMAGES,\u201d Ser. No. 10\/625,208, filed on Jul. 23, 2003; and \u201cSYSTEM OR METHOD FOR IDENTIFYING A REGION-OF-INTEREST IN AN IMAGE,\u201d Ser. No. 10\/663,521, filed on Sep. 16, 2003."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"p":["The present invention will be more fully understood upon reading the following detailed description in conjunction with the accompanying drawings.",{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 18"},{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIGS. 19","i":["a","b","c "],"b":["19","19"]},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 20"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 21"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 22"},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 23","b":"22"},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 24"},{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 25"},{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 26"},{"@attributes":{"id":"p-0041","num":"0040"},"figref":"FIG. 27"},{"@attributes":{"id":"p-0042","num":"0041"},"figref":"FIG. 28"},{"@attributes":{"id":"p-0043","num":"0042"},"figref":"FIG. 29"},{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 30"},{"@attributes":{"id":"p-0045","num":"0044"},"figref":"FIG. 31"},{"@attributes":{"id":"p-0046","num":"0045"},"figref":"FIG. 32","i":"a "},{"@attributes":{"id":"p-0047","num":"0046"},"figref":"FIG. 32","i":"b "},{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIG. 33"},{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIG. 34"},{"@attributes":{"id":"p-0050","num":"0049"},"figref":"FIG. 35"},{"@attributes":{"id":"p-0051","num":"0050"},"figref":"FIG. 36"},{"@attributes":{"id":"p-0052","num":"0051"},"figref":"FIG. 37"},{"@attributes":{"id":"p-0053","num":"0052"},"figref":"FIG. 38"},{"@attributes":{"id":"p-0054","num":"0053"},"figref":"FIG. 39"},{"@attributes":{"id":"p-0055","num":"0054"},"figref":"FIG. 40"}],"heading":"BRIEF DESCRIPTION OF THE DRAWINGS"},"DETDESC":[{},{}]}
