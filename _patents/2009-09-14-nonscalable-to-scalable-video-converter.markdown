---
title: Non-scalable to scalable video converter
abstract: Systems and methods are for implementing a NSV2SV converter that converts a non-scalable video signal to a scalable video signal. In an implementation, a non-scalable video signal encoded in H.264/AVC standard is decoded and segmented into spatial data and motion data. The spatial data is resized into a desired resolution by down-sampling the spatial data. The motion data is also resized in every layer, except in the top layer, of a scalable video coding (SVC) encoder by using an appropriate measure. Further, the motion data is refined based on the resized spatial data in every layer of the SVC encoder. The refined motion data and the down-sampled spatial data are then transformed and entropy encoded in the SVC standard in every layer. The SVC encoded output from every layer is multiplexed to produce a scalable video signal.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08395991&OS=08395991&RS=08395991
owner: STMicroelectronics S.R.L.
number: 08395991
owner_city: Agrate Brianza (MB)
owner_country: IT
publication_date: 20090914
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS"],"p":["The present invention relates to the field of video, and more particularly to video encoders and related methods.","Digital video services have enabled improved quality video signal transmission, resulting in an immaculate video display at the consumer end. Among various video coding standards available, Moving Picture Experts Group-2 (MPEG-2) is very popular, as it can be applied to diversified bit rates and sample rates. Additionally, the MPEG-2 video coding standard provides mature and powerful video coding methods and supports scalability.","In order to cater to newer transmission media such as Cable Modem, xDSL, or UMTS, H.264\/Advanced Video Coding (AVC) standard is gaining popularity as the basis of digital video transmission. This is because of its higher coding efficiency, lower bit rate, efficient bandwidth utilization, error resilience, low processing delay, support for scalability, and capability to produce high quality video. Moreover, H.264\/AVC enables transmission of more video channels or higher quality video representations within the existing digital transmission capacities.","With the advent of a variety of end user devices and time varying networks, video adaptation on the various end user devices for appropriate video presentation has become very critical. For example, in broadcast, simulcast, or multicast transmissions, the same signal may be received by various end user devices, such as televisions, cell phones, computing devices, etc. The end user devices can have different characteristics in screen sizes, life span of power supplies, memory capabilities, CPU's, etc. This makes the task of video adaptability on the targeted end devices very challenging even while using the video coding standards such as MPEG-2, H.264\/AVC, etc.","As a result, scalable video coding schemes are emerging that make it possible to adapt the bit rate and quality of a transmitted stream to the network bandwidth on which the stream is transmitted. The Scalable Video Coding or SVC standard has been developed as an extension to the H.264\/AVC standard. Among several scalabilities, spatial scalability, i.e. video adaptability through different spatial resolutions, is one of the key features generally required in scalable video streaming over heterogeneous devices such as mobile phones, televisions, personal digital assistants (PDAs), laptops, and so on. However, appropriate techniques for achieving spatial scalability and producing a scalable video signal subsuming multiple distinct resolutions from a non-scalable video signal are currently unavailable.","This summary is provided to introduce concepts related to a non-scalable to scalable video (NSV2SV) converter, which is further described below in the detailed description. This summary is not intended to identify essential features of the claimed subject matter, nor is it intended for use in determining the scope of the claimed subject matter.","In one embodiment, a system including a broadcasting system, a NSV2SV converter, and a receiving device can be used. The NSV2SV converter produces an output video signal in a scalable format, hereinafter referred to as scalable-video signal, from a received input video signal in a non-scalable format, hereinafter referred to as non-scalable video signal. The scalable video signal may be in the form of a multi-layer bit stream in which each layer corresponds to a different resolution. The non-scalable input video signal, typically, has a single bit-stream coded in a standard that supports scalability, such as H.264\/AVC standard, MPEG-2.","In one implementation, the broadcasting system can transmit the non-scalable video signal to an intermediate device. The intermediate device includes the NSV2SV converter, which is configured to convert the non-scalable video signal into the scalable video signal conforming to a scalable video standard, for example, the Scalable Video Coding (SVC) standard. The receiving device receives the scalable output video signal from the intermediate device. Further, the receiving device can extract and use the layers that correspond to a resolution supported by the receiving device, from the multiple layered bit stream included in the scalable output video signal. In another implementation, the receiving device may receive only those layers that correspond to the supported resolution, from the intermediate device.","In particular, the NSV2SV converter includes a decoder that decodes the non-scalable input video signal. Spatial data and motion data are segmented from the decoded video signal. A spatial down-converter module down-samples and resizes the spatial data to the different resolutions. The resized spatial data and the segmented motion data are then processed by an encoder. To generate the multi-layer bit stream, the encoder includes multiple encoding layers, each of which generates a bit-stream layer corresponding to a distinct resolution.","In the encoding layers, the motion data is resized and refined to correspond to the different resolutions. For this, the encoding layers include a motion\/texture information adaptation module (M\/TIAM), also referred to as an adaptation module, and a motion refinement module (MRM). The adaptation module adapts the motion data from the decoded signal to the corresponding resolution of that encoding layer. For this, the adaptation module reuses the original motion data included in the non-scalable input video signal and produces adapted motion data. The motion refinement module (MRM) refines the adapted motion data based on the down-sampled spatial data for improving the resolution quality in that encoding layer. In one embodiment, in the top most encoding layer, the MRM, refines the segmented motion data based on the down-sampled spatial data. In such an embodiment, the top most encoding layer may not include the adaptation module.","Further, in the encoding layers, the output of the MRM, including the refined spatial and motion data, is transformed and encoded in a transform and entropy encoding module (TEC). In one implementation, the refined spatial and motion data is transformed into discrete cosine transform (DCT) coefficient values. Subsequently, the DOT coefficient values are entropy encoded. Thus multiple encoded bit stream layers corresponding to different resolutions are generated by the encoding layers. Further, the output signals of the TEC modules from the different encoding layers are multiplexed to produce a scalable video signal having a multi-layer bit stream subsuming a distinct resolution in each layer.","The disclosed subject matter relates to a non-scalable to a scalable video (NSV2SV) converter. More particularly, the subject matter relates to techniques for trans-coding non-scalable video content coded in video compression standards which support scalability, such as MPEG-2, H.264\/AVC, and so on, into scalable video content in scalable video compression standards such as SVC. The disclosed techniques are based on fine-to-coarse-to-fine code conversion methods.","In an implementation, a system including a broadcasting system, a NSV2SV converter, and a receiving device can be used for generating and using a scalable-video signal from a broadcasted non-scalable video signal. The non-scalable video signal, typically, is a single bit-stream coded in a standard that supports scalability, such as 8.264\/AVC standard, MPEG-2 and so on. The scalable video signal is in the form of a multi-layer bit stream in which each layer corresponds to a different resolution. In one implementation, the broadcasting system can transmit the non-scalable video signal to an intermediate device. The intermediate device includes the NSV2SV converter, which is configured to convert the non-scalable input video signal into the scalable output video signal. The receiving device receives the scalable output video signal from the intermediate device.","The NSV2SV converter can be implemented in a variety of electronic or communication devices in which a non-scalable video signal possessing a high resolution can be adapted and transmitted for display according to the display capabilities of the targeted end devices. Devices that can implement the disclosed NSV2SV converter include, but are not limited to, set-top boxes, base transceiver system (BTS), computing devices, televisions, mobile phones, laptops, personal digital assistants (PDAs), and so on, which can be employed in a variety of applications such as streaming, conferencing, surveillance, etc.","The NSV2SV converter can be thus advantageously used for transmitting scalable video signals to a variety of end user devices, in a resolution that is supported by the end user devices. The NSV2SV converter also enables efficient decoding of the video content received over diverse networks as it provides an option of decoding only a part of a the plurality of signals of different resolutions included in the scalable video signal.","Additionally, as compared to alternative approaches of recreating the motion information from the non-scalable input video signal for producing the scalable output video signal, the NSV2SV converter reuses the original motion information included in the input video signal, thereby reducing the complexity and computational load on an encoder and maintaining higher coding efficiency. Further, the NSV2SV converter also provides for improved efficiency use of network bandwidth and system memory as the non-scalable video signal can be converted once into the scalable video signal and saved in memory. The scalable video signal can then be transmitted multiple times to different end user devices as per the resolution capabilities of the end user devices.","Exemplary Systems",{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 1","b":["100","100","102","104","106","104","104"]},"The network  may be a wireless or a wired network, or a combination thereof. The network  can be a collection of individual networks, interconnected with each other and functioning as a single large network (e.g., the Internet or an intranet). Examples of network  include, but are not limited to, Local Area Network (LAN), Wide Area Network (WAN), and so on.","The system further includes an intermediate device  communicating with the broadcasting station servers  via the network . The intermediate device  is connected to one or more end devices -, -, -, . . . -N (hereinafter collectively referred to as end devices ). The end devices  may be implemented as any of a variety of conventional computing devices, including, for example, a server, a desktop PC, a notebook or a portable computer, a workstation, a personal digital assistant (PDA), a mainframe computer, a mobile computing device, an Internet appliance, and so on.","In one implementation, the broadcasting station servers  can be configured to transmit video signals encoded in any of a variety of video coding standards such as H.264\/AVC, H.263, MPEG-2, and so on. The transmitted video signal can be a non-scalable video signal subsuming a single layer bit stream of a certain input resolution. The non-scalable video signal can be transmitted from the satellite  either directly or via the broadcasting station servers  to an intermediate device  such as a set-top box, a base station transceiver system (BTS), and so on.","The intermediate device  includes a non-scalable video to scalable video (NSV2SV) converter  for converting the non-scalable video signal into a scalable video signal. The output scalable video signal can be a signal that subsumes a multi-layer bit stream corresponding to a distinct resolution in each layer. The scalable video signal is then transmitted to one or more of the end devices  for further processing.","In one implementation, the intermediate device  can be equipped with an extractor that receives information regarding the resolution supported by a target end device, such as -. The extractor then extracts the layers corresponding to the supported resolution from the multi-layer bit stream and transmits the extracted layers to the target end device -. The extracted layers are then decoded and rendered at the target end device -. In another implementation, each of the end devices  includes an extractor that extracts the layers corresponding to the supported resolutions from the multi-layer bit stream. The extracted layers are then decoded and rendered at the end devices .",{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 2","b":["200","200","102","104","106"]},"The system  further includes end devices -, -, -, and - (collectively, devices ) communicating with the broadcasting station servers  via the network . The end devices  may be implemented as any of a variety of conventional computing devices, including, for example, a server, a desktop PC, a notebook or a portable computer, a workstation, a personal digital assistant (PDA), a mainframe computer, a mobile computing device, an Internet appliance, and so on.","In one implementation, the broadcasting station servers  can be configured to transmit video signals encoded in any of a variety of video coding standards such as H.264\/AVC, H.263, MPEG-2, and so on. The transmitted video signal can be a non-scalable video signal subsuming a single layer bit stream of a certain input resolution. The non-scalable video signal can be transmitted from the satellite  either directly or via the broadcasting station servers  to the end devices .","In another implementation, the Broadcasting servers can be integrated with the NSV2SV converter directly. The broadcaster can then, convert a non-scalable video signal subsuming a single layer bit stream of a certain input resolution to a scalable video signal that subsumes a multi-layer bit stream having distinct resolution in each layer. The scalable video can be transmitted to the end devices, from the broadcasting stations either directly to or via the satellite, through the network devices consisting of an bit stream extractor which would transmit the video signal at the resolution for the end device.","In one implementation, each of the end devices  can be equipped with an extractor that receives information regarding the resolution supported by the respective end device. The extractor then extracts the layers corresponding to the supported resolution from the multi-layer bit stream. The extracted layers are then decoded and rendered at the respective end device.",{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 3","b":["110","110","302","304","306","308","310","312","314","316","318","302","302","308"]},"The interface(s)  can include a variety of software interfaces, for example, application programming interface, hardware interfaces, for example cable connectors, or both. The interface(s)  facilitate receiving of the input non-scalable video signal and reliable transmission of the output scalable video signal","A decoder  decodes a received input signal to produce a decoded signal. The received input signal can be a non-scalable video signal coded in any video coding standard, such as H.264\/AVC, MPEG-2, and so on, that supports scalability. The decoded signal can be adapted in any intermediate format such as Common Intermediate Format (CIF), Quarter Common Intermediate Format (QCIF), and so on. The processors(s)  segment the decoded signal into spatial data and motion data, which are stored in the system memory .","The system memory  can include any computer-readable medium known in the art, including, for example, volatile memory (e.g., RAM) and\/or non-volatile memory (e.g., flash, etc.). In one implementation, the system memory  includes a spatial data module  and a motion data module . The spatial data module  stores the decoded spatial data as pixel data information. The pixel data can be associated with attributes such as, for example, picture data, picture width, picture height, and so on, in a picture frame of the video sequence. The motion data module  stores the segmented motion data describing motion attributes, such as, for example, frame rate, picture type, end of stream flag, sequence frame number, motion vectors, Intra prediction mode, the location of different components in a picture frame such as pixels, blocks, macroblocks (MBs) and so on, and other related attributes such as MB modes, MB type, MB motion type, etc.","In operation, on receipt of a non-scalable input video signal, the decoder  decodes the input signal, which is then segmented into the spatial data and the motion data. The spatial data and the motion data are stored in the spatial data module  and the motion data module  respectively.","The spatial down-converter module  receives the spatial data from the spatial data module  and down-samples the spatial data for resizing the spatial data to conform to the different resolutions to be provided in the scalable output video signal. The down-sampling operation can be performed using a variety of techniques well known in the art. For example, the spatial data can be down-sampled by using various image compression filters such as polyphase filters, wavelet filters, and so on. The down-sampled spatial data and the segmented motion data are then fed to the encoder  for further processing.","The encoder  processes the decoded signal including the resized spatial data and the segmented motion data to produce a scalable video signal conforming to a video coding standard that is adaptable to the end devices , . Towards this end, the encoder  includes one or more encoding layers for encoding the previously decoded signal successively into multiple encoded signals such that each encoded signal corresponds to a distinct resolution.","The multiple encoded signals include an encoded base signal that includes the video signal in the basic or most coarse resolution form. The successive encoded signals include information for enhancing or fine tuning the coarse resolution progressively. Thus each encoded signal, when used in combination with encoded signals from previous encoding layers, provides a video signal of the corresponding distinct resolution.","The encoded signals from the multiple encoding layers of the encoder  are fed to the multiplexer . The multiplexer  multiplexes the encoded signals into a single encoded output video signal. The encoded output video signal exhibits spatial scalability due to the presence of multiple layers corresponding to distinct resolutions.",{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 4","b":"110"},"The NSV2SV converter  is capable of converting a non-scalable input video signal , hereinafter referred to as input signal , to a scalable output video signal , hereinafter referred to as output signal . In an implementation, the input signal  can be coded in H.264\/AVC standard. In such a case, the input signal  is a single layer bit stream of a certain input resolution. The input signal  can be trans-coded into a scalable video coding (SVC) standard output signal  by the NSV2SV converter . In such a case, the output signal  subsumes a multi-layer bit stream corresponding to a distinct resolution in each layer and conforms to the SVC standard for video coding.","The scalable video coding (SVC) standard for video coding can be advantageously used for coding the output signal as it provides bit stream scalability for video signals. The SVC standard enables encoding of a high-quality input video signal into multiple sub-streams representing a lower spatial or temporal resolution or a lower quality video signal (each separately or in combination) as compared to the original bit stream. SVC also supports functionalities such as bit rate, format, and power adaptation. SVC further provides graceful degradation in lossy transmission environments as well as lossless rewriting of quality-scalable SVC bit streams to single-layer H.264\/AVC bit streams. Additionally, SVC has achieved significant improvements in coding efficiency with an increased degree of supported scalability relative to the scalable profiles of prior video coding standards.","In one implementation, the NSV2SV converter  transforms the input signal  into the spatially scalable output signal  by converting the input signal  into the SVC standard format. For the above mentioned purpose, in an embodiment, the NSV2SV converter  includes the decoder , a segmentation section , the encoder  and the multiplexer .","In an implementation, the input signal  including a single layer bit stream, which is coded in, for example, H.264\/AVC standard, is received by the decoder . At block , variable-length decoding (VLD) is performed on the received input signal  using techniques well known in the art. The variable-bit length decoding provides pixel and motion information based on the length of each Huffman code used to encode the input signal . The variable length decoded signal is then sent to block  for further processing for retrieval of spatial data and to the segmentation section  for storing and re-using motion data.","At block , the decoded signal is processed by application of inverse quantization (IQ) to determine the class and quantization number of the quantized discrete cosine transform (DCT) coefficient values included in the decoded signal. The magnitude of DOT coefficient values represent spatial frequencies corresponding to the average pixel brightness in the images of the input signal . The reciprocated quantized DCT coefficient values are then inversed at an inverse DCT (IDCT) block  to produce the Pixel values. The output of the MOT block  is capable of being used for reconstructing a video sequence from the DCT coefficients values.","Subsequently, the output of IDCT stage is added to the output of a motion compensation block  in an adder  to produce a decoded video sequence signal. Motion compensation is a technique of describing a picture in terms of transformation of a reference picture, located in a picture buffer , to the current picture. The output of the motion compensation block  provides differences between the reference picture and the current picture and is used for predicting picture frames for display. A variety of motion compensation techniques such as block motion compensation, overlapped motion compensation, variable block-size motion compensation, and so on can be employed to obtain the output and generate the decoded signal.","The decoded signal is then transmitted to the segmentation section . The segmentation section  includes a spatial data module , a motion data module , and a spatial down-converter module . In the segmentation section , the decoded signal is segmented to provide spatial data and motion data, which are stored in the spatial data module  and the motion data module  respectively. The spatial data module  stores the decoded spatial data as pixel data information. The pixel data can be associated with attributes such as, for example, picture data, picture width, picture height, and so on, in a picture frame of the video sequence. The motion data describes the location of different components such as pixels, blocks, macroblocks (MBs) and so on, of a picture frame and other related attributes such as MB modes, MB motion type and so on.","Subsequently, the spatial data is fed to the spatial down-converter module . The spatial down-converter module  down-samples the spatial data using various image compression filters such as polyphase filters, wavelet filters, and so on. Down-sampling of the spatial data reduces the data rate and\/or the size of data. The reduction in size of spatial data is achieved based on the desired resolutions such as Common Intermediate Format (CIF) resolution, Quarter CIF (QCIF) resolution, and so on, in the scalable video output signal. The resized spatial data and the segmented motion data are then forwarded to the encoder .","The encoder  subsumes a multitude of encoding layers with each layer corresponding to a distinct resolution as explained earlier with reference to . The following description of the working of the encoder  has been provided with reference to 3 encoding layers. However, it will be understood that the encoder  can include any number of encoding layers and will work in a similar manner as that described herein.","In one implementation, when the encoder  includes three encoding layers, the encoder  includes two motion\/texture info adaptation modules (M\/TIAMs) - and -, and three motion refinement modules (MRMs) -, -, and -. The encoder  also includes three transform and entropy encoding modules (TECMs) -, -, and -, and two inter-layer prediction modules (ILPMs) - and -. These modules can be distributed over the three encoding layers, namely a base layer , an enhancement layer I , and an enhancement layer II .","In one embodiment, the base layer  includes the M\/TIAM -, the MRM -, and the TECM -. The enhancement layer I  includes M\/TIAM -, MRM -, TECM -, and ILPM -. The enhancement layer II  includes MRM -, TECM -, and ILPM -. The motion data and the down-sampled spatial data received from the segmentation section  are fed to all the layers of the encoder .","Base Layer","In an implementation, the base layer  receives the spatial data from the spatial down-converter module, which has been resized to the desired QCIF resolution, for example, 176\u00d7144 by the spatial down-converter module . The base layer also receives the segmented motion data for calculating motion information at the M\/TIAM -. The motion data, which includes original motion information from the H.264\/AVC coded input video signal, is adapted so as to produce a resultant QCIF output. The M\/TIAM - calculates the motion information by reusing the original motion information subsumed in the original H.264\/AVC coded video signal. As the original motion information is re-used and adapted to generate the motion information, the computational complexity involved is reduced by a large extent as compared to generating the motion information from a completely decoded signal. The calculated and adapted motion information is then used for motion estimation.","The technique of motion estimation helps in finding the best match between the pixels in a current video frame, hereinafter referred to as current frame, and the pixels in a reference video frame, hereinafter referred to as reference frame. The current frame corresponds to the complete picture that is in the process of construction, for a video sequence. The reference frame corresponds to an already constructed complete picture in a video sequence used to describe the current frame.","In the technique of motion estimation, a search area within the reference frame is traversed to find a best match for the component in the current frame. For this, the size of search area and the evaluation metrics used for determining the best match are the most crucial factors. The size of the search area corresponds to the applied computational load and the evaluation metrics corresponds to the degree of coding efficiency. Different types of motion estimation techniques such as block matching, pixel recursive technique, and so on are used. The motion estimation techniques use a variety of evaluation metrics such as Sum of Absolute Differences (SAD), Mean Absolute Differences (MAD), Mean Square Error (MSE), etc.","In an implementation, a search is performed for a macroblock in the reference frame corresponding to a macroblock in the current frame. The best match is found by comparing the macroblock in the current frame with the macroblocks in the search area in the reference frame. In one implementation, the macroblocks are compared by using difference in the corresponding pixel values. This provides difference values called as SAD values. The minimum of the SAD values corresponds to the closest linked pixel value in the reference frame for the current frame. In other words, a variation in the SAD value can be referred to as the cost associated with the MB. The MB in the reference frame having the minimum cost corresponds to the best match or the best MB available for coding the MB in the current frame. After finding the best match, the difference values between the corresponding pixels are coded together with the difference between the corresponding pixel locations. The location of a pixel can be defined by a motion vector.","For the purpose of understanding, consider a block made up of 8\u00d78 pixels. Four blocks can be combined together to form a single macroblock of 16\u00d716 pixels i.e.\n\nOne Macroblock=Four Blocks\u2003\u2003(1)\n\n16\u00d716 pixels=4\u00d7(8\u00d78) pixels\u2003\u2003(2)\n","In an implementation, for producing the QCIF output, macroblock (MB) modes and motion vectors (MVs) are the prime attributes that need to be calculated. An MB mode refers to the degree of partition in the MB. For example, inter-mode 1 refers to a MB having one partition of 16\u00d716 pixels, inter-mode 2 refers to a MB having two partitions of 16\u00d78 pixels, inter-mode 3 refers to a MB having two partitions of 8\u00d716 pixels, and so on. Also, the macroblock can be coded using an inter mode or an intra mode. The motion vector refers to a two-dimension vector used for inter prediction that provides an offset from the coordinates in the current frame to the coordinates in a reference frame.","Conversion of Input Motion Data to 4QCIF","The MB modes and the MVs from the motion data module  together constitute the input motion data (D1), which is adapted to four times the intended QCIF resolution (4QCIF). The QCIF resolution corresponds to a resolution of 176\u00d7144. This is performed so that, in the next step, a direct mapping can take place for the intended QCIF resolution. This can be understood by referring to aforesaid equations (1) and (2) and to equations discussed below:\n\n16\u00d716 pixels to be reduced to 8\u00d78 pixels\u2003\u2003(3)\n\nTherefore, 4\u00d7(16\u00d716) pixels to be reduced to 4. (8\u00d78) pixels\u2003\u2003(4)\n\nOr, 4 Macroblocks to be reduced to 1 Macroblock\u2003\u2003(5)\n","The above equations show that the width and height ratio is exactly two between 4QCIF and QCIF. Also, since while mapping from D1 to 4QCIF the width and height ratios may not be exactly two, the principle of dominant MB mode can be used to map the MBs from D1 to 4QCIF. Further, based on the MB modes of the candidate MBs in D1, a single MB mode is calculated for the corresponding MB in 4QCIF. The candidate MBs are those MBs, which qualify for being mapped based on the width and height ratios, and overlap area.","The MB mode can be calculated by using either forward prediction method or intra prediction method known in the art. Further, if any of the candidate MB mode in D1 is intra mode, then the resultant MB mode in 4QCIF is calculated as intra mode, irrespective of the dominant MB mode. Further, based on the MVs of all the candidate MBs in D1, the single MV for 4QCIF can be derived using a variety of techniques already known in the art. In one implementation, a single MV for 4QCIF can be computed by calculating the median of the candidate MVs in D1.","Conversion of 4QCIF to QCIF","The MB modes and MVs from 4QCIF are mapped to the intended QCIF resolution, based on the principle of dominant mode. Depending upon the MB modes of a set of four candidate MBs in the 4QCIF resolution, a single MB mode is derived for the resultant QCIF MB. If any of the candidate MB modes is intra mode then the resultant MB is also coded using the intra mode, irrespective of the dominant MB mode among the four candidate MBs. Also, each MB in 4QCIF is checked by the encoder  for if it can be skipped. The MB can be skipped if the corresponding MV is zero or very close to zero. This situation can arise when the closet matching region is at the same or almost at the same location in the reference frame and the energy of a residual MB is low. The residual MB is formed by subtracting the reference MB (without motion compensation) from the current MB. The energy of the residual MB is approximated by SAD values. Therefore, if the cost of coding the MB as skip is lesser than that decided by the M\/TIAM -, then skip mode can be followed. This corresponds to the act of dealing away from the task of coding a current macroblock, thus reducing the computational load.","Subsequently, in an implementation, all MBs in the QCIF resolution are set in 8\u00d78 coding mode by using forward prediction method. The 8\u00d78 coding mode corresponds to four 8\u00d78 block partitions for each MB. However, if the MBs in the QCIF resolution are intra coded, then the QCIF MBs are not set in 8\u00d78 coding mode. An MV of each 8\u00d78 block of each MB in the QCIF resolution is set equal to an MV of the corresponding MB in the 4QCIF resolution by scaling it accordingly. The calculated motion information including MBs and MVs for the intended QCIF resolution and other related information thereto, is then sent to MRM - for further refinement of the QCIF resolution.","The MRM - receives the motion information from the M\/TIAM - and the resized spatial data from the spatial down-converter module . In the MRM -, all the calculated MVs of the intended QCIF resolution from M\/TIAM - undergo a refinement process based on the received resized spatial data. For the purpose, slight approximations are further added to the motion information, such as the calculated MVs, for finding the best possible mode for the MB located in the current frame. This is performed so that the MB located in the current frame is closest to the mode of the MB located in the reference frame. The approximations are found out by using various techniques already known in the art. The process of refinement is carried out at the most granular level corresponding to the refinement of motion information at quarter of a pixel (QPel) level. The refined motion data and spatial data (RMS) are then forwarded to TECM -.","In TECM -, in an implementation, RMS subsumed in each block of the QCIF macroblock, which forms the QCIF resolution, is transformed into low frequency DCT coefficient values using a variety of transformation techniques already known in the art. RMS is then entropy coded using a number of entropy coding techniques such as context-based adaptive variable-length coding (CAVLC), context-based adaptive binary arithmetic coding (CABAC), and so on. Therefore, a desired encoded QCIF resolution output is received as the output of TEC -, which is sent to a multiplexer .","Enhancement Layer","For obtaining an output of another resolution, for example, CIF resolution, as part of the final scalable video signal, the enhancement layer I  is used. In an implementation, the spatial data can be resited to the desired CIF resolution, i.e., 352\u00d7288 by the spatial down-converter module  before feeding it to the enhancement layer I . On the other hand, the motion information from the decoded signal, which includes the original motion information subsumed in the H.264\/AVC coded input video signal, is adapted so as to produce a resultant CIF output. Correspondingly, the motion information for the resultant CIF output is calculated by reusing the original motion information subsumed in the original H.264\/AVC coded video signal. The calculation of motion information is performed for motion estimation as described below.","The enhancement layer  includes ILPM -, M\/TIAM -, MRM -, and TECM -. The ILPM - provides an exploitation of the statistical dependencies between different layers for improving the coding efficiency of the enhancement layer. The improvement in coding efficiency can be referred to in terms of a reduction in bit rate. The ILPM - uses various inter-layer prediction methods employing the reconstructed samples of QCIF MB from the base layer  to produce an inter-layer prediction signal. The inter-layer prediction signal can be formed by using different methods, such as motion compensated prediction inside the enhancement layer, by up-sampling the reconstructed base layer signal, and so on. These methods are dependent on various parameters, for example, prediction of MB modes and associated motion vectors. Correspondingly, the encoder  checks for each MB at D1 resolution for it can be inter-layer predicted from the base layer . Therefore, the encoder  checks for intra, motion and residual inter-layer predictions while coding each MB.","The inter-layer prediction signal from the ILPM - and the D1 stored in the motion data module are fed to the M\/TIAM - for performing the process of motion estimation. In the M\/TIAM -, the received D1 is adapted to have the output of the desired resolution. In one implementation, the received D1 is adapted to the resultant CIF output.","For producing the CIF output, the MB modes and the MVs are the prime attributes to be calculated. The MB modes refer to the degree of partition in the macroblocks. For example, mode 1 refers to an MB having one partition of 16\u00d716 pixels, mode 2 refers to an MB having two partitions of 16\u00d78 pixels, mode 3 refers to an MB having two partitions of 8\u00d716 pixels, and so on. Also, the macroblock can be coded using inter mode or intra mode.","As part of the calculation, MB modes and MVs from D1 are adapted to the CIF output by using the principle of dominant mode. Since the overlap area may not be an integral multiple of the number of MBs, the candidate MBs from D1 are selected based on width and height ratios and overlap area. Further, based on the MB modes of the candidate MBs in D1, a single MB mode is calculated for a corresponding MB in CIF. The candidate MBs are those MBs that qualify for being mapped based on width and height ratios and the overlap area. A single MB mode can be calculated either by forward prediction or intra prediction. However, if any of the candidate MB modes is intra mode, then the resultant single MB mode is calculated as intra mode, irrespective of the dominant MB mode.","Also, each MB is checked by the encoder  if it can be skipped. The MB can be skipped if the corresponding MV is zero or very close to zero. This situation can arise when the closet matching region is at the same or almost at the same position in the reference frame and the energy of a residual MB is low. The residual MB is formed by subtracting the reference MB (without motion compensation) from the current MB. The energy of the residual MB is approximated by SAD values. Therefore, if the cost of coding the MB as skip is lesser than that decided by the M\/TIAM -, then skip mode can be followed. This corresponds to the act of dealing away from the task of coding the current macroblock, and thus reducing computational load.","The MB mode corresponds to the degree of partitioning appearing in a macroblock, such as partitions of 16\u00d716 or 16\u00d78 or 8\u00d716 or 8\u00d78. The MB mode for the CIF MB is decided using the principle of dominant MB mode partition. Depending upon the dominant partition mode among the candidate MBs of D1, the final CIF MB mode partition is decided. Further, an MV for the MB in the image of CIF resolution is decided based on MB mode partition and candidate MVs in D1 image. The candidate MVs are MVs corresponding to the candidate MBs. It is to be noted that four MBs of D1 are combined to form one MB of CIF image.","When the CIF MB mode is calculated as 16\u00d716, the final MV of the CIF MB is calculated as the median of candidates in a set of four candidate MVs in D1. Further, if any of the candidate MB in D1 is not 16\u00d716, the MV of the top left sub-partition is considered to be the MV of the complete MB.","When the CIF MB mode is calculated as 16\u00d78, the MV for the top 16\u00d78 partition is calculated as the median of the MVs of top two 16\u00d716 MBs in D1, while the MV of the bottom 16\u00d78 partition is the median of the MVs of bottom two 16\u00d716 MBs in D1. Further, if any of the candidate MBs in D1 is not 16\u00d716, the MV of top left sub-partition is considered to be the MV of the complete MB.","When the CIF MB mode is calculated as 8\u00d716, the final MV of the left 8\u00d716 partition is calculated as the median of left two 16\u00d716 MBs in D1, and the MV of the right 8\u00d716 partition is the median of the right two 16\u00d716 MBs in D1. Further, if any of the candidate MBs in D1 is not 16\u00d716, the MV of the top left-partition is considered to be the MV of the complete MB.","When the CIF MB mode is calculated as 8\u00d78, the MV for each 8\u00d78 MB partition at CIF resolution is equaled to MV of the corresponding 16\u00d716 MB in D1. The calculated motion information including MBs and MVs for the CIF resolution and other related information thereto, are then sent to MRM - for further refinement of the CIF resolution.","MRM - receives the motion information from the M\/TIAM - and the resized spatial data from the spatial down-converter module . In the MRM -, the calculated MVs of the CIF resolution from M\/TIAM - undergo a refinement process based on the received resized spatial data. For the purpose, slight approximations are further added to the motion information, including the calculated MVs, for finding the best possible mode for the MB located in the current frame. This is performed so that MB located in the current picture is closest to the mode of the MB located in the reference frame. The approximations are found out by using various techniques already known in the art. The process of refinement is carried out at the most granular level corresponding to the refinement of motion information at quarter of a pixel (QPel) level. The refined motion data and spatial data (RMS) are then forwarded to TECM -.","In TECM -, in an implementation, RMS subsumed in each block of the constructed macroblock, which forms the CIF resolution, is transformed into low frequency DOT coefficient values using a variety of transformation techniques already known in the art. The refined motion data and spatial data are then entropy coded using a number of entropy coding techniques such as context-based adaptive variable-length coding (OAVLC), context-based adaptive binary arithmetic coding (CABAL), and so on. Therefore, a desired encoded CIF resolution output is received as the output of TECM - that is sent to the multiplexer .","Enhancement Layer II","For obtaining an output that is having inherently the same resolution as that of the original input signal, which forms a part of the final scalable video signal, the enhancement layer II  is used. The spatial and motion information included in the original H.264\/AVC coded video signal is directly reused, thereby removing the requirement of M\/TIAM. The input spatial data from the spatial data module  is fed directly to the enhancement layer II  of the encoder  without resiting. In an encoder with more than 3 layers, the top-most enhancement layer will be configured in a manner similar to that of enhancement layer II , while the intermediate enhancement layers will be configured in a manner similar to that of enhancement layer I .","The enhancement layer II  includes ILPM -, MRM -, an TECM -. The ILPM - provides an exploitation of the statistical dependencies between different layers for improving the coding efficiency of the enhancement layer. The improvement in coding efficiency can refer to a reduction in bit rate. The ILPM - uses various inter-layer prediction methods employing the reconstructed samples from enhancement layer I  to produce an inter-layer prediction signal. The inter-layer prediction signal can be formed by using different methods such as motion compensated prediction inside the enhancement layer, by up-sampling the reconstructed enhancement layer I  signal, and so on. These methods are dependent on various parameters, for example, prediction of MB modes and associated motion vectors. Correspondingly, the encoder  checks for each MB at CIF resolution for it can be inter-layer predicted from the enhancement layer I . Therefore, the encoder checks for intra, motion, and residual inter-layer predictions while coding each MB. The inter-layer prediction signal from the ILPM - and the original spatial information stored in the spatial data module  are fed directly to the MRM -.","The MB information, such as MB mode, MB partition, and MVs, included in the motion information are directly replicated as the input signal. Also, each MB is checked by the encoder  if it can be skipped. The MB can be skipped if the corresponding MV is zero or very close to zero. This situation can arise when the closet matching region is at the same or almost at the same position in the reference frame and the energy of a residual MB is low. The energy of the residual MB formed by subtracting the reference MB (without motion compensation) from the current MB is approximated by SAD values. Therefore, if the cost of coding the MB as skip is less, then skip mode can be followed. This corresponds to the act of dealing away from the task of coding the current macroblock and reduces the computational load.","In MRM -, only MVs are refined based on the received spatial data. For the purpose, slight approximations are further added to the motion information, which includes the calculated MVs, for finding the best possible mode for the MB located in the current frame. This is performed so that an MB located in the current frame is closest to the mode of the MB located in the reference frame. The approximations are found out by using various techniques already known in the art. The process of refinement is carried out at the most granular level corresponding to the refinement of motion information at quarter of a pixel (QPel) level. The refined motion data and spatial data (RMS) are then forwarded to TECM -.","In TECM -, in an implementation, RMS subsumed in each block of the constructed macroblock, which is of the resolution equivalent to the original resolution, is transformed into low frequency DOT coefficient values using a variety of transformation techniques already known in the art. RMS is then entropy coded using a number of entropy coding techniques such as context-based adaptive variable-length coding (CAVLC), context-based adaptive binary arithmetic coding (CABAC), and so on. Therefore, the output of resolution equivalent to the original resolution is received as the output of TECM -, which is then sent to the multiplexer .","In the multiplexer , the SVC outputs of distinct resolutions from each layer, namely base layer , enhancement layer I , and enhancement layer II  are multiplexed. The multiplexing of SVC outputs from different layers refers to the act of combing the received multiple SVC outputs into a single signal. This single signal is the scalable output video signal  having multiple signals of different resolutions. Also, the multiplexer  can be of different configurations depending upon the number of signals to be multiplexed.",{"@attributes":{"id":"p-0088","num":"0087"},"figref":"FIG. 5","b":["500","110"]},"The order in which the method is described is not intended to be construed as a limitation, and any number of the described method blocks can be combined in any order to implement the method, or an alternate method. Additionally, individual blocks may be deleted from the method without departing from the spirit and scope of the subject matter described herein. Furthermore, the method can be implemented in any suitable hardware, software, firmware, or combination thereof.","At block , an input signal is received and decoded. In one implementation, the input signal can be a non-scalable video signal  subsuming a single layer bit stream of a certain resolution. For example, the non-scalable video signal  can be a video signal coded in any video coding standard such as H.264\/AVC, MPEG-2, and so on that supports scalability. The non-scalable video signal  is received by the NS2SV converter  from the broadcasting station servers  via the network . The received video signal is then decoded by the decoder .","In one implementation, the decoder  converts the received video signal into a signal that can be adapted in any intermediate format such as Common Intermediate Format (CIF), Quarter Common Intermediate Format (QCIF), and so on. The received video signal undergoes variable-length decoding (VLD)  using techniques known in the art. The decoded signal is subjected to inverse quantization  determining the class and quantization number of the quantized discrete cosine transform (DOT) coefficient values included in the decoded signal.","Further, the quantized DOT coefficient values are reciprocated at the inverse DCT (IDCT)  to produce the IDCT coefficient values. The output of the IDCT is added with output of the motion compensation stage  to produce a smooth video sequence. The motion compensation can be performed by a variety of techniques such as block motion compensation, overlapped motion compensation, variable block-size motion compensation and so on.","At block , a decoded signal is subjected to the segmentation section . In one implementation, the processors  segment the decoded signal using segmentation techniques well known in the art. The decoded signal is segmented into the spatial data, which is stored in spatial data module  and the motion data, which is stored in motion data module . The spatial data module  and the motion data module  can be stored in the system memory . The spatial data module  stores the decoded spatial data as pixel data information. The pixel data can be associated with attributes such as, for example, picture data, picture width, picture height, and so on, in a picture frame of the video sequence. The motion data describes the location of different components in a picture frame such as pixels, blocks, macroblocks (MBs) and so on, and other related attributes such as MB modes, MB motion type and so on.","At block , spatial data is sampled to a desired resolution. In one implementation, the spatial data is down sampled by a spatial down-converter module  to adjust the corresponding data size. The down sampling of the spatial data enables resizing of the spatial data to conform to the desired resolution depending upon the targeted end devices . The down sampling of the spatial data reduces the data rate or the size of the data. The reduction in size of the spatial data is achieved based on the desired resolution such as Common Intermediate Format (CIF) resolution, Quarter CIF (QCIF) resolution, and so on. The down-sampling operation can be performed using a variety of techniques well known in the art. For example, the spatial data can be sampled by using various image compression filters such as polyphase filters, wavelet filters, and so on.","At block , the sampled spatial data and original motion data is submitted to an encoder, such as the encoder . In one implementation, the down-sampled spatial data and the original motion data is submitted to different layers of the encoder . The original motion data is obtained from the VLD  and fed to the encoder . The encoder  includes a multitude of layers each having a distinct resolution. In one implementation, the encoder can include a base layer , an enhancement layer I , an enhancement layer II . For example, the base layer  has 640\u00d7480 resolutions, whereas the enhancement layer I  has 1280>720 resolutions, and so on. The base layer  includes M\/TIAM -, MRM - and TECM -, the enhancement layer  includes M\/TIAM -, MRM -, TECM - and ILPM -, and the enhancement layer II  includes MRM -, TECM - and ILPM -.","At block , motion data is adapted to a desired resolution. In one implementation, the motion data received from the segmentation section  is adapted to the desired resolution of each layer, except the top-most enhancement layer. The motion data is fed into the Motion\/Texture Info Adaptation Module (M\/TIAM)  of each layer for calculating the motion information such as motion vector (MV) for the resultant decoded signal. The motion information calculated in the previous layer can be used by the next layer. For example, the motion information calculated by the M\/TIAM - of the base layer  can be used by the ILPM - of the enhancement layer I . Further, the motion information calculated by the M\/TIAM - of the enhancement layer I  can be used by the ILPM - of the enhancement layer II , and so on. However, the M\/TIAM  is not present for the top-most enhancement layer since the information from the input is being directly reused. The top-most enhancement layer is adapted at the same resolution of the input signal.","At block , the encoder performs Inter-Layer Prediction for the current layer from the layer of lower resolution to provide further compression. At block , the adapted motion data are refined based on the down sampled spatial data. In one implementation, the calculated motion information from the M\/TIAM  undergoes a refinement process at the Motion Refinement Module (MRM)  based on the down sampled spatial data to improve the quality of the resulting motion information. The MRM  performs slight approximations on the calculated motion information including the calculated MVs in all the layers for finding the best possible mode for the MB located in the current picture frame. The approximations can be performed by using various techniques already known in the art.","At block , the refined motion data and spatial data are transformed into discrete cosine transform (DCT) coefficient values. In one implementation, the refined motion information and the spatial data undergo transformation, for example, discrete cosine transformation. The transformation results in discrete cosine transform (DCT) coefficient values of the refined motion data and spatial data.","At block , the DOT coefficient values are entropy-encoded. In one implementation, in each layer, the DOT coefficient values obtained in the block  are compressed using compression techniques known in the art. In one implementation, an entropy-encoding technique is applied on the DCT coefficient values and an encoded video signal is obtained.","At block , the transformed and entropy-encoded data is multiplexed. In one implementation, the transformed and encoded data obtained in each layer are provided as input to the multiplexer . The multiplexer  combines the transformed and encoded data from each layer into a single data corresponding to a single video signal.","At block , a scalable output video signal is produced. In one implementation, the output of the multiplexer  can be a single video signal that exhibits a scalable video signal . The scalable video signal  can be a multi-layer bit stream subsuming a distinct resolution in each layer. The scalable video signal  is encoded in the SVC standard.","Although embodiments for a NSV2SV converter have been described in language specific to structural features and\/or methods, it is to be understood that the appended claims are not necessarily limited to the specific features or methods described. Rather, the specific features and methods are disclosed as exemplary implementations for the programmable compensation network."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The detailed description is described with reference to the accompanying figures. In the figures, the left-most digit(s) of a reference number identifies the figure in which the reference number first appears. The same numbers are used throughout the drawings to reference like features and components.",{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIGS. 5","i":["a ","b "],"b":"5"}]},"DETDESC":[{},{}]}
