---
title: Disaster recovery system with cascaded resynchronization
abstract: A disaster recovery system with sequenced cascaded resynchronization comprises a plurality of data centers and a distributed control system. The individual data centers comprise a communication interface, a data storage, and a controller. The distributed control system is distributed and executable in the controllers of the plurality of data centers, and is capable of coordinating operations via the communication interfaces of the plurality of data centers to resynchronize a plurality of communication links between data center pairs of the plurality of data centers. The communication links including at least one synchronous link and at least one asynchronous link.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07149919&OS=07149919&RS=07149919
owner: Hewlett-Packard Development Company, L.P.
number: 07149919
owner_city: Houston
owner_country: US
publication_date: 20030515
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","SUMMARY","DETAILED DESCRIPTION"],"p":["Maintenance of multiple copies of data is part of the security function in data processing operations in case data is unavailable, damaged, or lost. Institutional users of data processing systems commonly maintain quantities of highly important information and expend large amounts of time and money to protect data against unavailability resulting from disaster or catastrophe. One class of techniques for maintaining redundant data copies is termed mirroring, in which data processing system users maintain copies of valuable information on-site on a removable storage media or in a secondary mirrored storage site positioned locally or remotely. Remote mirroring off-site but within a metropolitan distance, for example up to about 200 kilometers, protects against local disasters including fire, power outages, or theft. Remote mirroring over geographic distances of hundreds of kilometers is useful for protecting against catastrophes such as earthquakes, tornados, hurricanes, floods, and the like. Many data processing systems employ multiple levels of redundancy to protect data, positioned at multiple geographic distances.","Data processing systems utilize remote copies using synchronous mirroring or asynchronous mirroring. Synchronous remote copies have advantages when response time is relatively unimportant, distances between copy storage are short, and data cannot be lost. Synchronous mirroring is generally selected as a best choice for rapid recovery. In contrast, asynchronous mirroring is used when the smallest possible performance impact is to be imposed on the primary site, speed is important, and mirrored data is to travel long distances. Asynchronous mirroring is often used for very large geographical scale operations.","Data processing systems that store multiple mirrored copies spread over a range of distances may use synchronous links for some sites, generally relatively nearby sites, within metropolitan distances, and asynchronous links for other sites. One risk of systems that combine synchronous and asynchronous mirroring is the possibility of corrupted or unusable data when suspended links are not restored in a proper order.","According to some embodiments, a disaster recovery system with sequenced cascaded resynchronization comprises a plurality of data centers and a distributed control system. The individual data centers comprise a communication interface, a data storage, and a controller. The distributed control system is distributed and executable in the controllers of the plurality of data centers, and is capable of coordinating operations via the communication interfaces of the plurality of data centers to resynchronize a plurality of communication links between data center pairs of the plurality of data centers. The communication links including at least one synchronous link and at least one asynchronous link.","According to other embodiments, a disaster recovery system with sequenced cascaded resynchronization comprises at least three data centers in a configuration including a first data center, a second data center linked to the first data center by a communication link, and a third data center. The third data center is linked to the first data center and the second data center by communication links. Information is multicast at least from the first data center to the second and third data centers. The disaster recovery system also includes a distributed control system distributed and is executable in at least three data centers. The distributed control system is configured to respond to failure of the link between the first and second data centers by transferring a bitmap representing a differential copy of information from the first data center to the second data center via the third data center.","In accordance with further embodiments, a method is practiced for responding to failure of a link in a daisy-chained multiple-site disaster recovery configuration including a first data center, a second data center synchronously linked to the first data center, and a third data center asynchronously linked to the second data center, the third data center having an internal copy linkage. The method comprises distributing control operations among the first, second, and third data centers and coordinating control among the first, second, and third data centers. The method further comprises suspending the asynchronous linkage between the second and third data centers, disconnecting the internal copy linkage of the third data center when the asynchronous linkage is suspended, resynchronizing the synchronous linkage between the first and second data centers following disconnection of the internal copy linkage, and resynchronizing the asynchronous linkage between the second and third data centers following resynchronization of the synchronous linkage. The method further comprises reconnecting the internal copy linkage when the synchronous and asynchronous linkages are resynchronized.","According to additional embodiments, a method of sequenced cascaded resynchronization comprises communicating data and control information among at least three data centers, multicasting from a first data center to second and third data centers, and distributing control operations among the first, second, and third data centers. The method further comprises responding to failure of the link between the first and second data centers by transferring a differential copy of information from the second data center to the third data center, based on a differential bitmap copied from the first data center to the second data center via the third data center. The bitmap contains information relating to the unsent Logical Unit (LUN) tracks in an in-order side file previously used for asynchronous communications between the first and third data centers.","According to further additional embodiments, a disaster recovery system with sequenced cascaded resynchronization comprises at least four disk arrays in a configuration. The first disk array comprises remote mirror primary volume storage. The second disk array comprises a remote mirror secondary volume and local mirror primary volume storage linked to the remote mirror primary volume storage by a first communication link. The second disk array also comprises a local mirror secondary volume and remote mirror primary volume storage internally mirror linked to a remote mirror secondary volume and local mirror primary volume storage by a second communication link. The third disk array comprises a remote mirror secondary volume storage linked to the second disk array local mirror secondary volume and remote mirror primary volume storage by a third communication link. The fourth disk array is linked to the remote mirror secondary volume storage by a fourth communication link. The disaster recovery system further comprises a distributed control system distributed and executable in the at least four disk arrays. The distributed control system controls the communication links using Meta commands.","Referring to , a schematic block diagram illustrates an embodiment of a disaster recovery system  capable of implementing a sequenced cascaded resynchronization operation. The disaster recovery system  comprises a plurality of data centers , , and , and a distributed control system . The individual data centers , , and  comprise a communication interface A, A, and A, data storage B, B, and B, and a controller C, C, and C. The distributed control system  is distributed and executable in the controllers C, C, and C of the plurality of data centers , , and  and coordinates operations via the communication interfaces A, A, and A to resynchronize communication links, for example links L and L, between pairs of the data centers , , and . In the illustrative embodiment, the first communication link L that connects between the first data center  and the second data center  is a synchronous link and the second communication link L connecting between the second data center  and the third data center  is an asynchronous link.","Although any implementations are possible, the disaster recovery system  is commonly used in a configuration that the plurality of data centers includes data centers that are geographically removed. In the illustrative example, the synchronous link L extends a metropolitan distance of tens to hundreds of kilometers. Metropolitan distances from about 10 kilometers to 100 or 200 kilometers are common. The asynchronous link L is shown in a long distance implementation with distances of more than hundreds of kilometers.","In the illustrative embodiment, the third data center  also includes a redundant, safety-net internal copy  with data storage B and communication interface A, and generally also includes a controller C. The third data center internal copy  can be a mirrored storage device. In an illustrative embodiment, the third data center  includes a copy controller process, typically executable on the controller C and\/or C that controls copying of data in the data storage B to the data storage B as a safety-net, redundant internal copy. In some embodiments, the copy controller functions as a data mirror to copy an exact replication of data from the data storage B to the data storage B.","In other embodiments and configurations, a safety-net copy may be implemented in a fourth data center that can be remote from the third data center .","The distributed control system  can be distributed and executable in the various controllers C, C, C, and C, or some or all parts of the distributed control system  can be distributed and executable in control elements outside the data centers , , and . A particular controller may not execute any part of the distributed control system , although distribution of executable processes throughout the data centers can facilitate efficient operation.","Any of the data centers can be highly scaleable so that an illustrative data center may actually include a group of data centers. Redundant data storage structures can easily and commonly be included within a data center.","The distributed control system  performs coordinated actions among the multiple data centers , , and , and within data centers  to manage the operations in a prescribed sequence, thereby preventing unusable or corrupted data. The distributed control processes of the distributed control system  can be disseminated over various communication paths including the synchronous L and asynchronous L paths and also via network communications, such as Local Area Network (LAN), Metropolitan Area Network (MAN), or Wide Area Network (WAN) communications. In some embodiments, executable code for the distributed control system  can be programmed into firmware such as disk array firmware. In other embodiments, the distributed control system  operations can be implemented from storage in memory, disk, tape, supplied by electronic signals over a network, including wireless dissemination, and the like.","The distributed control system  can be programmed using various techniques and technologies. For example, the distributed control system  can be implemented in the operating system of a host processor, for example using an Application Programming Interface (API). In other examples, the distributed control system  can be implemented using Meta commands, for example operating at a centralized location, or using a Command Line Interface (CLI). In some applications, a CLI implementation can be based within a Redundant Array of Independent Disk (RAID) manager.","In the illustrative embodiment, the synchronous disaster recovery link L feeds the asynchronous disaster recovery link L in a cascaded disaster recovery configuration. A control process executable in the distributed control system  can detect suspension of the synchronous disaster recovery link L and respond by suspending the asynchronous disaster recovery link L prior to resynchronizing the synchronous disaster recovery link L. The operation of suspending a communications link involves postponing or canceling all accesses, including reads and writes, to the remotely mirrored volume to which a write has been performed until a particular event has been acknowledged. The distributed control system  coordinates the response so that the asynchronous link L is placed in the suspended state before the upstream synchronous link L enters an out-of-order copy, or resynchronization, state so that data at the downstream end of the asynchronous link L is not corrupted or unusable for purposes of disaster recovery in the event that the first two data centers  and  are lost.","Conventional systems that use manual resynchronization of the synchronous link are error prone, a high risk since a sequence or pair direction error or disaster that destroys the first two data centers, while the asynchronous link is in an out-of-order state, can leave unusable or corrupt data at the only remaining data center. Results can be catastrophic, for example a multi-million dollar disaster recovery plan of a Fortune 100 corporation could be rendered useless.","In various embodiments, the distributed control system  can execute several operations. Referring to  with reference to , a control process  executable as part of the distributed control system  detects suspension of the synchronous disaster recovery link . The control process responds to link suspension by suspending the asynchronous disaster recovery link  prior to resynchronizing the synchronous disaster recovery link  so that data in the third data center is not rendered corrupt and\/or unusable for disaster recovery if data is lost in the first and second data centers.","Referring to  with reference to , a flow chart shows an embodiment of another control process  that can be executed as part of the distributed control system . The control process  detects suspension of the synchronous communication link , and responds by disconnecting  the copy data storage B from the third data center data storage B prior to resynchronizing the synchronous communication link  and the asynchronous communication link . The copy data storage B contains a redundant, safety-net copy of data internal to the third data center .","Referring to  with reference to , a schematic flow chart shows an example of a daisy-chained multiple-site disaster recovery process . In the daisy-chained configuration, a single command can be issued from a device, such as a host computer, such as a Resync-CascadeL1( ) command. The Resync-CascadeL1( ) command invokes the distributed control system  to perform a sequence of operations in a prescribed order. In some embodiments, suspension of the asynchronous link occurs in an atomic operation in which the distributed control system  controls a system that can suspend or break a communication link in an atomic action. In an atomic action, the reflection of write data to a secondary volume occurs at a specific point in time. After an atomic break, the secondary volume is a valid image of the primary volume. A mirror split or break commonly results from a specific break command, or by a link failure in some systems.","After a mirror split, the distributed control system  suspends  the inter-site asynchronous link L. After the asynchronous link L is suspended, the distributed control system  breaks off or disconnects  the third data center safety net internal copy B from the third data center primary storage B. The distributed control system  resynchronizes , via an out-of-order COPY state, the synchronous link L and then  the asynchronous link L. The distributed control system  returns to normal operation by reconnecting  the third data center safety-net internal copy B to the third data center .","In embodiments that an atomic split of aggregated internal copy LUNs is not available, the distributed control system  begins the response by suspending  the inter-site synchronous link L.","The distributed control system  automates the resynchronization process for the synchronous link L to avoid or eliminate user error, and also enables coordinated disconnection or split-off of the safety-net internal volume copy B at the third data center  prior to the upstream asynchronous link L going into an out-of-order resynchronized state.","Referring to , a schematic block diagram shows a disaster recovery system  that implements sequenced multi-cast resynchronization and comprises three or more data centers. The configuration includes a first data center , a second data center  linked to the first data center by a synchronous communication link L, and a third data center . The third data center  is linked to the first data center  and the second data center  by asynchronous communication links L and L, respectively. The first data center  multicasts data to the second  and third  data centers. The disaster recovery system  also includes a distributed control system  that is distributed and executable in the data centers. The distributed control system  responds to failure of the link L between the first  and second  data centers by transferring a bitmap representing a differential copy of information from the first data center  to the second data center  via the third data center .","In a particular embodiment, the link L between the first  and second  data centers is a synchronous link, and the links L and L between the third data center  and the first  and second  data centers, respectively, are asynchronous links. Also in some embodiments, the links L, L, and L are mirror links that transfer mirror image data between data centers.","The distributed control system  is distributed either internally or externally to the data centers and executes by coordinated operation to control communication links between the data centers and data stored in the data centers. The distributed control system  can be disseminated via the communication links or network communications. The distributed control system  can be implemented using various techniques such as Application Programming Interface (API), Meta command, or as a Command Line Interface (CLI).","The distributed control system  can include a control process for handling the condition, in a three-site disaster recovery multi-cast system, in which a link L, for example a metropolitan distance link, between the first data center  and the second data center  is lost. One technique for restoring data in the second data center  is to establish a connection L that can be a long distance communication link, from the third data center  to the second data center  and supplying a copy. In various embodiments, the copy can be a full copy or partial copy. Transfer of a partial copy improves efficiency of the recovery process since time duration of a full copy can be lengthy, during which no valid third disaster recovery site is available.","In some embodiments, the transfer from the first data center  to the third data center  can be transfer of an asynchronous sidefile bitmap and supplying the bitmap to the second data center  via the third data center , so that the mirror pair connection between the second  and third  data centers can be performed with a differential copy. Accordingly, upon failure of the link L, a data center 2-link L bitmap resulting from the asynchronous sidefile is transferred to the third data center  that forwards the bitmap to the second data center  via link L. The second data center  can use the bitmap to update the third data center  via an out-of-order COPY state without the overhead of transferring a full copy, thereby maintaining two current data center sites in a more timely manner.","Referring to , a schematic block diagram depicts an example of a disaster recovery system  including sequenced cascaded resynchronization. The illustrative disaster recovery system  comprises four or more disk arrays , , , and  in a configuration. The first disk array  comprises remote mirror primary volume storage . The second disk array  comprises a remote mirror secondary volume and local mirror primary volume storage  linked to the remote mirror primary volume storage  by a first communication link L. The second disk array  also comprises a local mirror secondary volume and remote mirror primary volume storage  internally mirror linked to the remote mirror secondary volume and local mirror primary volume storage  by a second communication link L that is internal to the second disk array . The third disk array  comprises a remote mirror secondary volume storage  linked to the second disk array local mirror secondary volume and remote mirror primary volume storage  by a third communication link L. The fourth disk array  is linked to the remote mirror secondary volume storage  by a fourth communication link L. The disaster recovery system  further comprises a distributed control system  distributed and executable in the disk arrays that coordinates timing of data mirroring to promote consistency of the mirrored copies.","In other embodiments and configurations, the disk arrays may be variously arranged with multiple arrays contained in a single data center and connected by internal links, or arranged separately in data centers that have some degree of geographical remoteness.","A typical difficulty that the disaster recovery system  avoids or alleviates is that the interior mirror link L can be either inconsistent while in a pair state or stale while in a suspend state if the data mirroring operations are not coordinated.","In some embodiments, the distributed control system  controls the communication links and coordinates data mirroring operations using Meta commands.","Referring to  in combination with , a flow chart illustrates an example of a method  that can be performed by the disaster recovery system  to coordinate data handling. For a system that supports atomic LUN group splitting or break breaking of mirrors, a control operation begins by suspending  the internal link L, followed in sequence by suspending  and  the external links L and L, respectively. The disaster recovery system  then, in sequence, resynchronizes the links , , and  in the order link L, link L, and link L.","Referring to  in combination with , a flow chart illustrates an example of a method  that can be performed by the disaster recovery system  to coordinate data handling. For a system that does not support atomic LUN group splitting or breaking of mirrors, a control operation begins by suspending  the internal link L, followed in sequence by suspending , , and  the external links L, L, and L, respectively. The disaster recovery system  then, in sequence, resynchronizes , , , and  the links in the order link L, link L, link L, and link L.","Referring to , a schematic block diagram illustrates sidefile usage in a configuration with a one-to-many relationship between primary and secondary volumes. In a particular example, the configuration can be implemented in a design using a Fibre Channel infrastructure. Asynchronous replication uses sequence order tracking of update occurrence to ensure consistency. Tracking takes place in two primary volume sidefiles  and , and two secondary volume sidefiles  and . An additional primary volume sidefile is added for each secondary site that is application for asynchronous replication.","Updates are ordered, for example by a host , with a sequence number and transmitted to the remote disk volumes  and . When a remote disk volumes ,  receives the next sequence number in a set, the remote disk volumes ,  acknowledges receipt of the data according to sequence number to the primary disk volume  and the affected sequence number is removed from a primary volume sidefile list  and . If a transaction is lost between the primary volume  and one of the secondary volumes , , then retransmission of a specific sequence number's data can be requested.","The one-to-many configuration can be used for various cascaded configurations.","Referring to , a schematic block diagram illustrates a synchronous data replication method. Any input\/output commands  issued to a primary storage array  from a host  are copied  to a secondary storage array . Once data is written in memory on the secondary array , the input\/output is acknowledged  to the primary array  and then acknowledged  to the host . In a particular embodiment, a main control unit performs a write operation on a primary volume, starts the update copy operation on the secondary volume, and reports final ending status to a host only after results of the update copy operation are known. If either the primary volume write or the secondary volume update copy operation fails, the main control unit reports a unit check, and the host system and application program regard the write operation to the primary volume as failed. The method for preserving logical object integrity in a remote mirror cache prevents the secondary volume from containing inconsistent or incorrect data.","The illustrative example depicts a two-site data replication and is similarly extended to additional replication sites. In a two-site data replication method, the host application is responsible for data integrity. Because an input\/output command is only acknowledged to the application  when written to both arrays  and , the application only issues the next input\/output command once the first command is complete so that data is written to the secondary array  in order and consistent. Synchronous replication is relative unsuited to multiple site mirroring since each additional new site adds to the response time of the application.","Referring to , a schematic block diagram depicts an asynchronous data replication method with record ordering. An input\/output command issued  by the host  to the primary storage array  is immediately acknowledged  to the host  as soon as the command reaches the cache. A sequence number is added to the input\/output command and sent  to the secondary array . Since the path to the secondary array  can traverse any of multiple paths or routes, a possibility exists that the input\/output commands can arrive out of order. The secondary array  is responsible for reordering the incoming commands according to sequence number and applying data records in the correct sequence. Management by the secondary array  ensures an in-order, consistent database, although the most current transactions can be lost in the event of a failure. Asynchronous data replication is better suited for long-distance replication since latency impact on the application host is reduced or eliminated.","In a particular example, the main control unit completes primary volume operations independently of the associated update copy operations at the secondary volume. The remote control unit manages the secondary volume updates according to the recordset information and maintains sequence ordered data consistency for the secondary volumes. If the primary volume write operation fails, the main control unit reports a unit check and does not create an asynchronous recordset for the operation. If the update copy operation fails, the remote control unit can optionally suspend either the affected pair or all pairs in a consistency group, depending on the type of failure. At resumption of the suspended pair, the main control unit and remote control unit can negotiate resynchronization of the pairs. The method for preserving logical object integrity in a remote mirror cache prevents an operation from leaving incorrect information on a secondary volume.","Referring to , a schematic block diagram illustrates sidefile usage in asynchronous data replication. Asynchronous replication uses sequence order tracking of update occurrence to ensure consistency. Tracking takes place in a primary volume sidefile  and a secondary volume sidefile . The individual updates are ordered with a sequence number and transmitted to the remote array . When the remote array  has received the next sequence number in the set, the remote array  acknowledges receipt of the data according to sequence number to the primary array  and the affected sequence number is removed from the primary volume sidefile list . If a transaction is lost between the primary array  and the secondary array , retransmit of a specific sequence number's data can be requested.","The sequence of numbers is managed in memory of the primary array  and the remote array  and utilizes additional resources, the sidefiles  and . For an input\/output operation performed to the primary array , an entry is added to the sidefile  containing the sequence number and a pointer to the blocks affected by the update. If the same block is updated on a subsequent input\/output operation, contents of the block are also recorded in the sidefile . The sidefile size is dependent on performance of the links to the remote array  against the number of input\/output operations performed by the primary array . If the sidefile  reaches a predetermined percentage of the total cache memory in the array , for example if the input\/output operations are backing up in the cache due to a slow link, the input\/output rate from the host  is restricted in an attempt to give higher priority to the sidefile .","A sidefile is typically only used as long as a communication exists between the primary site  and the secondary site . If communication is disrupted, or pairs are suspended, overhead of a sidefile is considered to be too high so a bitmap is instead used to track changes, typically on a per-track or per-cylinder basis.","In various embodiments, the asynchronous recordsets can contain primary volume updates and associated control information, for example sequence number of the primary volume update to enable the remote control unit to maintain update consistency of the secondary volumes. Recordset operations can include creating and storing recordsets at the main control unit, sending recordsets to the remote control unit, storing recordsets in the remote control unit, and selecting and settling recordsets at the remote control unit. Other operations include controlling inflow for sidefiles.","In one example, upon a host-requested write input\/output operation the main control unit performs an update and creates a recordset. The recordset can include the updated record, sequence number, record location such as device, cylinder, track, and record number, and record length. The recordsets can be queued in cache storage of the main control unit and sent to the remote control unit independent of host input\/output processes. The remote control unit uses the sequence number in the recordsets to update the secondary volumes in the order of the primary volumes. The sequence number indicates the number of recordsets that the main control unit has created for each consistency group. Recordset information, other than updated records, is stored and queued in an area of cache known as sidefile cache.","In the example, the main control unit can send recordsets to the remote control unit by using main control unit initiator ports for issuing special input\/output operations, called remote I\/Os, to the remote control unit. The remote I\/Os transfer recordsets efficiently using a single channel command so that the main control unit can send multiple recordsets in a single remote I\/O call, even with noncontiguous sequence numbers. The remote control unit can store recordsets, maintaining queues to control storing of recordsets in the sidefile and commitment of updating records in the secondary volumes. Remote control unit queuing can use the sequence numbers to check for missing updates.","A bitmap table is an efficient technique to track changed records on a device from a particular point in time. Bit map tables record the changed track or cylinder number and typically do not maintain information concerning sequence or details of changes. During times of no communication between the primary site  and secondary site  or the pairs are suspended, a delta bit map table is maintained on both the primary  and secondary  arrays. Upon resynchronization of the pairs, only the changed cylinders are copied to the remote array , bringing the data mirror up to date. Thereafter, a sidefile is again used to continue updates. During resynchronization, data on the remote array  is inconsistent and unreliable.","Tracking of consistency groups is used to assure correct operation. An asynchronous consistency group is a user-defined set of volume pairs across which update sequence consistency is maintained and ensured at the remote site. Each asynchronous volume pair is assigned to a consistency group. In an illustrative system, the database system allows configuration of a predetermined number of consistency groups for each main control unit and supports group-based operations for the consistency groups. Consistency groups enable maintenance of update sequence consistency for databases that span multiple volumes, facilitating immediate database recovery at the remote site in the event of a failure or disaster.","An application commonly includes an aggregation of more than one physical device. Accordingly, correct operation can depend on assurance that all input\/output activities are consistently applied to remote devices. During asynchronous operations, all devices in a device group form the same consistency group. Sequence numbers in a sidefile are issued at the consistency group granularity level so that input\/output operations applied to the primary devices of that consistency group are applied to the secondary devices in the same sequence. If a device in the consistency group is not applied to be updated, the entire consistency group is placed into an error state. Consistency groups are defined and controlled so that writes to all devices in the consistency group are not destaged unless all are ready. Consistency is applied to all devices in the consistency group, not simply a single LUN.","The method for preserving logical object integrity in a remote mirror cache can be used in the various remote copy operations of the database system, such as initial copy and update copy operations. An initial copy operation synchronizes the primary volumes and secondary volumes, generally independently of host processes. The initial copy typically takes place when a user adds a volume pair or resumes a split or suspended volume pair. When a new pair is created, the entire contents of the primary volume are copied to the secondary volume cylinder by cylinder, except for diagnostic and unassigned alternate tracks. Various database system embodiments may implement or omit usage of the method for preserving logical object integrity in a remote mirror cache for initial copy. Because initial copy generally occurs for more controlled conditions of database usage, some database system embodiments may omit the overhead associated with the method for preserving logical object integrity in a remote mirror cache for initial copy.","An update copy operation occurs when a host issues a write input\/output operation to a primary volume of an established volume pair. The update copy operation duplicates the primary volume write input\/output operation at the secondary volume to maintain volume pair synchrony. Usage of the method for preserving logical object integrity in a remote mirror cache is useful in update copying to assure correct database operations.","While the present disclosure describes various embodiments, these embodiments are to be understood as illustrative and do not limit the claim scope. Many variations, modifications, additions and improvements of the described embodiments are possible. For example, those having ordinary skill in the art will readily implement the steps necessary to provide the structures and methods disclosed herein, and will understand that the process parameters, materials, and dimensions are given by way of example only. The parameters, materials, and dimensions can be varied to achieve the desired structure as well as modifications, which are within the scope of the claims. Variations and modifications of the embodiments disclosed herein may also be made while remaining within the scope of the following claims. For example, the disclosed apparatus and technique can be used in any database configuration with any appropriate number of storage elements. Although, the database system discloses magnetic disk storage elements, any appropriate type of storage technology may be implemented. The system can be implemented with various operating systems and database systems. The control elements may be implemented as software or firmware on general purpose computer systems, workstations, servers, and the like, but may be otherwise implemented on special-purpose devices and embedded systems."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["Embodiments of the invention relating to both structure and method of operation, may best be understood by referring to the following description and accompanying drawings.",{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIGS. 7 and 8"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIGS. 10A and 10B"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 11"}]},"DETDESC":[{},{}]}
