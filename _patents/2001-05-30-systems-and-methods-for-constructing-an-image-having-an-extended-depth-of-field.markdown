---
title: Systems and methods for constructing an image having an extended depth of field
abstract: Systems and methods are provided for constructing a composite image having an extended depth of focus from a plurality of spatially congruent source images of lesser depth of focus. The systems and methods are relatively fast, preserve detail, provide robust results on a variety of unpredictable workpiece features and configurations, and tend to suppress or reduce out-of-focus artifacts. The composite image is constructed by edge and/or boundary analysis to identify well-focused edges or boundaries in the source images. Each particular edge or boundary in the composite image is determined based on the source image containing the best-focused instance of each particular edge or boundary. The composite image is constructed outside of the previously constructed portions by surface analysis to identify well-focused surfaces in the source images, Each particular surface portion in the composite image is determined based on the source image with the best-focused instance of each particular surface portion.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07058233&OS=07058233&RS=07058233
owner: Mitutoyo Corporation
number: 07058233
owner_city: Kawasaki
owner_country: JP
publication_date: 20010530
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF PREFERRED EMBODIMENTS"],"p":["1. Field of Invention","This invention relates to image-processing systems and methods usable to construct a composite image.","2. Description of Related Art","High resolution machine vision systems, and especially microscopic machine vision systems, often provide a high-resolution, well-focused image of a small portion of a three-dimensional object. However, this is accomplished at the expense of the depth of field, also called the depth of focus, of the surrounding overall image. However, in some machine vision system applications, it is useful both to make high-resolution well-focused microscopic examination and\/or record of a small portion of an object and to clearly observe or record the surrounding overall image.","Methods are known for combining, or fusing, separately focused images of a three-dimensional scene or object to accomplish this useful goal. For example, U.S. Pat. No. 4,141,032 to Haeusler discloses focusing through an object at a plurality of levels to produce a plurality of images, filtering each image with a high pass filter, and summing the images passed by the filtering step to produce a composite image containing only the sharp details. Similarly, U.S. Pat. No. 4,584,704 to Ferren discloses obtaining visual in-focus \u201cslices\u201d of an entire image field. When processing the slices as video scan line signals, the signals pass through a high pass filter. This provides signals which may be discriminated to remove all but the spikes, which identify in-focus edges of objects. Signal information between two successive spikes (edges) may also be regarded as part of an in-focus object. The high pass filter may be combined with other filters. The filters may be made adaptive.","In general, such methods that are based on such relatively simple high-pass filtering are relatively fast. However, the composite images obtained are sensitive to the selected filter parameters and\/or signal thresholds employed to discriminate between in-focus and out-of-focus features. As such, these methods are not robust for an unpredictable variety of source image objects and\/or surface characteristics, and may introduce undesirable information loss and\/or admit undesirable high spatial frequency artifacts from out-of-focus image portions.","Methods based on various multi-resolution spatial filtering techniques are also known. For example, U.S. Pat. No. 4,661,986 to Adelson lists numerous papers by Burt describing various aspects of the Burt pyramid method, and discloses a variation of that method. Adelson discloses dividing respective spatial-frequency spectrums of M images into M substantially similar assemblages of N separate specified pixel sample sets that define N spatial frequency bands. A single one of the corresponding samples is selected to derive respective single sets of improved focus pixel samples for each of the N bands. Corresponding pixel samples of the respective single sets are then combined to derive the improved-focus two-dimensional image.","U.S. Pat. No. 5,325,449 to Burt discloses an image fusion method general enough to fuse not only differently-focused images, but qualitatively different images provided by qualitatively different types of sensing\/imaging systems. Burt notes that image fusion is successful to the extent that the composite image retains all useful information, does not contain artifacts, and looks natural. Burt notes that previous pyramid techniques, including multi-resolution spatial frequency techniques, have produced noticeable artifacts in composite images. Burt discloses an improved pattern selective method based upon using oriented functions, such as a disclosed gradient function, which improves the retention of edge-like source image patterns in the composite image. This method is enhanced by a local saliency analysis to refine the composite image.","In general, the pyramid methods of Adelson and Burt can be relatively fast, and relatively robust. However, these pyramid methods are primarily based on a single type of mathematical operation which is applied uniformly throughout all images. Thus, the methods remain sensitive, for example, to undesirable high-frequency artifacts which may be produced by unfocused edges in otherwise featureless and\/or out-of-focus portions of the source image.","Convolution methods and wavelet-based methods are also known to be usable to derive composite images. Another known \u201cimage fusion\u201d method relies on formulating a multi-component energy function and determining the image fusion result which minimizes the energy function. However, all of these methods generally apply a single type of mathematical operation uniformly throughout all images. Depending on the implementation details, such methods are either computationally-intensive, relatively slow, and\/or they also share the drawbacks of the previously-described methods.","Further, numerous methods are known that both identify edges in single images and segment single images. However, such methods have failed to consider how to select among multiple source images to create a composite image in a way that anticipates and suppresses the artifacts that are likely to exist in out-of-focus portions of an image.","Thus, considering that a composite image is successful to the extent that the composite image retains all useful information, does not contain artifacts, and looks natural, and furthermore considering the commercial value and convenience of rapid computation, and furthermore considering the need for robust and reliable results on variety of unpredictable workpiece features and configurations, there is a need for an improved, commercially-appealing solution that balances efficacy against throughput time and provides a high-resolution well-focused image of a small portion of a three-dimensional object while also allowing the user to clearly observe or record the area surrounding that small portion of the three-dimensional object. This problem is particularly significant in automated machine vision systems used for microscopic inspection and measurement. In such systems, there is a need for a composite image construction method that is relatively fast, preserves detail, provides robust results on variety of unpredictable workpiece features and configurations, and that tends to suppress or reduce out-of-focus artifacts.","This invention provides systems and methods that construct a desirable composite image having an extended depth of focus in a relatively short time, and preserves the image detail present in a plurality of source images.","This invention separately provides systems and methods that construct a desirable composite image reliably and robustly, for a variety of unpredictable workpiece features and configurations.","This invention further provides systems and methods that are readily adaptable to construct a desirable composite image even more reliably, robustly, and in an even shorter time, when certain workpiece features are predictable.","This invention separately provides systems and methods that construct a desirable composite image while suppressing likely artifacts arising from out-of-focus features.","In various exemplary embodiments of the systems and methods according to this invention, an object is placed within the overall field of view of a vision system. A plurality of source images of the object are captured, where each captured source image exhibits a different plane of the object in the nominal focus plane of the source image. The source images are either acquired with spatial congruence, or their data representations are mutually adjusted to be spatially congruent. A composite image having an extended depth of focus is constructed by identifying well-focused edges and\/or boundaries in the source images. Each particular edge or boundary in the composite image is determined based generally on the source image with the best-focused instance of each particular edge or boundary. The composite image is also constructed by identifying well-focused surfaces in the source images. Each particular surface portion in the composite image is determined based generally on the source image with the best-focused instance of each particular surface portion.","In various exemplary embodiments, the composite image is generated hierarchically, in that the determined edges or boundaries are given priority in the composite image regardless of whether an apparently well-focused surface portion is identified at the same spatial location as an edge or boundary. This tends to suppress artifacts in the composite image in the vicinity of boundaries and edges.","In various other exemplary embodiments of the systems and methods according to this invention, additional buffer zones are established immediately adjacent to determined edges or boundaries. The composite image generation in these buffer zones suppresses artifacts which might otherwise be created by out-of-focus edges \u201cbleeding over into surface regions\u201d in some source images and producing anomalous results during the surface analysis.","In various other exemplary embodiments of the systems and methods according to this invention, the some of the determined well-focused surface portions which initially provide a basis for the composite image are modified, based on consideration of the plane of their corresponding source image, relative to the source image plane(s) of neighboring surface portions.","In contrast to the previously discussed conventional methods, in the various exemplary embodiments of the systems and methods according to this invention, the composite image can be constructed relatively quickly while preserving detail in the composite image. The resulting composite images can be constructed robustly in view of a variety of unpredictable workpiece features and configurations, while out-of-focus artifacts in the resulting composite images tend to be suppressed or reduced.","These and other features and advantages of this invention are described in, or are apparent from, the following detailed description of various exemplary embodiments of the systems and methods according to this invention.","For simplicity and clarification, the operating principles, and design factors of various exemplary embodiments of the systems and methods according to this invention are explained with reference to one exemplary embodiment of a vision system , as shown in . The basic explanation of the operation of the vision system shown in  is applicable for the understanding and design of any vision system that incorporates the systems and\/or methods of this invention. Although the systems and methods of this invention are described in conjunction with this specific vision system , the systems and methods according to this invention can be used with any other known or later-developed vision system.",{"@attributes":{"id":"p-0039","num":"0038"},"figref":["FIG. 1","FIG. 1"],"b":["100","100","110","120","120","150","120","170"]},"The vision system components portion  includes a stage . The stage  may be movable relative to the remainder of the vision system components portion , in a plane normal to a z-axis , for example along an x-axis . A part  to be imaged using the vision system  is placed on the stage . The part  shown in  includes a number of relatively planar surface regions ,  and , a number of edges  and  and at least one inclined surface region . A light source  can be used to illuminate the part . The light source  may be a simple surface light, a ring light or a programmable ring light, and may include other lights sources, such as a stage light and\/or a coaxial light.","The light from the light source , after illuminating the part , passes through a lens system , and is gathered by a camera system  to generate an image of the part . The lens system  and the camera system  are fastened to a housing\/frame  to form an imaging assembly . The imaging assembly  is movable along the z-axis  by an actuator . By moving the imaging assembly  along the z-axis , the focal plane of the lens system  is moved along the z-axis  relative to the part . Thus, it is possible to move the imaging assembly  over a chosen range and capture a set of source images of the part , where different portions of the part  are in focus in different ones of the set of source images. Furthermore, the set of source images which are captured in this manner will, in general, be substantially spatially congruent.","Each image acquired by the camera system  is output on a signal line  to the control system portion . As shown in , the control system portion  includes a controller , an input\/output interface , a memory , the part program executor and\/or generator , a CAD file feature extractor , and the composite image processor , each interconnected either by a data\/control bus  or by direct connections between the various elements. The signal line  to and from the camera system , the signal line  to and from the actuator , and the signal line  to and from the light source  are connected to the input\/output interface . In various exemplary embodiments, the input\/output interface  may include one or more of an imaging control interface portion (not shown) usable to interact with the camera system , a motion control interface portion (not shown) usable to interact with the actuator , and a lighting control interface portion (not shown) usable to interact with the light source .","A display device  and one or more input devices  also can be connected to the input\/output interface  over a signal line  and one or more signal lines , respectively. The display  and the one or more input devices  can be used to view, create and modify part programs, to input CAD file information regarding the part , to view the source images captured by the camera system , to view composite images, and\/or to directly operate the control system portion  and\/or the vision system components . However, it should be appreciated that, in a fully automated system having a predefined part program, the display  and\/or the one or more input devices , and the corresponding signal lines  and\/or  may be omitted.","The memory  records and holds data as necessary for the operation of the vision system . In various exemplary embodiments, the memory  includes one or more of a source image memory portion , a part program memory portion , a composite image processing memory portion , and\/or a composite image memory portion . The source image memory portion  stores source images captured using the camera system  when operating the vision system . The part program memory portion  stores one or more part programs used to control the operation of the vision system  for particular types of parts. The composite image processing memory portion  contains data recorded and\/or created as needed during the operations of the composite image processor , as described in greater detail below. The composite image memory portion  contains composite image data representations and\/or composite images which are output by the composite image processor .","The exemplary composite image processor  as included in the control system portion  shown in , is one exemplary embodiment of a composite image processor usable according to the systems and methods of this invention. The exemplary composite image processor  of  includes an edge\/boundary processing circuit or software routine , and a surface processing circuit or software routine , each interconnected either by a data\/control bus , by one or more application programming interfaces or by direct connections between the various circuits or software routines. The data\/control bus  and the data\/control bus  may be the identical data\/control bus in various exemplary embodiments of the control system portion .","The exemplary edge\/boundary processing circuit or software routine  shown in  processes source image data from a plurality of source images and determines a set of edge or boundary pixels in the composite image based on the analyzed source images. That is, the edge\/boundary processing circuit or software routine  is operable to perform a type of analysis of the set of source images usable to determine a first set of pixels of the composite image corresponding to edges and\/or boundaries in the composite image. The exemplary surface processing circuit or software routine  shown in  processes source image data from a plurality of source images and determines a set of surface pixels in the composite image based on the analyzed source images. Similarly, the surface processing circuit or software routine  is operable to perform a type analysis of the set of source images usable to determine a second set of pixels of the composite image corresponding to surface regions in the composite image. The various elements and operation of the composite image processor  are described in further detail elsewhere herein.","It should be appreciated that the foregoing description of the vision system  usable according to this invention generally describes a system suitable for automatic program operation. However, the vision system  used with this invention may also operate substantially the same when the commands, including composite image construction commands, are issued manually through the one or more input devices  during manual or stepwise operation of the vision system . Furthermore, although the vision system  is capable of adapting to various components and operating parameters, the following discussion generally assumes that the configuration of the vision system components portion  and the workpiece under inspection are already known and\/or are predictable. In addition, any necessary operating conditions which are not specifically discussed are assumed to have been met according to well-known procedures for the operation of machine vision systems.","When a machine vision system operator wishes to construct a composite image of a specific portion of an object, such as a portion of the part  mounted on the stage , the user operates the vision system  to position that portion of the object within the field of view of the vision system . Various image acquisition parameters, for example, the lighting settings, and the positioning and\/or focus of the imaging assembly , are chosen to provide an acceptable set of source images. These parameters, positioning and\/or focus of the imaging assembly  are obtainable either by operator trial and error or with the assistance of a variety of automated \u201ctools\u201d available on a variety of commercially-available machine vision systems, such as the Quick Vision series of vision inspection machines available from Mitutoyo America Corporation (MAC), located in Aurora, Ill.","By moving the imaging assembly  along the z-axis , the focal plane of the lens system  is moved along the z-axis  relative to the part . Thus, it is possible to move the imaging assembly  over a chosen range and capture a set of source images of the part , where, in general, each important feature of the desired portion of the part  will be in focus in at least one of the different source images. The chosen range may be divided into a plurality of equidistant movements along the z-axis in order to acquire the plurality of source images. Alternatively, z-axis positions that allow important features of the part , such as the relatively planar surface regions ,  and , for example, to be specifically focused on or a combination of these approaches, may be chosen. The plurality of source images may be stored in the source image memory portion .","For the exemplary configuration of the vision system  shown in , the set of source images which are captured in this manner will be substantially spatially congruent. That is, a particular spatial location in one source image will correspond to the same portion of the object at the same spatial location in any of the other source images. For purposes of composite image processing, if it is not assured that the source images are spatially congruent, the plurality of source images may be analyzed by well-known techniques, such as, for example, by image correlation or other pattern-matching techniques so that the spatial locations of the plurality of source images may be adjusted to provide for spatial congruence. The spatial congruence\/feature congruence processing circuit or software routine  of the composite image processor  can be used to determine if the images are spatially congruent and\/or to implement one or more of these techniques for adjusting the spatial locations to obtain spatial congruence.","If the operator desires to obtain a composite image which corresponds to only a portion of a full source image then, by using the display device  and\/or an input device  of the vision system , the operator may choose and\/or define the desired portion of the object , for example, by using a so called \u201cbox tool\u201d on the display device . In this case, any subsequent composite image processing operations may be restricted to the corresponding spatial locations.","Any or all of the foregoing operator actions or instructions may be recorded by the part program generator and executor , and the results stored in the part program memory portion  for later recall and automatic execution by the part program generator and executor , for example to inspect and\/or make a record of a different specimen of the part . The foregoing describes one exemplary way of providing a plurality of source images usable according to the systems and methods of this invention. However, it should be appreciated that the systems and methods of this invention are useable with any similar plurality of source images, regardless of the details of the imaging system, the portion of the electromagnetic spectrum used to acquire the plurality of source images, and\/or the operations used to acquire and\/or prepare the plurality of source images.",{"@attributes":{"id":"p-0053","num":"0052"},"figref":["FIG. 2","FIG. 1","FIG. 2"],"b":["160","170","150","160","163","166","170","173","176"]},"It should be appreciated that the particular structures shown in  for the edge\/boundary processing circuit or software routine  and the surface processing circuit or software routine  are independent of each other. Thus, while specific exemplary structures for both of the edge\/boundary processing circuit or software routine  and the surface processing circuit or software routine  are shown together in , each of these specific structures of the edge\/boundary processing circuit or software routine  and the surface processing circuit or software routine  can be implemented separately of the other, and combined with other exemplary structures of the other of the edge\/boundary processing circuit or software routine  and the surface processing circuit or software routine . Thus, the specific combination of structures of the edge\/boundary processing circuit or software routine  and the surface processing circuit or software routine  shown in  should be understood as exemplary only, and not limiting or essential.","The source image edge\/boundary processing circuit or software routine  is usable to process and\/or characterize the source image data from a plurality of source images. The composite image edge\/boundary determining circuit or software routine  determines a set of edge or boundary pixels in the composite image based on the results of the edge\/boundary processing circuit or software routine . In various exemplary embodiments, the edge\/boundary processing circuit or software routine  may include an analysis portion (not shown) usable to analyze or characterize spatial locations in the source images and provide results indicative of potential edge or boundary pixels in the source images.","In various other exemplary embodiments, the source image edge\/boundary processing circuit or software routine  may further include a validating portion (not shown) usable to screen out some of the results indicative of potential edge or boundary pixels in the source images. If implemented, the validating portion leaves only the results actually indicating acceptable edge or boundary pixels in any of the source images. Further, in various exemplary embodiments, the composite image edge\/boundary determining circuit or software routine  may include a selector portion (not shown) usable to select or qualify the best edge or boundary pixels for the composite image from among the results provided by the edge\/boundary processing circuit or software routine . In various other exemplary embodiments, the composite image edge\/boundary determining circuit or software routine  may further include a refinement portion (not shown) usable to refine the results from the selector portion, for example, to reject image artifacts not otherwise rejected by the systems and methods according to this invention.","The source image surface processing circuit or software routine  is usable to process and\/or characterize source image data from a plurality of source images. The composite image surface determining circuit or software routine  determines a set of surface pixels in the composite image based on the results of the surface processing circuit or software routine . In various exemplary embodiments, the surface processing circuit or software routine  may include an analysis portion (not shown) usable to analyze or characterize spatial locations in the source images and provide results indicative of potential surface pixels in the source images.","Further, in various exemplary embodiments, the composite image surface determining circuit or software routine  may include a selector portion (not shown) usable to select or qualify the best edge or boundary pixels for the composite image from among the results provided by the surface processing circuit or software routine . In various other exemplary embodiments, the composite image surface determining circuit or software routine  may further include a refinement portion (not shown) usable to refine the results from the selector portion, for example, to reject image artifacts not otherwise rejected by the systems and methods according to this invention.","In various other exemplary embodiments, the composite image processor  may further include auxiliary circuits or software routines (not shown) including one or more of a spatial congruence and\/or feature congruence processing circuit or software routine, a composite image refinement processing circuit or software routine, and a composite image storage and\/or output processing circuit or software routine which are each interconnected either by a data\/control bus, by one or more application programming interfaces or by direct connections between the various circuits or software routines. The operations performed by these auxiliary circuits or software routines are described in greater detail further below.",{"@attributes":{"id":"p-0060","num":"0059"},"figref":"FIG. 3","b":["100","200","300","400","500"]},"In step S, if a composite image is desired for a portion of an object which corresponds to only a portion of the full source image frame, then the spatial location range for which a composite image is desired is defined. The bounds of this portion of the full source image frame can be defined manually, in a semi-automated fashion, or fully automatically. The subsequent composite image processing operations may be restricted to correspond to the defined bounds, or spatial location range, of the portion to reduce processing time. Then, in step S, each of the plurality of source images is characterized and\/or analyzed using a technique that is sensitive to and indicative of potential boundary pixels and\/or edge pixels in each analyzed source image. A wide variety of techniques usable in any known or later-developed edge-detection method are usable in various exemplary embodiments of the systems and methods according to this invention, as will be described further below. In various other embodiments, any known or later-developed edge detector, such as the Canny edge detector (the structure and operation of which is well-known to those of ordinary skill in the art) and the like, may be used in its entirety in step S to provide results usable in the systems and methods according to this invention. Next, in step S, the results of the edge and\/or boundary analysis technique are evaluated to determine a set of \u201cedge\u201d pixels in the composite image, as will be described further below. Operation then continues to step S.","In step S, each of the plurality of source images is characterized and\/or analyzed using a technique that is sensitive to and indicative of potential surface pixels in each analyzed source image. In the broadest sense under various exemplary embodiments of the systems and methods according to this invention, surface pixels includes any pixels that are not determined to be edge and\/or boundary pixels. A wide variety of techniques usable in any known or later-developed surface-focus detection method, region characterization method, and\/or image segmentation method, are usable in various exemplary embodiments of the systems and methods according to this invention, as will be described further below. Next, in step S, for pixels remaining undetermined in the composite image, that is, for those pixels of the composite image which have not been previously determined, the results of the surface pixel analysis technique are used to determine a set of \u201csurface\u201d pixels in the composite image. In various exemplary embodiments, region-filling or region-growing type operations are used to determine the composite image surface pixels according to the systems and methods of this invention, as described in detail further below. Operation then continues to step S.","In step S, any voids in the composite image representation resulting from the previous steps are filled. A variety of techniques are known for filling such voids in conventional images. Any known or later-developed void filling technique may be used in various exemplary embodiments of the systems and methods according to this invention. However, because there are a plurality of source images which may potentially be used as the basis for the pixels values to be used to fill the voids in the composite image, in various exemplary embodiments, the techniques used to fill the voids are supplemented with additional operations for selecting which source images will provide that basis, as will be described further below. Operation then continues with step S.","It should be appreciated that step S is optional, and thus may be omitted, depending on the details of the edge and\/or boundary and\/or the surface analyses used in various embodiments of the systems and methods according to this invention. If the edge and\/or boundary and\/or the surface analyses inherently determine all pixels of the composite image, then inspecting the composite image representation for voids and filling the voids is not strictly necessary. In this case, step S is omitted, and operation simply skips from step S to step S. However, if the previous steps do not determine all pixels of the composite image, then it is preferred to fill the voids in the composite image representation using step S.","In step S, the composite image representation is then stored to a memory device and\/or output to a display device or recording device in any of a variety of suitable forms. In various exemplary embodiments, this storing and\/or output step may include operations for transforming the composite image representation resulting from the previous steps into a subsequent electronic or physical representation of the composite image which is more convenient, desirable and\/or useful for a particular purpose. For example, depending on the details of the previous steps, in various embodiments of the systems and methods according to this invention, separately stored records of the edge-pixels and the surface-pixels of the composite image are assembled or merged into a single stored representation of the composite image.","As a further example, in various embodiments of the systems and methods according to this invention, the composite image representation may be in the form of a \u201cmap\u201d of the spatial locations in the composite image, where the map contains function values or indexes, in contrast to actual pixel intensity values. Furthermore, the spatial resolution in all or part of the map may be at the spatial resolution of the pixels in the source images, or it may at a coarser spatial resolution in the form of \u201cmeta-pixels\u201d. Thus, in such embodiments, before the corresponding composite image can be meaningfully displayed as a conventional image, conventional gray-scale or color image data must be assembled by transforming such a \u201cmap\u201d representation to the desired resolution and data format. In any case, in step S, a desired representation of the composite image which is convenient, desirable and\/or useful for a particular purpose is stored to a memory device and\/or output to a display device or recording device. Operation then continues to step S, where the operation ends.","It should be appreciated that steps S\u2013S are just one exemplary embodiment of the method for providing a plurality of source images usable according to the systems and methods of this invention. Thus, it should be appreciated that the systems and methods of this invention generally, and steps S\u2013S in particular, are useable with any similar plurality of source images, regardless of the details of the underlying imaging system, the portion of the electromagnetic spectrum used to acquire the plurality of source images, or the particular operations used to acquire and\/or prepare a similar plurality of source images.","With regard to the terms \u201cboundaries\u201d and \u201cedges\u201d as used herein, the terms \u201cboundaries\u201d and \u201cedges\u201d are generally used interchangeably with respect to the scope and operations of the systems and methods of this invention. However, when the context dictates, the term \u201cedge\u201d may further imply the edge at a discontinuity between different surface planes on an object and\/or the image of that object. Similarly, the term \u201cboundary\u201d may further imply the boundary at a discontinuity between two colors, or other relatively homogeneous surface properties, on a relatively planar surface of an object, and\/or the image of that object.","The terms \u201cboundary pixels\u201d and \u201cedge pixels\u201d are used interchangeably with respect to the scope and operations of the systems and methods of this invention. Both terms are intended to refer to pixels in the vicinity of local intensity changes in an image. These local intensity changes can be either grayscale intensity or a color intensity, for example. These local intensity changes are generally detectable by any conventional or enhanced, or any later-developed, edge-detection operation. Similarly, the terms \u201cboundary properties\u201d and \u201cedge properties\u201d are used interchangeably with respect to the scope and operations of the systems and methods of this invention. Both terms are intended to refer to any property, whether numerically characterizable or otherwise, that is usable, either alone or in combination, by any conventional or enhanced, or any later-developed, edge-detection operation.","Some operations usable to characterize boundary properties and to detect edges include, but are not limited to, high-frequency spatial filtering, gradient analyses, directional gradient analyses, gradient derivative analyses, aspect ratio analysis, extent and\/or connectedness analysis of candidate edges, and the like. Gradient analysis includes, but is not limited to, any one or more of gray-scale intensity, color intensity, texture measures, and the like. Aspect ratio analysis includes, but is not limited to, evaluating the length vs. width of candidate edges.","These and other operations which are sensitive to and indicative of boundary and\/or edge properties are known to one skilled in the art of image processing, and are included in one or more of a wide variety of known edge-detection techniques. The chapter regarding edge-detection in the Machine Vision, by Ramesh Jain, et al., McGraw Hill, 1995 discusses related teachings and references. It should also be appreciated that nearly all of the operations discussed above which are sensitive to and indicative of boundary properties and usable to identify edges in an image are furthermore sensitive to and indicative of the degree to which an edge is focused in an image, since out-of-focus edges are blurred and therefore exhibit a different spatial distribution of intensities than when focused, and, when extremely out-of-focus, are not visible at all.","It should be further appreciated that, in various exemplary embodiments, it is desirable to use an edge-detection operation which operates over all of the analyzed source images to determine the set of composite image edge pixels. This set of composite image edge pixels will include an edge pixel at any spatial location where analysis of the plurality of spatially congruent source images indicates that there is a valid edge pixel in at least one of the spatially congruent source images. Accordingly, it should be appreciated that, in various embodiments of the systems and methods according to this invention, the operations previously described corresponding to steps S and S combine to implement an edge-detection operation which operates over the plurality of source images to perform a type of analysis of the source images usable to determine a first set of pixels of the composite image corresponding to edges and\/or boundaries in the composite image.","Thus, the operations previously described as corresponding to steps S and S might alternatively be characterized as a single, more complex, multi-image composite edge-detection operation corresponding to a single step. Accordingly, in various embodiments, any known or later-developed multi-image composite edge detector, such as the multi-image fusion edge detector disclosed in the previously discussed \u2032449 Patent, may be used to provide results usable in the systems and methods according to this invention. Various other exemplary operations related to determining edge pixels in the composite image are discussed in detail further below.","With regard to the term \u201csurface pixels\u201d as used herein, as previously mentioned, in the broadest sense according to this invention, \u201csurface pixels\u201d include any pixels that are not determined to be boundary pixels and\/or edge-pixels. In addition, in many practical cases according to the systems and methods of this invention, \u201csurface pixels\u201d will be pixels that constitute an image of a relatively well-focused, homogenous, and relatively planar local region on an object. The term \u201csurface property\u201d, as used herein, generally refers to any property, whether numerically-characterizable or otherwise, that is usable, either alone or in combination, by any known or later-developed region characterization operation or technique, such as, for example, surface and texture classification methods.","Stated another way, surface properties are the properties associated with the statistical properties and spatial distribution of gray levels and\/or color levels in a particular local neighborhood. Some operations usable to characterize surface-properties include but are not limited to, for example, variance analysis, average intensity, other statistical texture and\/or contrast measures, fractal dimension, texture directionality measures, and the like. Variance analysis and average intensity can each be applied to, for example, gray level values or color level values.","These and other operations which are sensitive to and indicative of surface properties are known to one skilled in the art of image processing, and are included in one or more of a wide variety of known surface-focus detection (auto-focus) methods, region\/texture characterization methods, and\/or image segmentation methods. The chapters regarding texture and regions in the Machine Vision, by Ramesh Jain, et al., McGraw Hill, 1995 discuss related teachings and references. It should be appreciated that nearly all of the operations discussed above which are sensitive to and indicative of surface properties for the purpose of classifying surface regions, and especially those useful for the purpose of classifying textures, are furthermore sensitive to and indicative of the degree to which a surface is focused in an image, since out-of-focus surface textures are blurred and therefore exhibit a different statistical and spatial distribution of intensities than when focused, and when extremely out-of-focus are show only a uniform intensity (i.e., perfectly smooth texture).","With regard to the determination of surface pixels in the composite image, it should be appreciated that, in various exemplary embodiments, it is desirable to use a surface-pixel determining operation which operates over all of the analyzed source images to determine a set of composite surface pixels including any spatial location which is not otherwise determined to be the location of a boundary pixel and\/or an edge pixel. Accordingly, it should be appreciated that, in various embodiments of the systems and methods according to this invention, the operations previously described as corresponding to steps S and S combine to implement a surface-detection operation which operates over the plurality of source images to perform a type of analysis of the source images usable to determine a set of pixels of the composite image corresponding to surface regions in the composite image. Thus, the operations previously described as corresponding to steps S and S might alternatively be characterized as a single, more complex, multi-image composite surface-detection operation corresponding to a single step. Various exemplary operations related to determining surface pixels in the composite image are discussed in detail further below.",{"@attributes":{"id":"p-0078","num":"0077"},"figref":["FIG. 4","FIG. 3"],"b":["600","600","610","600"]},"Next, in step S, one of the source images determined in step S which has not been previously selected for analysis is selected as the current source image to be analyzed. Then, in step S, the data is accessed for the current source image to be analyzed. More particularly, the data is accessed for the spatial locations within the current source image which correspond to the spatial locations to be included in the desired composite image. Operation then continues to step S.","In step S, the accessed source image data is analyzed to provide edge-focus indicator results and to provide a local result at a desired spatial resolution. In various exemplary embodiments, in step S, the accessed source image data is analyzed using a mathematical or algorithmic technique which is sensitive to and indicative of local intensity changes in the image. These local intensity change can be either changes in the grayscale intensity values or changes in the color intensity values, for example.","In various exemplary embodiments, the data for the current source image is first normalized with respect to white level. Then, two gradient operators which are the derivatives of a 2-dimensional Gaussian in the directions of the rows and columns of the image data, respectively, are defined. In one exemplary embodiment the 2-dimensional Gaussian is set at a size of 11 by 11 pixels , and an effective width of 9 pixels. These gradient operators are systematically applied at each possible pixel location throughout the current source image, according to well-known convolution methods. The magnitude of the sum of the squares of the respective gradient operator results is taken as the local gradient result, for each pixel throughout the current source image. In this case, the local gradient results are the edge-focus indicator results. Such techniques are known to one skilled in the art of image processing. , by Ramesh Jain, et al., McGraw Hill, 1995 discusses related teachings.","Then, in step S, the edge-focus indicator results are combined into meta-pixels. Next, in step S, an edge-focus indicator result representative of each meta-pixel is determined. The edge-focus indicated result can be determined, for example, by averaging all of the edge-focus indicator results encompassed by the meta-pixel. Typically, if meta-pixels are used with respect to one source image, then meta-pixels should be used with respect to each source image. Operation then continues to step S.","It should be appreciated that steps S and S are optional, depending, for example, on the magnification and resolution of the source images relative to the features on the imaged object, and\/or whether it is desirable to minimize the composite image processing time. If the composite image processing time is not important in a particular application, then operation simply skips from step S directly to step S.","In step S, the determined edge-focus indicator results are recorded or stored for subsequent use. It should be appreciated that, regardless of the spatial resolution, that is, whether at a pixel a pixel resolution or at a meta-pixel resolution, the edge-focus indicator results for the current source image encompass, and are generally spatially congruent with, the spatial locations included in the desired composite image. Then, in step S, a determination is made whether all the determined source images have been analyzed. If there are more source images to be analyzed then operation returns to step S. Otherwise, operations corresponding to step S of  are complete. In this case, operation continues to step S, where the operation continues to step S.",{"@attributes":{"id":"p-0085","num":"0084"},"figref":["FIG. 5","FIG. 3"],"b":["700","700","705","710","715"]},"Depending on the particular structure of the composite image processing according to this invention, it may be necessary to establish or reestablish which source image analysis results are to be further analyzed. These source image analysis results to be further analyzed may be determined or defined manually, in a semi-automated fashion, or fully automatically. In various exemplary embodiments, the source image analysis results determined or defined in a previous step, such as step S of , represent the source image analysis results to be further analyzed.","Similarly, depending on the particular structure of the composite image processing according to this invention, it may be necessary to establish or reestablish which spatial locations in the source image analysis results are to be further analyzed. In general, these spatial locations correspond to the spatial locations included in the desired composite image. These may be determined or defined manually, in a semi-automated fashion, or fully automatically. In various exemplary embodiments, the source image analysis results determined or defined in a previous step, such as step S of , represent the spatial locations to be further analyzed.","In step S, one of the spatial locations identified in step S which has not been previously selected for analysis according to this step S is selected as the current spatial location to be analyzed. Next, in step S, the source image analysis results data corresponding to each source image identified in step S, are accessed for the current spatial location to be analyzed. Stated another way, all of the spatially congruent source image analysis results are accessed for all source images, for a current spatial location.","Then, in step S, the accessed data is analyzed to determine or select the best source image analysis result, to maximize the composite image focus results. For example, if the source image analysis results are numerical edge-focus indicator results, a most appropriate one of the plurality of numerical edge-focus indicator results is selected. In various exemplary embodiments, the most appropriate numerical edge-focus indicator result is the maximum result. Operation then continues to step S.","In step S, a determination is made whether the selected source image analysis result is indicative of a valid edge. For example, if the source image analysis result is a numerical edge-focus indicator result, this result is compared to a numerical threshold value, where only values exceeding the threshold value are taken to be indicative of a valid and adequately-focused edge for purposes of constructing the composite image. If the source image analysis result is not indicative of a valid edge, then operation returns to step S. Otherwise, operation continues to step S.","In an alternative embodiment of the operations described relative to steps S and S, processing time is minimized by selecting the first processed source image analysis result which is indicative of a valid and adequately-focused edge for purposes of constructing the composite image. For example, if the source image analysis results are the previously-discussed numerical edge-focus indicator results, the plurality of numerical edge-focus indicator results is systematically compared to a numerical threshold value indicative of a valid and adequately-focused edge. Then, the first result to exceed the threshold is selected as a source image analysis result for constructing the composite image. In this case, following the selection of a result exceeding the threshold, operation continues to step S. Otherwise, if no source image analysis result is indicative of a valid edge, then operation returns to step S.","In step S, the determined source image analysis result indicative of a valid edge, and\/or an indication of the corresponding source image, such as an index number, is recorded or stored relative to the current spatial location. It should be appreciated that it is convenient to refer to the data elements and their respective associations recorded in step S as elements of a \u201ccomposite edge map\u201d. However, it should be understood that whenever the term \u201cmap\u201d is used herein, the term is being used only as a convenient reference for the underlying data elements and their respective associations, regardless of whether these recorded data elements are ever actually structured into a map in any conventional sense. It should be further understood that the scope of the systems and methods of this invention is in no way restricted to any particular representation of the basic underlying data and data associations disclosed herein.","Then, in step S, a determination is made whether all of the previously determined spatial locations to be analyzed have been analyzed. If there are more spatial locations to be analyzed, then operation returns to step S. Otherwise, operation continues to step S.","In step S, a determination is made whether to modify or refine the data in the composite edge map. The inventor has determined that with or without modifying the composite edge map, the systems and methods of this invention usefully suppress artifacts in a wide variety of composite images. However, as discussed in detail further below, the composite edge map can also be refined by various operations to further suppress artifacts in the resulting composite image. Thus, both alternatives are within the scope of the systems and methods of this invention. The decision to modify or refine the composite edge map data may depend, for example, on the magnification and\/or resolution of the source image relative to the features on the imaged object. These factors influence the level of detail visible in the source images, as well as the size of artifacts that may be present in out-of-focus image portions.","This decision may also or instead depend on, for example, whether the composite image processing time should be minimized. Depending on the particular structure of the composite image processing system and\/or method according to this invention, this decision may be made manually, in a semi-automated fashion, or fully automatically. If the composite image processing time is not important in a particular application, then, as a default condition it may be decided to modify or refine the composite edge map. Alternatively, if an operator has previously observed similar composite images of similar objects, as may typically happen in a repetitive manufacturing or inspection environment, the operator may make the decision at least in part based on the operator's previous observations. If it is decided not to modify or refine the composite edge map, then operation jumps to step S. Otherwise, if the composite edge map is to be modified or refined, operation continues to step S.","In step S the composite edge map is refined. That is, the data associated with spatial locations corresponding to the composite image, and more particularly, the data associated with spatial locations in and around the spatial locations corresponding to valid edge results in the previously determined composite edge map, is modified through one or more operations to further suppress artifacts in the resulting composite image and\/or to otherwise enhance the resulting composite image. A variety of well-known techniques used in known edge-detection methods, as well as any appropriate later-developed techniques, are usable according to various embodiments of the systems and methods according to this invention. For example, techniques based second-derivative operators may be used to \u201cthin\u201d edges in the composite edge map, thus removing some \u201cblur\u201d and other artifacts adjacent to edges. Additionally or alternatively, weak edges and fragments which are likely artifacts may be detected and eliminated by these methods. Such techniques are known to one skilled in the art of image processing. Machine Vision, by Jain, et al., discusses related teachings. After the data in the composite edge map is refined, operation continues to step S.","In step S, depending on the particular structure of the composite image processing system and\/or method according to this invention, the data in the composite edge map, whether refined or not, is assembled, and then stored and\/or output, as a representation usable to determine the edge pixels in the composite image. It should be appreciated that, regardless of the spatial resolution, that is, whether at pixel or meta-pixel resolution, this representation of the edge pixels in the composite image encompasses, and is generally spatially congruent with, the spatial locations included in the desired composite image. As shown in , operations corresponding to step S of  are complete at this point. In this case, operation continues to step S, where operation continues to step S.",{"@attributes":{"id":"p-0098","num":"0097"},"figref":["FIG. 6","FIG. 5"],"b":["750","750","751","752"]},"In step S, the data referred to as the composite edge map is overruled and\/or refined for spatial locations where \u201cweak\u201d edge-indicating data occurs within a prescribed distance of \u201cstrong\u201d edge-indicating data. A strong edge, such as, for example, an edge which leads to a large numerical value in a gradient analysis, which is partially out of focus in a first one of the source images, may still be analyzed as an edge, although it is better-focused and more realistically imaged in a different one of the source images. When the strong edge is out of focus, the strong edge may be detected in a displaced or broaden location. This tends to produce artifacts if the strong edge is admitted to the composite image in an out-of-focus state. In step S, at least some such artifacts not otherwise eliminated by the systems and methods of this invention are suppressed or eliminated.","In an exemplary embodiment, the edge-indicating data is in the form of the previously discussed gradient results combined into 5\u00d75 meta-pixels. Each spatial location in the data referred to as the composite edge map is systematically analyzed to remove the edge-indicating data or to set it to zero at any spatial location where the original value of the edge-indicating data at that spatial location is less than the average of its 4-connected spatial neighbors.","Then, in step S, the particular source images and\/or focal planes represented in the current composite edge map are determined. In a first exemplary embodiment, each unique source image index represented in the data of the current composite edge map is recorded for governing subsequent iterative operations. In a second exemplary embodiment, the indexes of the entire plurality of source images are used for governing subsequent iterative operations. Next, in step S, one of the source images determined in step S which has not been previously selected for analysis is selected as the current source image to be analyzed. In various exemplary embodiments, the current source image is identified by its corresponding source image index. Operation then continues to step S.","In step S, the data of the current composite edge map which corresponds to the current source image index is accessed or identified for evaluation. That is, all spatial locations in the current composite edge map which correspond to the current underlying source image are to be operated upon in the present iteration.","Next, in step S, all spatial locations in the current composite edge map which correspond to the current underlying source image are analyzed to eliminate edge-pixels which are not part of an extended \u201cline\u201d or a spot. This further qualifies and cleans up the edges included in the data corresponding to the current composite edge map. In one exemplary embodiment, the analysis is done with reference to the previously described combined 5\u00d75 meta-pixels, that is, with reference to \u201cedge meta-pixels\u201d. Each spatial location corresponding to an edge meta-pixel is systematically analyzed. Each 5\u00d75 edge meta-pixel is required to be part of a local group of four or more edge meta-pixels, where each meta-pixel of the local group must touch at least one other 4-connected neighbor in the local group. For spatial locations where isolated edge meta-pixels fail this criterion, the corresponding edge-indicating data and\/or source image index is removed or set to zero. Operation then continues with step S.","In an alternative exemplary embodiment, the previous embodiment is used except that each meta-pixel of the local group must touch at least one other 8-connected neighbor in the local group. In yet another alternative exemplary embodiment, each spatial location corresponding to an edge meta-pixel is systematically eroded with four structuring elements. Each structuring element is one meta-pixel wide and is four meta-pixels long in one of the horizontal, vertical and each diagonal direction, respectively. Following the erosion operation, each spatial location corresponding to an edge meta-pixel is systematically dilated with the same four structuring elements. This combination of operations is sometimes known as \u201copening\u201d in the field of image processing. Such techniques are known to one skilled in the art of image processing. Machine Vision, by Ramesh Jain, et al., McGraw Hill, 1995 discusses related teachings. For the eliminated meta-pixels, the corresponding edge-indicating data and\/or source image index is removed or set to zero in the data corresponding to the composite edge map.","The previous operations of steps S-S have been directed toward further refining the data corresponding the to the composite edge map, such that edge artifacts and noise corresponding to the current underlying source image are further suppressed. The composite edge map data remaining after step S of the current iteration is likely to corresponds to \u201ctrue\u201d edges that are reasonably well-focused in the current underlying source image.","In step S, the set of spatial locations corresponding to the current underlying source image in the current composite edge map as modified in step S, is spatially \u201cexpanded\u201d to add adjacent edge pixels to the current composite edge map. Such added adjacent pixels will broaden the edge images a bit, and are selected in step S from the same source image as the adjacent well-focused edge. Thus, a good composite image should result. In one exemplary embodiment, the set of previously described binned 5\u00d75 edge meta-pixels corresponding to the current underlying source image are systematically operated on with a 3\u00d73 structuring element to perform a dilation.","Next, in step S, for each spatial location where an added edge meta-pixel is obtained in step S, the corresponding edge-indicating data and\/or source image index is recorded in the data corresponding to the composite edge map.","Then, in step S, a determination is made whether all of the data corresponding to all of the previously determined source images and\/or source image indexes have been analyzed. If there is data remaining to be analyzed then operation returns to step S, otherwise operation continues at step S.","As a result of the previous operations of steps S\u2013S, the data corresponding to all source images underlying the current composite edge map have been analyzed, refined and the refined results recorded. It should be appreciated that, as a result of these previous operations, it may occur that, for some spatial locations to be included in the composite image, more that one underlying source image is indicated to correspond to a valid well-focused edge. Thus, in step S, for each spatial location having any corresponding edge-indicating data and\/or source image index recorded in the data corresponding to the composite edge map, a determination is made regarding which underlying source image corresponds to the best edge image. For example, in various exemplary embodiments of step S, this determination is made by identifying the maximum-valued edge-indicating data. Furthermore, in step S, the corresponding best edge-indicating data and\/or corresponding source image index is recorded in the data corresponding to the refined composite edge map as the basis for determining the composite image. Operation then continues to step S, where the operation continues to step S of .",{"@attributes":{"id":"p-0110","num":"0109"},"figref":["FIG. 7","FIG. 3"],"b":["800","800","810","800"]},"Next, in step S, one of the source images determined in step S which has not been previously selected for analysis is selected as the current source image to be analyzed. Then, in step S, the data is accessed for the current source image to be analyzed. More particularly, the data is accessed for the spatial locations within the current source image which correspond to the spatial locations included in the desired composite image. Operation then continues to step S.","In step S, the accessed source image data is analyzed to provide surface-focus indicator results and to provide a local result at a desired spatial resolution. In various exemplary embodiments, in step S, the accessed source image data is analyzed using a mathematical or algorithmic technique which is sensitive to and indicative of local surface focus in the image. The local surface focus can be indicated, for example, by the spatial distribution of either grayscale intensities or color intensities, in a local neighborhood. Operation then continues to step S.","In various exemplary embodiments of step S, the data for the current source image is systematically operated upon such that, at each possible spatial location, an average image value, for example a gray level, is determined from the all the image values included in a 5\u00d75 square of spatial locations centered on that spatial location. The resulting average local image value is subtracted from the image data value at that spatial location to create a residual value for each corresponding spatial location. For convenience, the resulting set of residual values and corresponding spatial locations is herein called a \u201cresidual image\u201d.","Next, in various exemplary embodiments, for each spatial location, the standard deviation of the residual values in a 5\u00d75 region centered on that spatial location is determined. The standard deviation is divided by the mean of the residual values in that region to determine a surface-focus indicator result for that spatial location. For the surface-focus indicator results of the foregoing exemplary embodiments, a low value for the indicator corresponds to a smooth or unfocused surface region. In contrast, a high value for the indicator corresponds to an in-focus textured region. Many other techniques and metrics indicative of well-focused surface regions are known to one skilled in the art of image processing. Such techniques may be used as alternatives to the foregoing exemplary embodiments, to provide surface focus indicators usable according to the systems and methods of this invention. , by Ramesh Jain, et al., McGraw Hill, 1995 discusses related teachings.","In step S, the surface-focus indicator results are combined into meta-pixels. Next, in step S, a surface-focus indicator result representative of each meta-pixel is determined. The surface-focus indicator result can be determined, for example, by averaging all of the surface-focus indicator results encompassed by the meta-pixel. Typically, if meta-pixels are used with respect to one source image, then meta-pixels should be used with respect to each source image. It should be appreciated that, by matching the combined surface meta-pixel size to a previously used or prescribed edge meta-pixel size, and by further insuring that the combined surface meta-pixels and the combined edge meta-pixels spatially align in a one-to-one correspondence, computations and record keeping according to the systems and methods of this invention can be simplified. Operation then continues to step S.","It should be appreciated that steps S and S are optional, depending on, for example, the magnification and resolution of the source images relative to the features on the imaged object, and\/or whether it is desirable to minimize the composite image processing time. If the composite image processing time is not important in a particular application, then operation simply skips steps S and S and continues directly from step S to step S.","It should be further appreciated that, in various exemplary embodiments, steps S\u2013S may be effectively combined and\/or may be indistinguishable. For example, in an alternative exemplary embodiment, the residual image is determined as described above with regard to step S. The residual image is then divided into a regular array of 5\u00d75 regions. For each 5\u00d75 region, the standard deviation of the residual values in that region is determined The determined standard deviation is divided by the mean of the residual values in that region to determine a surface-focus indicator result representative of that entire region and\/or spatial location. In this alternative exemplary embodiment, it should be noted that the 5\u00d75 region size is chosen to correspond to the 5\u00d75 edge meta-pixel size discussed in association with the exemplary edge-pixel operations described above. In this alternative exemplary embodiment, steps S and S of determining a surface-focus indicator result at one spatial resolution and then combining the result at another spatial resolution, are effectively merged.","In step S, the determined surface-focus indicator results are recorded or stored for subsequent use. It should be appreciated that, regardless of the spatial resolution, that is, whether at a pixel resolution or at a meta-pixel resolution, the surface-focus indicator results for the current source image encompass and are generally spatially congruent with, the spatial locations included in the desired composite image. Then, in step S, a determination is made whether all the determined source images have been analyzed. If there are more source images to be analyzed, then operation returns to step S. Otherwise, operation continues to step S, where the operation continues to step S.",{"@attributes":{"id":"p-0119","num":"0118"},"figref":["FIG. 8","FIG. 3","FIG. 3"],"b":["900","900","905","800","910"]},"In step S, a determination is made whether to perform additional operations to further suppress potential edge-artifacts in the composite image. The inventor has determined that, with or without such additional operations, the systems and methods of this invention usefully suppress artifacts in a wide variety of composite images. However, the operations discussed below with regard to step S can further suppress image artifacts which might otherwise appear in the composite image and which correspond to out-of-focus edges or boundaries included in the plurality of source images. Thus, both alternatives are within the scope of the systems and methods of this invention.","The decision to further suppress potential edge-artifacts in the composite image may depend on, for example, the magnification and\/or resolution of the source image relative to the features on the imaged object. These factors influence the level of detail visible in the source images, as well as the size of artifacts that may be present in out-of-focus image portions. This decision may also or instead depend on, for example, whether the composite image processing time should be minimized. Depending on the particular structure of the composite image processing system and\/or method according to this invention, the decision may be made manually, in a semi-automated fashion, or fully automatically.","If the composite image processing time is not important in a particular application, then, as a default condition, it may be decided to further suppress potential edge-artifacts in the composite image. Alternatively, if an operator has previously observed similar composite images of similar objects, as may typically happen in a repetitive manufacturing or inspection environment, the operator may make the decision at least in part based on the operator's previous observations. If it is decided not to further suppress potential edge-artifacts in the composite image, then operation jumps to step S. If it is decided to further suppress potential edge-artifacts in the composite image, then operation continues with step S.","In step S, operations are performed to facilitate further suppressing potential edge-artifacts during subsequent surface-pixel determination operations. Then, in step S, the spatial locations to be analyzed in the determined source image analysis results are determined.","In various exemplary embodiments of step S, since strong edges can create artifacts in immediately adjacent regions of an image when such strong edges are out of focus, the data corresponding to the current composite edge map is analyzed to determine a set of spatial locations adjacent to the edges included in the current composite edge map. These spatial locations are treated as edge-artifact buffer zones during subsequent surface-pixel determination operations. In one exemplary embodiment, the data corresponding to the current composite edge map corresponds to the previously described 5\u00d75 combined edge meta-pixels. Operations are performed which are essentially equivalent to dilating the spatial locations corresponding to the current composite edge map using a 5\u00d75 structuring element, that is, 5\u00d75 in terms of meta-pixels. Methods of dilating a set of spatial locations are known to one skilled in the art of image processing. Machine Vision, by Jain, et al., discusses related teachings.","For the added spatial locations corresponding to the results of the dilation operation, that is, excluding the previous elements already included in the composite edge map, special data and\/or values are assigned and\/or recorded. The special data and\/or values identify these spatial locations as comprising edge-artifact buffer zones during further surface-pixel determining operations. Special rules are then applied to determine surface pixels in the edge-artifact buffer zone. For example, in an exemplary embodiment described in greater detail below, surface-pixel regions may \u201cgrow\u201d into the edge-artifact buffer zone. However, a surface-pixel region is not allowed to start from a seed pixel in the edge-artifact buffer zone.","It should be appreciated that the previously described edge-artifact buffer zones may be alternatively described and determined as zones generally surrounding each edge included in the current composite edge map. In various exemplary embodiments, each zone has an outer zone-boundary, which is generally located at a prescribed setback distance from the edge pixels that the zone surrounds. The prescribed setback distance from the edge is generally prescribed in units of pixels, meta-pixels, or any set of units usable according to the systems and methods of this invention.","Depending on the particular structure of the composite image processing system and\/or method according to this invention, it may be necessary to establish or reestablish in step S which spatial locations in the source image analysis results are to be further analyzed. In general, these spatial locations correspond to the spatial locations included in the desired composite image. These may be determined or defined manually, in a semi-automated fashion, or fully automatically. In general, these spatial locations correspond to the spatial locations included in the desired composite image.","In various exemplary embodiments, the source image analysis results determined or defined in a previous step, such as in step S of , represent the spatial locations to be further analyzed. If edge-artifact buffer zones have been determined as described below, then the spatial locations corresponding to those edge-artifact buffer zones are also included to be further analyzed. Furthermore, optionally, the spatial locations already determined corresponding to the composite edge map may be excluded from the spatial locations to be further analyzed.","In step S, one of the spatial locations determined in step S which has not been previously selected for analysis is selected as the current spatial location to be analyzed. Next, in step S, the source image analysis results data corresponding to each source image determined in step S are accessed for the current spatial location to be analyzed. Stated another way, all of the spatially congruent source image analysis results are accessed for all source images, for the current spatial location. Then, in step S, the accessed data is analyzed to determine or select the best source image analysis result, to maximize the composite image focus results. For example, if the source image analysis results are numerical surface-focus indicator results, a most appropriate one of the plurality of numerical surface-focus indicator results is selected. In various exemplary embodiments, the most appropriate numerical surface-focus indicator result is the maximum result. Operation then continues to step S.","In step S, a determination is made whether the selected source image analysis result is indicative of a valid surface. For example, if the source image analysis result is a numerical surface-focus indicator result, this result is compared to a numerical threshold value, where only values exceeding the threshold value are taken to be indicative of a valid and adequately-focused surface for purposes of constructing the composite image. If the source image analysis result is not indicative of a valid surface, then operation returns to step S. Otherwise, operation continues to step S. Thus, in embodiments where the most appropriate numerical surface-focus indicator result in step S is the maximum result, then the result is a maximum result that also exceeds the threshold.","In step S, the determined source image analysis result indicative of a valid surface, and\/or an indication of the corresponding source image, such as an index number, is recorded or stored relative to the current spatial location. Then, in step S, a determination is made whether all of the previously determined spatial locations to be analyzed have been analyzed. If there are more spatial locations to be analyzed, then operation returns to step S. Otherwise, operation continues to step S.","It should be appreciated that it is convenient to refer to the data elements and their respective associations recorded in step S as elements of a \u201ccomposite surface map\u201d. However, it should be understood that whenever the term \u201cmap\u201d is used herein, the term is being used only as a convenient reference for the underlying data elements and their respective associations, regardless of whether these recorded data elements are ever actually structured into a map in any conventional sense. It should be further understood that the scope of the systems and methods of this invention is in no way restricted to any particular representation of the basic underlying data and data associations disclosed herein.","In step S, a determination is made whether to modify or refine the data in the composite surface map. The inventor has determined that, with or without modifying the composite surface map, the systems and methods of this invention usefully suppress artifacts in a wide variety of composite images. However, as discussed in detail further below, the composite surface map can also be refined by various operations to further suppress artifacts in the resulting composite image. Thus, both alternatives are within the scope of the systems and methods of this invention. The decision to modify or refine the composite surface map data may depend on, for example, the magnification and\/or resolution of the source image relative to the features on the imaged object. These factors influence the level of detail visible in the source images, as well as the size of artifacts that may be present in out-of-focus image portions.","This decision may also or instead depend on, for example, whether the composite image processing time should be minimized. Depending on the particular structure of the composite image processing system and\/or method according to this invention, this decision may be made manually, in a semi-automated fashion, or fully automatically. If the composite image processing time is not important in a particular application, then, as a default condition it may be decided to modify or refine the composite surface map. Alternatively, if an operator has previously observed similar composite images of similar objects, as may typically happen in a repetitive manufacturing or inspection environment, the operator may make the decision at least in part based on the operator's previous observations. If it is decided not to modify or refine the composite surface map, then operation jumps directly to step S. Otherwise, if the composite surface map is to be modified or refined, operation continues to step S.","In step S, the composite surface map is refined. That is, the data associated with spatial locations corresponding to the composite image, and more particularly, the data associated with spatial locations in and around the spatial locations corresponding to valid surface results in the previously determined composite surface map, is modified through one or more operations to further suppress artifacts in the resulting composite image and\/or to otherwise enhance the resulting composite image. Operation continues with step S.","A variety of known image-enhancement techniques, as well as any appropriate later-developed techniques, are usable in step S according to various embodiments of the systems and methods according to this invention. For example, in various previously described exemplary embodiments, where a numerical surface-focus indicator result is compared to a numerical threshold value in step S to determine valid surface pixels, the data corresponding to the composite surface map may, in some cases, include voids surrounded by valid surface pixels. In this case, known void-filling techniques may be used to fill such voids. Such techniques are known to one skilled in the art of image processing. , by Jain, et al., discusses related teachings. However, because there are a plurality of source images which may potentially be used as the basis for the pixels values to be used to fill a void in the composite image, known methods for void filling must be supplemented with an additional method for selecting which source image, or source images, will provide the basis for filling a void.","In one exemplary embodiment of an additional method for selecting which source image, or source images, will provide the basis for filling a void, an erosion operation is followed by a dilation operation according to known techniques. Then, for each void which is filled according to these operations, the void is filled based on the source image index which is closest to the average of the source image indexes of the spatial locations which surround the void. In other exemplary embodiments, region-growing methods are used. Known region growing methods may be used, and such techniques are known to one skilled in the art of image processing. , by Jain, et al., discusses related teachings. However, similarly to known void-filling methods, known region growing methods must be also be supplemented with an additional method for selecting which source image, or source images, will provide the basis for the pixels which will be added to the growing region. An exemplary embodiment of region-growing operations usable for refinement of the composite surface map is described with reference to , further below.","In step S, depending on the particular structure of the composite image processing system and\/or method according to this invention, the data in the composite surface map, whether refined or not, is stored and\/or output, as a representation usable to determine the surface pixels in the composite image. Operation continues to step S, where operation continues to step S.","In various exemplary embodiments, the data corresponding to the composite surface map is combined with the data corresponding to the previously-determined composite edge map and\/or composite image The combined surface and edge-pixel data is stored and\/or output as a representation usable to determine the composite image. It should be appreciated that, regardless of the spatial resolution, that is, whether at pixel or meta-pixel resolution, this representation of the surface pixels and\/or edge pixels in the composite image encompasses, and is generally spatially congruent with, the spatial locations included in the desired composite image each spatially congruent surface and\/or edge pixel of the composite image may thus be determined at a pixel resolution from this representation.","For example, in various exemplary embodiments, surface-pixel and\/or edge-pixel data is represented in the form of surface- and\/or edge-meta-pixels, respectively, in the composite image representation. Each surface and\/or edge pixel that is spatially congruent with a particular surface and\/or edge meta-pixel, respectively, is given an image value based on the corresponding spatially congruent pixel in the source image corresponding to the source image index of that surface-meta-pixel.",{"@attributes":{"id":"p-0141","num":"0140"},"figref":["FIG. 9","FIG. 8"],"b":["960","960","961","962"]},"In step S, a determination is made whether operations have previously been performed to identify spatial locations comprising edge-artifact buffer zones, for example, as described above with respect to steps S\u2013S of . If it is determined that operations have not previously been performed to identify spatial locations comprising edge-artifact buffer zones, or if, in various exemplary embodiments, such previous operations are ignored, then operation jumps to step S. Otherwise, if it is determined to use previously determined edge-artifact buffer zones in subsequent operations to further suppress potential edge-artifacts in the composite image, then operation continues with step S.","In step S, the current data representation of the edge-artifact buffer zones is accessed or re-accessed as necessary so that spatial locations included in the zones may be identified and filled according to special buffer zone filling operations in subsequent steps for determining\/refining surface pixels in the composite image. Then, in step S, the data corresponding to the current composite surface map is segmented into surface regions exhibiting limited focal plane range. That is, the current composite surface map is segmented into surface regions where the image data for all the spatial locations included in a surface region corresponds to a limited number of spatially adjacent source image focal planes. Operation then continues with step S.","It should be appreciated that, in various exemplary embodiments, if operations corresponding to step S are performed, then the data corresponding to the edge-artifact buffer zones are included in the current composite surface map. It should be further appreciated that, in various exemplary embodiments, spatially adjacent source image focal planes are conveniently indicated by sequential source image indexes. In one exemplary embodiment, the data corresponding to the current composite surface map is in the form of the previously described 5\u00d75 surface meta-pixels and generally includes a corresponding source image index which applies to each included pixel. In this exemplary embodiment, a recursive region-growing algorithm is applied to the data corresponding to the current composite surface map.","In one exemplary embodiment of this recursive region-growing technique, the current composite surface map data is systematically searched for the meta-pixel spatial location having the maximum surface-focus indicator value. For the reasons described below, the maximum surface-focus indicator value should also be greater that zero. Then, that meta-pixel spatial location is set as the primary seed for growing a surface region having a limited focal plane range. Next, the region is grown around the primary seed by including in the region all 4-connected neighbors of the primary seed which have a source image index which is within a range of plus or minus two increments of the source image index of the primary seed. Subsequently, each neighbor added to the region is taken as a secondary seed Each of the 4-connected neighbors of each secondary seed which have a source image index which is within a range of plus or minus two of the source image index of the primary seed are also included in the region. Each such added neighbor becomes an additional secondary seed. This is repeated until the region can grow no further.","In various exemplary embodiments, if the total region size so determined is less than a threshold size, such as, for example, three meta-pixels, then the elements of that region are assigned a surface-focus indicator value of zero, and are returned to the pool of unassigned elements. The step of requiring a minimum threshold size tends to suppresses small surface artifacts due to noise and\/or minute out-of-focus surface particles, and the like. Otherwise, any spatial locations which have been associated with a region in any current or previous region-growing iteration are not considered in subsequent recursions.","When a region can grow no further, the recursive algorithm repeats for a new region, beginning with the search for the next spatial location having the maximum surface-focus indicator value. This next spatial location becomes a new primary seed for the new region. The recursive region-growing algorithm is repeated until all spatial locations included in the current composite surface map are included in the surface regions exhibiting limited focal plane range, or for a number of iterations predetermined to leave only a \u201cvisually insignificant\u201d number of scattered pixels which are not associated with a surface region of limited focal plane range.","Throughout the recursive region growing algorithm, in various exemplary embodiments, the primary seeds are not generally selected corresponding to spatial locations in any edge-artifact buffer zone, so that out-of-focus edge artifacts can be further suppressed. In one exemplary embodiment, this is accomplished by assigning a special value of zero as the surface-focus indicator value for these edge-artifact buffer zone spatial locations, such that no such edge-artifact buffer zone spatial location will ever be a \u201cmaximum seed\u201d.","However, it should be appreciated that it is generally desirable that spatial locations in edge-artifact buffer zones, as well as spatial locations in the regions that failed to attain the minimum size threshold described above, should be allowed to join adjacent surface regions that grow to include them. This is done simply because such spatial locations are most likely to be best-focused at the same focal plane as the immediately adjacent surface region. Thus, throughout the recursive surface region-growing algorithm, whenever a region grows to include a neighbor that has a surface-focus indicator value of zero, that neighbor is included in the region, regardless of the source image index of the current primary seed.","It should be appreciated that, in the foregoing description of exemplary operations of the step S, the range of source image indexes selected as the basis for admitting neighbors into a region will, in general, depend on the depth of focus of the plurality of source images relative to the focal plane step between adjacent source images. For example, in various respective exemplary embodiments, the nominal focal plane range allowed in a surface region may be set so that it does not extend beyond 1, 2, or 3 times the depth of field of the source image corresponding to the primary seed. In cases where the source image focal planes are relatively closely spaced, the number of source image indexes corresponding to the nominal focal plane range allowed in a region will be relatively greater. In contrast, in cases where the source image focal planes are relatively coarsely spaced, the number of source image indexes corresponding to the nominal focal plane range allowed in a region will be relatively fewer. It should also be appreciated that, in various alternative embodiments, 8-connected neighbors may be considered instead of the 4-connected neighbors used in the previously described surface region-growing operations.","It should be further appreciated that other surface region membership criteria may be added to overrule and\/or supplement the simple source image index criteria described above, and that larger regions may thus result. For example, in a further exemplary embodiment, the distance of any particular neighbor from the primary seed is considered, and\/or with the difference in the source image indexes between the neighbor and the primary seed, along with the associated surface-focus indicator values of the neighbor and the primary seed. The desired relationship between these various factors, which serves as the criteria for admitting a neighbor to a region, is determined by trial and error on a number of images representative of the expected application of the composite image system. In such an embodiment, slowly undulating or inclined surface portions which do not include a high degree of distinctive texture may be included in a region and a desirable composite image may still be obtained.","It should also be appreciated that, in various exemplary embodiments, spatial voids may remain in the data corresponding to the composite surface map resulting from the foregoing surface-pixel determining operations. Thus, in such exemplary embodiments, operations are performed to identify such voids and assign the source image index of the nearest neighbor to the spatial locations associated with each void. If there is more than one nearest neighbor, then the source image index of any nearest neighbor may be assigned to the void, according to any technique which is convenient in the processing operations used to determine the nearest neighbor.","In step S, each of the surface regions determined in the operations of the previous step S are analyzed to determined the average source image index for each surface region. For each surface region, the average source image index for that region is the average of all the source image indexes of all the spatial locations included in that region.","Next, in step S, for each of the surface regions analyzed in steps S\u2013S, all of the spatial locations included in a region are assigned the source image index which is closest in value to the average source image index for that region. The average source image index corresponds to the source image having the focal plane which is taken to be most representative of all the respective focal planes corresponding to the respective spatial locations included in the region. Operation then continues to step S, where operation continues to step S.","It should be appreciated that when the surface pixels of the composite image are constructed based on the source image indexes assigned in step S, each portion of the composite image corresponding to a particular surface region will thus be based on a single source image. In this way, the foregoing exemplary operations suppress or eliminate at least some out-of-focus artifacts not otherwise eliminated by the systems and methods of this invention.","As previously described, following execution of step S of , operation then continues to step S of . In one exemplary embodiment of the operations corresponding to step S of , a composite image representation resulting from the preceding steps of  may still contain voids. That is, a limited number of spatial locations may remain where the composite image pixels remain undetermined. For example, depending on the details of the particular surface pixel determining operations used, and especially in various exemplary embodiments which implement edge-artifact buffer zones, voids may be likely to occur at spatial locations near the edge-artifact buffer zones. In such a case, filling these and any other voids in the current composite image representation is preferably performed as part of step S.","In one exemplary embodiment of operations corresponding to step S, the source images and\/or source image indexes corresponding to all determined pixels in the composite image are known according to methods previously described. To fill the voids, the spatial locations in the composite image representation corresponding to each unique source image and\/or source image index included in the composite image representation are operated on in a separate iteration of the following operation: Initially, one of the unique source image indexes is chosen as the source image index for the current iteration. Then the spatial locations in the composite image representation corresponding to that index are dilated using a structuring element of a prescribed size, for example 3\u00d73 pixels, according to known methods of dilation. For each spatial location \u201cadded\u201d by the dilation operations of the current iteration, the current source image index is assigned as the basis for that spatial location in the composite image. However, it should be appreciated that, in this exemplary embodiment, a subsequent dilation operation never overrules the basis for a spatial location which has been previously determined in the composite image representation. Then these operations are repeated for a different source image index, until all source image indexes included in the composite image representation have been processed. If all voids are not filled after one pass through a set of iterations corresponding to all the source image indexes included in the representation of the composite image, the entire process is repeated.","In an alternative exemplary embodiment of the preceding operations, a subsequent dilation operation may overrule a source image index basis for a spatial location which was previously determined, but only for spatial locations which were voids filled by a previous dilation operation. In one exemplary embodiment, if a particular spatial location has been assigned two or more source image indexes after a particular pass through all the included source image indexes, then the source image index corresponding to the highest valued surface-focus indicator, or edge-focus indicator, respectively, is used as the basis for that spatial location. Furthermore, if all corresponding indicators are not of the same type, then the source image index corresponding to the best surface-focus indicator is used.","Returning now to a further discussion of operations corresponding to steps S and\/or S of , in various exemplary embodiments of the systems and methods according to this invention, artifacts in the composite image are further suppressed and the composite image processing time is shortened when additional preliminary information is available regarding the portion of the object to be shown in the composite image. For example, information such as a CAD file representing an object, or a previous composite image of a substantially identical object, is frequently available in an industrial applications of machine vision systems.","In the case of a CAD file representation, it should be appreciated that the locations of edges and boundaries in the CAD file representation may be determined manually, in a semi-automated fashion, or fully automatically from a CAD representation, by a variety of known methods of CAD file feature extraction. In such a case, the spatial locations of the corresponding edges and boundaries in a current set of source images of a corresponding object may then be determined by a further variety of known manual, semi-automated, or automated methods of spatial congruence and\/or feature congruence image processing. These methods may include, for example, coordinate matching, pattern matching, template matching, and the like. For example, such methods are routinely used for the inspection of the positions of edges and boundaries on objects in a variety of commercially-available machine vision systems, such as the Quick Vision series of vision inspection machines available from Mitutoyo America Corporation (MAC), located in Aurora, Ill.","When a desirable previous composite image according to the systems and methods of this invention is available, then the spatial locations of edge-pixels in that composite image are also available. Such a desirable previous composite image, of a substantially identical object, may be made spatially congruent with the current source images according to various well-known methods. These methods may include, for example, coordinate matching, pattern matching, template matching, image correlation, and the like. Thus, the corresponding spatial locations of edges in the current source images may be identified. In the case of such pre-existing CAD and\/or desirable composite image representations, all operations described herein for determining edge-pixels in the composite image may be restricted to spatial locations in the vicinity of the determined edges in the pre-existing CAD or desirable composite image representations. Thus, artifacts and aberrations that might otherwise occur due to edge-analysis operations in other parts of the images are effectively suppressed. In addition, the related composite image processing considers fewer spatial locations and proceeds faster.","Turning now to ,  is a first source image having a focal plane located at a first plane of an electronic assembly object, exemplifying the source images usable to construct a composite image according to the systems and methods of this invention. It should be noted that the source image shown in  is relatively well-focused at the right and left sides of the source image, but generally out-of-focus in the center portion.  is a second source image having a focal plane located at a second plane of the electronic assembly object, again exemplifying source images usable to construct the composite image according to the systems and methods of this invention. It should be noted that the source image shown in  is relatively well-focused at the central areas of the source image, but is generally out-of-focus at the left and right edge portions. The source images of  were acquired using a commercially available vision system from the Quick Vision series of vision inspection machines available from Mitutoyo America Corporation (MAC), located in Aurora, Ill.","These two typical sources images are representatives from a plurality of eleven source images, wherein each pair of adjacent source images was separated by a 0.1 mm focal plane step. The gray scale information for the plurality of eleven source images was captured using a 2.5\u00d7magnification lens, the system camera, and operator-determined lighting settings. The source image shown in  corresponds to the assigned source image index of , while the source image shown in  corresponds to the assigned source image index of . Thus, these two images correspond to focal planes which are separated by 0.5 mm.",{"@attributes":{"id":"p-0164","num":"0163"},"figref":["FIG. 12","FIG. 12","FIG. 12"]},{"@attributes":{"id":"p-0165","num":"0164"},"figref":["FIG. 13","FIG. 12","FIG. 13","FIG. 13","FIG. 13"]},"It should be appreciated that, the control system portion  shown in  is, in various exemplary embodiments, implemented using a programmed general purpose computer. However, the control system portion  can also be implemented on a special purpose computer, a programmed microprocessor or microcontroller and peripheral integrated circuit elements, an ASIC or other integrated circuit, a digital signal processor, a hardwired electronic or logic circuit such as a discrete element circuit, a programmable logic device such as a PLD, PLA, FPGA or PAL, or the like. In general, any device, capable of implementing a finite state machine that is in turn capable of implementing the operations corresponding to the flowcharts shown in , and as otherwise described herein, can be used to implement the control system portion . The particular form each of the circuits shown in the control system portion  of  will take is a design choice and will be obvious and predictable to those skilled in the art.","In , alterable portions of the memory  in various exemplary embodiments, whether volatile or non-volatile, can be implemented using any one or more of static or dynamic RAM, a floppy disk and disk drive, a writeable or rewriteable optical disk and disk drive, a hard drive, flash memory or the like. In , the generally static portions of the memory  are, in various exemplary embodiments, implemented using ROM. However, the static portions can also be implemented using other non-volatile memory, such as PROM, EPROM, EEPROM, an optical ROM disk, such as a CD-ROM or DVD-ROM, and disk drive, flash memory or other alterable memory, as indicated above, or the like. Thus, in , the memory  can be implemented using any appropriate combination of alterable, volatile or non-volatile memory or non-alterable, or fixed, memory.","Moreover, the control system portion  can be implemented as software executing on a programmed general purpose computer, a special purpose computer, a microprocessor or the like. In this case, the control system portion  can be implemented as a routine embedded in the vision system , as a resource residing on a server, or the like. The control system portion  can also be implemented by physically incorporating it into a software and\/or hardware system.","While this invention has been described in conjunction with the exemplary embodiments outlined above, it is evident that many alternatives, modifications and variations will be apparent to those skilled in the art. Accordingly, the exemplary embodiments of the invention, as set forth above, are intended to be illustrative, not limiting. Various changes may be made without departing from the spirit and scope of the invention."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["Various exemplary embodiments of this invention will be described in detail, with reference to the following figures, wherein:",{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":["FIG. 2","FIG. 1"]},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":["FIG. 4","FIG. 3"]},{"@attributes":{"id":"p-0029","num":"0028"},"figref":["FIG. 5","FIG. 3"]},{"@attributes":{"id":"p-0030","num":"0029"},"figref":["FIG. 6","FIG. 5"]},{"@attributes":{"id":"p-0031","num":"0030"},"figref":["FIG. 7","FIG. 3"]},{"@attributes":{"id":"p-0032","num":"0031"},"figref":["FIG. 8","FIG. 3"]},{"@attributes":{"id":"p-0033","num":"0032"},"figref":["FIG. 9","FIG. 8"]},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":["FIG. 12","FIGS. 9 and 10"]},{"@attributes":{"id":"p-0037","num":"0036"},"figref":["FIG. 13","FIG. 12","FIGS. 9 and 10"]}]},"DETDESC":[{},{}]}
