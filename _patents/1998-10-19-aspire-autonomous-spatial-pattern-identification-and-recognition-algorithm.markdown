---
title: Aspire (autonomous spatial pattern identification and recognition algorithm)
abstract: An automatic endmember classification algorithm for hyperspectral data cubes. This algorithm is an improved version of a pattern recognition technology which was developed at Massachusetts Institute of Technology (MIT). The MIT algorithm is called the Extended Cross Correlation (XCC) technique, and it was designed to separate patterns from time resolved spectroscopic data. ASPIRE uses XCC as one of its core algorithms, but it features many improvements. These include: the use of Principle Components Analysis (PCA) to preprocess the data, and automatic endmember searching algorithm, and a Bayesian algorithm which is used to unmix the end-members. This invention also represents a new use of the XCC technology, because it had never before been used to identify spatial targets and patterns in hyperspectral data.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=06952499&OS=06952499&RS=06952499
owner: The United States of America as represented by the Secretary of the Air Force
number: 06952499
owner_city: Washington
owner_country: US
publication_date: 19981019
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["STATEMENT OF GOVERNMENT INTEREST","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT"],"p":["The invention described herein may be manufactured and used by or for the Government for governmental purposes without tile payment of any royally thereon.","The present invention relates generally to pattern recognition systems, and more specifically the invention pertains to an automatic endmember classification algorithm for hyperspectral data cubes. This algorithm has been given the name ASPIRE (Autonomous Spatial pattern Identification and REcognition algorithm).","Much of the pattern recognition data analyzed is complicated, because it often consists of overlapping spectral\/spatial patterns which are difficult to separate. When studying the underlying physics and chemistry of this data, it is often necessary to first unmix (or separate) these patterns. By the end of 1997, Steve Coy's group at MIT demonstrated that their pattern recognition algorithm, Extended Cross-Correlation (XCC), can be successfully used to separate an optically thick, v=1, CO band from the other overlapping bands in the spectra. This success has motivated us to produce an in-house capability to use the XCC algorithm.","Prior art pattern recognition techniques are described in the following eight references, the disclosures of which are incorporated herein by reference:\n\n","The present invention includes an automatic endmember classification algorithm for hyperspectral data cubes. This algorithm has been given the name ASPIRE (Autonomous Spatial Pattern Identification and REcognition algorithm). This algorithm is an improved version of a pattern recognition technology which was developed at Massachusetts Institute of Technology (MIT). The MIT algorithm is called the Extended Cross Correlation (XCC) technique, and it was designed to separate patterns from time resolved spectroscopic data. ASPIRE uses XCC as one of its core algorithms, but it features many improvements. These include: the use of Principle Components Analysis (PCA) to preprocess the data, an automatic endmember searching algorithm, and a Bayesian algorithm which is used to unmix the end-members. This invention also represents a new use of the XCC technology, because it had never before been used to identify spatial targets and patterns in hyperspectral data.","Another aspect of the invention includes a flexible program that has been created to implement the ASPIRE algorithm. The code can be used in either a user interactive mode, or in an automatic batch processing mode. Users interact with the code through a simple command line interface. User commands can also be placed into a text file, called a macro. The code has the ability to execute these macros by performing a line-by-line execution of all the command within the macro. The program also has the ability to display graphical output. The PGPLOT graphics library was used to create all of the graphics produced by the program.","As mentioned above, the present invention includes an automatic endmember classification algorithm for hyperspectral data cubes. This algorithm has been given the name ASPIRE (Autonomous Spatial Pattern Identification and REcognition algorithm). This algorithm is an improved version of a pattern recognition technology which was developed at Massachusetts Institute of Technology (MIT). The MIT algorithm is called the Extended Cross Correlation (XCC) technique, and it was designed to separate patterns from time resolved spectroscopic data. ASPIRE uses XCC as one of its core algorithms, but it features many improvements. These include: the use of Principle Components Analysis (PCA) to preprocess the data, an automatic endmember searching algorithm, and a Bayesian algorithm which is used to unmix the end-members. This invention also represents a new use of the XCC technology, because it had never before been used to identify spatial targets in hyperspectral data.","In this description, it is assumed that the reader is familiar with the XCC technique. If this is not the case, I highly recommend reading the MIT documents mentioned above before continuing this literature, which is solely devoted to describing the LABCEDE implementation of the XCC algorithm. This implementation consists of a flexible FORTRAN program which is currently designed to run on a Unix based workstation. The discussion below gives a description of how to use this program as well as some practical examples. The program has been used in several different analyses. Two of the most prominent analyses are the analysis of the LABCEDE CO data, and the analysis of HYDICE hyperspectral data cubes.","The technical details about the code have been place into the appendix. These sections refer to the overall structure of the code, as well as the details as to how the code performs the pattern unmixing.","The program written to execute the XCC algorithm has been named, appropriately, XCC. This program was written and designed with the following goals in mind:\n\n","It was decided that XCC would be written in FORTRAN 77 since some compilers for this language are available on many different platforms. XCC uses the PGPLOT graphics library to produce all of its plots and images. These routines are a collection of FORTAN subroutines which give one the ability to easily produce graphical output.","A flexible user interface was essential for XCC. This is due to the fact that the user must have the ability to analyze a variety of different data sets, and have the ability to tailor the application to the specific needs of the analysis. Therefore, it was decided that XCC should have a command line interface which is similar in look and feel to IDL. This gives the user complete flexibility in how the analysis is performed. A listing of the available XCC commands are given below.","XCC has the ability to process data in an automated fashion. Once an initial analysis of a data set has been performed, the XCC commands necessary to perform this analysis can be placed into an ASCII text file. This file (called a macro) can be used by XCC in future analysis. When executing a macro, XCC reads the file line-by-line executing each command. Therefore, repeated experiments can be quickly processed by simply executing the appropriate macro. Furthermore, every time XCC is run, the macro \u2018last. mac\u2019 is created. This macro contains a listing of every command which was used during the most recent XCC session. This file can be used to record one's work for future reference, or as a way of creating a macro for future use.","Two versions of XCC have been produced: LARGE and SMALL. The SMALL version is the original program which was designed to handle the relatively small data sets produced at the LABCEDE facility. The LARGE version is a modified version which has been designed to handle extremely large hyperspectral data sets. To accommodate these large data sets, several of the features available in the SMALL version were removed from the LARGE version. The discussion below gives a listing of which XCC commands are not available in the LARGE version. A more detailed description of the differences between the LARGE and SMALL versions of the FORTRAN code are given in Appendix A.","Thus far, XCC has only been tested on a DEC Alpha platform using the Unix operating system. In the future, it may become necessary to port the code to other platforms, but this has not yet been accomplished. Before compiling XCC, the PGPLOT graphical subroutine library must be installed. Detailed information on the installation of this package can be obtained over the Internet. Compiling the XCC executable consists of the following steps.\n\n",{"@attributes":{"id":"p-0018","num":"0030"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"168pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"c",{}]},{"entry":[{},"c","Help file information"]},{"entry":[{},"c"]},{"entry":[{},{},"integer PATHL"]},{"entry":[{},{},"parameter(PATHL = 23)"]},{"entry":[{},{},"character*(PATHL) help_path"]},{"entry":[{},{},"data help_path\/\u2018\/data2\/vititoe\/XCC\/doc\/\u2019\/"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}},"ul":{"@attributes":{"id":"ul0004","list-style":"none"},"li":["\u2003The variable help_path gives the directory which contains the XCC documentation files. In this directory will be the files, menu.tex and xcc_help.tex. These files, which have been used to create this document, are also used by XCC to display interactive help information. If necessary, update help_path.","2. Edit the Makefile so that the variable PGPDIR points to the location of the PGPLOT libraries. This variable declaration should be found at the top of the Makefile.","3. Type \u2018make clean\u2019 to erase all of the stale object files.","4. To create the LARGE version of XCC type \u2018make large\u2019. Alternatively, type \u2018make small\u2019 to create the SMALL version of the code. This sets up the Makefile so that a production version of the executable will be created. If one wishes to create a debugging version of the code, then one should use the commands \u2018make large\u2019 and \u2018make dsmall\u2019 respectively.","5. Create the executable with the command \u2018make xcc\u2019. This will create the binary file, xcc, in the source directory.","6. Copy the binary file into a directory which can be accessed by others. A good choice for this is \/usr\/local\/bin\/. Everyone wishing to use XCC should have this directory in their PATH."]}},"XCC is a flexible program which accepts commands through it's command line interface. Executing the code gives one the following: irwin.plh.af.mil>xcc",{"@attributes":{"id":"p-0020","num":"0038"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"196pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"************************************************"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"154pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"*",{},"*"]},{"entry":[{},"*","XCC Pattern Identifier","*"]},{"entry":[{},"*","----------------------","*"]},{"entry":[{},"*",{},"*"]},{"entry":[{},"*","(Large Version)","*"]},{"entry":[{},"*",{},"*"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"196pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"************************************************"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}},"br":{}},"As mentioned earlier, there are two version of this code, LARGE and SMALL. The above example is taken from the LARGE version of the code and is clearly marked as such. Had this been the SMALL version, it would have been marked accordingly.","The following two sections list all of the XCC commands. These commands have been split into two sections. The first lists all menu specific commands. Commands which relate to the XCC recognition algorithm are given below.","In each of these sections, the following conventions shall be used:\n\n","The name of the \u2018command\u2019 is given first and is followed by various required and optional qualifiers. The qualifiers contained within parentheses, \u2018( )\u2019, are required. If this information is not supplied on the command line, XCC with interactively prompt the user for this information. Qualifiers which are enclosed within brackets, \u2018[ ]\u2019, are optional. This naming convention will be used throughout the following two sections.","Examples of how these menu commands can be used to analyze data are given below. In these sections, the XCC program will be demonstrated with a LABCEDE CO data set and a HYDICE hypercube respectively.","The following commands manipulate the menu system. They do not alter, display, or interact in any way with the data being manipulated by XCC.\n\n","Each of the commands in this section are designed to help one analyze data with the XCC pattern recognition algorithms.\n\n","The following example demonstrates how XCC can been used to analyze HYDICE hyperspectral data cubes. The data used in this demonstration was taken from the Forest Radiance 1 experiment. This data contains 320\u00d7640 pixels with each pixel containing 210 spectral bands. This is an enormous amount of data, therefore, the LARGE version of XCC was used to analyze this data set. Atmospheric corrections have been applied to the cube via the Empirical Line Method which uses calibration panels of known reflectance to remove atmospheric effects from the image.","This data set can be fully analyzed by issuing the following XCC commands:\n\n","It is not shown in , but there are rows of dead pixels along the top and bottom of this figure. The \u2018image crop\u2019 command is used to cut off these pixels. The \u2018crop\u2019 command can also be used to zoom in on sub-regions of the data cube. The two \u2018mask\u2019 commands are used to cut out the \u2018water bands\u2019 in the spectra. These spectral bands contain very little useful information, therefore they are eliminated. The file, water.mask is:\n\n","Finally, \u2018pca 15\u2019 is used to reduce the dimensionality of the problem down to 15. This is followed by \u2018xcc search\u2019 which autonomously searches for the endmembers. This command is described in detail in Appendix C.2. The last step, \u2018unf bayes\u2019, unmixes all of the patterns via the Bayesian unmixing technique described in Appendix C.2. The Bayesian approach was chosen because it constrains the patterns such that they are always positive, and it performs the necessary calculations much more quickly than the analogous non-linear least-squares techniques (Appendix C.1).","XCC manages to process this data set fairly quickly. On average it takes 15 minute to perform the PCA transformation while an additional 10 minutes is needed to perform the Bayesian unmixing. Thus, 25 minutes is required to fully unmix a HYDICE data cube such as this. However, if one is interested only in assigning pixels to it's \u2018closest class\u2019, then only 15 minutes are required.","It should be noted that XCC misidentifies some of the grass as trees. In actuality, the spectra for the grass in that region is very similar to the average tree spectra. Therefore, XCC has difficulty telling the two apart. It is possible to decrease the tolerance parameter and produce completely separate tree and grass classes. However, it was generally found that this results in too many unphysical patterns.","The original design for the XCC code placed the code for the menu systems, graphics displays, and algorithms into their own separate modules. A module, in this context, is a self-contained code segment which has a clear interface through which other modules may interact. All of the module implementation details are contained within the modules and are irrelevant as far as other modules are concerned.","This design was chosen because it makes future code upgrades easier. It also makes code debugging simpler in that problems are usually contained within a module. I had a mixed amount of success with maintaining this design goal. Unfortunately the FORTRAN 77 language does not directly support this type of modular design. Therefore, I have found that it was often difficult to maintain this design goal. Another problem I ran into was a lack of the time. Very often, I had to come up with quick code changes to test out new algorithm designs. Unfortunately, I never had sufficient time to go back and \u2018clean up\u2019 the code. The current state of the code is that there is one well defined module, the command line menu system, along, with several other loosely defined modules.","The source code for the command line interpreter module is contained within the file menu.F, and the interface for this module is contained within menu.inc. This module has only one subroutine which should be called, menu (prompt), where prompt is the character string which is displayed by the command line prompt. This routine reads the commands supplied by the user, and then parses the commands into strings, reals, and integers. The parsed commands are then passed to the subroutine main_control( ). This routine is responsible for properly interpreting these commands and executing them as necessary.","The remainder of the code segments are split among several different files according to the functionality of each subroutine. However, these code segments are not modular in that clearly defined interfaces between the segments do not exist. Further, the segments often share common data blocks which makes it impossible to properly hide module implementation details. The reason for this is simply that I didn't have time to do this correctly. One of the problems with using a lot of global variables, is that coding changes in one section of the code also effects all of the other \u2018modules\u2019.","The easiest way to figure out how this code works is to begin by studying xcc_menu.F. In here, the subroutine main_control( ) reads the parsed commands (such as plot, load, and unfold) and then transfers control to the subroutine which executes the request. As an example, one will find that main_control( ) calls the subroutine menu_plot( ) whenever it receives the plot command. To learn how XCC creates plots, one would then study menu_plot( ).","The following describes each of the source files used by XCC:\n\n","The command \u2018xcc search\u2019 can be used to automatically search for pattern direction vectors. This searching algorithm is designed to work with very large data sets which contain patterns which are not heavily overlapped. This section describes the details of how this algorithm works.","At the core of the XCC algorithm is the N-dimensional integration routine, POWELL. One of the inputs to this routine is an N-dimensional point from which to begin searching, for the merit function maxima, The question then becomes, how does one chose a set of reasonable starting points for the integration routine.","The simplest method involves setting up a grid of points in the N-dimensional recursion space. However, this method grows very cumbersome as the number of dimensions increases. If one chooses to place m points per dimension. Then the number of grid points is m. Therefore, this searching algorithm dramatically slows down as the number of dimensions increase.","An additional problem arises with very large data sets, because the amount of time needed to maximize the merit function increases with the number of data points. Typical hyperspectral cubes have more than 200,000 pixels. Therefore, for these data sets each call of POWELL is rather slow.","The \u2018xcc search\u2019 algorithm is designed to circumvent these problems by reducing the number of data points being integrated over at any particular time and reducing the number of integration starting points. The algorithm consists of the following steps.\n\n","Instead of integrating over all the points in the data set, a smaller set of 50 points is used. Integrating over the entire data set gives one hundreds (if not thousands) of points which lie along a given direction vector. Clearly, the slope of this line is described almost as well with only a handful of points (5 or more). Steps 1 and 2 are designed to implement this idea.","Once, the direction vector has been located, it is in our best interest to find all other points which lie along this vector (Step 3). Once these points are known, there is no point in processing them any further. Therefore, the are removed from further consideration.","At this point, one simply re-fills the buffer with a new set of points. By definition, none of these points are described by any of the direction vectors found thus far. Therefore, repeating steps 2-4 with this new buffer is guaranteed to find a new direction vector.","In general, I have found that this algorithm works very well. The primary thing to consider when using this method is that it may find extraneous direction vectors. An example of this can be seen in the Hydice example described. Here, roads and grass pixels were assigned classes. However, an extra class was assigned to many of the pixels which fall along the road\/grass boundary. Once the road and grass pixels are classified, there area a number of pixels remaining which consist of road\/grass mixed spectra. Since there are more than five of these pixels, they are assigned their own class.","I do have an idea on how to eliminate these extra classes, but I have not had time to test it. If we have three direction vectors which describe road, grass, and road\/grass mixed classes. then it is reasonable to expect thcse direction vectors to be co-planer. This is expected because the class is just a linear combination of the first two. Therefore, it is conceivable that one could reduce extraneous classes be searching for co-planer direction vectors. Given three such vectors, which are \u2018close\u2019 in spectral angle, the central one should be removed.","The measured data and patterns can be expressed mathematically as:\n\n+\u2003\u2003(1)\n\nwhere Iis the imeasured spectrum at the jresolution point, Jis pattern A at the jresolution point, and ais a scalar giving the amount of pattern A that contributes to measured spectrum i. Similarly, J, J, b, and band chave analogous meanings for patterns B and C. Depending, on the type of unmixing being performed, either the coefficients, a, or the patterns, J, can be the free parameters which are determined by the unmixing technique.\n","The purpose of this section is to give detailed information on how the XCC program performs this unmixing. Instead of dealing directly with Eq. 1, a more concrete example will be used. Once this example is understood, it should be clear how to generalize the method to more general cases. In this example we will assume that there are\n\n","Least-squares fitting techniques are an effective way of unmixing patterns from data. Two different fitting techniques are used by the XCC program. The first is a linear technique that uses Singular Value Decomposition (SVD) as its core algorithm. This is a very effective method, because it gives one the ability to systematically remove singularities which may corrupt the fit. One of the problems with the linear method is that it is impossible to easily constrain the parameters being fit. An alternative approach, which is presented here, uses a non-linear fitting algorithm to constrain the free parameters.","This problem is greatly simplified if we are only fitting for patterns. In this case, Eq. 1 reduces to \n\n\nNotice that coefficient matrix is independent of j, the number of resolution points. To solve this least-squares problem:\n\n","Similarly, if one is only fitting basis functions, then Eq. 1 can be expressed as \n\n\nIn this case, the \u2018coefficient\u2019 matrix contains the basis functions. This matrix is independent of i, the number of measured spectra. Therefore, one simply computes the singular value decomposition of the matrix, then loop over all of the measured spectra using SVBKSB to solve for the vector [a, b].\n","The problem becomes much more complicated when one fits for both patterns and basis functions. In this case, Eq. 1 becomes. \n\n\nFor realistic data sets, this matrix can become quite large. In fact the number of rows, n, and the number of columns, n, for this matrix are given by\n\nn=number of spectra\u00d7number of resolution points n=(number of patterns\u00d7number of resolution points)+(number of basis\u00d7number of measured spectra)\u2003\u2003(5)\n","It should be pointed Out the vector on the left hand side of Eq. 4 contains all of the data. Similarly, the vector on the right of the equation contains all of pattern and basis function scaling information. Therefore, solving this problem involves calling the subroutines SVDCMP and SVBKSB only one time. Once finished, the entire problem has been solved.","The problem with the combined fit is that it is computationally intensive. The subroutine SVDCMP is very slow for large matrices. Therefore, when performing the combined fit, one should reduce the number of patterns, basis functions, measured spectra, and resolution points to a minimum.","Patterns with negative intensities are non-physical. One method for constraining all free parameters to be positive is through the use of non-linear fitting techniques. Instead of using Eq. 1 to represent the data, we shall use\n\n=()+()\u00b7()+,\u2003\u2003(6)\n\nHere, all of the free parameters have been given a bold font. XCC uses the subroutine MRQMIN to solve for the parameters (a, b, and J) which satisfy the above equation. Once these parameters are found, the pattern is given by (J).\n","There are a few disadvantages to the least-squares unmixing techniques. First, linear unmixing can produce negative, non-physical, results. Second, the combined (basis function and pattern) fit is computationally intensive, and consequently, difficult to use with large data sets. A third disadvantage is that least-squares methods are global techniques. This is contrary to the philosophy of the XCC algorithm which uses a local fitting technique to determine endmembers. Ideally, one should use a localized technique to unmix the endmembers found by the XCC algorithm.","Proposed here is an alternative technique, which overcomes many of the difficulties mentioned above. It should be pointed out, that this new method is not as well understood as the least-squares methods. I have not had enough time to fully test the stability and effectiveness of this approach, but initial indications suggest that this new method performs reasonably well whenever the patterns are not highly overlapped.","The new unmixing algorithm is based upon the work of G. D'Agostini [7]. In his paper, he presents a method which iteratively uses Bayes' theorem to remove detector smearing and resolution effects from measured experimental data. I have modified these ideas for our use in hyperspectral pattern unmixing.","Before delving into Bayesian unmixing techniques, it is worth while to review some basic probability concepts. Here, I will attempt to keep mathematical symbolism to a minimum. Instead, I will concentrate on using concrete examples to describe the basic concepts. Further, I will attempt to explain how spectroscopic and hyperspectral data can be reformulated in terms of these probabilities. More detailed descriptions of probability theory can be found in any of the standard texts (Eg. [8]).","An unconditional probability can be defined in terms of an experiment which has many possible outcomes (or events). The probability for event X, p(X), is simply the likelihood that out of all possible events, X occurs. A simple example is the toss of coin, where the probability that the coin lands \u2018heads up\u2019 is \u00bd","Spectroscopic data can also be written in terms of such probabilities. As an example, refer to the top plot of , and let us suppose that this represents spectroscopic data plotted as a function of wavenumber. The spectrum represents a histogram of the various photons produced in the experiment. This \u2018photon distribution\u2019 can be turned into a probability distribution by simply normalizing the distribution: \n\n\nFor example, the probability that a photon will be measured in bin 2 is given by 5\/12. Of course, this is only an approximation of the true probability. In order to measure the true probability function, the \u2018photon distribution\u2019 in  would require infinite statistics.\n","Conditional probabilities define the likeliness that an event will occur given some prior condition. Again, a coin toss can be used to illustrate this idea. The probability that the coin will land \u2018heads up\u2019 given that it is a two-headed coin is equal to 1. Symbolically, this can be written as p(H|T), where H is represents the coin landing \u2018heads up\u2019, and T represents the fact that we are using a trick coin.","Characteristic of the conditional probability is the following normalization condition: \n\n\nEssentially this normalization condition states that one converts a conditional probability into an unconditional probability by summing over all of the possible conditions. Bayesian unmixing relies upon this normalization condition.\n","The synthetic spectroscopic data shown in  can also be used to demonstrate unconditional probabilities. The top plot of  represents a spectrum of measured data as a function of the four bins, labeled l). Let us further suppose that this measured data is a linear combination of two distinct spectral patterns (middle and bottoms plots). Each of these patterns were produced by separate processes within the experiment, and each has a unique spectral signature. Thus, the measured spectrum (top plot) is a bin-by-bin linear combination of these two patterns (bottom two plots).","p(D)|P) is the probability that one will measure a photon in bin Dgiven that a \u2018single photon\u2019 was produced in bin P. A few examples of this are:\n\n()=0\n\n()=1\n\n()=1.\u2003\u2003(9)\n\nNotice that these probabilities satisfy the normalization condition given by Eq. 8. \n\n","p(P|D) represents the probability that a photon was produced by pattern Pgiven that a \u2018single photon\u2019 has been measured in bin D. Some numerical examples are:","\u2003()=\u2155\n\n()=\u2155\n\n()=0.\u2003\u2003(12)\n\nAs before, these probabilities satisfy the normalization condition. \n\n\nBayes' theorem is: \n\n\nD and P can be thought of as a set of conditions (or events) with nand nelements in each. The individual elements of these sets are denoted by Dand Prespectively. In simple terms, Bayes' theorem tells one how to \u2018invert\u2019 conditional probabilities.\n","Note that Bayesian inversion is mathematically very different than matrix inversion. Bayes' theorem uses a priori information about the system, p(P), to perform the inversion. p(P) which are small contribute little to the resulting p(P|D). Thus, Bayes' theorem can be used as a type of localized-inversion technique. Outlying points, which have small values for p(P), will contribute very little to the inversion.","Once p(P|D) has been calculated for a given experiment, it is easy to unmix a data set. Let D represent the set of all measured data, and let P represents the set of all the patterns. Given a measured data distribution, n(D), the unmixed patterns are found by \n\n","Unfortunately, Eq. 14 tells us that it is impossible to directly calculate p(P|D) without first knowing p(P) which is the pattern distribution. After all, if p(P) were known, there would be no point to this exercise, because the desired patterns would calculating p(P|D):\n\n","The sample data set given at the beginning of Appendix C will now be used to give a detailed illustration of the Bayesian technique. To begin, Eq. 1 is rearranged by summing over each of the measured spectra. \n\n\nThis equation can then be placed into matrix form: \n\n\nEach row of this matrix represents a different measured resolution point, j. It should be noted that the Bayesian method can also be used with the full matrix given in Eq. 4. However, the matrix presented here was chosen because fewer floating point operations are required to perform the unmixing. Thus, this method was expected to require less processing time.\n","In shorthand notation, Eq. 17 can be written as \n\n\nI have chosen a notation that is suggestive of how the matrix, M(D|P), can be converted into a conditional probability.\n","The M(D|P) matrix is essentially an unnormamalized conditional probability. Thus, instead of satisfying the normalization condition given in Eq. 8, M(D|P) satisfies: \n\n\nMultiplying this equation by p(D)\/mand combining it with Eq. 8 gives: \n\n\nSubstituting Eq. 20 into Bayes' theorem, Eq. 14, yields: \n\n\nWith Bayes' theorem written in this form, it is no longer necessary to directly calculate p(D|P). Also, notice that this last equation depends upon p(P). p(P) contains our a\u2032 priori knowledge of the unmixed spectra. The following section describes in detail how knowledge of the experimental data can be used to build an appropriate p(P) vector.\n","When building p(P), it is helpful to understand the physical interpretation of this vector. For the purposes of this discussion, let us assume that we are dealing with a time-resolved spectroscopy experiment. At different points in time, a spectrum, as a function of wavelength, is recorded. Let us further assume that each measured spectrum is composed of a linear combination of four different patterns (A, B, C, and D). In a classical sense, the photons produced in the experiment will have been produced by one of the physical processes which is described by one of the four patterns. So, in some sense, each photon \u2018belongs\u2019 to one of the patterns. For each photon measured by the detector, there exists a probability that it was produced by pattern A, p(A). Similarly, there exist probabilities that the photon belongs to any of the other patterns: p(B), p(C), and p(D).","If the patterns occur with equal probability, the individual pattern probabilities are given by\n\n()=()=()=()=\u00bc.\u2003\u2003(22)\n\nHowever, if the patterns do not occur with equal probability,\n\n\u2003, where =1,\u2003\u2003(23)\n\nthen the individual pattern probabilities are: \n\n\nNote that the individual pattern probabilities must be constructed such that\n\n()=()+()+()+()=1\u2003\u2003(25)\n\nbecause it is assumed that the data is fully described by the patterns contained in set P.\n","The pattern probabilities can be further subdivided. For example, if each spectral pattern contains three wavelength bands, then there exist probabilities (p(A), p(A), p(A)) that a photon \u2018belonging\u2019 to pattern A falls within the iwavelength-band. For consistency we require that \n\n\nAnalogous equations can also be written for the remaining patterns.\n","There are several ways to estimate p(A). The XCC program currently uses the values of the XCC merit function to make this estimation. For each pattern, the value of the merit function, w, can be calculated at each of the i resolution points. Thus, one can write \n\n\nThis method works well so long as the patterns are not heavily overlapped. Whenever overlap occurs, the individual weights, w, will be very small, and will no longer properly approximate p(A). This technique was originally developed for use with hyperspectral data. Since, this data is not expected to be heavily overlapped in the spatial domain, this technique works well.\n","Another method for assigning the p(A) was also considered, but I lacked the time to implement this in the XCC code. This method consists of using the angular distance between resolution points and the pattern direction vectors as a measure of p(A). Points close to a direction angle would be given large p(A). Conversely, points far away (in angle) would be given small p(A) values. This is essentially what the previous method does except that the exponential term in the XCC merit function falls off much more quickly than is proposed here.","Once the individual probabilities have been determined, the Full p(P) can be constructed. The example given in Eq. 17 suggests that the set of patterns, P, will consist of the following sets of events:\n\n\u2003\u2003(28)\n\nUsing Eqs. 24 and 27, one can write \n\n","Therefore, if it is assumed that the patterns occur with equal probability, P(P) can be written as \n\n\nIf more knowledge of the patterns is know, then the initial p(P) can be improved. If, for example, one knows that the patterns occur with the ratios\n\n=( 1\/11):( 3\/11):( 2\/11):( 5\/11),\u2003\u2003(31)\n\nthen the p(P) would be \n\n","Now that M(D|P) and p(P) have been constructed, Eq. 21 can be used to construct p(P|D)). Next, Eq. 15 is used to calculate the unmixed patterns, n(P). One next uses this n(P) to construct a new p(P). One then proceeds in an iterative fashion until n(P) converges.","Unfortunately, it was found that this iterative approach did not work. In general, I found that when the method converged, the resulting patterns were totally unrecognizable. However, it was found that it one used an expanded version of Eq. 30 to calculate p(P) that reasonable results could be achieved after just one iteration. A behavior similar to this was noted by D'Agostini [7] when working ith low statistics data and Monte Carlo samples.","I do not fully understand why the consecutive iterations fail to converge. My suspicion is that it has something to do with the fact that the matrix in Eq. 17 was used to construct p(D|P) instead of the larger matrix in Eq. 4. The smaller matrix is heavily underdetermined in that there are fewer knowns (average measured spectra variables) than unknowns (pattern variables). Least-squares techniques generally fail with such systems, but the Bayesian method produces reasonable results after its first iteration. I believe that this is because the first iteration is heavily influenced by the input p(P). Therefore, the patterns found by the first iteration will be very similar to the original p(P). Since the technique presented here uses the XCC weights to estimate the original p(P), the method gives good answers after only one iteration. Further iterations produce patterns which quickly diverge away from this solution, because the under-determined solution space is simply too large.","One way to solve this problem is to use Eq. 4 to create p(D|P). More than likely, this will lead to more stable solutions which successfully converge after a few iterations. However, since this method was implemented as a fast unmixing technique for hyperspectral data, I have decided not to do this. Using the larder matrix in the unmixing would dramatically increase the number of floating point operations required for the calculation, consequently increasing the over-all data processing time. In addition, I could not afford the time required to allow the method to iterate. Therefore, since the current implementation of the Bayesian technique gives reasonable results quickly, I have decided to not make these changes.","It may prove instructive to perform studies to see if using the larger matrix (Eq. 4) improves the stability of the Bayesian method. If so, this technique may be useful for applications which are not time critical, and it may still be faster than the analogous non-linear constrained least-squares technique. Also, studies using \u2018truth data\u2019 need to be done to check the effectiveness of Bayesian unmixing. Preliminary studies, using synthetic spectra, show that the technique does a good job of reproducing the patterns contained within the data. However, the effectiveness of the method still needs to be verified with experimental data.","While the invention has been described in its presently preferred embodiment it is understood that the words which have been used are words of description rather than words of limitation and that changes within the purview of the appended claims may be made without departing from the scope and spirit of the invention in its broader aspects."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"DESCRIPTION OF THE DRAWINGS","p":{"@attributes":{"id":"p-0008","num":"0015"},"figref":"FIGS. 1","b":["1","1"]}},"DETDESC":[{},{}]}
