---
title: Method and apparatus for processing the output of a speech recognition engine
abstract: Data processing apparatus for receiving recognition data from a speech recognition engine and its corresponding dictated audio data where the recognition data includes recognized words or characters. A display displays the recognized words or characters and the recognized words or characters are stored as a file together with the corresponding audio data. The recognized words or characters can be processed and link data is formed to link the position of the words or characters in the file and the position of the corresponding audio component in the audio data.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=06961700&OS=06961700&RS=06961700
owner: AllVoice Computing PLC
number: 06961700
owner_city: Devon
owner_country: GB
publication_date: 20020318
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS AND CLAIM OF PRIORITY","FIELD OF THE INVENTION","BACKGROUND ART","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS"],"p":["This application is a divisional of U.S. application Ser. No. 09\/275,287 filed Mar. 24, 1999 now abandoned, which is a continuation under 37 CFR 1.53(b) of International Application No. PCT\/GB97\/02591 filed Sep. 24, 1997, which claimed priority under 35 U.S.C. 119 from Great Britain application No. 9619932.8 filed Sep. 24, 1996, which applications are incorporated herein by reference. Applicant claims priority under 35 U.S.C. 120 of U.S. application Ser. No. 09\/275,287, which was with U.S. application Ser. No. 08\/896,105, filed Jul. 17, 1997, now issued as U.S. Pat. No. 5,857,099 on Jan. 5, 1999, which is a divisional of U.S. application Ser. No. 08\/720,373, filed Sep. 27, 1996, now issued as U.S. Pat. No. 5,799,273 on Aug. 25, 1998. Applicant further claims priority under 35 U.S.C. 120 of U.S. application Ser. No. 08\/896,105 and U.S. application Ser. No. 08\/720,373.","The present invention generally relates to the field of data processing and in particular the field of processing the output of a speech recognition engine.","The use of speech recognition as an alternative method of inputting data to a computer is becoming more prevalent as speech recognition algorithms become ever more sophisticated and the processing capabilities of modern computers increases. Speech recognition systems are particularly attractive for people wishing to use computers who do not have keyboard skills.","There are several speech recognition systems currently on the market which can operate on a desktop computer. One such system is called DragonDictate (Trade Mark). This system allows a user to input both speech data and speech commands. The system can interface with many different applications to allow the recognised text output to be directly input into the application, e.g. a word processor. This system, however, suffers from the disadvantage that there is no audio recording of the dictation stored which can be replayed to aid the correction of the recognised text.","Another system which is currently on the market is IBM VoiceType version 1.1 (Trade Mark). In this system the recognised text from the speech recognition engine is input directly into a proprietary text processor and audio data is stored. This system, however, does not allow the recognised text to be directly input into any other application. The dictated text can only be input directly into the proprietary text processor provided whereupon at the end of dictation the text can be cut and pasted into other applications. Corrections to the dictated text in order to update the speech recogniser models can only be carried out within the text processor window. Text for recognition correction can be selected and the audio component corresponding to the text is played back to assist in the correction process. When all of the corrections have been completed, the text can either be saved or cut ready for pasting into another application. Either of these operations can cause the corrections made to be used to update the speech recogniser: the user has limited control over when the updates are made.","Not only is this system disadvantaged in not allowing direct dictation into applications, the system also does not allow the audio data to be stored in association with the text when the document is saved or when the text is cut and pasted into another application. Even a simple text processing operation, e.g. an insertion operation within a body of text, will prevent the playback of the audio component for that body of text including the change.","It is an object of the present invention to provide an interface between the output of a speech recognition engine and an application capable of processing the output which operates in a data processing apparatus to link the relationship between the output data and the audio data to allow the audio data to be played back for any output data which has been dictated even if the data as a whole has been processed in such a way as to move, reorder, delete, insert or format the data.","This object of the present invention can be achieved by a data processing apparatus comprising input means for receiving recognition data and corresponding audio data from a speech recognition engine, the recognition data including a string of recognised data characters and audio identifiers identifying audio components corresponding to a character component of the recognised characters; processing means for receiving and processing the input recognised characters to replace, insert, and\/or move characters in the recognised characters and\/or to position the recognised characters; link means for forming link data linking the audio and identifiers to the characters component positions in the character string even after processing; display means for displaying the characters being processed by the processing means; user operable selection means for selecting characters in the displayed characters for audio playback, where the link data identifies any selected audio components, if present, which are linked to the selected characters; and audio playback means for playing back the selected audio components in the order of the character component positions in the character string.","Thus, in accordance with this aspect of the present invention, positional changes of characters in the character string due to processing operations are monitored and the links which identify the corresponding audio component are updated accordingly. In this way, the corresponding audio component for any dictated character in the character string can be immediately identified even after processing. This allows for the audio component associated with any character to be played back by a selection operation by a user. This feature greatly enhances the ability to correct incorrectly recognised characters since a user will be able to hear what was dictated in order to decide what was actually said rather than what the speech recogniser recognised. This feature of being able to play back audio components corresponding to the characters is maintained even when dictated characters are inserted into previously dictated characters.","In the present invention the character data output from the speech recognition engine can comprise text or symbols in any language, numerals or any unicode. The characters can comprise words forming text or any unicode characters and the system can be configured to recognise dictated numbers and input the corresponding numeric characters to the application instead of the word descriptions.","The processing means of the present invention can comprise any application running on a processor which enables character data from a speech recognition engine to be entered and manipulated, e.g. a word processor, presentation applications such as Microsoft PowerPoint (Trade Mark) spreadsheets such as Excel (Trade Mark), email applications and CAD applications. In this aspect of the present invention the dictated character positions in the document, drawing or product of the application is linked to the corresponding audio component by link data.","In one aspect of the present invention the link data and audio data can all be stored. In this way the audio data is maintained for playback at a later time when, for instance, it may be wished to carry out corrections to correct speech recognition errors. The storage of the character data, link data and the audio data allows for corrections to be postponed or even delegated to another person on another machine.","Corrections to the incorrectly recognised character data can be made by correcting the character string which causes the playback of the audio component. The characters can then be corrected and the corrected characters and the audio identifier for the audio component corresponding to the corrected characters are passed to the speech recognition engine for updating user models used in the recognition process.","Where the output of the speech recognition engine includes a list of alternative characters together with an indicator which indicates the likelihood that the word is correct, when a word is selected for correction, a choice list can be displayed which comprises the alternative words listed alphabetically for ease of use. Corrections can then be carried out either by selecting one of the alternative characters or entering a new character.","In one embodiment, in order to maintain the links between the character components and the corresponding audio components, a list of character locations in the character string and positions in the corresponding audio components is kept. Where the character string is formed of a plurality of separate dictated passages, the audio data is separately stored and the list identifies in which of the stored audio passages and at which position the audio component lies in the audio passage.","In addition to the updating of the speech recognition model due to incorrectly recognised words, a passage of characters, or all of the characters, can be selected for updating the contextual model used by the speech recognition engine. Thus, in this embodiment of the invention the operator has control over when the contextual model is to be updated based on the corrections made to the characters.","It is an object of another aspect of the present invention to enable audio messages to be recorded and stored in association with a file containing character data output from a speech recognition engine to allow instructions or a reminder to be recorded.","In accordance with this object of the present invention there is provided data processing apparatus comprising means for receiving recognition data from a speech recognition engine and corresponding audio data, the recognition data including recognised characters; display means for displaying the recognised characters; storage means for storing the recognised characters as a file; means for selectively disabling the display and storage of recognised characters or recognition carried out by the speech recognition engine for a period of time; and means for storing the audio data for a period of time in the storage means as an audio message associated with the file.","It is an object of another aspect of the present invention to provide for the automatic detection of possibly incorrectly recognised characters in the character data output from the speech recognition engine.","In accordance with this aspect of the present invention, there is provided data correction apparatus comprising means for receiving recognition data from a speech recognition engine, said recognition data including recognised characters representing the most likely characters, and a likelihood indicator for each character indicating the likelihood that the character is correct; display means for displaying the recognised character; automatic error detection means for detecting possible errors in recognition of characters in the recognised characters by scanning the likelihood indicators for the recognised characters and detecting if the likelihood indicator for a character is below the likelihood threshold, whereby said display means highlights at least the first, if any, character having a likelihood indicator below the likelihood threshold; user operable selection means for selecting a character to replace an incorrectly recognised character highlighted in the recognised characters; and correction means for replacing the incorrectly recognised character and the selected character to correct the recognised characters.","The likelihood threshold can be selectively set by a user to a suitable level to reduce the number of characters which are falsely identified as incorrectly recognised whilst increasing the chances of correctly identifying incorrectly recognised characters. The provision of automatic detection of possible recognition errors can significantly decrease the time taken for correcting character data.","A specific embodiment will now be described with application to word processing of text output of a speech recognition engine.","Referring to  there is illustrated a speech recognition system in accordance with one embodiment of the present invention which comprises an IBM (Trade Mark) compatible PC (personal computer)  having a keyboard  for inputting and correcting text and a pointing device  which in this embodiment is a mouse. Software applications are loaded into the computer from computer storage medium such as the floppy disc , an optical disk (CD ROM), or digital tape. The software applications comprise the speech recognition application which comprises the speech recognition engine, the application for processing text such as a word processor and the interface application to control the flow of text into the text processing application, to control the flow of updating information from the text processing application to the speech recognition application and for maintaining links between the text and the audio data.","The system is provided with a microphone , a loudspeaker  and an interface device . During dictation the audio signal from the microphone  is input into the interface device  which includes an analog to digital converter and a digital signal processor to digitise and condition the signal for input into the computer . During playback of the recorded audio signal, the audio signal is output from the computer  to the interface device  in digital form and is converted to an analog signal by a digital to analog converter within the interface device . The analog signal is then output from the interface device  to play back the audio recording.","In the specific embodiment of the present invention the interface device  is provided with the IBM VoiceType system. Also, the speech recognition engine used in the specific example is the IBM VoiceType speech recognition engine. The present invention is not, however, limited to any specific speech recognition engine and can also be used with any conventional hardware for recording and playing back sound in a personal computer, e.g. in an IBM compatible machine the sixteen bit sound blaster compatible standard can be used. The present invention can be used with either continuous or discrete speech recognition engines.","Referring now to , this diagram illustrates a schematic overview of the internal architecture of the computer. A bus  links all of the components of the system and the Read Only Memory (ROM)  containing conventional systems programs and data. The processor  runs three applications simultaneously: the speech recognition engine application , the speech recognition interface application  and the text processor application . The memory , which can comprise random access memory (RAM) or in a Windows (Trade Mark) environment, virtual RAM. Within the memory  data is stored for the speech recognition engine application . This data comprises a user model  which can be updated to improve the accuracy of the recognition, a language model  and a dictionary  to which a user can add new words. The user model  comprises an acoustic model and a contextual model. During operation of the speech recognition engine application  the application utilises the user model , the language model  and the dictionary  in the memory  and outputs speech recognition data  to the memory . The speech recognition interface application  receives the speech recognition output data  and forms link data . The text component of the speech recognition output data  is also passed by the speech recognition interface application  to the text processor application  to form a current document  in the memory. The display  displays the text of the current document  stored in the memory  and the keyboard  can be used to insert, delete and move text. The pointing device  can also be used to select text and word processing operations in the conventional well known manner within Windows applications.","The system is also provided with non-volatile storage in the form of disk storage . Within the disk storage  two directories are provided. A temporary directory used by the speech recognition engine  for the storage of run time files which contain the speech recognition output data. A user's directory is also provided for the storage of document files by the text processor application  and associated link data formed by the speech recognition interface .","An audio input device  inputs the dictated audio signal to an analog to digital converter . Although in  the audio input device  is illustrated to be a microphone , the audio input could alternatively comprise a pre-recorded signal source, e.g. a digital audio tape (DAT). The digitised signal from the analog to digital converter  is then passed to a digital signal processor  for conditioning of the signal before input to the input\/output device  of the computer . In this way the speech recognition engine application  is able to read the digitised input audio data via the bus  and output speech recognition output data  into the memory .","When the speech recognition interface application  interacts with the text processor application  following the selection of text for audio playback by the user using the pointing device , audio data which is stored in the temporary directory in the disc storage  is accessed and output over the bus  via the input\/output device  to a digital to analog converter  to generate an analog audio signal to drive an audio output device  for playback of the audio signal selected by the user.","In the specific embodiment the audio data is stored in one or more files in the temporary directory of the disk storage  since the storage audio data requires a great deal of storage capacity and it is impractical to hold audio data of any length in the volatile memory .","In the specific embodiment the operating system operating by the processor  is Windows 3.1, 3.11, 95 or NT. The text processor application  can be any word processor such as Microsoft Word (Trade Mark), Wordperfect (Trade Mark) or Lotus Word Pro (Trade Mark). The speech recognition engine application  is the IBM VoiceType.","When the speech recognition engine application  is activated and receives audio data via the interface device , the speech recognition output data  is temporarily held in the volatile memory . The output data is then passed to files which are opened in the temporary directory of the disk storage . The audio data for each period of dictation is stored in a single file.","Also in temporary directory on the disc storage , two files are stored by the speech recognition engine application  which includes the information illustrated in tabular form in FIG. . For each period of dictation an audio data file, and a pair of information files are generated containing the information illustrated in FIG. . Each of the words recognised is identified by an identifier tag which identifies the position in the sequence of word. Also, the audio start point and audio end point of the audio component in the associated audio data file is indicated to enable the retrieval and playback of the audio component corresponding to the word. For each word, a list of alternative words and their scores is given where n is the score, i.e. the likelihood that the word is correct, and w is the word. The list of alternative words is ordered such that the most likely word appears first. Alternatives, if any, are then listed in order with the word having the highest score first and the word having the lowest score last.","The speech recognition interface application  receives the output of the speech recognition engine application  and forms link data  in the volatile memory .  illustrates the form of the link data for each recognised word output from the speech recognition engine . The speech recognition interface application  receives the recognised word at the head of the alternative list shown in FIG.  and outputs the word using the dynamic data exchange (DDE) protocol in the Windows operating system. The position of a word in the text in the text processor application  is determined by determining the counter number indicating the position of the first character in the text for the word. This character number is entered under the character number field. The link data  also includes information identifying where the audio data can be found in the files in the temporary directory of the disk storage . This information is provided in the tag field. The tag field will not only include the identified tag identifying the position of the audio component for a word within a file, it will also include an identification of which file contains the audio component. The next field is the word score which is an indication of the likelihood that the word has been recognised correctly. The next field is the word length field. This gives the number of characters forming the recognised word. The next field in the link data  is the character string forming the actual word and this is followed by the vocabulary length field which is a number indicating the number of characters in the vocabulary description string. The final field is the vocabulary description string which is a string of characters describing the vocabulary in which the word recognised by the speech recognition engine application  can be found in the dictionary .",{"@attributes":{"id":"p-0053","num":"0052"},"figref":"FIG. 5","b":["13","11","12","15"]},"Once the programs are loaded, a user can select whether to read an existing document in step S. If no existing document is to be read text can be entered using the dictation process step S which will be described in more detail hereinafter. When a passage of dictated text is complete, the dictation process is finished and in step S the user can decide whether to insert further dictated text. If further dictated text is to be inserted, the process returns to step S. If no further dictated text is to be inserted then the dictation process is finished.","If in step S after the programs have been loaded a user requests that an existing document be read, in step S the document to be read is selected and in step S it is determined whether the document selected has audio data associated with it. If there is no audio data associated with it, i.e. it is a conventional word processor document, in step S the document is read and the process moves to step S which is a point at which the document has been loaded and the user can insert dictated text if desired.","If in step S it is determined that the document does have audio data associated with it, the user is given the option to read the audio data in step S. If the user declines to read the audio data then only the document is read in step S and the document will be treated within the word processor as a conventional word processor document. If in step S the user selects to read the audio data, in step S the document is read together with the associated link data from the user's directory in the disk storage  and the speech recogniser run time created files are copied from the user's directory to the temporary directory in the disk storage . The document is thus open in the word processor and in step S the user can insert dictated text if desired.","If no more dictated text is to be inserted in step S, in step S the user can decide whether to correct recognition errors in the recognised text. If in step S it is decided by the user that they are to correct errors then the process moves to step S to correct the errors as will be described hereafter.","Once the recognition errors have been corrected by the user or if the recognition error is not to be corrected by the user, the process moves to step S wherein the user can decide whether to update the user's contextual model. This is a second form of correction for the speech recognition process. The user model  comprises an acoustic model and a contextual model. The recognition errors corrected in step S will correct the acoustic model, i.e. the recognition errors. Once all of the recognition errors have been corrected, the contextual model can be updated in step S by selecting the text to be used for the update and sending the number of corrected words together with a list of the corrected words to the speech recognition engine for updating the contextual model.","In step S the user can then decide whether or not to word process the document in the conventional manner. If a document is to be word processed, the word processing operation in step S is carried out as will be described in more detail hereinafter. This word processing operation can be carried out at any time after or before the dictation process. The document being formed in the word processor can thus comprise a mixture of conventionally entered text, i.e. via the keyboard or via the insertion of text from elsewhere, and directly dictated text.","When the user has finished dictating, inserting and editing the text, in step S the user has the option of whether or not to save the document. If the document is to be saved, in step S the user is given the option of saving the document without the audio data as a conventional word processor document in step S, or saving the document together with the link data and audio data in step S. In step S, in order to save the link data and audio data, the document and link data, by default, is saved in the user's directory and a copy of the speech recogniser run time created files is made in the user's directory.","Once the document has been saved, the user has the option to exit the word processor in step S. If the word processor is exited in step S the process terminates in step S, otherwise the user has the option of whether or not to read an existing document in step S.","Referring now to , this document illustrates the dictation process, step S, of  in more detail.","In step S the dictation is started and in step S the speech recognition engine application  outputs speech recognition data  and stores the data in run time files in a temporary directory of the disk storage . Also, the audio data is stored in parallel as a run time file in the temporary directory in step S. The speech recognition interface application  detects whether the most likely words output from the speech recognition engine application  are firm or infirm, i.e. whether the speech recognition engine application  has finished recognising that word or not in step S. If the speech recognition engine application  has not finished recognising that word, a word is still output as the most likely, but this could change, e.g. when contextual information is taken into consideration. In step S, the speech recognition interface application  forms links between positions of firm words and corresponding audio data components thus forming the link data . In step S the speech recognition interface application  outputs the words to the word processor application  and the text is displayed on the screen with the infirm words being displayed in reverse video format. In step S the process determines whether dictation is finished and if has not it returns to step S. If dictation has finished, in step S it is determined whether the dictated text is inserted into previously dictated text and, if so, the link data is updated to take into consideration the change in character positions of the previously dictated words. The dictation process is then terminated in step S.","Referring now to , this illustrates the word processing process of step S of  in more detail. In step S a user can position the cursor in the text on the display using the keyboard  or the pointing device . In step S the user can delete and\/or insert text by, for example, typing using a keyboard or inserting text from elsewhere using conventional word processing techniques. In step S the speech recognition interface application  updates the links between the recognised words and associated audio components, i.e. the character number in the first field of the link data  is amended to indicate the correct character position of the word in the text. The word processing process is then terminated in step S.","Referring now to , this diagram illustrates a manual method of carrying out the error correction of step S of FIG. . In step S the user selects a word which is believed to be incorrectly recognised for correction. The selected word is then highlighted on the display in step S and in step S the speech recognition interface application  determines the word location in the text. In step S it is determined whether the word is a dictated word or not by comparing the word location with the link data . If the word is not a dictated word a message is displayed informing the user that the word is not a dictated word in step S and in step S the system waits for more corrections. If the word is a dictated word, in step S the speech recognition interface application  determines the identified tag for the selected word using the link data  and the speech recognition output data. The audio component is then retrieved from the speech recognition run time created files in the temporary directory view the speech recognition engine application  in step S and in step S the audio component is played back via the speech recognition engine application . In step S, once the identified tag has been determined, in addition to retrieval of the audio component, the alternative words from the speech recognition output data in step S is obtained and the choice list is built and displayed on the display in step S. In step S a user can select an alternative word from the choice list, input a new word, default back to the original word or cancel if the original word is correct or the word was selected for correction in error. If a user cancels the operation in step Sthe process proceeds to determine whether more corrections are required. If the user does not cancel the operation in step S the displayed document is updated and in step S the corrected word and the corresponding identifier flag is sent to the speech recognition engine application . In step S the speech recognition engine application  updates the user's acoustic model within the user model . In step S the link data is updated, e.g. if the correct word is has more characters in it than the replaced word, the character position of all subsequent words will change and thus the link data will need to be updated. In step S, if more corrections are required the user will in step S select another word for correction and repeat the process. Otherwise the correction process is finished and terminates in step S.","Referring now to , this diagram illustrates a method of automatically detecting possible recognition errors in the text. In step S the user selects a threshold score to be used to detect possible recognition errors. In step S the document or selected text is scanned to compare the threshold score with the score for each of the words. In step S the document is scanned to compare the threshold score with the score for the next word. If in step S it is found that the score for the word is greater than the threshold, the process proceeds to step S where it is determined whether the end of the document has been reached. If it is not the end of the document then the process returns to step S to compare the score for the next word with the threshold score. If in step S it is determined that the score for the word is less than the threshold score, the word is highlighted on the display in step S. In step S the speech recognition interface application  determines the word location in the text and in step S the identifier tag for the word is determined. In step S the audio component is retrieved from the speech recognition run time created files in the temporary directory via the speech recognition engine application  for playback of the audio component via the speech recognition engine application  in step S. Once the identifier tag is determined in step S, in step S the alternative words for the word having the score less than the threshold is obtained from the output of the speech recogniser engine application . In step S a choice list is built and displayed on the display. The choice list comprises the list of alternative words displayed alphabetically. In step S a user can select an alternative word from the choice list, input a new word, default back to the original word, or cancel if the original word is thought to be correct. If a user cancels the operation in step S, the process proceeds to step S to determine whether the end of the document or selected text has been reached. If the user does not cancel the operation, in step S the displayed document is updated and in step S the corrected word and identifier flag is sent to the speech recogniser engine application . In step S the speech recognition engine application  updates the user's acoustic model in the user model . In step S the link data is updated, e.g. if the correct word contains more or less than characters than the original word, the character number indicating the position of the first character of all of the following words will change and thus the link data for these words must be updated. In step S it is determined whether the end of the document, or the selected text, has been reached. If so, the process is terminated in step S, otherwise the process returns to step S to continue scanning the document or selected text.","Thus in the process described with reference to  to , the user is able to harness the output of the speech recognition engine to maintain links between the words in the text and the corresponding audio components in the audio data even if the words are moved or are dispersed with non dictated text or text which has been dictated at some other time. Link data effectively acts as a pointer between the position of the text in the document and the position of the corresponding audio component in the audio data. In this way the dictated text can be ordered in any way and mixed with non dictated text without losing the ability to play back the audio components when selected by a user.","Also, since not only audio data but also the link data is stored in non-volatile storage such as the disk storage , the user is able to reopen a document and play back the corresponding audio data. This enables a user to dictate a document and store it without correction thereby allowing correction at a later date, i.e. delaying the correction. When the document link data and audio data is read, the system returns to a state as if the text had just been dictated. The text can be corrected and the corrections can be fed back to the speech recognition engine to update the user model .","Referring now to , there is illustrated a flow diagram illustrating the feature of another aspect of the present invention. In , many steps are the same as those illustrated in FIG.  and thus the same references are used. In this aspect of the present invention, when audio data is associated with a document (S) and a user selects to read audio data (step S), the system determines whether there are any audio messages associated with the document in step S.","If there are no audio messages associated with a document the process proceeds to step S where the document and link data is read and the speech recognition run time created files are copied from the user's directory to the temporary directory and the system proceeds as described with regard to FIG. . If however there are one or more audio messages associated with the document, the user is given the option to select the audio message which is to be played in step S. If an audio message is not to be played then the process proceeds to step S. If however the user selects to play a selected audio message, in step S the selected audio message is retrieved from the speech recognition run time created files via the speech recognition engine applications  and in step S the selected audio message is played via the speech recognition engine application . The process then proceeds to step S as described with reference to FIG. . Although  illustrates the audio note only being playable at a particular time, an audio note can be played at any time during the creation of a document or after a document has been read.","In  there is illustrated a procedure for dictating one or more audio messages which can be carried out at any time. In step S the user can elect whether or not to dictate an audio message to be associated with a document to be created. If no audio message is to be created the process terminates in step S. If an audio message is to be created in step S the dictation of the audio message is initiated and in step S the audio message is stored in the speech recognition run time files. In step S it is determined whether the dictation of the audio message has finished and if not the process returns to step S. If the audio message has finished in step S the link data is updated to indicate that the document includes an audio message and in step Sanother audio message can be selected to be dictated and the process returns to step S. Otherwise the process can be terminated in step S","This aspect of the present invention illustrated in  allows for a user to dictate one or more messages which is stored in association with a document. During the dictation of an audio message no recognised text is input to the text processor application . This is achieved in the specific embodiment by failing to pass the text to the text processor application . This could alternatively be achieved by disabling the recognition capability of the speech recogniser engine application  so that only the audio data is stored.","In the specific example the audio message merely comprises a normal audio data file which has the speech recognition data of  in corresponding run time files and which is ignored.","As can be seen with regard to , when a user opens a document the link data is examined to determine whether there are any audio messages associated with a document and if so an option is displayed to allow the user to select and play a message. If the user selects to play the message the link data identifies the audio data file containing the audio message which is retrieved and played back via the speech recognition engine .","This aspect of the present invention can be used without the features of correcting the user model and can in its simplest form comprise a method of recording and digitising audio messages and storing the audio messages with a document which could simply be created in a conventional manner without involving speech recognition. The audio message allows for instructions or reminding information to be attached to a document in audio form.","Another aspect of the present invention will now be described with reference to  to . In this aspect of the present invention the correction of the incorrectly recognised words in a dictated passage of text can be carried out on a machine which is separate to the machine containing the speech recognition engine  and user model . In  there is illustrated a network of author work stations , and which comprise the system as described with regard to  to . The author work stations , and are connected via a network  under the control of a network server  to an editor work station . The network  can comprise any conventional computer network such as an ethernet or token ring.","Although in  access to the files of the author work stations is achieved via the network , any method of obtaining copies of the documents, associated link data files, and associated speech recognition run time created files can be used. For instance, the documents could be transferred by copying the relevant files on to a computer readable medium such as a floppy disc which can be read by the editor work station and amended. Also correction files (to be explained hereinafter) can be stored on the disc and the disc can be re-read by the author work station for updating of the user model  by the speech recognition engine application . Further, although three other work stations and a single editor work station are illustrated any number can be used on the network.",{"@attributes":{"id":"p-0078","num":"0077"},"figref":["FIG. 12","FIG. 12","FIG. 2"],"b":["103","103","21","22","23","24","20","10","11","12","12","15","15","19","18","17","16","27","28","19"],"i":["a","a. "]},"The editor work station  is also provided with a network card  to interface the editor work station  with the network  to allow for the document, link data and speech recognition run time created files to be read from a correspondence path. Of course, although not illustrated in , the author work station , and will include a similar network card  in this embodiment.",{"@attributes":{"id":"p-0080","num":"0079"},"figref":"FIG. 13","b":["100","101","21","102","102","103","103","104","105","104","106","107","108","19","28","104","109","111","110","111","112","113","103","114","115","116","101"],"i":"a "},"Referring now to , this document is a flow diagram of the method of manually correcting the document corresponding to step S of FIG. . In step S the editor selects a word for correction and in step S the word is highlighted on the display. In step S the correction application determines the word location in the text and in step S it is determined whether the word is a dictated word or not by comparing the word location with the link data . If the word is not a dictated word a message is displayed informing the editor that the word is not a dictated word in step S and in step S the system awaits further corrections. If the word is a dictated word in step S the identified tag is determined. In step S the audio component from the speech recognition run time created file is retrieved from the correspondence path and the audio component corresponding to the selected word is converted to a conventional audio format (.WAV) in step S. The audio component is then played back using the conventional multimedia sound card and loudspeakers in step S.","Once the identified tag is determined in step S the alternative words are read from the speech recognition run time created files in the correspondence path in step S and in step S a choice list is built and displayed. The choice list comprises the alternative words listed alphabetically for ease of use. In step S the editor can select an alternative word from the choice list, input a new word, default back to the original word, or cancel if the original word is considered to be correct or the editor incorrectly selected the word. If an editor cancels the operation in step Sthe process proceeds to step S to determine whether more corrections are required. If the user does not cancel the operation, in step S the displayed document is updated and in step S the corrected word and identifier flag is stored in a word correction file in the user path. In step S the link data is updated e.g. if the correct word is of different length to the replaced word, the character number identifying the position of the first character of each of the proceeding words will be changed and thus the link data for all of the following words must be changed. In step S, if the user makes no more corrections, the process ends at step S otherwise the user can select another word in step S.",{"@attributes":{"id":"p-0083","num":"0082"},"figref":"FIG. 14","i":["b ","a "],"b":["104","13","140","141","142","155","156","141","142","143","144","145","25","146","147","148","19","28"]},"When the identifer tag is determined for the word in step S in step S the alternative words from the speech recognition run time created files can be read in the correspondence path and in step S a choice list can be built and displayed. The choice list comprises a list of the alternative words in alphabetical order. In step S the editor can select an alternative word from the choice list, input a new word, default back to the original word, or cancel if it is considered that the original word was correct. If the editor cancels the operation in step S the process proceeds to step S to determine whether the end of the document or selected text has been reached. If the editor does not cancel the operation, in step S the displayed document is updated and in step S the corrected word and identifer flag are stored in a word correction file in the user path. In step S the link data  is updated e.g. if the correct word has a different length to the original word the position of the following words will change and thus the link data needs to be updated. In step S it is determined whether it is the end of the document, or selected text, and if so the process terminates in step S.","Referring now to , this is a flow diagram of the additional steps which are carried out at a networked author work station when the speech recognition engine application and the speech recognition interface application is loaded. In step S the speech recognition interface application detects whether there are any word correction files or contextual correction files present in the user path. If no correction files are detected at present then the process terminates in step S allowing the user to continue to step S in  or . If correction files are detected to be present in step S the author is given the option as to whether to carry out updating of the user model  at this time for the selected correction files in step S. If no updating is to be carried out for the selected correction files the process proceeds to step S to determine if there are more correction files present. If the author selects to carry out the updating of the user model  using the selected correction files, in step S the associated word and\/or contextual correction files are read from the user path. In step S the speech recognition run time created files are copied from the correspondence path to the temporary directory and in step S the word and contextual update parameters are sent to the speech recognition engine application  by the speech recognition interface application . In step S the read correction files are then deleted in the user path. In step S it is then determined whether there are any more correction files present in the user path and if so the user is given the option as to whether to update using these files in step S. If in step S there are no more correction files present then the process terminates in step S allowing the user to proceed to step S in  or .","Although in step S the author can select each associated word and contextual correction file for updating, the author may also be given the opportunity to elect for the updating to be carried out for all of the correction files present in the user path.","This aspect of the present invention illustrated in  to  allows an author to dictate documents, save them and delegate correction to an editor by a separate machine. The corrections made by the editor are then fed back to update the author's user model to increase the accuracy of the speech recognition thereafter. However, since the author's user model is not copied, there is no danger of there being more than one copy of the user model whereby one of the copies could be out of date. Also, since the editor does not have access to the author's user model, the corrections being carried out by the editor does not prevent the author from continuing to use the speech recognition engine application which requires access to the user model. By delegating the correction to the editor whereby updates are generated in files, dictation by the author and correction by the editor can be carried out in parallel.","The delegated correction feature is enhanced by the provision of the audio note capability allowing an author to dictate instructions to the editor to be attached to the document to be edited. The audio message capability can not only be used in conjunction with the delegated correction facility, but can also be used on its own simply to provide audio messages with a document.","The delegated correction system also provides a cost reduction for users since the editor need not be supplied with the speech recognition software and system components. The editor work station  can simply comprise a standard multimedia PC. It is of course possible to provide a plurality of such editor work stations in the network to serve any number of author work stations.","The delegated correction system can also operate without a network by physically moving files between the author and editor work stations on computer readable storage media such as floppy disks.","Although in the embodiments described hereinabove word processing is described as occurring after dictation, word processing of the document can take place at any time.","Further, although in the embodiments the recording and playing of audio messages is described as occurring at specific points in the process they can be recorded or played at any time.","What has been described hereinabove are specific embodiments and it would be clear to a skilled person in the art that modifications are possible and the present invention is not limited to the specific embodiments."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":["FIG. 6","FIG. 5"]},{"@attributes":{"id":"p-0029","num":"0028"},"figref":["FIG. 7","FIG. 5"]},{"@attributes":{"id":"p-0030","num":"0029"},"figref":["FIG. 8","FIG. 5"],"i":"a "},{"@attributes":{"id":"p-0031","num":"0030"},"figref":["FIG. 8","FIG. 5"],"i":"b "},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0033","num":"0032"},"figref":["FIG. 10","FIG. 9"]},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":["FIG. 13","FIG. 11"]},{"@attributes":{"id":"p-0037","num":"0036"},"figref":["FIG. 14","FIG. 13"],"i":"a "},{"@attributes":{"id":"p-0038","num":"0037"},"figref":["FIG. 14","FIG. 13"],"i":"b "},{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 15"}]},"DETDESC":[{},{}]}
