---
title: Cluster of processing nodes with distributed global flash memory using commodity server technology
abstract: Approaches for a distributed storage system that comprises a plurality of nodes. Each node, of the plurality of nodes, executes one or more application processes which are capable of accessing persistent shared memory. The persistent shared memory is implemented by solid state devices physically maintained on each of the plurality of nodes. Each the one or more application processes, maintained on a particular node, of the plurality of nodes, communicates with a shared data fabric (SDF) to access the persistent shared memory. The persistent shared memory comprises a scoreboard implemented in shared DRAM memory that is mapped to a persistent storage. The scoreboard provides a crash tolerant mechanism for enabling application processes to communicate with the shared data fabric (SDF).
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09047351&OS=09047351&RS=09047351
owner: SANDISK ENTERPRISE IP LLC
number: 09047351
owner_city: Milpitas
owner_country: US
publication_date: 20110411
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CLAIM OF PRIORITY AND RELATED APPLICATION DATA","FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION"],"p":["The present application claims priority to U.S. provisional patent application 61\/323,302, entitled \u201cCluster of Processing Nodes with Distributed global Flash Memory Using Commodity Server Technology,\u201d filed on Apr. 12, 2010, and is hereby incorporated by reference for all purposes as if fully set forth herein.","This application also claims priority to U.S. provisional patent application No. 61\/323,351, entitled \u201cDistributed Data Access Using Solid State Storage,\u201d filed Apr. 12, 2010, invented by John Busch et al., the entire contents of which are incorporated by reference for all purposes as if fully set forth herein.","This application is related to U.S. non-provisional patent application Ser. No. 12\/276,540, entitled \u201cScalable Database Management Software on a Cluster of Nodes Using a Shared-Distributed Flash Memory,\u201d filed on Nov. 24, 2008, invented by Darpan Dinker et al., the entire contents of which are incorporated by reference for all purposes as if fully set forth herein.","This application is related to U.S. non-provisional patent application Ser. No. 12\/983,754, entitled \u201cEfficient Flash Memory-Based Object Store,\u201d filed on Jan. 3, 2011, invented by John Busch et al., the entire contents of which are incorporated by reference for all purposes as if fully set forth herein.","This application is related to U.S. non-provisional patent application Ser. No. 12\/983,758, entitled \u201cFlexible Way of Specifying Storage Attributes in a Flash-Memory Based Object Store,\u201d filed on Jan. 3, 2011, invented by Darryl Ouye et al., the entire contents of which are incorporated by reference for all purposes as if fully set forth herein.","This application is related to U.S. Non-provisional patent application Ser. No. 12\/983,762, entitled \u201cMinimizing Write Operations to a Flash Memory-Based Object Store,\u201d filed on Jan. 3, 2011, invented by Darpan Dinker, the entire contents of which are incorporated by reference for all purposes as if fully set forth herein.","This application is related to U.S. non-provisional patent application Ser. No. 13\/084,368, now U.S. Pat. No. 8,868,487, entitled \u201cEvent Processing in a Flash Memory Based Object Store,\u201d filed Apr. 11, 2011, invented by Manavalan Krishnan et al., the entire contents of which are incorporated by reference for all purposes as if fully set forth herein.","This application is related to U.S. non-provisional patent application Ser. No. 13\/084,432, entitled \u201cWrite Operations in a Flash Memory-Based Object Store,\u201d filed Apr. 11, 2011, invented by Xiaonan Ma et al., the entire contents of which are incorporated by reference for all purposes as if fully set forth herein.","This application is related to U.S. non-provisional patent application Ser. No. 13\/084,511, now U.S. Pat. No. 8,793,531, entitled \u201cRecovery and Replication of a Flash Memory-Based Object Store,\u201d filed Apr. 11, 2011, invented by Johann George et al., the entire contents of which are incorporated by reference for all purposes as if fully set forth herein.","This invention relates to distributed object stores, and more particularly to, shared global memory accessible to object stores executing on a plurality of nodes using flash memory.","Database programs are one of the most widely used and useful applications of computers. Data records may be stored in database tables that are linked to one another in a relational database. Queries from users allow database programs to locate matching records and display them to users for modification. Often a large number of users access different records in a database simultaneously.","Database records are typically stored on rotating hard disks. Computer hard-disk technology and the resulting storage densities have grown rapidly. Despite a substantial increase in storage requirements, disk-drive storage densities have been able to keep up. Disk performance, however, has not been able to keep up. Access time and rotational speed of disks, key performance parameters in database applications, have only improved incrementally in the last 10 years.","Web sites on the Internet may link to vast amounts of data in a database, and large web server farms may host many web sites. Storage Area Networks (SANs) are widely used as a centralized data store. Another widespread storage technology is Network Attached Storage (NAS). These disk-based technologies are now widely deployed but consume substantial amounts of power and can become a central-resource bottleneck. The recent rise in energy costs makes further expansion of these disk-based server farms undesirable. Newer, lower-power technologies are desirable.",{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 1","b":["16","12","10","10","16","12","16","14","16","14"]},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 2","b":["16","16","14","16","16","16"]},"Adding second database \u2032 increases the power consumption, since a second set of disks must be rotated and cooled. Operating the motors to physically spin the hard disks and run fans and air conditioners to cool them requires a substantially large amount of power.","It has been estimated (by J. Koomey of Stanford University) that aggregate electricity use for servers doubled from 2000 to 2005 both in the U.S. and worldwide. Total power for servers and the required auxiliary infrastructure represented about 1.2% of total US electricity consumption in 2005. As the Internet and its data storage requirements seem to increase exponentially, these power costs will ominously increase.","Flash memory has replaced floppy disks for personal data transport. Many small key-chain flash devices are available that can each store a few GB of data. Flash storage may also be used for data backup and some other specialized applications. Flash memory uses much less power than rotating hard disks, but the different interfacing requirements of flash have limited its use in large server farms. Flash memory's random-access bandwidth and latency are orders of magnitude better than rotating disks, but the slow write time of flash memory relative to its read time complicates the coherency problem of distributed databases.","Balancing workloads among the servers is also problematic. Database server  may become busy processing a particularly slow or difficult user query. Incoming user queries could be assigned in a round-robin fashion among database servers , \u2032, but then half of the incoming queries would back up behind the slow query in database server .","Improvements in cost, performance, and reliability in data processing systems are made possible by flash memory and other high speed, high density, solid-state storage devices. These improvements are of limited benefit in some scalable cluster systems where data must be partitioned across multiple processing nodes and locally accessed, or placed on a dedicated Storage Area Network, or shared through application inter-process communication. The overhead involved in these existing techniques consumes much of the performance and cost advantage inherent in high density solid-state memory.","Approaches for shared global memory accessible to a plurality of processes in a distributed object store that is implemented, at least partially, on flash memory are described. In the following description, for the purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of the embodiments of the invention described herein. It will be apparent, however, that the embodiments of the invention described herein may be practiced without these specific details. In other instances, well-known structures and devices are shown in block diagram form in order to avoid unnecessarily obscuring the embodiments of the invention described herein.",{"@attributes":{"id":"p-0050","num":"0049"},"figref":"FIG. 3","b":["242","243","244"]},"Database tables are stored in flash memory  in each node , , , with each node typically storing a different set of database tables. Shared address space  (also called a node address space) is used to access flash memory of one or more of the nodes. The nodes provide one or more partitions (also called shards) of the shared address space.","Multiple instances of DataBase Management System (DBMS) program , \u2032 are executing within shared address space  and are able to access data items in the distributed database that are physically stored in flash memory  on nodes , , . Having multiple instances of database management system (DBMS) program , \u2032 also improves reliability and reduces bottleneck problems, since user queries may be dispatched to different executing instances of DBMS program , \u2032.","Having DBMS program  execute in shared address space  allows the program to see just one address space, simplifying DBMS program . Ideally, it would be desirable for multiple executing instances of DBMS program , \u2032 running on different physical nodes to observe shared address space  so that each could operate as if it is the only executing instance of DBMS program . Thus major modifications and re-writes of the program code of DBMS program  could be avoided using shared address space . A DBMS program written for execution on a single address space is preferred since code does not have to be re-written. However, without other facilities, the multiple executing instances of DBMS program , \u2032 would contend with each other for the same resources, causing failures.",{"@attributes":{"id":"p-0054","num":"0053"},"figref":"FIG. 4","b":["200","200","242","243","244"]},"SDF  includes an interface for communications between high-level programs such as executing instances of DBMS program , \u2032 and lower-level hardware controllers and their software and firmware drivers. SDF  is accessible by high-level instances of DBMS program , \u2032 using an applications-programming interface (API). Communication between nodes to ensure coherency of database tables stored in flash memory  on the multiple nodes is performed by SDF .","Normally, adding nodes provides a less-than-linear performance improvement, since bottlenecks may occur to data stored in just one location on a node, such as shown on . However, using SDF , data records stored in flash memory  may be cached near executing instances of DBMS program , \u2032 on one or more nodes, allowing multiple processors to access the same data. Coherency of the cached database records is important to prevent data corruption.",{"@attributes":{"id":"p-0057","num":"0056"},"figref":"FIG. 5","b":["18","18","18","22","22","18","18"]},"While DRAM , \u2032 stores transient copies of objects, the objects are more permanently stored in flash memory , \u2032. Objects remain in flash memory , \u2032 and are copied to caches in DRAM , \u2032 in response to access requests by programs running on processors , \u2032.","Sharing data fabric (SDF)  is a middleware layer that includes SDF threads running on processors , \u2032, and APIs and tables of data. A physical interconnect, such as an Ethernet or InfiniBand\u00ae fabric, connects physical nodes together. Object copies are transferred across the physical interconnect by SDF  from flash memory , \u2032 to caches in DRAM , \u2032, and among DRAM , \u2032 caches as needed to ensure coherency of object copies.","Flash memory , \u2032 can be physically located on many nodes, such as having one flash memory  for each processor , or in other arrangements. SDF  makes all the objects stored in flash memory , \u2032 appear to be stored in a global address space, even though the global address spaced is shared among many processors , \u2032. Thus flash memory , \u2032 together appear to be one global, shared flash memory  via SDF . The database is partitioned into many objects, which are stored in a distributed fashion on many nodes within the global shared flash memory.",{"@attributes":{"id":"p-0061","num":"0060"},"figref":["FIG. 6","FIG. 6","FIG. 6"],"b":"36"},"Processor  executes an instance of a DBMS program, threads, and other routines and accesses a local memory that stores program code and data, such as DRAM . DRAM  also acts as a cache of objects such as database records in the global, shared flash memory.","Processor  also connects to PCIe switch . PCIe switch  allows processor  to communicate with other nodes through NIC  to send and receive object copies and coherency commands. Flash modules  contain arrays of flash memory that store permanent objects including database records and tables. Flash modules  are accessed by processor  through PCIe switch .",{"@attributes":{"id":"p-0064","num":"0063"},"figref":"FIG. 7","b":["116","122","119","116","112","102","114"]},"Compute nodes  are compute nodes, such as node  shown in , with processors, DRAM caches of objects, and interconnect. These compute nodes may be constructed from commodity parts, such as commodity processors, interconnect switches and controllers, and DRAM memory modules.","Sharing data fabric services  allow application programs  and DBMS database program  to control policies and attributes of objects by executing routines and launching threads of sharing data fabric  that are executed on compute nodes . The exact location of objects and database records within flash memory  is transparent to application programs  and DBMS database program  since sharing data fabric  copies objects from flash memory  to DRAM caches in compute nodes  and may obtain a copy from any location in flash memory  that has a valid copy of the object. Objects such as database tables may be replicated to make back-up copies in flash memory .","Sharing data fabric  performs consistency and coherency operations such as flushing modified objects in a DRAM cache to copy back and update the permanent object in flash memory . Sharing data fabric  may also migrate flash objects to new flash pages for wear-leveling or other purposes, and update version logs and transaction logs.","Interconnect  includes the PCIe switches in each of compute nodes , and the high-speed interconnect between nodes, such as Ethernet or InfiniBand links. Sharing data fabric  sends objects and coherency commands across interconnect  or directly within the compute node, such as directly to flash management .","Flash management  is activated to migrate flash blocks for wear-leveling and replication. Wear-leveling schemes assign flash blocks for writing in a rotating, least-written, or other fashion to even out usage of flash blocks and prevent early wear-out and failure. Write buffers of flash blocks, logs, and caches may be kept by flash management .","Flash interface  is an interface between flash management  and hardware flash controllers , which control low-level access of flash memory . While flash memory  may have separate modules on different nodes of compute nodes , sharing data fabric  uses interconnect , flash management , and flash interface  to transparently move objects to and from flash memory  on different nodes. Flash memory  in aggregate appears to be a single, unified flash memory that is transparently shared among many instances of DBMS database program  running on many compute nodes .",{"@attributes":{"id":"p-0071","num":"0070"},"figref":"FIG. 8","b":["119","120","119","119","120","128","119"]},"Instead, standard, substantially unmodified DBMS database program  is used, but instead of using network interface  directly, DBMS database program  accesses sharing data fabric  using API's . API's  include SDF_GET, SDF_PUT, SDF_LOCK, and other SDF-specific versions of start, abort, commit, savepoint, create, delete, open, and close commands. For example, lock and unlock commands lock and unlock an object using a lock table in sharing data fabric  to prevent another user from accessing that object while locked. A node map in sharing data fabric  maps objects to address locations in flash memory, allowing sharing data fabric  to read and write objects in flash memory through flash management  and flash interface .","Objects that reside in flash memory on a first node may be accessed over sharing data fabric  by sending and receiving messages, and sending object data from a second node over network interface . These messages may include commands  such as get, put, lock, unlock, start, and abort. These commands  are executed by SDF  using detailed information on the object's location, such as a file handle or address, that are obtained from a node map, a sharing directory, or a cache map in sharing data fabric . Commands  and messages are received by a sharing data fabric  on the first node, which may access its flash memory to obtain the object. On a read access, sharing data fabric  on the first node can then send the object data back through network interface  to the second node's sharing data fabric .",{"@attributes":{"id":"p-0074","num":"0073"},"figref":"FIG. 9","b":["40","36","36","50","52","54","18","24","22","40"]},"In response to a user database query, a thread executing on processor  on node  requests access to object , which is present in flash memory  on node . The SDF on node  reads object  from flash memory  and copies the object into its object cache in DRAM  as object copy \u2032. The DBMS program running on node  can then read object copy \u2032 from its DRAM . In this example transfer over data fabric switch  was not needed.","In a second example, a thread executing on processor  on node  requests access to object , which is not present in flash memory  on node , nor in DRAM  on node . The SDF on node  determines that node  is the home node for object . Node  may perform a lookup in a directory to locate the object's home node. The directory may have several parts or levels and may reside partially on local node  and partially on other nodes.","An SDF thread on node  sends a message to the home node, node , requesting a copy of object . In response, another SDF thread on home node  reads object  from flash memory  and sends object  over data fabric switch  to local node . The SDF thread on local node  copies the object data into its object cache in DRAM  as object copy \u2032. The DBMS program running on local node  can then read object copy \u2032 from its object cache in DRAM .","Object  may have already been copied into DRAM  on node  as object copy \u2033. Rather than read object  from flash memory , when object copy \u2033 is present, object copy \u2033 may be read from DRAM  and then sent over data fabric switch  to node  to load object copy \u2032 into DRAM  on node .",{"@attributes":{"id":"p-0079","num":"0078"},"figref":"FIG. 10","b":["200","23","200"]},"DBMS program  has two primary software components that cooperate with each other. Query process  receives requests from users that contain a database query. Storage management  performs access of database records that are cached in local buffer cache . Storage management  includes table reader  that reads records from portions of a database table cached in local buffer cache , table writer  that writes or modifies portions of a database table cached in local buffer cache , and log writer  that logs modifications of records cached in local buffer cache . While query process  performs query logical operations, query process  does not access database records, but instead calls storage management  to read or write a record. Separating query and access functions an improve software reliability.","SDF  is called by storage management  when a requested database record is not cached in local buffer cache , or during flushes when modified records are copied back to the more persistent storage in flash memory. SDF  performs the multi-node operations needed to access data at one of nodes ,  and copy that data into local buffer cache  or update that data in flash memory at the node.","The database record may be stored in flash memory at any of nodes , . Each node ,  has local map  that locates the requested data item stored in flash memory at that node. The requested data may be a database record stored in database tables , or a database index in database indices . Other data stored at a node may include log files  or checkpoints  that are useful for error recovery.","An example of a Database Management System (DBMS) is a system of one or more software programs, which are written to enable the storage and management of user information in a highly structured and well-defined way. The DBMS enables certain storage properties such as Atomicity, Consistency, Isolation, and Durability, the so-called ACID properties.","Information may be stored as data records organized in n-tuples (also termed simply tuples) of closely related information called rows. A field of a row stores one of the records. Collections of rows are called tables. One or more of a particular field within a table is called a column. A collection of related tables is called a database, and the structure of the related tables is called a schema. Data records may also be referred to as data attributes or data items or objects.","A relational database is a database where information is stored, accessed, and indexed according to specified values of the respective fields, known as a relation. The specified values include specified functions of values, such as ranges of values.","For example, a query is constructed for a relational database which is intended to retrieve only those tuples from the relational database that have a first item record conforming to a first specification in the query AND a second data item conforming to a second specification in the query, where the logical operator AND is also part of the query. Continuing the example, all of the field specifications and the logical operator AND comprise an entity called a relational query specification. In general, a relation refers to the relationship of data items or attributes within a table, or even to the table itself.","The DBMS may be transactional, allowing user operations to be performed as transactions that have well-defined properties. The properties may include an isolation property that ensures that multiple concurrent and distinct transactions operating on the database do not interfere with each other. The transactions each perceive the state of the system as if each of the multiple transactions is the sole transaction executing in the database management system.","Another transaction property is atomicity, meaning that the transaction can be aborted prior to committing any changes to the database management system. The result of aborting the transaction is no change to any record in the database.","The durability property indicates that once a transaction is committed to permanent storage, any database record changes due to the transaction remain stable even if the system restarts or a power failure or other kind of defined failure occurs.","These properties of transactions may be ensured for a database by a log file in log files . A log file is a data structure in which the database management system maintains a sequential record of all data modifications. A log file is used, for example, to record committed transactions and to record abort operations on uncommitted transactions.","When there are multiple users, there may be multiple local buffer caches  on multiple nodes. There may be several instances of query process  operating on one compute node, using a shared local buffer cache , for processing queries by different users.","All local buffer caches  and the permanent storage in flash memory of nodes ,  must be kept logically consistent with one another. Periodic checkpoints to flush modified contents of the buffer cache to the permanent storage as checkpoints  may be used as one way of maintaining consistency.",{"@attributes":{"id":"p-0093","num":"0092"},"figref":["FIG. 11","FIG. 10"],"b":["20","270","23","202","270","200","204"]},"The SDF optionally performs a global data lock operation, step , in order to ensure that there is a single modifiable copy of the particular data item. The SDF_GET operation may retrieve a modified copy of the particular data item from flash memory or from a local buffer cache on another one of the nodes. A data versioning operation may also be performed to identify the most recent version of the data item, and to save a copy of the most recent version, in case subsequent modifications to the data item need to be rolled back, or un-done. Lock and version operations may not need to be performed in some instances, such as database reads, but may be needed for transactions, such as when writing to database records.","The SDF reads the requested data item from the node, such as from flash memory at a home node for a database record, or from a local buffer cache of a sharing node that has earlier cached the database record, step . The requested data item is then loaded into local buffer cache  by the SDF, step . The SDF may also return a pointer to the data in the local buffer cache so that DBMS program  has the pointer.","A cache pin operation may be performed by the SDF, step . A cache pin operation ensures that the particular data item remains (is pinned) in the local buffer cache. This guards against another request from this or another thread causing replacement of the data item in cache.","SDF returns a success code to the calling program, such as storage management  in DBMS program , step . Storage management  may then access the data item, step . Writes to the data item may be allowed.","After DBMS program  has finished accessing the data item, step , DBMS program  calls the SDF to unlock the data item, step , if that data item was previously locked in step . When the data item was pinned by a cache pin operation in step , then DBMS program  calls the SDF to perform a cache unpin operation, step .","When the data item was written in step , the DBMS program optionally calls the SDF to flush the modified data item back to the flash memory at its home node, step . The SDF then stores the modified data item at its home node, step . Various logs may need to be written by the DBMS program or by the SDF to ensure that this write back is durable. The DBMS program optionally calls the SDF to perform a SYNC operation, step , so that the modified data item is made visible to other nodes, step . The SDF_SYNC operation makes the results of selected previous operations including the modified data item visible to other instances of DBMS program \u2032 that are executing on the local node or on other compute node in the system. A global cache directory at the home node may be updated to make the modified data item visible to other nodes.","Other database operations, such as row insertions, row deletions, and index updates, are performed in a similar manner using the SDF API.",{"@attributes":{"id":"p-0101","num":"0100"},"figref":["FIGS. 12A-D","FIG. 12A"],"b":["20","20","24","222","242","243","244","200","20","23"]},{"@attributes":{"id":"p-0102","num":"0101"},"figref":"FIG. 12B","b":["200","252","253","254","242","243","244","200","200","20"]},"DBMS program  accesses data items stores in data containers , ,  using SDF 's API, which is compatible with existing DBMS access methods using file systems and\/or device partitions. Durability and consistency of data stored in data containers may be automatically and transparently maintained by SDF .",{"@attributes":{"id":"p-0104","num":"0103"},"figref":"FIG. 12C","b":["248","200","200","248","200"]},"SDF  allows direct substitution of data fabric primitives such as SDF_MUTEX for similar primitives such as SMP_MUTEX provided by platforms supporting a single address space. For example, SDF  provides a test-and-set operation which is used to create a MUTEX operation that is a direct replacement for an SMP-based MUTEX operation.","Programs written for the SMP interfaces require little or no modification to operate with SDF . The MUTEX synchronization operations are the enabled to operate transparently across multiple nodes in a cluster.",{"@attributes":{"id":"p-0107","num":"0106"},"figref":"FIG. 12D","b":["200","200","240","20","200","240"]},"Special synchronization operations such as test-and-set, and compare-and-swap, are implemented efficiently within SDF  and provided to DBMS program  through the SDF API, which has special compatible functions for these operations.","Since the SDF API has functions with interfaces matching the interface of these primitive operations, programs written for the primitive-operation interfaces require little or no modification to operate with SDF . The existing calls to the synchronization operations are redirected to use objects in the new synchronization container. The SDF synchronization operations may be enabled to operate transparently across multiple nodes in a cluster.",{"@attributes":{"id":"p-0110","num":"0109"},"figref":"FIG. 13","b":["20","200","200"]},"For example, a DBMS storage manager instance inserts a record into log file container . In response to a request from a transaction executing in any DBMS program instance, the DBMS storage manager instance performs a data fabric PUT RECORD operation to add a log record to log file container . The SDF performs the operation, updating the contents of log file container  and updating internal pointers. This operation may be done atomically by the SDF and be recoverable by the SDF when failures occur in the hardware or software.","The SDF may perform replication operations to replicate the updated record, and wait for the log record to be permanently stored before returning a success code to the DBMS storage manager instance. Each PUT RECORD operation may be associated with a Logical Sequence Number (LSN) from LSN generator . DBMS program , \u2032 is able to subsequently use the LSN to retrieve the respective log record should a database recovery or rollback operation be necessary. Various techniques for logging and recovery may use the log file container .",{"@attributes":{"id":"p-0113","num":"0112"},"figref":"FIG. 14"},"Home node  is the permanent location of object  in flash memory . Action node  is executing an application program that requests an object. Sharing node  has a copy of the object in its object cache in DRAM .","A program executing on action node , such as executing instances of DBMS program  (), requests access of object . Object  is not yet present in DRAM  of action node , so the SDF determines the object's home node, such as by hashing the object's name or identifier or looking up the object in a table.","Transaction table  is stored in DRAM  of action node  and eventually contains information to identify home node . In response to a request from action node  to access object , the SDF on home node  looks up the object in its object directory . Object directory  indicates that although object  is present in flash memory  of home node , this object  is stale. A modified object \u2032 is present in DRAM  of sharing node  in this example.","Since modified object \u2032 is more current than stale object  in flash memory , SDF on home node  sends a message to SDF on sharing node . This message causes transaction table  on sharing node  to be checked to make sure that modified object \u2032 is not locked for use by a program executing on sharing node . If modified object \u2032 is locked, action node  waits until the lock is released. Release of the lock causes an update of transaction table  on sharing node .","When transaction table  indicates that modified object \u2032 is unlocked, SDF on sharing node  sends the object data of modified object \u2032 over data fabric switch  to action node . Object copy \u2033 is created in DRAM  on action node . The requesting program on action node  can now access object copy \u2033.","Other steps may be performed to update object directory  and stale object  in flash memory  on home node , although this may be delayed to allow home node  to update object copy \u2033 to reduce the number of writes and operations to flash memory .",{"@attributes":{"id":"p-0120","num":"0119"},"figref":["FIG. 15","FIG. 7","FIG. 6"],"b":["100","114"]},"Node  has threads running on processor , including application thread  and\/or SDF thread . Application thread  can be an executing instance of DBMS program  on local node .","Messaging  allows SDF thread  to send and receive messages from other nodes. Messaging  may use software and hardware such as interconnect  of , NIC's  of , , and other hardware such as switches.","Node  includes DRAM  and flash memory . The DRAM state shown is a snapshot of the contents of DRAM  at a particular point in time, while the flash state is a snapshot of the contents of flash memory  at approximately that same time.","Flash memory  at each node stores flash objects , which may be grouped into containers. A flash object may be uniquely addressable in the SDF using a container-identification and an object identifier. Metadata  stored in flash memory  may include container metadata and global metadata. Container metadata describes the attributes or properties of objects in a container (such as a number of replicas for the container). Global metadata may include virtual-to-physical node-identification mappings and\/or hash functions. The hash function is performed on the object's name to generate an identifier that can be used in further table lookups to locate the object using a global map.","Object cache  in DRAM  stores copies of objects that are also stored in flash memory  of the home node, which may be node  or may be another node. DRAM objects  are objects that have node  as their home node, but the object's attributes specify that the object primarily resides in DRAM  rather than in flash memory . For example, frequently-accessed objects such as database indices  of  may be selectively enabled to remain in DRAM  rather than the flash memory to improve look-up performance.","DRAM  also stores state information for particular classes of nodes. Action node state  stores state information for objects using node  as the action node, while home node state  stores state information for objects using node  as their home node, such as DRAM objects  and flash objects .","A home node is a well known, an authoritative source of the object, which resides in DRAM, flash memory, or another component in the storage sub-system. While an object may be cached and\/or replicated in DRAM and\/or flash memory at several nodes, only one of these nodes (at any one time) is considered to be the home node for that object. An action node stores a transient copy of an object. The action node usually obtains a copy of the object from the home node. Node  can operate as the home node for some objects, and as the action node for other objects.","Transaction table  in action node state  stored in DRAM  has entries to track transactions. Transaction table  keeps a list of all objects accessed (touched) during a transaction, and may keep a copy of the object's initial state when the object is modified, or a pointer to a log of changes from an initial state that is archived. The initial state pointed to by transaction table  aids in object recovery if an abort occurs. Transaction table  provides recoverability for threads of applications and SDF services that execute on the node's processor. There may be links between tables, such as links in entries in transaction table  to entries in transient protocol state table .","A transaction is a series of operations. The transaction as a whole succeeds as a unit or the transaction and all operations in the transaction are aborted. This may also be referred to as an atomic set of operations. In a transactional system that maintains isolation among concurrent transactions, there are no unexpected effects from an aborted transaction since either all operations in the transaction are executed completely, or any partially-executed operations of the transaction are aborted without visible side effects. Transactions are extremely useful for distributed database programs, since a database record may be updated as a transaction, preventing a partial update of that database record.","Transient protocol state table  in action node state  stored in DRAM  has entries to track outstanding requests. The requesting thread, type of request, request status, and order of requests from each thread are stored in this table, which is shared by all threads at node .","Home node state  stores information for objects that have node  as their home node, and are thus stored (homed) in DRAM  (as DRAM objects ) or flash memory  (as flash objects ) of node . Home node state  in DRAM  stores a sharing list within global cache directory , and flash object map . Global cache directory  stores the state of all objects that have node  as their home node and have been cached in one or more nodes in the system. The state of the object in global cache directory  is one of shared, modified, invalid, and either locked or unlocked. An indication of whether the object is being accessed within a transaction may also be stored in global cache directory . The locations of cached copies at other nodes, and the state and progress of any coherency operations for the objects are also kept in global cache directory .","Flash object map  maintains a map between an object identifier and its location in flash memory . Flash object map  may store an address, a partial address, or an identification of the location of the object in flash memory . Flash object map  maps at least a portion of a respective identifier of a particular one of the objects to a physical location in flash memory . Flash object map  has entries for all flash objects  that belong to this home node, regardless of whether that object is cached at any node. Flash object map  may be a homed object map that also has entries for all DRAM objects  at this home node.","Database Transaction Flows Using SDF\u2014.","Snapshots of the states and movements of database objects and SDF messages among two nodes in a multi-node shared flash memory system are shown in the examples of . Snapshot diagrams, similar to that of , are shown for action node  and home node .","Extensions of these flows could involve sharing node  () when sharing node  contains a modified copy of the requested object. Then the requested object is sent from sharing node  rather than from home node , since the flash object at home node  is stale. Messages passed among these nodes, including the database object being copied, are shown by the arrows.","These examples are for operations that are part of transactions. To provide ACID properties to executing instances of DBMS program , \u2032, operations that access database records are organized as transactions. Transactions allow the operations of a transaction to be committed together as an atomic unit, preventing partial updates of database records that can corrupt the database. Operations that are not part of transactions could also be used for less important accesses, such as status inquiries or database reads. When a requested object is already present in object cache  of action node  (a hit), application thread  may simply read the object from object cache .",{"@attributes":{"id":"p-0137","num":"0136"},"figref":"FIG. 16","b":["90","84","90","50","52","50"]},"Application thread  uses the address or identifier for home node  to send a message to home node . This message requests the object from home node . At home node , the message received from action node  activates SDF thread , which looks up the object identifier in global cache directory  at home node . In this example, no copies of the object have been cached by any other nodes, so a directory miss occurs.","SDF thread  running on home node  then looks up the object identifier in flash object map  to find the address of the object in flash memory  of home node . Flash memory  is read to copy flash object  stored in the flash memory of home node . A copy of flash object  is sent from SDF thread  at home node  to application thread  at action node  via an interconnect between home node  and action node . Application thread  (or a SDF miss-handling thread such as SDF thread  on action node  invoked by application thread ) then loads the copy of the object into object cache  at action node .","Transaction table  is updated to include an entry for the copy of the object that was just loaded into object cache . This entry identifies the current transaction that requested the object. At the start of a new transaction, application thread  can create a new entry in transaction table , and this entry is updated with a list of objects touched by the transaction as the transaction is processed, or with pointers to sub-entries for each object in that transaction. Application thread  can resume processing the transaction and read the object copy in its object cache .",{"@attributes":{"id":"p-0141","num":"0140"},"figref":["FIG. 17","FIG. 17"]},"When application thread  reaches a commit transaction operation, application thread  reads transient protocol state table . Transient protocol state table  contains a list of all outstanding requests for all prior threads at action node , the status of all requests, and the order of requests for each thread. Application thread  waits until completion of all outstanding requests for the current transaction for application thread . If there are any dependencies among threads, application thread  must wait for completion of dependent requests at other threads, according to any ordering rules.","Once all dependent outstanding requests have completed, as indicated by transient protocol state table , application thread  reads the transaction's entry in transaction table . A list of all objects touched by that transaction is read from transaction table . Objects that were only read do not need to be copied back to the home node, but modified (dirty) objects do need to be copied back.","Each of the modified objects for this transaction are sent back to their respective home nodes, or flushed. A flush operation causes a cached object to be sent to home node  in order to synchronize the most up-to-date state of the object with the source. A flush to the object source in flash-memory provides persistence and a level of durability to the object state.","A flush may not require that flash memory is immediately written with the modified object. Instead, the modified object may be stored in DRAM or a write buffer on home node  when the flush is completed. Later, home node  may perform the actual writing of the modified object to flash memory.","A two-phase commit may be used to avoid contentions with other nodes that may also be accessing one of these objects at about the same time. Action node  may in a first phase indicate a desire to write a modified object back to the home node, and receive a timestamp, and then in a second phase actually write the modified object if there are no objections from other nodes. If another node objects, such as by also indicating a desire to access the same object, the timestamps can be compared, and the node with the earlier timestamp wins.","After the two-phase commit process has succeeded, SDF thread  on home node  locates the homed object inside flash memory using flash object map , and the modified object from action node  is written into flash memory as one of flash objects . Global cache directory  may first be consulted to verify that no other nodes have this object, and invalidations may be sent to any sharing nodes.","Global cache directory  is updated to indicate that action node  no longer has this object locked. SDF thread  on home node  sends a message to action node  to unlock the modified object that was just updated at home node , and application thread  on action node  unlocks the object in object cache . The object could be deleted from object cache , or changed from modified to shared, and changed to the unlocked state to indicate that this object cannot be written until a new lock is obtained form home node .","The transaction's entry in transaction table  is deleted once all modified objects have been successfully flushed to their home nodes, and unlocked in object cache . The transaction in finished and has been committed.","If any modified object cannot be written back to its home node, such as if the home node crashed, then the transaction being committed must be aborted. Any modified objects that have already been written back to their home nodes must be restored to their initial conditions. Log files may be used to recover from this rare situation.",{"@attributes":{"id":"p-0151","num":"0150"},"figref":"FIG. 18"},"Application thread  reads transient protocol state table  and waits for all outstanding dependent requests that are ordered before this transaction to complete. Then transaction table  is read to obtain a list of all objects touched by the transaction being aborted. Transaction table  contains the initial states of all objects in the transaction, or pointers to these states, or other information that allows the initial states to be obtained or generated. For example, the initial state of an object may be stored on the home node of that object.","All touched objects in this transaction are restored to their initial state at the beginning of this transaction, such as by restoring objects in object cache  using the initial states from transaction table . Alternately, each of the touched objects may simply be invalidated in object cache .","Restored objects in object cache  that were locked by the aborting transaction are unlocked, with an unlock message being sent to the home node for each object being unlocked. Home node  updates global cache directory  to indicate that the object is unlocked. Other nodes may now access the object. Once all restored objects have been successfully unlocked, the entry for the aborting transaction can be removed from transaction table .",{"@attributes":{"id":"p-0155","num":"0154"},"figref":"FIG. 19","b":["74","50","74"]},"A sync operation, such as an SDF_SYNC, is performed to synchronize application thread  with other threads on action node  or on other nodes. Application thread  (or SDF thread  if called by application thread  to perform the sync) reads transient protocol state table , which has an ordered list of all outstanding requests for all threads at action node , and the status of those requests. Application thread  waits until all outstanding dependent requests have completed.","The sync operation uses a set of ordering rules to determine which outstanding requests must be completed prior to the sync operation completing. In some embodiments, each sync operation is enabled to select the ordering rules it uses. In a first example, a sync operation executed by an application does not complete until all outstanding operations of the application have completed. In a second example, a sync operation does not complete until all outstanding write operations of a particular transaction have completed. In a third example, a sync operation does not complete until all outstanding operations of the action node performing the sync operation have completed.","Once all outstanding requests, as determined by the ordering rules in use, have completed, the sync operation is completed. Application thread  can resume having synched to other threads.","A lock is requested before application thread  writes to an object that has already been loaded into object cache  on action node . Once locked, other nodes cannot write that object.","Application thread  sends a lock request message to home node . SDF thread  on home node  looks up the object's entry in global cache directory  and waits until the object is available and not locked by any other thread on any node. Once the object is free, SDF thread  on home node  returns the lock to action node .","An ordered queue may be used to process lock requests at home node . In addition to basic mutex-like single state locks, read-write, and upgrade locks, various complex sets of locks may also be implemented (e.g. multi-granularity and hierarchical locks).","Several other embodiments are contemplated by the inventors. For example, while PCIe switch  has been described, other local buses could be used, and switch  could be a HyperTransport switch rather than a PCIe switch. Multi-protocol switches or multiple switches or hubs could be used, such as for supporting HyperTransport and PCIe on the same node. Data fabric switch , PCIe switch , and interconnect  may have overlapping hardware or software and operate to allow messages to pass for SDF.","Rather than using a local-bus switch, other network topographies could be used, including rings, chains, hubs, and links. Although flash memory has been described, other solid-state memories could be used for storing the objects at the home node (homed objects), such as phase-change memory, ferroelectric random-access memory (FRAM), Magnetoresistive RAM (MRAM), Memristor, Phase-Change Memory (PCM), Silicon-Oxide-Nitride-Oxide-Silicon (SONOS) memory, Resistive RAM (RRAM), Racetrack memory, nano RAM (NRAM), and other non-mechanical non-volatile memories. Flash memory uses electrically-erasable programmable read-only memory (EEPROM), although other technologies such as Phase-change-memory (PCM) may be substituted. NAND flash memory may be used to provide advantages in performance, such as write bandwidth, compared to other non-volatile, electronic memory types. Additional levels of storage hierarchy could be added, such as hourly, daily, or weekly backups to disk, tape, or optical media. There could be many flash modules or only one flash module.","While the description herein may describe the global, shared flash memory as being accessible in one global shared address space, in other embodiments, the global, shared flash memory is accessible in a plurality of global address spaces. For example, in some embodiments, each container is accessible by a respective address space.","The Sharing Data Fabric (SDF) is a unified user-space mechanism to access and store data into hierarchical DRAM, flash memory and the storage sub-system of a clustered or distributed set of compute nodes. SDF uses user-defined attributes to control access, consistency, duplication, and durability of objects in storage. To each application executing on any compute node, the distributed data and storage appears to be logically one big device with integrated cache, memory and storage.","The layers of software and hardware in  may use various combinations of hardware, firmware, middleware, drivers, software, etc. and the layers may be modified in various ways. The connections and interfaces between layers and components may also vary from the simplified diagrams of , . Executing instances of DBMS program , \u2032 may operate on multiple nodes, with one instance per node, or many instances per node. Several query processors could share a common storage management , or each query processor could have its own storage management . Many other arrangements and partitionings of blocks are possible. DBMS  may be substantially unmodified, yet be relinked or use a different library, or may change some routine names, and may changing how locking is done.","When transactions are not supported or used, transaction table  and (optionally) transient protocol state table  may be omitted. Other tables, lists, or data structures may be used to track SDF operations at the action and home nodes. Tables may contain entries in table format, or as linked lists, or in other formats, and can be flat, hierarchal, multi-level, or in a variety of other formats. Global cache directory  may contain sharing lists with or without other information.","Transient protocol state table  in action node state  stored in DRAM  has entries to track outstanding requests. Rather than storing information on threads, information on contexts may be stored in state table . The requesting context, type of request, request status, and order of requests from each context are stored in this table, which is shared by all contexts and their threads at node . An indirection of \u201ccontext\u201d is used to link a sequence of activity of gets, puts, etc. An application thread can use multiple contexts, or multiple threads can use one context. Application threads cannot see any SDF related tables, only SDF protocol threads can. By using contexts, monitoring of what application thread is calling which SDF calls is not needed. This makes the API more flexible.","For , an asynchronous messaging model could be enabled by activating a receiving SDF thread  at action node  when a message returning an object copy is received from home node . Then the return arrow would go to SDF thread  rather than to application thread  at action node .","In this variation, receiving SDF thread  then loads the object copy into object cache  of action node  and application thread  can use the object copy. This handoff using the receiving SDF thread isolates application thread  from the details of MPI messaging and may improve robust multi-threaded execution.","While a database program requesting an object has been described, other kinds of programs such as networked services, applets, proxies, clients, servers, etc. may request objects and operate in a manner similar to that described for application programs . Each node could run one application program such as a server application, or multiple programs of the same or differing types. These programs may themselves perform some caching of data. Some applications or networked services may bypass SDF and reach the network interface directly, or may do so for some kinds of accesses that do not require SDF. Other kinds of API calls and network procedures or calls may be used than those listed in , and additional API functions may be added. Different kinds of messaging between nodes may be employed other than MPI or MPI-like messaging.","While computing nodes have been described as each having a processor, DRAM cache, flash memory, and a NIC, some nodes could be compute-only nodes without any flash memory. Other nodes may be storage-only and have flash memory but do not execute application programs . Nodes may have more than one processor, and may have additional memories, such as a read-only memory for program code, static random-access memory (SRAM), or other DRAM. Several levels of processor cache may be present that may keep even more transient copies of objects in DRAM . The processor may include one or more processor chips, which each may have one or more processor cores. For example, in some embodiments the processor includes two, quad-core AMD Opteron\u2122 processor chips.","A computing node may have a processor that executes both a web server and a database server application, or a combined application or applet. The compute node may be able to execute several such applications simultaneously, or several threads that can each handle one or more remote user requests. Software may be partitioned or combined in many different ways. In a first example, some or all of the SDF API's are statically linked into an application program. In a second example, some or all of the SDF API's are in a library that is dynamically linked into an application program. In a third example, some or all of the SDF API's are integrated with an operating system. In a fourth example, a stub library is used to replace at least some operating system calls (such as for file operations) with calls to API's in an SDF library.","The NIC may connect to an Ethernet, an InfiniBand, or some other kind of network, or even multiple network types. While two NIC's  are shown per node in , one NIC  may be used per node, or more than two. Nodes may be asymmetric and have different hardware rather than all be identical. In , the homed object may be stale, and a more recent modified object from a third-party sharing node may be fetched instead. An acknowledgement to home node  then may come from the sharing node rather than from action node . Other flow modifications are possible.","In some systems, compute nodes may have multiple processors that have separate DRAM caches but share flash memory. Groups of logical compute nodes may use the same physical hardware. One compute node could act as both action node  and home node  for one object in some of these variations. SDF could send messages between processors on the same compute node that are acting as action node  and home node . These intra-node messages may or may not use NIC .","Direct-memory access (DMA) may be used to transfer blocks of data, rather than using I\/O or other instructions by processor  (). The terms permanent and transient are relative to each other rather than absolute. Transient objects in DRAM caches may be flushed or copied back to flash periodically, or after a period of no accesses. DRAM caches may be organized per node, per application, per thread, per container, and various other organizations. A permanent object in flash may still be moved from one node to another, or from one flash block location to another at the same node, such as for wear-leveling, data migration, or de-fragmenting purposes. Permanent is meant to imply durability, or retention of data upon power loss, rather than an unlimited life-time of the object.","A particular object may be replicated, so that several copies of the object are located in flash memory of several different nodes. This replication improves reliability since the home node can fail and one of the back-up replica copies may then be used as the home object. At any given time, only one of the replicas may be designated as the permanent object in flash, while the others are kept as backups. The replica copies may provide additional read-only access to the object.","The node chosen as the home node could change over time to spread the wear of the replicas. Multiple replicas could be used as home objects, but for different groups of nodes, so that each group of nodes has only one of the replicas as its home for the object. In some embodiments, replicas provide read-only access, and write access is performed at the home node.","While the permanent object has been described as residing in flash memory of the home node, and transient copies of the object have been described as residing in DRAM cache on any node, some classes of object may have varying properties. For example, some objects may be stored primarily in DRAM rather than in flash memory, such as DRAM objects  of . Some objects may be DRAM-only objects that are never stored in flash memory. Instead, the permanent object is in DRAM at the home node. Alternately, some objects may not be allowed to be present in DRAM caches, but only in flash memory at the home node. These flash-only objects may be copied to DRAM for a specified short period of time only. Some objects may have time limits for residing in DRAM, or may specify how often they must be copied back to flash memory. The maximum number of shared copies allowed may be specified as an attribute.","Objects are a type of element that can be stored in a container. Elements can be fixed-sized or variable-sized. Variable-sized elements may be termed objects. The description above applies similarly to objects that are fixed-size elements, with some differences such as an ability to access fixed-size elements using, for example, a table look-up (based on an address of the element) rather than a hash (based on a key of the object's name).","To aid consistent distribution and location of data through the clustered or distributed flash-memory system, metadata  () may include an indication of a hash function to be performed on the object identifier to generate an identifier of a unit of storage known as a shard, and an indication of a hash function on the container name to generate a node name. A shard-mapping table maps shard identifiers to nodes (for example, via virtual node names), and another table may be used to map virtual node names to physical node addresses to provide a level of indirection in order to enable dynamic configuration of the distributed flash-memory system. Flash object map  may be a node map or a local map. Metadata  may be stored in flash memory  or may be stored in another memory such as a non-volatile write buffer or a battery-backed memory.","In addition to the threading model described where each request is handled by a thread, an event based model could also be used where contexts (with state and progress indicators) are maintained for each request utilizing a handful of threads. Application thread  in  uses the address or identifier for home node  to send a message to home node . Alternatively, at any time during processing of an outgoing request, the application thread may hand off the request to the SDF thread, which performs any of the determining or messaging functions on behalf of the application thread.","The word \u201cmay\u201d indicates optionally and\/or selectively. An object name may be an object identifier, and an identifier can be a name, key or address. The term thread is sometimes used generically to refer to a locus of execution, and may refer to one or more threads, processes, programs, applications, applets, objects, executing contexts, etc.","In addition to storing all tables in DRAM, tables could also be stored in a DRAM and flash-memory based hierarchy. Tables could be backed up to flash periodically. Backing up tables to flash memory could be specific to a local node or global. In addition to application thread  executing SDF protocols on the action node, SDF may process the protocol on a thread that may process more efficiently and act as a proxy for application thread . Such a thread may reside in the application process or in an SDF daemon on the same node. Various arrangements, partitionings, and levels of threads, programs, routines, and applications are possible.","The techniques described herein are applicable to various types of databases, such as row-oriented databases, column-oriented databases, relational databases, transactional databases, and databases with and\/or without some and\/or all of the ACID properties. While specific examples of SDF commands have been given, there are, of course, many possible ways of arranging and\/or constructing and\/or providing one or more data fabric commands and\/or other ways of controlling and\/or managing a data fabric to achieve similar effect. Transactions may include lightweight transactions such as for locks, and persistent transactions.","The specific API functions provided by the data fabric vary according to different embodiments. Standard APIs used by applications executing in a single address space are replaced with data fabric APIs that transparently enable the application to execute in a distributed fashion as shown in .","Flash interface  and flash management  () may be software, firmware, hardware, or various combinations. Hardware flash controllers  may include firmware. Primitive operation container  () may also contain log files, database indices, and database tables.","A global ordering of operations on the database may be implemented with an ordering protocol. A protocol for coordinating operations from multiple nodes to the same database in order to provide a global ordering, is two-phase commit. In two-phase commit, there are two communication phases between a particular set of nodes for every database update, the first phase to \u201cprepare\u201d the set of nodes and agree on the update, the second phase to perform the update. A logged 2-phase commit process may be used to facilitate failure recovery. Recoverable decision and update schemes for a clustered or a distributed system such as 3-phase commit, voting or consensus may be used in place of 2-phase commits.","Flash memory based log file containers are enabled to store log records in non-volatile memory, such as battery-backed DRAM, accessible via a flash memory controller, and return the success code with DRAM latency vs. flash memory write latency. For example, the latency to store a 256-byte record to non-volatile DRAM memory is less than 10 microseconds. By comparison, storing 256 bytes to some flash memory takes at least 200 microseconds, and could take longer, if small writes such as 256-byte writes are not directly supported by the flash memory. Other kinds of files, records, or containers could also be stored in a similar manner.","The SDF data fabric further enables efficient checkpoint operations. One or more buffer caches, such as one buffer cache per node, are managed by the data fabric for application programs to use. As data items, such as attributes, database records and rows, are modified, the data items are optionally and\/or selectively kept in one of the buffer caches rather than being flushed back to permanent storage. The data fabric may be enabled to provide a copy of a modified data item in the buffer cache of one of the nodes to a program, such as a DBMS storage manager instance executing on another of the nodes and performing a data fabric access operation, such as GET, in lieu of fetching a (stale) copy of the data item from permanent storage.","Periodically, a DBMS program may flush some or all of its modified items back to permanent storage, such as flash memory, to keep the permanent storage contents relatively current. The data fabric may be enabled to accelerate the flushing by automatically copying all of the modified data items back to permanent storage. For example, the DBMS software may use a data fabric GLOBAL FLUSH command to initiate this operation.","DBMS program , \u2032 are scalable, since additional compute nodes may be added, each with a DBMS program , to improve the number of user queries that can be processed simultaneously. All compute nodes are able to access the same partitioned database that is stored persistently in flash memory on the storage nodes. The SDF allows all compute nodes to access the same database on the storage nodes in a scalable fashion, since SDF caches portions of the database being accessed in each compute node's local buffer cache. The performance and query throughput is a linear (or nearly linear) function of system cost and\/or size (number of nodes). The system can be more cost effective and\/or scale to larger sizes than symmetric multiprocessor (SMP) systems. A DBMS which has been written to execute in a single address space can execute as DBMS program  on the scalable hardware and software platform shown in  with minimal or no modifications of software of the DBMS. Certain centralized functions of a DBMS, such as the logging and the checkpointing, are efficiently scalable (and thus efficient for larger systems) without extensive modifications to the DBMS software.","In an embodiment of the invention, a system comprises a number of processing nodes, each node optionally having one or the other or both of the ability to run application programs and the ability to manage some amount of flash memory and\/or other type of high density, solid-state memory. These various types of high density, solid-state memory are referred to herein as flash memory without being interpreted as limiting the scope or applicability of the techniques presented herein.","Advantageously, embodiments of the invention provide for a system in which application processes are executable on any processing node in a cluster, and the application processes are further executable on more than one node, such as for load balancing purposes. Embodiments of the invention further provide for flash memory that is be globally accessible by an application running on one or more nodes in the cluster.","Advantageously, embodiments of the invention also provide for flash memory that is accessible by applications without the overhead of Input\/Output subsystem operations, as is the case typically with, for example, magnetic disk storage. This overhead consumes time and CPU cycles and wastes storage and interconnect bandwidth due to the large fixed-block-sized operations. A lightweight access method is desired to allow the performance potential of flash memory to manifest in scalable cluster systems.","Embodiments of the invention also provide for mechanisms for data access that are either synchronous (meaning that the application program waits for completion), or asynchronous (meaning that the application proceeds in parallel with the operation being performed in the system, and determines at some later point if the operation has been performed, waiting for its completion if desired).","Advantageously, embodiments of the invention also provide for underlying shared memory mechanisms, which implement the lightweight access mechanism, and other system tables and data structures that are robust in the presence of failures, for example a power loss affecting one or more nodes, or a software crash on one or more nodes. In certain embodiments, when some nodes fail, other nodes continue to use the remaining system resources. Methods for resource partitioning and replication are enabled, by use of techniques presented herein, to implement a high availability and\/or fault tolerant system.","Advantageously, embodiments of the invention also provide the communication mechanisms which are configurable such that one or more of nodes, flash memory, and application processes can be added and\/or removed from the system without interrupting the operation of the nodes, flash memory, or application processes already in the system.","In an embodiment, nodes intercommunicate over a data fabric, such as the Schooner data fabric (SDF) or Sharing Data Fabric (discussed above). A data fabric is a unified user-space mechanism to access and store data into a hierarchical DRAM, flash memory, and storage sub-system of a clustered or distributed set of computer systems, and having user-defined attributes to control one or more of access, consistency, duplication, and durability.","Applications on processing nodes are enabled to access portions of the flash memory through node address spaces. The node address spaces can be thought of as reference spaces that application processes running (executing respective application program code) on various nodes in the cluster are enabled to use as a consistent way of referencing portions of the global distributed flash memory.","Additionally, the processing nodes are enabled to cache portions of the flash memory in other memory, such as DRAM in the memory address space of one or more of the application processes.","Application program code is enabled to access flash memory by performing data movement operations, for example SDP_GET and SDP_PUT. These operations in effect transfer portions of contents between flash memory and other memory, such as DRAM in the memory address space of application processes. Additionally, there are control operations which application programs use to determine when previous data operations have been performed in the system. SDP_SYNC is an example of such a control operation.","Techniques presented herein enable the implementation of a general set of data access primitives such as SDP_GET and SDP_PUT in a cluster of processing nodes and nodes containing flash memory. One skilled in the art will recognize that various other data access methods are also enabled by the techniques presented herein.","Application programs use a data Application Programming Interface (API) including, for example, SDP_GET and SDP_PUT, and various other data controls which are suitably enabled by techniques presented herein. One example embodiment of a data API is in a client library, which consists of software code that application programs link into their own code, and which includes application programming interface functions such as SOP_GET and SOP_PUT. Other embodiments are possible and do not limit the scope of the techniques presented herein.","Each node in the cluster runs one or more system processes that handle the processing of one or more of cross-node requests, inter-node communication, inter-node cache consistency, flash memory hardware access, flash memory wear management, data replication, and other cluster services and\/or management functions. System processes optionally and\/or selectively run in user mode as do application processes, and enable a logic layer, suitable to embody a high-level abstraction of the data in flash memory. The logic layer is usable by application programs. Techniques presented herein enable various abstractions, particularly suited for certain application programs, to be built in software on top of the basic data and communication mechanisms of the cluster. Other embodiments of system processes, such as co-location of system process code with the application code in a single process model, are possible within the scope of the techniques presented herein.","The client library code is invoked on data access operations such as SOP_GET and SOP_PUT. The client library code is run entirely in user mode without any context switches or switches to kernel mode. In some usage scenarios, the API library immediately returns the data, such as when there is already a cached copy of the data at the processing node. If communication with other nodes is required as part of processing a data access operation, then a shared memory SCOREBOARD mechanism is used to invoke a system process. Various control and routing information is enabled to be passed through the scoreboard from the application process to the system process. Other embodiments of client library communication with a scoreboard, such as UNIX domain sockets, pipes, messages, or other well known mechanisms, may be employed within the scope of the techniques presented herein.","A system process is responsible for the cross-node processing of SDP_GET and SDP_PUT, and communicates with an invoking application process, or a software agent of the application process, through a shared memory scoreboard. An example processing flow for the SDP_GET operation is presented in , which is an illustration of a process flow of a SDP_GET according to an embodiment of the invention.","As illustrated in , SDP_GET is invoked by the application process (or a thread of the application process) when referencing a particular data element stored in flash memory. Thereafter, execution of SDP library code determines if the operation can be performed locally, for example when a cache hit is detected (step ). If cross-node communication is needed to perform the operation, for example when a cache miss is detected, then the client library code creates a scoreboard entry and invokes the SDP process, which is an example of a system process.","As illustrated in , the system process examines the scoreboard entry and creates a system message to be sent to a system process on the node containing the flash memory or other data resource (step ). The cluster network controller delivers the message to the node containing the data and a system process is invoked on that node to process the message (steps , ). Next, the system process on the node containing the data performs the requested data access operation (steps -) (for example by reading flash for SDP_GET), creates a response message containing the data and other control information, and sends it back to the node running the application process (step ).","Again with reference to , the system process on the node running the application process receives the response message (step ), deposits the data into a buffer in the application process memory address space (step ), updates the scoreboard entry (step ), and optionally signals the application process (step ).","As shown in , the application process at this time considers the operation as having been performed and may proceed (step ) (if it was synchronously waiting), or alternatively may, at any subsequent time, issue a SYNC on the particular operation which will succeed. The scoreboard entry, no longer needed, is freed for future use by a subsequent data access operation.","The process flow illustrated in  illustrates a cache miss.  is an illustration of a process flow for a cache hit at the action node according to an embodiment of the invention. As shown in , a worker thread of the application process at the action node initially consults the object cache. If a cache hit is made, then the data stored in the cache may be obtained by the worker thread. Of course, if the desired data is not present in the object cache, then the process flow of  may be used to enable the worker thread to retrieve the desired data.","In the operation flow depicted in , the communication mechanism between application processes and the data fabric processes must be efficient as possible. Embodiments provide for a shared memory communication mechanism that allows a very efficient implementation and yet provides many additional features. Specifically, these features include meta-data that tracks the progress of an operation as it is processed by the data fabric, DRAM buffers for data read from flash or written to flash, DRAM cache that is shared by one or more client processes on the same node, cluster membership information, mapping tables used to map logical object ID's to their physical location in terms of physical system, flash device, and location with a flash device, and buffering and metadata for the internode messaging system.","In an embodiment, the scoreboard may act as a communication mechanism. The communication mechanism of an embodiment is implemented on top of file-based shared memory that can be mapped in a process address space. The backing file can be placed on traditional persistent media, such as hard disks or flash memory, or may be on a RAM disk. One approach for creating this form of shared memory is using the \u201cmmap\u201d call available on most Unix operating systems. This communication mechanism provides a collection of methods for creating, accessing and manipulating shared memory by one or more application or system processes. The communication mechanism of an embodiment allows objects in a particular shared memory arena to be accessed by multiple user and\/or system processes. Also, the communication mechanism of an embodiment provides for allocation and deallocation of shared memory objects by different processes and well as the dynamic addition\/removal of client\/system processes that have access to the shared memory. The communication mechanism of an embodiment also provides features beyond those provided by standard Unix shared memory facilities.","For example, the communication mechanism of an embodiment provides for reference counts are maintained per allocated object region so that multiple clients can reference the same memory region, and that region will not be deallocated until all clients have released it. As another example, the communication mechanism of an embodiment provides for specific regions of global shared memory to be marked as read-only to prevent data corruption and facilitate the detection of errors. Also, the communication mechanism of an embodiment provides a means of referencing an allocated region of shared memory (object) that can be passed between different user and system processes, and used as pointers within data structures kept within shared memory.","For maximum performance, communication mechanisms for an embodiment may allow shared memory regions to be marked as non-pageable.","A communication mechanism of an embodiment is crash tolerant, i.e., shared memory survives failures of client or system processes. This feature may be achieved by recording, for each allocated object, the identities of all processes that are currently accessing the object. This metadata is kept in the shared memory arenas, which survive process crashes. A shared memory manager process runs in the background and is responsible for handling process crashes. Advantageously, failure of one client on a node does not perturb activities of another client on the same node that doesn't fail. Also, if a system process crashes, then the process can be restarted and the state of shared memory prior to the crash may be recovered. This state of shared memory prior to the crash may provide information after the crash that can be used for debugging purposes. In an embodiment, if a client process crashes, then the shared memory manager updates all shared memory structures that were referenced by the now defunct client process.","The communication mechanism of an embodiment optionally allows an allocated region of shared memory to be asynchronously released by the shared memory manager, for example, if there is no free memory available and the memory manager needs memory for a high priority use, such as an incoming messaging buffer. This is useful for implementing and elastic object cache in which the amount of shared memory used for the cache can grow or shrink dynamically, based on how much memory is required for other SDF functions.","The communication mechanism of an embodiment supports multi-versioning, in which concurrent write operations may operate in parallel on distinct versions of the same object, improving performance. This may be achieved by providing an API (Application Programmer Interface) that allows an application to create multiple version and designate the current \u201cpermanent\u201d version. Note that in this context the \u201capplication\u201d is the Schooner Data Fabric (SDF) application library that implements the higher level get\/set\/etc. operations on top of this high function shared memory.",{"@attributes":{"id":"p-0220","num":"0219"},"figref":["FIG. 20","FIG. 20"]},{"@attributes":{"id":"p-0221","num":"0220"},"figref":["FIG. 21","FIG. 21"]},"As illustrated in , at the home node, the SDF processes use cooperative user level threads. These user level threads service incoming requests and switch on flash and third party cache requests. The scoreboard at the home node stores the state of outstanding requests. Background threads at the home node may perform performance related functionality, such as wear leveling.",{"@attributes":{"id":"p-0223","num":"0222"},"figref":["FIG. 24","FIG. 25"]},{"@attributes":{"id":"p-0224","num":"0223"},"figref":["FIGS. 25-31","FIGS. 25-31"]},"The communication mechanism of an embodiment provides synchronization and ordering constructs to facilitate orderly concurrent access from multiple user\/system processes. The state of the synchronization constructs is preserved in shared memory when a process crashes, so the global memory manager can clean up the synchronization state as needed (e.g., release locks that were held by a crashed process). Examples of such synchronization and ordering constructs are depicted in  according to an embodiment.","One skilled in the art will recognize that various other data access primitives are also enabled by the techniques presented herein. Other data access primitives may include data transactions, bulk copies, and index lookups, and various other protocols.","Many embodiments are possible. Not all of these features need to be present in all embodiments, and many variations and sub-combinations of these features are contemplated by the inventor. The invention could be implemented in hardware, such as hardware logic gates, by a programmable processor either using firmware, software, or other code, or various combinations.","In some embodiments, the client library executes operations such as SDP_GET and SDP_PUT on behalf of an application process completely through the scoreboard, including operations that can be completed locally. This enables further decoupling of the application process from the system process.","In various embodiments, threads of the system process may be run inside the application process. This enables tighter coupling of the application process to the system process.","Values can be inverted, offset, combined with other values, and manipulated in many ways using known mathematical properties. An inversion could be added to an XOR to generate an exclusive-NOR (XNOR), but this is simply a derivative of an XOR and within a family of XOR functions. Other logic tricks and manipulations are contemplated and considered to be within the scope of the invention.","In the foregoing specification, embodiments of the invention have been described with reference to numerous specific details that may vary from implementation to implementation. Thus, the sole and exclusive indicator of what is the invention, and is intended by the applicants to be the invention, is the set of claims that issue from this application, in the specific form in which such claims issue, including any subsequent correction. Any definitions expressly set forth herein for terms contained in such claims shall govern the meaning of such terms as used in the claims. Hence, no limitation, element, property, feature, advantage or attribute that is not expressly recited in a claim should limit the scope of such claim in any way. The specification and drawings are, accordingly, to be regarded in an illustrative rather than a restrictive sense."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["Embodiments of the invention are illustrated by way of example, and not by way of limitation, in the figures of the accompanying drawings and in which like reference numerals refer to similar elements and in which:",{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIGS. 12A-D"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 18"},{"@attributes":{"id":"p-0041","num":"0040"},"figref":"FIG. 19"},{"@attributes":{"id":"p-0042","num":"0041"},"figref":"FIG. 20"},{"@attributes":{"id":"p-0043","num":"0042"},"figref":"FIG. 21"},{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 22"},{"@attributes":{"id":"p-0045","num":"0044"},"figref":"FIG. 23"},{"@attributes":{"id":"p-0046","num":"0045"},"figref":"FIG. 24"},{"@attributes":{"id":"p-0047","num":"0046"},"figref":"FIGS. 25-31"},{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIGS. 32-36"}]},"DETDESC":[{},{}]}
