---
title: Transmitting video and sharing content via a network using multiple encoding techniques
abstract: Embodiments disclose systems and methods for transmitting user-extracted video and content more efficiently by recognizing that user-extracted video provides the potential to treat parts of a single frame of a user-extracted video differently. An alpha mask of the image part of the user-extracted video is used when encoding the image part so that it retains a higher quality upon transmission than the remainder of the user-extracted video.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09386303&OS=09386303&RS=09386303
owner: PERSONIFY, INC.
number: 09386303
owner_city: Chicago
owner_country: US
publication_date: 20131231
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD OF THE INVENTION","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["Embodiments of the present invention relate to transmitting video and sharing content via a network, and in particular, to more efficiently transmitting video and content via a network by transmitting them separately using optimized protocols.","Some video transmission systems merge video and content to be shared into one video stream. In such systems, the video stream may be transmitted using standard video codecs and streaming protocols. Upon receipt, the video and content are displayed for view on a web browser. These systems require no processing of the video stream at the viewer site aside from the processes related to receiving and displaying. Such systems typically treat the combined video and shared content similarly regarding methods of compression, transmission, reception, and display even though different methods may be more efficient or otherwise more suitable for each of the components that went into the video stream.","Where transmission systems send video and content separately, the video itself is typically transmitted using processes that treat the pixels of the video uniformly. Thus, such current transmission systems do not exploit the potential provided by user-extracted video to differentiate between an image part and a background part of the user-extracted video, or between an image part and a non-image part of a user-extracted video combined with another video or other content.","Also, current video transmission systems do not support the use of an alpha mask (also known as an \u201calpha channel\u201d), though there have been efforts to modify current systems to support WebM video with an alpha channel for VP8 video.","Embodiments of the claimed subject matter disclose methods and systems related to transmitting user-extracted video and content more efficiently. These embodiments recognize that user-extracted video provides the potential to treat parts of a single frame of the user-extracted video differently, e.g., the image part of the user-extracted video may be encoded to retain a higher quality upon decoding than the remainder of the user-extracted video. Such different treatment of the parts of a user-extracted video may allow more efficient transmission.","According to such embodiments, a user-extracted video is created along with an associated alpha-mask, which identifies the image part of the user-extracted video. If the image part is more important than the remainder of the user-extracted video, e.g., if it is a higher priority to have a high-resolution image part, it is processed for transmission using methods that preserve its quality or resolution in comparison to the remainder of the user-extracted video. During this processing the alpha mask is used to differentiate between the image part and the remainder of the user-extracted video. The processed video is then sent to a receiving computer.","In an embodiment, content is also selected and combined with the user-extracted video to create a composite video. During processing, the alpha mask is then used to differentiate between the image part and, in this embodiment, the remainder of the composite video.","In an embodiment, a chroma-key is employed to include the alpha mask in the encoded video. Dechroma-keying is then used to re-generate the alpha mask from the sent and decoded video. The re-generated alpha mask is used to determine an alpha value for each pixel of each frame of the decoded video, with the alpha value for a pixel being based on the difference between the pixel color in the decoded video and a key color. The alpha value is then used to determine whether to display that pixel color on the pixel.","In an embodiment, control information regarding a dynamic chroma-key is sent. The control information represents a dynamic chroma-key represents a key color that is not found within the associated image part of the video. This key color was used to replace the remainder of the associated user-extracted video. Should the image part of the video change and a pixel color changes to match the key color, a new key color is chosen to replace the remainder of the associated user-extracted video. The control information is then changed to represent the new key color.","In the following description, numerous details and alternatives are set forth for purpose of explanation. However, one of ordinary skill in the art will realize that embodiments can be practiced without the use of these specific details. In other instances, well-known structures and devices are shown in block diagram form to not obscure the embodiments with unnecessary detail. And the methods described within may be described in one order, but one of skill will realize the methods may be employed in a number of different orders.",{"@attributes":{"id":"p-0028","num":"0027"},"figref":["FIG. 1","FIGS. 11-15"],"b":["100","100","102","104","106","108","110","104","106","108","112","104","106","108","100","104","114","106","116","106","104","108","106","108","108","107","104","106","102","110","105","106","104","108","104","108","108","118"]},"In , receiver  receives content , control information , and user-extracted video . Receiver  processes content  and video  according to control information . An exemplary result of such processing is illustrated in .","Still regarding , user-extracted video  and shared content  may be streamed separately in different ways. This results in potentially three different types of transmitted data, with the differences between the data providing opportunities to individually tailor and optimize the transmission of each type separately from the others.","First, regarding user-extracted video data , chroma-keying processing may be used to embed an alpha mask in the video frame. Such embedding is typically performed in real-time. An alpha mask represents a video frame using 0 or 1 for each pixel of that frame. Where the alpha mask contains a 0, that pixel is part of the background part of the user-extracted video. Where the alpha mask contains a 1, that pixel is part of the image part of the user-extracted video. An alpha mask is created during the extraction of the user from the video, which is discussed within. Video data  may then be compressed using a standard encoder or an encoder according to an embodiment (\u201cZ-encoder,\u201d see the discussion of ). Subsequently, video data  may be sent peer-to-peer or broadcast by a media streaming server using network application layer protocols such as Real-Time Transport Protocol (RTP), Real-Time Messaging Protocols (RTMP), Real-Time Messaging Protocols Tunneled (RTMPT), HTTP Live Streaming (HLS), or HTTP Dynamic Streaming (HDS).","Second, regarding control information , this information is used to synchronize the sharing of content between host\/sender  and receiver\/viewer  displays. For example, should content  be a document and have been sent ahead of video data , then control information  would need information necessary to synchronize the page number of the document with video data . Control information  also contains rendering information, (e.g., the relative position, size, and degree of transparency of the user-extracted video , for rendering that video with the shared content ).","Third, regarding content , such content may include, (e.g., documents, photos, presentation slides, video clips, and web pages) which may be uploaded from, (e.g., a user computer), and also from shared cloud services like Google Docs\u2122, Microsoft Office 365\u2122, YouTube\u2122, Vimeo\u2122, and SlideShare\u2122. By splitting the data and handling different video streams with codecs and protocols that are matched to, or optimized for, the specific streaming data (e.g., still image or video), various system embodiments help to minimize transmission bit rate requirements while retaining visual quality. Codecs and protocols may, for example, be optimized to improve the resolution and frame rate of video , since video typically contains movement. And codecs and protocols for content  may be optimized to improve content details. In embodiments, \u201csmart\u201d strategies are employed that automatically choose different protocols based on the type of data (e.g., video, document, etc.) being transmitted.",{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 1","b":["102","104","110","110","102","102","110","110","102","102","110","102","110","102","110","102","110","102","110","110","102","104"]},{"@attributes":{"id":"p-0035","num":"0034"},"figref":["FIG. 2","FIG. 2"],"b":["200","200","104","202","104","204","108","208","206","108","216","218","218","218","216","204","106","110","204","208","210","212","214","110","110","210","214"]},"In some embodiments, the sender processing flow may be as follows. First, a user persona is extracted from a video (see ). Also, an alpha mask is created to define an image part and a background part of the user persona. And content may be selected for sharing. Then a dynamic chroma-keying technique is applied to the background part. With a dynamic chroma-keying technique, a color not initially found within the image part is used to replace the background part as defined by the alpha mask. Subsequently, should that initial \u201cbackground\u201d color be found in the image part, a new background color is chosen from the colors not appearing in the image part (see  for further discussion). After chroma-keying, the image part and the background part are compressed for transmission. Because it is preferable to have a detailed image part, while the level of detail in the background part is of less importance, the image part may be compressed with methods that preserve more detail while losing smoothness of an object's motion. The background part may be compressed with methods that lose detail, (i.e., \u201clossy\u201d methods, which generally lose detail but maintain smoothness of motion). Thus, using different compression techniques may decrease the bandwidth required to transmit the video while maintaining its overall quality. Information regarding the compression of the image and background parts is preferably included in the control information to facilitate accurate decoding. Control information is then created regarding the persona and content. Such control information preferably facilitates the transmission, reception, and display of the persona and content. And dynamic chroma-key information is added to the control information. Then the user persona, content, and control information are transmitted to a receiving computer. In some embodiments, the protocols for each are chosen to optimize the efficiency of transmission, which may result in reduced bandwidth and\/or an improved resolution. This may not require entirely different protocols. For example, the content and persona are similar data types, and thus the same protocol may be used to transmit the persona and content. Then the persona and content are displayed according to the control information by the receiving computer.","Still regarding , content , combined with user-extracted video , may be displayed by a software client. However, since shared content on the cloud is usually rendered from web pages, web browsers may be used to process, blend, and display content  and video . In some embodiments, the receiver processing flow may be as follows. First, the user-extracted video stream  may be decoded and dechroma-keyed to extract the alpha mask. Second, the background pixels may be set to be transparent when the frame is rendered on a HTML5 canvas  (). Third, shared content  is displayed on a Web iFrame object  () in a web page that viewers open on their web browsers. Finally, canvas , which contains video image part , is rendered on the top of the iFrame object . The location and size of the canvas is specified in the control information (control and signaling data) . Subsequently, additional canvasses containing additional video image parts may be rendered on top of iFrame object , depending on the number of users participating in the session.",{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 3","b":["300","302","304","306","308","310","312"]},"In additional embodiments, the method  may further include the following. Content may be selected to accompany the user-extracted video. This content may be combined with the user-extracted video to create a composite video. In such a case, at , the priority of the image part would be determined in relation to the remainder of the composite video, at  the alpha mask would be used to encode the image part and the remainder of the composite video differently based in part on the priority of the image part, and at  the encoded composite video would be sent to the at least one receiving computer.",{"@attributes":{"id":"p-0040","num":"0039"},"figref":["FIG. 4","FIGS. 5-9","FIG. 9"],"b":["400","402","404","406","408","410","412","414","416","418","420"]},"Regarding step , in some embodiments, the background part is not displayed at the receiver. Thus, it would be inefficient for the whole video frame to be compressed and transmitted for subsequent discarding of the background part at the receiver. Embodiments disclosed herein mitigate this inefficiency by embedding alpha mask information in the color frame and then executing a chroma-keying technique to separate the video frame into an image part and a background part. In such embodiments, the background part may be encoded or transmitted differently (these include, for example, its not being encoded or transmitted at all). Such is the case, for example, with conferencing applications where only the user's image (and not their surrounding environment) is to be combined or shared for embedding with virtual content. This treatment of the background part saves bandwidth by not transmitting unnecessary pixel data.",{"@attributes":{"id":"p-0042","num":"0041"},"figref":["FIG. 5","FIG. 2","FIG. 4"],"b":["500","218","502","504","506","406","218","508","404","510","506","502"]},"The choice of key color preferably satisfies the following requirements: 1) no pixel in the foreground area has the same color as the key color; 2) there is some safe Lnorm distance between the key color and the closest color in the foreground pixel; and 3) the key color does not require frequent change and is chosen to minimize the size of the encoded video packets. The safe Lnorm distance is chosen based on considerations such as data type, compression methods, and decoding methods.","Regarding the second requirement 2), the reason for the safe distance Lis that after applying encoding to the video and sending through the network, (e.g., the Internet), the color values may not be preserved correctly when uncompressed and decoded into the video for display. Rather, the decoder may give out color values that are similar to, but not the same as, the uncompressed ones. Thus, the presence of a safe Lnorm distance ensures that the decoded key color values of the background part are always separated from decoded color values of the image part (or foreground area) of the user-extracted video.","Almost all codecs, such as VP8 or H264, prefer the input video in YUV color space for the ease and efficiency of video compression. Thus, regarding the static chroma-key technique, to convert from RGB to YUV color space, a fixed-point approximation is applied in most digital implementations.","Since the value range of the output YUV is normally scaled to [16, 235], it is possible to use the {0, 0, 0} value for key color. This key color selection satisfies requirements 1-3, above. However, it is not always the case for all codec implementations that the range of YUV is limited to [16, 235]. In such cases, an embodiment proposes a dynamic chroma-key technique.",{"@attributes":{"id":"p-0047","num":"0046"},"figref":["FIG. 6","FIG. 4"],"b":["600","602","610","604","606","608","610"],"sub":"1 "},"Still regarding , should static chroma-keying be used, at  a key color is chosen, and at  the background is replaced.",{"@attributes":{"id":"p-0049","num":"0048"},"figref":["FIG. 7","FIG. 7"],"b":["702","704","706","708","706"]},"At , should no empty box of the chosen dimensions be found, the key color {y, u, v} is chosen to minimize the expression:",{"@attributes":{"id":"p-0051","num":"0050"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"E","mo":"=","mrow":{"mo":["(",")"],"mrow":{"munder":{"mo":"\u2211","mrow":{"msub":{"mi":["y","i"]},"mo":"\u2208","mrow":{"msub":{"mi":["\u03b4","y"]},"mo":"\u2061","mrow":{"mo":["[","]"],"msub":{"mi":["y","o"]}}}}},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"msub":{"mi":["u","j"]},"mo":"\u2208","mrow":{"msub":{"mi":["\u03b4","u"]},"mo":"\u2061","mrow":{"mo":["[","]"],"msub":{"mi":["u","o"]}}}}},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"msub":{"mi":["v","k"]},"mo":"\u2208","mrow":{"msub":{"mi":["\u03b4","v"]},"mo":"\u2061","mrow":{"mo":["[","]"],"msub":{"mi":["v","o"]}}}}},"mo":"\u2062","mrow":{"mrow":[{"mi":"w","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"msub":[{"mi":["y","o"]},{"mi":["u","o"]},{"mi":["v","o"]}],"mo":[",",","]}}},{"mi":"H","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"msub":[{"mi":["y","i"]},{"mi":["u","j"]},{"mi":["v","k"]}],"mo":[",",","]}}}],"mo":"*"}}}}}}}}},"Where:","w is the weight of each bin depending on its distance from the center of the box;","\u03b4, \u03b4, \u03b4is the neighborhood area in y, u, v axis, respectively; and","H[y,u,v] is the bin value of color {y,u,v}.","If, at , E>0, which means that there is at least one pixel value in the image part\/foreground area that has its color inside the center box and its neighboring boxes, then that pixel color value is modified so that it no longer lies inside the box. This works to avoid ambiguity in dechroma-keying step.","Compared to the static chroma-key method, the dynamic chroma-key method requires more computation and bandwidth. Therefore, it is preferable to use the dynamic method only when the static method cannot be applied.",{"@attributes":{"id":"p-0058","num":"0057"},"figref":"FIG. 8","b":["800","802","804","802","806","804","808","806","808","802","804","806","802","808"]},{"@attributes":{"id":"p-0059","num":"0058"},"figref":["FIG. 9","FIG. 1","FIG. 4"],"b":["900","900","910","950","406","110","110","920","900","920","406","920","930","920","940","950","960","930","940","950","920","960"]},"Regarding quantization block , at  an alpha mask  from the user-extraction process may be used to drive the quality of a quantization block  so that macro blocks in the user-image or user-extracted region of a video frame , i.e., the more important sections, are quantized with more bits than the background. Alpha mask  allows the encoder to identify the location of the image part  () in the canvas . This information is added to the quantization block , allowing the encoder method  to avoid encoding blocks that do not contain elements of image part . This preserves the quality of the image part , or user region, during encoding. And by using fewer bits to quantize the background part (in the case of a video of a user-extracted image) or content (in the case of a composite video with content and a user-extracted image), it reduces the bandwidth required to transmit the encoded video stream. Skipping, (i.e., not encoding), the background part of a user-extracted video also saves additional processing time.","Efficiencies are gained in compression by addressing the different requirements of the content. When content is shared, the changes in content that accompany a change in video frame are typically small. In such case the Z-encoder may compress only those changes in the content following the method  described above with respect to video frame . In an additional embodiment, should it be determined that the background or content portion of video frame  is actually more important than the user-extracted image, then the alpha mask  from the user-extraction process may be used to drive the quality of a quantization block  so that macro blocks in the background or content region of a video frame  are quantized with more bits than the user-extracted image using the method described. And, in general, method  does not require that video frame  has gone through the chroma-keying process. Furthermore, in an embodiment, alpha mask  may be used to drive the quality of a quantization block  with the information from alpha mask  added through optional path  to prediction block .",{"@attributes":{"id":"p-0062","num":"0061"},"figref":["FIG. 10","FIG. 4","FIG. 4","FIG. 4"],"b":["416","414","1010","416","1020","416","1030","1032","1032","1034","1036","1038","1034","1036","420","1010","1038","420","1010","1040","1042","1044","1050","1052","1010","1044","1044","1020","1044","1044","1060"]},"After dechroma-keying , the frame of decoded video  and the generated alpha mask  are sent to the alpha blending block  () to make the image for display block . Alpha blending block  may combine decoded video  with any additional content  (), or additional user-extracted video  (). Alpha mask  contains an alpha value for each pixel of the decoded video frame that specifies how much the pixel color value contributes to the blended color value of the output display. Side information may be used to modify alpha mask  according to control input from the user at the sender side. The alpha value may then range from 0 to 1 (or 0% to 100%). The alpha blending formula is as follows (where C, C, and Cequal the color values of the blended pixel, video pixel, and content pixel, respectively):\n\n+(1\u2212\u03b1)*\n","The following contains sample Javascript HTML5 code for implementing aspects of the embodiments, such as: streaming live video, initializing video and canvas sizes, and binding post-processing actions to video during streaming.",{"@attributes":{"id":"p-0065","num":"0064"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"315pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"<script type = \u201ctext\/javascript\u201d>"},{"entry":"\/***"},{"entry":"\u2009*Dechroma a streaming HTML5 video with deChromakey filter and render the"},{"entry":"\u2009* userExtraction (UE - with transparent background) into a Canvas using JavaScript"},{"entry":"\u2009* This Canvas is placed with absolute position over the web content using CSS."},{"entry":"\u2009*\/"},{"entry":"document.addEventListener(\u2018DOMContentLoaded\u2019, function( )"},{"entry":"{"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"301pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\/* Stream the Live video (rtmp, hls, hds...) *\/"]},{"entry":[{},"var html5Video = document.getElementById(\u2018html5Video\u2019),"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"273pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"cWidth = html5Video.clientWidth,"]},{"entry":[{},"cHeight = html5Video.clientHeight,"]},{"entry":[{},"UECanvas = document.getElementById(\u2018UECanvas\u2019), \/*Output the UE after dechromakey *\/"]},{"entry":[{},"UE = UECanvas.getContext(\u20182d\u2019),"]},{"entry":[{},"deChromaCanvas = document.createElement(\u2018canvas\u2019),\/* Use to run the dechromakey filter *\/"]},{"entry":[{},"deChroma = deChromaCanvas.getContext(\u20182d\u2019),"]},{"entry":[{},"deChromaRunner = null,"]},{"entry":[{},"deChromaInterval = 20,"]},{"entry":[{},"keyColor = {r:0, g:0, b:0, r_range:16, g_range:16, b_range:16}; \/* key color *\/"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"301pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\/* Init video & canvas, copy width and height to canvases *\/"]},{"entry":[{},"UECanvas.width = cWidth;"]},{"entry":[{},"UECanvas.height = cHeight;"]},{"entry":[{},"deChromaCanvas.width = cWidth;"]},{"entry":[{},"deChromaCanvas.height = cHeight;"]},{"entry":[{},"\/* Binding post processing actions to html5Video during its streaming *\/"]},{"entry":[{},"html5Video.addEventListener(\u2018play\u2019, function( )"]},{"entry":[{},"{"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"287pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\u2003html5Video.style.visibility = \u2018hidden\u2019;"]},{"entry":[{},"\u2003clearInterval(deChromaRunner);"]},{"entry":[{},"\u2003deChromaRunner = setInterval(deChromakey,deChromaInterval,html5Video,UE,"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"259pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"deChroma,keyColor,cWidth,cHeight);"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"301pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"},false);"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"315pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"},false);"},{"entry":"function deChromakey (html5Video,UE,deChroma,keyColor,cWidth,cHeight)"},{"entry":"{"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"301pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"if(html5Video.paused || html5Video.ended) return false;"]},{"entry":[{},"\/* Step 1: copy current video frame to the deChromaCanvas *\/"]},{"entry":[{},"deChroma.drawImage(html5Video,0,0,cWidth,cHeight);"]},{"entry":[{},"\/* Step 2: get the pixel data from the deChromaCanvas *\/"]},{"entry":[{},"var imageData = deChroma.getImageData(0,0,cWidth,cHeight);"]},{"entry":[{},"var data = imageData.data;"]},{"entry":[{},"\/* Step 3: Loop through the pixels, make them transparent if they match keyColor *\/"]},{"entry":[{},"for(var i = 0; i < data.length; i+=4)"]},{"entry":[{},"{"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"287pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"var r = data[i];"]},{"entry":[{},"var g = data[i+1];"]},{"entry":[{},"var b = data[i+2] ;"]},{"entry":[{},"if (Math.abs(r-keyColor.r)<=keyColor. r_range &&"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"273pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\u2003Math.abs(g-keyColor.g)<=keyColor.g_range &&"]},{"entry":[{},"\u2003Math.abs(b-keyColor.b)<=keyColor.b_range)"]},{"entry":[{},"\u2003data[i+3] = 0;"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"301pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"}"]},{"entry":[{},"imageData.data = data;"]},{"entry":[{},"\/* Step 4: Now render the userExtraction onto UE canvas *\/"]},{"entry":[{},"UE.putImageData(imageData,0,0);"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"315pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"}"},{"entry":"<\/script>"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"Creating a persona by extracting a user image from a video will now be described regarding .  illustrates an example video . In general, the example video  comprises a background portion  and a persona . For example, the background portion  may comprise a wall, outdoor scene, or any other background scene and the persona  may comprise a human user or presenter. However, the persona  may comprise any identifiable object or entity. Thus, the example video  may be divided into at least two portions\u2014a background  and a persona . For example, if the video  comprises a user typing on a keyboard, then the user may comprise the persona  and a wall of the room behind may comprise the background portion .",{"@attributes":{"id":"p-0067","num":"0066"},"figref":"FIG. 12","b":["1200","1200","1120","1100","1110","1200","1100","1110","1120","1200","1120"]},{"@attributes":{"id":"p-0068","num":"0067"},"figref":"FIG. 13","b":["1300","1300","1120","1310","1120","1310","1310","1300","1300","1120","1310","1120","1310"]},{"@attributes":{"id":"p-0069","num":"0068"},"figref":["FIG. 14","FIG. 14"],"b":["1400","1400","1440","1400","1420","1450","1440","1420","1450","1440","1450","1410","1120","1420"]},{"@attributes":{"id":"p-0070","num":"0069"},"figref":"FIG. 15","b":["1500","1500","1510","1520","1530"]},"As seen in , a camera  is connected to a computer . The camera  may comprise a three dimensional (3D) camera, depth camera, z-camera and\/or range camera. In some embodiments, the camera  may be comprised of a color or RGB camera and a depth camera or may comprise of a single camera with an RGB sensor and depth sensor. As such, the camera  receives color information and depth information. The received color information may comprise information related to the color of each pixel of a video. In some embodiments, the color information is received from a Red-Green-Blue (RGB) sensor . As such, the RGB sensor  may capture the color pixel information in a scene of a captured video image. The camera  may further comprise an infrared sensor  and an infrared illuminator . In some embodiments, the infrared illuminator  may shine an infrared light through a lens of the camera  onto a scene. As the scene is illuminated by the infrared light, the infrared light will bounce or reflect back to the camera . The reflected infrared light is received by the infrared sensor . The reflected light received by the infrared sensor results in depth information of the scene of the camera . As such, objects within the scene or view of the camera  may be illuminated by infrared light from the infrared illuminator . The infrared light will reflect off of objects within the scene or view of the camera  and the reflected infrared light will be directed towards the camera . The infrared sensor  may receive the reflected infrared light and determine a depth or distance of the objects within the scene or view of the camera  based on the reflected infrared light.","In some embodiments, the camera  may further comprise a synchronization module  to temporally synchronize the information from the RGB sensor , infrared sensor , and infrared illuminator . The synchronization module  may be hardware and\/or software embedded into the camera . In some embodiments, the camera  may further comprise a 3D application programming interface (API) for providing an input-output (IO) structure and interface to communicate the color and depth information to a computer system . The computer system  may process the received color and depth information and comprise and perform the systems and methods disclosed herein. In some embodiments, the computer system  may display the foreground video embedded into the background feed onto a display screen .",{"@attributes":{"id":"p-0073","num":"0072"},"figref":"FIG. 16","b":["1600","1602","1602","1604","1604","1606","1606","1650"],"sub":["1 ","N","1 ","N","1 ","N"]},"Any node of the network  may comprise a general-purpose processor, a digital signal processor (DSP), an application specific integrated circuit (ASIC), a field programmable gate array (FPGA) or other programmable logic device, discrete gate or transistor logic, discrete hardware components, or any combination thereof capable to perform the functions described herein. A general-purpose processor may be a microprocessor, but in the alternative, the processor may be any conventional processor, controller, microcontroller, or state machine. A processor may also be implemented as a combination of computing devices (e.g. a combination of a DSP and a microprocessor, a plurality of microprocessors, one or more microprocessors in conjunction with a DSP core, or any other such configuration, etc.).","In some embodiments, a node may comprise a machine in the form of a virtual machine (VM), a virtual server, a virtual client, a virtual desktop, a virtual volume, a network router, a network switch, a network bridge, a personal digital assistant (PDA), a cellular telephone, a web appliance, or any machine capable of executing a sequence of instructions that specify actions to be taken by that machine. Any node of the network may communicate cooperatively with another node on the network. In some embodiments, any node of the network may communicate cooperatively with every other node of the network. Further, any node or group of nodes on the network may comprise one or more computer systems (e.g. a client computer system, a server computer system) and\/or may comprise one or more embedded computer systems, a massively parallel computer system, and\/or a cloud computer system.","The computer system  includes a processor  (e.g. a processor core, a microprocessor, a computing device, etc.), a main memory  and a static memory , which communicate with each other via a bus . The machine  may further include a display unit  that may comprise a touch-screen, or a liquid crystal display (LCD), or a light emitting diode (LED) display, or a cathode ray tube (CRT). As shown, the computer system  also includes a human input\/output (I\/O) device  (e.g. a keyboard, an alphanumeric keypad, etc.), a pointing device  (e.g. a mouse, a touch screen, etc.), a drive unit  (e.g. a disk drive unit, a CD\/DVD drive, a tangible computer readable removable media drive, an SSD storage device, etc.), a signal generation device  (e.g. a speaker, an audio output, etc.), and a network interface device  (e.g. an Ethernet interface, a wired network interface, a wireless network interface, a propagated signal interface, etc.).","The drive unit  includes a machine-readable medium  on which is stored a set of instructions (i.e. software, firmware, middleware, etc.)  embodying any one, or all, of the methodologies described above. The set of instructions  is also shown to reside, completely or at least partially, within the main memory  and\/or within the processor . The set of instructions  may further be transmitted or received via the network interface device  over the network bus .","It is to be understood that embodiments may be used as, or to support, a set of instructions executed upon some form of processing core (such as the CPU of a computer) or otherwise implemented or realized upon or within a machine- or computer-readable medium. A machine-readable medium includes any mechanism for storing information in a form readable by a machine (e.g. a computer). For example, a machine-readable medium includes read-only memory (ROM); random access memory (RAM); magnetic disk storage media; optical storage media; flash memory devices; electrical, optical or acoustical or any other type of media suitable for storing information.","Although the present embodiment has been described in terms of specific exemplary embodiments, it will be appreciated that various modifications and alterations might be made by those skilled in the art without departing from the spirit and scope of the invention. The previous description of the disclosed embodiments is provided to enable any person skilled in the art to make or use the present invention. Various modifications to these embodiments will be readily apparent to those skilled in the art, and the generic principles defined herein may be applied to other embodiments without departing from the spirit or scope of the invention. Thus, the present invention is not intended to be limited to the embodiments shown herein, but is to be accorded the widest scope consistent with the principles and novel features disclosed herein."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 16"}]},"DETDESC":[{},{}]}
