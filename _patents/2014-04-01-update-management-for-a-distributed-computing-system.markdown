---
title: Update management for a distributed computing system
abstract: In one embodiment, a method receives a software upgrade package for a management computer and main computer. The management computer upgrades software of the management computer using the software upgrade package where the upgrading replaces an image of the software of the management computer with an image from the software upgrade package. Upon upgrade of the management computer, the management computer initiates an upgrade of the main computer. The main computer withdraws use of the services, and upon the withdrawing, the management computer reboots the main computer. Then, the main computer upgrades software of the main computer using the software upgrade package upon rebooting where the upgrading replaces an image of the software of the main computer with an image from the software upgrade package. Upon the upgrading, the main computer restores the use of the services.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09148465&OS=09148465&RS=09148465
owner: ORACLE INTERNATIONAL CORPORATION
number: 09148465
owner_city: Redwood Shores
owner_country: US
publication_date: 20140401
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS REFERENCE TO RELATED APPLICATIONS","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["The present disclosure claims priority to U.S. Provisional App. No. 61\/807,308, entitled \u201cSystems and Methods for Distributed Computing\u201d, filed Apr. 1, 2013, the contents of which is incorporated herein by reference in its entirety.","Unless otherwise indicated herein, the approaches described in this section are not prior art to the claims in this application and are not admitted to be prior art by inclusion in this section.","Software installed on a computing device may need to be upgraded multiple times over the lifecycle of the software. The upgrade process should be performed in a way that minimizes downtime of the computing device. Typically, the software may be upgraded by adding a patch to the software already installed on the computing device. The patch may fix security vulnerabilities or other bugs in the software. Other software updates may include installing a new version of the software on the computing device. In this case, a user may have to manually install the software and completely bring down the computing device. In some environments, the downtime that is required to install the new version may not be acceptable.","In one embodiment, a method receives a software upgrade package for a management computer and main computer within the computing device. The management computer manages the main computer, and the main computer controls services for a distributed computing system including a set of nodes. The management computer upgrades software of the management computer using the software upgrade package where the upgrading replaces an image of the software of the management computer with an image from the software upgrade package. Upon upgrade of the management computer, the management computer initiates an upgrade of the main computer. The main computer withdraws use of the services, and upon the withdrawing, the management computer reboots the main computer. Then, the main computer upgrades software of the main computer using the software upgrade package upon rebooting where the upgrading replaces an image of the software of the main computer with an image from the software upgrade package. Upon the upgrading, the main computer restores the use of the services.","In one embodiment, a method includes: receiving, by a computing device, a software upgrade package for a first computer in a plurality of computers of a distributed computing system, wherein the first computer is a designated leader of the plurality of computers; electing, by the first computer, a second computer in the plurality of computers to perform an upgrade of software of the second computer using the software upgrade package, wherein the upgrading replaces an image of the software of the second computer with an image from the software upgrade package; and upon upgrade of the first computer, sending, by the first computer, an indication to become the designated leader to the second computer, wherein after becoming the designated leader, the second computer initiates upgrading of the first computer and other computers in the plurality of computers.","In one embodiment, An apparatus includes: one or more computer processors; and a non-transitory computer-readable storage medium comprising instructions, that when executed, control the one or more computer processors to be configured for: receiving a software upgrade package for a management computer and main computer, wherein the management computer manages the main computer, and the main computer controls services for a distributed computing system including a set of nodes; upgrading, by the management computer, software of the management computer using the software upgrade package, wherein the upgrading replaces an image of the software of the management computer with an image from the software upgrade package; upon upgrade of the management computer, initiating, by the management computer, an upgrade of the main computer; withdrawing, by the main computer, use of the services; upon the withdrawing, rebooting, by the management computer, the main computer; upgrading, by the main computer, software of the main computer using the software upgrade package upon rebooting, wherein the upgrading replaces an image of the software of the main computer with an image from the software upgrade package; and restoring, by the main computer, the use of the services upon the upgrading.","The following detailed description and accompanying drawings provide a better understanding of the nature and advantages of particular embodiments.","Described herein are techniques for a distributed computing system for performing upgrades. In the following description, for purposes of explanation, numerous examples and specific details are set forth in order to provide a thorough understanding of particular embodiments. Particular embodiments as defined by the claims may include some or all of the features in these examples alone or in combination with other features described below, and may further include modifications and equivalents of the features and concepts described herein.","System Overview","Features and advantages of numerous aspects and embodiments of the present disclosure are described with reference to particular example embodiments of a distributed computing system that may be used for cloud computing, referred to herein as a distributed computing system. The distributed computing system may be advantageously used in a cloud computing application, for example. In certain embodiments of the distributed computing system, an orchestration service may be responsible for creating and maintaining a cohesive and unified system that appears as a single system to a user, despite failures of both hardware and software, and for coordinating the execution and management of all system services and ensuring their availability. Features of an orchestration service may be advantageous in managing and running a distributed computing system, for example.","In one example embodiment, a distributed computing architecture is decentralized, and may include a zone, a controller node, a physical node, and a service container. Each controller node, physical node, and service container may run an instance of the orchestration service, which collectively implements the overall distributed computing system service. This loosely coupled collection of orchestration servers is organized in a manner that decentralizes the overall management of a zone, and may require little direct communication between servers, for example.","In one example embodiment, a distributed computing system is a turnkey Infrastructure-as-a-Service (IaaS) product that provides on-demand allocation of virtual machines (VMs), virtualized networking, and virtualized data storage, the key functionalities for a cloud computing environment in a private data center. In another example embodiment, the IaaS product provides on-demand allocation of physical computing resources without virtualization, including networking configuration and physical storage. In one example embodiment, a distributed computing system is a large distributed system, implemented as a hierarchical collection of physical nodes (e.g., servers) and controller nodes that communicate over a common network fabric and presents the appearance of a single large system with large quantities of compute power, storage capacity, and bandwidth.","In one example distributed computing hardware architecture, the server nodes, called physical nodes, are organized typically by racks into separate communication domains, each of which is controlled by a controller node, a specialized hardware, which is unique to a distributed computing system. All physical nodes and controller nodes may be connected by cable directly to their rack's controller node. In multi controller configurations, the controller nodes communicate over a common aggregation switch to weave all the controller nodes into a cloud fabric.","In the distributed computing software architecture, the distributed computing software is deployed as a set of system services in the hardware, running on the physical nodes and on the controller nodes. These services work together to implement the crucial functions expected of a cloud infrastructure, as well as to ensure that the infrastructure itself provides uninterrupted service in spite of failures anywhere in the system. The system services are structured into a logical hierarchy that separates responsibilities at different levels of granularity in the system and maps into underlying hardware organization.","Example Hardware Architecture",{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 1","b":["100","100","107","100","101"]},"In each configuration, controller node  may be connected to one or more physical nodes  by a connection, such as a combined data and out of band management cable, hereinafter referred to as the cloud cable, or, if a cloud cable is not used, other compatible primary network cables  in conjunction with a separate out of band management network cable . The compatible primary network cables  and out of band management network cables  can include various types of conventional communication wires, such as CAT5e twisted pairs, CAT6 twisted pairs, and coaxial cable, for communication over Ethernet or other similar networking protocols. The network cables can also include fiber-optic bundles for communication over various optical network communication protocols. In one example embodiment, multi controller node configurations  of more than two controller nodes where over half of the controller nodes are available provide high availability of the distributed computing orchestration services and related cloud computing services. Each controller node in multi controller node configurations is connected to one or more physical nodes  by means of cloud cable or other compatible network cable .","Controller nodes  may communicate with each other via a connection. For example, each controller node  in a multi controller node configuration  may be attached to a separate out of band management switch . In such multi controller node configurations , controller nodes  are connected to one or more aggregation switches . Aggregation switches  interconnect controller nodes  in multi controller configurations , permitting communication between the controller nodes .","Controller Node Configuration",{"@attributes":{"id":"p-0043","num":"0042"},"figref":["FIG. 2A","FIG. 2A"],"b":["107","125","107","106","106","107","107","107","107","106","108"]},"Controller node  is an advantageous component of the distributed computing system to control orchestration functions and cloud services, including the provisioning and configuration of physical nodes . For example, when physical nodes  are attached to a controller node , controller node  exercises control over the physical node's basic power state and, in some embodiments, the physical node's boot order. Physical nodes  are configured to either seek boot images over their network interfaces or are configured to do so by the controller node. The physical node  then obtains its boot image from the controller node  which contains start up instructions that establish communication with the controller node such that the physical node is configured and included in the distributed computing resource pool. From there, controller node  may issue workloads to physical node  and physical node  will process the workloads, providing cloud services. In some embodiments, controller node  is a rack-mounted device of chassis dimensions substantially similar to typical rack-mounted server computers, including those attached to controller nodes as physical nodes . Rack-mounted embodiments of the controller node  include 4U, 2U, and 1U physical dimensions where a U is a rack unit of standard dimension, typically 1.75\u2033 high, 19\u2033 wide, and variable depth.","Referring to , one example controller node  may be comprised of an main network switch ; a main computer  (e.g., including its own central processing unit, storage, and memory (not shown)); an internal network switch ; one or more microcontrollers (e.g., master microcontroller  described in more detail below), one or more internal communication and management networks; fault tolerant power supply  and distribution ; management computer ; environmental subsystem ; one or more universal serial bus hubs; and physical administration interface  (e.g., an LCD touchscreen). Although main network switch  is shown as being included in controller node , main network switch  may be external to controller node . In this case, controller node  would communicate with main network switch  through an interface.","In one example, main network switch  is the interface by which the controller node  communicates with, provisions, and\/or manages attached physical nodes , communicates with one or more aggregation switches , communicates with one or more out of band management switches  if a cloud cable is not used, communicates with one or more other controller nodes  (e.g., through aggregate switches), as well as the interface by which the attached physical nodes  communicate with one another. The resultant network is one example of what may be referred to as a cloud fabric. In one example, the interfaces on the main network switch  comprise one or more primary network interfaces , one or more management network interfaces , one or more serial management interfaces, and one or more universal serial bus interfaces .","Primary network interfaces  on the main network switch  form the network pathways between the controller node  and physical nodes  carrying the majority of traffic between the devices, including orchestration, cloud service, and client traffic. Example implementations of the primary network interfaces  may include RJ-45, small form-factor pluggable, quad small form-factor pluggable, or other network interface. Controller node  attaches to physical nodes  by means of one or more cloud cable or one or more compatible network cable  through the main network switch . When more than one cloud cable or compatible network cable is utilized to attach a physical node  to controller node , such connections may be combined or bonded for either redundancy or increased throughput where the effective base network throughput between controller node  and physical node  is multiplied by the number of such additional connections. This method of channel bonding permits high throughput configurations. In some embodiments, the primary network interfaces  on the controller node's main network switch  are configured to utilize an inter-integrated circuit communication protocol management (\u201cI2C\u201d) bus present in the cloud cable. This configuration permits primary network traffic, inter-integrated circuit communication protocol management traffic, and inter-integrated circuit communication protocol system traffic to transit through any primary network interface  on the main network switch  to the attached physical nodes . Inter-integrated circuit communication protocol management traffic comprises distributed computing-specific traffic to the physical node, including control messages, management sessions, and other configuration and management data. Inter-integrated circuit communication protocol system traffic comprises messages normally issued in the course of initialization and operation of a network switch when attached to network cables capable of responding to data inquires, including manufacturer data, cable length, and connection status. When a cloud cable is used and attached to a cloud card in physical node , two effective network connections are established over a single physical link. In other embodiments, a separate out of band management network is created by attaching the main network switch  to a physically separate out of band management switch . Out of band management networks are used to communicate basic instructions such as turn on, turn off, change configuration, change boot order, and load operating system, for example, from a controller node  to an internal processor in each physical node  (e.g., a baseboard management controller chip operating according to the intelligent platform management interface protocol). In such embodiments, physical nodes  attached to controller node  by primary compatible network cable may also be connected to the separate out of band management switch, forming a secondary data network between controller node  and attached physical nodes . The out of band management switch  attaches to out of band management ports on the physical nodes , permitting controller node  to issue configuration and control messages to physical nodes  by means of an intelligent platform management interface. This out of band management data network is advantageous in communicating with, configuring, and provisioning physical nodes  when such physical node's primary network interface is not configured or not functional, such as when there is no operating system on physical node  or any operating system on physical node  is misconfigured, damaged, or otherwise in a degraded state which impacts the operation of the primary network interface.","The management network interfaces  on the main network switch  are coupled to management computer  through the controller node's internal network switch . In one example, management computer  uses interfaces  to establish administrative access to main network switch  and configure main network switch  it for use in the distributed computing system, including, virtual network configuration, routing configuration, network interface configuration, and other processes and configurations advantageous to rendering cloud computing services. Some main network switches  expose the management network interfaces  in-line with, or offset from but facing in the same direction as, the primary network interfaces  making them physical accessible from outside the controller node chassis. In some embodiments, such physical in-line management network interfaces  are disabled, and the corresponding logical interfaces on main network switch  are redirected to inward facing interfaces. In other embodiments, such physical in-line management network interfaces  are additional and subordinate to internal secondary management interfaces.","Management network interfaces  may take the form of one or more dedicated network interfaces or an Ethernet-to-universal serial bus adapter connected directly to an available universal serial bus interface, or universal serial bus hub connected to a universal serial bus interface, on a motherboard of the main network switch , exposing an additional physical and logical interface to the operating system on main network switch . The use of a universal serial bus hub permits multiple universal serial bus devices to be connected to main network switch  by means of one universal serial bus port on the main network switch's motherboard. When used, an Ethernet-to-universal serial bus adapter exposes an additional physical and logical interface to the operating system on main network switch .","Main network switch  is configured using standard device manager functions of the main network switch operating system to remap the logical secondary management interface to the logical interface exposed by the physical Ethernet-to-universal serial bus adapter interface. Internal network switch , management network interfaces  on the main network switch , and connections between the two devices are internal to the controller node, controlled by management computer , with no logical or physical user facing interfaces other than through the management computer when configured as a support gateway.","The serial management interfaces  on main network switch  are attached to serial interfaces on the controller node's management computer . These interfaces provide an additional pathway for management computer , or a technician leveraging access through management computer , to interface with the main network switch  in the event that the network management interfaces become unavailable or unreliable, such as in the case of misconfiguration. This pathway guards against software errors by permitting another channel for correcting errors which disable communication over the man network switch's internal network management interfaces. Some main network switches expose serial management interfaces in-line with, or offset from but facing in the same direction as, the primary network interfaces, making them physically accessible from outside the controller node chassis. In some embodiments, such physical in-line serial management interfaces are disabled, and the corresponding logical interfaces on the externally facing switch are redirected to inward facing interfaces. In other embodiments, such physical in-line serial management interfaces are additional and subordinate to internal serial management interfaces . Internal serial management interfaces  may take the form of one or more dedicated serial interfaces or a serial-to-universal serial bus adapter connected directly to an available universal serial bus interface or universal serial bus hub connected to a universal serial bus interface on main network switch  motherboard, exposing an additional physical and logical interface to the operating system on the main network switch . When a serial-to-universal serial bus adapter is used, the main network switch is configured using standard device manager functions on the main network switch operating system to remap the logical serial management interface to the logical interface exposed by the physical serial-to-universal serial bus adapter interface.","The universal serial bus interfaces  on main network switch's  may be both inward facing such that they may be attached to other controller nodes  or interfaces by wire or other physical pathway, or they may be externally facing interfaces in-line with, or offset from but facing in the same direction as, the primary network interfaces  making them accessible from outside the controller's node physical chassis. In some embodiments, such physical externally facing and externally accessible universal serial bus interfaces  are disabled, leaving only the internally facing interfaces operational and available for interconnection with other controller node interfaces. In other embodiments, such physical in-line universal serial buses interfaces  are additional to internal universal serial bus interfaces . The universal serial bus interfaces on main network switch  may advantageously be used to provide for additional communication pathways between main network switch  and other controller node components, such as management computer , beyond those interfaces physical present on the main network switch .","In one example embodiment, the controller node's main computer  includes a central processing unit, memory, and storage , for example, configured to operate the distributed computing software architecture, including the base operating system, orchestration service, and system service containers. Main computer  is the base platform from which distributed computing services are rendered. Typically, distributed computing services, including cloud computing services such as the main workload scheduler, identity service, storage service, disk image service, and user interface services; reside on and are independent servers. Many of these services are dependent on one another to perform their functions. This distributed computing system requires that communication between the services conducted through network interfaces. In order to approximate the expected barrier between cloud services, main computer  isolates services into partitions which each possess full virtual network interfaces and are independently addressable. The distributed computing orchestration service creates these network enabled and addressable partitions, populates them with the requisite software to enable the desired service, and configures the partition, the partition's network interface, and the service software within the partition to provide the desired service function. By using this partitioning scheme, main computer  is able to render cloud computer services requiring network communication with other services from within a single physical server.","The controller node's main computer  is coupled to other components of controller node  by one or more primary network interfaces, one or more secondary management network interfaces, one or more serial interfaces, one or more storage interfaces, one or more inter-integrated circuit communication protocol pathways, and by front panel header connections such as power switch, reset switch, and activity indicator lamp. These interfaces provide multiple independent pathways for other components in controller node  to connect with the main computer. As an integrated appliance, the availability of redundant interfaces is advantageous to guard against the failure or misconfiguration of any one interface, which would otherwise render the overall controller node unusable. These pathways provide both programmatic and technician access to the main computer to assist in configuration, reconfiguration, troubleshooting, diagnostics, and recovery from fault conditions including misconfiguration, primary operating system failure, or other interface failure. The main computer's primary network interfaces are attached to the controller node's main network switch  by one or more compatible network cables and carry cloud service traffic to and from the physical nodes. When multiple network cables are used, the channels may be bonded for redundancy or to multiply base effective throughput by the number of such additional connections. The main computer's management network interfaces are attached to the controller node's internal network switch by means of wire or other physical pathway and carry management traffic to and from the management computer. The main computer's serial interfaces are attached to main computer , permitting main computer  to obtain console access to main computer  as another means of controlling the main computer. The main computer's storage interfaces attach to storage devices within management computer . The intelligent management platform bus header on main computer  is attached to the master microcontroller by means of inter-integrated circuit communication protocol pathway so that the master microcontroller, or management computer through the master microcontroller, may control the state and configuration of main computer . The master microcontroller also attaches to the main computer's front panel header and thereby has a second means of controlling the main computer's state, as well as monitoring its activity.","The controller node's internal network switch  connects several of the controller node's internal systems and routes Ethernet based management traffic among them. Among the systems in this internal network are the main computer , main network switch , primary microcontroller , and the management computer . These interconnections are by means of wire, PCB trace, or other physical pathway, for example.","Controller node  hosts a number of microcontrollers and nonvolatile memories. Printed circuit boards in controller node  that host microcontrollers or other active logic circuitry, as opposed to simple circuit pathway or structural boards, contain nonvolatile memories for a variety of purposes. In some embodiments, nonvolatile memory is in the form of Electrically Erasable Programmable Read-Only Memory. Active printed circuit boards contain at least one nonvolatile memory for the storage of version, manufacture data such as date and location, and related metadata regarding the host printed circuit board. Each such metadata nonvolatile memory is electrically coupled with the primary microcontroller by means of inter-integrated circuit communication protocol pathways. Additional nonvolatile memories are present in some active printed circuit boards in order to store configuration or state data needed for the logic functions of other circuits on a given printed circuit board. One such nonvolatile memory stores the configuration data for the controller node's internal network switch. Another such nonvolatile memory stores font cache data used in the visual rendering of the controller node's physical administration interface.","The controller node microcontrollers comprise a master microcontroller , environmental microcontroller , and fascia microcontroller . The master microcontroller is responsible for general hardware regulation within the controller node, controlling power state and monitoring hardware health status. The master microcontroller  is attached by inter-integrated circuit communication protocol pathways to all metadata nonvolatile memories in the controller node, thermal probes in some printed circuit boards, the power distribution unit  by means of PMBus protocol, other microcontrollers, the physical administration interface , the intelligent platform management bus header on the main computer , by network interface to the internal network switch , and by universal serial bus to the management computer . The master microcontroller  is powered when electricity is supplied to controller node , even during a thermal or other fault related power interrupt condition, and provides overall orchestration and logic for the operation of base hardware components throughout controller node . In those embodiments where master microcontroller  has access to metadata nonvolatile memories, environmental microcontroller  and its fan speed data, the power distribution unit  and its PMBus data, and low level management control of main computer  by means of intelligent platform management interface, master microcontroller  is capable of performing health checks against major controller node subsystems. Health checks, which can take the form of thermal monitoring; power consumption monitoring, basic test functions, and electrical presence; are important in the operation of the controller node due to the multitude of internal, typically independent system components. Centrally gathering such health data and presenting the same through the controller node's physical administration interface  aids in system diagnostics and troubleshooting.","Master microcontroller  powers the controller node's physical administration interface . In some embodiments, this interface takes the form of a touchscreen liquid crystal display (\u201cLCD\u201d). Touch input from such a display is captured and relayed to master microcontroller  as user input, permitting the user to select among various options and issue commands to the master controller. Such commands include toggling the power state of controller node , configuring physical nodes , performing configuration or other audits, and entering support mode. Physical administration interface  is also used to display a range of information about controller node  and attached physical nodes , including the controller node's operational status, state, performance, configuration, and overall system capacity.","Master microcontroller  participates in environmental regulation by monitoring some thermal sensors in controller node . In the event master microcontroller  detects temperatures that exceed the controller node's maximum safe operating temperature, master microcontroller  may issue a power interrupt request to the power distribution unit  and shut controller node  down. Master microcontroller  also accepts power interrupt requests from management computer , and can issue fan duty cycle override commands to the environmental microcontroller.","Master microcontroller  bridges base hardware components in the controller with distributed computing orchestration software by means of interaction with management computer . An application programming interface (API), such as a RESTful HTTP API endpoint, on management computer  accessible by network connection provides the interface by which other software components in controller node  may issue requests to base hardware. Such API calls are received by management computer , processed, converted to into a corresponding universal serial bus human interface device class function, conveyed to master microcontroller  by means of the universal serial bus interface, processed, and converted into a specified command addressed to a hardware component.","Environmental microcontroller  is responsible for regulating environmental conditions within controller node . This task may be made complicated by the presence of multiple independent components within controller node , some of which may typically have independent thermal management systems and which may not function correctly without first verifying the presence of specific thermal management systems. The environmental microcontroller accommodates these components by maintaining overall thermal conditions and emulating the presence of expected thermal management systems for each component requiring such systems in the manner expected. For example, some components will verify the number of expected cooling fans before operating. The environmental microcontroller emulates the presence of the expected number of cooling fans, thus enabling operation of the affected component. Among the environmental microcontroller's functions are processing thermal data and control messages, including monitoring various thermal probes, monitoring fan performance, adjusting fan duty cycle in response to prevailing environmental conditions, responding to thermal sensor inquires and duty cycle adjustment requests from controller node sub-components, and issuing power interrupts as necessary to prevent thermal related damage from occurring. A fan duty cycle is the percentage of time the fan is active in a given timespan. The environmental microcontroller  is attached to and responsible for the operation of controller node chassis fans. The environmental microcontroller  collects thermal sensor data from thermal probes on printed circuit boards distributed throughout the controller and calculates the appropriate fan duty cycle for overall controller node cooling requirements based on this data. The cooling curve is defined according to the operating requirements of all components within controller node  such that the controller node's internal temperature approximates as nearly as possible the median optimal operating temperature of all controller node components while never exceeding the maximum thermal rating of any individual component. The environmental microcontroller  also monitors chassis fan performance. If fan performance degrades, or if fans fail, the environmental microcontroller  can trigger a fault alarm or interrupt power to the chassis, as necessary, to prevent thermal damage to controller node . In some embodiments, a dedicated interrupt circuit between the master microcontroller  and the environmental microcontroller  serves to effect power interruption. In such embodiments, if either microcontroller determines that a system fault or environmental condition necessitates a power interruption, the master microcontroller  will issue an interrupt request to the power distribution subsystem .","Controller node components hosting independent environmental regulation systems, such as fan speed sensors and logic for adjusting fan duty cycle in response to sensor data, are attached to the environmental microcontroller . Environmental microcontroller  intercepts and responds to both temperature data requests and duty cycle control signals from such components, including main network switch  and main computer . Reply messages to requesting components emulate expected responses, thereby maintaining the operational norm of the requesting components. In some embodiments, duty cycle control signals and thermal data from components with independent environmental regulation systems are weighted and factored when the environmental microcontroller  calculates the appropriate duty cycle for controller node chassis fans. In other embodiments, only the prevailing environmental condition as determined by a plurality of available thermal sensors is used in calculating the appropriate fan duty cycle suitable for overall controller node  operation.","Fascia microcontroller  is attached to management computer  by means of serial interface connection and powers the controller node's fascia . Fascia microcontroller  controls the face panel of the controller chassis, which may be a touch screen interface, for example. In some embodiments, light emitting diodes on the controller node's front panel (fascia) can convey system state information, including initializing, on, fault condition, new node added, node removed, node fault condition, and off. Management computer  issues state information is to the fascia microcontroller , which sequences and controls the light emitting diode array in the controller node's fascia to indicate a corresponding state. For example, a fault condition in controller node  may be communicated to the fascia microcontroller through the management computer HTTP API. A call to the API corresponding with error state and severity will be relayed to the fascia microcontroller  through the master microcontroller . In response, fascia microcontroller  may adjust the color, light output, and pattern of light emitting diodes in the fascia to relate the failure state. One such representation may take the form of flashing red across the face of the failed controller node. Another example may include an API call placed to management computer  indicating that the main computer orchestration service is initializing. Such API call will be relayed to fascia microcontroller  through the master microcontroller . Fascia microcontroller  may then adjust the fascia LED array to pulsating blue. Incremental initialization states between uninitialized and fully initialized, such as building containers, initializing main network switch , and establishing communication with physical nodes, may be represented by different colors with similar flashing pattern. The speed of the flashing may be used to indicate progress during each step, such as increasing speed until solid to indicate success, or fixed change to flashing pattern to indicate processing or failure. Each of such combinations may be represented by single API calls with represent multi-step complex logic, or the grouping and sequential request of several individual API calls, which represent primitive hardware functions, such as on, off, flash, and adjust color. API definitions supporting the above examples may be for entering pulsating mode, set pulsation frequency, and set LED color, for example.","Power for controller node  may be provided by redundant, fault tolerant power supplies  attached to a power distribution unit  that communicates state data with the controller node using a protocol, such as the PMBus protocol. The power supplies and power distribution system in controller node  are able to accommodate the electrical requirements of each of the controller node's varied components. Voltages in the controller node comply with a specification, such as the Advanced Technology eXtended (ATX), power specification and are available in 12v, 5v, 3.3v, and other voltages. The PMBus protocol is used to interrupt power to controller node  in the event of a thermal condition or other environmental condition outside of specified normal operating ranges to prevent physical damage to any of the controller node's components. In some embodiments, power is distributed throughout controller node  by means of PCB using blind mate interfaces. Traces are of sufficient width and copper content to accommodate expected voltage and amperage over given distances. For example, higher current traces, longer traces, or both, are wider and contain more copper content to prevent the trace from heating to the trace copper's melting point. In other embodiments, one or more insulated aluminum bus bars are used to carry high current power. Such bus bars are used in lieu of traditional PCB traces to prevent over heating or other power quality and safety issues. Each such bus bar conducts only one voltage. In various embodiments, standard power interfaces are exposed to connect with controller node subsystems that require specific power interfaces. For example, main computer  may require power interfaces in the form of two standard ATX 8 pin power connectors and one standard ATX 24 pin power connector.","Management Computer","Management computer  may be independent of the main computer  and is responsible for management of controller node . Management computer  and main computer  may be separate computing chips or processors such that management computer  can manage main computer . In other examples, management computer  and main computer may be the same processor or chip. Management computer  is the starting point and stable basis from which other controller node operations are provisioned, configured, and maintained. Management computer  may include a central processing unit with hardware public key cryptographic features, true random number generator, memory, storage, one or more network interfaces, one or more serial interfaces, and one or more universal serial bus interfaces. These interfaces provide multiple independent pathways between the management computer, the main computer, and the main switch. The availability of multiple communication pathways between management computer  and other controller node components ensures that the failure of any one interface does not obstruct all communication pathways with other controller node components.","At least one network interface on management computer  is attached to the controller node's internal network switch , thereby permitting communication with main network switch , main computer , microcontrollers, and other systems present on the internal network. At least one other network interface on management computer  is attached to a network interface accessible from outside the controller node chassis , permitting physical access from outside of the controller node's chassis. This interface is advantageous as it permits a technician to directly connect with management computer  and utilize its multiple, redundant pathways to the controller node's other internal systems, such as main computer  and main network switch . The management interfaces on main computer  and main network switch  may be otherwise inaccessible from outside of the controller node's chassis, and any maintenance or diagnostic tasks on these components would require opening the chassis and disassembling controller node . The externally accessible network interface coupled with the embedded management controller therefore provides an administrative and maintenance pathway to all controller node components without requiring disassembly of controller node . In some embodiments, such externally accessible network interface  is disabled when controller node  is operating normally, and may be selectively enabled through the controller node's physical administration interface , remotely, in response to fault conditions, or by other restricted means to provide authorized diagnostic and support functions.","At least one serial interface on management computer  is attached to a serial interface of the main network switch . This interconnection provides for management access to the main network switch  in addition to and independent of other management network interconnections with the main network switch . At least one other serial interface on management computer  is attached to a serial interface of the main computer . This interconnection provides for management access to the main computer  in addition to and independent of other management network interconnections with main computer . The management computer's universal serial bus may be used individually, or in conjunction with a universal serial bus hub, to expose additional required interfaces by means of adapters such as an Ethernet-to-universal serial bus adapter or serial-to-universal serial bus adapter. Management computer  interfaces with the master microcontroller  by means of universal serial bus interface.","Management computer  performs several functions within controller node , including initial provisioning of main computer  from signed disk images, upgrades of main computer  from signed upgrade disk images, an interface between the distributed computing orchestration system and lower level microcontrollers within controller node , initial provisioning and configuration of the main network switch , upgrades of the main network switch's  operating system, out of band management access to the main network switch , out of band management access to main computer , and an externally accessible diagnostic and support interface .","The management computer controls the basic states of main computer , such as on, off, and reset. It also controls the boot order of main computer , either through direct access to the main computer's bios, or by selectively disabling and enabling the main computer's primary boot disk, thereby controlling which boot device is available to main computer . If the main computer's primary boot device is not available to it during the boot process, it will attempt to boot from the next device in its boot order. Exercising this control, management computer  can force main computer  to search for a boot image through the main computer's network interfaces, to which management computer  is attached. Management computer  is then able to provide a boot image to main computer  by means of network interface. This process is used in main computer  initial provisioning as well as in upgrades of the main computer's software.","Management computer  contains a cryptographically signed factory disk image of the initial operating state of main computer . In some embodiments, main computer's  disk images are also encrypted. These cryptographic measures ensure the integrity of the main computer's disk image. Any modifications to the disk image, such as by user intervention, may change the image's signature. By verifying that the image is signed by distributed computing, management computer  prevents the execution of unauthorized software on controller node . In embodiments where the disk image is encrypted, the form and structure of the disk image is concealed so as to prevent potential attackers from inspect the controller node's system software.","Upon first boot, main computer  is configured to obtain its operating system image from its network interfaces using a protocol, such as the preboot execution environment (PXE) protocol. Management computer  verifies the cryptographic signature of main computer's initial disk image against cryptographic keys ephemerally or irreversibly written to management computer . Management computer  may store cryptographic keys as normal data on its storage disk, or it may write the cryptographic keys using a one-time write process where fuses or other circuits are permanently modified to prevent modification of the cryptographic keys. If verified, the disk image is made available to main computer  from management computer  by means of a protocol, such as trivial file transfer protocol (TFTP), for example, or other PXE compatible data distribution protocol, over the controller node's internal network. In one example embodiment, an intermediate network bootloader capable of HTTP and other communication protocols in delivered to main computer  from management computer  by means of TFTP. A server node may obtain the intermediate bootloader from main computer . The intermediate bootloader is a small application that is executed by a device asking for a network boot (e.g., main computer, server node). Once loaded, the intermediate bootloader causes main computer  to download the main boot image using HTTP or other communication protocols which improve reliability and efficiency of the download function. Main computer  downloads the disk image, writes it to a boot partition on persistent storage, and proceeds to boot from this disk image. Main computer  obtains its disk image from a management computer . A server node may obtain its disk image from main computer  in controller node , for example. The intermediate boot loader construct with reliable and scalable distribution protocol is advantageous when distributing boot images to multiple physical nodes  concurrently, such as when new physical nodes  are added and initialized.","Management computer  also provides an upgrade disk image to the main computer . This process will be described in more detail below. During an upgrade, main computer  downloads from management computer  the latest disk image (the upgrade) and saves it to storage  on management computer , marking the upgrade as the current version of the disk image and marking the previous disk image (the version operating before the upgrade) as the previous version. To aid in system restoration, management computer  retains original main computer  factory disk image as a baseline. Management computer  verifies the cryptographic signature of the main computer's upgrade disk image against cryptographic keys irreversibly written to management computer . In embodiments where the upgrade disk image is also encrypted, management computer  decrypts the disk image before transmitting it to main computer . In a multi-controller configuration, a subordinate controller is upgraded first. If successfully upgraded and joined back into the distributed computing rack, the lead controller node in the rack assigns the leader role to the upgraded controller node, which then iterates over the remaining controllers, upgrading each in turn according to the order in which the other controller nodes were added to the rack.","Each individual controller node , and the only controller node  in single controller node configuration, is upgraded by writing important configuration and state data to persistent storage  in partitions other than the boot partitions. When main computer  and the overall distributed computing system have written all essential data and are prepared for the temporary absence of controller node , controller node  restarts to obtain the upgrade disk image from management computer  over the controller node's internal network using the PXE protocol. During the main computer's absence, physical nodes  and any system services or virtual machines on the physical nodes  should remain operational and accessible as the controller node's main network switch  and physical node network components remain functional. Following the successful upgrade of the controller node's main computer , controller node  may issue software upgrade commands to attached physical nodes , potentially resulting in service disruptions. Such physical node software upgrades are performed on one physical node first, and if found successful, the upgrade commands iterate to the remainder of the physical nodes attached to the controller node.","Upgrades to the management computer  are achieved by partitions on the management computer's primary storage device (not shown). New management computer software is written to an inactive partition. When written, the management computer  restarts and boots from the partition containing the new software. In the event of a fault related to booting from the new software, management computer  restarts and boots from the previous software partition.","Using Management Computer as API to Bridge Software Functions with Hardware Functions","Management computer  serves as a bridge between the main computer  and lower level controller node functions, including the physical administration interface, fascia LED arrays, and I2C communications through the master microcontroller . In some embodiments, a high level API , such as a RESTful HTTP API, is made available to the controller node's main computer . The API is an endpoint for remote procedure calls. The calls to this API  are translated to specific hardware functions, including on, off, read temperature, read speed, set speed, read luminance, set luminance, read color, set color, which are issued to the appropriate microcontroller by established communication pathways and protocols, including, for example, a universal serial bus using the protocol's human interface device class. A universal serial bus interface between management computer  and master microcontroller  may be used for reliability. The human interface device class typically used with computer input peripherals is used for its extensibility and suitability for translating API calls into instructions that may be processed by master microcontroller .","Management computer  is ideal for API  because management computer  has communication pathways to multiple hardware elements  in addition to the other components of controller node . Management computer  thus can be a broker to translate communications from different hardware elements  that may communicate in different low level hardware primitives to higher level software calls. This makes hardware elements  appear as software to software elements  as software elements  can use software commands, such as remote procedure calls, directed to hardware elements .","The HTTP API on management computer  is advantageous in the overall operation of controller node . For example, the various components in controller node  are each independently powered such that they may not directly affect the power state of other components. Therefore, when main computer  receives a user signal to power off controller node , software on main computer , including the orchestration service, may issue an API call to management computer  to initiate the power off process across all controller node components. Alternatively, a power off event triggered from the physical administration interface may be communicated to the rest of the system by relaying the appropriate instruction from the physical administration interface through management computer  to the relevant API endpoints within the system. System service state data may also be made available to the physical administration interface and front fascia through API call. Alert and fault API calls to management computer  may be related to the master microcontroller  to the physical administration interface and fascia such that specific details may be displayed on the physical administration interface and the fascia may adjust the color or pattern of its LED array to visually indicate the existence of an alert or fault.",{"@attributes":{"id":"p-0080","num":"0079"},"figref":"FIG. 2B","b":["126","130","138","130","138","140","140","126"]},"In one embodiment, the orchestration service instances or system services may need to communicate with hardware elements , such as environmental microcontroller , power distribution , power supply , LCD touch screen , and\/or fascia microcontroller . For example, hardware elements  may contribute to the health, state, and configuration of both hardware elements  and software elements . The health means, for example, the availability, quality, and draw characteristics of electrical power, component and ambient temperature, and the availability and duty cycle of system fans. The state refers, for example, to the power state of controller node , either being on or off, and presentation of some useful subset of controller node 's operational information. For example, the fascia may output a pattern of LED lights based on the operational state. The configuration refers to accepting configuration data for initial set up of the distributed computing system.","To communicate with hardware elements , software commands from software elements  may need to be translated into hardware commands that hardware elements  understand. To provide separation between the software commands and hardware commands, management computer  may be leveraged to provide the translation. This allows software elements  to be developed to solely communicate using software commands. In this case, a developer does not need to know how to interact with hardware elements  via hardware commands. Rather, software elements  may make software calls to an API  in management computer . In one embodiment, management computer  provides a RESTful API endpoint  that can be accessed by software elements . For example, the orchestration service instances may query and write to API  to communicate with hardware elements .","Having the API in management computer  provides many advantages. Management computer  operates a full operating system capable of hosting an HTTP API endpoint using software of the same type that operates elsewhere in controller node . This allows management computer  to communicate with other elements in controller node  such as main computer . Also, management computer  holds critical communications pathways in a variety of interfaces and through a number of protocols to hardware components in controller node . These pathways may be dedicated pathways. This allows management computer  to interface with the software on main computer  using the same type of remote procedure calls used by main computer  for inter process communication, effectively abstracting the business of hardware control into primitives easily manipulated by the software of main computer .","When API  receives the software call from software elements , a translator  translates the software calls into lower level hardware commands. Translator  may include logic that translates software commands into hardware commands that hardware elements  can understand. For example, management computer  may have an API for a \/poweroff uniform resource identifier (URI) that, when a software element  on main computer , typically the orchestration service, makes a call to the \/poweroff URI of the API, management computer  receives the command, interprets it, and issues a corresponding hardware command to perform the function over any of a number of communication pathways and protocols to the hardware. In this example, the call to \/poweroff URI may be an HTTP call and may include the identifier for a specific hardware component. Management computer  parses this information, determines if the identifier corresponds to a hardware element  it has communication with, determines the corresponding command and pathway for the specific hardware element  in question from a predefined and configured array of choices, and issues the identified command. The translation is thus moved out of main computer  and also software elements  do not need to know how to perform the translation. Software elements  do not need to communicate with an API on main computer  to hardware elements  via hardware commands. Rather, the API on management computer  is accessed via software commands.","Once the translation is performed, translator  sends the hardware commands to hardware elements . In one embodiment, management computer  sends the hardware commands via internal network switch  through a universal serial bus interface. Then, the hardware commands are forwarded from internal switch  via the USB to master microcontroller . Master microcontroller  may then communicate the hardware command to hardware elements .","The communication may also be bi-directional. In this case, hardware elements  may communicate hardware commands to management computer . The hardware commands may include various information from hardware elements that may be used by software elements , such as power information. The hardware commands are communicated to management computer  via master microcontroller , or an internal network switch , or other available communication pathway and protocol, such as via the USB.","Translator  receives the hardware commands and may translate the hardware commands into software commands. Then, translator  sends the software commands through API  to main computer . Software elements  may then use the software commands to perform actions. In this case, software elements  do not need to translate hardware commands into software commands, which simplify the operation of main computer  and software elements .","In one example, activation of remote user support may be provided. A user may navigate to a menu on a touch screen interface or other hardware human interface device of controller node  to enable remote support. When selected, the hardware human interface device (e.g., touchscreen ) communicates with management computer  to register the event. The communication may be using hardware commands. Management computer  may then determine software elements  that need to be notified of the hardware command. Also, translator  may translate the hardware command into a software command (or commands). Management computer  then issues the software command to relevant software elements . Each software element  may then configure itself to enable correct communication pathways to accept authorized connections to a system troubleshooting and diagnostics. In this case, hardware component input events may be translated into higher level API interaction that software elements  can use at its expected level of abstraction and not have to drop to lower level communication protocols to interact with hardware elements .","Accordingly, leveraging management computer  as an API endpoint, software elements  can communicate with hardware elements  using a constant level of abstraction that exists between other software elements of the distributed computing system. In other words, management computer  exposes hardware functions as software functions and may be called in the same way that software elements  interact with other components. This provides rapid development of software elements using hardware parameters and negates the need for other forms of hardware interaction such as fixed routines independently developed or written to EEPROM or other memory that would diverge from prevalent architecture present in the distributed computing system.","Management computer  may serve as the primary software support and diagnostics gateway to all other components in controller node . Multiple out of band interconnections with other controller node components, primarily by means of Ethernet network, serial, or universal serial bus, permit management computer  to fully access and administer main network switch , main computer , and microcontrollers.","In some embodiments, certain thermal sensor data is available only by means of internet protocol network, such as by the Simple Network Monitoring Protocol (SNMP). In such embodiments, management computer  interfaces with components offering thermal sensor data over SNMP to collect, process, and monitor thermal sensor data from these components. Examples of devices rendering thermal sensor data over SNMP include devices that host independent environmental regulation systems, such as main network switch  and main computer . Thermal sensor data gathered over internet protocol network are conveyed to master microcontroller  and to environmental microcontroller  for use in calculating fan duty cycle and determining whether power interrupt is necessary to prevent thermal damage from occurring to controller node .","The multiple independent components that comprise controller node  each require appropriate power supplies and thermal conditions. To accommodate these environment requirements; the controller node's active PCBs host at least one thermal sensor. The data from these thermal sensors is made available throughout the controller node, including to management computer , main computer , and main network switch . Microcontrollers supply thermal data to the controller node's components and respond to requests to increase or decrease fan speed from the various controller node components by making the appropriate adjustments to the controller node's fans. Controller node  includes at least three fans which are operable in both directions. Fan direction is recorded in memory and can be adjusted to match the desired airflow characteristics of the facility in which controller node  is deployed. A failure of any two or more fans triggers a power interruption to the chassis to prevent an unsafe thermal condition from occurring. Power consumption and power supply fan speed data is made available to controller node components by means of the PMBus protocol. If controller node components, such as the main network switch  and main computer , ordinarily have dedicated, fan cooled power supplies, signals from these components which query, increase, or decrease fan speed are intercepted by the environmental microcontroller. Such requests are accommodated by increasing or decreasing controller node fan speed, and appropriate response signals are provided to the requesting components in the signal format such components expect. Power supply fan speed response signals emulate those that would ordinarily be issued by a dedicated, fan cooled power supply, and include adjustments to fan speed data the requesting component would expect in response to commands to increase or decrease fan speed. This emulation ensures proper functionality of the independent controller component while maintaining a thermal and power environment common to and suitable for all controller node components. General fan data for controller node components that ordinarily have and expect independent fans is also emulated and provided to the requesting components, including the number and characteristics of the expected fans. Controller node component requests to increase or decrease fan speed are serviced by making the appropriate adjustments to controller node fan speed and responding to the requesting components in the signaling format the requesting component expects with emulated data, including the expected increase or decrease in fan speed, as appropriate.","Controller Node and Physical Node Interaction","Controller node  may have a physical administration interface in the form of an externally accessible, user facing touchscreen LCD display. The physical administration interface is powered independently of the main controller node components and permits the controller node's administrator to power on the remainder of the controller node's components. The physical administration interface displays real time data about attached physical nodes, including number, state, and capacity. In addition, the physical administration interface can be configured to display support information and controls, including log data, performance data, fault data, software version numbers, hardware version numbers, and enabling or disabling the external support network interface.","In some embodiments, each physical node  in the distributed computing system is attached to a controller node  by means of cloud cable terminated into a cloud card on the physical node. A cloud card is a network interface device containing at least one management processor and high performance storage. In one embodiment, the cloud card interfaces with the host physical node as an expansion card utilizing a PCI-E interconnection. Additional interfaces on the cloud card include an intelligent platform management bus interface, side-band Ethernet interface, general purpose input output pins, and serial bus interface. Where available, the intelligent platform management bus interface attaches to the corresponding intelligent platform management bus header on the physical node's motherboard, providing access to the physical node's baseboard management controller, which implements intelligent platform management (on, off, etc. . . . , as described above). A direct connection between the cloud card and the physical node's baseboard management controller by means of intelligent platform management bus interface permits the cloud card to control the physical node using standard intelligent platform management interface commands, including power on, power off, reset, read power status, read system event logs, and read sensor data. Alternatively, where the baseboard management controller cannot be interfaced directly by means of intelligent platform management bus interface, the cloud card may achieve some of the same command functions over physical node  by means of attaching the cloud card's general purpose input output pins to the physical node motherboard front panel header containing power switch, reset switch, power status indicator, and disk activity indicator pins. When attached to a physical node's front panel header, the cloud card is able to implement a subset of the intelligent platform management interface commands, including power on, power off, reset, and read power status. The cloud card's management processor is responsible for interfacing with an I2C protocol bus in the attached cloud cable, interpreting signals delivered thereby, and issuing appropriate commands to the physical node by means of intelligent platform management interface commands, front panel header switch emulation, or other suitable means of effecting control of the physical node's power state and configuration.","Other functions of the cloud card's management processor include configuration of baseboard management controller, configuration of the cloud card network interface, firmware upgrades for the cloud card network interface, firmware upgrades of the cloud card management processor, serial interface relay, and keyboard-video-mouse relay. In some embodiments, the physical node baseboard management controllers are configurable, including username and password. The cloud card management processor interfaces with the baseboard management controller through the intelligent platform management bus header and configures these properties to the distributed computing system's desired operational specification. For example, in the case of username and password, these properties are set to values controlled by the distributed computing system to ensure successful authentication and control of the baseboard management controller.","In some embodiments, the distributed computing system correlates physical node network interface MAC addresses with the physical location of the physical node in relation to controller node  and other physical nodes  in the same server rack. To maintain this consistency, a specific cloud cable is associated with a defined location in the server rack. When a cloud cable so associated is attached to a cloud card in a physical node, an expected MAC address is communicated with the cloud card from an associated controller. The cloud card then modifies the MAC address of its network interface device to match the MAC address received from the controller and expected by the distributed computing system for the particular rack location the physical node has been installed in. This level of correlation permits management and administration decisions to be made in accordance with defined rack location. For instance, a well-defined IP address scheme may be administered according to physical rack location, such that the physical node in a designated rack location will always receive a certain IP address in a given allocation of IP addresses.","In some embodiments, the cloud card provides additional channels for unattended management and control of the physical node through serial interface relay and keyboard-video-mouse relay functions. The serial interface relay attaches to the physical node's serial interface by means of universal asynchronous receiver\/transmitter which permits the physical node's serial console to be interacted with over the cloud cable I2C bus. Due to the higher bandwidth requirements of keyboard-video-mouse, this functionality is implemented using the network controller sideband interface standard, which provides higher throughput up to controller node . In embodiments offering the keyboard-video-mouse channel, the cloud card management processor may contain a graphic subsystem and universal serial bus human interface device profile to relay the video output of physical node  and to issue keyboard and mouse commands, as needed.","In embodiments which do not utilize cloud cables and cloud cards, an out of band management network may be created between controller node  and physical nodes  independent of the primary network connections between controller node  and physical nodes . This independent out of band management network is used to issue intelligent platform management interface commands to physical nodes.","The basic controls controller node  has over the physical nodes , including on, off, restart, and change boot order, can be grouped and executed to achieve varying management and administration objectives. The power control commands are used by the distributed computing system to stagger the initial power on of physical nodes  in order to distribute the network and power impact of such initial power on events over a period of time, resulting in lower overall datacenter network and power consumption. The delay in starting subsequent physical nodes  can be configured to equate with either the amount of time a physical node  typically takes to complete power-on self tests, the amount of time required to become fully provisioned and operational, or another period which approximates the duration of initial high current consumption following a power on event. Staggered start is useful both in initial power on of a new system as well as recovering from electrical faults in an orderly fashion. Controlled power on can assist facility operators in managing overall power consumption by mitigating the high initial power draw of physical nodes when booting as opposed to power draw when operational. As a result, overall electrical current need not in all cases equate with the maximum potential current draw of a distributed computing system. In addition, the power control commands can be used by the distributed computing system to balance resource consumption and resource capacity. If the distributed computing orchestration service determines that overall system use as manifested in physical node resource consumption falls below system capacity, the distributed computing system can migrate and concentrate workloads onto a subset of physical nodes. Once physical nodes are freed of workloads, the appropriate management commands, typically in the form of intelligent platform management interface commands, may be issued to power down the unused physical nodes until needed. The distributed computing system may then power on additional physical nodes as needed and distribute workloads to those physical nodes to meet fluctuating workload demands.","Management controls may also be used for identification of physical nodes . This is useful in configurations with multiple physical nodes  when one such physical node fails or otherwise requires physical maintenance. By issuing management commands to a physical node , the distributed computing system is able to control the power and activity lights on physical node , illuminating them in patterns which distinguish the desired physical node  from other physical nodes , and thereby visually denoting physical node  requiring physical maintenance to facility personnel.","The ability to control the power state and configuration of physical nodes  permits the distributed computing system to provision newly attached physical nodes  from a powered but off state through to installation into the distributed computing system as an operational resource. This is achieved by either manually ensuring that physical nodes  are configured to seek a boot image through their network interface card (e.g., using the PXE protocol) or using management interfaces to configure the physical node's boot order to select boot from the network interface card. Upon initial network boot, physical node  will obtain its operating system image from the controller node that the physical node is attached to (e.g., through the PXE protocol). In particular example embodiments, controller node  provides attached physical nodes  with an intermediate boot loader (e.g., by means of TFTP). This intermediate boot loader may permit the physical node to obtain its primary operating system image by more reliable transport protocols, such HTTP. Once booted, this operating system image is configured to initiate communication with controller node  through a well-defined IP address scheme where controller node  uses a specified network address. Further configuration of physical node  may be delivered from the controller node once successful communication is established with the controller node. Configuration may include allocation of physical nodes  storage capacity for different tiers of storage, configuration of the orchestration service instance on the physical node, configuration of the cloud compute service scheduler on the physical node, and any software updates which may be required. Upon final configuration, software updates, and registration with the controller node, the physical node is fully provisioned and added to the resource pool.","Upgrade of Controller Node","The distributed computing system may be deployed in various configurations, some of which may not be remotely accessible, and the software installed on the distributed computing system should be operated and maintained with reliability and predictability. The distributed computing system is able to receive and authenticate new software, distribute the software among other nodes in the cluster configuration, and orchestrate the upgrade without significant operational impact to nodes in the distributed computing environment. The software upgrade may be performed using management computer , main computer , and physical nodes .","In one embodiment, a software upgrade package may be received by controller node . The software upgrade may be an operating system and\/or applications that are running on controller node  for one or more components. For example, a system administrator may receive a signed software upgrade package in the form of an upgrade disk image and upload it to the distributed computing system through a user interface. In one embodiment, an internal network to a controller node  is used to upload the software upgrade package. The software upgrade package may be an upgrade disk image that contains a complete image of the software. That is, the previously-installed software being used by the distributed computing system may be replaced by software on the software upgrade package. This provides a consistent image to the distributed computing system.","Management computer  may coordinate the upgrade. In this way, the upgrade may be performed automatically and also in a controlled fashion without user input after receiving the software upgrade package. In the upgrade flow, management computer  may first upgrade itself. Then, management computer  may coordinate the upgrade of controller node  via main computer . This is in a single controller node  system. However, a multiple controller node  system upgrade may be performed and will be described in more detail below.",{"@attributes":{"id":"p-0106","num":"0105"},"figref":"FIG. 3","b":["126","107","107","107","126","130","126","126","130","126","130","130","130","126","126","130"]},"The above process provides many advantages. As the upgrade is a full image upgrade, main computer  (e.g., controller node ) cannot wipe itself and load software upgrade without having a failure recovery scenario. By having management computer  provide the upgrade image and issue management commands to main computer  to effect the upgrade, management computer  ensure success of the upgrade or a failure recovery. Without using management computer , a failure may result in a broken state. However, using management computer , management computer  can attempt to roll back to the previous software version to recover from any failures. In the distributed computing system, this level of resiliency is very important. The threat of failure is minimized on management computer  as it is a fairly static configuration. However, main computer  may contain valuable date, such as customer data, and directly services customer requests, which can vary in load and can potentially produce unexpected outcomes that may obstruct the upgrade process. Management computer  can also access state and configuration data and pass that data to main computer  at key points in main computer's  startup and build out. Examples include network information, which varies on controller node , but remains static on management computer . Thus, management computer  provides a reliable, consistent, always-on system to navigate and debug the pre-install environment on main computer . The pattern of image wiping provides certainty as to outcome and is may be much faster than trying to upgrade the individual software components in-place.","In the upgrade process, in a step #1 (reference ), management computer  may also verify the software upgrade package. For example, management computer  may verify the cryptographic signature of the upgrade disk image against cryptographic keys irreversibly written to management computer .","Management computer  may maintain the currently-installed software disk image as a baseline. In this case, the upgrade software package will not upgrade this baseline image. Rather, this image may be available for restoration at a later time. This allows management computer  to roll back to a known state if the update fails. To keep the currently-running disk image as a baseline, at step #2 (reference ), management computer  may write elements of the software upgrade package relevant to upgrading management computer  to a partition in storage  for management computer . For example, storage  may include a first partition (A partition) - and a second partition (B partition) -. These may be separate partitions on a persistent storage device that is associated with management computer . These partitions may be in an alpha-beta (A\/B) scheme where one partition includes the active software and the other partition is inactive, but is suitable for storing the relevant portions of the software upgrade package. For example, as shown, management computer  has stored the relevant upgrade software components from the software upgrade package in a file system in B partition -. The current software is stored in a file system in A partition -, which is the active partition right now.","In a step #3 (reference ), upon a successful writing of the update software to B partition -, management computer  designates the inactive file system as the active file system (i.e., designating the B partition as active). This also causes A partition - to become inactive. Once this occurs, management computer  can start the upgrade transition. In a step #4 (reference ), management computer  reboots itself. The rebooting ensures that management computer  starts from an initial state using the software upgrade. Upon rebooting, management computer  finds the active partition, which is B partition -, and boots from the upgrade of the software stored in the file system. This effectively upgrades management computer . Management computer  may also perform other upgrade actions, such as upgrading the firmware for attached peripheral interface controllers through serial interfaces to those devices. The upgrade process for management computer  may now be complete.","After upgrading management computer , controller node  may upgrade itself via main computer .  depicts an example of the upgrade process of main computer  according to one embodiment. In a step #1 (reference ) management computer  starts the upgrade of controller node  upon finishing the upgrade of itself. In one embodiment, management computer  may send a message to main computer  indicating the upgrade was successful and telling main computer  to initiate upgrade of controller node . In this case, upgrade of controller node  may include upgrading the software for main computer .","When main computer  receives the indication that management computer  has successfully upgraded itself, in a step #2 (reference ), main computer  verifies the health and eligibility of controller node  to upgrade. For example, main computer  may evaluate that all expected services are available and that each expected service satisfies health checks specific to the service types. If the services conform to defined operational parameters in the health checks, then the health checks pass and the upgrade process can proceed. If one of the health checks fails, then main computer  may attempt to recover from the failure and the upgrade may proceed after that. The upgrade may not proceed if a failed health check cannot be resolved.","Assuming the verification is successful, in a step #3 (reference ), main computer  starts withdrawing the use of services that controller node  is controlling with respect to physical nodes . The services being performed may be stateless services or stateful services. Stateless services do not need to have any state information stored that will persist across the upgrade. However, stateful services may need to have state information persist across the upgrade. In this case, in a step #4 (reference ), main computer  commits relevant data for the services to storage , which is storage associated with main computer . Even though stateless services do not commit any state data, stateless services may commit configuration data to storage  before being withdrawn. In one embodiment, the configuration data includes an initial configuration for the service and state data may include data that was determined based on the operation of a stateful service. Stateless services do not need to use any state data. However, stateful services may store configuration data and also state data to storage . The state data may then be used by the stateful services upon the upgrade. Also, the configuration data and state data may persist across the upgrade in storage . In a step #5 (reference ), main computer  stores configuration data to storage  on management computer . This configuration data may be configuration data for main computer  instead of for the services. This configuration data is stored with management computer  to ensure that the data persists across the upgrade process, which can be destructive to data stored elsewhere on main computer  or controller node . Other storage devices service main computer  and are attached during main computer 's normal operations, and are not available prior to main computer  being operational. In addition, configuration and state data in management computer  may be accessed during the upgrade process prior to restoration of functionality in main computer . Upon the storing of the data, the withdrawal of the services is completed. The order of the withdrawal results in a consistent image of the state of controller node  prior to the upgrade being initiated.","In a step #6 (reference ), main computer  issues an upgrade request to management computer . The upgrade request is sent to management computer  because management computer  coordinates the upgrade in an automatic manner. For example, management computer  is contacted to initiate the upgrade because management computer  may provide the upgrade image to main computer  upon reboot. In response to receiving the upgrade request, in a step #7 (reference ), management computer  causes main computer  (i.e., controller node ) to reboot. The reboot may be performed such that main computer  reboots from the new upgrade image. The reboot permits controller node  to download the upgrade image from management computer  and boot from the upgrade image.","Upon reboot, main computer  may start the upgrade process by attempting to determine the update image.  depicts an example of the upgrade process for main computer  according to one embodiment. In a step #1 (reference ), main computer , upon reboot, polls management computer  for a software image, such as the update software image. For example, main computer  may send a request to management computer  for the update software image. The request is sent to management computer  because management computer  has received the software update package, has verified the software update package, and has communication pathways with main computer  sufficient for main computer  to download the upgrade image from management computer  during main computer's  start up routine. In a step #2 (reference ), management computer  determines relevant components of the software update package and sends an update image to main computer . In a step #3 (reference ), main computer  stores the update image in storage  as its boot disk. This is the image that main computer  boots from upon any restart. Once stored, in a step #4 (reference ), main computer  concludes its start-up routine by booting from the update software image that was stored in storage . The reboot is used to ensure controller node  enters into a known state. This is on contrast to an in-place upgrade, which may permit the possibility of entering into unknown error states. In particular embodiments, main computer  is rebooted from a new or known master update image. The reboot permits controller node  to download its new update image from management computer  and boot from that new image.","At this point, main computer  (controller node ) has no state data due to the update. Thus, controller node  does not know if controller node  is part of a multi-controller system (e.g., a cluster) or not. In this case, in a step #5 (reference ), main computer  attempts to join a pre-existing cluster. In a case of a single-controller system, no cluster exists, and therefore main computer  does not join a cluster. In this example, it is assumed this is a single-controller system. However, a multi-controller system will be described in more detail below.","In a step #6 (reference ), main computer  retrieves configuration data and state data that was written to management computer  previously. This is the configuration and state data for main computer  and can be used to reconstruct the previously-withdrawn services. Thus, in a step #7 (reference ), main computer  restarts the use of the services in an ordered fashion. For example, stateful services, such as database services and databases, are initialized and populated with the pre-upgrade state data first. Main computer  may perform any migrations or transformations to this data before proceeding with further service restarting so that any services that rely on this data are presented with a consistent presentation of data. After restoring the stateful services, main computer  restores the stateless services by retrieving configuration data from storage  for the stateless services. Once the services have been restored, main computer  performs any post-update actions.","In a multi-controller node system, orchestration of the upgrade between controller node systems  is needed.  depicts an example of the upgrade process in a multi-controller node system according to one embodiment. In one embodiment, the multi-controller node system may include two or more controllers. The individual controller upgrade steps may be the same as discussed above, but the order of upgrade for each controller node  may be defined. In one embodiment, the multi-controller node system may have a zone leader that acts as the leader and holds authoritative data for the cluster. As shown, a controller node - is the zone leader. Other member controller nodes ---N are included in the multi-controller system.","In a step #1 (reference ), zone leader controller node - may receive and validate the upgrade software package. In a step #2 (reference ), when validated, zone leader controller node - distributes the upgrade software package to other controller nodes ---. Each controller node ---N also validates the upgrade software package.","In a step #3 (reference ), zone leader controller node - performs health checks across the cluster. The health checks ensure that all controller nodes  in the cluster are operating without any problems that may affect the upgrade.","Then, in a step #4 (reference ), zone leader controller node  communicates with controller nodes ---N to agree on a version of the upgrade software to upgrade to. This ensures that all controller nodes  are upgrading to the same version. In a step #5 (reference ), if a consensus on a version is agreed on, zone leader controller node - selects a member controller ---N to undergo the upgrade procedure first. In this case, zone leader controller - does not upgrade itself first. A member controller node - is selected to upgrade first, after which such controller node - can be named zone leader while zone leader - may maintain the essential data for the current software. This may be important if the upgrade fails. In the case of a failure, the cluster may revert back to the original software.","Once being elected to perform the upgrade, in a step #6 (reference ), member controller - performs the upgrade. This upgrade may be performed as described above in the single-controller upgrade process. When member controller node - completes the upgrade process, member controller node - rejoins the cluster. In a step #7 (reference ), member controller node - becomes the zone leader of the multi-controller zone. In this case, zone leader controller node - abdicates the zone leadership to member controller node -, which has been upgraded. By abdicating the leadership, member controller node - is the zone leader and operating at the updated software version. This ensures that the zone leader is operating using the latest version of the software. This may be important because the zone leader is the master source of many key services, such as database services, and thus needs to reflect the latest version of the software.","In a #step 8 (reference ), new zone leader controller node - instructs other controller nodes  to upgrade. For example, former zone leader controller node - and other controller nodes  may perform the upgrade process in series and\/or parallel. In one embodiment, the controller nodes may be upgraded in series such that a quorum may be maintained. For example, the multi-controller node system works on a quorum system so that a majority of the controller nodes  are available to ensure consistent data. When controller node - is successfully upgraded, new resources may be scheduled on controller nodes  or existing ones may be terminated, and preexisting resources will have been available throughout the upgrade process.","When controller nodes  have been upgraded, controller nodes  may also instruct attached physical resource nodes  to perform in-place upgrades of individual software packages. For example, the individual software packages may be updated in place without disrupting cloud instances that may be running on these physical nodes.","Accordingly, the upgrade process may be performed to upgrade a distributed computing system that includes insular, but interdependent components. The withdrawal of services prevents various services from generating or committing changes that may corrupt the state of the system. The consistent state image that is maintained provides for a reliable upgrade. Also, the multiple verification steps may establish a root of trust that chains the validation from management computer  to main computer , to physical nodes . The timing and sequence of events, the preservation of state and configuration data on management computer , other persistent data storage, and the coordination of functions across multiple controllers provide the ability to upgrade components of the distributed computing system without user intervention.","Orchestration Service","Orchestration Service Architecture","The distributed computing system is arranged in a hierarchy in which instances of an orchestration service are distributed in various entities and interact via a communication service. The orchestration service is responsible for creating and maintaining a cohesive and unified system that appears as a single system to the user, despite failures of both hardware and software, and for coordinating the execution and management of all system services and ensuring their availability. The orchestration service's basic functions include starting, stopping, restarting, monitoring, configuring, and reconfiguring various system components. The hierarchy of the orchestration service gives the distributed computing system its turnkey character. In this example, this turnkey cohesion is achieved by operating an instance of the orchestration service on each controller node , physical node , and zone , which collectively implement the overall orchestration system service. This example of loosely coupled orchestration service instances (OSI) is organized in a manner that decentralizes the overall management of a zone, requiring little direct communication between orchestration service instances in general, and enabling better scalability as a distributed computing system grows in the number of controller nodes  and physical nodes  without unacceptably increasing the cost of communication within the system.",{"@attributes":{"id":"p-0128","num":"0127"},"figref":"FIG. 7","b":["702","107","102","107","102","107","102","107","102","107","102"]},"The distributed computing system may provide various services, such as an orchestration service, controller system services, physical node services, and object storage services. In particular, each controller node , physical node , and zone  runs an instance of the orchestration service (OSI) , which manages the overall functions of the distributed computing system. Further, a hierarchy of other orchestration service instances , , and  operate together to collectively implement the orchestration service. As will be described in more detail below, the hierarchy of the orchestration service instances communicate indirectly through a communication service referred to as a \u201cblackboard service\u201d, which maintains a global system state of the distributed computing system. All orchestration service instances  and  on controller node  may maintain this global system state. The indirect communication allows the orchestration service to be decentralized and the distributed computing system can be scaled more efficiently as new physical nodes  and controller nodes  can communication through the blackboard service when added to the distributed computing system. The blackboard service is a highly available configuration and synchronization service. It may exist on each controller node  and can thus survive the failure of any single controller node  in a multi controller system. By appealing to this blackboard service for configuration and state data, the various subsystems and components in the distributed computing system have a common authoritative location for this information. This reduces cross talk in the distributed computing system and provides for consistent authoritative data that does not need to be replicated among each of the components of the distributed computing system.","Controller  may have multiple orchestration service instances running on it, such as orchestration service instances  and . Orchestration service instance  manages the controller node and orchestration service instances  manage respective system services . For example, system services  operate in the controller node within containers on a respective controller node . Orchestration service instances  are responsible for locally managing the system services in the containers. Also, orchestration service instance  may oversee the containers and other controller node operations. Additionally, orchestration service instance  may coordinate with other controller nodes  or other physical nodes  on demand. The inclusion of orchestration service instances  and  on controller nodes  allows the distributed computing system to manage the overall coordination and health of the service containers, as opposed to the services within those containers, and in the case of the zone leader, manage coordination and health of the cluster, such as controller node  and the services on those controller nodes .","Each physical node  runs a set of system services  that operate on respective physical nodes . These system services  perform operations, such as launching virtual machines (VMs) on behalf of customers, storing VM data on node-local persistent storage, and accessing the distributed Object Storage Service . In one example, a portion of a computer system service  runs on a controller node  and is responsible for choosing a physical node  that satisfies the resource requirements demanded by the customer for a given VM and coordinating with a compute service  on physical node . Each running VM is guaranteed a portion of the local disk storage attached to the node.","In the hierarchy of orchestration service instances, an orchestration service instance  also runs on physical node  to orchestrate a respective system service  running on physical node . Orchestration service instances  may be responsible for locally managing a compute service, a volume service, and a network service, verifying the local service's health, and ensuring the local services availability in spite of failures. The inclusion of orchestration service instances  on physical nodes  allows the distributed computing system to scale efficiently as physical nodes  can be added to the system in a reliable fashion. Orchestration service instance  on physical node  is responsible for ensuring that required services are running and configured to interact with the attached controller node . The detection of failures due either to software or hardware fault results in physical node  being marked as offline such that controller node  will no longer schedule new resources to be created or operated on the failed physical node .","In one example embodiment, an Object Storage Service (OSS)  consolidates all remaining physical storage from all disk storage on all physical nodes into a single large pool of storage. OSS  is decentralized and masks the inevitable failures of nodes and disks; it replicates data for high availability. To emphasize that OSS  is a zone-wide resource,  shows the distributed Object Storage Service spanning the entire zone of controllers and nodes, assimilating the disk storage from all physical nodes.","Example System Orchestration Service",{"@attributes":{"id":"p-0135","num":"0134"},"figref":"FIG. 8","b":["107","1","107","2","107","3","107","1","107","708","708","1","708","2","708","3","802","107","102"]},"Each controller node  includes a set of system service containers . Containers  isolate system services , such as the operating system and application software, including user-space operation system virtualization such as LXC or chroot jails and full virtualization such as KVM. Although containers are described, the container may be are may be any means of isolating system services , and may be considered a virtual machine or other implementation that isolates a system service . Each container  contains an orchestration service instance  and associated system service . Orchestration service instance  monitors an associated system service  that is found in a container . This is in contrast to orchestration service instance , which monitors containers .","Each physical node  contains an orchestration service instance  and a set of one or more system services . Orchestration service instances  monitor the associated system services  on physical node . For example, for each system service , a corresponding orchestration service instance  may be provided to monitor a respective system service .","As mentioned earlier, orchestration service instances , , , and  are organized hierarchically, each with a core set of functionality and some additional functionality depending on their place in the hierarchy. The zone's orchestration service instance  may present the illusion of a single system and may be responsible for exposing customer-facing functionality, adding and removing controller nodes  and physical nodes  from zone , verifying the health of all nodes, maintaining the global state of the system, backing up any data or state information, and masking failures, for example. Orchestration service instances  have functionality that monitor controller node level information, orchestration service instances have functionality that monitor system service  information for containers , and orchestration service instances  have functionality that monitor system service  information in physical nodes .","In this example, the controller's node orchestration service instance  manages the controller node  including the status of service containers . This includes managing the set of controller-specific system services running on it (starting, stopping, restarting, and configuring), verifies their health, backs up any data or state information, and ensures that their capabilities are available in spite of failures. An example system service may include, for example, a system service provided in OpenStack\u2122 for supporting cloud computing functionality. Local data or state information may be recorded on persistent storage associated with that controller node .","Orchestration service instances  manage system services  within a respective container . If any system service  fails for whatever reason, it is the responsibility of the associated orchestration service instance  to restart that service. Orchestration service instances , therefore, behave very much like a watchdog over that service.","The physical node's orchestration service instance  manages that physical node's system services  (starting, stopping, restarting, and configuring) and ensures their availability. Orchestration service instance  may also record local data and state information on persistent storage associated with that node.","There may be two types of communication in the present example orchestration service architecture. First, each orchestration service instance , , and  shares a common blackboard service  as a means of communicating state information, both static and dynamic, with one another. Each orchestration service instance , , and , whether in a controller node , container , or on a physical node , establishes a session to the blackboard service  to record and update the global system state. The global system state may include the names and states of all controller nodes  and physical nodes , as well as the names and states of all the system services  and  running in the zone. This global state incorporates the current known state of all the controller nodes  and physical nodes . Second, each orchestration service instance , , and  is equipped with an API. An entity in the distributed computing system may invoke operations of the API to cause that orchestration service instance to perform the indicated function, such as asking for status of a system service like MySQL.","Each controller node  may record its existence and some additional state information in the shared blackboard service . In addition, every system service  on a controller node  may also record its existence and some state information in the shared blackboard service (indicating which controller the system services  are running on). In one particular example, it is through the shared blackboard service  that the zone orchestration service instance - can learn about a new controller node  and all of the controller node's system services , which constitutes a portion of the global system state. Further, orchestration service instance  may directly communicate with the orchestration service instances  running on each physical node  in its rack only when that physical node  is booting for the first time and while that physical node  is being integrated into the cloud fabric. Orchestration service instance , too, directly communicates with the controller node's orchestration service instances \/ only during the physical node's booting sequence to incorporate it into the cloud fabric.","In one example implementation, unlike a controller node , every system service  (compute, volume, network) on a physical node  does not record its existence in the shared blackboard service. Instead, these services  update a central store residing on the physical node  at a pre-determined interval to indicate that they are alive (e.g., a \u201cheartbeat\u201d). Orchestration service instance , through its function definition, may detect whether the local store was updated or the service is not running; if the status has not been updated or the service is dead, for example, then orchestration service instance  updates the corresponding physical node's status to \u201coffline\u201d on blackboard service , which indicates that something is wrong, and the whole physical node may go offline. In this way, the zone controller node - may discover a problem with that physical node  through its own periodic probing of the global system state in the blackboard service . The service in question may be restarted by the orchestration service instance  on the physical node .","Particular embodiments maintain the currency of the state that captures and reflects an ever-changing distributed computing system over a period of time in the face of failures\u2014especially as the distributed computing system grows in size in terms of increasing network traffic and in terms of the number of controller nodes , the number of physical nodes , and their storage capacity. The hierarchical organization of a distributed computing system mitigates this complexity by constraining the communication domains and limiting the impact of hardware failures. Physical nodes  in a rack are directly connected to their controller node  only\u2014not to any other controller node , which might be done for high availability in other systems; such an organization both defines a communication domain for the physical nodes  in the rack and isolates physical nodes  from other physical nodes  in other racks. Communication patterns are well-defined, as described earlier, because the communication in the system flows over different system-wide logical networks that are layered on top of the same physical network. For example, data traffic between running virtual machines occurs over the guest logical network, whereas all the orchestration service server instances communicate over the management logical network.","A portion of this global system state is dynamic, changing as system components join or leave the system. A major portion of the global system state is static, characterized typically by configuration data that is fixed. This configuration data in the distributed computing system is represented by distributed computing \u201cmodels\u201d, which are schema definitions for data that is gathered for objects in the system that have state information. Orchestration service instances , , and  create these model objects in the memory of the associated controller node , container , or physical node . Controller nodes  make changes to the state of these model objects, and these changes are reflected in the blackboard service  by invoking the appropriate methods on the objects; thus, the \u201cclients\u201d of the model objects leave the details of interacting with blackboard service  to the model objects. Some of the attributes of these objects change over time, and thus are dynamic, like the status of a container , which could be \u201conline\u201d or \u201coff-line.\u201d What portion of the global system state is dynamic and what portion depends on the semantics of the objects that are stored in the state.","The following will now discuss the blackboard service in more detail.","Example Orchestration Service Architecture Using the Blackboard Service",{"@attributes":{"id":"p-0149","num":"0148"},"figref":"FIG. 9","b":["804","107","802","102"]},"Each orchestration service instance , , and  may establish a session to blackboard service  to register its existence so that other orchestration service instances , , and  may become aware of it. In one embodiment, a presence service (M-P)  performs this function to announce the existence of an orchestration service instance. In one embodiment, the orchestration service instance and presence service  exist as a pair. They are logically part of the same service, and therefore may be a single component rather than separate components as illustrated in this example implementation. Presence service  may also perform a second function\u2014conducting an election on behalf of a system service that must be organized as a master with one or more slaves, which will be discussed in more detail below.","Each presence service  may have a single session to the blackboard service . Also, each orchestration service instance , , and  may have a separate, distinct, session to the blackboard service . This session from each orchestration service instance , , and  may be used for its communication with as a shared service, rather than for existence, which is the function of the M-P server instance. When a single session is mentioned, it is generic and may actually encompass multiple sessions, depending on how presence service  and each orchestration service instance , , and  are configured and is not meant to limit the implementation. Note that the line indicating the session from the orchestration service is shown to intersect with the session line emanating from the M-P server instance for clarity; the sessions are, in this example, separate and distinct sessions and are not shared.","Each orchestration service instance , , and  may have a second communication path via an orchestration service API . Entities communicate with an orchestration service instance by invoking API operations. For example, presence service  can ask its associated orchestration service instance , , or : \u201cAre you healthy?\u201d through API  by sending an appropriate HTTP request. Further, orchestration service instance , , and  can respond to API invocations from other components, such as other orchestration server instances.","The dynamic state is determined by the existence or non-existence of presence service . For example, if either presence service  in a container  fails or the container  itself fails (causing presence service  instance to also fail), then the data node corresponding to container  will be automatically deleted from the global system state in blackboard service . It may not be enough to record existence or nonexistence of a data object because some entity may be required to detect these changes or be notified of them and in either case, take some appropriate action, if necessary. That entity is the set of orchestration service instances that are responsible for periodically checking the global system state for the existence of all the service containers  residing on their respective controllers, detecting these changes, and updating the affected model objects. In turn, this translates into updating the corresponding data objects in blackboard service .",{"@attributes":{"id":"p-0154","num":"0153"},"figref":"FIG. 10","b":["1000","804","1002","804","107","102","802","107","102","802","804","708","709","712","804","712","102","102","804"]},"At , orchestration service instances , , and  may monitor blackboard service  for changes. When changes are detected, orchestration service instances , , and  determine if an action needs to be taken. The actions may include changing data structures to represent the changes, or taking a remedial action if there is a problem. If no action needs to be taken, then the process reiterates to monitoring blackboard service  for more changes. If there is an action to take, at , orchestration service instances , , and  determine an action to perform. An action may be restarting a service or electing a new master. At , orchestration service instances , , and  perform the action. In the above, orchestration service instances , , and  perform the monitoring and performing the action through blackboard service . Indirectly communicating through blackboard service  allows the monitoring to be performed by a hierarchy of distributed orchestration service instances , , and . The blackboard exists outside of the hierarchy of distributed orchestration service instances , , and  and is therefore available of all components of the hierarchy. In addition, the blackboard itself is structured hierarchically, providing elements of the hierarchy the ability to walk a tree and determine the relationship of components in a hierarchical fashion.","In one embodiment, the hierarchy of orchestration service instances , , and  determines what each orchestration service instance is monitoring. For example, orchestration service instance  of controller node  manages controller node , which includes the status of service containers . Orchestration service instances  are responsible for monitoring the related system services  in service containers . This includes managing system service health, controlling and managing system services , and report system service status to blackboard service . Orchestration service instances  on physical nodes  monitor system services  on physical nodes . The zone controller node -, in addition to performing controller node operations on the local controller, is responsible for sweeping the cluster and inspecting health and issuing management commands.","System services may be operated in a master-slave configuration. When a system service  is created in a container , an election process may be performed. This process is described below in the presence component.","Example Presence Component",{"@attributes":{"id":"p-0159","num":"0158"},"figref":["FIG. 11","FIG. 11"],"b":["902","902","902","902","1","107","1","902","2","709","802","902"]},"In census mode (configured based on a configuration file), presence service - executes is a process and may register itself with blackboard service  to indicate that presence service - exists and is operational on behalf of orchestration service instance . This registration involves creating a data node in an established session (the connection to the blackboard service) between presence service - and the blackboard service . In one example implementation, a data node under the blackboard service is named uniquely by a path that resembles a UNIX filesystem to a file or directory such as \/orchestration\/presence\/node\/{controller#}-{MAC address}, which names a specific node as a combination of the controller node number of the controller node and the MAC address of the controller node's primary network interface. Controller nodes  are assigned integer values, and these are the controller node numbers. A data node representing existence is sometimes referred to as \u201cephemeral\u201d because its lifetime is tied to the session and if either the session or the client application program fails, the data node may be automatically deleted by the blackboard service.","In order to test for existence, one approach is to query the blackboard service periodically and check to see whether the data node for orchestration service instance  in question exists. If the data node does not exist, then this means orchestration service instance  likely failed because presence service - died and ultimately released the data node. If the data node still exists, then the associated service still exists. In addition, census mode may further determine the \u201chealth\u201d of the associated orchestration service instance . In census mode, presence service - queries its companion orchestration service instance  via the API and asks the question \u201cAre you healthy?\u201d In turn, the orchestration service instance  performs a service-specific health check of orchestration service instance . The path of the health check may start with presence service - in service container  making a query to orchestration service instance  via API  asking \u201cAre you healthy?,\u201d which in turn invokes a probe to ask the same question of system service .","Orchestration service instance  (e.g., the zone controller node leader) may have two responsibilities: first, as explained earlier, orchestration service instance  manages all containers  for system services on controller node ; and second, orchestration service instance  periodically inspects the blackboard service  for the presence of the ephemeral data nodes for all the controller nodes  and all physical nodes  in a distributed computing system. It is this second function that enables orchestration service instance  to quickly react to failure anywhere in the system and take appropriate measures to rectify the situation. The other controller nodes  pay attention only to their own containers . In , periodic inspection is shown with a line emanating from the \u201cperiodic\u201d function group to blackboard service . Since the orchestration service leader is inspecting the global system state recorded in the blackboard service on a recurring basis for any changes, whether good (such as a node joining the system) or bad (such as a container failing), the orchestration service leader is also responsible for updating other portions of the global system state that were affected by the change. For example, the zone leader polls the blackboard services  periodically (inspects) to see if all the controller nodes  and physical nodes  that it expects to be in the state are present and accounted for by checking for the data objects corresponding to presence services . If the data object has disappeared, then the zone leader concludes that the controller node  or physical node  has failed and marks as \u201coffline\u201d the model or data object corresponding to this specific controller node or specific physical node. Alternatively, the zone leader could wait for a notification that the data object corresponding to presence service  has been deleted from the global system state and take action then, rather than constantly checking for changes. Waiting for notification about an event may be a more advantageous approach than polling, particularly as the system grows in size as well as the corresponding global system state because polling may consume unnecessary CPU cycles.","In election mode, orchestration service instances  run a second, standalone process, whose job may be to manage elections, which encompasses both electing and unelecting. Note that not every orchestration service instance requires presence service  to operate in both census and election modes. As explained earlier, if the system service is organized in a configuration with a single master and one or more slaves, then there will be presence service  of two processes to handle both the census and election modes.","In container , the election mode process in presence service - establishes a long-lived session to the blackboard service . Each election mode process works in conjunction with election mode processes on the other controller nodes  for a specific system service to ensure that if the service requires a single leader, then between themselves presence services  will elect a new leader. Further, the election mode processes ensure that there is always one leader, not two or three.",{"@attributes":{"id":"p-0165","num":"0164"},"figref":"FIG. 12A","b":["1200","1202","710","1204","902","1206","1208","902"]},"If there are other instances, at , the master presence service  of the orchestration service will trigger a health check and initiate election to differentiate system service  as either master or slave. At , during the election process, presence service  will find that another instance of the system service  already exists and is operating as master. Finding this to be the case, at , presence service  builds itself as a slave to the existing master if the master passes health checks and records its presence and operational state as a replicated slave in blackboard service .","Example Global System States","As described above with respect to blackboard service , the global system state of the distributed computing system is composed of the state of all controller nodes , containers , physical nodes , and the zone, among other things.  depicts an example of the global system state according to one embodiment. Orchestration service server instances , , and  and presence service instances  create the global system state as controller nodes , physical nodes , and containers  are started up. As physical nodes , controller nodes , and containers  fail over time and leave the zone, or as they return to service and join the zone, the global system state keeps track of this ever-changing situation; all the controller nodes and the zone leader detect changes in the system and maintain this state. The state, as described earlier, is recorded in the blackboard service , a global resource shared by all orchestration service instances , , and  and all presence services . Blackboard aptly describes its function; data is written, and viewers with appropriate permission can read the data nearly instantaneously and determine if changes have been made. Any interested (but authenticated) party may query the global system state to find out at a glance such things like which controller nodes  are operational, which physical nodes  are down, and whether a specific system service is online",{"@attributes":{"id":"p-0168","num":"0167"},"figref":"FIG. 12B","b":["102","107","804"]},"The path \/orchestration\/presence identifies all the presence service components for physical nodes , controller nodes , and containers . Every instance of presence service  whether in a controller node , physical node , or container , establishes a session to blackboard service  and creates an ephemeral data object on behalf of the controller node , physical node , or container  named respectively.  shows three examples of the presence state information registered on behalf of a controller node , a physical node , and a container  when presence service  is configured in census mode according to one embodiment. Specifically, presence state information for controller node 1, presence state information for physical node 2-00:8c:fa:10:b7:90 (Ethernet address) in controller node 2, and presence state information registered on behalf of container named 3-c2:7c:73:37:7e:61 (Ethernet address) on controller node 3. If a container , controller node , or physical node  fails, presence service  also fails, and consequently, the ephemeral data node in the blackboard service  is automatically deleted. If a client registered interest in the fate of this data node, blackboard service  would send a notification back to the client when it deleted the data node.","In , the containers label is a component in the path \/orchestration\/containers and identifies all the service containers  created in the distributed computing system that have both created their models and registered their existence in the blackboard service  under the containers label. The presence service  associated with the service container  is responsible for establishing a session to blackboard service  and creating an ephemeral data node representing the existence of the service container . If the container  fails, then the data node is automatically deleted from the \/orchestration\/containers data object, and thereafter is no longer part of the global system state. Containers  are the data object stored in blackboard service  as a child of the orchestration data object. As an example, \/orchestration\/containers\/dnspublic-1 identifies a specific service container  named dnspublic-1 for the system service called DNSPublic. In the distributed computing implementation, the name dnspublic-1 also identifies the unique name assigned to every controller node  starting with the number 1, and so in this case, the DNSPublic service container resides on controller node 1. This naming scheme can be seen in  for the other system service containers. Similarly, there is an instance of the service container for DNSPublic on controller node 2 and controller node 3, and the instances are named dnspublic-2 and dnspublic-3, respectively.  shows the state of a container data object in blackboard service  corresponding specifically to haproxy-1 residing on controller node 1. Some of the static attributes are \u201ccontroller_num\u201d (value 1), \u201cexternal_ip\u201d (10.13.56.8), and \u201ccontainer_name\u201d (haproxy). There are two dynamic attributes for the container \u201cstatus\u201d (online) and \u201cstate\u201d (running). Recall that these two dynamic attributes will be maintained by the orchestration service instance  in controller node .","The path \/orchestration\/controllers identifies all the controller nodes  that have registered both their \u201cmodels\u201d and their existence with blackboard service . Controller nodes  are named by a unique integer, starting at 1. There are three controller nodes  in the distributed computing system described by the global system state in . The presence service  associated with the controller node  is responsible for establishing a session to blackboard service  and creating an ephemeral data node representing controller node . If the controller node  fails, then the ephemeral data node is automatically deleted in blackboard service .  shows the state information specifically for controller node 2 given the path \/orchestration\/controllers\/2. The state information is stored directly in the data object labeled \u201c2.\u201d The data resembles a dictionary that associates a key like \u201cstatus\u201d with a value like \u201conline.\u201d In general, all state information for the distributed computing system is stored as a kind of dictionary. Further, \u201cip_addresses\u201d identifies the three logical networks to which the controller is attached, namely, \u201cdata_net,\u201d \u201cpxe_net,\u201d and \u201cmgmt_net.\u201d \u201cswitch_net\u201d is empty (null). The orchestration service instance and the associated presence services  communicate with each other over the management logical network identified by \u201cmgmt_net.\u201d The \u201cmac_adddress\u201d key identifies the Ethernet, or MAC, address of the network interface card on controller node .","The \/orchestration\/nodes path in blackboard service  identifies all physical nodes  that were operational at some point in their lifetime and have registered their \u201cmodel\u201d with blackboard service . Operational at some time is used because unless that physical node  is taken out of service permanently, it remains in the structure of the global system state as an entry; only its \u201cstate\u201d requires updating.  shows the physical node state for physical node named 1-00:8c:fa:10:b9:60 in controller node 1. The name of a physical node is a two-tuple, including the controller node number (an integer) and the Ethernet or MAC address associated with the physical node's network interface card: <controller#>-<Ethernet address>. Physical node  is associated with controller node 1 and an Ethernet address 00:8c:fa:10:b9:60 and so has the name 1-00:8c:fa:10:b9:60. Nearly all of this physical node state is static configuration data that will not change. There is a dynamic component indicated by \u201cstate,\u201d which shows whether physical node  is \u201conline\u201d or \u201coffline.\u201d If physical node  fails, then it is this \u201cstate\u201d attribute in the model that will be updated by the Zone leader to \u201coffline.\u201d","The \/orchestration\/zone path in blackboard service  includes the election and external_ips data objects. As described above, an orchestration service controller node has an elected leader and thus must participate in any election amongst the orchestration service controller nodes. Associated with the orchestration service controller node is presence service  configured in both census and election mode, the former to register existence and the latter to conduct the election if one is needed. Election mode uses the blackboard service to help in conducting an election. Specifically, the blackboard service does this by assigning monotonically increasing and non-overlapping sequence numbers (starting at 0) to the data objects as they are created. If three presence services  in election mode try to create the data object in blackboard service  to register its existence, the first to succeed in creating a data object has integer 0 assigned as part of the name of the data node, the second has integer 1 assigned as part of the name of the data node, and so on. The leader is the data object with the lowest assigned integer, in this case, 0, and so presence service  associated with that data node is deemed the \u201cleader.\u201d The other two presence services  in election mode \u201cwatch\u201d these data objects just in case the acknowledged leader dies (and the data object removed) and set in motion a new election.  shows the data objects for the orchestration service zone controller node as children in the path \/orchestration\/zone\/election in the blackboard service according to one embodiment. This method of electing a leader is a particular recommended example, but other ways may exist. The path\u2014\n\n","The \/orchestration\/zone\/external_ips data object contains an exhaustive and complete set of all the external IP addresses that can be assigned to controller nodes, physical nodes , and containers  in a specific distributed computing system. These IP addresses are divided into two sets, a set of reserved IP addresses in a set of available IP addresses. A reserved IP address is prefixed with the letter \u201cr\u201d, separated from the IP address by a hyphen. An available IP address is prefixed with the letter \u201ca\u201d with a hyphen separating them. In the following example the available IP addresses are shown in bold from amongst all the reserved IP addresses.","r-10.130.61.160, r-10.130.61.161, r-10.130.61.162, r-10.130.61.163, r-10.130.61.169, r-10.130.61.168,","r-10.130.61.165,","r-10.130.61.164, r-10.130.61.167, r-10.130.61.166, r-10.130.61.151, r-10.130.61.152, r-10.130.61.150,","r-10.130.61.159,","r-10.130.61.158, r-10.130.61.157, r-10.130.61.156, r-10.130.61.155, r-10.130.61.154, r-10.130.61.153,","r-10.130.61.180,","r-10.130.61.181, r-10.130.61.182, r-10.130.61.183, r-10.130.61.184, r-10.130.61.185, r-10.130.61.187,","r-10.130.61.186,","r-10.130.61.189, r-10.130.61.188, a-10.130.61.254, a-10.130.61.253, a-10.130.61.255, r-10.130.61.10,","r-10.130.61.11,","a-10.130.61.9, r-10.130.61.170, r-10.130.61.173, r-10.130.61.174, r-10.130.61.171, r-10.130.61.172, r-10.130.61.178,",". . .","The path \/orchestration\/services names all the system services that run on the controller node , not those that run on physical nodes . They include services named nova, haproxy, pxe, dnspublic, glance, stats, rabbitmq, keystone, logging, novautils, dashboard, and mysql. For those services that are organized in a master-slave relationship, there must be an election to pick a leader. This is implemented using blackboard service  and the same recipe for selecting a leader, namely, the lowest numbered integer.  shows the path for two of the services dnspublic and mysql, each ending in the election component. Below that component in the path is another data object that corresponds to presence service  (in election mode) associated with an instance of that service.","In summary, the orchestration service instance on behalf of some controller node , physical node , or container  creates a corresponding configuration model as a data object in blackboard service  when that system component is created. This model is represented as a kind of dictionary that maps keys to values and is the state of the system component. It has a configuration portion that is static and unchanging, and has a dynamic component that may change during the lifetime of the service. When a presence service  instance in census mode registers its existence in the blackboard service, it creates an ephemeral data object corresponding to a controller node , container , or physical node . If that presence service  fails or the associated orchestration service instance fails (or the controller node , container , or physical node ), then the data object will be automatically deleted in blackboard service . Since the state of that service has now changed, it is the job of the zone leader to determine what has changed by periodically inspecting the global system state and updating the corresponding models. It is the collection of orchestration service instances and the associated presence services  (in census mode or in election mode, or both), in conjunction with the zone leader, that maintains the global system state for a running distributed computing system. The global system state is a view into the \u201chealth\u201d of the distributed computing system, enabling a whole host of actions, such as the following: potentially faster recovery from failures, isolating parts of the system that may be offering degraded performance, better capacity planning, and more effective use of system resources.","Failure Recovery","In one example embodiment, a distributed computing system copes with failures of a physical node , a controller node , or a service container  by detecting failures and by automatically restoring service functionality. The orchestration service instance functions may include keeping the system infrastructure running despite failures. A separate sub-component of the orchestration service operates to detect and report presence data by registering its controller node existence in the global state. Orchestration service controller nodes  periodically probe the shared blackboard service  to detect changes in the dynamic attributes of all service containers . In addition, an orchestration service zone controller node periodically inspects all controller nodes and physical nodes. Changes are detected by noting the presence or absence of the existence registration. When changes are detected, controller node  updates status information for the affected records in the shared blackboard service . If a problem occurs, action is taken appropriate to the affected service, such as restarting a service or electing a new master.","Failures may occur in the distributed computing system. However, due to the distributed nature, portions of the distributed computing system may continue to operate. That is, the distributed computing system may be partially operational and partially failed at the same time. As described above, controller node  includes containers  that isolate services  from other services  operating in other containers  on controller node . The containerization of services  is required as the software is designed to operate across machine boundaries. The distributed computing system expects dedicated file systems, process name space, and network stacks to be available for communication with other discrete components each with their own file system, network stack, and process name space. Each service  in a container  provides an aspect of the overall service being provided by the distributed computing system, but operates independently without sharing elements such that containers  may be easily replaced by another container  designed to perform the same service. Particular embodiments leverage this concept to recover from failures quickly.",{"@attributes":{"id":"p-0191","num":"0193"},"figref":"FIG. 13","b":["107","708","802","107","802","709","706","708","706","706","708","708","708","802","706"]},"In the process flow, in a step 1 (reference ), orchestration service instance  detects a failure of container . In one embodiment, orchestration service instance  may detect the failure by monitoring blackboard service . For example, as discussed above, presence service  may lose its session to blackboard service  when service  fails. This may cause the removal of state information for service  in blackboard service . Orchestration service instance  may detect the change in the status on blackboard service . In this case, service  in container  does not directly notify orchestration service instance  of the failure. This simplifies the communication of failures in the distributed computing system as orchestration service instance  can monitor from a central point whether failures are occurring.","In a step 2 (reference ), orchestration service instance  terminates service container . Instead of troubleshooting the failure and attempting to continue using service  in container , orchestration service instance  terminates the container. By not troubleshooting the failure, speed in recovering from the failure may be gained as will be discussed in more detail below.","In a step 3 (reference ), orchestration service instance  determines a last-known good state for service . For example, because operating system-level virtualization or containerization is used such that various services  are isolated from other services  and also other components, such as main computer , using process name space partitioning and independent or otherwise isolated network stacks, the last-known good state of service  can be determined. In one example, a copy on write scheme is used where a file system for container  is frozen and service  in container  operates from this frozen file system image, recording deviations from the frozen file system in the course of operating the service. The frozen file system may constitute the last known good state of service  and is a full image needed to restart the service from scratch. Since the changes have not been written to the known good state of service , orchestration service instance  can use this last-known good state with confidence that it will not fail.","In a step 4 (reference ), orchestration service instance  restarts a new service container  using the last known good state of service . New service container  includes orchestration service instance  and service . However, the differences from the file system of the last known good state have been discarded and service  in new service container  begins anew from the known good state. This may remove any problem that occurred while operating from the last known good state. This method of failure recovery is faster than recovering from the failure. Because of the isolation of services  in containers , orchestration service instance  can terminate a failed container  and restart a new container  very quickly.","In a step 5 (reference ), service  in new container  may recover state data and configuration data for service . The state data and configuration data may be found in blackboard service , persistent storage , or other local storage for container .","In a distributed computing system, failures are inevitable. However, due to the speed and knowledge that services  will be started from a known good state, the distributed computing system may reliably perform in light of failures. The failure recovery leverages operating system-level virtualization, storage of configuration and state data outside of container , using a copy-on-write approach for the file system of container  to recover from the failure and allow for a new container  to be used when a failure occurs. This allows the recovery from a failure from a broad array of known and unknown failures. The distributed computing system does not need to determine the failure state, the path of that state, and a remediation from the failure. Rather, the failure is recovered from by rapidly reverting to a known good state.","Entropy Generation",{"@attributes":{"id":"p-0198","num":"0200"},"figref":"FIG. 14","b":["102","712"]},"Accordingly, particular embodiments provide high-quality entropy throughout the distributed computing system. In one embodiment, a true random number generator is used to generate entropy. The true random number generator generates random numbers from a physical process rather than from a computer program. The random numbers provided by the true random number generator may be completely unpredictable and thus reliable. The true random number generator may be a hardware random number generator.","As shown, management computer  includes a true random number generator . True random number generator  may be included in management computer  as an independent hardware platform separate and apart from main computer . True random number generator  generates the random numbers as entropy in a time-based manner via a hardware process. Then, management computer  sends the entropy to main computer . Management computer  may communicate the entropy through a communication network using a protocol, such as transfer control protocol\/internet protocol (TCP\/IP), UNIX sockets, UNIX devices, or combinations thereof. Main computer  may communicate the combined entropy via a communication network through protocols as discussed above, such as through TCP\/IP.","To provide additional entropy, a pseudo-random software entropy generator  is used to add entropy to be combined with the entropy received from true random number generator . By using the true random number generator and the software random number generator, a larger amount of entropy may be generated, but may be reliable in that the true random number generator is generating a part of the entropy. Other sources may also contribute to the entropy. In contrast to using just pseudo-random software entropy generator , using true random number generator  in management computer  provides more reliable entropy and more entropy than can be generated by pseudo-random software entropy generator . That is, true random number generator  may be able to generate entropy faster than pseudo-random software entropy generator . Also, true random number generator  generates higher quality and more reliable entropy than pseudo-random software entropy generator  resulting in superior cryptographic functions. Further, hardware resources of main computer  do not need to be used to generate entropy when the hardware entropy is generated by management computer . Rather, as discussed above, main computer  operates containers  that include services , and services  control services  that are operating on physical nodes . Resources for these operations may not be used in using a hardware generator in main computer .","Main computer  then sends the entropy to physical nodes  in which controller node  is controlling. For example, physical nodes  may be located in the same rack as controller node . In other embodiments, main computer  may send the entropy to other physical nodes  in other racks.","Each physical node  may receive the entropy. For example, a hypervisor  within physical node  may receive the entropy. Multiple virtual machines  may be running on top of hypervisor . Each virtual machine  may be running a service  in addition to an orchestration service instance . Service  may require the entropy for performing certain operations, such as for cryptography operations.","To provide the entropy from hypervisor  to virtual machine , hypervisor  may provide an emulated entropy device . Emulated entropy device  may be a virtual device that is stored in an address space. To read the entropy, virtual machine  includes an entropy driver  that knows the address where to read the entropy from entropy device . When entropy is needed, entropy driver  retrieves entropy from entropy device . In this case, hypervisor  may retrieve the entropy from main computer , present the entropy to entropy device , and then entropy driver  retrieves the entropy from entropy device . The above process may be performed in each physical node  where a hypervisor  provides entropy retrieved from main computer  of controller node  to virtual machines . Due to the large amount of entropy provided using management computer  and main computer , it is possible to have a distributed computing environment that can on demand expand the number of virtual machines  without exhausting the entropy. The use of true random number generator  in management computer  allows the distributed computing system to generate the large amount of entropy.","The providing of a large amount of entropy is important in the distributed computing system because there is potential for great variance and demand for entropy. Some operational states of the distributed computing system may be so virtual machine turnover, that is, the creation and destruction of virtual machines , while other states may see exceptionally high turnover. When high turnover results, the need for entropy may increase dramatically. The distributed computing system can handle the high turnover using the generation of entropy via true random number generator  in management computer . The entropy provided via controller node  to physical nodes  allows the creation of virtual machines  on physical nodes . By leveraging management computer  to generate the entropy, the number of components in the distributed computing system is reduced as physical nodes  do not need to generate entropy. There may be hundreds of physical nodes , and having each one have to have a true random number generator increases complexity. Instead, management computer  serves as a true random number generator for a collection of physical nodes  attached to a single controller node .","Example Service Specific Behavior","In one example embodiment, physical nodes  each of which exhibits a service-specific behavior or personality. These personalities are captured in function definitions, which in this example may be referred to as \u201cmelodies.\u201d The function definitions may manage the service configuration, monitor the health of the associated system service, controller or node, and\/or react to changes in the health status, and cope with failures in the system, for example.","In certain example embodiments, each orchestration service instance , , and  is configured to be service-specific and is not just a single, monolithic entity. What functions a particular orchestration service instance , , and  might perform may depend on the associated system service. For example, the orchestration service instance  associated with a MySQL server service in a container  is configured to check the health of the MySQL server, elect a new master, periodically back up the database into a file, determine the virtual IP address of the MySQL Server, or initialize a new MySQL slave, among many other functions.","Service-specific behavior of an example orchestration service instance , , and  may be referred to as a \u201cpersonality.\u201d For example, there may be a personality for the orchestration service instance  residing on a physical node , which is configured to manage the system services  on physical node , varying personalities for the orchestration service instance  residing in a container  that is specific to the system service , and there may be a personality for the orchestration service instance  running in a controller node . A controller node instance of an orchestration service instance  may have a very different personality from the orchestration service instance  in a container  and the orchestration service instance  on the physical node  because the controller node instance manages all the containers  for system services on controller node , for example.","In this example, orchestration service instances , , and  capture this notion of a personality in certain function definitions. Each orchestration service instance , , and  is configured at runtime with its specific personality by loading specific modules that correspond to a particular function definition. In one example implementation of the distributed computing system, these modules may be Python programs. In one example embodiment, there may be six such modules making up the function definition.  shows some examples of an orchestration service instance , , or  configured with service specific personalities according to one embodiment. The following are descriptions of example function definitions:","1. Phrases: A phrase is a recipe for the specific business logic for the service, such as installing a MySQL server in a container, obtaining the revocation status for a MySQL rack, or managing the MySQL rack.","2. Periodics: A periodic is a recurring task such as backing up a MySQL database every two minutes, or managing the MySQL rack every five minutes. A periodic may be specific to a system service.","3. Tasks: A task is typically a function performed by an orchestration service instance , , and  (e.g., in the background) and may be invoked from phrases or directly from routes. For example, in the MySQL service container , a common task is to install the service container by spitting up an instance of the MySQL server, or initializing a slave replica in a MySQL rack (such as when a new slave replica is created on the new controller that has joined the distributed computing zone).\n\n4. Probes: A probe is typically a query to discover some status information about the service in question. As an example of a probe, in MySQL, the probe might ask which MySQL server replica has the floating (or virtual) IP address associated with it.\n\n5. Routes: A route may be an endpoint (e.g., an API endpoint typically using the HTTP protocol) for accessing a function definition. For example, if a user wants to know the status of a presumably operational MySQL service container , particular embodiments invoke the \u201cget_status\u201d route against the API of the associated orchestration service instance, which may invoke the GET operation given a URL that describes the route.\n\n6. Election: An election function group is configured only for those system services that are organized in a master-slave relationship, such as MySQL system service. Other system services, such as Identity or Compute, are organized as peers, and do not require an election. As described above, an election function definition comprises \u201celect\u201d and \u201cunelect\u201d functions, which are called by the presence service in the present example.\n",{"@attributes":{"id":"p-0211","num":"0213"},"figref":["FIG. 15","FIG. 16"],"b":["708","709","712","709","107","709","709","708","107","708","107","712"]},"Example Service Implementation","In one embodiment, the orchestration service is implemented as a large web-based application.  illustrates the components that make up one implementation of the orchestration service instance , , or  according to one embodiment. A front-end component  fields all orchestration service API operations and may handle multiple, simultaneous operations. A backend component  executes all background tasks and periodic tasks, drawn from a queue of tasks  submitted by front-end component . Since the end state results of background tasks and periodic tasks are typically transient, they are recorded on a persistent data store  associated with the orchestration service instance. The present example orchestration service server instance is not a single component or even a single process, but a collection of processes that work in concert.","Front-end component  includes an orchestration service API  and two separate processes. The methods available on orchestration service API  vary depending on whether the API fronts the orchestration service instance for a controller node , a container , or a physical node . There is a core set of methods common across controller nodes , containers , and physical nodes . For example, installing a system service  in a container  and retrieving the status of a task are examples of common methods not peculiar to a personality.  shows these two methods among others entering OSI API . In this example, these methods are invoked using the HTTP protocol.","A first server  may act as an HTTP server and reverse proxy server. The first server is one of the two separate processes making up the front-end. A reverse proxy server is a type of proxy server that retrieves resources on behalf of the client from one or more servers. These resources are then returned to the client as though they originated from the proxy server self. The idea of a reverse proxy is that it can hide the existence and the characteristics of the originating servers. The orchestration service API methods are, in this example, HTTP requests using a well-defined URL and HTTP operation such as put and get. A web server fields these HTTP requests and passes the requests onto the next layer. Any responses to these HTTP requests are returned to the client that invoked the API method, so the client is does not know that the method was actually executed by a collection of processes hiding behind the API.","A second management process is the second of the two separate processes making up the front end. Though a reverse proxy server fields orchestration service API methods as HTTP requests, it does not itself handle multiple simultaneous HTTP requests. The second management process may be a server that manages a dynamic set of worker processes that execute the individual HTTP requests and responses passed to it from a reverse proxy server. In this example, the second management process is implemented using web server gateway interface server . The second management process may be the set of worker processes that can execute HTTP requests concurrently. Further, as part of an orchestration service instance, the web server gateway interface process  is loaded with at least three of the components of a service-specific function group, which are the HTTP endpoints for orchestration service: routes, tasks, and probes. In the figure, this is shown as a box labeled \u201corchestration service (routes, tasks, probes)\u201d under the web server gateway interface process . Each HTTP request is not necessarily executed immediately. Some, like asking for the status of the system service may be executed immediately as a synchronous, or blocking call, since a timely answer is demanded. Others, like initializing a MySQL slave replica, are potentially time-consuming tasks, for which the client making the request may not have the patience to wait for such blocking invocations. These sorts of requests are usually executed asynchronously as background tasks. Though web server gateway interface  can accommodate applications making long blocking calls or streaming requests and responses asynchronously, an orchestration service instance may instead employ a separate task queue . In one example implementation, each task is a program written in Python. The web server gateway interface process  submits the HTTP requests as individual tasks to a task queue service .","Task queue service  is a message broker. It accepts and forwards messages (e.g., as a post office eventually delivers mail to a recipient on behalf of a sender). Each task submitted to the message broker from the web server gateway interface  process is assigned a unique task ID and queued for eventual execution. Task queue service  does not execute any of the submitted tasks, instead, that function is assigned to a background worker process. Task queue  is shared across all other orchestration service instances running on a controller node , that is, it is a controller-wide service. Since any task can originate from any controller node or any container , each task must be uniquely identified across all controller nodes  and all containers  to avoid conflicts in naming a task. The task ID may be a 128-bit UUID, which is highly likely to be unique (and not clash with other UUIDs) across all controller nodes  over a long period of time. The tasks stored in the task queue  may not be persistent; therefore, the tasks may not survive failure of either the task queue process itself or the controller node . Clients may need to reissue their API operations when the task queue returns to service.","The other part of the task queue service  is implemented by worker processes each of which may be run in the background. Background workers comprise a set of worker processes, each of which is usually a daemon run in the background. Each worker process dequeues the next task from the task queue and operates on it. This is shown in the figure by the \u201cbackground worker processes\u201d  operating on the first three tasks in the queue. Other than reporting success or failure, a worker process may record any end state results of the task in a persistent store  such as a key-value store. There is a single persistent storage server process for each orchestration service instance. Each end state result for an executed task is associated with a task ID such as [taskID, endState]. As long as the task ID is known the end state results can be retrieved given the task ID, as can be seen in  where one of the core set of orchestration service API methods is obtaining the status of a task. In this example, the only data that is persistent is the end state result of an executed task. The data manipulated in memory by a worker process executing a task is not persistent; if the process fails before it has completed executing the task then any data it was operating on may be lost and presumably the uncompleted task has no effect on the state of the system. The task may need to be resubmitted by the client.","Some embodiments may process a recurring task, for example, a task that must be executed every two minutes. To implement recurring tasks, a time-based task scheduler  executes jobs (commands or shell scripts) to run periodically at certain times or dates (e.g., similar to cron in UNIX-like computer operating systems). This example includes another independent process that kicks off tasks at regular intervals, which are not executed directly by the background worker processes, but first are submitted as tasks to the task queue service  and thereafter are treated just like any other task that has been enqueued. There may be one such process per orchestration service instance, for example. The scheduled tasks may be defined in the periodics and tasks of the associated function group for the system service. An example of a periodic comes from MySQL where each MySQL server replica is either backed up via a dump or has fetched a backup from the master.","Finally, another purpose of a function group is the \u201celection,\u201d which is mandatory in this example implementation for all system services organized in a master-slave configuration and optional for all other system services. When the associated presence service process discovers that a master for some system service has failed\u2014it determines failure when the ephemeral node it was watching is no longer present in blackboard service \u2014it \u201celects\u201d a new master by invoking the elect method of the election function group associated with the orchestration service instance. There is a corresponding unelect method in the function group. These two methods are shown in an election process , which corresponds to the election in the function group. The election process includes the \u201celect\u201d path and the \u201cunelect\u201d path. The election may use a backdoor to an orchestration service instance. In this example, the \u201celection\u201d part of a function group directly executes the elect and unelect functions.","Note that presence service  does not itself elect a new master, but merely informs the associated orchestration service instance that it should schedule the election of a new master. The orchestration service instance code delegates leader election to the blackboard service ; the blackboard service  already provides a recipe to correctly implement leader election, ensure that a leader will be elected and ensure that there will be exactly one leader. In addition, the elect method may also perform some service-specific functions when the election complete such as clean up and initialization or, in the case of MySQL, asserting its mastership and reconfiguring other replicas to be slaves.","Particular embodiments may be implemented in a non-transitory computer-readable storage medium for use by or in connection with the instruction execution system, apparatus, system, or machine. The computer-readable storage medium contains instructions for controlling a computer system to perform a method described by particular embodiments. The computer system may include one or more computing devices. The instructions, when executed by one or more computer processors, may be operable to perform that which is described in particular embodiments.","As used in the description herein and throughout the claims that follow, \u201ca\u201d, \u201can\u201d, and \u201cthe\u201d includes plural references unless the context clearly dictates otherwise. Also, as used in the description herein and throughout the claims that follow, the meaning of \u201cin\u201d includes \u201cin\u201d and \u201con\u201d unless the context clearly dictates otherwise.","The above description illustrates various embodiments along with examples of how aspects of particular embodiments may be implemented. The above examples and embodiments should not be deemed to be the only embodiments, and are presented to illustrate the flexibility and advantages of particular embodiments as defined by the following claims. Based on the above disclosure and the following claims, other arrangements, embodiments, implementations and equivalents may be employed without departing from the scope hereof as defined by the claims."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 2A"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 2B"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 12A"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 12B","b":"107"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 12C"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 12D"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 12E"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 12F"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 17"}]},"DETDESC":[{},{}]}
