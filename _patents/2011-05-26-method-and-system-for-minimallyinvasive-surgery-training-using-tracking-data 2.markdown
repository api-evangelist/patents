---
title: Method and system for minimally-invasive surgery training using tracking data
abstract: A system and method for training a person in minimally-invasive surgery (“MIS”) utilizing a video of the MIS. The system comprises a processor, a display, and a first interaction device. The processor programmed to receive the video and to obtain tracking data. The tracking data may correspond to the motion of a tool controller. The tracking data may correspond to motion of a first surgical tool in the video. The processor programmed to calculate motion of the first interaction device corresponding to the tracking data, to display the video, and to cause the first interaction device to move according to the calculated motion. The method comprises receiving the video, obtaining the tracking data, calculating a motion of a first interaction device corresponding to the tracking data, displaying the video, and causing the first interaction device to move according to the calculated motion.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09595207&OS=09595207&RS=09595207
owner: The Research Foundation for The State University of New York
number: 09595207
owner_city: Amherst
owner_country: US
publication_date: 20110526
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATION","FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","BRIEF SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION"],"p":["This application claims the benefit of priority to U.S. provisional patent application Ser. No. 61\/348,732, filed May 26, 2010, the disclosure of which is incorporated herein by reference.","The invention relates to surgical training, and more particularly to training a person in performing minimally-invasive surgical procedures.","Minimally invasive surgery (\u201cMIS\u201d) has been accepted as a useful alternative to open surgery for many health conditions. While safer for the patient, MIS poses a number of unique challenges to the surgeon performing them. The challenges fall into two broad domains: (i) the cognitive domain, wherein the surgeon uses knowledge and prior experience to make decisions regarding the procedure; and (ii) the motor control domain, where the surgeon uses physical skills to carry out specific decisions made through their cognitive process. For example, in laparoscopic surgery, a type of MIS, the surgery is conducted through small incisions made in the thorax or the abdomen of the body. Since the surgery takes place inside the closed volume of the human body, a small flexible camera called an endoscope is inserted inside the body to provide visual feedback. This set up gives rise to a number of cognitive challenges that make this form of surgery especially challenging, including:","(1) lack of visual feedback\u2014the visual feedback is provided by images captured through the endoscope and displayed on a screen, lacking depth information;","(2) poor image quality\u2014since the procedure is carried out within closed body cavities, the images received from the endoscope are affected by a number of factors, including improper lighting, smoke from cauterization of tissue and lensing effects;","(3) landmarks\u2014Unlike open surgery, anatomical landmarks are not readily discernible and it is difficult to get oriented and navigate correctly inside the body without making mistakes; and","(4) patient differences\u2014pathology and individual variations in physiology create visual differences in two bodies, this effect is amplified in MIS.","Some ramifications of the above described problems result in making the cognitive process of the surgeons exceedingly difficult. It is for the same reasons residents require extensive training with a number of procedures before they can graduate to performing surgery on their own.","Currently available simulators may train surgical residents for motor skill improvement. However, the current training methods do not adequately address the issue of improving the cognitive ability of the resident. Therefore, a resident typically gets acquainted with identifying anatomical landmarks by watching actual surgeries and training under a surgeon. This makes the learning curve slow, difficult, and expensive.","Accordingly, there is a need for an MIS training method and system that better prepares the operator by improving both the motor skills and the cognitive skills of the trainer.","The currently disclosed training method and simulator may be used to enhance a trainee's motor learning while also teaching the steps of a surgical procedure. A method and system according to the present invention allows a trainee to watch a video of an MIS procedure while the trainees hand(s) are guided by an interaction device so that the trainee's hands moves in the same way as the hands of the surgeon who performed the surgery.","The present invention may be embodied as a system for training a person to perform minimally-invasive surgery (\u201cMIS\u201d). The training utilizes a video of an MIS. The system comprises a processor. The system has a display in communication with the processor. The system further comprises a first interaction device in communication with the processor. The processor is programmed to receive the video of the MIS. The processor is also programmed to obtain the tracking data. The tracking data may correspond to the motion of a tool controller controlled by a surgeon who performed the MIS of the video. In another embodiment, the tracking data may correspond to motion of the first surgical tool in the video. The processor may obtain the tracking data by processing the video to calculate motion of the first surgical tool.","The processor is programmed to calculate motion of the first interaction device corresponding to the tracking data. The processor is programmed to display the video on the display. The processor is programmed to cause the first interaction device to move according to the calculated motion. In this way, a trainee grasping or observing the first interaction device may experience motor learning. The system may have a second interaction device.","The present invention may be embodied as a method comprising the steps of receiving the video, obtaining the tracking data, calculating a motion of a first interaction device corresponding to the tracking data, displaying the video on the display, and causing the first interaction device to move according to the calculated motion.","The present invention may be embodied as a computer program for performing any of the methods described herein. The computer program may be embodied on a computer readable medium. The computer readable medium may be a computer network or a storage location (e.g., server, NAS, SAN, etc) attached to a computer network.","The present invention may be embodied as a system  for training a person to perform minimally-invasive surgery (\u201cMIS\u201d). The training utilizes a video of an MIS performed by a surgeon. In embodiments of the invention, the video is not part of the system , but is captured separately from the system . The video may be pre-recorded on storage media (e.g., tape, disc, hard-drive, flash drive, etc.) as is known in the art, or the video may be a live video of a surgery being performed in real time. The video comprises a plurality of frames, where each frame is an image at a point in time. The video may be a stereoscopic video, captured from two points-of-view in fixed relation to each other. In an example, the video may show a prostatectomy using a da Vinci\u00ae Surgical System (\u201cDVSS\u201d), where one of the robot's tools is visible. Such tools may include, but are not limited to, a scalpel, scissors, or bovie. In another example, the video may show a conventional (non-robotic) laparoscopic procedure. Other videos of suitable MIS procedures will be apparent to those having skill in the art.","In the video, a first surgical tool is visible. The first surgical tool may be performing tasks such as, for example, grasping, suturing, cauterizing, etc. The first surgical tool is operated by the surgeon using a first tool controller. Such a first tool controller may be, in a non-limiting example, the master of a DVSS. Here again, the first tool controller is not a part of the system , but is used separately during the surgery shown in the video. In some embodiments of the present invention, motion of the first tool controller is captured as tracking data. The motion includes, in a non-limiting example, position, orientation, and end-effector operation. In a particular example, the motion captured by tracking data includes the position and orientation of each joint and\/or segment of the tool controller over time. In another example, the motion includes the position and orientation of the distal end of the tool controller over time. Other forms of capturing the motion of the tool controller as tracking data will be apparent to those having skill in the art. In such embodiments, the first tool controller motion is captured over a duration of time corresponding to the time of the video. In this way, the tracking data is synchronized with the video. As such, the movement of the surgeon's hand may be captured vis-\u00e0-vis the tracking data of the motion of the first tool controller. In such embodiments, capturing the motion of the tool controller as tracking data does not necessarily make up a part of the present system or method.","The system  comprises a processor . The processor may be, for example, a computer or a controller. The system  has a display  in communication with the processor . The processor  is capable of displaying the video on the display . The display  may be an LCD monitor commonly used with computers, a head-mounted (wearable) display, a projector, or any type of device capable of displaying a video to a person.","In the embodiment where the video is a stereoscopic video, the display  may also be a stereoscopic display capable of showing each view of the video separately to each eye of the trainee. In this way, a three-dimensional representation is reconstructed from two two-dimensional images\/videos. This type of three-dimensional construction is often referred to as 2.5-dimensional (two-and-a-half dimensional). Three dimensional and 2.5 dimensional may be used interchangeably in this disclosure. A stereoscopic display may have two display devices, or may make use of wearable glasses configured to cause each view of the video to be displayed to each eye of the wearer. Other stereoscopic display technologies are known in the art and may be used as part of the present invention.","The system  further comprises a first interaction device  in communication with the processor . The first interaction device  may be, for example, a PHANTOM\u00ae Omni\u00ae device. The first interaction device  is capable of motion according to instructions received by the first interaction device  from the processor . For the purposes of this disclosure, \u201cmovement\u201d or \u201cmotion\u201d means that the first interaction device  is capable of independent motion unguided by a user and\/or \u201cmovement\u201d or \u201cmotion\u201d means that the first interaction device  is capable of being moved by a user and the first interaction device guiding (moving) the user's hand (i.e., restricting the user's motion) through haptic feedback.","The processor  is programmed to receive the video of the MIS. As such, the processor  may receive the video from a storage location in a memory device, from a network location, or elsewhere. The video may be received as a live stream from a network location or pre-recorded and stored. The video may be of any format capable of being processed by a processor as is known in the art. For example, the video may be an AVI, MPEG, WMV, etc. The processor  may receive the video by first requesting the video from another device, by receiving the video sent without a request, or otherwise.","The processor  is also programmed to obtain the tracking data. In the aforementioned embodiment where the tracking data corresponds to the motion of the tool controller, the processor  may obtain the tracking data by receiving the tracking data. The processor  may receive the video by first requesting the video from another device, by receiving a video sent without a request, or otherwise. The tracking data may be co-located with the video or in another location.","In another embodiment of the present invention, the tracking data may correspond to motion of the first surgical tool in the video. Tracking data corresponding to motion of the first surgical tool may be determined in a manual process by a person viewing the video and recording the motion of the first surgical tool (e.g., position of the end-effector, or other representative motion) as tracking data. The person may record this as, for example, a data file. In such embodiments, the tracking data may be obtained by the processor  by receiving the tracking data.","In another embodiment of the present invention where the tracking data corresponds to motion of the first surgical tool in the video, the processor  may obtain the tracking data by processing the video to calculate motion of the first surgical tool. This process is further described below (\u201cAutomated Tool Position Determination\u201d).","The processor  is programmed to calculate motion of the first interaction device  corresponding to the tracking data. In embodiments where the tracking data corresponds to the motion of the first tool controller, such tool controller motion is mapped to a corresponding motion of the first interaction device . The mapping parameters are pre-determined and further described in an exemplary embodiment (\u201cWorkspace Mapping\u201d below). The mapping parameters may be created such that the motion of the first tool controller has any relationship to the motion of the first interaction device . In a preferred embodiment, the mapping parameters are created such that movement of the first interaction device  causes a hand of a trainee grasping the first interaction device  to re-create the movement of the surgeon's hand (moving the first tool controller). In this way, the trainee may experience motor learning.","In embodiments where the tracking data corresponds to the calculated motion of the first surgical tool, such surgical tool motion is mapped to a corresponding motion of the first interaction device . The mapping parameters are pre-determined and further described in an exemplary embodiment (\u201cInverse Kinematics Mapping\u201d below).","The processor  is programmed to display  the video of the MIS on the display. The processor  is programmed to cause the first interaction device  to move according to the calculated motion. The motion of the first interaction device  may be such that the first interaction device  moves affirmatively (the first interaction device  moves on its own, such as with a robotic arm). Alternatively, the motion of the first interaction device  may take the form of restriction of movements of a trainee (user) by way of haptic feedback. In this way, a trainee grasping the first interaction device  will be restricted to moving the first interaction device  in the appropriate motion.","A signal provided by the processor  to the first interaction device  may cause the first interaction device  to create the movements necessary to accomplish (mimic) the tool controller movement. For example, if the tracking data captured movement of the first controller from left to right (and the corresponding left-to-right movement of the first surgical tool in the video), the processor  would provide a signal to the first interaction device  moving the first interaction device  from left to right. In this way, a user of the system , holding the first interaction device , would feel the first interaction device  moving as needed to create the surgical tool movements on the display (and moving the users hand in the same way). The necessary signal may be calculated based on the tracking data captured during the original MIS, or calculated based on the first surgical tool position in the video.","The master console of an actual surgical robot or a simulator may be used as the system  of the present invention to \u201cplayback\u201d a surgical procedure through video and interaction device movement allowing a trainee to see (through the video on the display) and feel (through the movement of the interaction device) the surgical procedure. This technique could be used at a tandem console during a live surgical procedure. In a non-limiting example, while a surgeon is performing an MIS, a tandem console may be used as a system  of the present invention to recreate the surgeon's movements for a student to see and feel.","In another embodiment, the system  has a second interaction device . For example, in systems intended to train a trainee to perform robot-assisted surgery, a second interaction device is used to emulate a second tool controller of the surgical robot master. In this way, a surgeon uses two hands to control two tool controllers. One example of a suitable system , the Robotic Surgical Simulator (\u201cRoSS\u2122\u201d) from Simulated Surgical Systems LLC, is depicted in , although it should be understood that other simulators may be used. Additional interaction devices may be used as necessary to accommodate additional tool controllers.","In another embodiment, the video and tracking data is of a surgeon performing a procedure using a simulator. In such embodiments, a trainer may use a simulator to perform procedures and establish training routines for a trainee to replay. The trainer may perform simulated surgeries or other training tasks, such as, but not limited to, so-called pick-and-place tasks. In another alternative, the trainer may be using a simulator at the same time as a trainee is using a system of the present invention.","Force Feedback","In embodiments where the tracking data corresponds to motion of the tool controller(s) by the surgeon, forces encountered by the actual surgical tools during the MIS may be captured within the tracking data and synchronized to the video. Such force data may be utilized by a system of the present invention to playback the forces through the haptic feedback of the interaction device. In this way, tissue structures and other sensation may be experienced in the virtual surgical space using the interaction device. For example, when the MIS is performed with a surgical robot, forces encountered by the robot slave may be recorded at the joints of the slave. Such haptic feedback is considered within the meaning of movement (see above, restricting the movement of a user's hand), and within the scope of the present invention.","Visual Feedback","In embodiments of the present invention, the first interaction device  has the same number of degrees of freedom as the first tool controller. In other embodiments, the first interaction device  has fewer degrees of freedom than the first tool controller. In still other embodiments, the first interaction device  may have degrees of freedom not capable of inducing a force. Using a non-limiting example of a simple joystick as an interaction device, the joystick may be capable of three degrees of freedom\u2014two translation (x and y) and one rotational (rotating the joystick along its longitudinal axis). Such a joystick may be capable of inducing a force in the translational degrees of freedom (i.e., the joystick may be able to move up, down, left, and right independent of a user). However, the joystick may not be capable of inducing a force to rotate about the longitudinal axis. In such embodiments, where the first interaction device  has fewer degrees of freedom or degrees of freedom not capable of inducing a force, cues may be presented to the trainee to move the first interaction device  in an appropriate way. Such cues may be visual, audible, tactile, or any other mode capable of prompting the trainee. Using the joystick example above, the trainee can be prompted with a visual cue, such as an arrow on the display, to rotate the joystick about its longitudinal axis. , depict another example involving an MIS simulator where the trainee is prompted by an arrow  to rotate the first interaction device  appropriately.","A method  according to an embodiment of the present invention is used to train an individual in performing MIS. The training utilizes a video of an MIS performed by a surgeon. In embodiments of the invention, the video itself and capturing the video are not a part of the method. The video may be pre-recorded on storage media (e.g., tape, disc, hard-drive, flash drive, etc.) as is known in the art, or the video may be a live video of a surgery being performed in real time. The video comprises a plurality of frames, where each frame is an image at a point in time. The video may be a stereoscopic video, captured from two points-of-view in fixed relation to each other. In an example, the video may show a prostatectomy using a DVSS, where one of the robot's tools is visible. Such tools may include, but are not limited to, a scalpel, scissors, or bovie. In another example, the video may show a conventional (non-robotic) laparoscopic procedure. Other videos of suitable MIS procedures will be apparent to those having skill in the art.","In the video, a first surgical tool is visible. The first surgical tool may be performing tasks such as, for example, grasping, suturing, cauterizing, etc. The first surgical tool is operated by the surgeon using a first tool controller. Such a first tool controller may be, in a non-limiting example, the master of a DVSS. Here again, use of the first tool controller is not a step of the method, but is used separately during the surgery shown in the video. In some embodiments of the present invention, motion of the first tool controller is captured as tracking data. The motion includes, in a non-limiting example, position, orientation, and end-effector operation. In such embodiments, the first tool controller motion is captured over a duration of time corresponding to the time of the video. In this way, the tracking data is synchronized with the video. As such, the movement of the surgeon's hand may be captured vis-\u00e0-vis the tracking data of the motion of the first tool controller.","A method  of the present invention comprises the step of receiving  the video. As such, the video may be received  from a storage location in a memory device, from a network location, or elsewhere. The video may be received  as a live stream (e.g., from a network location, etc.) or pre-recorded and stored. The video may be of any format capable of being processed by a computer as is known in the art. For example, the video may be an AVI, MPEG, WMV, etc. The video may be received  by first requesting the video from another device, by receiving the video sent without a request, or otherwise.","The method comprises the step of obtaining  the tracking data. In the aforementioned embodiment where the tracking data corresponds to the motion of the tool controller, the tracking data may be obtained by receiving  the tracking data. The tracking data may be received  by first requesting the video from another device, by receiving the video without a request, or otherwise. The tracking data may be co-located with the video or in another location.","In another embodiment of the present invention, the tracking data may correspond to motion of the first surgical tool in the video. Tracking data corresponding to motion of the first surgical tool may be determined by a manual process (separate from the method of the present invention) by a person viewing the video and recording the motion of the first surgical tool (e.g., position of the end-effector, or other representative motion) as tracking data. The person may record this as, for example, a data file. In such embodiments, the tracking data may be obtained  by receiving  the tracking data.","In another embodiment of the present invention where the tracking data corresponds to motion of the first surgical tool in the video, the tracking data may be obtained  by processing  the video to calculate motion of the first surgical tool. This process is further described below (\u201cAutomated Tool Position Determination\u201d), although other methods of mapping are known in the art.","The method  comprises the step of calculating  a motion of a first interaction device corresponding to the tracking data. In embodiments where the tracking data corresponds to the motion of the first tool controller, such tool controller motion is calculated  by mapping to a corresponding motion of the first interaction device. The mapping parameters are pre-determined and further described in an exemplary embodiment (\u201cWorkspace Mapping\u201d below). The mapping parameters may be created such that the motion of the first tool controller has any desired relationship to the motion of the first interaction device. In a preferred embodiment, the mapping parameters are created such that movement of the first interaction device causes a hand of a trainee grasping the first interaction device to re-create the movement of the surgeon's hand.","In embodiments where the tracking data corresponds to the calculated motion of the first surgical tool, such surgical tool motion is calculated  by mapping to a corresponding motion of the first interaction device. The mapping parameters are pre-determined and further described in an exemplary embodiment (\u201cInverse Kinematics Mapping\u201d below), although other methods of mapping are know in the art.","The method  comprises the step of displaying  the video on the display. The method  comprises causing  the first interaction device to move according to the calculated motion. The motion of the first interaction device may be such that the first interaction device moves affirmatively (the first interaction device moves on its own, such as with a robotic arm). Alternatively, the motion of the first interaction device may take the form of restriction of movements of a trainee (user) by way of haptic feedback. In this way, a trainee grasping the first interaction device will be restricted to moving the first interaction device in the appropriate motion.","The present invention may be embodied as a computer program for performing any of the methods described herein. The computer program may be embodied on a computer readable medium. The computer readable medium may be a computer network or a storage location (e.g., server, NAS, SAN, etc) attached to a computer network.","Non-Surgical Embodiments","A system and\/or method of other embodiments of the present invention are capable of, and configured for, training a trainee in the use of other equipment where the equipment is controlled through movement of a user's body. Such equipment may be referred to as remotely operated in that the operator uses a proxy (e.g., a joystick, lever, pedal, etc.) to operate the equipment. Remotely operated does not necessarily mean, and should not be interpreted to require, that the operator is located at a particular distance from the equipment. For example, the system may be configured to train a trainee to operate construction equipment, such as, for example, a crane, a front-loader, a back-hoe, etc. Such equipment is generally operated by a person using joysticks or other levers. Other systems may be configured to train in the use of an industrial robot, a welder, an automobile, a airplane, etc. In certain embodiments, interaction devices may interface with a trainee's hand, foot, or other body parts. For example, in an automobile training system, an interaction device may be usable by a trainee's foot to simulate an accelerator pedal, brake pedal, or clutch pedal.","In these non-surgical embodiments, a system for training a trainee uses a video showing the equipment (the subject of the training) from the point of view of an actual operator. For example, in a system for training a trainee to operate a crane, the video will show the crane from the point of view of a crane operator. The video shows the equipment in use to perform one or more functions. In the crane example, the crane may be shown picking up materials. The operator uses the controls the equipment by way of a first tool controller. Depending on the particular use, two or more tool controllers may be used. For example, a plurality of levers may be necessary to operate the crane of the previous example.","Similar to the aforementioned MIS embodiments, the system has a processor, a display in communication with the processor, and a first interaction device in communication with the processor. There may be more than one interaction device corresponding with the number of tool controllers. In other embodiments, there may be fewer interaction devices than tool controllers. In these embodiments, the interaction device(s) may be switchable such that the first interaction device can switch from simulating a first tool controller to simulating a second tool controller, and so on. As such, the system may further comprise a switch  for switching the first interaction device (or other interaction device) to simulate each of two or more tool controllers. The switch  may be a push-button, a throw switch, a lever, a foot pedal, or any other switching device commonly known in the art.","The processor is programmed to receive the video. The processor is programmed to obtain the tracking data. The tracking data may be obtained as described above\u2014corresponding to movement of the tool controller, or corresponding to movement of the equipment in the video (determined manually or automatically). The processor calculates a motion of the first interaction device which corresponds to the tracking data. The processor displays the video on the display and causes the first interaction device to move according to the calculated motion. In this way, a trainee using a system of the present invention will be able to experience the visuals and motions of the operator of the physical equipment.","Workspace Mapping Example","In the following example, intended to be non-limiting, a tool controller, in the form of a DVSS master input device, was kinematically modeled and mapped to an interaction device, in the form of a PHANTOM\u00ae Omni\u00ae device. In this example, the PHANTOM\u00ae Omni\u00ae was mounted in an inverted configuration as found in a RoSS\u2122 device from Simulated Surgical System LLC.",{"@attributes":{"id":"p-0077","num":"0076"},"figref":["FIG. 14A","FIG. 14B"]},"Modified Denavit-Hartenberg (\u201cDH\u201d) notation was used to kinematically model the devices. In order to achieve the above mapping, the DH parameters were calculated for the DVSS master and RoSS input devices. It is a systematic notation for assigning orthonormal coordinate frames to the joints. The following steps were used to assign coordinate frames to the joints of the devices:\n\n","After assigning coordinate frames, the DH parameters may be calculated using the following conventions (see ):","(1) \u03b8is the angle of rotation from xto xmeasured about z;","(2) dis the distance measured along z;","(3) ais the distance measured along x; and","(4) \u03b1is the angle of rotation from zto zabout x.","Each homogeneous transformation T may be represented as a product of four basic transformations associated with joints i and j (l-link length, \u03b1-link twist, d-link offset, and \u03b8-joint angle) and I is a 4\u00d74 identity matrix. The position and orientation of the end-effector is denoted by a position vector P and the 3\u00d73 rotation matrix R. Based on the above DH parameters, a homogeneous transformation matrix is constructed which maps frame i coordinates into i\u22121 coordinates as follows:",{"@attributes":{"id":"p-0085","num":"0089"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"msubsup":{"mi":"T","mrow":[{"mi":"i","mo":"-","mn":"1"},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mo":"\u2062","mi":"i"}]},"mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mrow":{"mi":"cos","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["\u03b8","i"]}}},{"mrow":{"mrow":{"mo":"-","mi":"sin"},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msub":[{"mi":["\u03b1","i"]},{"mi":["\u03b8","i"]}],"mi":"cos"}},{"mrow":{"mi":["sin","sin"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msub":[{"mi":["\u03b1","i"]},{"mi":["\u03b8","i"]}]}},{"mrow":{"msub":[{"mi":["a","i"]},{"mi":["\u03b8","i"]}],"mo":["\u2062","\u2062","\u2062"],"mi":"cos","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}]},{"mtd":[{"mrow":{"mi":"sin","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["\u03b8","i"]}}},{"mrow":{"mrow":{"mo":"-","mi":"cos"},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msub":[{"mi":["\u03b1","i"]},{"mi":["\u03b8","i"]}],"mi":"cos"}},{"mrow":{"mi":["sin","cos"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msub":[{"mi":["\u03b1","i"]},{"mi":["\u03b8","i"]}]}},{"mrow":{"msub":[{"mi":["a","i"]},{"mi":["\u03b8","i"]}],"mo":["\u2062","\u2062","\u2062"],"mi":"sin","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}]},{"mtd":[{"mn":"0"},{"mrow":{"mi":"sin","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["\u03b1","i"]}}},{"mrow":{"mi":"cos","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["\u03b1","i"]}}},{"msub":{"mi":["d","i"]}}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"1"}]}]}}}},{"mrow":{"mo":["(",")"],"mn":"1"}}]},{"mtd":[{"mrow":{"msubsup":{"mi":"T","mrow":[{"mi":"i","mo":"-","mn":"1"},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mo":"\u2062","mi":"i"}]},"mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mi":"R"},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mi":"P"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"1"}]}]}}}},{"mrow":{"mo":["(",")"],"mn":"2"}}]}]}}}},"After calculating the homogeneous transformation matrix for each link, the composite transformation matrix is calculated. This matrix maps the tool coordinates to the base coordinates. This yields the transformation matrix as:\n\n\u2003\u2003(3)\n","This final composite transformation matrix is calculated with respect to the base frame. The DH parameters for the DVSS master are shown in Table 1.",{"@attributes":{"id":"p-0088","num":"0092"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 1"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"DH Parameters of DVSS master"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"42pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Link","Parameters","\u03b8","d","a","A"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"6","align":"center","rowsep":"1"}}]},{"entry":[{},"1","\u03b8","\u03b8","d","0","\u2212pi\/2"]},{"entry":[{},"2","\u03b8","\u03b8","0","L","0"]},{"entry":[{},"3","\u03b8","\u03b8","0","L","\u2212pi\/2"]},{"entry":[{},"4","\u03b8","\u03b8","d","0","\u2009\u2009pi\/2"]},{"entry":[{},"5","\u03b8","\u03b8","d","0","\u2212pi\/2"]},{"entry":[{},"6","\u03b8","\u03b8","d","0","\u2009\u2009pi\/2"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"6","align":"center","rowsep":"1"}}]}]}}]}}},"The DH parameters for the RoSS console are shown in Table 2.",{"@attributes":{"id":"p-0090","num":"0094"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 2"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"DH Parameters of RoSS Console"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"42pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Link","Parameters","\u03b8","d","a","\u03b1"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"6","align":"center","rowsep":"1"}}]},{"entry":[{},"1","\u03b8","\u03b8","d","0","\u2212pi\/2"]},{"entry":[{},"2","\u03b8","\u03b8","0","L","0"]},{"entry":[{},"3","\u03b8","\u03b8","0","0","\u2212pi\/2"]},{"entry":[{},"4","\u03b8","\u03b8","d","0","\u2009\u2009pi\/2"]},{"entry":[{},"5","\u03b8","\u03b8","0","0","\u2212pi\/2"]},{"entry":[{},"6","\u03b8","\u03b8","d","0","\u2009\u2009pi\/2"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"6","align":"center","rowsep":"1"}}]}]}}]}}},"Based on these DH parameters, the individual transformation matrix for each link may be calculated and the composite transformation matrix may be constructed after multiplying each of the individual transformation matrices as follows\n\n\u2003\u2003(4)\n","To find the overall workspaces of the RoSS input device and DVSS input device, the range of angles of all the joints is found.","The range of each of the joint angles of RoSS input device is:","Joint 1: \u22121.45<\u03b8<1.05 (radians)","Joint 2: 0.0<\u03b8<1.727 (radians)","Joint 3: 1.0<\u03b8<2.1 (radians)","Joint 4: 0.0<\u03b8<4.71 (radians)","Joint 5: 0.0<\u03b8<3.0 (radians)","Joint 6: 0.0<\u03b8<4.71 (radians)","The range of each of the joint angles of DVSS input device is:","Joint 1: \u22120.53<\u03b8<1.57 (radians)","Joint 2: 0.265<\u03b8<0.785 (radians)","Joint 3: 0.0<\u03b8<1.03 (radians)","Joint 4: \u22123.14<\u03b8<1.57 (radians)","Joint 5: \u22121.57<\u03b8<3.14 (radians)","Joint 6: \u22120.707<\u03b8<0.707 (radians)","Each of the joint angles is varied incrementally to yield the end-effector positions in the workspace. The end-effector position matrix is homogenized by adding a fourth column to the x, y and z columns. The workspace positions for both the RoSS and DVSS input devices are calculated. The 4\u00d74 transformation matrix between the two workspaces is calculated by:\n\n=pin()*\u2003\u2003(5)\n","where: Pis the set of homogenized positions for RoSS input device; and","Pis the set of homogenized positions for DVSS input device.","Since the end-effector encoder position values from the RoSS input device were spatially transformed to the calculated position values of RoSS input device from DH notation, these positions may either be transformed to the RoSS workspace or transformed to the DVSS master workspace. Therefore, a set of device positions consisting of a large number of 3D spatial position values (9261 in number) and the end effector positions are homogenized by adding a fourth column to x, y and z columns. Then the 4\u00d74 transformation matrix was found between the two workspaces.","Inverse Kinematics Mapping Example","Inverse kinematics may be used to find a set of joint configurations of an articulated structure based upon a desirable end-effector location. Inverse kinematics was used to determine a set of joint angles in an articulated structure based upon the position (or motion) of the surgical tool in the video. This results in multiple joint angle solutions and infinite solutions at singularities. It may be generally used in software to control the joints. Control software should be able to perform the necessary calculations in near real time.","The mathematical representation of the inverse kinematics technique is defined as\n\n\u03b8=()\u2003\u2003(8)\n","Inverse kinematics may be implemented based upon the Jacobian technique. This technique incrementally changes joint orientations from a stable starting position towards a joint configuration that will result in the end-effector being located at the desired position in absolute space (corresponding to the location of the surgical tool in the video). The amount of incremental change on each iteration is defined by the relationship between the partial derivatives of the joint angles, \u03b8, and the difference between the current location of the end-effector, X, and the desired position, X. The link between these two sets of parameters leads to the system Jacobian, J. This is a matrix that has dimensionality (m\u00d7n) where m is the spatial dimensional of X and n is the size of the joint orientation set, q.\n\n(\u03b8)\u2003\u2003(9)\n","The Jacobian is derived from Equation 9 as follows. Taking partial derivatives of Equation 9:",{"@attributes":{"id":"p-0116","num":"0120"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":[{"mrow":[{"mo":"\u2146","mi":"X"},{"mrow":[{"mi":"J","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03b8"}},{"mo":"\u2146","mi":"\u03b8"}],"mo":"\u2062"}],"mo":"="},{"mi":"Where","mo":"\u2062","mstyle":{"mtext":":"}}],"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}}}},{"mrow":{"mo":["(",")"],"mn":"10"}}]},{"mtd":[{"mrow":{"msub":{"mi":["J","ij"]},"mo":"=","mfrac":{"mrow":[{"mo":"\u2202","msub":{"mi":["f","i"]}},{"mo":"\u2202","msub":{"mi":["\u03b8","j"]}}]}}},{"mrow":{"mo":["(",")"],"mn":"11"}}]}]}}}},"Rewriting Equation 10 in a form similar to inverse kinematics (Equation 9), results in Equation 12. This form of the problem transforms the under-defined system into a linear one that can be solved using iterative steps.\n\n\u2003\u2003(12)\n","The problem now is that Equation 12 requires the inversion of the Jacobian matrix. However because of the under-defined problem that the inverse kinematics technique suffers from, the Jacobian is very rarely square. Therefore, the right-hand generalized pseudo-inverse may be used to overcome the non-square matrix problem, as given in equation 14.","Generating the pseudo-inverse of the Jacobian in this way can lead to inaccuracies in the resulting inverse that need to be reduced. Any inaccuracies of the inverse Jacobian can be detected by multiplying it with the original Jacobian then subtracting the result from the identity matrix. A magnitude error can be determined by taking the second norm of the resulting matrix multiplied by dP, as outlined in Equation 15. If the error proves too big then dP can be decreased until the error falls within an acceptable limit.","An overview of the algorithm used to implement an iterative inverse kinematics solution is as follows:\n\n",{"@attributes":{"id":"p-0121","num":"0127"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["J","ij"]},"mo":"=","mfrac":{"mrow":[{"mo":"\u2202","msub":{"mi":["P","i"]}},{"mo":"\u2202","msub":{"mi":["\u03b8","j"]}}]}}},{"mrow":{"mo":["(",")"],"mn":"14"}}]}}}},"ul":{"@attributes":{"id":"ul0005","list-style":"none"},"li":{"@attributes":{"id":"ul0005-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0006","list-style":"none"},"li":["(3) Calculate the pseudo-inverse of the Jacobian.\n\n()\u2003\u2003(15)\n","(4) Determine the error of the pseudo-inverse error:\n\nerror=\u2225\u2212()\u2003\u2003(16)\n","(5) If error>e then dP=dP\/2 restart at step (4)","(6) Calculate the updated values for the joint orientations and use these as the new current values. Check the bounds for theta values."]}}}},{"@attributes":{"id":"p-0122","num":"0132"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"\u03b8","mo":"=","mrow":{"mo":"{","mtable":{"mtr":[{"mtd":{"mrow":{"mrow":{"mrow":[{"mi":["lowerbound","if","\u03b8"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]},{"msup":{"mi":"J","mrow":{"mo":"-","mn":"1"}},"mo":"\u2062","mrow":{"mo":"\u2146","mi":"P"}}],"mo":"+"},"mo":"<","mi":"lowerbound"}}},{"mtd":{"mrow":{"mrow":{"mrow":[{"mi":["upperbound","if","\u03b8"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]},{"msup":{"mi":"J","mrow":{"mo":"-","mn":"1"}},"mo":"\u2062","mrow":{"mo":"\u2146","mi":"P"}}],"mo":"+"},"mo":">","mi":"upperbound"}}},{"mtd":{"mrow":{"mi":"\u03b8","mo":"+","mrow":{"msup":{"mi":"J","mrow":{"mo":"-","mn":"1"}},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"mrow":{"mo":"\u2146","mi":"P"},"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":["if","otherwise"]}}}}]}}}},{"mrow":{"mo":["(",")"],"mn":"17"}}]}}}},"ul":{"@attributes":{"id":"ul0007","list-style":"none"},"li":{"@attributes":{"id":"ul0007-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0008","list-style":"none"},"li":"(7) Using forward kinematics determine whether the new joint orientations position the end-effector close enough to the desired absolute location. If the solution is adequate then terminate the algorithm otherwise go back to step (1)."}}}},"The time to complete the Inverse Kinematics algorithm for a given end-effector is an unknown quantity due to an arbitrary number of iterations required. However, the time to complete a single iteration is constant with respect to the dimensionality of X and \u03b8 which is unchanged under a complete execution of the algorithm. Therefore by placing an upper limit on the number of iterations we can set a maximum time boundary for the algorithm to return in. If the solver reaches the limit then the algorithm returns the closest result it has seen.","Automated Tool Position Determination","Example Tracking Techniques","Two techniques for locating targets in video images are shown in detail as exemplary embodiments of the present invention. However, the present invention should not be limited by these examples.","\u201cTarget Representation and Localization\u201d techniques may be viewed as bottom-up processes. These methods give a variety of tools for identifying the moving object. Locating and tracking the target object successfully is dependent on the algorithm. For example, using blob tracking is useful for identifying human movement because a person's profile changes dynamically. Typically the computational complexity for these algorithms is low.","\u201cFiltering and Data Association\u201d techniques are generally top-down processes, which involve incorporating prior information about the scene or object, addressing object dynamics, and evaluation of different hypotheses. These methods allow the tracking of complex objects along with more complex object interaction like tracking objects moving behind obstructions. The computational complexity for these algorithms is usually higher.","Filtering and Data Association Technique Example","The images (frames) which make up the video may be enhanced to cause the at least one surgical tool to better stand out from the background of the image. In some embodiments, the background of the image will be the surgical environment\u2014tissues and other structures of the patient. Various image processing options are discussed below, including reference to a specific example (selection of image processing steps). The options and specific example are intended to be non-limiting. Other image processing techniques are known in the art and are within the scope of the present invention. In a method  of the present invention, one or more filters may be applied  to the frames of the video.","Reference is made to a specific example, where two needle tools are tracked. Each needle tool comprises a long shaft, having a base, and a wrist joint. The bases of the shafts are allowed to rotate freely. The video of this example is a stereoscopic video produced using two cameras slightly offset with respect to each other to produce set of images of the same scene from different angles. This stereoscopic video can be used to create a three-dimensional video of the MIS. The objective of this example was to track two surgical tools captured by two separate video cameras and find the locations of the tools. The following assumptions were made:","1. the tools may be described by features (e.g., color, number, length, etc.) which are known prior to analysis; and","2. the tools are the only objects which can be defined using straight lines.","Extract Images from Video Input","Videos are generally comprised of a plurality of still images taken over time. The video used by the present invention may first be parsed to extract the separate images (frames) from the video. In the present example, the openCV application programming interface (\u201cAPI\u201d) was used to extract image frames from the video of tool motion captured by camera. Each of these images served as input for further image processing. In the present example, the video\u2014and therefore each frame of the video\u2014measured 720\u00d7486 pixels (a common, wide-screen format).  is a sample image showing two surgical tools ,  and an internal portion of an abdomen of a patient. A method  of the present invention may comprise the step of extracting a frame from video, extracting each frame from the video successively, and\/or extracting all frames from the video.","Retrieving Physical Characteristic Data","The properties (physical characteristics) of the tools were captured and retrieved for use during image processing (e.g., color (RGB format), length, width, etc.). The color information, if provided was used during thresholding (described below), while other properties were used during feature detection (described below). In the present example, the length of the tool shaft in a 720\u00d7486 resolution image is approximately 200 pixels.","Pre-Processing\u2014Noise\/Clutter","It may be beneficial to filter the image to reduce noise and clutter within the image. Noise can be defined as random errors in pixel brightness values, while clutter can be defined non-useful image components (patient tissue may involve a great deal of clutter). Both noise and clutter can be reduced by applying filters. For example, blurring (averaging) filters, Gaussian blur filters, and\/or median filters may be applied to the image. Gaussian blur filters will remove high-frequencies from an image and may thus be considered to be low-pass filters. Median filters replace pixels with the median value of neighboring pixels according to rules which vary by filtering algorithm. Median filters reduce \u201cimpulse noise\u201d considerably without excessive blurring of edges in the image. Both median and Gaussian filters are suited to noise\/clutter reduction, however, median filters are better suited to preserving edges within an image. After considerable testing using different filters, the median filter technique was used for the present example.","Pre-Processing\u2014Thresholding","In a method of the present invention, thresholding may be used to provide a binary image (or other quantized image). Thresholding is an image processing technique where pixels are assigned a new value based on its particular characteristic compared to a threshold value of that characteristic. For example, thresholding may assign a new value to a pixel based on the brightness of that pixel. In another examples, thresholding may assign a new value to a pixel based on its color (using an appropriate color space\u2014e.g., red, green, blue (\u201cRGB\u201d), hue-saturation-luminosity (\u201cHSL\u201d), etc.). Thresholding is useful for producing binary images\u2014images that have only two possible values for each pixel. In the present example, a binary image is well-suited for other steps, including edge detection.","In the present example, thresholding based on HSL value () proved to yield better results than brightness thresholding techniques (). HSL thresholding is a useful method for achieving desirable results from color thresholding where an image was converted from another color space. A color space like HSL can result in a better-designed filter, as it allows the characteristics of color hue, saturation, and luminosity (brightness) to be addressed separately, i.e. allowing a more forgiving filter for luminosity if a large range of input brightness is anticipated. For certain data sets, HSL thresholding may render the noise\/clutter reduction operation (described above) unnecessary. The output from thresholding based on HSL values provides a binary image with very few unwanted pixels.","Pre-Processing\u2014Morphological Operations","A method of the present invention may use a morphological function to reduce the number of noise pixels and\/or resolve discontinuities in the image. Erosion and dilation are methods of morphological operations which can be used independently or together to produce desired outputs. Erode will reduce the size of blobs of pixels in the image, and Dilate will increase the size of such blobs, either adding or subtracting pixels (their brightness value) from around the perimeter of the blob. For vision processing, these functions are useful because they can either accentuate or eliminate smaller blocks of pixels in the image. In addition, first applying Dilate and then Erode (called \u201cClosing\u201d) can cause adjacent blobs of pixels to become connected, while application in the reverse order (called \u201cOpening\u201d) can cause them to disconnect, without changing the general size of the blobs. Such morphological operations are suited to further reduce the number of noise pixels and\/or reduce discontinuities in the image. For example, a few isolated pixels may be dilated to produce a uniform line. These operations operate best on binary images, although non-binary images may also be used. In morphological operations, a structural element (of a particular shape) is used to \u201cprobe\u201d an image and reduce (erode) or expand (dilate) the shape of a structure in the image.","Erosion may be expressed as:",{"@attributes":{"id":"p-0146","num":"0157"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"Erosion","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},"mrow":{"mo":["(",")"],"mrow":{"mi":["A","B"],"mo":","}}},{"munder":{"mo":"\u22c3","mrow":{"mi":["b","B"],"mo":"\u2208"}},"mo":"\u2062","msub":{"mi":"A","mrow":{"mo":"-","mi":"b"}}}],"mo":"="}}}},"where A is the binary image and B is the structural element. When the structural element B has a center, the erosion of binary image A may be understood as the set of points reached by the center of B as B is moved within structures of the binary image A.","Dilation may be expressed as:",{"@attributes":{"id":"p-0149","num":"0160"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"Erosion","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},"mrow":{"mo":["(",")"],"mrow":{"mi":["A","B"],"mo":","}}},{"munder":{"mo":"\u22c3","mrow":{"mi":["b","B"],"mo":"\u2208"}},"mo":"\u2062","mrow":{"mi":"A","mo":"."}}],"mo":"="}}}},"Where the structural element B has a center, the dilation of binary image A may be understood as the set of points reached by the periphery of B as the center of B is moved along structures of the binary image A.","In the present example, the image was first eroded and then dilated which connected disconnected pixels (see  (erosion) and  (dilation)).","Pre-Processing\u2014Edge Detection","A method of the present invention may apply an edge detection function to the frames of the video. Edge detection involves scanning the image for places of sudden change (in color, brightness, or other appropriate characteristic), which usually denotes a division or an \u201cedge.\u201d There are several methods that can be used, each performing better on different types of images. There are edge detection algorithms which use first derivative (usually search-based methods) and second order derivative (usually zero-crossing based methods) information. For example, algorithms using Sobel operators or Laplacian operators may be used to detect gradient variation, and thus edges.","In the present example, the \u201cCanny\u201d edge detection algorithm was used. The Canny algorithm is a type of zero-crossing algorithm, meaning it checks the zero crossing of second order derivative. The Canny algorithm applies thresholding to the obtained output by applying a Sobel operator and creates a binary image comprising all possible edges in the image. Parameters may be selected in the function to refine the size of the edges. The sample image obtained from Canny Algorithm is shown in .","Pre-Processing\u2014Hough Transform","A method of the present invention may use a Hough transform to determine a set of tool edge candidates from among a set of edges. A Hough transform is a technique for extracting features (in the present case, edges) from an image. Hough space is an image space that describes the probability that certain shapes exist at locations in an image. The OpenCV API contains two functions that make use of Hough transforms to identify instances of straight lines (cvHoughLines2) or circles (cvHoughCircles) within an image. Both functions require the input image to be grayscale\/binary (single channel). The line function returns an array containing all possible lines, which is used for feature detection.","In the present example, cvHoughLines2 was used to determine the locations of lines existing in the image of . The resulting array may be considered a set of tool candidates. The set of tool candidates was used to recognize the tools based on the tool characteristics (physical characteristic data).","Feature Detection and Object Recognition","A method of the present invention may use the physical characteristic data to determine at least one tool edge from among the tool edge candidates. In the present example, after the set of tool candidates was determined from the image using the aforementioned techniques (or other appropriate techniques), the lines defining the surgical tools may be identified from among the set. The physical characteristic data was used. For example, the length of the tool was used to determine lines in the set of candidates of an suitable length. Also, the tool width was used to determine lines which were the proper distance apart. Also, the slope of the proper lines could be determined since the shaft of the tool would be identified by two substantially parallel lines. Using this domain knowledge in the form of standard IF-ELSE conditions with generic image analysis data, each tool shaft was identified in the image (see ).","Tool Location Determination","End points of the identified lines which define the edges of the tools were used for extracting the 2-D coordinates of the tools. Further, using features of the tools, the tool's two-dimensional orientation was determined. By analyzing two corresponding images of the stereoscopic video (taken simultaneously from two different cameras), the three-dimensional location of tool tips and orientation of tools were determined.","Target Representation and Localization Technique Example","This technique uses the concept of \u201coptical flow\u201d to track the apparent motion of objects (in the present case, the surgical tools) in the frames of a video. The optical flow techniques shown in this example, use either the Horn-Schunck method or the Lucas-Kanade method. Other methods are possible and within the scope of this disclosure.","At a high level, the following constraint equation can be used to compute the optical flow between two images:\n\n=0\n","In this equation, I, I, and Iare the intensity of a pixel at location x, y, t, u is the horizontal optical flow, and v is the vertical optical flow. This equation is under-constrained; however, there are several methods to solve for u and v:","Horn-Schunck Method","By assuming that the optical flow is smooth over the entire image, the Horn-Schunck method computes an estimate of the velocity field that minimizes the global energy functional equation:",{"@attributes":{"id":"p-0168","num":"0179"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"E","mo":"=","mrow":{"mrow":[{"mo":"\u222b","mrow":{"mo":"\u222b","mrow":{"msup":{"mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":["I","x"]},"mo":"\u2062","mi":"u"},{"msub":{"mi":["I","y"]},"mo":"\u2062","mi":"v"}],"mo":["+","+"],"msub":{"mi":["I","t"]}}},"mn":"2"},"mo":["\u2062","\u2062"],"mrow":[{"mo":"\u2146","mi":"x"},{"mo":"\u2146","mi":"y"}]}}},{"mo":"\u222b","mrow":{"mo":"\u222b","mrow":{"mrow":[{"mo":["{","}"],"mrow":{"msup":[{"mrow":{"mo":["(",")"],"mfrac":{"mrow":[{"mo":"\u2202","mi":"u"},{"mo":"\u2202","mi":"x"}]}},"mn":"2"},{"mrow":{"mo":["(",")"],"mfrac":{"mrow":[{"mo":"\u2202","mi":"u"},{"mo":"\u2202","mi":"y"}]}},"mn":"2"},{"mrow":{"mo":["(",")"],"mfrac":{"mrow":[{"mo":"\u2202","mi":"v"},{"mo":"\u2202","mi":"x"}]}},"mn":"2"},{"mrow":{"mo":["(",")"],"mfrac":{"mrow":[{"mo":"\u2202","mi":"v"},{"mo":"\u2202","mi":"y"}]}},"mn":"2"}],"mo":["+","+","+"]}},{"mo":"\u2146","mi":"x"},{"mo":"\u2146","mi":"y"}],"mo":["\u2062","\u2062"]}}}],"mo":["+","+"],"mi":"\u03b1"}}}}},"The Horn-Schunck method minimizes the previous equation to obtain the velocity field, [u v], for each pixel in the image, which is given by the following equations:",{"@attributes":{"id":"p-0170","num":"0181"},"maths":[{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msubsup":{"mi":"u","mrow":[{"mi":["x","y"],"mo":","},{"mi":"k","mo":"+","mn":"1"}]},"mo":"=","mrow":{"msubsup":{"mi":"u","mrow":[{"mi":["x","y"],"mo":","},{"mo":"-","mi":"k"}]},"mo":"-","mfrac":{"mrow":[{"msub":{"mi":["I","x"]},"mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"msub":{"mi":["I","x"]},"mo":"\u2062","msubsup":{"mover":{"mi":["u","_"]},"mrow":{"mi":["x","y"],"mo":","},"mi":"h"}},{"msub":{"mi":["I","y"]},"mo":"\u2062","msubsup":{"mover":{"mi":["v","_"]},"mrow":{"mi":["x","y"],"mo":","},"mi":"k"}}],"mo":["+","+"],"msub":{"mi":["I","t"]}}}},{"msup":{"mi":"\u03b1","mn":"2"},"mo":["+","+"],"msubsup":[{"mi":["I","x"],"mn":"2"},{"mi":["I","y"],"mn":"2"}]}]}}}}},{"@attributes":{"id":"MATH-US-00008-2","num":"00008.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msubsup":{"mi":"v","mrow":[{"mi":["x","y"],"mo":","},{"mi":"k","mo":"+","mn":"1"}]},"mo":"=","mrow":{"msubsup":{"mi":"v","mrow":[{"mi":["x","y"],"mo":","},{"mo":"-","mi":"k"}]},"mo":"-","mfrac":{"mrow":[{"msub":{"mi":["I","y"]},"mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"msub":{"mi":["I","x"]},"mo":"\u2062","msubsup":{"mover":{"mi":["u","_"]},"mrow":{"mi":["x","y"],"mo":","},"mi":"k"}},{"msub":{"mi":["I","y"]},"mo":"\u2062","msubsup":{"mover":{"mi":["v","_"]},"mrow":{"mi":["x","y"],"mo":","},"mi":"k"}}],"mo":["+","+"],"msub":{"mi":["I","t"]}}}},{"msup":{"mi":"\u03b1","mn":"2"},"mo":["+","+"],"msubsup":[{"mi":["I","x"],"mn":"2"},{"mi":["I","y"],"mn":"2"}]}]}}}}}]},"In this equation,",{"@attributes":{"id":"p-0172","num":"0183"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mo":["[","]"],"mrow":{"msubsup":[{"mi":["u","k"],"mrow":{"mi":["x","y"],"mo":","}},{"mi":["v","k"],"mrow":{"mi":["x","y"],"mo":","}}],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}}}}}},"br":{}},"A method according to an embodiment of the present invention using Horn-Schunck solves for u and v by computing and using the Sobel convolution kernel and its transposed form for each pixel in the first image. Then, compute between frames  and  using the kernel. Assume the previous velocity to be 0, and compute the average velocity for each pixel using as a convolution kernel. Finally, iteratively solve for u and v.","Lucas-Kanade Method:","To solve the optical flow constraint equation for u and v, the Lucas-Kanade method divides the original image into smaller sections and assumes a constant velocity in each section. Then, the method performs a weighted least-square fit of the optical flow constraint equation to a constant model in each section by minimizing the following equation:",{"@attributes":{"id":"p-0176","num":"0187"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"munder":{"mo":"\u2211","mrow":{"mi":["x","\u03a9"],"mo":"\u2208"}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mrow":{"msup":{"mi":"W","mn":"2"},"mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"msub":{"mi":["I","x"]},"mo":"\u2062","mi":"u"},{"msub":{"mi":["I","y"]},"mo":"\u2062","mi":"v"}],"mo":["+","+"],"msub":{"mi":["I","t"]}}}},"mn":"2"}}}}},"Here, W is a window function that emphasizes the constraints at the center of each section. The solution to the minimization problem is given by the following equation:",{"@attributes":{"id":"p-0178","num":"0189"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mrow":[{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mrow":{"mo":"\u2211","mrow":{"msup":{"mi":"W","mn":"2"},"mo":"\u2062","msubsup":{"mi":["I","x"],"mn":"2"}}}},{"mrow":{"mo":"\u2211","mrow":{"msup":{"mi":"W","mn":"2"},"mo":["\u2062","\u2062"],"msub":[{"mi":["I","x"]},{"mi":["I","y"]}]}}}]},{"mtd":[{"mrow":{"mo":"\u2211","mrow":{"msup":{"mi":"W","mn":"2"},"mo":["\u2062","\u2062"],"msub":[{"mi":["I","y"]},{"mi":["I","x"]}]}}},{"mrow":{"mo":"\u2211","mrow":{"msup":{"mi":"W","mn":"2"},"mo":"\u2062","msubsup":{"mi":["I","y"],"mn":"2"}}}}]}]}},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mi":"u"}},{"mtd":{"mi":"v"}}]}}],"mo":"\u2061"},{"mo":"-","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mrow":{"mo":"\u2211","mrow":{"msup":{"mi":"W","mn":"2"},"mo":["\u2062","\u2062"],"msub":[{"mi":["I","x"]},{"mi":["I","t"]}]}}}},{"mtd":{"mrow":{"mo":"\u2211","mrow":{"msup":{"mi":"W","mn":"2"},"mo":["\u2062","\u2062"],"msub":[{"mi":["I","y"]},{"mi":["I","t"]}]}}}}]}}}],"mo":"="}}}},"The block computes using a difference filter or a derivative of a Gaussian filter (below)","Difference Filter:","Compute and use the kernel and its transposed form. For fixed-point data types, the kernel values are signed, fixed-point values with a word length equal to 16 and fraction length equal to 15.","1. Compute between images  and  using the kernel.","2. Smooth the gradient components and use a separable and isotropic 5-by-5 element kernel. For fixed-point data types, the kernel values are unsigned, fixed-point values with word length equal to 8 and fraction length equal to 7.","3. Solve the 2-by-2 linear equations for each pixel by:\n\n",{"@attributes":{"id":"p-0185","num":"0197"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mi":"A","mo":"=","mrow":{"mrow":[{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mi":"a"},{"mi":"b"}]},{"mtd":[{"mi":"b"},{"mi":"c"}]}]}},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mrow":{"mo":"\u2211","mrow":{"msup":{"mi":"W","mn":"2"},"mo":"\u2062","msubsup":{"mi":["I","x"],"mn":"2"}}}},{"mrow":{"mo":"\u2211","mrow":{"msup":{"mi":"W","mn":"2"},"mo":["\u2062","\u2062"],"msub":[{"mi":["I","x"]},{"mi":["I","y"]}]}}}]},{"mtd":[{"mrow":{"mo":"\u2211","mrow":{"msup":{"mi":"W","mn":"2"},"mo":["\u2062","\u2062"],"msub":[{"mi":["I","y"]},{"mi":["I","x"]}]}}},{"mrow":{"mo":"\u2211","mrow":{"msup":{"mi":"W","mn":"2"},"mo":"\u2062","msubsup":{"mi":["I","y"],"mn":"2"}}}}]}]}}],"mo":"="}},"mo":","}}},"br":{}},{"@attributes":{"id":"p-0186","num":"0198"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"msub":{"mi":["\u03bb","i"]},"mo":"=","mrow":{"mfrac":[{"mrow":{"mi":["a","c"],"mo":"+"},"mn":"2"},{"msqrt":{"mrow":{"mrow":{"mn":"4","mo":"\u2062","msup":{"mi":"b","mn":"2"}},"mo":"+","msup":{"mrow":{"mo":["(",")"],"mrow":{"mi":["a","c"],"mo":"-"}},"mn":"2"}}},"mn":"2"}],"mo":"\u00b1"}},"mo":";"}}},"ul":{"@attributes":{"id":"ul0011","list-style":"none"},"li":{"@attributes":{"id":"ul0011-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0012","list-style":"none"},"li":"In the fixed-point diagrams,"}}}},{"@attributes":{"id":"p-0187","num":"0200"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"P","mo":"=","mfrac":{"mrow":{"mi":["a","c"],"mo":"+"},"mn":"2"}},{"mi":"Q","mo":"=","mfrac":{"msqrt":{"mrow":{"mrow":{"mn":"4","mo":"\u2062","msup":{"mi":"b","mn":"2"}},"mo":"+","msup":{"mrow":{"mo":["(",")"],"mrow":{"mi":["a","c"],"mo":"-"}},"mn":"2"}}},"mn":"2"}}],"mo":","}}}},"4. When the eigenvalues are computed, they are compared to a threshold (noise reduction) parameter that is user selectable. Selection is made to eliminate the effect of small movements between frames\u2014the higher the threshold value, the less small movements impact the optical flow calculation. The results fall into one of the following cases:\n\n","Derivative of Gaussian:","Compute and use a Gaussian filter to perform temporal filtering. Specific temporal filter characteristics such as the standard deviation and number of filter coefficients are selected as appropriate.","Compute and use a Gaussian filter and the derivative of a Gaussian filter to smooth the image using spatial filtering. Specific standard deviation and length of the image-smoothing filter are selected as appropriate.","1. Compute between images  and  using the following steps:","Use the derivative of a Gaussian filter to perform temporal filtering. Specific temporal filter characteristics such as the standard deviation and number of filter coefficients are selected as appropriate.","a. Use a filter to perform spatial filtering on the output of the temporal Filter.","2. Smooth the gradient components and using a gradient smoothing filter. The standard deviation and the number of filter coefficients for the gradient smoothing filter are selected as appropriate.","3. Solve the 2-by-2 linear equations for each pixel using the following method:\n\n",{"@attributes":{"id":"p-0197","num":"0214"},"maths":{"@attributes":{"id":"MATH-US-00015","num":"00015"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mi":"A","mo":"=","mrow":{"mrow":[{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mi":"a"},{"mi":"b"}]},{"mtd":[{"mi":"b"},{"mi":"c"}]}]}},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mrow":{"mo":"\u2211","mrow":{"msup":{"mi":"W","mn":"2"},"mo":"\u2062","msubsup":{"mi":["I","x"],"mn":"2"}}}},{"mrow":{"mo":"\u2211","mrow":{"msup":{"mi":"W","mn":"2"},"mo":["\u2062","\u2062"],"msub":[{"mi":["I","x"]},{"mi":["I","y"]}]}}}]},{"mtd":[{"mrow":{"mo":"\u2211","mrow":{"msup":{"mi":"W","mn":"2"},"mo":["\u2062","\u2062"],"msub":[{"mi":["I","y"]},{"mi":["I","x"]}]}}},{"mrow":{"mo":"\u2211","mrow":{"msup":{"mi":"W","mn":"2"},"mo":"\u2062","msubsup":{"mi":["I","y"],"mn":"2"}}}}]}]}}],"mo":"="}},"mo":","}}},"br":{}},{"@attributes":{"id":"p-0198","num":"0215"},"maths":{"@attributes":{"id":"MATH-US-00016","num":"00016"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"msub":{"mi":["\u03bb","i"]},"mo":"=","mrow":{"mfrac":[{"mrow":{"mi":["a","c"],"mo":"+"},"mn":"2"},{"msqrt":{"mrow":{"mrow":{"mn":"4","mo":"\u2062","msup":{"mi":"b","mn":"2"}},"mo":"+","msup":{"mrow":{"mo":["(",")"],"mrow":{"mi":["a","c"],"mo":"-"}},"mn":"2"}}},"mn":"2"}],"mo":"\u00b1"}},"mo":";"}}}},"4. When the eigenvalues are computed, they are compared to a threshold (noise reduction) parameter that is user selectable. Selection is made to eliminate the effect of small movements between frames\u2014the higher the threshold value, the less small movements impact the optical flow calculation. The results fall into one of the following cases:\n\n","Software Interface","In a non-limiting example, the aforementioned optical flow techniques were implemented in software. The main pane of the Optical Flow dialog box is shown in , and the \u201cData Types\u201d pane is shown in . The main field parameters are described below.","Method:","Select the method the block uses to calculate the optical flow. The choices are \u201cHorn-Schunck\u201d or \u201cLucas-Kanade.\u201d","Compute Optical Flow Between:","Select two images to compute the optical flow between two images. Select \u201cCurrent frame and N-th frame back\u201d to compute the optical flow between two video frames that are N frames apart. This parameter is visible when the Method parameter is set to \u201cHorn-Schunck\u201d or the Method parameter is set to \u201cLucas-Kanade\u201d and the Temporal gradient filter to \u201cDifference filter [\u22121 1].\u201d","N:","Enter a scalar value that represents the number of frames between the reference frame and the current frame. This parameter becomes available when the Compute optical flow between parameter is set to a selection requiring a value for N.","Smoothness Factor:","If the relative motion between the two images or video frames is large, enter a large positive scalar value. If the relative motion is small, enter a small positive scalar value. This parameter becomes available when the Method parameter is set to \u201cHorn-Schunck.\u201d","Stop Iterative Solution:","Use this parameter to control when the block's iterative solution process stops. If the iteration should stop when the velocity difference is below a certain threshold value, select \u201cwhen velocity difference falls below threshold.\u201d If the iteration should stop after a certain number of iterations, choose \u201cwhen maximum number of iterations is reached.\u201d There is also an option for \u201cwhichever comes first.\u201d This parameter becomes available when the Method parameter is set to Horn-Schunck.","Maximum Number of Iterations:","Enter a scalar value that represents the maximum number of iterations the block should perform. This parameter is only visible if \u201cwhen maximum number of iterations is reached\u201d or \u201cwhichever comes first\u201d is selected for the Step iterative solution parameter. This parameter becomes available when the Method parameter is set to \u201cHorn-Schunck.\u201d","Velocity Difference Threshold:","Enter a scalar threshold value. This parameter is only visible if, for the Stop iterative solution parameter, \u201cWhen velocity difference falls below threshold\u201d or \u201cWhichever comes first\u201d is selected. This parameter becomes available when the Method parameter is set to \u201cHorn-Schunck.\u201d","Velocity Output:","If \u201cMagnitude-squared\u201d is selected, the block outputs the optical flow matrix where each element is of the Magnitude-squared form. If \u201cHorizontal and vertical components in complex form\u201d is selected, the block outputs the optical flow matrix where each element is of the horizontal and vertical form.","Temporal Gradient Filter:","Specify whether the block solves for u and v using a \u201cdifference filter\u201d or a \u201cderivative of a Gaussian filter.\u201d This parameter becomes available when the Method parameter is set to \u201cLucas-Kanade.\u201d","Number of Frames to Buffer for Temporal Smoothing:","Use this parameter to specify the temporal filter characteristics such as the standard deviation and number of filter coefficients. This parameter becomes available when the temporal gradient filter parameter is set to \u201cDerivative of Gaussian.\u201d","Standard Deviation for Image Smoothing Filter:","Specify the standard deviation for the image-smoothing filter. This parameter becomes available when the temporal gradient filter parameter is set to \u201cDerivative of Gaussian.\u201d","Standard Deviation for Gradient Smoothing Filter:","Specify the standard deviation for the gradient smoothing filter. This parameter becomes available when the temporal gradient filter parameter is set to \u201cDerivative of Gaussian.\u201d","Discard Normal Flow Estimates when Constraint Equation is Ill Conditioned:","Select this check box if the block should set the motion vector to zero when the optical flow constraint equation is ill conditioned. This parameter becomes available when the temporal gradient filter parameter is set to \u201cDerivative of Gaussian.\u201d","Output Image Corresponding to Motion Vectors (Accounts for Block Delay):","Select this check box if the block should output the image that corresponds to the motion vector being output by the block. This parameter becomes available when the temporal gradient filter parameter is set to \u201cDerivative of Gaussian.\u201d","Threshold for Noise Reduction:","Enter a scalar value that determines the motion threshold between each image or video frame. The higher the number, the less small movements impact the optical flow calculation. This parameter becomes available when the Method parameter is set to \u201cLucas-Kanade.\u201d","The parameters on the Data Types dialog box become visible only when the \u201cLucas-Kanade\u201d method is selected.","Rounding Mode:","Select the rounding mode for fixed-point operations.","Overflow Mode:","Select the overflow mode for fixed-point operations.","Product Output:","Use this parameter to specify how to designate the product output word and fraction lengths.","When \u201cBinary point scaling\u201d is selected, the word length and the fraction length of the product output in bits may be entered. When \u201cSlope and bias scaling\u201d is selected, the word length in bits and the slope of the product output may be entered. The bias of all signals in the Video and Image Processing Blockset blocks is 0.","Accumulator:","Use this parameter to specify how to designate this accumulator word and fraction lengths.","When \u201csame as product output\u201d is selected, these characteristics match those of the product output. When \u201cBinary point scaling\u201d is selected, the word length and the fraction length of the accumulator in bits may be entered. When \u201cSlope and bias scaling\u201d is selected, the word length in bits and the slope of the accumulator may be entered. The bias of all signals in the Video and Image Processing Block setblocks is 0.","Gradients:","Choose how to specify the word length and fraction length of the gradients data type. When \u201csame as accumulator\u201d is selected, these characteristics match those of the accumulator. When \u201csame as product output\u201d is selected, these characteristics match those of the product output. When \u201cBinary point scaling\u201d is selected, the word length and the fraction length of the quotient, in bits, may be entered. When \u201cSlope and bias scaling\u201d is selected, the word length in bits and the slope of the quotient may be entered. The bias of all signals in the Video and Image Processing Blockset blocks is 0.","Threshold:","Choose how to specify the word length and fraction length of the threshold data type: When \u201csame word length as first input\u201d is selected, the threshold word length matches that of the first input. When \u201cSpecify word length\u201d is selected, enter the word length of the threshold data type. When \u201cBinary point scaling\u201d is selected, the word length and the fraction length of the threshold, in bits, may be entered. When \u201cSlope and bias scaling\u201d is selected, the word length in bits and the slope of the threshold may be entered. The bias of all signals in the Video and Image Processing Block set blocks is 0.","Combinations of Techniques","The above example techniques, and others, can be combined in ways which enhance the accuracy and\/or lower the processing requirements of the automated tool tracking methods. For example, the video frames may be pre-processed to remove considerable detail before using optical flow techniques to identify the moving objects.","In an exemplary (non-limiting) embodiment of the present invention, the tool location is determined through a combination of these techniques. The video is received by the processor. The frames of the video are preprocessed by reducing the resolution of the video, removing noise and clutter by application of an appropriate filter, thresholding to further reduce the amount of information in the video, and eroding and dilating the objects in the video in order to further consolidate (i.e., simplify) the objects. Optical flow techniques are used on this pre-processed video to detect the movement (velocity) of the features of the video. The moving features are processed to determine regions of interest (i.e., blobs) and the centroids of the blobs are determined in each frame. This centroid is used as a proxy for the overall tool location. In the case of a stereoscopic video, the centroids from each image of a two-image frame are processed using, for example, a Kalman filter to determine the three-dimensional location of the centroid.","Although the present invention has been described with respect to one or more particular embodiments, it will be understood that other embodiments of the present invention may be made without departing from the spirit and scope of the present invention. There are numerous embodiments of the invention described herein including examples, all of which are intended to be non-limiting examples (whether explicitly described as non-limiting or not). Hence, the present invention is deemed limited only by the appended claims and the reasonable interpretation thereof."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"DESCRIPTION OF THE DRAWINGS","p":["For a fuller understanding of the nature and objects of the invention, reference should be made to the following detailed description taken in conjunction with the accompanying drawings, in which:",{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 1A"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":["FIG. 1B","FIG. 1A"]},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 2A"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 2B"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":["FIG. 4A","FIG. 3"]},{"@attributes":{"id":"p-0024","num":"0023"},"figref":["FIG. 4B","FIG. 3"]},{"@attributes":{"id":"p-0025","num":"0024"},"figref":["FIG. 5A","FIG. 4B"]},{"@attributes":{"id":"p-0026","num":"0025"},"figref":["FIG. 5B","FIG. 5A"]},{"@attributes":{"id":"p-0027","num":"0026"},"figref":["FIG. 6","FIG. 5B"]},{"@attributes":{"id":"p-0028","num":"0027"},"figref":["FIG. 7","FIG. 6"]},{"@attributes":{"id":"p-0029","num":"0028"},"figref":["FIG. 8","FIG. 7"]},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 9A"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 9B"},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 14A"},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 14B"},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 15A"},{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 15B"}]},"DETDESC":[{},{}]}
