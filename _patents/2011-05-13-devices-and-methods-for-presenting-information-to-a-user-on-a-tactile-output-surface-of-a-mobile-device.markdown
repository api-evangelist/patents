---
title: Devices and methods for presenting information to a user on a tactile output surface of a mobile device
abstract: Methods and devices provide a tactile output surface that can communicate information to users via their sense of touch. The tactile output surface may include a plurality of tactile elements which may be activated to represent various information in a manner that can be understood by touching the device. A mobile device may present tactile output surfaces on one or multiple surfaces, and the user may touch the device after of tactile feedback functionality to obtain information from the one or more tactile output surfaces. In an embodiment, a mobile device may be configured to obtain information from an information source and present the information on a tactile output surface so that the user can perceive the information without having to look at the device. A variety of technologies may be used to create actuatable tactile elements.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08717151&OS=08717151&RS=08717151
owner: QUALCOMM Incorporated
number: 08717151
owner_city: San Diego
owner_country: US
publication_date: 20110513
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD OF THE INVENTION","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["The present invention relates generally to mobile device user interface systems and more particularly to a haptics-based interface that provides information to a user.","Personal electronic devices (e.g. cell phones, PDAs, laptops, gaming devices) provide users with increasing functionality. In addition to serving as personal organizers, personal electronic devices serve as portals to the Internet and electronic mail. These devices allow users to access a wide range of information through their device, such as messages in multiple accounts, social networking sites, and, if configured with a GPS receiver, location and geographical distance information. Due to their portability, small size, communications capabilities and computing power, mobile devices application developers and users are creating new uses and functions for mobile devices.","The various embodiments provide devices and methods in which information can provided to mobile device users without generating a visual display or sounding an auditory output. The various embodiments allow users to \u201cfeel\u201d information provided by a mobile device so that others may not be aware of the output and so the device may remain out of sight (e.g., in a pocket or bag). In an example embodiment, information may be represented on the surface of a mobile device in tactile (e.g., raised or haptic) output surfaces.","The various embodiments include a tactile output surface coupled to a processor of the mobile device, and methods for presenting on the tactile output surface information from a variety of data sources. The embodiments may format information for presentation on a tactile output surface by scaling the information to match a range assigned to the tactile output surface, calculating a relative magnitude value by dividing the scaled information by the range assigned to the tactile output surface, and using the calculated relative magnitude value as the formatted information. Such information may be presented to the user by creating sensible features on a tactile output surface, in which the dimensions, shape, and\/or orientation of the sensible features on the surface. Creating sensible features on the tactile output surface may involve activating at least one tactile unit that creates a tactile effect that can be felt by a user touching the tactile output surface. Creating a tactile effect that can be felt by a user may involve raising a portion of the surface, depressing a portion of the surface, changing a roughness of a portion of the surface, vibrating a portion of the surface, generating an electrostatic field than can be sensed in skin of the user, changing a temperature of a portion of the surface, and combinations of these effects. The information that is presented on a tactile output surface may be any type of information, which may be obtained from internal or external data stores or generated by a function.","The various embodiments will be described in detail with reference to the accompanying drawings. Wherever possible, the same reference numbers will be used throughout the drawings to refer to the same or like parts. References made to particular examples and implementations are for illustrative purposes and are not intended to limit the scope of the invention or the claims.","The word \u201cexemplary\u201d is used herein to mean \u201cserving as an example, instance, or illustration.\u201d Any implementation described herein as \u201cexemplary\u201d is not necessarily to be construed as preferred or advantageous over other implementations.","As used herein, the terms \u201cmobile device\u201d, \u201cmobile computing device\u201d, and \u201ccomputing device\u201d, \u201crefer to any one or all of cellular telephones, personal data assistants (PDA's), palm-top computers, notebook computers, personal computers, wireless electronic mail receivers and cellular telephone receivers (e.g., the Blackberry\u00ae and Treo\u00ae devices), multimedia Internet enabled cellular telephones (e.g., the Blackberry Storm\u00ae), and similar electronic devices which include a tactile feedback surface on an exterior surface.","The term \u201ctap gesture\u201d is used herein to mean a touch or tap on a mobile device that the mobile device can sense based upon a touch to a touch sensitive surface, such as a touch screen or touchpad, or acceleration of the device as measured by an accelerometer.","As used herein, an \u201cinput event\u201d refers to a detected user input to a mobile device which may include key presses, tap gesturers, or a change in spatial orientation of the mobile device. For example, on a touchscreen or touchpad user interface device, an input event may refer to the detection of a user touching the device with one or more fingers.","Haptics is the branch of psychology that investigates cutaneous sense data. The term \u201chaptic\u201d is used herein to refer to devices that generate sensations in the skin that may be perceived by a person touching or nearly touching the device. As discussed below, there are technologies that can evoke a sense of touch even though the surface is smooth. Examples include electrostatic and vibrating surfaces.","As used herein, the term \u201ctactile output surface\u201d refers to various embodiments which are configured to communicate information by generating a surface feature that can be felt by a user through the sense of touch, such as through the finger tips. The various embodiments include different types of tactile output surfaces, such as surfaces configured to raise a portion of the surface to create a bump or raised portion that can be felt, haptic surfaces which can create a texture or sensation that can be felt through fingers (e.g., an electrostatic), and vibrating surfaces (e.g., surfaces with piezoelectric actuators) that generate localized vibrations that can be felt by a user. As used herein, a \u201chaptic output surface\u201d is a type of tactile output surface that uses haptic mechanisms. Since haptic output surfaces are example types of tactile output surfaces, references to \u201chaptic\u201d and \u201chaptic output surfaces\u201d should not be construed to limit the claims to any particular type of tactile technologies except as specifically recited in the claims.","As used herein, the term \u201ctixel\u201d (from the contraction of \u201ctexture\u201d and \u201cpixel\u201d) refers to a smallest portion of a texture-based tactile output surface that can be activated individually. For example, a tactile surface made up of a plurality of tixels may be configured such that tixels are arranged in a two-dimensional grid or array conceptually similar to pixels in a visual display. By individually actuating tixels, a mobile device processor can generate a tactile pattern that communicates information to a user via the user's sense of touch. Reference to tixels in the various embodiments described herein is made merely as one example tactile output surfaces that may be used, and is not intended to limit the embodiments or claim elements.","As used herein, the term \u201cvixel\u201d (from the contraction of \u201cvibration\u201d and \u201cpixel\u201d) refers to a smallest portion of a vibrating haptic surface that can be vibrated individually. For example, a tactile output surface made up of a plurality of vixels may be configured such that vixels are arranged in a two-dimensional grid or array conceptually similar to pixels in a visual display. By individually vibrating a pattern of vixels, a mobile device may generate a tactile pattern that can communicate information to a user via the user's sense of touch.","Personal computing devices rely upon user interface devices to receive commands and data inputs from and to provide output to users. A few types of user interface devices have become standard, including the keyboard, computer mouse, touchpads, touchscreen displays, and trackballs. Such conventional user interface devices may be specialized for particular types of input and\/or output tasks, such as entering text or typing commands (e.g., a keypad or keyboard), navigating within a graphical user interface (e.g., a computer mouse or trackball), graphically displaying information (e.g., an LCD monitor), and audio feedback (e.g., speakers). Touchscreens have become popular for some computing devices since they enable users to navigate a user interface and make inputs via a single user interface surface. Currently, mobile devices communicate information to users via either a display that the user must look at or audio sounds that can be heard by everyone nearby. The exception to this is Braille output devices that communicate through the sense of touch with those trained to read Braille.","Currently, touch sensing technologies, such as used in touchscreens, are also being widely developed and integrated into mobile device user interfaces to allow users to perform a variety of tasks using touch inputs. However, such technologies do not address the manner in which a device provides information and\/or feedback to a user.","Today, there are also technologies that enable a mobile device to execute a function or command with minimal user interaction, such as voice activated calling, key press shortcuts, touch screen taps, etc. For example, three taps to a mobile device, which may be sensed by accelerometers in the device, may be interpreted as a user input command to advance an mp3 player to the next song. A benefit of such minimal user interface techniques and technologies is that users do not need to take the mobile device out of their pocket or bag and unlock it in order to accomplish a given task. There are also many systems today that allow a user to set a mobile device to output event alerts (e.g., an alarm, incoming call, new text message, etc.) by vibrating. For example, users frequently set their cell phones to vibrate mode in circumstances in which audio alerts would be disruptive, such as during a meeting or in quiet area.","A shortcoming of conventional information output devices and mechanisms is their inability to communicate information to the user without requiring the user to look at or listen to the mobile device. Further, the current types of vibration settings on devices as an alternative to audio alerts inform users only of the occurrence of an event for which the vibration mode is set (e.g., an incoming phone call). Thus, users not trained to read Braille have no options for receiving information from their mobile devices except looking at their displays or setting them to speak or otherwise audibilize the information in a manner that is not private.","To overcome these limitations, the embodiments utilize a variety of tactile or haptic technologies that allow users to feel information by touching their mobile devices. In the various embodiments, a tactile output surface may function similar to a visual display for outputting information in a manner that a user can \u201cfeel\u201d in order to communicate information without the user having to look at or listen to the mobile device. For example, a user may feel information while a device remains in the user's pocket, thereby leaving the user's vision focused on driving, or not divulging that the user is checking the mobile device in a meeting. Rather than feedback from a mobile device being a generalized event (e.g., vibrating to indicate reception of a new message), a tactile output surface of the various embodiments may be localized to specific regions of the mobile device, and the location of tactile features or haptic actuations may convey information. Further, a tactile output surface may be placed on any surface of the mobile device. For example, a tactile output surface may be positioned on the back of a mobile device, and thus can supplement a visual display that may be on the front surface. As another example, tactile elements may be implemented on a display, such as a touch screen display, to convey information to users via their sense of touch as well as visually.","The sense of touch is initiated by cutaneous sensory receptors (i.e., sensory neurons) such as in the finger tips. Various types of cutaneous receptors have different preferential activation thresholds for sensing movement, pain, pressure, vibration, and temperature, and thus the sense of touch is different on different parts of the body. Receptors for every sensory modality, including the sense of touch, are limited by the amount of stimulation necessary to elicit a sensation (absolute threshold). The absolute threshold of a tactile feature that can be felt depends on the spatial resolution required for stimuli, which for tactile senses is determined in large part by the density of cutaneous receptors. In the human somatosensory system, fingertips are among the areas with the highest spatial resolution of receptors, and the brain process sensory input from the hands and fingers with a high degree of discrimination relative to the sense of touch of other body parts. The resolution of peripheral mechanoreceptive units (touch receptors) in the finger tips is approximately 1 mm, so surface features smaller than this may not be resolved (i.e., sensed as separate raised areas). In addition to surface textures and raised features, humans can sense vibrations. Taking into account the various receptor types, vibrations may be felt at 10-1000 Hz, with an optimum frequency at approximately 250 Hz. In addition to these senses of touch, humans can also feel electrostatic potentials on a surface and temperature differences (hot or cold compared to ambient or body temperature). Any and combinations of these senses of touch may be used in a tactile output surface.","The relative sensitivity of the various sensory modalities may be determined by comparing the amount of stimulus change that can be detected by each sense for typical users. The difference threshold, also called the \u201cjust noticeable difference\u201d (jnd), is the smallest physical difference between two stimulus conditions that a person can detect. According to Weber's law, the jnd for each sensory modality is equal to a percentage of the first stimulus, and the percentage holds constant regardless of the magnitude of the first stimulus.","While the jnd of photopic vision in humans is approximately 0.017, the jnd is approximately 0.143 for pressure applied to skin, and approximately 0.140 for vibrotactile stimulation (with slight variation based on frequency). Thus, while a visual stimulus need only change by 1.7% to be detectable, a vibratory stimulus must change by 14% to be reliably detected. Also, tactile senses modalities will have resolution characteristics (i.e., distance between two tactile features that can be perceived by a user) which are more coarse than the resolution provided by the sense of sight. Therefore, since touch is comparatively much less sensitive than is vision in humans, a challenge is naturally presented in designing tactile output systems that can generate outputs that may be unambiguously perceived by users.","The various embodiments account for the unique characteristics of the sense of touch by formatting specific types of information into localized units that may be easily understood by users through tactile perception. Specifically, the various embodiments may use comparative or relative information presentation forms rather than absolute information forms typically employed in visual displays. For example, the embodiments may present information on tactile output surfaces in the form blocks or bars which communicate relative magnitude information in terms of the size of blocks or length of bars. Analogous to the transfer of metadata for data content, information that may be presented to users via tactile output surfaces may communicate a property of a data set, in contrast to visual displays which would display the information itself According to an exemplary embodiment, a mobile device may be configured to obtain information of interest to a user, format it consistent with a tactile output surface, and communicate it to the user through appropriate actuation of the tactile output surface. Such information may be within the mobile device itself or obtained via a network connection, such as via the Internet.","In the various embodiments, tactile output surfaces may be implemented in mobile devices using a variety of different technologies that create surface contours (e.g., bumps or ridges) that may be felt, or apply forces, vibration or electrostatic charges to the skin that can be felt. Examples of tactile and haptic technologies include, but are not limited to: actuators that can raise a portion of the surface to create a peak, bump, ridge or other raise shape; fluidic actuators that can raise a blister or other shape in response to pressure of a fluid in the surface being increased; piezoelectric actuators that may change shape or vibrate in response to an applied electrical signal; capacitive surfaces that can apply an electrostatic potential to the surface that can be sensed; electroactive polymers that may change shape or vibrate when actuated by an electrical signal; electrostatic actuators; thermal output circuits (e.g., resistive heaters or thermoelectric chiller circuit elements), to name just a few.","One embodiment of a tactile output surface uses physical actuators to raise perceivable portions of a surface (i.e., portions of a surface large enough to be felt by a user's fingers). A number of known types of actuator technologies may be used in such surfaces, some examples of which are described below with reference to .","In another embodiment, a tactile output surface may be configured using electrostatic technology, such as the E-Sense\u2122 technology developed by Senseg (Helsinki, Finland). The E-Sense\u2122 technology uses a positively charged membrane that can be layered over a liquid crystal display (LCD) or other surface (e.g., sides and\/or back) of a mobile device. The E-Sense\u2122 technology membrane utilizes Coulomb forces to \u201ctug\u201d on human skin, which is typically negatively charged, in order to produce a tactile sensation. Currently the E-Sense\u2122 technology is able to create ten tixels within a 3 inch\u00d74 inch output surface, and higher resolutions may be developed in the future. However, this is but one haptic technology that may be used in the various embodiments.","Another example haptic technology that may be used in the various embodiments involves generating small vibrations by localized vibration generators, such as piezoelectric crystals that may be integrated into the surface of a mobile device. By individually energizing such piezoelectric elements with an alternating current or signal of an appropriate frequency, small vibration dots or \u201cvixels\u201d may be generated that users may sense with their finger tips. As mentioned above, the frequency of the signal applied to such piezoelectric vibration may be between about 10 Hz and about 1000 Hz.","As mentioned above, the sense of temperature may also be used in a tactile output surface. However, the resolution of the thermal sense of touch may not enable fine resolution thermal output surfaces. For this reason, thermal elements may be included as an auxiliary or augmenting sensation that is combined with other types of tactile or haptic technologies.","In various embodiments, a tactile output surface may be configured as a two-dimensional array of tactile units, such as moveable bump features, vibrators, electrostatic features, etc. that are individually actuatable by a processor. Each individually actuatable tactile element may be referred to and processed as a \u201ctixel\u201d or \u201cvixel.\u201d A tixel array may contain of any number of tixels controlled by a processor. In the various embodiments, a processor may individually activate each tixel, analogous to the individual activation of pixels in a raster image.","Similar to how pixel size and pixel density define the resolution of visual displays, the size and spacing of individual tactile elements (or tixels) defines the \u201cresolution\u201d of a tactile output surface, which is the inter-element distance or number of elements per inch that a user can distinguish. The resolution of a tactile output surface will typically be limited by the resolving ability of the sense of touch modality of the tactile element (e.g., raised surfaces, vibration, electrostatic forces, etc.), as well as physical limitations imposed by the mechanism used in the tactile element. The sense of touch associated with feeling surface features, such as bumps, is typically on the order of a millimeter or so. The Marburg Medium Braille Specification requires that Braille dots have a diameter of 1.6 mm (which implies a height of approximately 0.8 mm), and that the dots be spaced apart by 2.5 mm from dot center to dot center, with the inter-character spacing set at 6.0 mm. The American Braille Technical specifications require that dots be 0.020 inches in height, and 0.09 inches apart with an inter-character spacing of 0.240 inches. Thus, a tactile output surface based on raised bumps will likely have a resolution no better than about 1.6 mm or 0.09 inches, or about 11 dots per inch (DPI) based only on the sense of touch. The resolution of the tactile output surface may be less than that if the mechanisms used to raise such bumps cannot be placed within 0.09 inches of each other. Since the haptic perception of vibration may require a larger area to be perceived (i.e., a larger vibrating dot), vibrating tactile output surfaces may have lower resolution capabilities. As mentioned above, the E-Sense\u2122 technology currently has a resolution that enables ten tixels within a 3 inch\u00d74 inch output surface.","Each tixel may have its own address that the processor uses to actuate it. For example, the address for each tixel may correspond to its coordinates within the tixel array. A processor outputting information to a user on a tactile output surface may assign to each tixel a value that is dependent upon on the information to be presented to the user. In an example embodiment, an individual tixel may have assigned a value of either \u201con\u201d or \u201coff\u201d, creating a 1-bit per tixel \u201ctixmap\u201d. The tixmap may be stored in a tactile output buffer similar to a display buffer used for generating images on a visual display.","In some embodiments, individual tactile elements may be capable of more than one output setting (similar to how pixels may vary in brightness and color). For example, a tactile element based on an actuator, piezoelectric crystal or electroactive polymer may be able to generate raised \u201cbumps\u201d with a range of heights. In an embodiment, a processor may control the height of such tactile elements by assigning a value to the tixel that corresponds to the relative height to which the element should be raised. Thus, the data in the tactile output buffer may indicate both whether a tactile element is to be actuated and the degree or magnitude of the actuation.","In a further embodiment, multiple types of tactile modalities may be implemented on a given tixel, such as elevation and vibration (e.g., by applying an alternating current with a bias to a piezoelectric element), elevation and electrostatic, elevation and thermal, vibration and electrostatic, vibration and thermal, and electrostatic and thermal. In such embodiments, the data in the tactile output buffer may indicate actuation of a tactile element, plus the specific modality and magnitude to be implemented.","In order to output tactile information on a tixel array, a processor or dedicated tactile output circuitry may read the values stored in a tactile output buffer for each tixel address and send actuation signals to each tactile element accordingly. The processor or tactile output circuitry may generate activation signals, such as corresponding to an \u201con\u201d value for selected tixels within the tixel array. In this manner, multiple activated tixels within a tixel array may be activated to create a variety of shapes of different sizes on a tactile output surface.","In various embodiments, multiple tixels may be activated in patterns or groups on a tactile output surface to communicate information to a user. Users may perceive the information by feeling the tactile characteristic (e.g., shape, size, etc.) of the activated portions of the tactile output surface. By correlating the dimensions, shapes and other characteristic to the information to be communicated, relatively sophisticated information can be communicated to users. In an example embodiment illustrated in some of the figures, the tactile characteristics used to communicate may be the length of the activated portion of the tactile output surface (the \u201chaptic activation area\u201d) relative to the dimensions of the tactile output surface. The haptic activation length may correspond, for example, to relative magnitude information. Thus, if half of the tactile output surface is activated, a user would understand the magnitude of the indicated value is approximately 50 percent or about half of the maximum value indicated by the full length of the surface. In this manner, a dimension of the activated portion may communicate relative magnitude information to the user. Provided the user knows the meaning of the full length of the output surface, such a relative indication can provide very useful information in an easily perceived manner. Other example tactile characteristics that may be used to communicate relative magnitude information may include, but are not limited to, area, shape, width, and orientation of the haptic activation area.","Some illustrative examples of relative magnitude information that may presented to users in various use cases include, but are not limited to: the relative number of unread emails when the data set is the content of a folder in an email account; the number of new voicemails when the data set is the content of a voice mailbox; the amount time remaining until an event when the data set is information in an electronic personal organizer or calendar; and distance or time until a location is reached when the data set is the device's current geographic location. Relative magnitude information presented on a tactile output surface may also be associated with a status of the mobile device, for example, the amount of remaining battery charge, the cellular signal or wireless signal strength, etc.",{"@attributes":{"id":"p-0055","num":"0054"},"figref":["FIG. 1A","FIG. 1B"],"b":["100","102","104","105","100","104","102","100","104","100","102","106"]},"A mobile device  may be configured with one or more tactile output surfaces on the back surface of the mobile device , as is illustrated in . As previously discussed, a characteristic of the activated portion of the tactile output surface (e.g., length\/height), in comparison to the entire tactile output surface, may communicate relative magnitude information to a user. Tactile perception of a relative size characteristic may require a user to have the ability to readily determine the physical borders of the tactile output surface. Where a border of the tactile output surface aligns with the border of, for example, a display on the mobile device, a user may easily feel the physical boundary.","However, where the tactile output surface is implemented on another surface of the mobile device, such as the back surface, the only boundaries discernable by touch may be the edges of the mobile device. Accordingly, a mobile device may include tactile borders that enclose each tactile output surface, with the boarders formed by ridges or grooves. In an example embodiment illustrated in , grooves  on surface  may surround tactile output surfaces  to provide tactile boundaries distinguishing each region of tactile output surfaces. In another example embodiment illustrated in , a tactile boundary may be ridge  surrounding the tactile output surface . Grooves  and ridges  are merely two examples of embodiment tactile borders that may be provided on the mobile device .","In an embodiment, a user may make an input to a mobile device to initiate a function that obtains data and provides the user with requested information via a tactile output surface. The mobile device may recognize a user input, and determine from the type or pattern of the input a function to be performed or a type of information to be obtained and presented to the user. User inputs may be configured to enable the user to unambiguously signal the mobile device without looking at it, such as in the form of a particular button press, a tap or pat of the mobile device, moving the mobile device in a certain pattern or direction, holding the mobile device in a particular orientation with respect to the force of gravity, etc. Such user inputs may be sensed by accelerometers included in the mobile device and coupled to the processor.","In the various embodiments, a user input event may signal the mobile device to activate an application to obtain data from one or more sources and to present the obtained information a form compatible with the device's tactile output surface. The manner in which obtained information is presented on a tactile output surface may depend upon the type of data, user settings and the value or magnitude of values in the data. For example, the mobile device may determine, from settings stored in memory, the type of information to be presented on a tactile output surface for the application activated by the user input. Further the format for presenting data on a tactile output surface may be adjusted or specified in user settings, which may be entered via a user interface menu application.","The mobile device  may also be configured to request data from other devices and to present information associated with the received data set in a tactile output surface. This embodiment may enable users to obtain information from Internet websites, or mobile devices or computers accessible via the Internet, and present the information on the tactile output surface. For example, a parent may configure their mobile device to recognize a particular user input (e.g., a double tap) as a command to execute an application that accesses a website which tracks the location of their child's cell phone, requests the current location of their child, and then presents the data on a tactile output surface in the form of a haptic activation area. In this example, the mobile device may send a pull request to the child's mobile device or a server in communication with that device by communicating through a cellular data network (e.g., by sending an SMS message or making a data call to an Internet access server). The child's mobile device may respond by sending its own GPS coordinates, or by sending its current distance from a fixed pre-set destination, such as distance from home. Distance information received by the mobile device may be converted to a relative magnitude, such as a percentage of a preset distance that corresponds to the full length of a tactile output surface. Thus, if the received information is 5 miles, and the preset distance corresponding to 100 percent of the tactile output surface is 10 miles, the half of the tactile output surface may be activated to convey this information to the user. A mobile device configured with a tactile output surface comprising an array of individually actuatable bumps (like a Braille array) could present the location information in the form of a map of dots that a person may comprehend by feeling the surface.","Further, a mobile device may be configured to update the obtained information and reflect changes in the information by updating the haptic activation area periodically. In an example embodiment, the mobile device  may maintain an active wireless link to the Internet, and the user may configure the mobile device  to automatically refresh one or more of the downloaded data sets on a preset period so that the haptic activation area reflects the updated data. Refreshing data may be accomplished, for example, by the mobile device requesting a new data packet. The new data packet may contain data previously sent as well as new data acquired over the time interval.","To enable representation of numerical information on a tactile output surface , the mobile device  may apply a scaling factor to the information. The mobile device  may be further configured to transform the scaled information into signals to activated particular tactile unit according to present the scaled information on the tactile output surface. Tactile units with an \u201con\u201d value in a tactile output surface  constitute the haptic activation area.","Similar to a representative fraction shown in a map legend, a value for each set of information may be scaled to a \u201ctixel ratio\u201d to enable representation on a tactile output surface. In one example embodiment, a maximum value for tactile representation may be pre-set for each tactile output surface (for example, the numerical value corresponding to activation of all tixels), and the tixel ratio may be stored in memory of the mobile device. In an alternative embodiment, a percentage scale factor for each tactile output surface may be pre-set and stored in memory of the mobile device. In applying the percentage scale factor to a set of information, the mobile device may compute an optimum tixel ratio with which to present the information on the tactile output surface.","Depending on the size of the device surfaces and on the resolution provided by the tactile technologies, a mobile device may be capable of presenting more than one tactile output surfaces.  illustrates an example embodiment mobile device  configured with four tactile output surfaces , , and on a back surface of the device. For example, the lengths of haptic activation areas , , , may represent four sets of relative magnitude information, which may be associated with the same or different data sets. The length of a haptic activation area may be measured as the distance from a bottom margin to top portion , with this distance comparable by a user to the full length of each tactile output surface , , and . In this manner, relative magnitude information may be represented in the length of the haptic activation area in comparison to the length of the entire tactile output surface . Since the user can easily feel how much of the surface is raised or activated, the user can quickly comprehend the relative magnitude information being presented without having to look at the mobile device. For ease of reference, such linear tactile output surfaces are referred to herein as \u201cprogress bars\u201d because the length of the haptic activation area can easily communicate relative information, such as current progress towards a goal, objective, limit or other value represented by the length of the tactile output surface. However, the information conveyed by progress bars is not limited to a progress or progression value.","As an illustrative example, a tactile output surface may be configured to communicate to a user the number of new text messages in the user's inbox. Referring to the first tactile output surface shown in , a characteristic such as the length of haptic activation area may represent the number of new text messages in the user's inbox, for example. For example, the tactile output surface may be configured such that the length of the corresponding haptic activation area corresponds to the range of 0-20 messages. Thus, if no portion of the tactile output surface is activated this may inform the user that there are no messages in the inbox, while if the activated area extends to the top of the tactile output surface this may inform the user that there are 20 new messages in the inbox. So, the relative magnitude information (i.e., number of new text messages in this example) represented by the haptic activation area shown in  would inform the user that there are approximately 10 (9-11) new text messages. Examples of different types of relative magnitude information are discussed in further detail below, with respect to .",{"@attributes":{"id":"p-0066","num":"0065"},"figref":["FIG. 2B","FIG. 2B","FIG. 2A"],"b":["204","204","204","100","214","214","206","206","206","204","204","204","206","206","206","204","204","204","204","204","204","204","204","204","204"],"i":["a","b","c ","a","b","c","a","b","c ","a","b","c ","a","b","c","a","b","c","a","b","c ","d. "]},{"@attributes":{"id":"p-0067","num":"0066"},"figref":"FIG. 2B","b":["204","204","204","105","100"],"i":["a","b","c "]},"In embodiments in which numerical information that has no natural upper limit is to be presented to a user, representation of relative magnitude on a tactile output surface may be enabled by assigning a given range of values to the bottom and top of each tactile output surface. In this manner, multiple adjacent tactile output surfaces (e.g., illustrated in ) may be used to communicate numbers based on the percentage of the surface that is activated. For example, numbers between 0 and 999 could be presented in a base 10 format, by a first tactile output surface representing units by the distance the haptic activation area extends from the bottom (which represents 0) and the top (which represents 9), a second tactile output surface representing tens by the distance the haptic activation area extends from the bottom (which represents 10) and the top (which represents 90), and a third tactile output surface representing hundreds by the distance the haptic activation area extends from the bottom (which represents 100) and the top (which represents 900).","In another embodiment, numbers greater than the range assigned to the length of a tactile output surface may be presented by reversing the orientation of the haptic activation area within the surface. Presenting haptic activation areas in an \u201cupside down\u201d orientation on the tactile output surface may communicate that the value is greater than the range encompassed by the surface in the normal orientation, so that the relative magnitude may with respect to a larger number range.  illustrates an example of an exemplary embodiment mobile device  with tactile output surfaces and presenting upside down haptic activation areas , . \u201cRight side up\u201d haptic activation area may represent information that is within the number range assigned to the length of the surface. Upside down haptic activation areas ,  may represent numerical information that exceeds the value represented by top point ","Continuing with the previous example of a number of email messages within a user's inbox, a right side up haptic activation area within a tactile output surface (e.g., surface) may convey the number of new voicemail messages within a range of 0 at the bottom to 20 at the top , while an upside down haptic activation area ,  may convey the number of email messages from 21 at the top and 40 at the bottom . In this manner, presenting upside down haptic activation areas on the tactile output surface may expand the range of information that may be represented on the tactile output surfaces  without compromising resolution. Thus, a single tactile output surface may present numerical information spanning twice that which is feasible given the resolution of the type of tactile element modalities.","In an alternative embodiment, an upside down haptic activation area ,  may be used to represent information based on different units, a different scale factor, etc. In other alternative embodiments, properties other than orientation of the haptic activation areas (for example, shape, width, etc.) may be used to convey information in a different range.",{"@attributes":{"id":"p-0072","num":"0071"},"figref":"FIG. 3","b":["300","100","302","304","100","304","100","306"]},"If the detected event is above the threshold level (i.e. determination block =\u201cYes\u201d), the mobile device may determine whether the detected event is a recognized user input event, at determination block . This determination may be with reference to a user's pre-programmed settings that may designate the type of input event(s) to be recognized as triggers for activating an application (such as an application to gather information to present the gathered information on a tactile output surface). If the detected event is not recognized as a user input event (i.e., determination block =\u201cNo\u201d), the mobile device  may ignore the event and remain in the low-power state at block . If the detected event is recognized by the mobile device as a user input event (i.e. determination block =\u201cYes\u201d), the mobile device may activate the application that is correlated or linked to the recognized user input at block . Any form of application may be activated in this manner, but for the sake of illustrating aspects of the various embodiments, the description of  continues presuming that the activated application is one that obtains data from a data source and then presents the obtained data in suitable format on a tactile output surface.","At block , the mobile device processor implementing the activated application may access data from a pre-selected source define for the application or that is associated with the recognized user input event. The data source may be internal, such as information stored in an internal memory, external, such a server accessed via a communication network, or an algorithm or calculation that is performed to generate the data, such as a calculation performed based upon information obtained from internal and\/or external sources. Any conventional software and communication mechanism may be used to obtain the information.","At block , using the obtained data the mobile device processor may reformat or scale the obtained information, or generate a value or relative magnitude based upon the obtained information that is suitable for presentation on a tactile output surface. As described above, the course resolution of the tactile output surface may be better suited to presenting information in terms of relative magnitudes or levels within a given range. Therefore, the operations in block  may transform a wide variety of obtained information into such a relative format. At block , the mobile device processor may also scale the information to match the size, resolution or given range of values of a tactile output surface. Scaling processes are discussed in more detail below with reference to . At block , the mobile device processor may generate the signals that activate the tactile units on a tactile output surface to generate a pattern representative of the scaled information.","In various embodiments, user inputs may also be made through a touchscreen. Further, determining whether a user input event is above a threshold in determination block  may be performed in the same step as detecting a user input event. For example, in a mobile device  with a capacitive touchscreen, a user input event may be recognized based on a change in capacitance above a threshold amount resulting from a finger touching the surface, where the threshold for a user input event is also the trigger threshold in determination block .",{"@attributes":{"id":"p-0077","num":"0076"},"figref":"FIG. 4","b":["400","100","406","414","404","405","402","414","414","406","412","412","406"]},"Users may communicate with the application  through, for example, a keypad  and\/or an accelerometer , such as to activate the application or cause it to execute certain functions. The accelerometer  may be, for example, a three-axis accelerometer, and may further be coupled to various other sensors in the mobile device. Example user input events to activate an application include, but are not limited to, depressing a physical key, or touching a virtual key, on keypad , and moving the mobile device such that the change is detected by the accelerometer .","Information received through keypad  and\/or accelerometer  may be communicated to the operating system  and via drivers . Drivers  translate information and commands between the keypad  and\/or accelerometer , and the operating system . The information received via the drivers  may be communicated to the application  via an application programming interface . The application  may present information to users interfacing with the operating systems  to cause actuation of tactile output surface actuators , such as E-Sense\u2122 elements, surface vibration elements, or surface actuators some examples of which are discussed in further detail below with reference to .",{"@attributes":{"id":"p-0080","num":"0079"},"figref":["FIGS. 5A-5C","FIG. 5A"],"b":["504","104","502","502","504","504","504","504","504","504","502"],"i":["a","b","c","a","b","c "]},{"@attributes":{"id":"p-0081","num":"0080"},"figref":["FIG. 5B","FIG. 5A","FIG. 5B"],"b":["504","502","504","502","504","502","506","506","506","502","24"],"i":["a ","b ","c ","a","b","c"]},{"@attributes":{"id":"p-0082","num":"0081"},"figref":["FIG. 5C","FIG. 5C"],"b":["102","100","104","102","104","102","507","507","507","106","507","106","507","507"],"i":["a","b ","a ","b ","a","b "]},"The type of information represented on a tactile output surface may be determined according to a number of different settings on the mobile device, including, but not limited to, user selections, factory settings, as defined by an application, etc. In an exemplary embodiment, the information may be pre-selected by users and stored in memory in the mobile device, such as in a user profile data file.","In an example embodiment, the mobile device may present users with a variety of different information types and may prompt the user to select a type to be represented on a tactile output surface of the mobile device. In another example embodiment, the mobile device may be configured to utilize more than one tactile output surface at a time, and may prompt the user to select a type of information to be presented on each tactile output surface. Further, the mobile device may include hardware enabling presentation of more than one type of tactile output surface. For example, the mobile device may display for the user a variety of tactile properties (e.g., \u201cvibration\u201d, \u201cwarmth\u201d, \u201cbump elevation\u201d, \u201croughness\u201d, etc.) and may prompt the user to select a property for each type of information that may be represented in a tactile output surface.","In various embodiments, the mobile device may be configured to determine the type of information to present to the user by referencing a user-configurable data table  that may be stored in memory, an example of which is illustrated in . In such a data table , column  of data may list recognizable user input events, and a column  of data may list corresponding sets of information that are to be presented on a tactile output surface. In addition, the data table  may include values of associated parameters for generating a tactile representation of information, such as, for example, a representation range in column  and a ratio or value to be assigned to each tactile units (e.g., a tixel ratio) in column . In the illustrated example, the first row indicates that the mobile device recognize detect two taps (listed in column ) as indicating that the information requested by the user, listed in column , is the number of new emails in the user's inbox. The data table  further identifies the range \u201c0-20\u201d in column  as the number of new emails to be represented by the length of the haptic activation area on a tactile output surface, with each activated tixel representing four new emails in the user's inbox, as listed in column .","As discussed above, more than one tactile output surface may be present on the surface of a mobile device, with each used to present information from different data sets. To support such an embodiment, a configuration data table  may include more data elements to specify factors associated with each data set, such as illustrated in . For example, the data table  may be organized to list information or units, , , range , , and tixel ratio , , for each tactile output surface and for each type of user input event.",{"@attributes":{"id":"p-0087","num":"0086"},"figref":["FIGS. 7A and 7B","FIGS. 6A and 6B","FIG. 7B"],"b":["700","100","702","704","704","704","708","600","650","708","710"]},"If the processor recognizes that the user input event is associated with email information (i.e., determination block =\u201cYes\u201d), the processor may send a request for the pending messages in the \u201cInbox\u201d to a designated email server in block . The email server address may be predefined by the user in a registration step. This request for information may be accomplished by the processor establishing a wireless communication link to the Internet via a wireless data network, and then sending an appropriate query message to the email server via the Internet. At block , the mobile device processor may receive and store in memory the data requested from the server. At block  the mobile device processor may determine from the received data the number of unread email messages. At block , the mobile device processor may scale the number of emails to fit the tactile output surface according to pre-defined settings, which may be user defined or modified. At block  the mobile device processor may activate tactile units to present a haptic activation area in a first tactile output surface that has a size and\/or shape representing the scaled information regarding the number of emails in the user's inbox.","Turning to , method  may continue at determination block  with the processor determining whether the user input event is associated with distance information, such as the distance from home. If the gesture is not a user event associated with distance information (i.e., determination block =\u201cNo\u201d), the mobile device may return to the default inactivated state. If the user input event is associated with distance information (i.e., determination block =\u201cYes\u201d), the mobile device may activate a Global Positioning System (GPS) receiver at block . The device's GPS receiver may receive radio signals from satellites at block , and calculate position information at block . At block  the mobile device processor may compare the location information received from the GPS receiver to a present location, such as the user's home, and calculate the distance between these two points. At block  the mobile device may scale the computed distance from home based on tixel units (i.e., the number of distance units corresponding to each tixel in the tactile output surfaced). At block  the mobile device may activate tixels accordingly to generate a haptic activation area in a second tactile output surface representing the distance information.","As discussed above, a variety of technologies may be used in tactile output surfaces. In an embodiment, the tactile output surface may be configured with tactile rendering technology to form virtual objects with different shapes, textures, etc. Virtual objects may be created through forces generated by a tactile interface that manipulates forces and geometry in touch \u201cillusions\u201d based on how the human brain would perceive the information from a real object. This is analogous to the manner in which optical illusions may be created by manipulating the way humans' perceive visual information.","In another embodiment, tactile units may be raised\/elevated from a tactile output surface when activated.  illustrates a mobile device  configured to represent information on a surface  with raised tactile units. Tactile units -may form a first tactile output surface , and tactile units -may form a second tactile output surface . In an example embodiment, raised tactile units  may be in the shape of buttons or keys when activated. As illustrated in , the raised tactile units  may be dome-shaped. A variety of technologies may be used to raise the tactile units , including fluidic pressure, electrostatic pressure (see e.g., ), mechanical actuators (see e.g., ), piezoelectric actuators, electroactive polymers, etc. In an example embodiment, each tactile unit  may be formed from electroactive polymers which when activated by an electrical signal change shape, such as expanding in dimensions, causing the material to rise up in the manner illustrated. In another example embodiment, a fluid may be pumped into a blister formed by a flexible cover layer sealed around the edges of the tactile units. The mobile device processor may be configured to open and close valves connecting each blister to a fluid reservoir to enable the activation and deactivation of each tactile unit.",{"@attributes":{"id":"p-0092","num":"0091"},"figref":["FIGS. 9A-9D","FIG. 9A","FIG. 9B"],"b":["804","906","912","902","904","906","908","910","912","912","912","914","902","904","902","904","906"]},"As shown in , a plurality of the tactile element actuators  may be closely spaced and configured as an array of tactile elements. Such a configuration may enable actuation of different portions of the pliable surface layer  in response to individual signals applied by the processor via connectors  in order to create haptic activation areas of different shapes and sizes.  illustrates a tactile output surface  without any tactile units activated, and accordingly none of the tactile element actuators  are energized.  illustrates the mobile device  in a mode in which information is represented on the surface by multiple raised tactile units, with a haptic activation area  formed by energized actuators . By organizing or configuring the tactile element actuators  to be individually energized and arranging them in an array, a wide variety of different shapes of tactile units can be generated.","In various embodiments, raised tactile units may be activated using piezoelectric mechanisms. Piezoelectric materials change shape (e.g., elongate) in the presence of an applied electric field. The piezoelectric elements may be coupled directly to a surface of the mobile device , or may be coupled to mechanical pins which contact the surface response to individual activation signals, such as to generate raised tactile units , as shown in . Electric fields used to actuate piezoelectric elements may be individually connected to the elements by a processor, such as through an interface of solid state switches that connect the elements to a voltage source. Multiple independently actuating piezoelectric elements can be controlled by a processor to generate any of a variety of raised surface shapes.","An example of a piezoelectric element that may be used in various embodiments is Macro Fiber Composite (MFC), manufactured and sold by Smart Material Corp. of Sarasota, Fla. The MFC comprises rectangular piezo-ceramic rods sandwiched between layers of adhesive and electroded polyimide film. This film contains interdigitated electrodes that transfer applied voltages directly to and from the ribbon shaped rods. This assembly enables in-plane poling, actuation, and sensing in a sealed, durable, ready-to-use package. When embedded in a surface or attached to flexible structures, the MFC can provide distributed solid-state deflection and vibration control or strain measurements.","A further example of a tactile element mechanism utilizing electrostatic forces is illustrated in . In an example embodiment, the surface  of mobile device  may include a top surface layer  and a bottom support layer  which may be selectively energized by a processor, such as applying a voltage (e.g., Vcc). If the top surface layer  and bottom support layer  are separated by an insulator layer , charges applied to the top and bottom layers may result in electrostatic repulsive or attractive forces. By configuring the top surface layer  with a pliable portion , when the voltages of the same polarity are applied to both the top surface layer  and bottom support layer  as illustrated in , the electrostatic forces may cause the pliable portion  to raise above the rest of the top surface layer , providing a raised tactile unit  in a tactile output surface. As illustrated in , the tactile unit  may be retracted and the surface  may return to being smooth by coupling one or both of the top surface layer  and bottom support layer  to ground, or by applying voltages of opposite polarities to the top surface layer  and bottom support layer .","In another embodiment illustrated in , the electrostatic mechanism illustrated in  may be used to cause a surface depression (e.g., a dimple) in a portion of the surface rather than a raised portion. In this embodiment, the bottom surface support layer  may be provided with a depressed or dimpled portion . When voltages of opposite polarity are applied to the top surface layer  and bottom support layer , the top surface layer  will be pulled towards the dimpled portion  of the bottom surface support layer , generating a dimpled portion in the top surface. Such a dimple in the top surface layer  may be felt by a user running their finger over the surface.","In a further embodiment, the mechanisms illustrated in  may be combined so that the top surface may have three tactile configurations, flat, raised and depressed.","The embodiments may be implemented in a variety of mobile devices, particularly mobile computing devices. An example of a mobile device that may implement the various embodiments is a smart phone  illustrated in . A multi-processor mobile device, such as a smart phone , may include a processor  coupled to memory  and to a radio frequency data modem . The modem  may be coupled to an antenna  for receiving and transmitting radio frequency signals. The smart phone  may also include a display , such as a touchscreen display. The mobile device may also include user input devices, such as buttons , to receive user inputs. In the various embodiments the smart phone  includes a tactile output surface, which may be positioned on the display  (e.g., using E-Sense\u2122 technology), on a back surface , or another surface of the mobile device .","The mobile device processor  may be any programmable microprocessor, microcomputer or multiple processor chip or chips that can be configured by software instructions (applications) to perform a variety of functions, including the functions of the various embodiments described herein.","Typically, software applications may be stored in the internal memory  before they are accessed and loaded into the processor . In some mobile computing devices, additional memory chips (e.g., a Secure Data (SD) card) may be plugged into the mobile device and coupled to the processor . The internal memory  may be a volatile or nonvolatile memory, such as flash memory, or a mixture of both. For the purposes of this description, a general reference to memory refers to all memory accessible by the processor , including internal memory , removable memory plugged into the mobile device, and memory within the processor .","The foregoing method descriptions and the process flow diagrams are provided merely as illustrative examples and are not intended to require or imply that the steps of the various embodiments must be performed in the order presented. As will be appreciated by one of skill in the art the order of steps in the foregoing embodiments may be performed in any order. Words such as \u201cthereafter,\u201d \u201cthen,\u201d \u201cnext,\u201d etc. are not intended to limit the order of the steps; these words are simply used to guide the reader through the description of the methods. Further, any reference to claim elements in the singular, for example, using the articles \u201ca,\u201d \u201can\u201d or \u201cthe\u201d is not to be construed as limiting the element to the singular.","The various illustrative logical blocks, modules, circuits, and algorithm steps described in connection with the embodiments disclosed herein may be implemented as electronic hardware, computer software, or combinations of both. To clearly illustrate this interchangeability of hardware and software, various illustrative components, blocks, modules, circuits, and steps have been described above generally in terms of their functionality. Whether such functionality is implemented as hardware or software depends upon the particular application and design constraints imposed on the overall system. Skilled artisans may implement the described functionality in varying ways for each particular application, but such implementation decisions should not be interpreted as causing a departure from the scope of the present invention.","The hardware used to implement the various illustrative logics, logical blocks, modules, and circuits described in connection with the embodiments disclosed herein may be implemented or performed with a general purpose processor, a digital signal processor (DSP), an application specific integrated circuit (ASIC), a field programmable gate array (FPGA) or other programmable logic device, discrete gate or transistor logic, discrete hardware components, or any combination thereof designed to perform the functions described herein. A general-purpose processor may be a microprocessor, but, in the alternative, the processor may be any conventional processor, controller, microcontroller, or state machine. A processor may also be implemented as a combination of computing devices, e.g., a combination of a DSP and a microprocessor, a plurality of microprocessors, one or more microprocessors in conjunction with a DSP core, or any other such configuration. Alternatively, some steps or methods may be performed by circuitry that is specific to a given function.","In one or more exemplary embodiments, the functions described may be implemented in hardware, software, firmware, or any combination thereof If implemented in software, the functions may be stored on or transmitted over as one or more instructions or code on a computer-readable medium. The steps of a method or algorithm disclosed herein may be embodied in a processor-executable software module executed which may reside on a tangible non-transitory computer-readable medium or processor-readable medium. Non-transitory computer-readable and processor-readable media may be any available media that may be accessed by a computer or processor. By way of example, and not limitation, such non-transitory computer-readable media may comprise RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium that may be used to carry or store desired program code in the form of instructions or data structures and that may be accessed by a computer. Disk and disc, as used herein, includes compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), floppy disk, and blu-ray disc where disks usually reproduce data magnetically, while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer-readable media. Additionally, the operations of a method or algorithm may reside as one or any combination or set of codes and\/or instructions on a non-transitory processor-readable medium and\/or computer-readable medium, which may be incorporated into a computer program product.","The preceding description of the disclosed embodiments is provided to enable any person skilled in the art to make or use the present invention. Various modifications to these embodiments will be readily apparent to those skilled in the art, and the generic principles defined herein may be applied to other embodiments without departing from the scope of the invention. Thus, the present invention is not intended to be limited to the embodiments shown herein but is to be accorded the widest scope consistent with the principles and novel features disclosed herein."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The accompanying drawings, which are incorporated herein and constitute part of this specification, illustrate exemplary aspects of the invention. Together with the general description given above and the detailed description given below, the drawings serve to explain features of the invention.",{"@attributes":{"id":"p-0007","num":"0006"},"figref":"FIG. 1A"},{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 1B"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 1C"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 1D"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 2A"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 2B"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 2C"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIGS. 5A-5C"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 6A"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 6B"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIGS. 7A and 7B"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIGS. 8A and 8B"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIGS. 9A-9D"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIGS. 10A-10C"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 11"}]},"DETDESC":[{},{}]}
