---
title: Methods and apparatus for increasing data storage capacity
abstract: A environment and method are provided for increasing the storage capacity of a data storage environment. Additional storage clusters may be added to the storage environment without affecting the performance of each individual storage cluster. When data is written to the storage environment, a selection may be made as to which storage cluster is to store the data. When data is read from the storage environment, it may be determined which storage cluster stores the data and the data may be retrieved from that storage cluster.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09229646&OS=09229646&RS=09229646
owner: EMC Corporation
number: 09229646
owner_city: Hopkinton
owner_country: US
publication_date: 20040226
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD OF THE INVENTION","DESCRIPTION OF THE RELATED ART","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION"],"p":["The present invention relates to data storage and, more particularly, to methods and apparatus for increasing data storage capacity.","The capacity and performance of a data storage system depends on the physical resources of the storage system. For example, the quantity of data that a storage system is capable of storing is dependent on the number and capacity of the physical storage devices that the storage system possesses. As the quantity of data stored on the storage system approaches the storage capacity of the storage system, it may be desired to increase the storage system capacity by adding additional physical storage devices to the storage system. However, there may be physical limits imposed by the hardware configuration of the storage system on the number of storage devices that the storage system may have. Consequently, when a storage system approaches or nears it storage capacity, it may no longer be possible or desirable to add more physical storage devices to the storage systems. Rather, if it is desired to increase storage capacity, one or more additional storage systems may be used.","One illustrative embodiment is directed to a method of accessing a unit of data stored in a storage environment that includes a plurality of storage clusters. The method comprises acts of: receiving a request from a host computer to locate the unit of data previously stored in the storage environment; and in response to receipt of the request, determining on which one of the plurality of storage clusters the unit of data is stored. Another illustrative embodiment is directed to at least one computer readable medium encoded with instructions that, when executed on a computer system, perform the above-described method.","A further illustrative embodiment is directed to an apparatus for storing data. The apparatus comprises: an input that receives a request from a host computer to locate a unit of data on at least one of a plurality of storage clusters in a storage environment, and at least one controller, coupled to the input, that: receives the request from the input; and in response to receipt of the request, determines on which of the plurality of storage clusters the unit of data is stored.","Another illustrative embodiment is directed to a method of accessing a unit of data stored in a storage environment that includes a plurality of storage clusters, the storage environment storing data for a host computer. The method comprises acts of: receiving a request from an application program executing on the host computer to store a unit of data; and in response to receipt of the request, selecting one of the plurality of storage clusters to store the unit of data in a manner that is transparent to the application program so that the application program is not made aware that the selected one of the plurality of storage clusters stores the unit of data. A further illustrative embodiment is directed to at least one computer readable medium encoded with instructions that, when executed on a computer system, perform the above-described method.","A further illustrative embodiment is directed to an apparatus for storing data. The apparatus comprises: an input that receives a request from an application program executing on a host computer to store a unit of data on at least one of a plurality of storage clusters in a storage environment, and at least one controller, coupled to the input, that: receives the request from the input; and in response to receipt of the request, selects one of the plurality of storage clusters to store the unit of data in a manner that is transparent to the application program so that the application program is not made aware that the selected one of the plurality of storage clusters stores the unit of data.","Another illustrative embodiment is directed to a method of accessing a unit of data stored in a storage environment that includes a plurality of storage clusters. The method comprises acts of: receiving a request from an application program executing on a host computer to store a unit of data; selecting one of the plurality of storage clusters to store the unit of data; and storing the unit of data on the selected one of the plurality of storage clusters without storing information identifying the selected one of the plurality of storage clusters. A further illustrative embodiment is directed to at least one computer readable medium encoded with instructions that, when executed on a computer system, perform the above-described method.","Another illustrative embodiment is directed to an apparatus for storing data comprising: an input that receives a request from an application program executing on a host computer to store a unit of data on at least one of a plurality of storage clusters in a storage environment, and at least one controller, coupled to the input, that: receives the request from the input; selects one of the plurality of storage clusters to store the unit of data; and stores the unit of data on the selected one of the plurality of storage clusters without storing information identifying the selected one of the plurality of storage clusters.","On application for aspects of the present invention described herein relates to content addressable storage in a distributed storage environment that includes a plurality of interconnected independent nodes. Such an environment is described in the commonly assigned U.S. applications shown in Table 1, each of which is incorporated herein by reference in its entirety. The applications listed in Table 1 are referred to collectively herein as the Related applications.",{"@attributes":{"id":"p-0021","num":"0020"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"84pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"77pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"56pt","align":"left"}}],"thead":{"row":[{"entry":"TABLE 1"},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":["Title","Serial No.","Filing Date"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Content Addressable","09\/236,366","Jan. 21, 1999"]},{"entry":["Information, Encapsulation,",{},{}]},{"entry":["Representation, And",{},{}]},{"entry":["Transfer",{},{}]},{"entry":["Access To Content","09\/235,146","Jan. 21, 1999"]},{"entry":["Addressable Data Over A",{},{}]},{"entry":["Network",{},{}]},{"entry":["System And Method For","09\/391,360","Sep. 7, 1999"]},{"entry":["Secure Storage Transfer",{},{}]},{"entry":["And Retrieval Of Content",{},{}]},{"entry":["Addressable Information",{},{}]},{"entry":["Method And Apparatus For","10\/731,790","Dec. 9, 2003"]},{"entry":["Data Retention In A",{},{}]},{"entry":["Storage System",{},{}]},{"entry":["Methods And Apparatus","10\/731,613","Dec. 9, 2003"]},{"entry":["For Facilitating Access To",{},{}]},{"entry":["Content In A Data Storage",{},{}]},{"entry":["System",{},{}]},{"entry":["Methods And Apparatus","10\/731,796","Dec. 9, 2003"]},{"entry":["For Caching A Location",{},{}]},{"entry":["Index In A Data Storage",{},{}]},{"entry":["System",{},{}]},{"entry":["Methods And Apparatus","10\/731,603","Dec. 9, 2003"]},{"entry":["For Parsing A Content",{},{}]},{"entry":["Address To Facilitate",{},{}]},{"entry":["Selection Of A Physical",{},{}]},{"entry":["Storage Location In A Data",{},{}]},{"entry":["Storage System",{},{}]},{"entry":["Methods And Apparatus","10\/731,845","Dec. 9, 2003"]},{"entry":["For Generating A Content",{},{}]},{"entry":["Address To Indicate Data",{},{}]},{"entry":["Units Written To A Storage",{},{}]},{"entry":["System Proximate In Time",{},{}]},{"entry":["Methods And Apparatus","10\/762,044","Jan. 21, 2004"]},{"entry":["For Modifying A Retention",{},{}]},{"entry":["Period For Data In A",{},{}]},{"entry":["Storage System",{},{}]},{"entry":["Methods And Apparatus","10\/761,826","Jan. 21, 2004"]},{"entry":["For Extending A Retention",{},{}]},{"entry":["Period For Data In A",{},{}]},{"entry":["Storage System",{},{}]},{"entry":["Methods And Apparatus","10\/762,036","Jan. 21, 2004"]},{"entry":["For Indirectly Identifying A",{},{}]},{"entry":["Retention Period For Data",{},{}]},{"entry":["In A Storage System",{},{}]},{"entry":["Methods And Apparatus","10\/762,043","Jan. 21, 2004"]},{"entry":["For Indirectly Identifying A",{},{}]},{"entry":["Retention Period For Data",{},{}]},{"entry":"In A Storage System"},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}}}},"An example of a distributed storage environment  is shown in . Distributed storage environment  includes a plurality access nodes -and a plurality of storage nodes -. Access nodes  may receive and respond to access requests from a host computer , and storage nodes  may store data sent to storage environment  by host computer . Access nodes  and storage nodes  may be coupled by a network (not shown) and communicate over the network such that each node may make its presence on the network known to the other nodes. In this manner, the nodes may operate together to process access requests and store data for host computer .","Each node may include processing resources (e.g., processor and memory) and storage devices. The nodes communicate with each other to store data, respond to access requests, and perform other environment functions. To a user of the storage environment (e.g., the host computer  or an application program executing thereon), the storage environment may appear as single entity. That is, the user need not be aware that the storage environment includes a plurality of separate nodes or on which storage node a certain unit of data is stored or mirrored.","To increase the storage capacity of the storage environment , more storage nodes may be added and coupled to the network. These additional storage nodes may make their presence known on the network, thus allowing access nodes  to employ the additional storage in the storing of data. Adding more storage nodes to the storage network without increasing the number of access nodes may result in the access nodes acting as a bottleneck for the storage environment and a degradation in performance. Thus, it may desirable when increasing the number of storage nodes to also increase the number of access nodes.","Storage environment  may perform a number of functions described in the above-referenced Related applications, such as determining on which storage node  to store data in response to a write request from host , determining on which storage node  data is stored in response to a read request from host , performing garbage collection of data that may be deleted from the storage environment, enforcing retention periods that specify a period of time that data should not be deleted from the storage environment, mirroring data (i.e., creating one or more mirror copies on different nodes of the storage environment), self-healing to compensate for failure of one or more nodes, and other functions. Such functions may be performed by storage nodes, access nodes, or both, and performing such functions may cause network traffic between the nodes.","For example, to perform self-healing functions, other nodes may detect when a node fails. In response, the environment  may re-direct access requests to data stored on the failed node to other nodes that store a mirrored copy of that data and may build another mirror for continued fault tolerance. A node may broadcast keep-alive messages on the network to indicate that it is operational and has not failed. If keep-alive messages from that node are not received by other nodes, the other nodes may determine that the node has failed. Adding more nodes to the storage environment causes more keep-alive messages to be transmitted on the network and results in more network traffic.","As another example, the storage environment  may maintain an index such as the blob location index (BLI) described in the Related applications, to aid in locating data. The BLI may specify on which storage node units of data are stored. Each access or storage node in the network may be responsible for administering a portion of the BLI. Because the BLI may be distributed across the access and\/or storage nodes, maintaining and updating the BLI when units of data are written to or deleted from the storage environment causes network traffic to be generated among nodes. Adding more nodes may cause the administration responsibilities of the BLI to be shared among a greater number of nodes, thus casing a greater amount of network traffic to be generated.","Other functions, including some described in the Related applications, such as performing garbage collection, locating content on the storage environment (e.g., via a broadcast message to all nodes), and re-ranging the BLI (i.e., when nodes are added or removed from the storage environment), may cause a greater amount of network traffic as nodes are added to the storage environment. Such increased network traffic may result in a decrease in performance of the storage environment.","As mentioned above, storage nodes  may be added to the storage environment to increase the storage capacity of the storage environment. Additionally, access nodes  may be added to counteract degradation in performance caused by adding the additional storage nodes. However, Applicants have appreciated that because adding access nodes and storage nodes causes increased network traffic, once a certain number of nodes in the storage environment is reached, the performance benefit gained by adding additional nodes is offset at least somewhat by the increased network traffic generated by the nodes in the storage environment, as well as the increased amount of processing resources used in performing the infrastructure functions (such as those described above) that support the storage environment. Thus, as additional nodes are added to the storage environment to increase storage capacity and\/or performance, the overall performance of the storage environment may increase less than expected, or might in some cases even decrease. This performance impact is referenced below as performance degradation, which term is used herein to refer to not only actual decreases in performance, but also performance improvements that are less than would be expected to be achieved through the addition of an additional storage resource.","One embodiment of the invention is directed to providing additional storage clusters that work together to provide combined storage, but that are independent so that the addition of an additional storage node in one cluster does not raise the performance degradation issues discussed above for other clusters. As used herein, the term storage cluster refers to a group of one or more interconnected nodes that share at least one software utility that logically connects them. For example, the nodes of a storage cluster may share a self-healing software utility, such that nodes in a cluster monitor keep-alive messages transmitted by other nodes in that cluster, but not by nodes outside the cluster. The nodes may also share a common BLI so that the BLI of one storage cluster may be independent of the BLI of any other storage cluster. The nodes of a storage cluster may also, or alternatively, share other utilities such as a garbage collection utility and\/or a data mirroring utility that keeps track of where data stored in the cluster is mirrored. In one embodiment, each node in a cluster knows the address (e.g., an IP address) of every other node in the cluster, although the present invention is not limited in this respect.","Because nodes in a storage cluster do not share software utilities with nodes outside the cluster (e.g., in a different storage cluster), the internal network traffic and use of processing resources of the nodes within one storage cluster does not affect the performance of any other storage cluster. Consequently, an additional storage cluster may be added to a storage environment to increase storage capacity without experiencing the above-described performance degradation issues.","It should be appreciated that the description of the types of software utilities that may be shared among nodes in a cluster (but not shared among nodes in different clusters) is provided merely for the purposes of illustration, and that the aspects of the present invention described herein are not limited to use in a storage environment wherein the nodes in a storage cluster share the particular types of software utilities discussed above, or any other particular type of software utility. Similarly, while examples of a storage environment having characteristics such as those described in the Related applications are described herein for illustrative purposes, it should be appreciated that the aspects of the present invention described herein are not limited in this respect, and can be employed in storage environments having numerous other configurations. For example, the reference to a storage cluster comprising one or more interconnected nodes is not limited to an access node and storage node configuration as described in connection with the Related applications, as the reference to a node can refer to any device or collection of devices that function together as an entity.","In one embodiment of the invention, multiple storage clusters are managed in a manner that is transparent to users (e.g., host computers or application programs executing thereon) of the storage environment, so that users need not know on which storage cluster a particular unit of data is stored to access that unit of data. Thus, application programs that store data to and retrieve data from the storage environment may treat the multiple storage clusters as a single storage environment, without knowledge of on which cluster data written to the storage environment is stored. Thus, in one embodiment of the invention, when an application program (e.g., on a host) issues a write request for a unit of data to the storage environment, it is determined on which storage cluster the unit of data is to be written, and when the application issues a read request, it is determined on which storage cluster the data is stored, both in a manner transparent to the application program.","The present invention is not limited to any particular implementation technique as to where the aspects of the computer system that determine on which storage cluster to store a unit of data and that retrieve a previously written unit of data from the appropriate cluster are practiced. In one embodiment, these functions may be performed by an application programming interface (API) on the host computer. This implementation is described below in connection with .",{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 2","b":["201","206","207","207","201","203","205","206","201","201","213","213","207","207","206","201","206","213","213"],"i":["a ","b","a ","b ","a ","b ","a ","b "]},"Application program  may be any computer application program that a user or administrator of host  uses and that stores data to and\/or retrieves data from storage environment . Application program  is linked with API  so that application program  may use API  to communicate with the storage clusters . In one embodiment, application program  need not be aware that storage environment  even includes multiple storage clusters, as the management of data storage between the clusters  may be done in a manner transparent to the application program .","API  may be implemented in any suitable way. For example, API  may include computer object code that is provided to the programmer of application program . The computer object code includes routines that may be used to communicate with the storage environment. When linked with API , an application program may call these routines to communicate with storage environment . API  may be linked with any number of applications, and shields the programmer of application program  from the internal protocol by which storage environment  communicates.","When application program  calls a routine or routines of API  to write data to storage environment , API  may select a storage cluster to store the data and send a write request  including the data to the appropriate storage cluster. The selection of the storage cluster may be performed using any suitable technique. For example, the API  may employ a round robin scheme by which the clusters in the storage environment are selected sequentially for each write request. In the example in , API  could alternate between selection of storage cluster and storage cluster for processing write requests. Alternatively, the API  may select a storage cluster to store the data based on, for example, the available storage capacity on each storage cluster, the load on each storage cluster, the size or type of the data to be stored, or any other suitable criteria. The load on a storage cluster may be determined, for example, based on factors such as how many objects (e.g., blobs or CDFs) are being processed by the storage cluster, how busy the CPU(s) of the storage cluster are, how busy the disk(s) of the storage cluster are, and\/or any other suitable factor.","When application program  calls a routine or routines of API  to read a previously stored unit of data from the storage environment, API  may determine on which storage cluster the requested unit of data is stored. This can be done in any suitable way. For example, the API  may store information that identifies the cluster on which each data unit is stored. However, in another embodiment no such information is stored, and the API  determines where the data is stored in response to a read request.","In one embodiment, API  sends a command that identifies the requested unit of data to each storage cluster in the storage environment and requests the storage cluster to determine if the specified unit of data is stored in that cluster. The command can be a read command, or a command (referred to herein as an \u201cexists command\u201d) that does not seek to read the data unit but only to determine on which cluster it is stored.","The commands may be sent to the storage clusters either serially or in parallel, as the invention is not limited in this respect. For example, if sending the commands serially, API  may first send an exists command  to storage cluster . If cluster responds that the unit of data is stored therein, API  may then send a read request for that unit of data to storage cluster and need not send any additional exists commands to other clusters. If cluster responds that the unit of data is not stored therein, API  may then send another exists command  to cluster . If cluster responds that the specified unit of data is stored therein, API  may then send a read request to cluster for the unit of data. If API  sends the exists commands in parallel, then exists command  and exists command  may be sent to clusters and , respectively, at approximately the same time. When one of the storage clusters responds to API  that the specified unit of data is stored therein, then API  may send a read request for the unit of data to that storage cluster. As mentioned above, instead of employing exists commands followed up by read requests, API  may only issue read requests (i.e., without issuing exists commands) to the storage clusters either serially or in parallel.","Sending the exists commands or read requests in parallel provides the advantage that the time spent in locating the unit of data is reduced because each cluster is performing a search for the unit of data simultaneously. However, these simultaneous searches consume the processing resources (and thereby impact performance) on each of the clusters, even though such searches will fail on many of the storage clusters. Conversely, sending the exists or read commands serially may increase the time spent in locating the unit of data because the exists command is not sent to a storage cluster until the search on the preceding storage cluster completes, but the performance of the storage environment may not be impacted as greatly because some clusters will not need to perform a search for units of data found before the exists command or read request is sent to those clusters. The search within each cluster can be performed in any suitable manner, as the present invention is not limited in this respect. Examples of searching techniques are described in the above-referenced Related applications.","To reduce the overall processing load incurred by the storage clusters in searching for a unit of data, the API  may orchestrate a layered searching approach wherein clusters initially perform a relatively cursory (and not computationally intensive) search, and perform a more thorough search only when necessary (e.g., when the cursory search among all clusters does not result in a successful search).","For example, as described in the Related applications, each cluster may include a BLI that identifies some of the data units stored on the cluster. A relatively cursory search may involve initially searching only the BLI in a cluster, which is relatively quick and comparatively inexpensive. If a BLI search (either serially or in parallel) fails for all clusters in the environment, a more thorough search may be performed. It should be appreciated that the BLI search is merely one example of a technique for performing a relatively cursory search and that other techniques are possible, as the aspect of the present invention that employs a layered searching approach is not limited in this respect. Further, it should be appreciated that the aspects of the present invention described herein are not limited to use with a storage environment having the configuration shown in the Related applications, and is not limited to storage clusters that employ a BLI.","In the example of , storage environment  is shown as having two storage clusters. It should be appreciated that this is given only as an example, as storage environment  may include three or more storage clusters, as the invention is not limited in this respect. Similarly,  shows only a single host computer  sending data to and receiving data from storage environment . However, it should be appreciated the storage environment  may operate with many more host computers, as the invention is not limited in this respect.","In the environment discussed above, the aspects of the present invention that relate to selecting of the storage clusters to store the data, and\/or the determining of which of the clusters stores a previously-written unit of data is implemented by the API. However, it should be appreciated that this explanation is provided merely for illustrative purposes, as the present invention is not limited in this respect. As mentioned above, an advantage of one illustrative embodiment of the present invention is that the implementation of the storage environment among multiple clusters is transparent to an application program executing on the host, so that the application program need not even be aware that multiple clusters exist, and need not be concerned with the overhead associated with managing multiple clusters. However, other aspects of the present invention are not limited in this respect, such that the management of the multiple clusters can alternatively be implemented in a manner that is not transparent to the application program. In addition, whether the management of multiple clusters is transparent to the application program or not, such management need not be implemented in an API in the manner described above, as it can alternatively be implemented in other locations of the computer system. For example, in one embodiment of the present invention, the management of multiple clusters can be implemented within the clusters themselves. In this embodiment, the API may see only a single cluster, but the clusters are capable of communicating among themselves to manage the distribution of data among the multiple clusters. Alternatively, the API may see all the access nodes of all the clusters, but may not be aware that these nodes are divided amongst two or more clusters, such that the API may send any access request to any access node, regardless of which cluster the access node forms a part. In an alternative embodiment of the present invention, the management of the multiple clusters can be implemented in an appliance that it is separate from any of the clusters and from the host. The appliance can take any of numerous forms, and may be, for example, a network component (e.g. a router) that forms part of a network for interconnecting the host computer with the storage clusters.",{"@attributes":{"id":"p-0047","num":"0046"},"figref":"FIG. 3","b":["303","303","301","305","303","303","301","305","301","303","305"]},"Appliance  may use any suitable technique to determine on which storage cluster to store a unit of data, and for determining on which storage cluster a previously written unit of data is stored, as the present invention is not limited in this respect.","As should be appreciated from the foregoing, with the embodiment of , an application program or API executing on host computer  need not even be aware that storage environment  includes multiple storage clusters. Further, the application program or API may write a unit of data to or retrieve a unit of data from storage environment  without knowing where the particular unit of data is stored. The host computer may simply send access requests to appliance , which then determines where to a write a unit of data or where a previously written unit of data is stored.","In the example of , storage environment  is shown having two storage clusters. It should be appreciated that this is given only as an example, as storage environment  may include three or more storage clusters as the invention is not limited in this respect. Similarly,  shows only a single host computer  sending data to and receiving data from storage environment  However, it should be appreciated the storage environment  may operate with many more host computers and the invention is not limited in this respect. Further, although  depicts only a single appliance  operating between host  and storage environment , multiple appliances may be used. In this manner, a bottleneck, wherein all access requests to the storage environment are processed by a single appliance, may be avoided. Additionally, although in the example of , an API is employed to communication between the host computer and the storage environment, some embodiments of the invention do not require the use of an API and may allow the application program to communicate directly with the storage environment.",{"@attributes":{"id":"p-0051","num":"0050"},"figref":["FIG. 4","FIG. 4","FIG. 1","FIG. 4"],"b":["401","403","405","405","405","407","411","405","409","413","421","405","405"],"i":["a ","b","a ","a","c ","a","d","b ","a","c ","a","c ","a","b "]},"When an access node  or  receives a request to retrieve a unit of data, the access node may determine whether the requested unit of data is stored in its cluster. If it is not, the access node may send exists commands and\/or read commands to the other storage clusters in the storage environment, either serially or in parallel, in much the same manner as described above.","Application programs executing on host computer  may send access requests (e.g., via an API) to the access nodes  and  of storage clusters and . In one embodiment, the application program and\/or API is unaware that access nodes  and  belong to different storage clusters. From the point of view of the application program and\/or API, access nodes  and  may receive access requests, regardless of which storage cluster stores a requested unit of data. Indeed, the application program and\/or API need not even be aware that storage environment includes multiple storage clusters, such that where data is stored on the storage environment is transparent to the application program and\/or API in one embodiment of the invention.","When an access node  or  receives a request to write a unit of data from host , the access node may determine whether to store the unit of data on its cluster or send the unit of data to another cluster for storage. Such a determination may be made based on any suitable factor, examples of which were discussed above (e.g., a round robin scheme, the size or type of the data to be stored, the available capacity on each of the storage clusters, the current processing load on each of the storage clusters).","Alternatively, instead of allowing an application program and\/or API to send requests to any access nodes in the storage environment, the application program and\/or API may only be permitted to send access requests to the access nodes of one storage cluster (e.g., cluster ). The access nodes of this storage cluster are then responsible for writing data to and retrieving data from the storage cluster of which they are a part, and\/or other storage clusters in the storage environment of which the access nodes are not a part.","In one embodiment, aspects of the invention can be employed with content addressable storage (CAS) environments of the type described in the Related applications. In a content-addressable storage environment, data is stored using a content address generated based upon the content of the data itself. The content address may be generated by applying a hash function to the data to be stored. The output of the hash function may be used as at least part of the content address used in communication between the host and storage environment to refer to the data. The content address can be mapped (e.g., within the storage environment) to one or more physical storage locations within the storage environment.","A unit of data in the architecture defined in the CAS applications is referred to as a blob. As illustrated in , blob  may be, for example, a unit of binary data to be stored by a host on a storage environment (e.g., storage environment ), such as, for example, a patient x-ray, company financial records, or any other type of data. When the blob  is stored to the content addressable storage environment, a unique address is generated for the blob  based upon its content.","Each blob  has at least one content descriptor file (CDF)  associated with it. CDF  may include metadata  and a plurality of references , , . . . , . A CDF may reference one or more blobs or other CDFs. Thus, the references  may be, for example, references to the blobs and\/or CDFs referenced by CDF . Metadata  may, for example, include the creation date of CDF  (e.g., the date that CDF  was stored on the storage environment), a description of the content of blob , and\/or other information.","As discussed above, a storage environment may employ a garbage collection utility to clean up blobs and CDFs that are available for deletion. In one implementation of the system described in the Related applications, a blob or CDF is available for deletion when it is not referenced by any CDFs.","Blobs and their corresponding CDFs may be written separately to the storage environment. In accordance with one embodiment of the invention, a blob and the CDF(s) that reference(s) it may be stored on different storage clusters. This may present challenges when used on a system that employs a garbage collection utility that relies on the absence of a CDF pointing to a blob to delete the blob, and that in response to deletion of a CDF updates any blobs previously referenced thereby to delete a pointer to the CDF. For example, when a CDF is deleted and any blobs that it references are stored on different storage clusters, the garbage collection utility may first search the storage cluster on which the CDF is stored for the referenced blobs. When it does not find the referenced blobs on that storage cluster, it may locate the blobs on other storage clusters (e.g., via exists commands and\/or read requests), and indicate to the storage clusters that store the blobs that the reference to the deleted CDF should be removed. This process may incur significant overhead in communication between clusters. Furthermore, in some embodiments, it may be desirable to have each storage cluster not even be aware that there are other related storage clusters in the storage environment, such that a cluster might not know to look to other clusters for a referenced blob when a CDF is deleted.","In one embodiment of the invention, the above-described challenges are alleviated by storing a blob and the CDFs that reference that blob on the same storage cluster. This may be accomplished in any suitable way. For example, the same placement algorithms discussed above with reference to  can be employed, but a blob and the CDF(s) that reference it can be written to the storage environment in a single write transaction so that the blob and the CDF are written to the same storage cluster. If, at a later time, additional CDFs that reference the blob are written to the storage environment, the entity determining the location for storing the additional CDFs (e.g., API, appliance, or storage cluster) may first determine which storage cluster stores the blob and then store the additional CDFs to that same storage cluster.","In one embodiment of the invention, application programs keep track of previously stored data by storing the content addresses only of CDFs, but do not track the content addresses of blobs referenced thereby, as that information can be retrieved from the corresponding CDF. Thus, to access a blob, an application program may request retrieval of a CDF that references the blob. The application program may receive the requested CDF and determine the content address of the blob referenced by the CDF. The application program may then use this content address to send a read request for the blob. Because the blob and the CDF are stored on the same storage cluster in one embodiment of the invention, the entity that sent the request for the CDF can send a read request for the blob to the same storage cluster, without needing to perform a search to determine which storage cluster stores the blob.","In another embodiment of the invention, instead of using a search technique to locate data stored on a storage environment, a concept known as intrinsic location addressing may be used. Intrinsic location addressing relates to placing a unit of data on a particular storage cluster based on the content address of the unit of data. Thus, when the unit of data is later read, it is not necessary to perform a search to determine on which storage cluster the unit of data is stored. Instead, the storage cluster that stores the unit of data may again be determined based on the content address of the unit of data.","For example, selected bits of the content address may be used to determine on which storage cluster to store a unit of data. Each storage cluster may be assigned a range of addresses to be stored. For example, the first character of the content address of a unit of data may determine on which storage cluster the unit of data is to be stored. The storage environment of  includes three storage clusters. When the first character is \u2018A\u2019-\u2018I\u2019, the unit of data may be stored on storage cluster , when the first character is \u2018J\u2019-\u2018R\u2019, on storage cluster , and when the first character is \u2018S\u2019-\u2018Z\u2019, on storage cluster . It should be appreciated that this example is illustrative and not limiting. For example it need not be the first character that is examined to determine the storage cluster, and the ranges can differ from those shown so that they need not be of approximately equal size and need not be contiguous.","As discussed above, the content address of a unit of data may be based, at least in part, on the content of the unit of data, and may be generated in any suitable way, as the present invention is not limited in this respect. For example, the unit of data may be hashed using a hashing algorithm, such as MD5, to generate a portion of the content address. The content address may include other information such as a timestamp that indicates when the unit of data was first written to the storage environment and\/or a guaranteed unique identifier (GUID) that ensures the content address is unique, as described in the Related applications. Thus, as shown in , a content address  may include a hash value, a timestamp, and a GUID.","In some situations, the unit of data to be written may be too large to compute the hash value before it is desired to have selected a storage location on one of the storage clusters to store the unit of data. For example, the storage environment may not have a buffer large enough to store the entire unit of data while the hash is computed for the unit of data. Thus, in one embodiment of the invention, a technique is employed to enable a storage location in one of the storage clusters to be selected before the hash value of the content address is computed.","Because the hash value may not be available in some situations to select a storage cluster to store the unit of data, it may be desirable to use another portion of the content address (e.g., the timestamp or GUID portion) to select the appropriate storage cluster. As discussed in the Related applications, employing timestamp information can be beneficial in providing a temporal locality of reference for data units written at approximately the same time within a storage system such as one cluster. However, it is desired to not have all data units written at around the same time be written to the same cluster, as this would negatively impact performance by preventing clusters from processing write requests in parallel. Thus, as shown in , to ensure that the portion of the content address used to select the storage cluster (e.g., the timestamp) is sufficiently random, this portion of the content address may be hashed at  using an algorithm (MD5 or any other suitable algorithm) to generate a location identifier . A portion  (or all) of the location identifier  may be used to determine which storage cluster should be used to store the unit of data. In this manner, the pseudo-randomness of the hashing algorithm may be employed so that portion  is sufficiently random to have an even distribution across the storage clusters for units of data written contemporaneously.","In one embodiment, the location identifier  is not saved to assist in retrieving the data unit. Rather, when it is desired to read the unit of data, the portion  of the content address may be hashed again to generate location identifier  and the portion  of the location identifier may be used to determine on which of the storage clusters the unit of data is stored.","It should be appreciated that the use of intrinsic location addressing techniques for selecting a storage cluster to store a unit of data obviates the need to perform a search to locate the unit of data when a read request for the unit of data is issued.","It should be appreciated that when used in conjunction with a content addressable storage system that employs blobs, CDFs and a garbage collection utility as discussed above, the use of the intrinsic location addressing technique may result in a blob and corresponding CDF being stored on different clusters, which can present challenges as discussed above. Accordingly, one embodiment of the invention is directed to a technique for using intrinsic locations to select a storage cluster to store a unit of data in conjunction with a technique to ensure that a CDF and its referenced blob(s) are stored on the same storage cluster. In one embodiment, the technique for ensuring that blobs and its CDFs are stored on the same cluster involves using a common identifier in the content addresses of the CDF and its referenced blob(s), and using this common identifier to determine on which storage cluster to store the CDF and its referenced blobs.","For example, a CDF and its referenced blobs may be provided with the same GUID in their content addresses. A portion of this GUID (or a portion of a hash of the GUID) may be used to determine on which storage cluster to store a unit of data, so that the CDFs and blobs with the same GUIDs will map to the same storage cluster. For example,  shows a CDF  that references two blobs,  and . In accordance with one embodiment of the invention, these three units of data are stored on the same storage cluster as follows. First, the application program (e.g.,  in ) sends blob  to the storage environment. In those embodiments where the host computer  has an API to interface with the storage environment, the API may generate a GUID as part of the content address of the blob . The facility for selecting a storage cluster (e.g., API , appliance  or one of the clusters) may select a storage cluster on which to store blob . The application program then sends blob  to the API. Because the API has not yet received a CDF, the API recognizes that blob  and  are part of the same transaction (i.e., both referenced by the same CDF) and may use the same GUID in the content address of blob . Consequently, blob  and  will be stored on the same storage cluster. The application program then sends CDF  to the API to be stored on the storage environment. The reception of a CDF by the API indicates to the API that the CDF is the last unit of data in the transaction. Thus, API assigns the same GUID to the content address of CDF , such that CDF  is stored on the same storage cluster as blobs  and .","In one embodiment, a blob cannot be accessed directly by an application without accessing it through a corresponding CDF. Thus, the CDF must first be read. In accordance with this embodiment, the GUID may be included in the content address of the CDF and used to locate the storage cluster on which the CDF is stored to facilitate a read of the CDF, but the GUID need not be provided in the content address of the blob because the entity charged with locating the storage cluster on which the blob is stored will recognize that the blob is stored on the same storage cluster as the previously read CDF.","When new storage clusters are added to the storage environment, it may be desired to change the selection algorithm for determining on which storage cluster a unit of data is stored so that an even distribution of data units across all of the storage clusters results. For example, if there are three storage clusters in the storage environment, a selection algorithm such as the GUID (in the content address) modulo three may be used to determine on which storage cluster the content is stored. If a fourth storage cluster is added, it may be desired to change the algorithm to GUID modulo four. However, if the new algorithm is applied to content addresses when units of data are read after the addition of the new cluster, the new algorithm may not properly locate data units that were stored on the storage environment prior to the change in algorithm.","Thus, one embodiment of the invention is directed to maintaining information that indicates what algorithm was in making a selection of the cluster for a previously stored data unit. In one embodiment this information identifies the selection algorithm used during certain time periods. Because the content address of the data unit may include a timestamp, the timestamp information may be used to index into the information to determine what algorithm was in place at the time the unit of data was stored. This algorithm can be applied to the appropriate portion of the content address to determine on which storage cluster the unit of data is stored.","An example of information that indicates what algorithm was in use during certain periods of time is shown in . In , line  indicates a time frame for a particular selection algorithm, and indicates that the algorithm was used from time \u201c\u22121\u201d to time \u201c1019922093021.\u201d A value of \u22121 indicates the beginning of the existence of the storage environment. Line  indicates what algorithm was in effect during the time frame indicated by line , and in the example of  indicates that an algorithm called Fast Blob Naming or FBN was in use. Fast blob naming is an algorithm where a modulus of the hash value and timestamp of the content address is used to determine on which storage cluster a unit of data should be stored. The \u201cmod\u201d indicator indicates the modulus value used in the Fast Blob Naming algorithm. Thus, for example, in line , an algorithm of hash value and timestamp modulo one was used. The result field indicates which results of the algorithm correspond to which storage clusters. In the example of line , the only possible result when using a modulus of 1 is zero. Thus, when the result is zero, the unit of data is stored on storage cluster C.","Line  indicates that a new algorithm was in effect between time period \u201c101992093021\u201d and \u201c101998383838.\u201d As shown in lines  and , the Fast Blob Naming algorithm was used with a modulus of two during that time period, so that when the result of the modular operation was zero, the data unit was stored on cluster C, and when the result of the modular operation was one, the data unit was stored on cluster C.","The timestamp of the content address of a unit of data may be used to determine which algorithm to employ in locating a unit of data. For example, if the timestamp of a unit of data were 10199209566, such a timestamp falls within the time period indicated in line . Thus, the algorithm specified by lines  and  may be used to determine which storage cluster stores that unit of data.","Line  indicates that a new algorithm came into effect at time \u201c101998383838,\u201d and the end time of \u201c\u22121\u201d indicates that the algorithm is currently in effect. Lines , , , , and  indicate that a modulus value of 5 was used in the Fast Blob Naming algorithm and that three clusters are currently in use (i.e., clusters C, C, and C). Line  indicates that if the result of the hash value and timestamp modulo five is zero, then the unit of data is stored on cluster C. If the result is one or two, then the unit of data is stored on cluster C. If the result is three or four, then the unit of data is stored on cluster C. Consequently, lines - provide an example of how storage clusters in a storage environment may be weighted so that some storage clusters store more units of data than other storage clusters. Weighting clusters in this manner may be desirable, for example, if one storage cluster has less available capacity than the other storage clusters in the storage environment, or for any other desirable purpose.","Thus, in the example of , of the five possible results of computing the hash value and timestamp modulo five, only one of those results (i.e., zero) causes a unit of data to be stored on cluster C, while two of those results (i.e., one and two) cause the unit of data to be stored on cluster C, and the remaining two results (i.e., three and four) cause the unit of data to be stored on cluster C. Consequently, approximately one fifth of the units of data processed using this algorithm are stored on cluster C, while approximately two fifths of the units of data are stored on cluster C and approximately two fifths of the units of data are stored on cluster C.","It should be appreciated that the various techniques described herein for selecting a particular storage cluster can be used with any suitable technique for choosing storage locations within a cluster, as the present invention is not limited in this respect. For example, intrinsic location addressing techniques can be used to select a storage cluster but not the storage location within a cluster, or vice-versa, as any pairing of selection techniques can be used.","The above-described embodiments of the present invention can be implemented in any of numerous ways. For example, the embodiments may be implemented using hardware, software or a combination thereof. When implemented in software, the software code can be executed on any suitable processor or collection of processors, whether provided in a single computer or distributed among multiple computers. It should be appreciated that any component or collection of components that perform the functions described above can be generically considered as one or more controllers that control the above-discussed functions. The one or more controllers can be implemented in numerous ways, such as with dedicated hardware, or with general purpose hardware (e.g., one or more processors) that is programmed using microcode or software to perform the functions recited above.","In this respect, it should be appreciated that one implementation of the embodiments of the present invention comprises at least one computer-readable medium (e.g., a computer memory, a floppy disk, a compact disk, a tape, etc.) encoded with a computer program (i.e., a plurality of instructions), which, when executed on a processor, performs the above-discussed functions of the embodiments of the present invention. The computer-readable medium can be transportable such that the program stored thereon can be loaded onto any computer environment resource to implement the aspects of the present invention discussed herein. In addition, it should be appreciated that the reference to a computer program which, when executed, performs the above-discussed functions, is not limited to an application program running on a host computer. Rather, the term computer program is used herein in a generic sense to reference any type of computer code (e.g., software or microcode) that can be employed to program a processor to implement the above-discussed aspects of the present invention.","It should be appreciated that in accordance with several embodiments of the present invention wherein processes are implemented in a computer readable medium, the computer implemented processes may, during the course of their execution, receive input manually (e.g., from a user).","In various examples described above, content addresses were described to include alphabetic characters \u2018A\u2019-\u2018Z\u2019. It should be understood that these content addresses were given only as examples, and that content addresses may include any alphanumeric character, series of bits, or any other suitable character, as the invention is not limited in this respect.","The phraseology and terminology used herein is for the purpose of description and should not be regarded as limiting. The use of \u201cincluding,\u201d \u201ccomprising,\u201d \u201chaving,\u201d \u201ccontaining\u201d, \u201cinvolving\u201d, and variations thereof, is meant to encompass the items listed thereafter and additional items.","Having described several embodiments of the invention in detail, various modifications and improvements will readily occur to those skilled in the art. Such modifications and improvements are intended to be within the spirit and scope of the invention. Accordingly, the foregoing description is by way of example only, and is not intended as limiting. The invention is limited only as defined by the following claims and the equivalents thereto."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 10"}]},"DETDESC":[{},{}]}
