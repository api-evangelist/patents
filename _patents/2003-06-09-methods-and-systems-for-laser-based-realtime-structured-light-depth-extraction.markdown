---
title: Methods and systems for laser based real-time structured light depth extraction
abstract: Laser-based methods and systems for real-time structured light depth extraction are disclosed. A laser light source () produces a collimated beam of laser light. A pattern generator () generates structured light patterns including a plurality of pixels. The beam of laser light emanating from the laser light source () interacts with the patterns to project the patterns onto the object of interest (). The patterns are reflected from the object of interest () and detected using a high-speed, low-resolution detector (). A broadband light source () illuminates the object with broadband/light, and a separate high-resolution, low-speed detector () detects broadband light reflected from the object (). A real-time structured light depth extraction engine/controller () based on the transmitted and reflected patterns and the reflected broadband light.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07385708&OS=07385708&RS=07385708
owner: The University of North Carolina at Chapel Hill
number: 07385708
owner_city: Chapel Hill
owner_country: US
publication_date: 20030609
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["RELATED APPLICATIONS","GOVERNMENT INTEREST","TECHNICAL FIELD","BACKGROUND ART","DISCLOSURE OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION","Structured Light Triangulation Methods","Mathematical Model for Recovering Depth","Depth from Stereo","Depth Extraction Using a Projector and a Camera","Image Processing","Non-Optimized Method","Optimized Method","Performance","Results from Prototype","Prototype Calibration and Depth Extraction"],"p":["This application claims the benefit of U.S. Provisional Patent Application Ser. No. 60\/386,871, filed Jun. 7, 2002, the disclosure of which is incorporated herein by reference in its entirety.","This invention was made with U.S. Government support under Grant No. DABT63-93-C-0048 from the Advanced Research Projects Agency (ARPA) and under grant number ASC8920219 awarded by the National Science Foundation. The U.S. Government has certain rights in the invention.","The present invention relates to methods and systems for real-time structured light depth extraction. More particularly, the present invention relates to methods and systems for laser-based real-time structured light depth extraction.","Structured light depth extraction refers to a method for measuring depth that includes projecting patterns onto an object, detecting reflected patterns from the object, and using pixel displacements in the transmitted and reflected patterns to calculate the depth or distance from the object from which the light was reflected. Conventionally, structured light depth extraction has been performed using a slide projector. For example, a series of patterns on individual slides may be projected onto an object. A detector, such as a camera, detects reflected patterns. The pixels in the projected patterns are mapped manually to the pixels in the reflected patterns. Given the position of the projector and the camera and the pixel offsets, the location of the object can be determined.","In real-time structured light depth extraction systems, images are required to be rapidly projected onto an object and detected synchronously with the projection. In addition, the calculations to determine the depth of the object are required to be extremely fast. Currently, in real-time structured light depth extraction systems, an incandescent lamp and a collimator are used to generate a collimated beam of light. In one exemplary implementation, the collimated beam of light is projected onto a pattern generator. The pattern generator reflects the transmitted light onto the object of interest. The reflected pattern is detected by a camera that serves a dual purpose of detecting structured light patterns and reflected broadband light. A specialized image processor receives the reflected images and calculates depth information.","While this conventional structured light depth extraction system may be effective in some situations, incandescent light has poor photonic efficiency when passed through the optics required to image surfaces inside a patient in an endoscopic surgical environment. One reason that incandescent light has been conventionally used for real-time structured light depth extraction systems is that an incandescent lamp was the only type of light source thought to have sufficient power and frequency bandwidth to illuminate objects inside of a patient. Another problem with this conventional structured light depth extraction system is that using a single camera for both broadband light detection and depth extraction is suboptimal since the pixel requirements for broadband light detection are greater than those required for depth extraction, and the required frame speed is greater for depth extraction than for broadband light detection. Using a single camera results in unnecessary data being acquired for both operations. For example, if a high-resolution, high-speed camera is used for depth extraction and broadband light detection, an unnecessary number of images will be acquired per unit time for broadband light detection and the resolution of the images will be higher than necessary for depth extraction. The additional data acquired when using a single camera for both broadband light detection and depth extraction increases downstream memory storage and processing speed requirements.","Accordingly, in light of these difficulties associated with conventional real-time structured light depth extraction systems, there exists a long-felt need for improved methods and systems for real-time structured light depth extraction for an endoscopic surgical environment.","According to one aspect of the invention, a laser-based real-time structured light depth extraction system includes a laser light source for producing a collimated beam of laser light. A pattern generator is optically coupled to the laser light source and generates structured light patterns. Each pattern includes a plurality of pixels, which are simultaneously projected onto an object of interest. A detector is coupled to the light source and is synchronized with the pattern generator for receiving patterns reflected from the object and generating signals based on the patterns. A real-time structured light depth extraction engine\/controller is coupled to the detector and the pattern generator for controlling projection and detection of patterns and for calculating, in real-time, depth values of regions in the object based on signals received from the detector. In one implementation, the pattern generator comprises a reflective display, the detector comprises separate cameras for depth extraction and imaging, and the real-time structured light depth extraction engine\/controller comprises a general-purpose computer, such as a personal computer, with a frame grabber.","Using a laser light source rather than an incandescent lamp provides advantages over conventional real-time structured light depth extraction systems. For example, laser-based real-time structured light depth extraction systems have been shown to have better photonic efficiency than incandescent-lamp-based real-time structured light depth extraction systems. In one test, the efficiency of a laser-based real-time structured light depth extraction system for endoscopic surgery was shown to have 30% efficiency versus 1% efficiency for incandescent light. This increase in efficiency may be due to the fact that a laser-based real-time structured light depth extraction system may not require a colliminator in series with the laser light beam before contacting the pattern generator. In addition, a laser can be easily focused to fit within the diameter of the optical path of an endoscope, while the focal width of an incandescent light beam is limited by the width of the filament, which can be greater than the width of a conventional endoscope. Using a laser for real-time structured light depth extraction in an endoscopic surgical environment may thus allow a smaller diameter endoscope to be used, which results in less trauma to the patient.","Another surprising result of using a laser for real-time structured light depth extraction in an endoscopic surgical environment is that lasers produce sufficient information to extract depth in real time, even given the low power and narrow beam width associated with surgical illumination lasers. As stated above, endoscopic light sources were believed to be required for endoscopic surgical environments because they were the only light sources believed to have sufficient output power for surgical applications. The present inventors discovered that a laser with a beam width of about 2 mm and power consumption of about 5 mW can be used for real-time structured light depth extraction in an endoscopic surgical environment. Such a low power light source can be contrasted with the 40 W lamp used in conventional incandescent-lamp-based real-time structured light depth extraction systems.","In an alternate implementation of the invention, rather than using a laser light source and a separate display, the present invention may include using a self-illuminating display, such as an organic light emitting (OLE) display. Using an OLE display decreases the number of required optical components and thus decreases the size of the real-time structured light depth extraction system. Patterns can be generated and projected simply by addressing the appropriate pixels of the OLE display.","As used herein, the term \u201creal-time structured light depth extraction\u201d is intended to refer to depth extraction that results in rendering of images at a sufficiently high rate for a surgical environment, such as an endoscopic surgical environment. For example, in one implementation, images of an object with depth may be updated at a rate of at least about 30 frames per second.","Accordingly, it is an object of the present invention to provide improved methods and systems for real-time structured light depth extraction in an endoscopic surgical environment.","Some of the objects of the invention having been stated hereinabove, and which are addressed in whole or in part by the present invention, other objects will become evident as the description proceeds when taken in connection with the accompanying drawings as best described hereinbelow.",{"@attributes":{"id":"p-0024","num":"0023"},"figref":["FIG. 1","FIG. 1"],"b":["100","102","104","106","108","110","100","100","100"]},"An exemplary commercially available laser light source suitable for use with embodiments of the present invention is the NT56-499 available from Edmund Optics. The NT-56-499 is a 50 mW fiber coupled solid state laser that operates at 532 nm. Using a laser light source rather than a conventional incandescent lamp reduces the need for collimating optics between a laser light source and the pattern generator. However, collimating optics may be included between laser  and display  without departing from the scope of the invention.","A broadband light source  may be provided for broadband illumination of surfaces within a patient's body so that color images of the surfaces can be generated. An example of a broadband light source suitable for use with embodiments of the present invention is a white incandescent lamp.","Pattern generator  may be any suitable device for generating patterns that interact with the light from laser light source  and projecting the patterns onto an object of interest. In one exemplary implementation, pattern generator  may be a ferro-reflective display. In an alternate embodiment, pattern generator  may be a MEMs array, a liquid crystal display, or any other type of display for generating patterns, altering, i.e., reflecting or selectively transmitting, the beam of light generated by light source  to generate structured light patterns, and projecting the patterns onto an object of interest.","Detector  may include a single camera or a plurality of cameras. In the illustrated example, detector  includes a high-speed, low-resolution depth extraction camera  for detecting reflected structured light patterns and a low-speed, high-resolution color camera  for obtaining color images of an object of interest. For real-time structured light depth extraction applications for endoscopic surgery, depth extraction camera  may have a frame rate ranging from about 60 frames per second to about 260 frames per second and a resolution of about 640\u00d7480 pixels and is preferably tuned to the wavelength of light source . Color camera  may have a frame rate of no more than about 30 frames per second with a resolution of about 1000\u00d71000 pixels and is preferably capable of detecting reflected light over a broad frequency band.","Light sources  and  and cameras  and  may be operated synchronously or asynchronously with respect to each other. In an asynchronous mode of operation, structured light depth extraction and broadband illumination may occur simultaneously. In this mode of operation, a filter corresponding to the frequency band of light source  is preferably placed in front of light source  or in front of camera  to reduce reflected light energy in the frequency band of light source . In a synchronous mode of operation, light source  may be turned off during broadband illumination of the object by light source . In this mode of operation, since there should be no excess energy caused by light source  that would adversely affect the detection of broadband light, the bandpass or notch filter in front of light source  or camera  may be omitted.","Separating the depth extraction and broadband light detection functions using separate cameras optimized for their particular tasks decreases the data storage and processing requirements of downstream devices. However, the present invention is not limited to using separate cameras for depth extraction and broadband light detection. In an alternate embodiment, a single camera may be used for both real time structured light depth extraction and broadband light detection without departing from the scope of the invention. Such a camera preferably has sufficient resolution for broadband light detection and sufficient speed for real-time structured light depth extraction.","Real-time depth extraction engine\/controller  may be a general-purpose computer, such as a personal computer, with appropriate video processing hardware and depth extraction software. In one exemplary implementation, real-time depth extraction engine\/controller  may utilize a Matrox Genesis digital signal processing board to gather and process images. In such an implementation, real-time depth extraction software that determines depth based on transmitted and reflected images may be implemented using the Matrox Imaging Library as an interface to the digital signal processing board.","In an alternate implementation, the specialized digital signal processing board may be replaced by a frame grabber and processing may be performed by one or more processors resident on the general purpose computing device. Using a frame grabber rather than a specialized digital signal processing board reduces the overall cost of the real-time structured light depth extraction system over conventional systems that utilize specialized digital signal processing boards. In yet another exemplary implementation, depth extraction engine\/controller  may distribute the processing for real-time depth calculation across multiple processors located on separate general purpose computing platforms connected via a local area network. Any suitable method for distributed or single-processor-based real-time structured light depth extraction processing is intended to be within the scope of the invention.","In , the system also includes imaging optics for communicating light from laser  to pattern generator . In the illustrated example, the optics include a linear polarizer  and expansion optics  and . Linear polarizer  linearly polarizes light output from laser . Linear polarization  is desired for imaging surfaces within a patient's body to reduce specularity. Expansion optics  and  may include a negative focal length lens spaced from pattern generator  such that the beam width of the beam that contacts pattern generator  is equal to the area of pattern generator  used to generate the patterns. A collimator may be optionally included between light source  and pattern generator . Using a collimator may be desirable when laser light source  comprises a bare laser diode. The collimator may be omitted when laser light source  is physically structured to output a collimated beam of light.","In order to illuminate objects  in interior region  of a patient's body, the depth extraction hardware illustrated in  is coupled to the interior of a patient's body via an endoscope . Endoscope  may be a laparoscope including a first optical path  for transmitting structured light patterns into a patient's body and a second optical path  for communicating reflected structured light patterns from the patient's body. In order to fit the patterns within optical path , the system illustrated in  may include reduction optics  and . Reduction optics  and  may include a positive focal length lens spaced from the aperture of endoscope  such that the beamwidth of the structured light entering endoscope  is less than or equal to the diameter of optical path . In experiments, it was determined that even when the images reflected from pattern generator  are compressed, due to the collimated nature of light emanating from laser , little distortion is present in the images. As a result, depth information can be extracted more quickly and accurately.",{"@attributes":{"id":"p-0035","num":"0034"},"figref":["FIG. 2A","FIG. 1","FIG. 2A"],"b":["200","110","100","202","102","204","102","202","102"]},"The present invention is not limited to displaying positive and negative images on a ferro-reflective display. Any sequence of images suitable for real-time structured light depth extraction is intended to be within the scope of the invention. In one implementation, real-time depth extraction engines\/controller  may adaptively change images displayed by display  to extract varying degrees of depth detail in an object. For example, for higher resolution, real-time depth extraction engine\/controller  may control display  to display images with narrower stripes. In addition, within the same image, a portion of the image may have wide stripes and another portion may have narrow stripes or other patterns to extract varying degrees of detail within the image.","The resolution of the depth detail extracted may be controlled manually by the user or automatically by real-time depth extraction engine\/controller . For example, in an automatic control method, real-time depth extraction engine\/controller  may automatically adjust the level of detail if depth calculation values for a given image fail to result in a depth variation that is above or below a predetermined tolerance range, indicating that a change in the level of depth detail is needed. In a manual control method, the user may change the level of detail, for example, by actuating a mechanical or software control that triggers real-time depth extraction engine\/controller  to alter the patterns and increase or decrease the level of depth detail.","In step , display  reflects the beam of light from laser  to produce a structured light pattern. In step , beam compression optics  and  shrink the pattern to fit within optical path  of endoscope . In step , the pattern is projected through the endoscope optics. In step , the pattern is reflected from object  within the patient's body. The reflected pattern travels through optical path  of endoscope . In step , high speed camera  detects the reflected pattern and communicates the reflected pattern to real-time depth extraction engine\/controller .","In step , real-time depth extraction engine\/controller  calculates depth based on the transmitted and reflected patterns and the positions of camera  and display  as projected through the optics of endoscope . For example, the depth value may indicate the distance from object  to the end of endoscope . An exemplary algorithm for classifying pixels and calculating depth will be described in detail below.","As stated above, if a ferro-reflective display is used, a negative image may be projected after display of a positive image. Accordingly, in step , steps - are repeated and depth values are calculated for the negative image. In an alternate implementation of the invention, pattern generator  may generate only positive images.","In step , real-time depth extraction engine\/controller  generates a new image and steps - are repeated for the new image. The new image may be the same as or different from the previous image. The process steps illustrated in  are preferably repeated continuously during endoscopic surgery so that real-time depth information is continuously generated. The depth information may be used to generate a synthetic 3-D image of object . The synthetic image may be combined with a real image and displayed to the surgeon.",{"@attributes":{"id":"p-0042","num":"0041"},"figref":["FIG. 2B","FIG. 2B","FIG. 2A","FIG. 2B","FIG. 2A"],"b":["202","216","222","100","102","100","118","100","224","110","118","111","111","122","122","126","122","226","108","228","110","118"]},{"@attributes":{"id":"p-0043","num":"0042"},"figref":["FIG. 3","FIG. 3","FIG. 1"],"b":["100","102","300","102","100","100","300","300","106","300"]},"Using OLE display  eliminates the need for beam expansion optics  and . Beam compression optics  and  may be still be used to compress the size of the image to fit within optical path  of endoscope . Alternatively, OLE display  may be configured to produce a sufficiently small image to fit within the diameter of optical path . In such an implementation, beam compression optics  and  may be omitted. Thus, using an OLE display may reduce the number of components in a real-time structured light depth extraction system for endoscopic surgery according to an embodiment of the present invention.",{"@attributes":{"id":"p-0045","num":"0044"},"figref":["FIG. 4","FIG. 3","FIG. 4"],"b":["400","110","300","402","128","130","124","122","404","124","406","118","126","122","408","106","410","110"]},"If a performance advantage can be achieved by displaying sequences of positive and negative images, display  may be controlled to display a negative image after each positive image. Accordingly, in step , steps - are repeated for the negative image. In step , real-time depth extraction engine\/controller generates a new image that is preferably different from the original image. Steps - are then repeated for the new image. The steps illustrated in  for broadband illumination and color image generation may be performed concurrently with the steps illustrated in  to produce a color image that is combined with the depth image. The only difference being that in step , the OLE display, rather than the laser, may be turned off during broadband illumination. In addition, as discussed above with regard to laser-illumination, detection of structured light patterns and detection of reflected broadband illumination for OLE display illumination may be performed synchronously or asynchronously. Thus, using the steps illustrated in , an OLE display may be used to perform real-time structured light depth extraction in an endoscopic surgical environment.","The term \u201cstructured light\u201d is often used to specifically refer to structured light triangulation methods, but there are other structured light methods. These include the depth from defocus method. Laser scanning methods are distinct from structural light methods because laser scanning methods scan a single point of light across an object. Scanning a point of light across an object requires more time than the structured light methods of the present invention which simultaneously project a plurality of pixels of laser light onto the object being imaged. As stated above, the present invention may utilize multiple stripes of varying thickness to extract depth with varying resolution. The mathematics of calculating depth in depth-from-stereo or structured light triangulation methods will be described in detail below. Any of the structured light triangulation methods described below may be used by real-time depth extraction engine\/controller  to calculate depth information in real time.","Structured light is a widely used technique that may be useful in endoscopic surgical environments because it:\n\n","Structured light methods usually make some assumptions about how much variation in surface properties can be present in the scene. If this tolerance is exceeded, the method may not be able to recover the structure from the scene and may fail.","Light structures can be any pattern that is readily (and ideally unambiguously) recovered from the scene. The difference between the pattern injected into the scene and the pattern recovered gives rise to the depth information. Stripe patterns are frequently used because of the easy way that they can be recognized and because of some mathematical advantages discussed below. Many methods have been proposed to distinguish projected stripes from one another. These include color coding, pattern encoding (like barcodes for each stripe), and temporal encoding. Color coding schemes rely on the ability of the projector to accurately produce the correct color, the surface to reflect back the correct color, and the camera to record the correct color. While projectors and cameras can be color calibrated, this puts serious constraints on the appearance and reflective properties of objects to be scanned. Pattern encoding schemes assume enough spatial coherence of the object being scanned. Pattern encoding schemes assume enough spatial coherence of the object being scanned that enough of the code can be found to identify a stripe. Temporal encoding of stripe patterns is very popular because it makes few assumptions about the characteristics of the surface, including possibly working with surface texture. Motion during the scanning sequence can be a significant problem.","A simple and effective way to temporally encode many stripes is to project multiple patterns into the scene and use a binary encoding for each stripe. Assuming no movement of the scene or the scanner, the stripe that each pixel of the image belongs to is encoded by whether or not the pixel in each image is illuminated or not. This method reduces the number of projected patterns needed to encode n stripes to logn. A variety of sequences have been developed to reduce the potential for error by misidentification of a pixel's status in one or more images.","Several systems use the position of the edge of stripes rather than the position of the stripe itself for triangulation. One of the advantages to doing this is that subpixel accuracy for this position can be achieved. Another advantage of this method is that the edges may be easier to find in images with variations in texture than the stripes themselves. If the reflective properties of the anticipated scene are relatively uniform, the point of intersection of a plot of intensity values with a preset threshold, either in intensity value or in the first derivative of intensity, can be sufficient to find edges with subpixel precision. If a wider range of reflective patterns are anticipated, structure patterns can be designed to ensure that, in addition to labeling pixels as to which stripe they belong, each of these edges is seen in two images but with the opposite transition (that is, the transition from stripe one to two is on-off in one image and off-on in a later image). The point where a plot of intensity values at this edge cross each other can be used as a subpixel measure of the position.","The real-time system developed by Rustinkiewicz and Hall-Hot uses edges in an even more clever way. They recognized that a stripe must be bounded by two stripes and, over a sequence of our projected patterns, the status of both stripes can be changed arbitrarily. This means that 256 (2) different coding sequences can be projected over the sequence of four frames. After removing sequences in which the edge cannot be found (because the neighboring stripes are \u201con-on\u201d or \u201coff-off\u201d) in two sequential frames and where a stripe remains \u201con\u201d or \u201coff\u201d for the entire sequence, the latter sequence is removed to avoid the possible confusion of their edge-finding method of these stripes with texture on the surface of the object. After such considerations, one hundred ten (110) stripe edge encodings may be used. This method could potentially be applied using more neighboring stripes to encode a particular stripe edge or a larger number of frames. For n frames and m stripes to encode each boundary the number of possible stripes that can be encoded is approximately proportional to (m). It should be noted that to project x stripe patterns, O(logx) frames are needed as in the case of binary encoded stripes except the scaler multipliers on this limiting function are generally much smaller for this technique as compared to binary encoding.","The Rustinkiewicz triangulation system uses the rate of change (first derivative) of intensity to find boundary edges with the sign determining what type of transition is observed. Efficient encoding demands that many edges be encoded by sequences in which the edge is not visible in every frame. The existence and approximate position of these \u201cghost\u201d edges is inferred and matched to the nearest found edge or hypothesized edge in the previous frame. The encoding scheme limits the number of frames in which an edge can exist as a ghost, but the matching process limits the motion of objects in the scene to not more than half of the average stripe width between frames. The Rustinkiewicz triangulation system may have a lower threshold for motion if objects have high-frequency textures or any sudden changes in surface texture that would be misinterpreted as an edge. The Rustinkiewicz triangulation method might also fail for objects that are moving in or out of shadows (that is, the camera sees a point on the surface, but the projector cannot illuminate it). The primary application for this system is building high-resolution three-dimensional models of objects. A modified iterated closest point (ICP) algorithm is used to match and register the range images acquired from frame to frame in order to build up the dataset.","The speed of structured light triangulation systems is limited by three major factors. These are the rate at which structured patterns can be projected, the rate at which an image of the scene with the structured pattern present can be captured, and the rate at which the captured images can be processed. The method of Rusinkiewicz reduces the demands on the projector and camera by use of sophisticated processing and judicious use of assumptions of spatial and temporal coherence. An alternative approach that may be used with implementations of the present invention is to use simple, high-speed, hardware acceleratable algorithms, while placing greater demands on the camera and projector portion of the system both to compensate for motion and to achieve a useful rate of output for an augmented reality visualation system for endoscopic surgery.","Many currently available displays including LCD and micro-electromechanical systems (MEMS) (e.g. Texas Instruments' DMD\u2122 digital micromirror display) based devices are used to project images with refresh rates over 60 Hz. These devices are frequently used to project color imagery by using small time slices to project component colors of the total displayed imagery. The underlying display in these devices is capable of projecting monochrome images (black and white, not grayscale) at over 40 kHz (for a DMD based device). High-speed digital cameras are available that capture images at over 10 kHz. While cameras are advertised that capture at these extremely high rates, most cannot capture \u201cfull frame\u201d images at the rate. Currently advertised cameras can record sequences at 1 kHz with a resolution equivalent to standard video (640\u00d7480, non-interlaced). While most of these devices are simply used for recording short high-speed sequences, image capture and analysis cards are available which are capable of handling the data stream and performing simple analysis at high data rates.","Given the devices that are currently available, the remaining challenges include the development of algorithms that can be executed at high speed, algorithms that can deal with texture or the absence of texture, specularity, and small amounts of motion. An additional challenge is to build a device that satisfies these conditions and is compatible with laparoscopic surgery. Exemplary algorithms for recovering depth in a laparoscopic surgical environment will now be described.","A structured light depth extraction method suitable for use with the present invention can be thought of as a special case of obtaining depth from stereo. Instead of a second camera, a projector is used so that disparity can be defined as the difference between where the projector puts a feature and where the camera finds the feature (rather than being the difference between where the two cameras find a scene feature). First, a simple mathematical model of depth from stereo will be discussed, then the changes needed to apply this model to a camera and projector will be discussed.","A schematic diagram of a single scan line depth from stereo depth extraction system is shown in . In such a simple case of depth from stereo, the cameras are coplanar, are directed parallel to each other, and are ideal \u201cpin-hole\u201d cameras with identical internal parameters. In , the thick lines represent the cameras, P is the point in space observed, d is the separation between the cameras, D is the distance to P, f is the distance between the pinhole and the imaging plane of the cameras, and xand xare the position of the observed object on the imaging plane of the two cameras.","In this case, the relationship between the depth of a point seen by both cameras (D), focal distance (f), camera separation (d), and the observed position of the point on the image planes of both cameras (xand x) is (by similar triangles):",{"@attributes":{"id":"p-0061","num":"0063"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"D","mo":"=","mfrac":{"mrow":[{"mi":["d","f"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"msub":[{"mi":"x","mn":"1"},{"mi":"x","mn":"2"}],"mo":"-"}]}}},{"mrow":{"mo":["(",")"],"mn":"1"}}]}}}}},"In Equation (1), the denominator contains the term xand x, which is the disparity or difference of position of the object in the two cameras.","This relationship holds if the cameras have two-dimensional imagers rather than the one-dimensional imagers that were initially assumed. This can be demonstrated by observing that the angle of elevation between the ray starting at the pinhole and ending at the point in space (\u03c6) is the same if the earlier assumptions about the cameras are maintained. The implication of this observation is that if the point is observed at a position (x, y) in the first camera, its position in the second camera must be at (x, y)\u2014that is, its position in the second camera is limited to a single horizontal line. If some a priori information is known about the distance to the point, the portion of the image can be further reduced.",{"@attributes":{"id":"p-0064","num":"0066"},"figref":["FIG. 6","FIG. 6"],"sub":["1 ","2 "]},"As the initial assumptions are broken, the relationship becomes more complicated. Fortunately, points projected onto a two-dimensional plane can be reprojected onto a different plane using the same center of projection. Such a reprojection can be done in two or three-dimensional homogenous coordinates and takes the following form:",{"@attributes":{"id":"p-0066","num":"0068"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msub":{"mi":["x","r"]}}},{"mtd":{"msub":{"mi":["y","r"]}}},{"mtd":{"mi":"\u03c9"}}]}},{"mrow":[{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"msub":{"mi":"c","mn":"11"}},{"msub":{"mi":"c","mn":"12"}},{"msub":{"mi":"c","mn":"13"}}]},{"mtd":[{"msub":{"mi":"c","mn":"21"}},{"msub":{"mi":"c","mn":"22"}},{"msub":{"mi":"c","mn":"23"}}]},{"mtd":[{"msub":{"mi":"c","mn":"31"}},{"msub":{"mi":"c","mn":"32"}},{"msub":{"mi":"c","mn":"33"}}]}]}},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mi":"x"}},{"mtd":{"mi":"y"}},{"mtd":{"mn":"1"}}]}}],"mo":"\u2061"}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"2"}}]}}}},"br":{},"sub":["c","c"]},{"@attributes":{"id":"p-0067","num":"0069"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msub":{"mi":["x","r"]}}},{"mtd":{"msub":{"mi":["y","r"]}}}]}},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mfrac":{"msub":{"mi":["x","c"]},"mi":"\u03c9"}}},{"mtd":{"mfrac":{"msub":{"mi":["y","c"]},"mi":"\u03c9"}}}]}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"3"}}]}}}}},"Reprojection of this type is performed very efficiently by current graphics and image processing hardware. In the special case where reprojection is done onto parallel planes, \u03c9 becomes a constant for all coordinates (x, y) in the original image. An important implication is the observation that, because the operation of perspective projection preserves straight lines as straight lines in the resulting image, an object observed in a particular location in one camera's view must be located somewhere along a straight line (not necessarily on a scan line) in the other camera's image. This constraint is known as the epipolar constraint.","Further simplification can be achieved if reprojection is done to planes that are nearly parallel. This assumes that the original planes of projection are very similar to the rectified ones. That is, the center of projections are approximately aligned. In this case, \u03c9 varies little in the region being reprojected. Therefore, rough approximation of the depth calculation, without using any reprojection, is simply (where C, k, and K are appropriately selected arbitrary constants):",{"@attributes":{"id":"p-0070","num":"0072"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"D","mo":"=","mfrac":{"mi":"K","mrow":{"mrow":[{"msub":[{"mi":"C","mn":"1"},{"mi":"x","mn":"1"}],"mo":"\u2062"},{"msub":[{"mi":"C","mn":"2"},{"mi":"y","mn":"1"}],"mo":"\u2062"},{"msub":[{"mi":"C","mn":"3"},{"mi":"y","mn":"2"}],"mo":"\u2062"}],"mo":["+","+","+","+"],"msub":[{"mi":"x","mn":"2"},{"mi":"C","mn":"4"}]}}}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}}}}},"The approximation in Equation (4) can be used as the basis of a simple calibration for a structured light depth extraction system. As it is a rough approximation, it should be applied with great caution if a high degree of accuracy is needed.","The process of rectification can also be used to correct for distortion of camera images from an idealized pin-hole model. There are a variety of methods to measure the distortion in camera images. These methods use graphics or image processing hardware to rapidly correct for these distortions to create a distortion-free image. While rectification, particularly the reprojection in homogeneous coordinates can be done in hardware, it is not always advisable to do so. Practical implementation of reprojection usually entails a loss in image quality. Minor corrections usually cause little loss of information, but large differences in the position of the planes of projection can cause a loss of a large number of pixels in some portions of the resultant image and smearing of pixels across wide areas in other portions of the resultant image. In these cases it is wise to find positions of objects in the original images, and reproves only the found (x, y) coordinates of those objects. In most cases it is easier to correct for camera distortion before looking for objects in the images so that the epipolar constraints are straight lines and not the curved lines that are possible in camera images with distortions.","When one of the cameras from the model discussed in the previous section is replaced with a projector, the meaning of some of the steps and variables changes. These changes have several important implications.","In a real-time structured light depth extraction system of the present invention, one of the cameras in  may be replaced with a projector that is essentially an inverse of a pinhole camera, like a camera obscura projecting on the wall of a darkened room. The projector by itself gathers no data about the point in space. If a single ray of light is projected into space, and no other light is available, then the second camera can now unambiguously see the point where that ray strikes a distant surface. In this situation, the coordinate of the light being projected is xand the point where the camera sees that point of light is x. If two-dimensional cameras and projectors are used, the ray (x, y) is illuminated by the projector, illuminating the surface at a point which is then seen by the camera at the point (x, y). The mathematical relationships of the previous section then hold.","Reprojection and rectification, as described in the previous section, take on a new meaning for the projector. Rather than \u201cundistorting\u201d an acquired image, the projected image can be \u201cpredistorted\u201d to account for the projection optics and variations in the position, orientation, and characteristics of the relative position of the camera and projector combination. This predistortion should be the inverse of the undistortion that would need to be applied if it were a camera rather than a projector.","Another implication of the epipolar constraints in the case of the projector-camera combination is that more than one ray can be illuminated simultaneously and they can easily be distinguished. In the simplest case, where the camera and projector have identical internal characteristics, are pointed in a parallel direction, and have coplanar planes of projection, the epipolar lines run parallel to the x axis. A ray projected to (x, y) will only be seen along the yscanline in the camera. A second ray projected through (x, y) will only possibly be seen on the yscanline in the camera. In practice it is usually simplest to project vertical stripes (assuming that the camera and projector are horizontally arranged). Because each scanline in the camera corresponds to a single horizontal line in the camera, the found x position of the stripe projected through position xcan be converted to depth using Equation (1), after appropriately substituting x:",{"@attributes":{"id":"p-0077","num":"0079"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"D","mo":"=","mfrac":{"mrow":[{"mi":["d","f"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"msub":{"mi":["x","s"]},"mo":"-","mi":"x"}]}}},{"mrow":{"mo":["(",")"],"mn":"5"}}]}}}}},"As stated above, in order to perform real-time structured light depth extraction, it is necessary to locate or classify pixels or stripes from the transmitted image in the reflected image so that offsets can be determined. The following is an example of a method for classifying pixels in a real-time structured light depth extraction system suitable for use with embodiments of the present invention. In the example, the following source data is used:","m images, P, with projected stripe patterns","m images, N, with the inverse stripe patters projected","m is the number of bits of striping (for binary encoded structured light)","One sub-optimal method for classifying pixels includes the following steps:\n\n",{"@attributes":{"id":"p-0083","num":"0087"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["L","n"]},"mo":"=","mrow":{"mo":"{","mtable":{"mtr":[{"mtd":[{"mn":"0"},{"mi":"if"},{"mrow":{"msub":{"mi":["C","n"]},"mo":"\u2264","mrow":{"msup":{"mn":"2","mrow":{"mi":"b","mo":"-","mn":"1"}},"mo":"-","mi":"\u03b3"}}}]},{"mtd":[{"mn":"1"},{"mi":"if"},{"mrow":{"msub":{"mi":["C","n"]},"mo":"\u2265","mrow":{"msup":{"mn":"2","mrow":{"mi":"b","mo":"-","mn":"1"}},"mo":"+","mi":"\u03b3"}}}]},{"mtd":[{"mi":"undefined"},{"mi":"if"},{"mrow":{"mrow":[{"msup":{"mn":"2","mrow":{"mi":"b","mo":"-","mn":"1"}},"mo":"-","mi":"r"},{"msup":{"mn":"2","mrow":{"mi":"b","mo":"-","mn":"1"}},"mo":"+","mi":"\u03b3"}],"mo":["<","<"],"msub":{"mi":["C","n"]}}}]}]}}}},{"mrow":{"mo":["(",")"],"mn":"7"}}]}}}},"ul":{"@attributes":{"id":"ul0005","list-style":"none"},"li":{"@attributes":{"id":"ul0005-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0006","list-style":"none"},"li":"3) Record which pixels cannot be identified in this image pair."}}}},{"@attributes":{"id":"p-0084","num":"0089"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["U","n"]},"mo":"=","mrow":{"mo":"{","mtable":{"mtr":[{"mtd":[{"mn":"0"},{"mi":"if"},{"mrow":{"mrow":[{"msup":{"mn":"2","mrow":{"mi":"b","mo":"-","mn":"1"}},"mo":"-","mi":"r"},{"msup":{"mn":"2","mrow":{"mi":"b","mo":"-","mn":"1"}},"mo":"+","mi":"\u03b3"}],"mo":["<","<"],"msub":{"mi":["C","n"]}}}]},{"mtd":[{"mn":"1"},{"mi":"otherwise"},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}]}]}}}},{"mrow":{"mo":["(",")"],"mn":"8"}}]}}}}},"In the case of binary encoding of the stripes, an image with each pixel labeled with the stripe encoded, S can be created by",{"@attributes":{"id":"p-0086","num":"0091"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"S","mo":"=","mrow":{"mrow":[{"mo":["(",")"],"mrow":{"mn":"1","mo":"+","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"msup":{"mn":"2","mi":"i"},"mo":"\u2062","msub":{"mi":["L","i"]}}}}},{"munderover":{"mo":"\u220f","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","msub":{"mi":["U","i"]}}],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}},{"mrow":{"mo":["(",")"],"mn":"9"}}]}}}}},"The multiplication by the product of all images Uensures that any pixel in which the stripe pattern could not be identified in any image pair is not labeled as belonging to a stripe. Thus, the image S contains stripes labeled from 1 to 2+1 with zero value pixels indicating that a stripe could not be identified for that pixel in one or more image pairs.","Equation (6) is approximately normalized so the result fits into the image data type (b bits). This will most likely result in the loss of the low order bit.",{"@attributes":{"id":"p-0089","num":"0094"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"C","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},{"mfrac":[{"msub":{"mi":["P","n"]},"mn":"2"},{"msub":{"mi":["N","n"]},"mn":"2"}],"mo":["+","-"],"msup":{"mn":"2","mrow":{"mi":"b","mo":"-","mn":"1"}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"10"}}]}}}}},"The next step is to apply thresholds to identify and mark the \u2018on\u2019 and \u2018off\u2019 pixels. First, threshold to find the \u2018on\u2019 pixels by setting bit n\u22121 if the intensity of that pixel is greater than 128 plus a preset level \u03b3. Next, threshold \u2018off\u2019 pixels by setting bit n\u22121 to zero for pixels with intensity greater than 128 minus a preset level \u03b3. Pixels that are not part of these two ranges are labeled non-identifiable",{"@attributes":{"id":"p-0091","num":"0096"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},{"mo":"{","mtable":{"mtr":[{"mtd":[{"mn":"0"},{"mi":"if"},{"mrow":{"mrow":[{"mi":"C","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},{"msup":{"mn":"2","mrow":{"mi":"b","mo":"-","mn":"1"}},"mo":"-","mi":"\u03b3"}],"mo":"\u2264"}}]},{"mtd":[{"msup":{"mn":"2","mrow":{"mi":"n","mo":"-","mn":"1"}}},{"mi":"if"},{"mrow":{"mrow":[{"mi":"C","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},{"msup":{"mn":"2","mrow":{"mi":"b","mo":"-","mn":"1"}},"mo":"+","mi":"\u03b3"}],"mo":"\u2265"}}]},{"mtd":[{"msup":{"mn":"2","mi":"m"}},{"mi":"if"},{"mrow":{"mrow":[{"msup":{"mn":"2","mrow":{"mi":"b","mo":"-","mn":"1"}},"mo":"-","mi":"\u03b3"},{"mi":"C","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},{"msup":{"mn":"2","mrow":{"mi":"b","mo":"-","mn":"1"}},"mo":"+","mi":"\u03b3"}],"mo":["<","<"]}}]}]}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"11"}}]}}}},"br":{}},"In this way, Equation (11) combines Equations (7) and (8) from the non-optimized version into a single operation. Likewise, two images, Uand L, in the non-optimized method can be combined into a single L.","The set of images L may be combined with a bitwise OR as they are generated or as a separate step creating a stripe encoded image, S.",{"@attributes":{"id":"p-0094","num":"0099"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"S","mo":"=","mrow":{"munderover":{"mo":"\u22c3","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":"\u2062","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"i"}}}}},{"mrow":{"mo":["(",")"],"mn":"12"}}]}}}}},"The resulting image S from the optimized method differs from that obtained in the non-optimized method in that non-identified pixels have values greater than 2rather than having a value of 0.","The stripe labeled image S may be converted to a range image using normal techniques used in structured light as discussed above. Pre-calibrated look-up tables and regression fitted functions may be useful methods to rapidly progress from segmented images to renderable surfaces.","Table 1 below shows the time to complete the image processing phase in a prototype real-time structured light depth extraction system. This implementation is capable of finding stripes for 7-bits of stripe patterns at a rate of ten times per second. Significant optimizations still remain that result in much faster operation. One exemplary implementation uses the Matrox Imaging Library (MIL) as a high-level programming interface with the Matrox Genesis digital signal processing board. Using MIL greatly simplifies programming the digital signal processors but adds overhead and loss of fine level tuning. Additionally, image processing commands through MIL are issued to the Genesis board via the system bus while low level programming of the processors allow the processors to run autonomously from the rest of the PC. Other processes on the PC cause delays up to 50 milliseconds.",{"@attributes":{"id":"p-0098","num":"0103"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 1"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Depth Extraction Performance"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"4","colwidth":"42pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Convolution","n = 7","n = 5","n = 3"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"4","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"5","colwidth":"42pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Camera","ON","\u2002100 ms","\u200373 ms","\u200247 ms"]},{"entry":["capture off","OFF","\u200388 ms","\u200364 ms","\u200241 ms"]},{"entry":["Camera","ON","2203 ms","1602 ms","967 ms"]},{"entry":["capture on","OFF","2201 ms","1568 ms","966 ms"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}}]}}},"Table 1 illustrates performance on a PII\/400 MHz with a Matrox Genesis board for capture and processing. Performance is based on the mean time of 20 uninterrupted runs. The value n refers to the number of bits of striping used. For each run, 2n images were processed. Runs performed with camera capture turned off were completed by pre-loading sample images into buffers on the Matrox Genesis card and copying them to new buffers for processing when needed.","When camera capture is enabled, the process is dramatically slowed. A large portion of the time is used for synchronization. Each time a camera capture is needed, the prototype waits at least one and a half frames of video. This is done to ensure that the updated stripe pattern is actually being projected by the projector and to make sure that the camera is at the beginning of the frame when capture starts. Some of these delays can be reduced by triggering the camera only when a capture is needed. In one exemplary implementation, no image processing is performed while waiting for the next images to be captured. A lower level implementation of the system should have the processors on the capture and processing board operate while the next image is being captured.","Two experiments were performed on animal tissues to test these methods. The first used a porcine cadaver and an uncalibrated system with off-line processing. The second experiment was performed on chicken organs with on-line segmentation and off-line rendering.","Further simplification of Equation 4 is useful to get a reasonable calibration for the prototype system. If the projected patterns and captured images are nearly rectified, then it can be assumed that a scan line in the camera images implies only a single scan line from the projector. Ideally the y terms could all be dropped, but instead the y value from the camera image is maintained:\n\n\u2003\u2003(13)\n","This is the assumption that a y position in the first camera implies (linearly) a y position in the second camera. This implies that the original center of projection is very similar (except for scaling) to that used for reprojection (mostly to account for differences in camera characteristics) that might be needed. Equation (13) can be written as Cy+Cy+x=By+Bx+k as a very rough accommodation for greater variation from the ideal model. While this approach is more correct if one were to measure what the coefficients should be based on camera parameters, the result, Equation (15) is the same for achieving a good fit with a regression model using either model. Under these assumptions",{"@attributes":{"id":"p-0104","num":"0109"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"D","mo":"=","mfrac":{"mi":"K","mrow":{"mrow":{"msub":[{"mi":"C","mn":"1"},{"mi":"x","mn":"1"}],"mo":"\u2062"},"mo":["+","+","+"],"msub":[{"mi":"x","mn":"2"},{"mi":"By","mn":"2"},{"mi":"C","mn":"4"}]}}}},{"mrow":{"mo":["(",")"],"mn":"14"}}]}}}}},"Which is equivalent to:",{"@attributes":{"id":"p-0106","num":"0111"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mfrac":{"mn":"1","mi":"D"},"mo":"=","mrow":{"mrow":[{"msub":[{"mi":"c","mn":"1"},{"mi":"x","mn":"1"}],"mo":"\u2062"},{"msub":[{"mi":"c","mn":"2"},{"mi":"x","mn":"2"}],"mo":"\u2062"},{"msub":[{"mi":"c","mn":"3"},{"mi":"y","mn":"2"}],"mo":"\u2062"}],"mo":["+","+","+"],"msub":{"mi":"c","mn":"4"}}}},{"mrow":{"mo":["(",")"],"mn":"15"}}]}}}},"br":{},"sub":"n "},"A rough calibration of the prototype can then be achieved by linear regression with 1\/D as the independent variables, the stripe number, the stripe's position, the scan line in the camera image, and a constant. A linear model also lends itself to rapid calculation on processed images.","The methods and systems described herein may be used to generate depth information in real-time and are particularly well suited for surgical environments, such as endoscopic surgical environments. For example, the methods and systems described herein may be used to generate synthetic images that are projected onto real images of a patient for use in augmented reality visualization systems. One example of an augmented reality visualization system with which embodiments of the present invention may be used as described in commonly-assigned U.S. Pat. No. 6,503,195, the disclosure of which is incorporated herein by reference in its entirety. By using a laser rather than an incandescent lamp, the real-time structured light depth extraction systems of the present invention achieve greater photonic efficiency and consume less power than conventional real-time structured light depth extraction systems. In addition, by separating the structured light image gathering and broadband illumination detection functions, cameras optimized for each function can be used and downstream image processing is reduced.","The present invention is not limited to using laser-based or OLE-display-based depth extraction in an endoscopic surgical environment. The methods and systems used herein may be used to obtain depth in any system in which it is desirable to accurately obtain depth information in real time. For example, the methods and systems described herein may be used to measure depth associated with parts inside of a machine, such as a turbine.","It will be understood that various details of the invention may be changed without departing from the scope of the invention. Furthermore, the foregoing description is for the purpose of illustration only, and not for the purpose of limitation, as the invention is defined by the claims as set forth hereinafter."],"GOVINT":[{},{}],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["Preferred embodiments of the invention will now be explained with reference to the accompanying drawings, of which:",{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":["FIG. 2A","FIG. 1"]},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 2B"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":["FIG. 4","FIG. 3"]},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 6"}]},"DETDESC":[{},{}]}
