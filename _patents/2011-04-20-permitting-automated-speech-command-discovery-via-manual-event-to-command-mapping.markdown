---
title: Permitting automated speech command discovery via manual event to command mapping
abstract: An input from a manually initiated action within a computing system can be received. The system can be associated with a speech component. The input can be associated with a system function. The function can be an operation within the computing system and can be linked to a function identifier. The identifier can be translated to a command data. The command data can be associated with a command identifier, a command, and an alternative command. The command data can be a speech command registered within the speech component. The command data can be presented within a speech interface responsive to the translating. The speech interface can be associated with the speech component.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09368107&OS=09368107&RS=09368107
owner: Nuance Communications, Inc.
number: 09368107
owner_city: Burlington
owner_country: US
publication_date: 20110420
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","BRIEF SUMMARY","DETAILED DESCRIPTION"],"p":["The present invention relates to the field of speech user interfaces.","Applications which utilize a speech interface operate via speech recognition to identify a speech (e.g., spoken) command. Spoken commands must utilize one or more correct phrases in order to actuate a function provided by the application. For example, to close a vehicle window within a speech enabled automobile, phrases such as \u201cRaise the window\u201d or \u201cRoll-up the window\u201d can be equivalent, resulting in the closing of the window. In almost all cases, however, speech commands and\/or phrases must be discovered by trial and error, by referring to a list provided, or in an off-line manual (e.g., vehicle manual).","These solutions fail to fully utilize the speech interface functionality to address the discovery of commands. Provided lists can be organized into task domains, which do not permit easy identification of a specific command readily. For instance, to manually discover a tune station speech command, a user must select \u201cRadio Control\u201d task domain, select \u201cMode Management\u201d, then select \u201cTuning\u201d to discover the appropriate speech command. Further, many functions can be associated with a single speech command. Consequently, identifying an appropriate speech command to utilize can be troublesome in many scenarios. For example, within an automobile, traditional manual speech command discovery can result in safety issues by distracting a driver from maintaining road awareness.","One aspect of the present invention can include a system, an apparatus, a computer program product, and a method for permitting automated speech command discovery via manual event to command mapping. An input from a manually initiated action within a computing system can be received. The system can be associated with a speech component. The input can be associated with a system function. The function can be an operation within the computing system and can be linked to a function identifier. The identifier can be translated to a command data. The command data can be associated with a command identifier, a command, and an alternative command. The command data can be a speech command registered within the speech component. The command data can be presented within a speech interface responsive to the translating. The speech interface can be associated with the speech component.","Another aspect of the present invention can include a method, an apparatus, a computer program product, and a system for permitting automated speech command discovery via manual event to command mapping. A discovery engine can be configured to present a speech command responsive to a manually initiated action detected by a computing system. The manually initiated action can be linked to a function within the computing system. The function can be an operation within the computing system associated with a function identifier. The computing system can be associated with a speech component. A data store can be able to persist a command mapping associated with the speech component. The mapping can link the function identifier with a command data. The command data can include a command identifier, a command, and an alternative command.","The present disclosure is a solution for permitting automated speech command discovery via manual event to function mapping. In the solution, a manual event (e.g., manually performed operation) within a computing system can be mapped to a function within a speech interface. The mapping can trigger speech command discovery via one or more customizable rulesets. The rulesets can permit discovery using a grammar tree, utterance length, text strings, and the like. In one embodiment, the disclosure can be present within an automobile allowing rapid discovery of speech commands without requiring manual discovery. In the embodiment, driver actions can trigger speech command presentation. For instance, when a driver adjusts the air conditioning temperature manually, a speech interface of the automobile can announce \u201cYou could have said \u2018increase temperature one degree\u2019 to adjust the air conditioning\u201d.","As will be appreciated by one skilled in the art, aspects of the present invention may be embodied as a system, method or computer program product. Accordingly, aspects of the present invention may take the form of an entirely hardware embodiment, an entirely software embodiment (including firmware, resident software, micro-code, etc.) or an embodiment combining software and hardware aspects that may all generally be referred to herein as a \u201ccircuit,\u201d \u201cmodule\u201d or \u201csystem.\u201d Furthermore, aspects of the present invention may take the form of a computer program product embodied in one or more computer readable medium(s) having computer readable program code embodied thereon.","Any combination of one or more computer readable medium(s) may be utilized. The computer readable medium may be a computer readable signal medium or a computer readable storage medium. A computer readable storage medium may be, for example, but not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, or device, or any suitable combination of the foregoing. More specific examples (a non-exhaustive list) of the computer readable storage medium would include the following: an electrical connection having one or more wires, a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), an optical fiber, a portable compact disc read-only memory (CD-ROM), an optical storage device, a magnetic storage device, or any suitable combination of the foregoing. In the context of this document, a computer readable storage medium may be any tangible medium that can contain, or store a program for use by or in connection with an instruction execution system, apparatus, or device.","A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein, for example, in baseband or as part of a carrier wave. Such a propagated signal may take any of a variety of forms, including, but not limited to, electro-magnetic, optical, or any suitable combination thereof. A computer readable signal medium may be any computer readable medium that is not a computer readable storage medium and that can communicate, propagate, or transport a program for use by or in connection with an instruction execution system, apparatus, or device.","Program code embodied on a computer readable medium may be transmitted using any appropriate medium, including but not limited to wireless, wireline, optical fiber cable, RF, etc., or any suitable combination of the foregoing. Computer program code for carrying out operations for aspects of the present invention may be written in any combination of one or more programming languages, including an object oriented programming language such as Java, Smalltalk, C++ or the like and conventional procedural programming languages, such as the \u201cC\u201d programming language or similar programming languages. The program code may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software package, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario, the remote computer may be connected to the user's computer through any type of network, including a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).","Aspects of the present invention are described below with reference to flowchart illustrations and\/or block diagrams of methods, apparatus (systems) and computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and\/or block diagrams, and combinations of blocks in the flowchart illustrations and\/or block diagrams, can be implemented by computer program instructions.","These computer program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute via the processor of the computer or other programmable data processing apparatus, create means for implementing the functions\/acts specified in the flowchart and\/or block diagram block or blocks.","These computer program instructions may also be stored in a computer readable medium that can direct a computer, other programmable data processing apparatus, or other devices to function in a particular manner, such that the instructions stored in the computer readable medium produce an article of manufacture including instructions which implement the function\/act specified in the flowchart and\/or block diagram block or blocks.","The computer program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other devices to cause a series of operational steps to be performed on the computer, other programmable apparatus or other devices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions\/acts specified in the flowchart and\/or block diagram block or blocks.",{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 1","b":["100","100"]},"In one embodiment, the computing system can be an in-vehicle communications and\/or entertainment system. In the embodiment, an operation can be linked to a speech command which can be easily discovered when the operation is manually performed. Operations can include, but is not limited to, telephone calls, audio control, vehicle functionality adjustments, and the like. For example, when a user opens a car window via toggling an electronic power window switch, a speech interface can audibly present an appropriate voice activated command to the user.","Speech-enabled computing system can include a conventional and\/or non-conventional computing system associated with a speech interface. Speech interface can be a user interactive interface able to present a speech command responsive to a manual event. Speech interface can include, a loudspeaker, a microphone, a visual display, a user input device (e.g., interactive buttons), and the like. The speech interface can be a user interface which can include, but is not limited to, a graphical user interface (GUI), voice user interface (VUI), a mixed-mode interface, a text-based interface, and the like. In one embodiment, computing system can be associated with a speech-enabled application. In the embodiment, application can utilize disclosure functionality to allow speech command discovery of application operations.","In step , a manual event can be received by the speech-enabled computing system. The manual event can be a manually initiated action including, but not limited to, interaction with a physical component of the speech-enabled computing system, interaction with a digital element of the speech-enabled computing system, and the like. For instance, the manual event can be physically tuning a radio station utilizing a tuning knob on a head unit.","In step , an event can be mapped to a function within the computing system. Function can be mapped to the event using a command mapping. Command mapping can link a system function to a speech command which can automatically trigger the execution of the function. Command mapping can include, but is not limited to, a system component identifier, a function identifier, command identifier, a command, an alternative command, and the like. Command mapping can be manually and\/or automatically generated. In one instance, command mapping can be generated during runtime. In the instance, user customizable phrases can be present within the command mapping. That is, the disclosure can adapt to user customizations.","In step , a speech command data associated with the function can be determined within the speech-enabled computing system. In one instance, the speech command data can be a portion of the command mapping. In another instance, the speech command data can be a portion of an externally linked resource. In the embodiment, the externally linked resource can be a non-native speech database allowing. For example, the speech database can assist non-native speakers to verbally reiterate the command. In step , command data can be conveyed to a speech interface. The command data can be communicated via one or more networks communicatively linking the speech-enabled computing system to the speech interface. In one embodiment, the speech command can be visually presented within an interface visible to a user. In another embodiment, the speech command can be audibly presented within a speech interface proximate a user. In the embodiment, the speech command can be enunciated permitting the user to be assisted in learning the command pronunciation.","In step , if an alternative command is requested, the method can continue to step , else proceed to step . The request can be received via the speech interface and\/or an interface associated with the speech-enabled computing system. In step , an alternative command data can be identified. The alternative command data can be identified utilizing command mapping and\/or an alternative command resource (e.g., synonym list). In step , the alternative command data can be conveyed to the speech interface of the speech-enabled computing system. In step , the method can end.","Speech-enabled computing system can include one or more computing devices capable of speech recognition. Computing system can include, but is not limited to hardware, software, firmware, and the like. Hardware can include, but is not limited to, computing devices communicatively linked together. Computing devices can include, but is not limited to, a laptop, a mobile phone, a portable digital assistant (PDA), a tablet computing device, a portable media player, a car-puter, and the like. Computing system can be an operating system, a collection of applications, and the like. Computing system can include, but is not limited to, a vehicle control system, a Global Navigation System (GPS) navigation system, a communication system, an entertainment system, a domotic system (e.g., home automation), a robotic computing system, a personal computing system, and the like.","Drawings presented herein are for illustrative purposes only and should not be construed to limit the invention in any regard. Method  can be performed for each manually initiated action associated with a component of the computing system. Method  can be associated with one or more software applications including, but not limited to, data entry applications, word processing software, hands-free computing software, and the like. It should be understood that method  can be performed in real-time and\/or near real-time.",{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 2","b":["200","200","100","200","210","250","280","210","252","256","232","250","246","246","246","230","256"]},"In one embodiment, engine  can be a component of a home automation system allowing reduced learning curves for voice controls. In another embodiment, engine  can be a component of a communication system allowing call termination and management speech commands to be presented. In yet another embodiment, engine  can be a component of a mobile phone computing device, enabling multiple software applications to leverage engine  functionality.","Function data  can be one or more data sets associated with application  indicating an operation within the application which is linked to a manual event. Function data  can include, but is not limited to, function identifier, function description, function values, and the like. Function identifier can include, but is not limited to, numeric values, alphanumeric values, and the like. In one instance, data  can be encrypted to protect application  infrastructure and\/or data.","Speech data  can be one or more entities for identifying a speech command linked to an application function triggered by a manual event. Data  can include, but is not limited to, function information, speech synthesis information, and the like. Speech synthesis information can include, but is not limited to, linguistic analysis information, wave form data, and the like.","Discovery server  can be a hardware\/software component for enabling automated discovery of speech commands in response to a manual event. Server  can include, but is not limited to, engine , data store , and the like. Server  can be a component of a speech recognition entity. In one embodiment, discovery server  can be a component of an IBM WEBSPHERE middleware. In another embodiment, server  functionality can be a portion of a Web-enabled service. In yet another embodiment, server  functionality can be a portion of an application programming interface (API).","Discovery engine  can be a hardware\/software entity for receiving function data  and\/or identifying speech data . Engine  can include, but is not limited to, application handler , mapping engine , command component , ruleset , configuration settings , and the like. In one instance, engine  can be utilized during a software development cycle, permitting verification and quality assurance policies to be enforced. In one configuration of the instance, engine  can be used to verify ruleset  can be applied appropriately. In another configuration of the instance, engine  can be utilized to generate command mapping  prior to runtime. In yet another configuration, engine  can be employed to verify annotations within speech data  exist within grammars specified within application .","Application handler  can be a hardware\/software component for registering applications within engine . Application handler  can utilize an application registry (not shown) to process application requests and\/or function identifiers. Handler  can be configured to receive traditional and\/or proprietary formats associated with function data . In one instance, handler  can utilize unique application identifiers to track application requests triggered from a manual event.","Command component  can be a hardware\/software component for identifying and\/or conveying a relevant speech command responsive to a manual event occurring within the system . Component  can utilize command mapping  to transmit an appropriate speech command to application . A function identifier obtained from an application request (e.g., function data ) can be mapped to a command data. For example, in command data , a function associated with identifier F_A can be linked to a \u201cCall\u201d command and an alternative command \u201cDial\u201d.","Ruleset  can be one or more criteria for determining an appropriate mapping utilizing mapping . Ruleset  can conform to one or more traditional and\/or proprietary formats including, but not limited to, Backus-Naur Form (BNF), JAVA Speech Grammar Format (JSGF), and the like. Ruleset  can be utilized during a software development cycle and\/or during runtime of system . In one instance, ruleset  can be utilized to generate mapping . In one embodiment, ruleset  can include rules for determining phrases including, but not limited to, a phrase most likely to occur, the first phrase of a grammar, shortest phrase, and the like.","Configuration setting  can be one or more options for establishing the behavior of server  and\/or system . Setting  can include, but is not limited to, server  settings, engine  options, application hander  parameters, mapping engine  settings, command component  options, and the like. In one instance, setting  can be dynamically altered permitting flexible runtime configuration of system . In one embodiment, setting  can be heuristically determined from historic settings, user preferences, application settings, and the like.","Data store  can be a hardware\/software component for storing command mapping . Data store  can include, but is not limited to, a Storage Area Network (SAN), Network Attached Storage (NAS), and the like. Data store  can be a component of a Relational Database Management System (RDBMS), Object Oriented Database Management System, and the like. Data store  can be communicatively linked to discovery server  via one or more networks.","Command mapping  can be a data set permitting associating of a manual event with a speech command within an application . It should be appreciated that mapping  is not limited to exemplary data set presented herein. In one embodiment, mapping  can be dynamically altered during runtime. Mapping  can support arbitrary quantity of commands, synonyms, and\/or alternative commands for a function. In one embodiment, mapping  can be dynamically generated from an existing speech recognition software development kit (SDK). It should be understood that mapping  can exist for each application and\/or function set.","Application server  can be a hardware\/software component for executing an application . Application server  can include, but is not limited to, application , application settings (not shown), and the like. Server  functionality can include, but is not limited to, speech recognition, communications functionality, brokering, and the like. In one instance, server  can be an IBM WEBSPHERE middleware. In one instance, server  can be a software service executing application  (e.g., application thread).","Application  can be a speech-enabled software component for automating a manual operation utilizing a speech command. Application  can include, but is not limited to speech engine , speech interface , application settings (not shown), and the like. Application  can receive input from a manual event which can trigger the communication of function data  to server . Function data  can include function identifier and relevant function data allowing server  to complete an event to speech command mapping. Application  can include, but is not limited to, a desktop application, a mobile computing application, a firmware program, and the like. For example, application  can be a speech program of a household appliance.","Speech engine  can be a hardware\/software speech processing component within application . Speech engine  can include one or more speech processing algorithms including, but not limited to, phonetic algorithms, keyword spotting algorithms, large-vocabulary, continuous speech recognition algorithms, and the like. Speech engine  functionality can include, but is not limited to, speech recognition, text-to-speech, speech-to-text, speech synthesis, speech enhancement, and the like.","Speech interface  can include, but is not limited to, a traditional speech interface, a proprietary speech interfaces, silent speech interface, and the like. In one instance, speech interface  can include multiple distributed components. In the instance, engine  can intelligently direct speech command presentation to an appropriate component. For instance, when a passenger within a vehicle initiates a manual operation, engine  can utilize a speech interface component (e.g., loudspeaker) proximate to the passenger to present speech command vocalization.","Drawings presented herein are for illustrative purposes only and should not be construed to limit the invention in any regard. System  can be a networked computing environment, distributed computing environment, cloud computing environment, and the like. System  can utilize one or more conventional and\/or non-conventional computing conventions including, but not limited to, Extensible Markup Language (XML), Extensible Hypertext Markup Language (XHTML), and the like. It should be appreciated that server  and\/or engine  can represent one embodiment of the disclosure. In one configuration of the embodiment, server  can be a \u201cdrop-in\u201d solution able to extend the functionality of an existing speech-enabled system. For example, server  can be communicatively linked with an Interactive Voice Response system permitting command discovery to be achieved.","The flowchart and block diagrams in the  illustrate the architecture, functionality, and operation of possible implementations of systems, methods and computer program products according to various embodiments of the present invention. In this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of code, which comprises one or more executable instructions for implementing the specified logical function(s). It should also be noted that, in some alternative implementations, the functions noted in the block may occur out of the order noted in the figures. For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse order, depending upon the functionality involved. It will also be noted that each block of the block diagrams and\/or flowchart illustration, and combinations of blocks in the block diagrams and\/or flowchart illustration, can be implemented by special purpose hardware-based systems that perform the specified functions or acts, or combinations of special purpose hardware and computer instructions."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0007","num":"0006"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 2"}]},"DETDESC":[{},{}]}
