---
title: Removing noise from feature vectors
abstract: A method and computer-readable medium are provided for identifying clean signal feature vectors from noisy signal feature vectors. One aspect of the invention includes using an iterative approach to identify the clean signal feature vector. Another aspect of the invention includes using the variance of a set of noise feature vectors and/or channel distortion feature vectors when identifying the clean signal feature vectors.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07451083&OS=07451083&RS=07451083
owner: Microsoft Corporation
number: 07451083
owner_city: Redmond
owner_country: US
publication_date: 20050720
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["REFERENCE TO RELATED APPLICATION","BACKGROUND","SUMMARY","DETAILED DESCRIPTION OF ILLUSTRATIVE EMBODIMENTS"],"p":["This application is a divisional of and claims priority from U.S. patent application Ser. No. 09\/812,524, filed on Mar. 20, 2001, and entitled \u201cMETHOD AND APPARATUS FOR REMOVING NOISE FROM FEATURE VECTORS\u201d, now U.S. Pat. No. 6,985,858, which is hereby incorporated by reference.","A pattern recognition system, such as a speech recognition system, takes an input signal and attempts to decode the signal to find a pattern represented by the signal. For example, in a speech recognition system, a speech signal is received by the recognition system and is decoded to identify a string of words represented by the speech signal.","To decode the incoming signal, most recognition systems utilize one or more models that describe the likelihood that a portion of the test signal represents a particular pattern. Typically, these models do not operate directly on the incoming signal, but instead operate on a feature vector representation of the incoming signal. In speech recognition, such feature vectors can be produced through techniques such as linear predictive coding (LPC), LPC derived cepstrum, perceptive linear prediction (PLP), and mel-frequency cepstrum coefficients (MFCC) feature extraction.","The incoming signal is often a combination of signals from different sources, each modified by a channel. For example, the incoming signal may be a mixture of an original signal, which contains the pattern to be recognized, and one or more obscuring signals, such as additive noise and channel distortion. In speech recognition, the incoming signal may be a combination of the speech signal to be fed into a speech recognizer, additive noise, and channel distortion such as telephone channel distortion, or reverberations generated by the speech signal bouncing off walls in a room. Or, the incoming signal may be a combination of a speech signal with a channel signal (impulse response of the channel), where the channel signal is to be fed into a system that recognizes channel types. Or, the incoming signal may be a mixture of the speech signals from two different speakers, each modified by a different channel, and each of which is to be fed into a speech recognizer.","Because noise and channel distortion make it more difficult to recognize a pattern in the incoming signal, it is often desirable to remove the noise and the channel distortion before performing pattern recognition. However, removing noise and channel distortion from the incoming signal itself is computationally difficult because of the large amount of data that has to be processed. To overcome this problem, some prior art techniques have tried to remove noise from the feature vector representation of the incoming signal instead of the incoming signal itself because the feature vector representation is more compact than the incoming signal.","However, past techniques for removing noise from feature vectors have relied on point models for the noise and the channel distortion. In other words, the noise reduction techniques have assumed that one single feature vector can represent the noise and another single feature vector can represent the channel distortion. The point models may be adapted to a sequence of input features, but they are held constant across the sequence. Because the noise and channel distortion vary across the sequence of input features, techniques that use this approximation do not accurately remove the noise or channel distortion.","Some prior art techniques for removing noise from feature vectors attempt to identify the most likely combination of a noise feature vector, a channel distortion feature vector, and an original signal feature vector that would have produced the noisy feature vector. To make this determination, the prior art relies on an approximation of the relationship between noise, channel distortion, original signals, and incoming signals.","However, prior art systems do not take the error present in the approximation into account when identifying possible combinations of noise, channel distortion, and original signals based on the incoming signal. In addition, the form of the approximation is typically set once and then used to identify the best combination. If the form of the approximation is not accurate, the resulting identified combination of noise, channel distortion, and original signal will be inaccurate. However, the prior art does not provide a means for adjusting the form of the approximation to improve the resulting identified combination.","A method and computer-readable medium are provided for identifying clean signal feature vectors from noisy signal feature vectors. One aspect of the invention includes using an iterative approach to identify the clean signal feature vector. Another aspect of the invention includes using the variance of a set of noise feature vectors and\/or channel distortion feature vectors when identifying the clean signal feature vectors.",{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 1","b":["100","100","100","100"]},"The invention is operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of well known computing systems, environments, and\/or configurations that may be suitable for use with the invention include, but are not limited to, personal computers, server computers, hand-held or laptop devices, multiprocessor systems, microprocessor-based systems, set top boxes, programmable consumer electronics, network PCs, minicomputers, mainframe computers, distributed computing environments that include any of the above systems or devices, and the like.","The invention may be described in the general context of computer-executable instructions, such as program modules, being executed by a computer. Generally, program modules include routines, programs, objects, components, data structures, etc. that perform particular tasks or implement particular abstract data types. The invention may also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment, program modules may be located in both local and remote computer storage media including memory storage devices.","With reference to , an exemplary system for implementing the invention includes a general-purpose computing device in the form of a computer . Components of computer  may include, but are not limited to, a processing unit , a system memory , and a system bus  that couples various system components including the system memory to the processing unit . The system bus  may be any of several types of bus structures including a memory bus or memory controller, a peripheral bus, and a local bus using any of a variety of bus architectures. By way of example, and not limitation, such architectures include Industry Standard Architecture (ISA) bus, Micro Channel Architecture (MCA) bus, Enhanced ISA (EISA) bus, Video Electronics Standards Association (VESA) local bus, and Peripheral Component Interconnect (PCI) bus also known as Mezzanine bus.","Computer  typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by computer  and includes both volatile and nonvolatile media, removable and non-removable media. By way of example, and not limitation, computer readable media may comprise computer storage media and communication media. Computer storage media includes both volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information such as computer readable instructions, data structures, program modules or other data. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by computer .","Communication media typically embodies computer readable instructions, data structures, program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term \u201cmodulated data signal\u201d means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media includes wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, FR, infrared and other wireless media. Combinations of any of the above should also be included within the scope of computer readable media.","The system memory  includes computer storage media in the form of volatile and\/or nonvolatile memory such as read only memory (ROM)  and random access memory (RAM) . A basic input\/output system  (BIOS), containing the basic routines that help to transfer information between elements within computer , such as during start-up, is typically stored in ROM . RAM  typically contains data and\/or program modules that are immediately accessible to and\/or presently being operated on by processing unit . By way o example, and not limitation,  illustrates operating system , application programs , other program modules , and program data .","The computer  may also include other removable\/non-removable volatile\/nonvolatile computer storage media. By way of example only,  illustrates a hard disk drive  that reads from or writes to non-removable, nonvolatile magnetic media, a magnetic disk drive  that reads from or writes to a removable, nonvolatile magnetic disk , and an optical disk drive  that reads from or writes to a removable, nonvolatile optical disk  such as a CD ROM or other optical media. Other removable\/non-removable, volatile\/nonvolatile computer storage media that can be used in the exemplary operating environment include, but are not limited to, magnetic tape cassettes, flash memory cards, digital versatile disks, digital video tape, solid state RAM, solid state ROM, and the like. The hard disk drive  is typically connected to the system bus  through a non-removable memory interface such as interface , and magnetic disk drive  and optical disk drive  are typically connected to the system bus  by a removable memory interface, such as interface .","The drives and their associated computer storage media discussed above and illustrated in , provide storage of computer readable instructions, data structures, program modules and other data for the computer . In , for example, hard disk drive  is illustrated as storing operating system , application programs , other program modules , and program data . Note that these components can either be the same as or different from operating system , application programs , other program modules , and program data . Operating system , application programs , other program modules , and program data  are given different numbers here to illustrate that, at a minimum, they are different copies.","A user may enter commands and information into the computer  through input devices such as a keyboard , a microphone , and a pointing device , such as a mouse, trackball or touch pad. Other input devices (not shown) may include a joystick, game pad, satellite dish, scanner, or the like. These and other input devices are often connected to the processing unit  through a user input interface  that is coupled to the system bus, but may be connected by other interface and bus structures, such as a parallel port, game port or a universal serial bus (USB). A monitor  or other type of display device is also connected to the system bus  via an interface, such as a video interface . In addition to the monitor, computers may also include other peripheral output devices such as speakers  and printer , which may be connected through an output peripheral interface .","The computer  may operate in a networked environment using logical connections to one or more remote computers, such as a remote computer . The remote computer  may be a personal computer, a hand-held device, a server, a router, a network PC, a peer device or other common network node, and typically includes many or all of the elements described above relative to the computer . The logical connections depicted in  include a local area network (LAN)  and a wide area network (WAN) , but may also include other networks. Such networking environments are commonplace in offices, enterprise-wide computer networks, intranets and the Internet.","When used in a LAN networking environment, the computer  is connected to the LAN  through a network interface or adapter . When used in a WAN networking environment, the computer  typically includes a modem  or other means for establishing communications over the WAN , such as the Internet. The modem , which may be internal or external, may be connected to the system bus  via the user input interface , or other appropriate mechanism. In a networked environment, program modules depicted relative to the computer , or portions thereof, may be stored in the remote memory storage device. By way of example, and not limitation,  illustrates remote application programs  as residing on remote computer . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.",{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 2","b":["200","200","202","204","206","208","210"]},"Memory  is implemented as non-volatile electronic memory such as random access memory (RAM) with a battery back-up module (not shown) such that information stored in memory  is not lost when the general power to mobile device  is shut down. A portion of memory  is preferably allocated as addressable memory for program execution, while another portion of memory  is preferably used for storage, such as to simulate storage on a disk drive.","Memory  includes an operating system , application programs  as well as an object store . During operation, operating system  is preferably executed by processor  from memory . Operating system , in one preferred embodiment, is a WINDOWS\u00ae CE brand operating system commercially available from Microsoft Corporation. Operating system  is preferably designed for mobile devices, and implements database features that can be utilized by applications  through a set of exposed application programming interfaces and methods. The objects in object store  are maintained by applications  and operating system , at least partially in response to calls to the exposed application programming interfaces and methods.","Communication interface  represents numerous devices and technologies that allow mobile device  to send and receive information. The devices include wired and wireless modems, satellite receivers and broadcast tuners to name a few. Mobile device  can also be directly connected to a computer to exchange data therewith. In such cases, communication interface  can be an infrared transceiver or a serial or parallel communication connection, all of which are capable of transmitting streaming information.","Input\/output components  include a variety of input devices such as a touch-sensitive screen, buttons, rollers, and a microphone as well as a variety of output devices including an audio generator, a vibrating device, and a display. The devices listed above are by way of example and need not all be present on mobile device . In addition, other input\/output devices may be attached to or found with mobile device  within the scope of the present invention.",{"@attributes":{"id":"p-0036","num":"0035"},"figref":["FIG. 3","FIGS. 1 and 2","FIG. 3"],"b":["400","401","402","404","406"]},"A-to-D converter  converts the analog signal from microphone  into a series of digital values. In several embodiments, A-to-D converter  samples the analog signal at 16 kHz and 16 bits per sample, thereby creating 32 kilobytes of speech data per second.","The output of A-to-D converter  is provided to feature extractor , which extracts a feature from the digital speech signal. Examples of feature extraction modules include modules for performing Linear Predictive Coding (LPC), LPC derived cepstrum, Perceptive Linear Prediction (PLP), Auditory model feature extraction, and Mel-Frequency Cepstrum Coefficients (MFCC) feature extraction. Note that the invention is not limited to these feature extraction modules and that other modules may be used within the context of the present invention.","The feature extraction module receives the stream of digital values from A-to-D  and produces a stream of feature vectors that are each associated with a frame of the speech signal. In many embodiments, the centers of the frames are separated by 10 milliseconds.","The stream of feature vectors provided by A-to-D converter  represents a noisy speech signal which is the combination of a clean speech signal, additive noise and channel distortion. These noisy feature vectors are provided to a noise reduction module  of the present invention, which generates a stream of \u201cclean\u201d feature vectors from the noisy feature vectors.","The stream of \u201cclean\u201d feature vectors produced by noise reduction module  is provided to a decoder , which identifies a most likely sequence of words based on the stream of \u201cclean\u201d feature vectors, a lexicon , a language model , and an acoustic model .","In some embodiments, acoustic model  is a Hidden Markov Model consisting of a set of hidden states. Each linguistic unit represented by the model consists of a subset of these states. For example, in one embodiment, each phoneme is constructed of three interconnected states. Each state has an associated set of probability distributions that in combination allow efficient computation of the likelihoods against any arbitrary sequence of input feature vectors for each sequence of linguistic units (such as words). The model also includes probabilities for transitioning between two neighboring model states as well as allowed transitions between states for particular linguistic units. By selecting the states that provide the highest combination of matching probabilities and transition probabilities for the input feature vectors, the model is able to assign linguistic units to the speech. For example, if a phoneme was constructed of states 0, 1 and 2 and if the first three frames of speech matched state 0, the next two matched state 1 and the next three matched state 2, the model would assign the phoneme to these eight frames of speech.","Note that the size of the linguistic units can be different for different embodiments of the present invention. For example, the linguistic units may be senones, phonemes, noise phones, diphones, triphones, or other possibilities.","In other embodiments, acoustic model  is a segment model that indicates how likely it is that a sequence of feature vectors would be produced by a segment of a particular duration. The segment model differs from the frame-based model because it uses multiple feature vectors at the same time to make a determination about the likelihood of a particular segment. Because of this, it provides a better model of large-scale transitions in the speech signal. In addition, the segment model looks at multiple durations for each segment and determines a separate probability for each duration. As such, it provides a more accurate model for segments that have longer durations. Several types of segment models may be used with the present invention including probabilistic-trajectory segmental Hidden Markov Models.","Language model  provides a set of likelihoods that a particular sequence of words will appear in the language of interest. In many embodiments, the language model is based on a text database such as the North American Business News (NAB), which is described in greater detail in a publication entitled CSR-III Text Language Model, University of Penn., 1994. The language model may be a context-free grammar or a statistical N-gram model such as a trigram. In one embodiment, the language model is a compact trigram model that determines the probability of a sequence of words based on the combined probabilities of three-word segments of the sequence.","Based on the acoustic model, the language model, and the lexicon, decoder  identifies a most likely sequence of words from all possible word sequences. The particular method used for decoding is not important to the present invention and any of several known methods for decoding may be used.","The most probable sequence of hypothesis words is provided to a confidence measure module . Confidence measure module  identifies which words are most likely to have been improperly identified by the speech recognizer, based in part on a secondary frame-based acoustic model. Confidence measure module  then provides the sequence of hypothesis words to an output module  along with identifiers indicating which words may have been improperly identified. Those skilled in the art will recognize that confidence measure module  is not necessary for the practice of the present invention.","Although the noise reduction technique of the present invention can be used in noise reduction module , the invention is not limited to being used in speech recognition. Those skilled in the art will recognize that the invention may be used in any appropriate pattern recognition system. In addition, the invention may be used during the training of a pattern recognizer as well as during the detection of patterns. Those skilled in the art will also recognize that the method can be extended to multiple sources and multiple channels. Also, the invention may be used for purposes other than automatic recognition, such as denoising features for the purpose of reconstructing an enhanced signal.","The noise reduction technique of the present invention identifies an optimal combination of obscuring signals and clean speech given an observed noisy speech vector. To do this, the present invention relies in part on the well-known Baysian rule:",{"@attributes":{"id":"p-0050","num":"0049"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["n","x"],"mo":[",",","],"mrow":{"mi":["c","y"],"mo":"\u2758"}}}},"mo":"=","mfrac":{"mrow":[{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["y","n"],"mo":"\u2758"},"mo":[",",","],"mi":["x","c"]}}},{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["n","x","c"],"mo":[",",","]}}}],"mo":"\u2062"},{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"y"}}]}}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"1"}}]}}}},"br":{}},"Once an approximation to the posterior p(n,x,c|y) has been found, the maximum a posteriori features may be chosen. Or, in another embodiment, the average values of the features may be chosen. Those skilled in the art will recognize that other statistics can be extracted from the posterior. In fact, a representation of the posterior probability distribution itself can be fed into a recognition system.","Since p(y) is the same for all combinations of noise feature vectors, clean signal feature vectors, and channel distortion feature vectors, it can be ignored when searching for an approximation to the posterior distribution over noise feature vectors, clean signal feature vectors, and channel distortion feature vectors.","Under one embodiment of the present invention, the posterior probability (as well as the prior probability) is represented by a mixture of Gaussians, where each mixture component of the posterior probability is determined separately using a separate prior probability mixture component and a separate observation probability mixture component. Thus, mixture component i of the posterior probability is formed based on mixture component i of the prior probability and mixture component i of the observation probability.","The process for identifying the combination of noise, channel distortion and original signal that provides the most likely posterior probability, p(n,x,c|y) is shown in . The process of  begins at step  where the means and variances for the mixture components of the prior probability, observation probability and posterior probability are initialized. The process of step  is shown in greater detail in .","In step  of , the means and variances for each mixture component of the prior probability are generated. To generate the means and variances, the process of the present invention first generates a mixture of Gaussians that describes the distribution of a set of training noise feature vectors, a second mixture of Gaussians that describes a distribution of a set of training channel distortion feature vectors, and a third mixture of Gaussians that describes a distribution of a set of training clean signal feature vectors. The mixture components can be formed by grouping feature vectors using a maximum likelihood training technique or by grouping feature vectors that represent a temporal section of a signal together. Those skilled in the art will recognize that other techniques for grouping the feature vectors into mixture components may be used and that the two techniques listed above are only provided as examples.","After the feature vectors have been grouped into their respective mixture components, the mean and variance of the feature vectors within each component is determined. In an embodiment in which maximum likelihood training is used to group the feature vectors, the means and variances are provided as by-products of grouping the feature vectors into the mixture components.","After the means and variances have been determined for the mixture components of the noise feature vectors, clean signal feature vectors, and channel feature vectors, these mixture components are combined to form a mixture of Gaussians that describes the total prior probability. Using one technique, the mixture of Gaussians for the total prior probability will be formed at the intersection of the mixture components of the noise feature vectors, clean signal feature vectors, and channel distortion feature vectors.","Once the means and variances for the mixture components of the prior probability have been determined, the process of  continues at step  where initial means for the mixture components of the posterior probability are set. Under one embodiment of the invention, the initial means are set to be equal to the means of the prior probability's mixture components.","At step , the variances for the mixture components of the observation probability are determined. Under one embodiment, these variances are formed using a closed form expression of the form:",{"@attributes":{"id":"p-0060","num":"0059"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"VAR","mo":["(",")"],"mrow":{"mrow":{"mi":["y","x"],"mo":"\u2758"},"mo":",","mi":"n"}},"mo":"=","mfrac":{"msup":[{"mi":"\u03b1","mn":"2"},{"mrow":{"mi":"cosh","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":["n","x"],"mo":"-"}},"mo":"\/","mn":"2"}}},"mn":"2"}]}}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"2"}}]}}}},"br":{}},"Under other embodiments, these variances are formed using a training clean signal, a training noise signal, and a set of training channel distortion vectors that represent the channel distortion that will be applied to the clean signal and noise signal.","The training clean signal and the training noise signal are separately converted into sequences of feature vectors. These feature vectors, together with the channel distortion feature vectors are then applied to an equation that approximates the relationship between observed noisy vectors and clean signal vectors, noise vectors, and channel distortion vectors. Under one embodiment, this equation is of the form:\n\n(ln(1+))\u2003\u2003EQ. 3\n\nwhere  is an observed noisy feature vector,  is a channel distortion feature vector,  is a clean signal feature vector,  is a noise feature vector, C is a transformation matrix, and Cis the inverse of the transformation matrix. In equation 3:\n",{"@attributes":{"id":"p-0063","num":"0062"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":[{"mi":"ln","mo":["\u2062","\u2062"],"mstyle":{"mtext":"("},"mn":"1"},{"msup":{"mi":"\u2147","mrow":{"mo":["(",")"],"mrow":{"msup":{"mi":"C","mrow":{"mo":"-","mn":"1"}},"mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"munder":[{"mi":["n","_"]},{"mi":["c","_"]},{"mi":["x","_"]}],"mo":["-","-"]}}}}},"mo":"\u2062","mstyle":{"mtext":")"}}],"mo":"+"},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mrow":{"mi":"ln","mo":["(",")"],"mrow":{"mn":"1","mo":"+","msup":{"mi":"\u2147","mrow":{"mo":["(",")"],"mrow":{"munder":{"mo":"\u2211","mi":"j"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msubsup":{"mi":"C","mrow":[{"mn":"1","mo":"\u2062","mi":"j"},{"mo":"-","mn":"1"}]},"mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"msub":[{"mi":["n","j"]},{"mi":["c","j"]},{"mi":["x","j"]}],"mo":["-","-"]}}}}}}}}}},{"mtd":{"mrow":{"mi":"ln","mo":["(",")"],"mrow":{"mn":"1","mo":"+","msup":{"mi":"\u2147","mrow":{"mo":["(",")"],"mrow":{"munder":{"mo":"\u2211","mi":"j"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msubsup":{"mi":"C","mrow":[{"mn":"2","mo":"\u2062","mi":"j"},{"mo":"-","mn":"1"}]},"mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"msub":[{"mi":["n","j"]},{"mi":["c","j"]},{"mi":["x","j"]}],"mo":["-","-"]}}}}}}}}}},{"mtd":{"mi":"\u22ee"}},{"mtd":{"mrow":{"mi":"ln","mo":["(",")"],"mrow":{"mn":"1","mo":"+","msup":{"mi":"\u2147","mrow":{"mo":["(",")"],"mrow":{"munder":{"mo":"\u2211","mi":"j"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msubsup":{"mi":"C","msub":{"mi":["K","j"]},"mrow":{"mo":"-","mn":"1"}},"mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"msub":[{"mi":["n","j"]},{"mi":["c","j"]},{"mi":["x","j"]}],"mo":["-","-"]}}}}}}}}}}]}}],"mo":"="}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"4"}}]}}}},"br":{},"sub":["j","j","j ","ij "],"sup":["\u2212","\u22121"]},"In one embodiment of equation 3 above, the transform matrix C is an orthonormal matrix of discreet cosine transformation coefficients when the feature extraction technique produces cepstrum feature vectors. For embodiments that use a log spectrum feature extraction technique, C is the identity matrix. Those skilled in the art will recognize that other transform matrices, C, will be used in equation 3 depending on the particular feature extraction technique used to form the feature vectors.","In fact, Cneed not be square or of full rank, in which case Cis a pseudoinverse matrix or another suitable matrix.","Under one embodiment, the training clean signal feature vectors, training noise feature vectors, and channel distortion feature vectors used to determine the mixture components of the prior probability, are reused in equation 3 to produce calculated noisy feature vectors. Thus, each mixture component of the prior probability produces its own set of calculated noisy feature vectors.","The training clean signal is also allowed to pass through a training channel before being combined with the training noise signal. The resulting analog signal is then converted into feature vectors to produce a sequence of observed noisy feature vectors. The observed noisy feature vectors are aligned with their respective calculated noisy feature vectors so that the observed values can be compared to the calculated values.","For each mixture component in the prior probability, the average difference between the calculated noisy feature vectors associated with that mixture component and the observed noisy feature vectors is determined. This average value is used as the variance for the corresponding mixture component of the observation probability. Thus, the calculated noisy feature vector produced from the third mixture component of the prior probability would be used to produce a variance for the third mixture component of the observation probability. At the end of step , a variance has been calculated for each mixture component of the observation probability.","After the mixture components of the prior probability, observation probability, and posterior probability have been initialized, the process of  continues at step  where the first mixture component of the prior probability and the observation probability is selected.","At step , an iteration is performed to find the mean for the posterior probability, p(n,x,c|y) of the selected mixture. The process for performing this iteration is shown in .","In step  of , the prior probability, the observation probability, and the last determined mean of the posterior probability are used to identify a new mean for the posterior probability. In particular, the new mean for the posterior probability is calculated according to the variational inference principle and procedures as:\n\n=+(\u2032()\u03a8\u2032())((\u2212)+()\u03a8()()))\u2003\u2003EQ. 5\n\nwhere  is the newly calculated mean for the posterior probability of the current mixture,  is the past mean for the posterior probability, is the inverse of the covariance matrix for this mixture component of the prior probability, \u03bc is the mean for this mixture component of the prior probability, \u03a8 is the variance of this mixture component of the observation probability, g() is the right-hand side of equation 3 evaluated at the last mean, g\u2032() is the matrix derivative of equation 3 calculated at the last mean, and  is the observed feature vector.\n","In equation 5, \u03bc,  and  are M-by-1 matrices where M is three times the number of elements in each feature vector. In particular, \u03bc,  and  are described by vectors having the form:",{"@attributes":{"id":"p-0073","num":"0072"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"munder":[{"mi":["\u03bc","_"]},{"mi":["\u03b7","_"]}],"mo":[";","\u2062",";","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.em","height":"0.ex"}}},{"mspace":{"@attributes":{"width":"0.em","height":"0.ex"}}}],"mrow":{"munder":{"msub":{"mi":["\u03b7","p"]},"mi":"_"},"mo":"\u2237","mrow":{"mo":["[","\u2062","\u2062","]"],"mstyle":[{"mspace":{"@attributes":{"width":"0.em","height":"0.ex"}}},{"mspace":{"@attributes":{"width":"0.em","height":"0.ex"}}}],"mtable":{"mtr":[{"mtd":{"mrow":{"mfrac":{"mi":"M","mn":"3"},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mi":["Elements","For","Clean","Signal","Feature","Vector"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]}}},{"mtd":{"mrow":{"mfrac":{"mi":"M","mn":"3"},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mi":["Elements","For","Noise","Feature","Vector"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]}}},{"mtd":{"mrow":{"mfrac":{"mi":"M","mn":"3"},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mi":["Elements","For","Channel","Distortion","Feature","Vector"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]}}}]}}}}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"6"}}]}}}}},"Using this definition for \u03bc,  and , equation 3 above can be described as:",{"@attributes":{"id":"p-0075","num":"0074"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"g","mo":"\u2061","mrow":{"mo":["(",")"],"munder":{"msub":{"mi":["\u03b7","p"]},"mi":"_"}}},{"mrow":[{"munder":{"msub":{"mi":["\u03b7","p"]},"mi":"_"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mfrac":{"mrow":{"mn":"2","mo":"\u2062","mi":"M"},"mn":"3"},"mo":"+","mrow":{"mn":"1","mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mo":":"},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"M"}}}},{"munder":{"msub":{"mi":["\u03b7","p"]},"mi":"_"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mo":":"},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mfrac":{"mi":"M","mn":"3"}}}},{"mi":"C","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"ln","mo":["(",")"],"mrow":{"mn":"1","mo":"+","msup":{"mi":"\u2147","mrow":{"msup":{"mi":"C","mrow":{"mo":"-","mn":"1"}},"mo":"\u2062","munder":{"mrow":{"mo":["(",")"],"mrow":{"mrow":[{"munder":{"msub":{"mi":["\u03b7","p"]},"mi":"_"},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mfrac":{"mi":"M","mn":"3"},"mo":"+","mrow":{"mn":"1","mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mtext":":"},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mfrac":{"mrow":{"mn":"2","mo":"\u2062","mi":"M"},"mn":"3"}}}}},{"munder":{"msub":{"mi":["\u03b7","p"]},"mi":"_"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mfrac":{"mrow":{"mn":"2","mo":"\u2062","mi":"M"},"mn":"3"},"mo":"+","mrow":{"mn":"1","mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mtext":":"},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"M"}}}},{"msub":{"munder":{"mi":["\u03b7","_"]},"mi":"p"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mtext":":"},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mfrac":{"mi":"M","mn":"3"}}}}],"mo":["-","-"]}},"mi":"_"}}}}}}],"mo":["+","+"]}],"mo":"="}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"7"}}]}}}},"br":{}},"In equation 5, the derivative g\u2032() is a matrix of order",{"@attributes":{"id":"p-0077","num":"0076"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mfrac":{"mi":"M","mn":"3"},"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mtext":"-"},{"mtext":"-"}],"mi":["by","M"]}}},"br":{}},{"@attributes":{"id":"p-0078","num":"0077"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mrow":[{"mo":["[","]"],"mrow":{"munder":{"mi":["g","_"]},"mo":"\u2061","mrow":{"mo":["(",")"],"munder":{"msub":{"mi":["\u03b7","p"]},"mi":"_"}}}},{"mi":"i","mo":"\u2062","msub":{"mo":",","mi":"j"}}]},"mo":"=","mfrac":{"mrow":[{"mo":"\u2202","msub":{"mrow":{"mo":["[","]"],"mrow":{"munder":{"mi":["g","_"]},"mo":"\u2061","mrow":{"mo":["(",")"],"munder":{"msub":{"mi":["\u03b7","p"]},"mi":"_"}}}},"mi":"i"}},{"mo":"\u2202","msub":{"mrow":{"mo":["[","]"],"munder":{"msub":{"mi":["\u03b7","p"]},"mi":"_"}},"mi":"j"}}]}}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"8"}}]}}}},"br":{},"u":["\u03b7","\u03b7","\u03b7"],"sub":["5","5"]},"Note that when the transform matrix, C, of equation 7 is equal to the identity matrix, the ith element of g() is defined as:",{"@attributes":{"id":"p-0080","num":"0079"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mrow":{"mo":["[","]"],"mrow":{"munder":{"mi":["g","_"]},"mo":"\u2061","mrow":{"mo":["(",")"],"munder":{"msub":{"mi":["\u03b7","p"]},"mi":"_"}}}},"mi":"i"},"mo":"=","mrow":{"msub":[{"mi":["c","i"]},{"mi":["x","i"]}],"mo":["+","+"],"mrow":{"mi":"ln","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"+","msup":{"mi":"\u2147","mrow":{"msub":[{"mi":["n","i"]},{"mi":["c","i"]},{"mi":"x","mrow":{"mi":"i","mo":"-"}}],"mo":["-","-"]}}}}}}}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"9"}}]}}}},"br":{},"u":"\u03b7","sub":["j ","i","i","i"]},"After equation 7 has been used to determine a new mean for the posterior probability at step , the process of  continues at step  where a stopping criterion is tested. For example, the mean may be examined to determine whether it has converged. Under most embodiments, this is determined by comparing the new mean to the last mean determined for the posterior probability. If the difference between these two means is larger than some threshold, the process returns to step  using the new mean as the last mean and determining a revised new mean. In another embodiment, a fixed number of iterations is performed. Those skilled in the art will recognize that other techniques for determining when to stop the iterations may be used with the present invention.","Steps  and  are repeated until the stopping criterion is satisfied, at which point the iteration of  ends at step . When the process of  reaches step , step  of  is complete. The process of  then continues at step  where the variance for the posterior probability of the current mixture component is determined. Under one embodiment of the present invention, the variance for the posterior probability is found in the premultiplier in the second term of the right hand expression of equation 5. The variance is determined by evaluating this factor at the selected mean for the posterior probability.","The effects of the iterations of  are shown in  for a single feature of the feature vectors. In , feature i of a clean signal feature vector is shown along horizontal axis  while feature i of a noise feature vector is shown along vertical axis . A distribution  with a mean  is shown in this space for a mixture component of the prior probability.  also includes a distribution  for the observation probability.","Before the iterations of  begin, the mean of the posterior probability is set equal to mean . After a first iteration through step , the mean of the posterior probability has shifted to location . After a second iteration, the mean has shifted to location . On the final iteration, the mean moves to location  and the iteration ends. The variance of the posterior probability is then determined at step  providing a distribution  for the posterior probability.","Note that the mean of the posterior probability settles at a location between the prior probability distribution and the observation probability distribution . Thus, the posterior probability distribution is a balance between the prior probability distribution  and the observation probability .","After the mean and variance for the first mixture component of the posterior probability has been determined, the process of  continues by determining whether there are more mixture components at step . If there are more mixture components, the next mixture component is selected at step  and steps  and  are repeated for the new mixture component.","Once all of the mixture components have had their mean and variance determined for the posterior probability, the process of  continues at step  where the mixture components are combined to identify a most likely noise feature vector, channel distortion feature vector, and clean signal feature vector given the observed noisy signal feature vector. Under one embodiment, the noise feature vector, channel feature vector, and clean signal feature vector are calculated as:",{"@attributes":{"id":"p-0088","num":"0087"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"msub":{"mi":["n","post"]},"mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"s","mo":"=","mn":"1"},"mi":"S"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":["\u03c1","s"]},"mo":"\u2062","mrow":{"munder":{"msub":{"mi":["\u03b7","s"]},"mi":"_"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mfrac":{"mi":"M","mn":"3"},"mo":"+","mrow":{"mn":"1","mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mtext":":"},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mfrac":{"mrow":{"mn":"2","mo":"\u2062","mi":"M"},"mn":"3"}}}}}}}}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"10"}}]},{"mtd":[{"mrow":{"msub":{"mi":["c","post"]},"mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"s","mo":"=","mn":"1"},"mi":"S"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":["\u03c1","s"]},"mo":"\u2062","mrow":{"munder":{"msub":{"mi":["\u03b7","s"]},"mi":"_"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mfrac":{"mrow":{"mn":"2","mo":"\u2062","mi":"M"},"mn":"3"},"mo":"+","mrow":{"mn":"1","mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mtext":":"},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"M"}}}}}}}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mn":"11"}}]},{"mtd":[{"mrow":{"msub":{"mi":["x","post"]},"mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"s","mo":"=","mn":"1"},"mi":"S"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":["\u03c1","s"]},"mo":"\u2062","mrow":{"munder":{"msub":{"mi":["\u03b7","s"]},"mi":"_"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mtext":":"},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mfrac":{"mi":"M","mn":"3"}}}}}}}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"12"}}]}]}}},"br":{},"sub":"s "},{"@attributes":{"id":"p-0089","num":"0088"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"munder":{"msub":{"mi":["\u03b7","s"]},"mi":"_"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mfrac":{"mi":"M","mn":"3"},"mo":"+","mrow":{"mn":"1","mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mtext":":"},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mfrac":{"mrow":{"mn":"2","mo":"\u2062","mi":"M"},"mn":"3"}}}}}}},"br":{}},{"@attributes":{"id":"p-0090","num":"0089"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"munder":{"msub":{"mi":["\u03b7","s"]},"mi":"_"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mo":["(",")"],"mrow":{"mfrac":{"mrow":{"mn":"2","mo":"\u2062","mi":"M"},"mn":"3"},"mo":"+","mrow":{"mn":"1","mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mtext":":"},{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}}],"mi":"M"}}}}}},"br":{}},{"@attributes":{"id":"p-0091","num":"0090"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"munder":{"msub":{"mi":["\u03b7","s"]},"mi":"_"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mtext":":"},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mfrac":{"mi":"M","mn":"3"}}}}}},"br":{},"sub":["post","post","post "]},"The weight for each mixture component, \u03b7is calculated as:",{"@attributes":{"id":"p-0093","num":"0092"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["\u03c1","s"]},"mo":"=","mfrac":{"mrow":[{"msub":{"mi":["\u03c0","s"]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mi":"\u2147","msub":{"mi":["G","s"]}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"S"},"mo":"\u2062","msub":{"mi":["\u03c1","i"]}}]}}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"13"}}]}}}},"br":[{},{},{}],"sub":["s ","s","s","s","s","s","s","s"],"in-line-formulae":[{},{}],"sup":["x","n","c","x","n ","c "]},"In equation 13, Gis a function that affects the weighting of a mixture component based on the shape of the prior probability and posterior probability, as well as the similarity between the selected mean for the posterior probability and the observed noisy vector and the similarity between the selected mean and the mean of the prior probability. Under one embodiment, the expression for Gis:",{"@attributes":{"id":"p-0095","num":"0094"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["G","s"]},"mo":"=","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"mrow":[{"mo":"-","mfrac":{"mn":["1","2"]}},{"mo":["\uf603","\uf604"],"mrow":{"mn":"2","mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mi":"\u03c0","msub":{"munder":{"mi":["\u03a3","_"]},"mi":"s"}}}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mi":"ln"},{"mfrac":{"mn":["1","2"]},"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mi":"ln","mrow":{"mo":["\uf603","\uf604"],"mrow":{"mn":"2","mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}],"mi":"\u03c0","msub":{"mi":["\u03a6","s"]}}}},{"mfrac":{"mn":["1","2"]},"mo":["\u2062","\u2062","\u2062"],"msup":{"mrow":{"mo":["(",")"],"mrow":{"munder":{"mi":["y","_"]},"mo":"-","mrow":{"munder":{"mi":["g","_"]},"mo":"\u2061","mrow":{"mo":["(",")"],"munder":{"msub":{"mi":["\u03b7","s"]},"mi":"_"}}}}},"mi":"T"},"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},"mrow":{"msup":{"mi":"\u03a8","mrow":{"mo":"-","mn":"1"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"munder":{"mi":["y","_"]},"mo":"-","mrow":{"munder":{"mi":["g","_"]},"mo":"\u2061","mrow":{"mo":["(",")"],"munder":{"msub":{"mi":["\u03b7","s"]},"mi":"_"}}}}}}},{"mfrac":{"mn":["1","2"]},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"msup":{"mrow":{"mo":["(",")"],"mrow":{"munder":[{"msub":{"mi":["\u03b7","s"]},"mi":"_"},{"msub":{"mi":["\u03bc","s"]},"mi":"_"}],"mo":"-"}},"mi":"T"},"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msubsup":{"munder":{"mi":["\u03a3","_"]},"mi":"s","mrow":{"mo":"-","mn":"1"}},"mrow":{"mo":["(",")"],"mrow":{"munder":[{"msub":{"mi":["\u03b7","s"]},"mi":"_"},{"msub":{"mi":["\u03bc","s"]},"mi":"_"}],"mo":"-"}}},{"mfrac":{"mn":["1","2"]},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":["sum","of","diagonal","elements"],"mrow":{"mi":"of","mo":["\u2062","(",")"],"mrow":[{"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mo":"\u2062"},{"msubsup":{"munder":{"mi":["\u03a3","_"]},"mi":"s","mrow":{"mo":"-","mn":"1"}},"mo":"\u00b7","msub":{"mi":["\u03a6","s"]}}]}},{"mfrac":{"mn":["1","2"]},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":["sum","of","diagonal","element"],"mrow":{"mi":"of","mo":["\u2062","(",")"],"mrow":[{"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mo":"\u2062"},{"msup":[{"mrow":{"msup":{"munder":{"mi":["g","_"]},"mi":"\u2032"},"mo":"\u2061","mrow":{"mo":["(",")"],"munder":{"msub":{"mi":["\u03b7","s"]},"mi":"_"}}},"mi":"T"},{"mi":"\u03a8","mrow":{"mo":"-","mn":"1"}}],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}],"mrow":{"msup":{"munder":{"mi":["g","_"]},"mi":"\u2032"},"mo":"\u2061","mrow":{"mo":["(",")"],"munder":{"msub":{"mi":["\u03b7","s"]},"mi":"_"}}},"msubsup":{"mi":["\u03a3","s"],"mrow":{"mo":"-","mn":"1"}}}]}}],"mo":["+","-","-","-","-"]}}}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"15"}}]}}}},"br":{},"u":"\u03a3","sub":["s","s"]},"Those skilled in the art will recognize that there are other ways of using the mixture approximation to the posterior to obtain statistics. For example, the means of the mixture component with largest \u03c1 can be selected. Or, the entire mixture distribution can be used as input to a recognizer.","An example of the determination of the posterior noise feature vector, channel distortion feature vector, and clean signal feature vector are shown in . In , as in  above, feature i of the clean signal is shown along horizontal axis  and feature i of the noise is shown along vertical axis . Note that for simplicity, feature i for the channel is not shown, but would provide a third dimension if placed in .","In , there are six mixture components , , , ,  and  for the prior probability. The prior mixture components are associated with six mixture components for the posterior probability indicated as distributions , , , ,  and , respectively. These posterior mixture probabilities are combined to identify a single vector  that describes the most likely clean signal feature vector, noise feature vector, and channel distortion feature vector given the observed noisy feature vector. Note that in FIG. , only one feature is shown, however the discussion of  should be interpreted as extending to all of the features of the feature vectors. Thus, in practice,  is an M dimensional space and vector  is an M dimensional vector.","After the noise feature vector, channel distortion feature vector, and clean signal feature vectors have been determined from the posterior mixture components, the process of  continues at step  by determining if there are anymore noisy vectors that need to be cleaned. If there are, steps , , , ,  and  are repeated to generate a clean signal vector for the noisy vector. When all of the noisy vectors have been processed at step , the noise reduction technique of  ends at step .","The present invention differs from the prior art in a number of ways. First, as discussed above, the present invention utilizes the variance of the noise and channel features. Also, the present invention utilizes a mixture of Gaussians to represent the noise component of the prior probability and to represent the channel distortion component of the prior probability. By using mixtures of Gaussians to model the noise and channel distortion, it is thought that the present invention will remove noise and channel distortion more accurately than if the noise and channel were modeled as single points as was done in the prior art. In practice, it is highly likely that the use of mixture of Gaussians for noise and for channels allows the algorithm to deal effectively with time-varying noise and channels, because an instantaneous noise and channel value can be accurately represented by one of many Gaussian components in the mixture distribution.","Similarly, the present invention, as shown in equation 5, also takes the variance of the observation probability into consideration when identifying the mean for the posterior probability. In prior art noise reduction techniques, this variance was assumed to be zero. By taking this variance into account, the present invention is able to more accurately select the mean for the posterior probability because it takes into account the error present in the approximation of equation 3.","Lastly, the iterative technique of the present invention is not shown in the prior art. Thus, prior art noise reduction systems do not iteratively modify the estimate of the clean signal vector. Instead, the prior art makes a single selection for the clean feature vector and does not try to improve upon that selection once it has been made.","Although the invention has been described above with reference to two signals (a clean signal and a noise signal), and one channel, the invention is not limited to this combination. In particular, additional signals from additional sources may be present and the signals may pass through more than one filter or channel. Those skilled in the art will recognize that the equations described above may be extended to cover any number of signals and any number of channels.","Although the present invention has been described with reference to particular embodiments, workers skilled in the art will recognize that changes may be made in form and detail without departing from the spirit and scope of the invention."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":["FIG. 5","FIG. 4"],"b":"450"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":["FIG. 6","FIG. 4"],"b":"454"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":["FIG. 7","FIG. 6"]},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 8"}]},"DETDESC":[{},{}]}
