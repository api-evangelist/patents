---
title: Multi-label active learning
abstract: Multi-label active learning may entail training a classifier with a set of training samples having multiple labels per sample. In an example embodiment, a method includes accepting a set of training samples, with the set of training samples having multiple respective samples that are each respectively associated with multiple labels. The set of training samples is analyzed to select a sample-label pair responsive to at least one error parameter. The selected sample-label pair is then submitted to an oracle for labeling.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08086549&OS=08086549&RS=08086549
owner: Microsoft Corporation
number: 08086549
owner_city: Redmond
owner_country: US
publication_date: 20071217
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE(S) TO RELATED APPLICATION(S)","BACKGROUND","SUMMARY","DETAILED DESCRIPTION","1: Introduction to Multi-Label Active Learning","2: Example Embodiments for Active Sampling and Labeling (ASL)","2.1: Example ASL Strategy","2.2: Example Error Bound for ASL Selection","3: Example Embodiments for an ASL-Capable Classifier","3.1: Example Kernelized Maximum Entropy Model (KMEM) for Classifier","3.2: Example Expectation Maximization (EM) Algorithm for Classifier","4: Example Embodiments for a Multi-Label Active Learning System","5: Example Device Implementations for Multi-Label Active Learning"],"p":["This U.S. Nonprovisional Patent Application claims the benefit of copending U.S. Provisional Patent Application No. 60\/986,881, filed on 9 Nov. 2007 and entitled \u201cMulti-Label Active Learning\u201d. U.S. Provisional Patent Application No. 60\/986,881 is hereby incorporated by reference in its entirety herein.","Many tasks today are performed faster or more efficiently with a computing machine. Before the computing machine can perform a given task, however, it is first taught how to do so. One task that machines can perform, after sufficient learning, is categorizing objects in accordance with one or more potential labels. To categorize an object, the object is input to a machine having a classification function, and the machine outputs a label for the object.","Supervised learning is a machine learning technique for creating a classification function from training data. Training data typically includes multiple sample objects with labels that are already categorized. After training with the labeled samples, the machine can accept a new object and produce a label for the new object without user interaction.","Creating the training data, on the other hand, does entail user interaction. In fact, significant and expensive human interaction may be required to create the training data, especially if the set of sample objects is extensive. To decrease this time and expense, active learning may be employed. Active learning is traditionally a technique in which an algorithm requests that a human manually label a subset of the training data samples. An active learning algorithm can carefully select which samples are to be labeled so that the total number of samples that need to be labeled in order to adequately train the machine is decreased.","The reduced labeling effort can therefore save significant time and expense as compared to labeling all of the possible training samples. Moreover, a quality active learner can judiciously select which subset of samples is to be labeled such that the classification function of the machine is not significantly compromised. Unfortunately, achieving such a judicious active learning algorithm is challenging, especially when using training samples that have multiple labels per sample.","Multi-label active learning may entail training a classifier with a set of training samples having multiple labels per sample. In an example embodiment, a method includes accepting a set of training samples, with the set of training samples having multiple respective samples that are each respectively associated with multiple labels. The set of training samples is analyzed to select a sample-label pair responsive to at least one error parameter. The selected sample-label pair is then submitted to an oracle for labeling.","In another example embodiment, a method further includes receiving from the oracle a relevancy indication for the selected sample-label pair. The relevancy indication is added to the set of training samples at the selected sample-label pair to update the set of training samples. The classifier is updated using the updated set of training samples. In yet another example embodiment, the at least one error parameter may be a generalization or classification error parameter that is capable of being reduced based on mutual information among the multiple labels.","This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used as an aid in determining the scope of the claimed subject matter. Moreover, other method, system, apparatus, device, media, procedure, API, arrangement, etc. embodiments are described herein.","Conventional active learning methods are directed to samples with a single label per sample. Such active learning methods iteratively annotate a set of elaborately selected samples so that the generalization error is minimized with each iteration. Active learning is traditionally aimed at selecting a set of most, or at least relatively more, informative training samples so that the statistical classification models can be efficiently trained with a smaller training set while still preserving an expected level of performance for the classifier. Existing strategies for sample selection in active learning mainly focus on the binary classification scenario.","However, in many real-world applications such as text search, image retrieval, and bioinformatics, a sample is usually associated with multiple concepts rather than a single one. Under such a multi-label setting, each sample is manually annotated by an oracle with a \u201cpositive\u201d or \u201cnegative\u201d label for each concept. For example, given an image to be annotated and four concepts, the annotating oracle (e.g., one or more human observers) separately judges if each of the four concepts can be assigned to the given image. For instance, a human may determine if each of Beach, Field, Mountain, and Urban is relevant to the given image. As a result of the additional human interaction, active learning with multi-labeled samples is often much more time-consuming than with single-labeled samples, especially when the number of labels per sample is large.","A straight-forward approach for tackling active learning in a multi-label setting is to translate it into a set of binary problems. In other words, each labeling concept may be independently handled by a binary-based active learning algorithm. However, such a relatively na\u00efve solution does not take the rich correlations among multiple labels into consideration. These rich correlations have a great potential to improve the efficiency of active learning algorithms.","Thus, for active learning in multi-label settings, not only can the samples be appropriately selected for labeling, but the label set to be manually annotated by an oracle for a particular selected sample may also be appropriately selected. Selecting labels for annotation from among multiple potential labels may be pertinent because the varying contribution levels of different labels to the minimization of the generalization error may be different due to the existence of label correlations.","In a general example embodiment, partial labels (i.e., fewer than all of the potential labels) for a specific sample may be selected to be annotated while other ones can be inferred by exploiting the label correlations. Consequently, the human labor involved with active learning in a multi-label environment can be significantly reduced because all of the labels that are associated with a given sample need not be annotated. To this end, examples of efficient multi-label active learning strategies are described herein.","In a specific example embodiment, a step-by-step procedure entails selecting sample-label pairs, instead of only samples as in conventional active learning approaches, to minimize a derived Multi-Label Bayesian Classification Error Bound. Also, an example Bayesian classifier, which utilizes a Kernelized Maximum Entropy Model (KMEM), is described as an active learner. This Bayesian classifier is able to model the correlations among labels. Furthermore, an example Expectation Maximization (EM)-based parameter estimation algorithm is described to handle the incomplete labels resulting from the aforementioned partial labeling.","In another example embodiment, an active learning strategy as described herein iteratively selects sample-label pairs to minimize, or at least reduce, the expected classification error. For instance, with each iteration, the annotating oracle may be asked to annotate or confirm a selected portion of the potential labels while the remaining unlabeled categories are inferred according to the label correlations. Versions of this algorithmic strategy are termed herein Active Sampling and Labeling (ASL).","An intuitive explanation of an example embodiment for this ASL strategy is as follows: There exists both sample and label redundancy for multi-labeled samples. Annotating a set of selected sample-label pairs can therefore provide sufficient information for training a classifier because the information in the selected sample-label pairs can be propagated to the remaining pairs along both sample and label \u201cdimensions\u201d. Thus, unlike traditional binary-based active learning strategies that only take the sample redundancy into account when selecting samples, example ASL embodiments as described herein additionally consider the label dimension to leverage the rich dependencies and other redundancies embedded in multiple labels.","Certain embodiments of ASL are to efficiently select an optimal, or at least relatively more informative, portion of the labels instead of all of the labels for a particular sample. This strategy can significantly reduce the human labors involved with active learning. By way of example, Field and Mountain labels tend to be relevant simultaneously to an image. It can therefore be reasonable to select one of the two concepts for annotation while omitting the other because the uncertainty of the other can be appreciably decreased after annotating the selected one. Another example is provided by Mountain and Urban labels. In contrast to Field and Mountain, these two concepts often do not occur together. Thus, positively annotating one of them most likely eliminates the existence of the other.","Two relevant issues are addressed herein for implementing an example ASL embodiment. First, an example selection strategy is described for finding appropriate sample-label pairs for annotation. To handle this first issue, the selection of sample-label pairs is performed by minimizing, or at least reducing, a derived Multi-Label Bayesian Classification Error Bound. Selecting sample-label pairs under this paradigm can efficiently reduce both the sample and the label uncertainty.","Second, an example approach is described for modeling the label correlations for a classifier. Because versions of the ASL strategy utilize the label dependencies to reduce label annotation labors, the corresponding classifier is also to model the label correlations. In example embodiments, this second issue is handled with implementations of a Kernelized Maximum Entropy Model (KMEM), which is capable of modeling such correlations. Furthermore, because embodiments of the ASL strategy omit annotating a portion of the labels for at least some of the samples, the label information for training the KMEM is incomplete. An Expectation-Maximization (EM) algorithm is also adopted in example embodiments to handle this issue.","Other general and specific example embodiments are described herein below. Although certain example aspects may be described in a specific context of hardware or software, such description is by way of example only. In other words, the example embodiments described herein may be implemented fully or partially in hardware, software, firmware, fixed logic circuitry, combinations thereof, and so forth.",{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 1","b":["100","100","102","104","110","112","104","106","106","108"]},"Samples  may correspond to text items, images, videos, biological data, combinations thereof, or any other type of data set. Although a single sample  that is associated with two labels and is explicitly shown, there may be many (e.g., dozens, hundreds, thousands, or more of) such samples . Also, each sample  may be associated with any number of labels . During the process to train classifier , each label  may be in a labeled or an unlabeled state.","In operation for an example embodiment, active learning classifier trainer  selects from training samples set  a sample  for labeling at arrow . The sample selection may be based on one or more criteria. As described further herein below, this sample selection may include selecting a sample -label  pair for labeling. Active learning classifier trainer  submits the selected sample to oracle  to request that the sample be labeled at arrow .","Oracle  may also be termed a teacher, an annotator, and so forth. Oracle  is typically a human or a group of humans that is capable of labeling each sample. The labeling may indicate, for example, a relevancy of label  to its associated sample . If two labeling categories are permitted for each label concept, the relevancies may be positive\/negative, relevant\/not relevant, related\/not related, and so forth. Alternatively, the relevancy labeling may be part of a scale having more than two categories (e.g., not relevant, slightly relevant, fairly relevant, and very relevant). Oracle  provides or inputs the labeled relevancy at arrow  to active learning classifier trainer .","At arrow , active learning classifier trainer  returns the labeled sample having the newly-categorized label to the set of training samples  at the selected sample -label  pair to update the set of training samples . Using the additional labeling of the updated training samples set , classifier  is updated at arrow . The process can thus include sample selection , oracle labeling \/, training sample set updating , and classifier updating . The process may be iterated until a desired criterion is reached. This criterion may be, for example, a predetermined number of iterations, a convergence of expected\/estimated error performance, a number of labels that can be submitted to and returned from oracle , some combination thereof, and so forth.","After training, classifier  may be given an input sample object from a target data set. In response, classifier  outputs one or more predicted labeled concepts in accordance with its trained classifying algorithm. Classifier  may employ any classifying algorithm. Example general classifying algorithms include, but are not limited to, Neural Network Multi-layer Perception), Support Vector Machines (SVM), k-Nearest Neighbors, Gaussian Mixture Model, Gaussian, Naive Bayes, Decision Tree, RBF classifiers, combinations thereof, and so forth. An example classifying algorithm that is tailored to support multi-label active learning is described herein below in Section 3.","Active learning classifier trainer , set of training samples , and classifier  may comprise processor-executable instructions. By way of example, active learning classifier trainer  and classifier  may be software realized on processor-accessible media. Also, training samples set  may be a data structure realized on processor-accessible media. Although illustrated as separate blocks, the processor-executable instructions may be implemented jointly. For instance, the functionality of active learning classifier trainer  and classifier  may be combined into one set of processor-executable instructions (e.g., multi-label active learning system  of ).",{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 2","b":["200","212","200","102","104","110","112","102","202"]},"Generally, each respective sample () is associated with multiple labels (). Training samples set  may include any number of samples , each of which may have any number of associated labels . As illustrated in , however, there are three samples ()-(), each of which has three labels -. Specifically, a sample () is associated with label (), label (), and label (). A sample () is associated with label (), label (), and label (). A sample () is associated with label (), label (), and label (). It should be understood that different samples  may alternatively be associated with different numbers of labels  within a single set of training samples .","In operation of an example embodiment, sample-label pair selector  selects at arrow  a sample () and an associated label () to jointly form a sample-label pair  for labeling by oracle . This sample-label pair selection may be made responsive to an error parameter, such as a generalization or classification error parameter. For instance, it may be made responsive to an error bound. By way of example, a sample-label pair may be selected responsive to a Bayesian classification error bound for a multi-label scenario. More specifically, a sample-label pair may be selected so as to reduce, if not minimize, an expected Bayesian error. As described herein below in Section 2.2 with particular reference to formulation (3), the sample-label pair selection may be effectuated by maximizing the entropy of the selected sample-label pair and by maximizing the mutual information between the selected label and the other labels that are also associated with the selected sample.","After sample-label pair selector  has selected a sample-label pair , active learning classifier trainer  submits the selected sample-label pair  to oracle  at arrow  for labeling. At arrow , oracle  returns an indication of relevance  of the submitted label  to its associated sample . This indicated relevancy labeling  is incorporated into the set of training samples  to update it. With the updated training samples set , active learning classifier trainer  updates classifier  at arrow .","The following describes an example operation. Sample-label pair selector  may consider each of the samples  and associated labels  of training samples set . From these variables, a sample-label pair  is selected at arrow  so as to minimize, or at least reduce, an expected Bayesian error Assume that the selected sample-label pair  is sample () and label (). It should be noted that one or more other labels ()that are associated with sample () can be excluded from this selection (at least during a single individual iteration). The selected sample-label pair  is submitted to oracle  at arrow . Active learning classifier trainer  requests that oracle  indicate the relevance of label ()to its associated sample (). After relevancy indication  is returned from oracle  at arrow , active learning classifier trainer  can update the set of training samples . With the updated training samples set , active learning classifier trainer  can update classifier  at arrow .","In Section 2.1 below, the dual dimensionality along both sample and label dimensions of an example ASL embodiment in a multi-label setting is described with reference to . In Section 2.2, a Bayesian error bound is derived that gives the expected classification error given a selected sample-label pair. Also in Section 2.2, an ASL strategy for an example embodiment is deduced by selecting the sample-label pairs responsive to this bound.","Traditional active learning algorithms employ a one-dimensional active selection approach, which only reduces the sample uncertainty. Yet multi-label classifiers have uncertainty among different labels as well as different samples. In contrast to traditional binary active learning approaches that select the most informative samples for annotation, ASL embodiments as described herein jointly select both the samples and the labels. Different labels of a certain sample have different contributions to minimizing the expected classification error of the to-be-trained classifier. Thus, annotating a well-selected portion of the labels may provide sufficient information for learning the classifier. This ASL strategy trades off between the annotation labors and the learning performance along two dimensions\u2014the sample and the label dimensions.",{"@attributes":{"id":"p-0044","num":"0043"},"figref":["FIG. 3","FIG. 3"],"b":["300","304","306","302","302","302","106","108","302","302","306","304","306","304"]},"Samples  are illustrated as circles and identified by the variable \u201cX\u201d, X. . . X. . . X. . . X. Labels  are illustrated as squares with rounded corners. As indicated by legend , each label may be categorized or labeled as a positive concept (\u201cP\u201d), as a negative concept (\u201cN\u201d), as an unlabeled concept (\u201c?\u201d), or it may be selected for labeling of the concept (\u201cS\u201d). As indicated by the ellipses (\u201c . . . \u201d) in each matrix , more samples  and labels  than those that are explicitly illustrated may be present.","The illustrated example labeling states for matrices B and A are as follows. For the before ASL matrix B, sample Xhas three associated labels that are: ?, ?, and P. Sample Xhas three associated labels that are: ?, P, and N. Sample Xhas three associated labels that are: ?, ?, and ?. Sample Xhas three associated labels that are: P, ?, and P. For the after ASL matrix A, sample Xhas three associated labels that are: S, ?, and P. Sample Xhas three associated labels that are: ?, P, and N. S ample Xhas three associated labels that are: S, ?, and S. Sample Xhas three associated labels that are: P, ?, and P. Thus, example ASL procedure  has selected three sample-label pairs for labeling. These three sample-label pairs include one with sample Xand two with sample X.","ASL  is therefore a two-dimensional active learning strategy, which reduces the uncertainty along the dimensionalities of both samples and labels. More specifically, it is recognized that along the label dimension the labels correlatively interact. Consequently, once at least a portion of the labels are annotated, the remaining unlabeled concepts can be inferred based on the label correlations. This approach can save significant labor as compared to fully annotating all of the multiple labels. Its efficiency is increased all the more when the number of labels is extraordinarily large. For instance, an image may be associated with hundreds, thousands, or even more concepts that can be labeled. Manually labeling each and every potential concept would incur large labor costs for even just one such image. Hence, because ASL  selects the more, if not most, informative labels for annotation, it can save significant labor costs.",{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIG. 4","b":["400","400"]},"Flow diagram  includes nine blocks -. By way of example, the description of flow diagram  includes references to other figures, such as , , and . In an example embodiment of flow diagram , at block , a current set of training samples is input to and accepted by the system. For example, a set of training samples  may be input to active learning classifier trainer , with set of training samples  including multiple respective samples  that are each respectively associated with multiple labels .","At block , a current set of training samples is analyzed. For example, the current state of training samples set , which is likely partially but not fully labeled, may be analyzed. At block , a sample-label pair is selected for labeling responsive to at least one error parameter. For example, a sample-label pair  having a sample () and associated label ()may be selected for labeling responsive to an error parameter, such as a generalization or classification error parameter. The other labels () that are associated with sample () may be labeled or unlabeled. If unlabeled, they may remain unlabeled or may be subsequently requested for labeling.","At block , at least one selected sample-label pair is submitted to an oracle. For example, sample () and label ()of sample-label pair  may be submitted to oracle . At block , a relevancy indication for the selected sample-label pair may be received from the oracle. For example, a positive or negative indication of the conceptual relevancy  of label ()to sample () may be received from oracle .","At block , the current set of training samples is updated with the received relevancy indication. For example, active learning classifier trainer  may add the positive\/negative relevancy indication  at label ()of associated sample () in set of training samples . At block , the classifier is updated. For example, a classification algorithm of classifier  may be updated using the updated training samples set .","At block , it is determined if additional classifier training is to be performed. For example, this determination may be made with reference to one or more criteria. If more training is to be performed (\u201cYes\u201d branch), the method of flow diagram  continues at block . If no more training is to be performed (\u201cNo\u201d branch), then at block , the final classifier is produced. Classifier  may then be used to label new objects.","By way of example, the error parameter (of block ) may be a generalization or classification error, such as Bayesian error bound that is described herein below. The error parameter may be reduced, including minimized. For instance, the Bayesian error bound may be minimized to select the sample-label pair. As described below in Section 2.2 with reference to formulation (3), the bound may be minimized by maximizing the mutual information between a selected label of a selected sample and the labels that are associated with the selected sample. Furthermore, the parameter bound may be separated into an entropy portion that involves the selected label and a mutual information portion that excludes the selected label.","In the following Section 2.2, a two-dimensional selection criterion based on a derived Bayesian classification error bound for multi-label settings is described. It should be noted that as illustrated in , during the learning process, some samples may lack some labels because a portion of the labels remain un-annotated. This differs from traditional active learning algorithms. In Section 3.2 below, a strategy for training the classification model from incomplete labels is described.","For an example embodiment, the ASL learner requests label annotations on the basis of sample-label pairs which, once incorporated into the training set, are expected to result in the lowest generalization error. A Multi-Labeled Bayesian Error Bound is derived with a selected sample-label pair under a multi-label setting, and ASL accordingly selects the optimal pairs to minimize this bound.","The following notations are utilized herein. For each sample x, it has m labels y(1\u2266i\u2266m) Each label indicates whether its corresponding concept occurs. In each ASL iteration, some of these labels have already been annotated while others have not. Let U(x)={i|(x,y) is unlabeled} denote the set of indices of the unlabeled part, and L(x)={i|(x,y) is labeled} denote the labeled part. It should be noted that L(x) can be the empty set \u00d8 when no label has yet been annotated for sample x. Let P(y|x) be the unknown conditional distribution over the samples, where y={0, 1}is the complete label vector and P(x) is the marginal sample distribution.","First, the establishment of a Bayesian error bound is described. The error bound is for classifying one unlabeled yonce yis actively selected for annotating.","Lemma 1: Given is a sample x and its unlabeled and labeled parts U(x) and L(x). Once yis tentatively selected to be requested for labeling (but not yet annotated by the oracle), the Bayesian classification error E(y|y,y,x) for an unlabeled y, i\u03b5U(x) is bounded as given below by formulation (1):",{"@attributes":{"id":"p-0060","num":"0059"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":[{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"msub":[{"mi":["y","i"]},{"mi":["y","s"]}],"mo":"\u2758"},"mo":";","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","mi":"x"}}},{"mfrac":{"mn":["1","2"]},"mo":"\u2062","mrow":{"mi":"H","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"msub":[{"mi":["y","i"]},{"mi":["y","s"]}],"mo":"\u2758"},"mo":";","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","mi":"x"}}}}],"mo":"\u2264"},"mo":"=","mfrac":{"mrow":{"munder":{"mo":"\u2211","mrow":{"mi":"t","mo":",","mrow":{"mi":"r","mo":"\u2208","mrow":{"mo":["{","}"],"mrow":{"mn":["0","1"],"mo":","}}}}},"mo":"\u2062","mrow":{"mrow":[{"mo":"-","mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":["y","i"]},"mo":"=","mi":"t"},{"msub":{"mi":["y","s"]},"mo":"=","mrow":{"mi":"r","mo":"\u2758","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}}}],"mo":[",",","],"mi":"x"}}}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"msub":{"mi":["y","i"]},"mo":"=","mrow":{"mrow":{"mi":"t","mo":"\u2758","msub":{"mi":["y","s"]}},"mo":"=","mi":"r"}},"mo":";","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","mi":"x"}}}],"mo":["\u2062","\u2062","\u2062"],"mi":"log","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},"mn":"2"}}},{"mrow":{"mo":["(",")"],"mn":"1"}}]}}}},"br":{},"sub":["i","s","L(x)","i ","s ","i ","s ","L(x) "]},"Proof of Lemma 1: Because the selected ycan take on two values {0, 1}, there are two possible posterior distributions for the unlabeled y, P(y|y=1;y,x) and P(y|y=0;y,x) If y=1 holds, the Bayesian classification error is as follows:\n\n(=1)=min{(=1=1),(=0=1)}.\n\nGiven the inequality\n",{"@attributes":{"id":"p-0062","num":"0061"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":[{"mi":"min","mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mi":"p","mo":",","mrow":{"mn":"1","mo":"-","mi":"p"}}}},{"mfrac":{"mn":["1","2"]},"mo":"\u2062","mrow":{"mi":"H","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"p"}}}],"mo":"\u2264"},"mo":","}}},"br":{}},{"@attributes":{"id":"p-0063","num":"0062"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"mrow":{"msub":[{"mi":["y","i"]},{"mi":["y","s"]}],"mo":"\u2758"},"mo":"=","mn":"1"},"mo":";","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","mi":"x"}}},{"mfrac":{"mn":["1","2"]},"mo":"\u2062","mrow":{"mrow":{"mi":"H","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"mrow":{"msub":[{"mi":["y","i"]},{"mi":["y","s"]}],"mo":"\u2758"},"mo":"=","mn":"1"},"mo":";","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","mi":"x"}}},"mo":"."}}],"mo":"\u2264"}}},"br":{},"sub":"s"},{"@attributes":{"id":"p-0064","num":"0063"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"mrow":{"msub":[{"mi":["y","i"]},{"mi":["y","s"]}],"mo":"\u2758"},"mo":"=","mn":"0"},"mo":";","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","mi":"x"}}},{"mfrac":{"mn":["1","2"]},"mo":"\u2062","mrow":{"mrow":{"mi":"H","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"mrow":{"msub":[{"mi":["y","i"]},{"mi":["y","s"]}],"mo":"\u2758"},"mo":"=","mn":"0"},"mo":";","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","mi":"x"}}},"mo":"."}}],"mo":"\u2264"}}}},"Hence, the Bayesian classification error bound given the selected sample ycan be computed as follows:",{"@attributes":{"id":"p-0066","num":"0065"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"msub":[{"mi":["y","i"]},{"mi":["y","s"]}],"mo":"\u2758"},"mo":";","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","mi":"x"}}},{"mrow":[{"mrow":[{"mrow":[{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["y","s"]},"mo":"=","mrow":{"mn":"1","mo":"\u2758","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}}},"mo":",","mi":"x"}}},{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"mrow":{"msub":[{"mi":["y","i"]},{"mi":["y","s"]}],"mo":"\u2758"},"mo":"=","mn":"1"},"mo":";","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","mi":"x"}}}],"mo":"\u2062"},{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["y","s"]},"mo":"=","mrow":{"mn":"0","mo":"\u2758","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}}},"mo":",","mi":"x"}}},{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"mrow":{"msub":[{"mi":["y","i"]},{"mi":["y","s"]}],"mo":"\u2758"},"mo":"=","mn":"0"},"mo":";","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","mi":"x"}}}],"mo":"\u2062"}],"mo":"+"},{"mrow":[{"mfrac":{"mn":["1","2"]},"mo":["\u2062","\u2062"],"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["y","s"]},"mo":"=","mrow":{"mn":"1","mo":"\u2758","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}}},"mo":",","mi":"x"}}},{"mi":"H","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"mrow":{"msub":[{"mi":["y","i"]},{"mi":["y","s"]}],"mo":"\u2758"},"mo":"=","mn":"1"},"mo":";","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","mi":"x"}}}]},{"mfrac":{"mn":["1","2"]},"mo":["\u2062","\u2062"],"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["y","s"]},"mo":"=","mrow":{"mn":"0","mo":"\u2758","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}}},"mo":",","mi":"x"}}},{"mi":"H","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"mrow":{"msub":[{"mi":["y","i"]},{"mi":["y","s"]}],"mo":"\u2758"},"mo":"=","mn":"0"},"mo":";","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","mi":"x"}}}]}],"mo":"+"}],"mo":"\u2264"},{"mfrac":{"mn":["1","2"]},"mo":"\u2062","mrow":{"mi":"H","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"msub":[{"mi":["y","i"]},{"mi":["y","s"]}],"mo":"\u2758"},"mo":";","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","mi":"x"}}}}],"mo":"="}],"mo":"="}}},"br":{}},"It should be noted that that this bound is irrelevant to the true label of the selected sample yIn fact, before the annotating oracle provides the annotation of y, the true value of yis unknown. However, regardless of what yis categorized to, 1 or 0, this error bound still holds.","Based on Lemma 1, the following theorem, which bounds the multi-label error, can be obtained:","Theorem 1: (Multi-labeled Bayesian classification error bound). Under the condition of Lemma 1, the Bayesian classification error bound E(y|y; y,x) for sample x over the label vector y is as follows:",{"@attributes":{"id":"p-0070","num":"0069"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"mi":"y","mo":"\u2758","msub":{"mi":["y","s"]}},"mo":";","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","mi":"x"}}},{"mfrac":{"mn":["1","2"]},"mo":"-","mrow":{"mfrac":{"mn":"1","mrow":{"mn":"2","mo":"\u2062","mi":"m"}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"mi":"M","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"I","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["y","i"]},"mo":";","mrow":{"msub":[{"mi":["y","s"]},{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}],"mo":"\u2758"}},"mo":",","mi":"x"}}}}}}}],"mo":"\u2264"}}},"br":{},"sub":["i","s","L(x)","i ","s ","L(x)"]},"Proof of Theorem 1: The derivation follows:",{"@attributes":{"id":"p-0072","num":"0071"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"mi":"y","mo":"\u2758","msub":{"mi":["y","s"]}},"mo":";","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","mi":"x"}}},{"mrow":{"mfrac":{"mn":"1","mi":"m"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"msub":[{"mi":["y","i"]},{"mi":["y","s"]}],"mo":"\u2758"},"mo":";","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","mi":"x"}}}}},"mo":"\u2062","mover":{"mo":"\u2264","mrow":{"mo":["(",")"],"mn":"2"}}}],"mo":["\u2062","\u2062","\u2062"],"mover":{"mo":"=","mrow":{"mo":["(",")"],"mn":"1"}},"mi":{}}}},{"mtd":{"mrow":{"mi":{},"mo":"\u2062","mrow":{"mfrac":{"mn":"1","mrow":{"mn":"2","mo":"\u2062","mi":"m"}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"mi":"H","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"msub":[{"mi":["y","i"]},{"mi":["y","s"]}],"mo":"\u2758"},"mo":";","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","mi":"x"}}}}}}}},{"mtd":{"mrow":{"mover":{"mo":"=","mrow":{"mo":["(",")"],"mn":"3"}},"mo":["\u2062","\u2062"],"mi":{},"mrow":{"mrow":{"mfrac":{"mn":"1","mrow":{"mn":"2","mo":"\u2062","mi":"m"}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"H","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":[{"mi":["y","i"]},{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}],"mo":"\u2758"},"mo":",","mi":"x"}}},{"mi":"M","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"I","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["y","i"]},"mo":";","mrow":{"msub":[{"mi":["y","s"]},{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}],"mo":"\u2758"}},"mo":",","mi":"x"}}}}],"mo":"-"}}}},"mo":"\u2062","mover":{"mo":"\u2264","mrow":{"mo":["(",")"],"mn":"4"}}}}}},{"mtd":{"mrow":{"mi":{},"mo":"\u2062","mrow":{"mfrac":{"mn":"1","mrow":{"mn":"2","mo":"\u2062","mi":"m"}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mn":"1","mo":"-","mrow":{"mi":"M","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"I","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["y","i"]},"mo":";","mrow":{"msub":[{"mi":["y","s"]},{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}],"mo":"\u2758"}},"mo":",","mi":"x"}}}}}}}}}}},{"mtd":{"mrow":{"mover":{"mo":"=","mrow":{"mo":["(",")"],"mn":"5"}},"mo":["\u2062","\u2062"],"mi":{},"mrow":{"mfrac":{"mn":["1","2"]},"mo":"-","mrow":{"mfrac":{"mn":"1","mrow":{"mn":"2","mo":"\u2062","mi":"m"}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"mi":"M","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"I","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["y","i"]},"mo":";","mrow":{"msub":[{"mi":["y","s"]},{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}],"mo":"\u2758"}},"mo":",","mi":"x"}}}}}}}}}}]}}},"br":{}},"In an example embodiment, ASL is implemented in the context of pool-based active learning. In other words, a large pool P is available to the trainer as sampled from P(x), and the ASL algorithm then selects the most informative sample-label pairs from the pool. Given the above Theorem 1, the expected Bayesian classification error over the samples in P when selecting a sample-label pair (x,y) for labeling can be written as:",{"@attributes":{"id":"p-0074","num":"0073"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"P"}},{"mfrac":{"mn":"1","mrow":{"mo":["\uf603","\uf604"],"mi":"P"}},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"mi":"y","mo":"\u2758","msub":{"mi":["y","s"]}},"mo":";","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","msub":{"mi":["x","s"]}}}},{"munder":{"mo":"\u2211","mrow":{"mi":"x","mo":"\u2208","mrow":{"mi":["P","\\"],"mo":["\u2062","\u2062"],"msub":{"mi":["x","s"]}}}},"mo":"\u2062","mrow":{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"y","mo":"\u2758","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","mi":"x"}}}}],"mo":"+"}}}],"mo":"="}}},{"mtd":{"mrow":{"mo":"=","mrow":{"mfrac":{"mn":"1","mrow":{"mo":["\uf603","\uf604"],"mi":"P"}},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"mi":"y","mo":"\u2758","msub":{"mi":["y","s"]}},"mo":";","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","msub":{"mi":["x","s"]}}}},{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"y","mo":"\u2758","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","msub":{"mi":["x","s"]}}}},{"munder":{"mo":"\u2211","mrow":{"mi":["x","P"],"mo":"\u2208"}},"mo":"\u2062","mrow":{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"y","mo":"\u2758","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","mi":"x"}}}}],"mo":["-","+"]}}}}}}]}}}},"The above classification error on the pool can be used to estimate the expected error over the full distribution P(x) because the pool not only provides a finite set of sample-label pairs but also an estimation of P(x). Thus, a goal is to select a best sample-label pair (x*,y*) so as to minimize the above-identified expected error. This can be rewritten as:",{"@attributes":{"id":"p-0076","num":"0075"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["x","s"],"mo":"*"},{"mi":["y","s"],"mo":"*"}],"mo":","}},{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":"min","mrow":{"mrow":[{"msub":{"mi":["x","s"]},"mo":"\u2208","mi":"P"},{"msub":{"mi":["y","s"]},"mo":"\u2208","mrow":{"mi":"U","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","s"]}}}}],"mo":","}},"mo":"\u2062","mrow":{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"P"}}}}],"mo":"="}}},{"mtd":{"mrow":{"mo":"=","mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":"min","mrow":{"mrow":[{"msub":{"mi":["x","s"]},"mo":"\u2208","mi":"P"},{"msub":{"mi":["y","s"]},"mo":"\u2208","mrow":{"mi":"U","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","s"]}}}}],"mo":","}},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"mi":"y","mo":"\u2758","msub":{"mi":["y","s"]}},"mo":";","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","msub":{"mi":["x","s"]}}}},{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"y","mo":"\u2758","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","msub":{"mi":["x","s"]}}}}],"mo":"-"}}}}}}}]}}}},"Applying the conclusion in Theorem 1, the following formulation (2) can be derived:",{"@attributes":{"id":"p-0078","num":"0077"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":[{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"mi":"y","mo":"\u2758","msub":{"mi":["y","x"]}},"mo":";","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","msub":{"mi":["x","s"]}}}},{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"y","mo":"\u2758","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","msub":{"mi":["x","s"]}}}}],"mo":"-"},{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"mi":"y","mo":"\u2758","msub":{"mi":["y","s"]}},"mo":";","msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":",","msub":{"mi":["x","s"]}}}},{"mfrac":{"mn":["1","2"]},"mo":"-","mrow":{"mfrac":{"mn":"1","mrow":{"mn":"2","mo":"\u2062","mi":"m"}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"mi":"M","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"I","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["y","i"]},"mo":";","mrow":{"msub":[{"mi":["y","s"]},{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}],"mo":"\u2758"}},"mo":",","msub":{"mi":["x","s"]}}}}}}}}],"mo":["\u2264","\u2264"]}},{"mrow":{"mo":["(",")"],"mn":"2"}}]}}}},"br":{},"sub":["s","L(x)","s","L(x)","s","s","L(x)","s","s","L(x)","s"]},"Consequently, by minimizing the obtained Bayesian error bound of formulation (2), the most informative sample-label pair for annotation may be selected according to formulation (3) as given below:",{"@attributes":{"id":"p-0080","num":"0079"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["x","s"],"mo":"*"},{"mi":["y","s"],"mo":"*"}],"mo":","}},{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":"min","mrow":{"mrow":[{"msub":{"mi":["x","s"]},"mo":"\u2208","mi":"P"},{"msub":{"mi":["y","s"]},"mo":"\u2208","mrow":{"mi":"U","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","s"]}}}}],"mo":","}},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mfrac":{"mn":["1","2"]},"mo":"-","mrow":{"mfrac":{"mn":"1","mrow":{"mn":"2","mo":"\u2062","mi":"m"}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"mi":"MI","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["y","i"]},"mo":";","mrow":{"msub":[{"mi":["y","s"]},{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}],"mo":"\u2758"}},"mo":",","msub":{"mi":["x","s"]}}}}}}}}}}],"mo":["=","\u2062"],"mi":{}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":"max","mrow":{"mrow":[{"msub":{"mi":["x","s"]},"mo":"\u2208","mi":"P"},{"msub":{"mi":["y","s"]},"mo":"\u2208","mrow":{"mi":"U","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","s"]}}}}],"mo":","}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"mi":"MI","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["y","i"]},"mo":";","mrow":{"msub":[{"mi":["y","s"]},{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}],"mo":"\u2758"}},"mo":",","msub":{"mi":["x","s"]}}}}}}}}}}]}},{"mrow":{"mo":["(",")"],"mn":"3"}}]}}}}},"As described above in Section 2.1, example ASL embodiments implement an active learning algorithm along two dimensions, which can reduce not only sample uncertainty but also label uncertainty. The selection-strategy of formulation (3) accurately reflects these two uncertainty reduction targets. The last term of formulation (3) can be rewritten as follows:",{"@attributes":{"id":"p-0082","num":"0081"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"mi":"MI","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["y","i"]},"mo":";","mrow":{"msub":[{"mi":["y","s"]},{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}],"mo":"\u2758"}},"mo":",","msub":{"mi":["x","s"]}}}}},{"mrow":{"mi":"MI","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["y","s"]},"mo":";","mrow":{"msub":[{"mi":["y","s"]},{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}],"mo":"\u2758"}},"mo":",","msub":{"mi":["x","s"]}}}},"mo":"+"}],"mo":["=","\u2062"],"mi":{}}}},{"mtd":{"mrow":{"mi":{},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mrow":[{"mi":"i","mo":"=","mn":"1"},{"mi":["i","s"],"mo":"\u2260"}],"mo":","},"mi":"m"},"mo":"\u2062","mrow":{"mi":"MI","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["y","i"]},"mo":";","mrow":{"msub":[{"mi":["y","s"]},{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}],"mo":"\u2758"}},"mo":",","msub":{"mi":["x","s"]}}}}}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"mrow":{"mi":"H","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":[{"mi":["y","s"]},{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}],"mo":"\u2758"},"mo":",","msub":{"mi":["x","s"]}}}},"mo":"+"}}}},{"mtd":{"mrow":{"mi":{},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mrow":[{"mi":"i","mo":"=","mn":"1"},{"mi":["i","s"],"mo":"\u2260"}],"mo":","},"mi":"m"},"mo":"\u2062","mrow":{"mrow":{"mi":"MI","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["y","i"]},"mo":";","mrow":{"msub":[{"mi":["y","s"]},{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}],"mo":"\u2758"}},"mo":",","msub":{"mi":["x","s"]}}}},"mo":"."}}}}}]}}}},"As is apparent from the formulation above, an objective selection function for ASL may be divided into two portions: H(y|y, x) and",{"@attributes":{"id":"p-0084","num":"0083"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mrow":[{"mi":"i","mo":"=","mn":"1"},{"mi":["i","s"],"mo":"\u2260"}],"mo":","},"mi":"m"},"mo":"\u2062","mrow":{"mrow":{"mi":"MI","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["y","i"]},"mo":";","mrow":{"msub":[{"mi":["y","s"]},{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}],"mo":"|"}},"mo":",","msub":{"mi":["x","s"]}}}},"mo":"."}}}},"br":{},"sub":["s","s"]},"When computing the mutual information terms of formulation (3) above, the posterior distribution P(y|x) is used. Although the true distribution is unknown, it can be estimated using the current state of the learner. Such an approximation is reasonable because the most useful labeling is usually consistent with the learner's prior belief over the majority (but not necessarily all) of the unlabeled pairs.","It should be understood that the posterior distribution P(y|x) is usually relevant to modeling the label correlations. It can be assumed that there is independence among the different labels (e.g., P(y|x)\u2212\u03a0P(y|x)) and correspondingly that the mutual information term becomes MI(y;y|y,x)=0,i\u2260s. In such cases, the selection criterion reduces to (x*,y*)=arg maxH(y|y,x), which implies that the most uncertain sample-label pair is to be selected. Such a criterion neglects the label correlations and is therefore less efficient at reducing label uncertainty. Consequently, a statistical method that can model the label correlations may be adopted. Such a Bayesian model is described in the following Section 3.",{"@attributes":{"id":"p-0087","num":"0086"},"figref":["FIG. 5","FIG. 2"],"b":["500","500","500","502","508","500","500"]},"In an example embodiment of flow diagram , at block , a mutual information total between a selected label and each label of an associated selected sample is accumulated. For example, assuming sample () is the selected sample and label ()is the selected label, the mutual information total between label ()and labels ()-(), which are associated with sample (), may be accumulated.","At block , the mutual information total accumulation (of block ) is duplicated for each unlabeled label of the selected sample. For example, assuming label ()is unlabeled and label ()is labeled, the mutual information total accumulation is duplicated for label ()","At block , the mutual information total accumulation (of block ) and the duplication (of block ) is repeated for each sample in the set of training samples. For example, the mutual information total accumulation for each unlabeled label of a selected sample may be repeated for each of samples () and (). The actions of blocks - result in a number of mutual information total accumulations being determined.","At block , from the multiple determined mutual information total accumulations, the maximum accumulated total of mutual information is ascertained. This ascertained maximum corresponds to the sample-label pair  that is to be selected for the next relevancy indication  annotation by the oracle .","In the ASL strategies as described in Section 2 above, it is noted that a statistical model may be employed to measure label correlations. However, common multi-label classifiers, such as one-against-rest encoded binary SVM and others, address the classification of multi-labeled samples in an independent manner. Such models disregard the label correlations, and they therefore cannot be effectively implemented with ASL, at least without some measure of modification. In this Section 3, a multi-labeled Bayesian classifier is described in which the correlative relations between and among different labels are well modeled.","The principle of a Maximum Entropy Model (MEM) is to model the known and to assume nothing about the unknown. Traditional single-label data classification suffers from the same problem as binary SVM. Although the single-labeled MEM can be extended to multi-labeled scenarios, the model is linear and does not make use of a powerful kernel method like SVM. However, MEM can be adjusted to address the difficulty that arises from incomplete labels. In this Section 3.1, MEM is first described generally and then it is extended to a nonlinear case by incorporating a kernel function into the model. This extended and kernelized MEM classification function can be used as an underlying classifier for ASL.","Let {tilde over (Q)}(x,y), Q(x,y) denote the empirical and the model distribution, respectively. The multi-label model can be obtained by solving the following formulation (4):",{"@attributes":{"id":"p-0095","num":"0094"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"mover":{"mi":"P","mo":"^"},"mo":["=","\u2062"],"mi":{},"mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":["max","P"]},"mo":"\u2062","mrow":{"mi":"H","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"x","mo":",","mrow":{"mi":["y","Q"],"mo":"\u2758"}}}}}}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":[{"mi":["min","P"]},{"mrow":{"mo":["\u2329","\u232a"],"mrow":{"mi":"log","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["y","x"],"mo":"\u2758"}}}}},"mi":"Q"}],"mo":"\u2062"}}}}},{"mtd":{"mrow":{"mrow":{"mrow":[{"mi":["s","t"],"mo":[".","."],"msub":{"mrow":{"mo":["\u2329","\u232a"],"msub":{"mi":["y","i"]}},"mi":"Q"}},{"msub":[{"mrow":{"mo":["\u2329","\u232a"],"msub":{"mi":["y","i"]}},"mover":{"mi":["Q","_"]}},{"mi":["\u03b7","i"]}],"mo":"+"}],"mo":["=","\u2062"],"mi":{}},"mo":",","msub":{"mrow":{"mo":["\u2329","\u232a"],"mrow":{"msub":[{"mi":["y","i"]},{"mi":["y","j"]}],"mo":"\u2062"}},"mi":"Q"}}}},{"mtd":{"mrow":{"mrow":[{"mo":["=","\u2062"],"mi":{},"mrow":{"msub":[{"mrow":{"mo":["\u2329","\u232a"],"mrow":{"msub":[{"mi":["y","i"]},{"mi":["y","j"]}],"mo":"\u2062"}},"mover":{"mi":["Q","_"]}},{"mi":["\u03b8","il"]}],"mo":"+"}},{"mn":"1","mo":["\u2264","<","\u2264"],"mi":["i","j","m"]}],"mo":","}}},{"mtd":{"mrow":{"mrow":[{"msub":{"mrow":{"mo":["\u2329","\u232a"],"mrow":{"msub":[{"mi":["y","l"]},{"mi":["x","l"]}],"mo":"\u2062"}},"mi":"Q"},"mo":["=","\u2062"],"mi":{},"mrow":{"msub":[{"mrow":{"mo":["\u2329","\u232a"],"mrow":{"msub":[{"mi":["y","l"]},{"mi":["x","l"]}],"mo":"\u2062"}},"mover":{"mi":["Q","_"]}},{"mi":["\u03d5","il"]}],"mo":"+"}},{"mn":"1","mo":["\u2264","\u2264"],"mi":["i","m"]},{"mrow":[{"mn":"1","mo":["\u2264","\u2264"],"mi":["l","d"]},{"mrow":{"munder":{"mo":"\u2211","mi":"y"},"mo":"\u2062","mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["y","x"],"mo":"\u2758"}}}},"mo":"=","mn":"1"}],"mo":";"}],"mo":[",",","]}}}]}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}}}},"br":{},"img":{"@attributes":{"id":"CUSTOM-CHARACTER-00001","he":"3.56mm","wi":"3.56mm","file":"US08086549-20111227-P00001.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},"sub":["l ","i","il ","il "]},"Formulation (4) can be solved by Lagrange Multiplier algorithms, and the obtained posterior probability is",{"@attributes":{"id":"p-0097","num":"0096"},"maths":{"@attributes":{"id":"MATH-US-00015","num":"00015"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":[{"mover":{"mi":"P","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["y","x"],"mo":"\u2758"}}},{"mfrac":{"mn":"1","mrow":{"mi":"Z","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}},"mo":"\u2062","mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msup":{"mi":["y","T"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["b","Ry","Wx"],"mo":["+","+"]}}}}}}],"mo":"="},"mo":","}}},"br":{}},{"@attributes":{"id":"p-0098","num":"0097"},"maths":{"@attributes":{"id":"MATH-US-00016","num":"00016"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"Z","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}},{"munder":{"mo":"\u2211","mi":"y"},"mo":"\u2062","mrow":{"msup":{"mi":["y","T"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["b","Ry","Wx"],"mo":["+","+"]}}}}],"mo":"="}}},"br":{}},{"@attributes":{"id":"p-0099","num":"0098"},"maths":{"@attributes":{"id":"MATH-US-00017","num":"00017"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["b","R","W"],"mo":[",",","]}}},{"msub":{"mrow":{"mo":["\u2329","\u232a"],"mrow":{"mrow":[{"mo":"-","mi":"log"},{"mover":{"mi":"P","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["y","x"],"mo":"\u2758"}}}],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},"mover":{"mi":["Q","_"]}},"mo":["+","+"],"mrow":{"mfrac":{"msub":{"mi":["\u03bb","b"]},"mrow":{"mn":"2","mo":"\u2062","mi":"n"}},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mi":"b"},"mn":["2","2"]}}}],"mo":["=","\u2062"],"mi":{}}}},{"mtd":{"mrow":{"mi":{},"mo":"\u2062","mrow":{"mrow":[{"mfrac":{"msub":{"mi":["\u03bb","R"]},"mrow":{"mn":"2","mo":"\u2062","mi":"n"}},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mi":"R"},"mi":"F","mn":"2"}},{"mfrac":{"msub":{"mi":["\u03bb","W"]},"mrow":{"mn":"2","mo":"\u2062","mi":"n"}},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mi":"W"},"mi":"F","mn":"2"}}],"mo":"+"}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"msub":{"mrow":{"mo":["\u2329","\u232a"],"mrow":{"mrow":[{"mo":"-","mrow":{"msup":{"mi":["y","T"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["b","Ry","Wx"],"mo":["+","+"]}}}},{"mi":"log","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"Z","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}],"mo":"+"}},"mover":{"mi":["Q","_"]}},"mo":"+"}}}},{"mtd":{"mrow":{"mi":{},"mo":"\u2062","mrow":{"mrow":[{"mfrac":{"msub":{"mi":["\u03bb","b"]},"mrow":{"mn":"2","mo":"\u2062","mi":"n"}},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mi":"b"},"mn":["2","2"]}},{"mfrac":{"msub":{"mi":["\u03bb","R"]},"mrow":{"mn":"2","mo":"\u2062","mi":"n"}},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mi":"R"},"mi":"F","mn":"2"}},{"mfrac":{"msub":{"mi":["\u03bb","W"]},"mrow":{"mn":"2","mo":"\u2062","mi":"n"}},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mi":"W"},"mi":"F","mn":"2"}}],"mo":["+","+"]}}}}]}},{"mrow":{"mo":["(",")"],"mn":"5"}}]}}}},"br":{},"sub":"F "},"The above-described multi-labeled MEM can be extended to a nonlinear model by following the idea of the imported vector machine. A transformation \u03c6 maps samples into a target space in which a kernel function k(x\u2032, x) gives the inner product. The multi-labeled MEM can be rewritten as",{"@attributes":{"id":"p-0101","num":"0100"},"maths":{"@attributes":{"id":"MATH-US-00018","num":"00018"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mover":{"mi":"P","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["y","x"],"mo":"\u2758"}}},{"mfrac":{"mn":"1","mrow":{"mi":"Z","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}},"mo":"\u2062","mrow":{"mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msup":{"mi":["y","T"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["b","Ry"],"mo":"+"}}},{"msup":{"mi":["y","T"]},"mo":"\u2062","mrow":{"mi":"k","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["W","x"],"mo":","}}}}],"mo":"+"}}},"mo":"."}}],"mo":"="}}},"br":{},"sub":"i"},"This vector approach is shown below by formulation (6):",{"@attributes":{"id":"p-0103","num":"0102"},"maths":{"@attributes":{"id":"MATH-US-00019","num":"00019"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"mi":"\u03d5","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"W"}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":"\u2062","mrow":{"mrow":[{"mi":"\u03b8","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","i"]}}},{"msup":{"mi":["\u03d5","T"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","i"]}}}],"mo":"\u2062"}}],"mo":["=","\u2062"],"mi":{}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"mo":"[","mrow":{"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"\u03b8","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"x","mn":"1"}}}},{"mrow":{"mi":"\u03b8","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"x","mn":"2"}}}},{"mi":"\u2026"},{"mrow":{"mrow":{"mi":"\u03b8","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","n"]}}},"mo":"]"}}]}},"mo":"\u2061","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mrow":{"msup":{"mi":["\u03d5","T"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"x","mn":"1"}}}}},{"mtd":{"mrow":{"msup":{"mi":["\u03d5","T"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"x","mn":"2"}}}}},{"mtd":{"mi":"\u22ee"}},{"mtd":{"mrow":{"msup":{"mi":["\u03d5","T"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","n"]}}}}}]}}}}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"mi":"\u0398","mo":"\u00b7","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mrow":{"msup":{"mi":["\u03d5","T"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"x","mn":"1"}}}}},{"mtd":{"mrow":{"msup":{"mi":["\u03d5","T"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"x","mn":"2"}}}}},{"mtd":{"mi":"\u22ee"}},{"mtd":{"mrow":{"msup":{"mi":["\u03d5","T"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","n"]}}}}}]}}}}}}]}},{"mrow":{"mo":["(",")"],"mn":"6"}}]}}}},"br":{},"sub":"i"},{"@attributes":{"id":"p-0104","num":"0103"},"maths":{"@attributes":{"id":"MATH-US-00020","num":"00020"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mtable":[{"mtr":[{"mtd":{"mrow":{"mrow":[{"mi":"K","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["W","x"],"mo":","}}},{"mrow":[{"mi":"\u03d5","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"W"}},{"mi":"\u03d5","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}],"mo":"\u00b7"}],"mo":["=","\u2062"],"mi":{}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"mi":"\u0398","mo":"\u00b7","mrow":{"mo":"[","mtable":{"mtr":{"mtd":[{"mrow":{"mi":"k","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"x","mn":"1"},"mo":",","mi":"x"}}}},{"mi":"\u2026"},{"msup":{"mrow":{"mrow":{"mi":"k","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["x","n"]},"mo":",","mi":"x"}}},"mo":"]"},"mi":"T"}}]}}}}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"mi":"\u0398","mo":"\u00b7","mrow":{"mi":"k","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}}}}]},{"mtr":[{"mtd":{"mrow":{"mrow":[{"mover":{"mi":"P","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["y","x"],"mo":"\u2758"}}},{"mfrac":{"mn":"1","mrow":{"mi":"Z","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}},"mo":"\u2062","mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msup":{"mi":["y","T"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["b","Ry"],"mo":"+"}}},{"msup":{"mi":["y","T"]},"mo":"\u2062","mrow":{"mi":"k","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["W","x"],"mo":","}}}}],"mo":"+"}}}}],"mo":["=","\u2062"],"mi":{}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"mfrac":{"mn":"1","mrow":{"mi":"Z","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}},"mo":"\u2062","mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msup":{"mi":["y","T"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["b","Ry"],"mo":["+","+"],"mrow":{"mi":"\u0398","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"k","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}}}}}}}}}}]}],"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}}}},{"mrow":{"mo":["(",")"],"mn":"7"}}]}}}},"br":{},"sub":["1","n","F"],"sup":["T ","2","T","T"]},{"@attributes":{"id":"p-0105","num":"0104"},"maths":{"@attributes":{"id":"MATH-US-00021","num":"00021"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["b","R","\u0398"],"mo":[",",","]}}},{"msub":{"mrow":{"mo":["\u2329","\u232a"],"mrow":{"mrow":[{"mo":"-","mi":"log"},{"mover":{"mi":"P","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["y","x"],"mo":"\u2758"}}}],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},"mover":{"mi":["Q","_"]}},"mo":["+","+","+"],"mrow":[{"mfrac":{"msub":{"mi":["\u03bb","b"]},"mrow":{"mn":"2","mo":"\u2062","mi":"n"}},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mi":"b"},"mn":["2","2"]}},{"mfrac":{"msub":{"mi":["\u03bb","R"]},"mrow":{"mn":"2","mo":"\u2062","mi":"n"}},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mi":"R"},"mi":"F","mn":"2"}},{"mfrac":{"msub":{"mi":["\u03bb","W"]},"mrow":{"mn":"2","mo":"\u2062","mi":"n"}},"mo":"\u2062","mrow":{"mi":"tr","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["\u0398","K"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msup":{"mi":["\u0398","T"]}}}}}]}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"8"}}]}}}},"br":{},"sub":["i","j","n\u00d7n "]},"An EM algorithm can be applied in statistics generally for finding maximum likelihood estimates of parameters in probabilistic models, where the model depends on unobserved latent variables. Typical EM implementations alternate between performing an expectation (E) step and a maximization (M) step. The E-step computes an expectation of the likelihood by including the latent variables as if they were observed. The M-step computes the maximum likelihood estimates of the parameters by maximizing the expected likelihood found on the E-step. The parameters found with the M-step are then used to begin another E-step, and the process is repeated.","An EM algorithm can be adapted for the classifier and implemented to handle the incomplete labeling resulting from ASL. Given that the training set constructed by ASL is partially unlabeled, the incomplete labels can be accommodated by integrating out the unlabeled part to yield the marginal distribution of the labeled part",{"@attributes":{"id":"p-0108","num":"0107"},"maths":{"@attributes":{"id":"MATH-US-00022","num":"00022"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mover":{"mi":"P","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}},"mo":"\u2758","mi":"x"}}},{"munder":{"mo":"\u2211","msub":{"mi":"y","mrow":{"mi":"U","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":"\u2062","mrow":{"mrow":{"mover":{"mi":"P","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"y","mrow":{"mi":"U","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}},"mo":",","mrow":{"msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}},"mo":"\u2758","mi":"x"}}}},"mo":"."}}],"mo":"="}}},"br":{}},{"@attributes":{"id":"p-0109","num":"0108"},"maths":{"@attributes":{"id":"MATH-US-00023","num":"00023"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["b","R","\u0398"],"mo":[",",","]}}},{"msub":{"mrow":{"mo":["\u2329","\u232a"],"mrow":{"mrow":[{"mo":"-","mi":"log"},{"munder":{"mo":"\u2211","msub":{"mi":"y","mrow":{"mi":"U","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},"mo":"\u2062","mrow":{"mover":{"mi":"P","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"y","mrow":{"mi":"U","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}},"mo":",","mrow":{"msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}},"mo":"\u2758","mi":"x"}}}}}],"mo":"\u2062"}},"mover":{"mi":["Q","_"]}},"mo":["+","+","+"],"mrow":[{"mfrac":{"msub":{"mi":["\u03bb","b"]},"mrow":{"mn":"2","mo":"\u2062","mi":"n"}},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mi":"b"},"mn":["2","2"]}},{"mfrac":{"msub":{"mi":["\u03bb","R"]},"mrow":{"mn":"2","mo":"\u2062","mi":"n"}},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mi":"R"},"mi":"F","mn":"2"}},{"mfrac":{"msub":{"mi":["\u03bb","W"]},"mrow":{"mn":"2","mo":"\u2062","mi":"n"}},"mo":"\u2062","mrow":{"mi":"tr","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["\u0398","K"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msup":{"mi":["\u0398","T"]}}}}}]}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"9"}}]}}}}},"By minimizing formulation (8), the optimal parameters for KMEM may be obtained. However, it is mathematically difficult to minimize it directly. Instead, an Expectation Maximization (EM) algorithm may be used to solve this optimization problem. An example implementation of the EM algorithm is described below in terms of an expectation step (E-Step) and a maximization step (M-Step).","E-Step; Given the current t-th step parameter estimation b,R,\u0398, the Q-function (i.e., the expectation of the Lagrangian formulation (8) under the current parameters given the labeled part) can be written as provided by formulation (10) below:",{"@attributes":{"id":"p-0112","num":"0111"},"maths":{"@attributes":{"id":"MATH-US-00024","num":"00024"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"Q","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["b","R"],"mo":[",",",",",",","],"mrow":{"mi":"\u0398","mo":"\u2758","msub":{"mi":["b","t"]}},"msub":[{"mi":["R","t"]},{"mi":["\u0398","t"]}]}}},{"msub":{"mrow":{"mo":["\u2329","\u232a"],"mrow":{"mrow":[{"mo":"-","msub":{"mi":"E","mrow":{"mrow":{"mrow":{"mrow":[{"mi":"U","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}},{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}],"mo":"\u2758"},"mo":";","msub":{"mi":["b","t"]}},"mo":[",",","],"msub":[{"mi":["R","t"]},{"mi":["\u0398","t"]}]}}},{"mover":{"mi":"P","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"y","mrow":{"mi":"U","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}},"mo":[",",",",","],"mrow":{"mrow":{"msub":{"mi":"y","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}},"mo":"\u2758","mi":"x"},"mo":";","mi":"b"},"mi":["R","\u0398"]}}}],"mo":["\u2062","\u2062","\u2062"],"mi":"log","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},"mover":{"mi":["Q","_"]}},"mo":["+","+","+"],"mrow":[{"mfrac":{"msub":{"mi":["\u03bb","b"]},"mrow":{"mn":"2","mo":"\u2062","mi":"n"}},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mi":"b"},"mn":["2","2"]}},{"mfrac":{"msub":{"mi":["\u03bb","R"]},"mrow":{"mn":"2","mo":"\u2062","mi":"n"}},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mi":"R"},"mi":"F","mn":"2"}},{"mfrac":{"msub":{"mi":["\u03bb","W"]},"mrow":{"mn":"2","mo":"\u2062","mi":"n"}},"mo":"\u2062","mrow":{"mi":"tr","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["\u0398","K"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msup":{"mi":["\u0398","T"]}}}}}]}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"10"}}]}}}},"br":{},"sub":["U(x)|(x);b",{"sub2":"t"},"R",{"sub2":"t"},"\u0398",{"sub2":"t "},"U(x)","L(x)","t","t","t"]},"M-Step: The parameters are updated by minimizing the Q-function as shown below in formulation (11):\n\n,\u0398min(,\u0398)\u2003\u2003(11)\n\nThe derivatives of the Q-function with respect to its parameters b, R, \u0398 are provided by formulation (12):\n",{"@attributes":{"id":"p-0114","num":"0113"},"maths":{"@attributes":{"id":"MATH-US-00025","num":"00025"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":{"mfrac":{"mrow":[{"mo":"\u2202","mi":"Q"},{"mo":"\u2202","msub":{"mi":["b","i"]}}]},"mo":["=","\u2062"],"mi":{},"mrow":{"msub":[{"mrow":{"mo":["\u2329","\u232a"],"msub":{"mi":["y","i"]}},"mi":"Q"},{"mrow":{"mo":["\u2329","\u232a"],"mrow":{"msub":[{"mi":"E","mrow":{"mrow":{"mrow":{"msub":{"mi":["y","i"]},"mo":"\u2758","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}},"mo":";","mi":"b"},"mo":[",",","],"mi":["R","\u0398"]}},{"mi":["y","i"]}],"mo":"\u2062"}},"mover":{"mi":["Q","_"]}}],"mo":["-","+"],"mrow":{"mfrac":{"msub":{"mi":["\u03bb","b"]},"mi":"n"},"mo":"\u2062","msub":{"mi":["b","i"]}}}},"mo":",","mfrac":{"mrow":[{"mo":"\u2202","mi":"Q"},{"mo":"\u2202","msub":{"mi":["R","ij"]}}]}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"msub":[{"mrow":{"mo":["\u2329","\u232a"],"mrow":{"msub":[{"mi":["y","i"]},{"mi":["y","j"]}],"mo":"\u2062"}},"mi":"Q"},{"mrow":{"mo":["\u2329","\u232a"],"mrow":{"msub":[{"mi":"E","mrow":{"msub":{"mi":["y","i"]},"mo":[",",",",","],"mrow":{"mrow":{"msub":{"mi":["y","j"]},"mo":"\u2758","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}},"mo":";","mi":"b"},"mi":["R","\u0398"]}},{"mi":["y","i"]},{"mi":["y","j"]}],"mo":["\u2062","\u2062"]}},"mover":{"mi":["Q","_"]}}],"mo":["-","+"],"mrow":{"mfrac":[{"msub":{"mi":["\u03bb","R"]},"mi":"n"},{"mrow":[{"mo":"\u2202","mi":"Q"},{"mo":"\u2202","msub":{"mi":["\u0398","il"]}}]}],"mo":["\u2062","\u2062"],"msub":{"mi":["R","ij"]}}}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"msub":[{"mrow":{"mo":["\u2329","\u232a"],"mrow":{"msub":{"mi":["y","i"]},"mo":"\u2062","mrow":{"mi":"k","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["x","l"]},"mo":",","mi":"x"}}}}},"mi":"Q"},{"mrow":{"mo":["\u2329","\u232a"],"mrow":{"msub":[{"mi":"E","mrow":{"msub":{"mi":["y","i"]},"mo":[",",",",","],"mrow":{"mrow":{"msub":{"mi":["y","l"]},"mo":"\u2758","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}},"mo":";","mi":"b"},"mi":["R","\u0398"]}},{"mi":["y","i"]}],"mo":["\u2062","\u2062"],"mrow":{"mi":"k","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["x","l"]},"mo":",","mi":"x"}}}}},"mover":{"mi":["Q","_"]}}],"mo":["-","+"]}}}},{"mtd":{"mrow":{"mi":{},"mo":"\u2062","mrow":{"mfrac":{"msub":{"mi":["\u03bb","W"]},"mi":"n"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"1"},"mi":"n"},"mo":"\u2062","mrow":{"msub":{"mi":["\u0398","ik"]},"mo":"\u2062","mrow":{"mi":"k","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["x","k"]},{"mi":["x","l"]}],"mo":","}}}}}}}}}]}},{"mrow":{"mo":["(",")"],"mn":"12"}}]}}}},"br":{}},"As described above, an implementation of the EM algorithm is applied to handle the partial labeling resulting from ASL embodiments. In formulation (10) for the E-step, the expectation of the complete log-likelihood function \u2212log {circumflex over (P)}(y,y|x;b,R,\u0398) is computed under the unlabeled part distribution P(U(x)|L(x);b,R,\u0398). For formulation (11) of the M-step, it computes the maximum likelihood estimates of the parameters by maximizing the expected likelihood of formulation (10) of the E-step. Formulation (12) of the M-step gives the derivatives of the objective function to be maximized, so that an optimization algorithm based on these derivatives can be adopted.",{"@attributes":{"id":"p-0116","num":"0115"},"figref":"FIG. 6","b":["600","102","112","102","202","202","602","604","112","606","608"]},"In an example embodiment, classifier  is to classify objects in accordance with multiple labels that are also associated with samples of a set of training samples  (of ). Sample-label pair selector  analyzes the set of training samples  and selects a sample-label pair  (of ) responsive to at least one error parameter. Active learning classifier trainer  submits the selected sample-label pair  to an oracle  (of ) for labeling and receives from the oracle a relevancy indication  (of ) for the selected sample-label pair . Active learning classifier trainer  also adds the relevancy indication  to the set of training samples  at the selected sample-label pair  so as to update the set of training samples . Active learning classifier trainer  updates the classifier  using the updated set of training samples.","Bayesian error bound minimizer  minimizes a Bayesian error bound when selecting the sample-label pair , such as is described herein above with particular reference to formulation (3). Mutual information maximizer  maximizes, or at least increases, the mutual information between (i) a particular label  that is associated with a particular sample  of the multiple samples  of the set of training samples  and (ii) each label  that is associated with the particular sample .","Kernelization enables a mapping from a lower dimensional space to a higher dimensional space (e.g., from 100s to 1000s of dimensions). Implementing kernelization with a classifying algorithm enables a linear approach in the kernelized space to realize a non-linear classification function. In an example embodiment, KMEM classification unit  uses a linear classification algorithm in kernelized space while realizing a corresponding non-linear classification function for classifier  to account for correlations among the multiple labels . EM functionality unit  handles the labels  of the set of training samples  that are not assigned a relevancy indication  during the classifier learning process. Example embodiments for KMEM classification unit  and EM functionality unit  are described further herein above in Section 3.2.",{"@attributes":{"id":"p-0120","num":"0119"},"figref":"FIG. 7","b":["700","702","702","1","702","714","702","702","714"],"i":"d"},"Generally, a device  may represent any computer or processing-capable device, such as a server device; a workstation or other general computing device; a data storage repository apparatus; a personal digital assistant (PDA); a mobile phone; a gaming platform; an entertainment device; a router computing node; a mesh or other network node; a wireless access point; some combination thereof; and so forth. As illustrated, device  includes one or more input\/output (I\/O) interfaces , at least one processor , and one or more media . Media  include processor-executable instructions .","In an example embodiment of device , I\/O interfaces  may include (i) a network interface for communicating across network , (ii) a display device interface for displaying information on a display screen, (iii) one or more human-device interfaces, and so forth. Examples of (i) network interfaces include a network card, a modem, one or more ports, a network communications stack, a radio, and so forth. Examples of (ii) display device interfaces include a graphics driver, a graphics card, a hardware or software driver for a screen or monitor, a screen, and so forth. Examples of (iii) human-device interfaces include those that communicate by wire or wirelessly to human-device interface equipment  (e.g., a keyboard, a remote, a mouse or other graphical pointing device, etc.) as well as a speaker, microphone, and so forth.","Generally, processor  is capable of executing, performing, and\/or otherwise effectuating processor-executable instructions, such as processor-executable instructions . Media  is comprised of one or more processor-accessible media. In other words, media  may include processor-executable instructions  that are executable by processor  to effectuate the performance of functions by device . Processor-executable instructions may be embodied as software, firmware, hardware, fixed logic circuitry, some combination thereof, and so forth.","Thus, realizations for multi-label active learning may be described in the general context of processor-executable instructions. Generally, processor-executable instructions include routines, programs, applications, coding, modules, protocols, objects, components, metadata and definitions thereof, data structures, application programming interfaces (APIs), etc. that perform and\/or enable particular tasks and\/or implement particular abstract data types. Processor-executable instructions may be located in separate storage media, executed by different processors, and\/or propagated over or extant on various transmission media.","Processor(s)  may be implemented using any applicable processing-capable technology, and one may be realized as a general purpose processor (e.g., a central processing unit (CPU), a microprocessor, a controller, etc.), a graphics processing unit (GPU), a special-purpose processor, a derivative or combination thereof, and so forth. Media  may be any available media that is included as part of and\/or accessible by device . It includes volatile and non-volatile media, removable and non-removable media, storage and transmission media (e.g., wireless or wired communication channels), hard-coded logic media, combinations thereof, and so forth. Media  is tangible media when it is embodied as a manufacture and\/or as a composition of matter. For example, media  may include an array of disks or flash memory for longer-term mass storage of processor-executable instructions , random access memory (RAM) for shorter-term storing of instructions that are currently being executed and\/or otherwise processed, link(s) on network  for transmitting communications, and so forth.","As specifically illustrated, media  comprises at least processor-executable instructions . Generally, processor-executable instructions , when executed by processor , enable device  to perform the various functions described herein. Such functions include, but are not limited to: (i) those acts that are illustrated in flow diagrams  and  (of ); (ii) those acts that are performable by the components of , , and ); (iii) those acts that are performed to implement ASL  (of ); (iv) those acts that are performed to implement the algorithms and formulations (e.g., formulations (3), (10), (11), (12), etc.) that are described herein; combinations thereof; and so forth.","The devices, acts, aspects, features, functions, procedures, components, techniques, algorithms, etc. of  are illustrated in diagrams that are divided into multiple blocks and other elements. However, the order, interconnections, interrelationships, layout, etc. in which  are described and\/or shown are not intended to be construed as a limitation, and any number of the blocks and\/or other elements can be modified, combined, rearranged, augmented, omitted, etc. in any manner to implement one or more systems, methods, devices, procedures, media, apparatuses, arrangements, etc. for multi-label active learning.","Although systems, media, devices, methods, procedures, apparatuses, mechanisms, schemes, approaches, processes, arrangements, and other example embodiments have been described in language specific to structural, logical, algorithmic, and functional features and\/or diagrams, it is to be understood that the invention defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather, the specific features and acts described above are disclosed as example forms of implementing the claimed invention."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The same numbers are used throughout the drawings to reference like and\/or corresponding aspects, features, and components.",{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 7"}]},"DETDESC":[{},{}]}
