---
title: Simulating an infrared emitter array in a video monitoring camera to construct a lookup table for depth determination
abstract: A process generates lookup tables for estimating spatial depth in a scene. The process identifies subsets of illuminators of a camera system that has a 2-dimensional array of image sensors and illuminators in fixed locations relative to the array, and partitions the image sensors into a plurality of pixels. For each pixel, and for each of m distinct depths from the respective pixel, the process simulates a virtual surface at the respective depth. For each of the subsets of illuminators, the process determines an expected light intensity at the pixel based on the respective depth. The process forms an intensity vector using the expected light intensities for each of the distinct subsets and normalizes the intensity vector. For each pixel, the process constructs a lookup table comprising the normalized vectors corresponding to the pixel. The lookup table associates each normalized vector with the depth of the corresponding simulated surface.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09235899&OS=09235899&RS=09235899
owner: GOOGLE INC.
number: 09235899
owner_city: Mountain View
owner_country: US
publication_date: 20150612
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["RELATED APPLICATIONS","TECHNICAL FIELD","BACKGROUND","SUMMARY","DESCRIPTION OF IMPLEMENTATIONS"],"p":["This application is related to U.S. Provisional Application Ser. No. 62\/021,620, filed Jul. 7, 2014, entitled \u201cActivity Recognition and Video Filtering,\u201d which is incorporated by reference herein in its entirety.","This application is related to U.S. patent application Ser. No. 14\/723,276, filed May 27, 2015, entitled \u201cMulti-Mode LED Illumination System,\u201d which is incorporated by reference herein in its entirety.","This application is related to U.S. patent application Ser. No. 14\/738,818, filed Jun. 12, 2015, entitled \u201cUsing a Scene Illuminating Infrared Emitter Array in a Video Monitoring Camera for Depth Determination\u201d, which is incorporated by reference herein in its entirety.","This application is related to U.S. patent application Ser. No. 14\/738,806, filed Jun. 12, 2015, entitled \u201cUsing Infrared Images of a Monitored Scene to Identify Windows\u201d, which is incorporated by reference herein in its entirety.","This application is related to U.S. patent application Ser. No. 14\/738,817, filed Jun. 12, 2015, entitled \u201cUsing a Depth Map of a Monitored Scene to Identify Floors, Walls, and Ceilings\u201d, which is incorporated by reference herein in its entirety.","This application is related to U.S. patent application Ser. No. 14\/738,825, filed Jun. 12, 2015, entitled \u201cUsing Depth Maps of a Scene to Identify Movement of a Video Camera\u201d, which is incorporated by reference herein in its entirety.","This application is related to U.S. patent application Ser. No. 14\/738,811, filed Jun. 12, 2015, entitled \u201cUsing a Scene Illuminating Infrared Emitter Array in a Video Monitoring Camera to Estimate the Position of the Camera\u201d, which is incorporated by reference herein in its entirety.","This application is related to U.S. patent application Ser. No. 14\/738,816, filed Jun. 12, 2015, entitled \u201cUsing a Scene Information from a Security Camera to Reduce False Security Alerts\u201d, which is incorporated by reference herein in its entirety.","The disclosed implementations relate generally to video cameras, and more specifically to using illumination emitters from a video camera to identify properties of the scene monitored by the camera or to identify properties of the camera itself.","Video surveillance cameras are used extensively. Usage of video cameras in residential environments has increased substantially, in part due to lower prices and simplicity of deployment. In many cases, surveillance cameras include infrared emitters in order to illuminate a scene when light from other sources is limited or absent.","Some video cameras enable a user to identify \u201czones\u201d within the scene that is visible to the camera. This can be useful to identify movement or changes within those zones.","Because a surveillance camera can capture a very large amount of data (e.g., running 24 hours a day, 7 days a week), some cameras enable a user to set up alerts based on specific criteria. The criteria can include movement within a scene, movement of a specific type, or movement within a certain time range.","Accordingly, there is a need for camera systems that provide simpler usage and better utilization. In various implementations, the disclosed functionality complements or replaces the functionality of existing camera systems.","In accordance with some implementations, a process generates lookup tables for use in estimating spatial depth in a visual scene. The process is performed at a server having one or more processors and memory. The memory stores one or more programs configured for execution by the one or more processors. The process identifies a plurality of distinct subsets of IR illuminators of a camera system. The camera system has a 2-dimensional array of image sensors (e.g., photodiodes) and a plurality of IR illuminators in fixed locations relative to the array of image sensors. The process partitions the image sensors into a plurality of pixels. In some implementations, each pixel comprises a single image sensor. In some implementations, each pixel comprises a plurality of image sensors, which can be 50 or more. For each pixel and for each of m distinct depths from the respective pixel, the process simulates a virtual surface at the respective depth. In some implementations, the simulated virtual surfaces are planar, but in other implementations the simulated surfaces are spherical, parabolic, or cubic. For each of the distinct subsets of IR illuminators, the process determines an expected IR light intensity at the respective pixel based on the respective depth and based on only the respective subset of IR illuminators emitting IR light. The process then forms an intensity vector using the expected IR light intensities for each of the distinct subsets, and normalizes the intensity vector. For each pixel, the process constructs a lookup table comprising the normalized vectors corresponding to the pixel. The lookup table associates each respective normalized vector with the respective depth of the respective simulated surface.","In some implementations, the expected IR light intensity at the respective pixel is based on characteristics of the IR illuminators of the camera system. In some implementations, the characteristics include lux, orientation of the IR illuminators relative to the sensor array, and\/or location of the IR illuminators relative to the sensor array.","In some implementations, the process normalizes each intensity vector by computing a respective magnitude of the intensity vector and dividing each component of the intensity vector by the respective magnitude.","In some implementations, the array of image sensors comprises more than one million image sensors. In some implementations, the array of image sensors is downsampled to a smaller number of pixels. For example, an array of image sensors with one million individual sensors may be downsampled to 10,000 pixels. The downsampling used (if any) may depend on available resources, such as memory, bandwidth, processor speed, and\/or number of processors.","In accordance with some implementations, a process creates a depth map of a scene. The process is performed at a computing device having one or more processors and memory. The memory stores one or more programs configured for execution by the one or more processors. For each of a plurality of distinct subsets of IR illuminators of a camera system, the process receives a captured IR image of a first scene taken by a 2-dimensional array of image sensors of the camera system while the respective subset of IR illuminators are emitting IR light and the IR illuminators not in the respective subset are not emitting IR light. The image sensors are partitioned into a plurality of pixels. In some implementations, each pixel comprises a single image sensor, but in other implementations, each pixel comprises a plurality of image sensors. In some implementations, the computing device is a server, and the captured images are received from a remotely located camera. In some implementations, the computing device is included in a camera, and the images are processed locally at the camera. For each pixel of the plurality of pixels, the process uses the captured IR images to form a respective vector of light intensity at the respective pixel. The process then estimates a depth in the first scene at the respective pixel by looking up the respective vector in a respective lookup table. In some implementations, the lookup table is stored at the camera system during a calibration process.","In some implementations, looking up the respective vector in the respective lookup table includes computing an inner product of the respective vector with records in the lookup table. In some implementations, the inner product is computed for each record in the lookup table. The process computes the depth in the first scene at the pixel as a depth corresponding to a record in the lookup table whose inner product with the respective vector is greatest among the computed inner products for the respective vector.","In some implementations, each respective vector for a respective pixel comprises a plurality of components, with each of the components corresponding to a respective IR light intensity for the respective pixel for a respective captured IR image. In some implementations, computing an inner product comprises computing a dot product.","In some implementations, the IR illuminators are orientated at a plurality of distinct angles relative to the array of image sensors.","In some implementations, the depth map of the first scene is created in response to detecting a trigger event. In some implementations, the trigger event is detecting movement of a first object in the first scene from a first location to a second location. In some implementations, the trigger event is a power interruption event.","In some implementations, a respective lookup table is generated during the calibration process. In some implementations, the calibration process includes simulating a virtual planar surface at a plurality of respective depths in the first scene and determining, for each pixel and each respective depth, an expected IR light intensity.","Implementations select the distinct subsets of IR illuminators in various ways. In some implementations, each of the distinct subsets of IR illuminators comprises two adjacent IR illuminators, and the distinct subsets of IR illuminators are non-overlapping.","In some implementations, each respective lookup table includes a plurality of normalized IR light intensity vectors, and each normalized light intensity vector corresponds to a respective depth in the first scene.","In some implementations, the respective lookup tables are downloaded to the camera system from a remote server during an initialization process prior to creating the depth map.","In some implementations, prior to capturing the IR images, the process switches from a first mode of the camera system to a second mode of the camera system, including deactivating the first mode and activating the second mode. In some implementations, the array of image sensors has an associated first pixel gain curve while the first mode is activated, and the array of image sensors has an associated second pixel gain curve while the second mode is activated.","In some implementations, the process receives a baseline IR image of the scene captured by the array of sensors while none of the IR illuminators are emitting IR light. Then, forming each respective vector of light intensity at a respective pixel comprises subtracting a light intensity at the pixel of the baseline IR image from the light intensity at the pixel of each of the captured IR images.","In accordance with some implementations, a process classifies objects in a scene. The process is performed at a computing device having one or more processors and memory. The memory stores one or more programs configured for execution by the one or more processors. In some implementations, the computing device is included in a camera system. In some implementations, the computing device is a server distinct from the camera system. The process receives a captured IR image of a scene taken by a 2-dimensional image sensor array of the camera system while one or more IR illuminators of the camera system are emitting IR light. In this way, the process forms an IR intensity map of the scene with a respective intensity value determined for each pixel of the IR image. The process uses the IR intensity map to identify a plurality of pixels whose corresponding intensity values are within a predefined intensity range (e.g., all intensity values between 0 and a positive finite value or all values between two positive finite values). The process then clusters the identified plurality of pixels into one or more regions that are substantially contiguous. The process determines that a first region of the one or more regions corresponds to a specific material based, at least in part, on the intensity values of the pixels in the first region, and stores information in the memory that identifies the first region.","In some implementations, each pixel of the IR image corresponds to a unique respective image sensor in the image sensor array. In some implementations, the pixels of the IR image form a partition of the image sensors in the image sensor array and at least one pixel corresponds to a plurality of image sensors in the image sensor array.","In some implementations, the camera system has a plurality of IR illuminators, and forming an IR intensity map of the scene includes receiving a respective IR sub-image of the scene for each of a plurality of distinct subsets of IR illuminators. Each IR sub-image is captured while the respective subset of IR illuminators are emitting IR light and the IR illuminators not in the respective subset are not emitting IR light. The respective intensity value for a respective pixel is the average of intensity values at the pixel in each of the sub-images.","In some implementations, clustering the identified plurality of pixels into one or more regions further comprises using a depth map that was constructed using the image sensor array.","In some implementations, clustering the identified plurality of pixels into one or more regions further comprises using an RGB image of the scene captured using the image sensor array.","In some implementations, determining that a first region of the one or more regions corresponds to a specific material comprises determining that the first region is substantially a quadrilateral. In some implementations, the first region is substantially a quadrilateral when a total absolute difference in area between the first region and the quadrilateral is less than a threshold percentage of the quadrilateral's area (e.g., 5%, 10%, or 20%).","In some implementations, the predefined intensity range includes all intensity values below a threshold value, and the specific material is glass. The process thereby determines that the first region corresponds to a window in the scene.","In some implementations, the process receives a video stream of the scene from the camera system and reviews the video stream to detect movement in the scene. The first region is excluded from movement detection. The process generates a motion alert when there is motion detected at the scene outside of the first region.","In accordance with some implementations, a process identifies large planar surfaces in scenes, such as floors, walls, and ceilings. The process is performed at a computing device having one or more processors and memory. The memory stores one or more programs configured for execution by the one or more processors. The process receives a plurality of captured IR images of a scene taken by a 2-dimensional array of image sensors of a camera system. Each IR image is captured when a distinct subset of IR illuminators of the camera system are illuminated. The process constructs a depth map of a scene using the plurality of IR images, and uses the depth map to compute a binary depth edge map for the scene. The binary depth edge map identifies which points in the depth map comprise depth discontinuities. The process identifies a plurality of contiguous components based on the binary depth edge map. The process determines that a first component of the plurality of contiguous components represents a large planar surface in the scene by fitting a plane to points in the first component, determining the orientation of the plane, and determining that the plane fitting residual error is less than a predefined threshold.","In some implementations, the nature of the large plane is determined by its orientation. When the orientation of the plane is upwards, the plane is determined to be a floor. When the orientation of the plane is downwards, the plane is determined to be a ceiling. And when the orientation of the plane is horizontal, the plane is determined to be a wall.","In some implementations, the computing device is a server distinct from the camera system. In other implementations, the computing device is included in the camera system.","In some implementations, the image sensors are partitioned into a plurality of pixels. For each pixel, the process uses the captured IR images to form a respective vector of light intensity at the respective pixel and estimates a depth in the first scene at the respective pixel using the respective vector and a respective lookup table. In this way, the process constructs the depth map.","In accordance with some implementations, a process recomputes zones for a scene. The process is performed at a computing device that has one or more processors and memory. The memory stores one or more programs configured for execution by the one or more processors. The process receives a first RGB image of a scene taken by a 2-dimensional array of image sensors of a camera system at a first time. The process also receives a first plurality of distinct IR images of the scene taken by the array of image sensors temporally proximate to the first time. Each of the IR images is taken while a different subset of IR illuminators of the camera system is emitting light. Using the first plurality of IR images, the process constructs a first depth map of the scene. The first depth map indicates a respective depth in the scene at a plurality of pixels, where each pixel corresponds to one or more of the image sensors. The process receives designation from a user of a zone within the first RGB image. The zone corresponds to a contiguous plurality of pixels. At a second time later, the process receives a second plurality of distinct IR images of the scene taken by the array of image sensors. Each of the IR images in the second plurality is taken while a different subset of IR illuminators of the camera system is emitting light. Using the second plurality of IR images, the process constructs a second depth map of the scene. The process then determines physical movement of the camera system based on the first and second depth maps. Based on the determined physical movement, the process translates the zone in the first RGB image into an adjusted zone.","In some instances, the determined physical movement is an angular rotation. In some instances, the determined physical movement is a lateral displacement. In some instances, the determined physical movement includes both an angular rotation and a lateral displacement. Lateral displacements are commonly horizontal, but they can be vertical as well. As used herein, a lateral displacement is any movement in which the camera continues to point in the same direction. This includes any combination of left\/right, up\/down, and\/or forward\/backward.","In some implementations, determining the physical movement of the camera system includes identifying a plurality of points in the first depth map and a corresponding plurality of points in the second depth map and the process determines a respective displacement for each of the points between the first and second depth maps.","In some instances, the zone is a first quadrilateral. In some instances, the adjusted zone is a second quadrilateral, and a first edge of the first quadrilateral has a length that is different from a corresponding second edge of the second quadrilateral.","In some implementations, the process creates the first depth map of the scene by partitioning the image sensors into a plurality of pixels. For each pixel, the process forms a respective vector of the received IR images at the respective pixel and estimates a depth in the scene at the respective pixel by looking up the respective vector in a respective lookup table.","In some implementations, the computing device is a server distinct from the camera system. In other implementations, the computing device is included in the camera system.","In some implementations, the process receives a second RGB image of the scene taken by the image sensor array of the camera system temporally proximate to the second time and correlates the adjusted zone to a set of pixels from the second RGB image.","In some implementations, the process determines the physical movement of the camera system using point clouds. The process forms a first point cloud using a first plurality of points from the first depth map and forms a second point cloud using a second plurality of points from the second depth map. The process then computes a minimal transformation that aligns the first point cloud with the second point cloud. This process is referred to as \u201cregistration.\u201d","In accordance with some implementations, a process estimates the height and tilt angle of a camera system. The camera system has a 2-dimensional array of image sensors and a plurality of IR illuminators in fixed locations relative to the array of image sensors. The process is performed at a computing device having one or more processors and memory. The memory stores one or more programs configured for execution by the one or more processors. In some implementations, the computing device is included in the camera system. In some implementations, the computing device is a server distinct from the camera system. The process identifies a plurality of distinct subsets of the IR illuminators. In some implementations, each of the distinct subsets of the IR illuminators comprises two adjacent IR illuminators, and the distinct subsets of the IR illuminators are non-overlapping. In some implementations, one or more of the subsets of IR illuminators comprises a single IR illuminator. The process partitions the image sensors into a plurality of pixels. In some implementations, each pixel corresponds to a single image sensor. In some implementations, some of the pixels correspond to multiple image sensors (e.g., by downsampling).","In accordance with some implementations, for each of a plurality of heights and tilt angles, the process constructs a dictionary entry that corresponds to the camera system having the respective height and tilt angle above a floor. The respective dictionary entry includes respective IR light intensity values for pixels in images corresponding to activating individually each of the distinct subsets of the IR illuminators.","In some implementations, the constructed dictionary entries are based on simulating the camera, the floor, and the images, and computing expected IR light intensity values for pixels in the simulated images. In some implementations, each expected IR light intensity value is based on characteristics of the IR illuminators, including one or more characteristics selected from the group consisting of lux, orientation of the IR illuminators relative to the array of image sensors, and location of the IR illuminators relative to the array of image sensors. In some implementations, a respective dictionary entry for a respective height and respective tilt angle is based on measuring IR light intensity values of actual images captured by the camera having the respective height and respective tilt angle with respect to an actual floor.","In accordance with some implementations, for each of the plurality of distinct subsets of the IR illuminators, the process receives a captured IR image of a scene taken by the array of image sensors while the respective subset of the IR illuminators are emitting IR light and the IR illuminators not in the respective subset are not emitting IR light. Using at least one of the captured IR images, the process identifies a floor region corresponding to a floor in the scene. In some implementations, identifying the floor region includes constructing a depth map of the scene using the captured IR images, identifying a region bounded by depth discontinuities, and determining that the region is substantially planar and facing upwards.","In accordance with some implementations, the process forms a vector (sometimes referred to as a feature vector) including pixels from the captured IR images in the identified floor region and estimates the camera height and camera tilt angle relative to the floor by comparing the feature vector to the dictionary entries.","In some implementations, the respective expected IR light intensity is based on characteristics of the IR illuminators. In some implementations, these characteristics include one or more of: illuminator lux; orientation of the IR illuminators relative to the array of image sensors; and location of the IR illuminators relative to the array of image sensors.","In some implementations, constructing a dictionary entry includes normalizing the dictionary entry. In some implementations, normalizing a dictionary entry includes determining a respective total magnitude of the light intensity features in the dictionary entry and dividing each component of the dictionary entry by the respective total magnitude. In some implementations, the dictionary entries are downloaded to the camera system from the computing device during an initialization process.","In some implementations, the process receives a baseline IR image of the scene captured by the array of image sensors while none of the IR illuminators are emitting IR light and subtracts the light intensity at each pixel of the baseline IR image from the light intensity at the corresponding pixel of each of the other captured IR images.","In some implementations, estimating the camera height and camera tilt angle relative to the floor includes computing a respective distance between the feature vector and respective dictionary entries. The process selects a first dictionary entry whose corresponding computed distance is less than the other computed distances and estimates the camera height and tilt angle to be the height and tilt angle associated with the first dictionary entry. In some implementations, computing a respective distance between the feature vector and respective dictionary entries comprises computing a Euclidean distance that uses only vector components corresponding to pixels in the identified floor region. In some implementations, the process normalizes the feature vector and the dictionary entries prior to computing the distances.","In accordance with some implementations, a process reduces false positive security alerts. The process is performed at a computing device having one or more processors, and memory storing one or more programs configured for execution by the one or more processors. In some implementations, the computing device is a server distinct from a video camera. In some implementations, the computing device is included in the video camera. The process computes a depth map for a scene monitored by a video camera using a plurality of IR images captured by the video camera and uses the depth map to identify a first region within the scene having historically above average false positive detected motion events. The process monitors a video stream provided by the video camera to identify motion events. The monitored area excludes the first region. The process generates a motion alert when there is detected motion in the scene outside of the first region and the detected motion satisfies threshold criteria. In some implementations, satisfying the threshold criteria includes detecting movement of an object in the scene, and the detected movement exceeds a predefined distance within a predefined period of time. In some implementations, satisfying the threshold criteria includes detecting movement for an object that exceeds a predefined size. In some implementations, satisfying the threshold criteria includes detecting simultaneous movement of two or more objects in the scene.","In some implementations, the video camera has a plurality of IR illuminators and each of the plurality of IR images captured by the video camera is taken when a different subset of the illuminators is emitting light.","In some instances, the first region is identified as a ceiling. In some implementations, identifying the first region as a ceiling includes using the depth map to compute a binary depth edge map for the scene. The binary depth edge map identifies which points in the depth map comprise depth discontinuities. In some implementations, identifying the first region as a ceiling also includes identifying a contiguous component based on the binary depth edge map. In some implementations, identifying the first region as a ceiling also includes fitting a plane to points in the contiguous component, determining that the plane fitting residual error is less than a predefined threshold, and determining that the plane is oriented downward.","In some instances, the first region is identified as a window. In some implementations, identifying the first region as a window includes identifying the first region as a region of low light intensity within a captured IR image of the scene, fitting the first region with a quadrilateral, and determining that the absolute difference between the first region and the quadrilateral is less than a threshold percentage of the area of the quadrilateral.","In some instances, the first region is identified as a television.","In accordance with some implementations, process for generating depth maps is performed by a camera having a plurality of illuminators, a lens assembly, an image sensing element, a processor, and memory. The illuminators are configured to operate in a first mode to provide illumination using all of the illuminators, the lens assembly is configured to focus incident light on the image sensing element, the memory is configured to store image data from the image sensing element, and the processor is configured to execute programs to control operation of the camera. The process reconfigures the plurality of illuminators to operate in a second mode, where each of a plurality of subsets of the plurality of illuminators provides illumination separately from other subsets of the plurality of illuminators. The process sequentially activates each of the subsets of the illuminators to illuminate a scene and receives reflected illumination from the illuminated scene incident on the lens assembly and focused onto the image sensing element. The process measures light intensity values of the received reflected illumination at the image sensing element and stores to the memory the measured light intensity values associated with activation of each of the subsets.","In some implementations, each of the subsets of illuminators is configured at a different angle relative to the image sensing element.","In some implementations, each of the subsets of illuminators highlights a different portion of the scene.","In some implementations, the process transmits the stored light intensity values to a depth mapping module configured to estimate spatial depths of objects in the scene based on the stored light intensity values, predetermined illumination specifications of the illuminators, and response specifications of the image sensors.","In some implementations, the illuminators are IR illuminators.","In some implementations, the illuminators comprise 8 IR illuminators and each of the subsets of the illuminators comprises 2 adjacent IR illuminators.","In some implementations, the image sensing element is a 2-dimensional array of image sensors.","In some implementations, differences in the stored light intensity values associated with activation of each of the subsets for a respective image sensor correlate with spatial depth of an object in the scene from which reflected light was received at the respective image sensor.","In some implementations, the process captures a baseline image while none of the illuminators are emitting light. The captured baseline image measures ambient light intensity of the scene at each of the image sensors. The process stores the captured baseline image to the memory and for each image sensor, the process subtracts the baseline intensity value from the stored intensity values for the respective image sensor to correct the stored intensity values for ambient light at the scene.","In some implementations, the image sensors are partitioned into a plurality of pixels and for each pixel of the plurality of pixels the process using the captured IR images to form a respective vector of light intensity at the respective pixel. For each pixel, the process also estimates a depth in the first scene at the respective pixel by looking up the respective vector in a respective lookup table. In some implementations, looking up the respective vector in the respective lookup table includes computing an inner product of the respective vector with records in the lookup table and determining the depth in the first scene at the pixel as a depth corresponding to a record in the lookup table whose inner product with the respective vector is greatest among the computed inner products for the respective vector. In some implementations, computing an inner product of the respective vector with records in the lookup table includes computing an inner product of the respective vector and the respective record for each record in the respective lookup table. In some implementations, the respective vector for a respective pixel has a plurality of components, each of the components corresponds to a respective IR light intensity for the respective pixel for a respective captured IR image, and computing an inner product comprises computing a dot product.","In some implementations, each respective lookup table includes a plurality of normalized IR light intensity vectors, each normalized light intensity vector corresponds to a respective depth in the first scene.","In some implementations, the respective lookup table is downloaded to the camera system from a remote server during an initialization process.","In accordance with some implementations, a computing device has one or more processors, memory, and one or more programs stored in the memory. The programs are configured for execution by the one or more processors. The one or more programs including instructions for performing any of the processes described herein. In some implementations, the computing device is a server, which is distinct from a camera system. In other implementations, the computing device includes a camera.","In accordance with some implementations, a non-transitory computer readable storage medium stores one or more programs configured for execution by a computing device having one or more processors and memory. The one or more programs include instructions for performing any of the processes described herein. In some implementations, the computing device is a server, which is distinct from a camera system. In other implementations, the computing device includes a camera.","Thus, computing devices, server systems, and camera systems are provided with more efficient methods for utilizing IR emitters and a sensor array to classify objects in a scene or simplify creation of alerts. These disclosed camera systems thereby increase the effectiveness, efficiency, and user satisfaction with such systems. Such methods may complement or replace conventional methods.","Like reference numerals refer to corresponding parts throughout the several views of the drawings.","Security cameras typically include illuminators so that video capture is possible even in low light conditions or in complete darkness. Many such cameras use infrared (IR) illuminators, which allow video capture without illuminating a scene with visible light. Typically, when illumination is needed, all of the illuminators are turned on.","Disclosed implementations utilize existing illuminators in different ways so that the camera can provide more information about a scene. One step in some implementations is to control the illuminators individually or in small groups rather than turning them all on or off together. Because the illuminators are in different locations with respect to the image sensor array, captured images are slightly different depending on which illuminators are on, as illustrated below in .","As described below, some implementations build a depth map of a scene using the differences in captured images when different illuminators are on. A depth map estimates the distance between the image sensor array of the camera and the nearest object for each pixel in the field of vision of the camera. In some implementations, the depth map is implemented as an m\u00d7n matrix of depths, where m\u00d7n is the arrangement of pixels corresponding to image sensor array.","In some implementations, there is a one-to-one correspondence between pixels and individual image sensors in the array, but in many implementations the images are downsampled to create a more manageable set of pixels (e.g., 10,000 pixels instead of 1,000,000 pixels).","A depth map can be used in various ways to determine information about a scene. In some implementations, the depth map is used to help identify floors, walls, and ceilings. In some implementations, the depth map helps to identify when a camera has moved slightly, enabling automatic zone correction for previously defined zones in the scene. In some implementations, the depth map helps to identify the position of the camera (e.g., height above the floor and angle). These features provide useful information, and also allow for more accurate alerts. For example, if a region is identified as a ceiling, perceiving \u201cmovement\u201d in that region is likely to be light reflections instead of an intruder. As another example, automatic zone correction can ensure that the proper region is monitored (e.g., a doorway) even if the zone is in a different location relative to a new camera position (e.g., because the camera was bumped).","Some implementations also enable detection of windows using characteristics of windows that are different from other objects. For example, whereas light incident on most objects scatters in all directions, light incident on a window either passes through the window or reflects off like a mirror. Identifying windows can be useful in various ways, including the prevention of false alerts. For example, movement of leaves on a tree outside of a window does not constitute an intruder inside a monitored room with the window.","These features may be implemented for an independent camera, but in some implementations, the camera is part of a smart home environment , as described below in .","Video-based surveillance and security monitoring of a premises generates a continuous video feed that may last hours, days, and even months. Although motion-based recording triggers can help trim down the amount of video data that is actually recorded, there are a number of drawbacks associated with video recording triggers based on simple motion detection in the live video feed. For example, when motion detection is used as a trigger for recording a video segment, the threshold of motion detection must be set appropriately for the scene of the video; otherwise, the recorded video may include many video segments containing trivial movements (e.g., lighting change, leaves moving in the wind, shifting of shadows due to changes in sunlight exposure, etc.) that are of no significance to a reviewer. On the other hand, if the motion detection threshold is set too high, video data on important movements that are too small to trigger the recording may be irreversibly lost. Furthermore, at a location with many routine movements (e.g., cars passing through in front of a window) or constant movements (e.g., a scene with a running fountain, a river, etc.), recording triggers based on motion detection are rendered ineffective, because motion detection can no longer accurately select out portions of the live video feed that are of special significance. As a result, a human reviewer has to sift through a large amount of recorded video data to identify a small number of motion events after rejecting a large number of routine movements, trivial movements, and movements that are of no interest for a present purpose.","Due to at least the challenges described above, it is desirable to have a method that maintains a continuous recording of a live video feed such that irreversible loss of video data is avoided and, at the same time, augments simple motion detection with false positive suppression and motion event categorization. The false positive suppression techniques help to downgrade motion events associated with trivial movements and constant movements. The motion event categorization techniques help to create category-based filters for selecting only the types of motion events that are of interest for a present purpose. As a result, the reviewing burden on the reviewer may be reduced. In addition, as the present purpose of the reviewer changes in the future, the reviewer can simply choose to review other types of motion events by selecting the appropriate motion categories as event filters.","In addition, in some implementations, event categories can also be used as filters for real-time notifications and alerts. For example, when a new motion event is detected in a live video feed, the new motion event is immediately categorized, and if the event category of the newly detected mention event is a category of interest selected by a reviewer, a real-time notification or alert can be sent to the reviewer regarding the newly detected motion event. In addition, if the new event is detected in the live video feed as the reviewer is viewing a timeline of the video feed, the event indicator and the notification of the new event will have an appearance or display characteristic associated with the event category.","Furthermore, the types of motion events occurring at different locations and settings can vary greatly, and there are many event categories for all motion events collected at the video server system (e.g., the video server system ). Therefore, it may be undesirable to have a set of fixed event categories from the outset to categorize motion events detected in all video feeds from all camera locations for all users. In some implementations, the motion event categories for the video stream from each camera are gradually established through machine learning, and are thus tailored to the particular setting and use of the video camera.","In addition, in some implementations, as new event categories are gradually discovered based on clustering of past motion events, the event indicators for the past events in a newly discovered event category are refreshed to reflect the newly discovered event category. In some implementations, a clustering algorithm automatically phases out old, inactive, and\/or sparse categories when categorizing motion events. As a camera changes location, event categories that are no longer active are gradually retired without manual input to keep the motion event categorization model current. In some implementations, user input to edit the assignment of past motion events into respective event categories is also taken into account for future event category assignment and new category creation.","In some circumstances, there are multiple objects moving simultaneously within the scene of a video feed. In some implementations, the motion track associated with each moving object corresponds to a respective motion event candidate, such that the movement of the different objects in the same scene may be assigned to different motion event categories.","In general, motion events may occur in different regions of a scene at different times. Out of all the motion events detected within a scene of a video stream over time, a reviewer may only be interested in motion events that occur within or enter a particular zone of interest in the scene. In addition, the zones of interest may not be known to the reviewer and\/or the video server system until long after one or more motion events of interest have occurred within the zones of interest. For example, a parent may not be interested in activities centered around a cookie jar until after some cookies have mysteriously disappeared. Furthermore, the zones of interest in the scene of a video feed can vary for a reviewer over time depending on the present purpose of the reviewer. For example, the parent may be interested in seeing all activities that occurred around the cookie jar one day when some cookies are missing, and the parent may be interested in seeing all activities that occurred around a mailbox the next day when some expected mail is missing. Accordingly, in some implementations, the techniques disclosed herein allow a reviewer to define and create one or more zones of interest within a static scene of a video feed, and then use the created zones of interest to retroactively identify all past motion events (or all motion events within a particular past time window) that have touched or entered the zones of interest. In some implementations, the identified motion events are presented to the user in a timeline or in a list. In some implementations, real-time alerts for any new motion events that touch or enter the zones of interest are sent to the reviewer. The ability to quickly identify and retrieve past motion events that are associated with a newly created zone of interest addresses the drawbacks of conventional zone monitoring techniques. Conventionally, the zones of interest must be defined first based on a certain degree of guessing and anticipation that may later prove to be inadequate or wrong. Also, in conventional systems, only future events (as opposed to both past and future events) within the zones of interest can be identified.","In some implementations, when detecting new motion events that have touched or entered some zone(s) of interest, the event detection is based on the motion information collected from the entire scene, rather than just within the zone(s) of interest. In particular, aspects of motion detection, motion object definition, motion track identification, false positive suppression, and event categorization are all based on image information collected from the entire scene, rather than just within each zone of interest. As a result, context around the zones of interest is taken into account when monitoring events within the zones of interest. Thus, the accuracy of event detection and categorization may be improved as compared to conventional zone monitoring techniques that perform all calculations with image data collected only within the zones of interest.",{"@attributes":{"id":"p-0120","num":"0119"},"figref":["FIGS. 1-4","FIGS. 5-8"]},"Reference will now be made in detail to implementations, examples of which are illustrated in the accompanying drawings. In the following detailed description, numerous specific details are set forth in order to provide a thorough understanding of the various described implementations. However, it will be apparent to one of ordinary skill in the art that the various described implementations may be practiced without these specific details. In other instances, well-known methods, procedures, components, circuits, and networks have not been described in detail so as not to unnecessarily obscure aspects of the implementations.",{"@attributes":{"id":"p-0122","num":"0121"},"figref":"FIG. 1","b":["100","150","100","150","150","150","114","116","150"]},"The depicted structure  includes a plurality of rooms , separated at least partly from each other via walls . The walls  may include interior walls or exterior walls. Each room may further include a floor  and a ceiling . Devices may be mounted on, integrated with, and\/or supported by a wall , a floor , or a ceiling .","In some implementations, the smart home environment  includes a plurality of devices, including intelligent, multi-sensing, network-connected devices, that integrate seamlessly with each other in a smart home network  and\/or with a central server or a cloud-computing system to provide a variety of useful smart home functions. The smart home environment  may include one or more intelligent, multi-sensing, network-connected thermostats  (\u201csmart thermostats\u201d), one or more intelligent, network-connected, multi-sensing hazard detection units  (\u201csmart hazard detectors\u201d), and one or more intelligent, multi-sensing, network-connected entryway interface devices  (\u201csmart doorbells\u201d). In some implementations, the smart thermostat  detects ambient climate characteristics (e.g., temperature and\/or humidity) and controls a HVAC system  accordingly. The smart hazard detector  may detect the presence of a hazardous substance or a substance indicative of a hazardous substance (e.g., smoke, fire, and\/or carbon monoxide). The smart doorbell  may detect a person's approach to or departure from a location (e.g., an outer door), control doorbell functionality, announce a person's approach or departure via audio or visual means, and\/or control settings on a security system (e.g., to activate or deactivate the security system when occupants go and come).","In some implementations, the smart home environment  includes one or more intelligent, multi-sensing, network-connected wall switches  (\u201csmart wall switches\u201d), along with one or more intelligent, multi-sensing, network-connected wall plug interfaces  (\u201csmart wall plugs\u201d). The smart wall switches  may detect ambient lighting conditions, detect room-occupancy states, and control a power and\/or dim state of one or more lights. In some instances, smart wall switches  may also control a power state or speed of a fan, such as a ceiling fan. The smart wall plugs  may detect occupancy of a room or enclosure and control supply of power to one or more wall plugs (e.g., such that power is not supplied to the plug if nobody is at home).","In some implementations, the smart home environment  includes a plurality of intelligent, multi-sensing, network-connected appliances  (\u201csmart appliances\u201d), such as refrigerators, stoves, ovens, televisions, washers, dryers, lights, stereos, intercom systems, garage-door openers, floor fans, ceiling fans, wall air conditioners, pool heaters, irrigation systems, security systems, space heaters, window AC units, motorized duct vents, and so forth. In some implementations, when plugged in, an appliance may announce itself to the smart home network, such as by indicating what type of appliance it is, and it may automatically integrate with the controls of the smart home. Such communication by the appliance to the smart home may be facilitated by either a wired or wireless communication protocol. The smart home may also include a variety of non-communicating legacy appliances , such as old conventional washer\/dryers, refrigerators, and the like, which may be controlled by smart wall plugs . The smart home environment  may further include a variety of partially communicating legacy appliances , such as infrared (\u201cIR\u201d) controlled wall air conditioners or other IR-controlled devices, which may be controlled by IR signals provided by the smart hazard detectors  or the smart wall switches .","In some implementations, the smart home environment  includes one or more network-connected cameras  that are configured to provide video monitoring and security in the smart home environment .","The smart home environment  may also include communication with devices outside of the physical home but within a proximate geographical range of the home. For example, the smart home environment  may include a pool heater monitor  that communicates a current pool temperature to other devices within the smart home environment  and\/or receives commands for controlling the pool temperature. Similarly, the smart home environment  may include an irrigation monitor  that communicates information regarding irrigation systems within the smart home environment  and\/or receives control information for controlling such irrigation systems.","By virtue of network connectivity, one or more of the smart home devices may further allow a user to interact with the device even if the user is not proximate to the device. For example, a user may communicate with a device using a computer (e.g., a desktop computer, laptop computer, or tablet) or other portable electronic device (e.g., a smartphone) . A webpage or application may be configured to receive communications from the user and control the device based on the communications and\/or to present information about the device's operation to the user. For example, the user may view a current set point temperature for a device and adjust it using a computer. The user may be in the structure during this remote communication or outside the structure.","As discussed above, users may control the smart thermostat and other smart devices in the smart home environment  using a network-connected computer or portable electronic device . In some examples, some or all of the occupants (e.g., individuals who live in the home) may register their devices  with the smart home environment . Such registration may be made at a central server to authenticate the occupant and\/or the device as being associated with the home and to give permission to the occupant to use the device to control the smart devices in the home. Occupants may use their registered devices  to remotely control the smart devices of the home, such as when an occupant is at work or on vacation. The occupant may also use a registered device to control the smart devices when the occupant is actually located inside the home, such as when the occupant is sitting on a couch inside the home. It should be appreciated that instead of or in addition to registering the devices , the smart home environment  may make inferences about which individuals live in the home and are therefore occupants and which devices  are associated with those individuals. As such, the smart home environment may \u201clearn\u201d who is an occupant and permit the devices  associated with those individuals to control the smart devices of the home.","In some implementations, in addition to containing processing and sensing capabilities, the devices , , , , , , , , and\/or  (\u201cthe smart devices\u201d) are capable of data communications and information sharing with other smart devices, a central server or cloud-computing system, and\/or other devices that are network-connected. The required data communications may be carried out using any of a variety of custom or standard wireless protocols (IEEE 802.15.4, Wi-Fi, ZigBee, 6LoWPAN, Thread, Z-Wave, Bluetooth Smart, ISA100.11a, WirelessHART, MiWi, etc.) and\/or any of a variety of custom or standard wired protocols (CAT6 Ethernet, HomePlug, etc.), or any other suitable communication protocol.","In some implementations, the smart devices serve as wireless or wired repeaters. For example, a first one of the smart devices communicates with a second one of the smart devices via a wireless router. The smart devices may further communicate with each other via a connection to one or more networks  such as the Internet. Through the one or more networks , the smart devices may communicate with a smart home provider server system  (also called a central server system and\/or a cloud-computing system herein). In some implementations, the smart home provider server system  may include multiple server systems, each dedicated to data processing associated with a respective subset of the smart devices (e.g., a video server system may be dedicated to data processing associated with camera(s) ). The smart home provider server system  may be associated with a manufacturer, support entity, or service provider associated with the smart device. In some implementations, a user is able to contact customer support using a smart device itself rather than needing to use other communication means, such as a telephone or Internet-connected computer. In some implementations, software updates are automatically sent from the smart home provider server system  to smart devices (e.g., when available, when purchased, or at routine intervals).",{"@attributes":{"id":"p-0133","num":"0132"},"figref":"FIG. 2","b":["200","202","204","100","102","104","106","108","110","112","114","116","118","202","204","202","204","166","164","204","202","100","204","204","1","204","9","204","100","154","100","164"]},"In some implementations, some low-power nodes are incapable of bidirectional communication. These low-power nodes send messages, but they are unable to \u201clisten\u201d. Thus, other devices in the smart home environment , such as the spokesman nodes, cannot send information to these low-power nodes.","As described, the spokesman nodes and some of the low-powered nodes are capable of \u201clistening.\u201d Accordingly, users, other devices, and\/or the central server or cloud-computing system  may communicate control commands to the low-powered nodes. For example, a user may use the portable electronic device  (e.g., a smartphone) to send commands over the Internet to the central server or cloud-computing system , which then relays the commands to one or more spokesman nodes in the smart home network . The spokesman nodes drop down to a low-power protocol to communicate the commands to the low-power nodes throughout the smart home network , as well as to other spokesman nodes that did not receive the commands directly from the central server or cloud-computing system .","In some implementations, a smart nightlight  is a low-power node. In addition to housing a light source, the smart nightlight  houses an occupancy sensor, such as an ultrasonic or passive IR sensor, and an ambient light sensor, such as a photo resistor or a single-pixel sensor that measures light in the room. In some implementations, the smart nightlight  is configured to activate the light source when its ambient light sensor detects that the room is dark and when its occupancy sensor detects that someone is in the room. In other implementations, the smart nightlight  is simply configured to activate the light source when its ambient light sensor detects that the room is dark. Further, in some implementations, the smart nightlight  includes a low-power wireless communication chip (e.g., a ZigBee chip) that regularly sends out messages regarding the occupancy of the room and the amount of light in the room, including instantaneous messages coincident with the occupancy sensor detecting the presence of a person in the room. As mentioned above, these messages may be sent wirelessly, using the mesh network, from node to node (i.e., smart device to smart device) within the smart home network  as well as over the one or more networks  to the central server or cloud-computing system .","Other examples of low-power nodes include battery-operated versions of the smart hazard detectors . These smart hazard detectors  are often located in an area without access to constant and reliable power and may include any number and type of sensors, such as smoke\/fire\/heat sensors, carbon monoxide\/dioxide sensors, occupancy\/motion sensors, ambient light sensors, temperature sensors, humidity sensors, and the like. Furthermore, the smart hazard detectors  may send messages that correspond to each of the respective sensors to the other devices and\/or the central server or cloud-computing system , such as by using the mesh network as described above.","Examples of spokesman nodes include smart doorbells , smart thermostats , smart wall switches , and smart wall plugs . These devices , , , and  are often located near and connected to a reliable power source, and therefore may include more power-consuming components, such as one or more communication chips capable of bidirectional communication in a variety of protocols.","In some implementations, the smart home environment  includes service robots  that are configured to carry out, in an autonomous manner, any of a variety of household tasks.",{"@attributes":{"id":"p-0140","num":"0139"},"figref":["FIG. 3","FIG. 1","FIG. 1","FIGS. 2-4"],"b":["300","100","300","164","102","104","106","108","110","112","114","116","118","164","162","160"]},"In some implementations, the devices and services platform  communicates with and collects data from the smart devices of the smart home environment . In addition, in some implementations, the devices and services platform  communicates with and collects data from a plurality of smart home environments across the world. For example, the smart home provider server system  collects home data  from the devices of one or more smart home environments, where the devices may routinely transmit home data or may transmit home data in specific instances (e.g., when a device queries the home data ). Example collected home data  includes, without limitation, power consumption data, occupancy data, HVAC settings and usage data, carbon monoxide levels data, carbon dioxide levels data, volatile organic compounds levels data, sleeping schedule data, cooking schedule data, inside and outside temperature and humidity data, television viewership data, inside and outside noise level data, pressure data, video data, etc.","In some implementations, the smart home provider server system  provides one or more services  to smart homes. Example services  include, without limitation, software updates, customer support, sensor data collection\/logging, remote access, remote or distributed control, and\/or use suggestions (e.g., based on the collected home data ) to improve performance, reduce utility cost, increase safety, etc. In some implementations, data associated with the services  is stored at the smart home provider server system , and the smart home provider server system  retrieves and transmits the data at appropriate times (e.g., at regular intervals, upon receiving a request from a user, etc.).","In some implementations, the extensible devices and the services platform  includes a processing engine , which may be concentrated at a single server or distributed among several different computing entities. In some implementations, the processing engine  includes engines configured to receive data from the devices of smart home environments (e.g., via the Internet and\/or a network interface), to index the data, to analyze the data and\/or to generate statistics based on the analysis or as part of the analysis. In some implementations, the analyzed data is stored as derived home data .","Results of the analysis or statistics may thereafter be transmitted back to the device that provided home data used to derive the results, to other devices, to a server providing a webpage to a user of the device, or to other non-smart device entities. In some implementations, use statistics, use statistics relative to use of other devices, use patterns, and\/or statistics summarizing sensor readings are generated by the processing engine  and transmitted. The results or statistics may be provided via the one or more networks . In this manner, the processing engine  may be configured and programmed to derive a variety of useful information from the home data . A single server may include one or more processing engines.","The derived home data  may be used at different granularities for a variety of useful purposes, ranging from explicit programmed control of the devices on a per-home, per-neighborhood, or per-region basis (for example, demand-response programs for electrical utilities), to the generation of inferential abstractions that may assist on a per-home basis (for example, an inference may be drawn that the homeowner has left for vacation and so security detection equipment may be put on heightened sensitivity), to the generation of statistics and associated inferential abstractions that may be used for government or charitable purposes. For example, processing engine  may generate statistics about device usage across a population of devices and send the statistics to device users, service providers or other entities (e.g., entities that have requested the statistics and\/or entities that have provided monetary compensation for the statistics).","In some implementations, to encourage innovation and research and to increase products and services available to users, the devices and services platform  exposes a range of application programming interfaces (APIs)  to third parties, such as charities , governmental entities  (e.g., the Food and Drug Administration or the Environmental Protection Agency), academic institutions  (e.g., university researchers), businesses  (e.g., providing device warranties or service to related equipment, targeting advertisements based on home data), utility companies , and other third parties. The APIs  are coupled to and permit third-party systems to communicate with the smart home provider server system , including the services , the processing engine , the home data , and the derived home data . In some implementations, the APIs  allow applications executed by the third parties to initiate specific data processing tasks that are executed by the smart home provider server system , as well as to receive dynamic updates to the home data  and the derived home data .","For example, third parties may develop programs and\/or applications, such as web applications or mobile applications, that integrate with the smart home provider server system  to provide services and information to users. Such programs and applications may be, for example, designed to help users reduce energy consumption, to preemptively service faulty equipment, to prepare for high service demands, to track past service performance, etc., and\/or to perform other beneficial functions or tasks.",{"@attributes":{"id":"p-0148","num":"0147"},"figref":["FIG. 4","FIG. 3"],"b":["400","300","306","402","404","406","408","300","300"]},{"@attributes":{"id":"p-0149","num":"0148"},"figref":"FIG. 4","b":["306","410","306","410","306","410","306","410","102"],"i":["a ","b ","c "]},"In some implementations, the processing engine  includes a challenges\/rules\/compliance\/rewards paradigm that informs a user of challenges, competitions, rules, compliance regulations and\/or rewards and\/or that uses operation data to determine whether a challenge has been met, a rule or regulation has been complied with and\/or a reward has been earned. The challenges, rules, and\/or regulations may relate to efforts to conserve energy, to live safely (e.g., reducing exposure to toxins or carcinogens), to conserve money and\/or equipment life, to improve health, etc. For example, one challenge may involve participants turning down their thermostat by one degree for one week. Those participants that successfully complete the challenge are rewarded, such as with coupons, virtual currency, status, etc. Regarding compliance, an example involves a rental-property owner making a rule that no renters are permitted to access certain owner's rooms. The devices in the room having occupancy sensors may send updates to the owner when the room is accessed.","In some implementations, the processing engine  integrates or otherwise uses extrinsic information  from extrinsic sources to improve the functioning of one or more processing paradigms. The extrinsic information  may be used to interpret data received from a device, to determine a characteristic of the environment near the device (e.g., outside a structure that the device is enclosed in), to determine services or products available to the user, to identify a social network or social-network information, to determine contact information of entities (e.g., public-service entities such as an emergency-response team, the police or a hospital) near the device, to identify statistical or environmental conditions, trends or other information associated with a home or neighborhood, and so forth.",{"@attributes":{"id":"p-0152","num":"0151"},"figref":["FIG. 5","FIG. 5","FIG. 1"],"b":["500","508","118","508","522","118","100","522","508","522","504","166","504"]},"In some implementations, the smart home provider server system  or a component thereof serves as the video server system . In some implementations, the video server system  is a dedicated video processing server that provides video processing services to video sources and client devices  independent of other services provided by the video server system .","In some implementations, each of the video sources  includes one or more video cameras  that capture video and send the captured video to the video server system  substantially in real-time. In some implementations, each of the video sources  includes a controller device (not shown) that serves as an intermediary between the one or more cameras  and the video server system . The controller device receives the video data from the one or more cameras , optionally performs some preliminary processing on the video data, and sends the video data to the video server system  on behalf of the one or more cameras  substantially in real-time. In some implementations, each camera has its own on-board processing capabilities to perform some preliminary processing on the captured video data before sending the processed video data (along with metadata obtained through the preliminary processing) to the controller device and\/or the video server system .","As shown in , in accordance with some implementations, each of the client devices  includes a client-side module . The client-side module  communicates with a server-side module  executed on the video server system  through the one or more networks . The client-side module  provides client-side functionality for the event monitoring and review processing and communications with the server-side module . The server-side module  provides server-side functionality for event monitoring and review processing for any number of client-side modules  each residing on a respective client device . The server-side module  also provides server-side functionality for video processing and camera control for any number of the video sources , including any number of control devices and the cameras .","In some implementations, the server-side module  includes one or more processors , a video storage database , an account database , an I\/O interface to one or more client devices , and an I\/O interface to one or more video sources . The I\/O interface to one or more clients  facilitates the client-facing input and output processing for the server-side module . The account database  stores a plurality of profiles for reviewer accounts registered with the video processing server, where a respective user profile includes account credentials for a respective reviewer account, and one or more video sources linked to the respective reviewer account. The I\/O interface to one or more video sources  facilitates communications with one or more video sources  (e.g., groups of one or more cameras  and associated controller devices). The video storage database  stores raw video data received from the video sources , as well as various types of metadata, such as motion events, event categories, event category models, event filters, and event masks, for use in data processing for event monitoring and review for each reviewer account.","Examples of a representative client device  include a handheld computer, a wearable computing device, a personal digital assistant (PDA), a tablet computer, a laptop computer, a desktop computer, a cellular telephone, a smart phone, an enhanced general packet radio service (EGPRS) mobile phone, a media player, a navigation device, a game console, a television, a remote control, a point-of-sale (POS) terminal, a vehicle-mounted computer, an ebook reader, or a combination of any two or more of these data processing devices or other data processing devices.","Examples of the one or more networks  include local area networks (LAN) and wide area networks (WAN) such as the Internet. The one or more networks  are implemented using any known network protocol, including various wired or wireless protocols, such as Ethernet, Universal Serial Bus (USB), FIREWIRE, Long Term Evolution (LTE), Global System for Mobile Communications (GSM), Enhanced Data GSM Environment (EDGE), code division multiple access (CDMA), time division multiple access (TDMA), Bluetooth, Wi-Fi, voice over Internet Protocol (VoIP), Wi-MAX, or any other suitable communication protocol.","In some implementations, the video server system  is implemented on one or more standalone data processing apparatuses or a distributed network of computers. In some implementations, the video server system  also employs various virtual devices and\/or services of third party service providers (e.g., third-party cloud service providers) to provide the underlying computing resources and\/or infrastructure resources of the video server system . In some implementations, the video server system  includes, but is not limited to, a handheld computer, a tablet computer, a laptop computer, a desktop computer, or a combination of any two or more of these data processing devices or other data processing devices.","The server-client environment  shown in  includes both a client-side portion (e.g., the client-side module ) and a server-side portion (e.g., the server-side module ). The division of functionality between the client and server portions of operating environment  can vary in different implementations. Similarly, the division of functionality between a video source  and the video server system  can vary in different implementations. For example, in some implementations, the client-side module  is a thin-client that provides only user-facing input and output processing functions, and delegates all other data processing functionality to a backend server (e.g., the video server system ). Similarly, in some implementations, a respective one of the video sources  is a simple video capturing device that continuously captures and streams video data to the video server system  with limited or no local preliminary processing on the video data. Although many aspects of the present technology are described from the perspective of the video server system , the corresponding actions performed by a client device  and\/or the video sources  would be apparent to one of skill in the art. Similarly, some aspects of the present technology may be described from the perspective of a client device or a video source, and the corresponding actions performed by the video server would be apparent to one of skill in the art. Furthermore, some aspects of the present technology may be performed by the video server system , a client device , and a video sources  cooperatively.",{"@attributes":{"id":"p-0161","num":"0160"},"figref":"FIG. 6","b":["508","508","512","604","504","522","606","608","606","606","606","512","606","606","606","606"],"ul":{"@attributes":{"id":"ul0001","list-style":"none"},"li":{"@attributes":{"id":"ul0001-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0002","list-style":"none"},"li":["an operating system , including procedures for handling various basic system services and for performing hardware dependent tasks;","a network communication module  for connecting the video server system  to other computing devices (e.g., the client devices  and the video sources  including camera(s) ) connected to the one or more networks  via the one or more network interfaces  (wired or wireless);","a server-side module , which provides server-side data processing and functionality for the event monitoring and review, including but not limited to:\n        \n        ","server data , which includes data for use in data processing of motion event monitoring and review. In some implementations, this includes one or more of:\n        \n        "]}}}},"Each of the above identified elements may be stored in one or more of the previously mentioned memory devices, and corresponds to a set of instructions for performing a function described above. The above identified modules or programs (i.e., sets of instructions) need not be implemented as separate software programs, procedures, or modules, and thus various subsets of these modules may be combined or otherwise re-arranged in various implementations. In some implementations, the memory  stores a subset of the modules and data structures identified above. In some implementations, the memory  stores additional modules and data structures not described above.",{"@attributes":{"id":"p-0163","num":"0179"},"figref":"FIG. 7","b":["504","504","702","704","706","708","504","710","710","712","710","714","504","504","504","715","504"]},"The memory  includes high-speed random access memory, such as DRAM, SRAM, DDR RAM, or other random access solid state memory devices. In some implementations, the memory  includes non-volatile memory, such as one or more magnetic disk storage devices, one or more optical disk storage devices, one or more flash memory devices, or one or more other non-volatile solid state storage devices. In some implementations, the memory  includes one or more storage devices remotely located from the one or more processing units . The memory , or alternatively the non-volatile memory within the memory , comprises a non-transitory computer readable storage medium. In some implementations, the memory , or the non-transitory computer readable storage medium of memory , stores the following programs, modules, and data structures, or a subset or superset thereof:\n\n","Each of the above identified elements may be stored in one or more of the previously mentioned memory devices, and corresponds to a set of instructions for performing a function described above. The above identified modules or programs (i.e., sets of instructions) need not be implemented as separate software programs, procedures, modules or data structures, and thus various subsets of these modules may be combined or otherwise re-arranged in various implementations. In some implementations, the memory  stores a subset of the modules and data structures identified above. In some implementations, the memory  stores additional modules and data structures not described above.","In some implementations, at least some of the functions of the video server system  are performed by the client device , and the corresponding sub-modules of these functions may be located within the client device  rather than the video server system . In some implementations, at least some of the functions of the client device  are performed by the video server system , and the corresponding sub-modules of these functions may be located within the video server system  rather than the client device . The client device  and the video server system  shown in , respectively, are merely illustrative, and different configurations of the modules for implementing the functions described herein are possible in various implementations.",{"@attributes":{"id":"p-0167","num":"0198"},"figref":"FIG. 8","b":["118","118","118","118","802","804","806","808","118","810","118","812","118","814","118"]},"As illustrated in  below, the camera includes a sensor array  that captures video images, and a plurality of illuminators , which illuminate a scene when there is insufficient ambient light. Typically, the illuminators emit infrared (IR) light. In some implementations, the camera  includes one or more optional sensors , such as a proximity sensor, a motion detector, an accelerometer, or a gyroscope.","In some implementations, the camera includes one or more radios . The radios  enable radio communication networks in the smart home environment and allow the camera  to communicate wirelessly with smart devices using one or more of the communication interfaces . In some implementations, the radios  are capable of data communications using any of a variety of custom or standard wireless protocols (e.g., IEEE 802.15.4, Wi-Fi, ZigBee, 6LoWPAN, Thread, Z-Wave, Bluetooth Smart, ISA100.11a, WirelessHART, MiWi, etc.), custom or standard wired protocols (e.g., Ethernet, HomePlug, etc.), and\/or any other suitable communication protocol.","The communication interfaces  include, for example, hardware capable of data communications (e.g., with home computing devices, network servers, etc.), using any of a variety of custom or standard wireless protocols (e.g., IEEE 802.15.4, Wi-Fi, ZigBee, 6LoWPAN, Thread, Z-Wave, Bluetooth Smart, ISA100.11a, WirelessHART, MiWi, etc.) and\/or any of a variety of custom or standard wired protocols (e.g., Ethernet, HomePlug, USB, etc.), or any other suitable communication protocol.","The memory  includes high-speed random access memory, such as DRAM, SRAM, DDR RAM, or other random access solid state memory devices. In some implementations, the memory  includes non-volatile memory, such as one or more magnetic disk storage devices, one or more optical disk storage devices, one or more flash memory devices, or one or more other non-volatile solid state storage devices. The memory , or alternatively the non-volatile memory within the memory , comprises a non-transitory computer readable storage medium. In some implementations, the memory , or the non-transitory computer readable storage medium of the memory , stores the following programs, modules, and data structures, or a subset or superset thereof:\n\n","Each of the above identified elements may be stored in one or more of the previously mentioned memory devices, and corresponds to a set of instructions for performing a function described above. The above identified modules or programs (i.e., sets of instructions) need not be implemented as separate software programs, procedures, or modules, and thus various subsets of these modules may be combined or otherwise re-arranged in various implementations. In some implementations, the memory  stores a subset of the modules and data structures identified above. In some implementations, the memory  stores additional modules and data structures not described above.","In some implementations, at least some of the functions of the camera  are performed by a client device , the server system , and\/or one or more smart devices , and the corresponding sub-modules of these functions may be located within the client device , the server system , and\/or smart devices , rather than the camera . Similarly, in some implementations, at least some of the functions of the client device, the server system, and\/or smart devices are performed by the camera , and the corresponding sub-modules of these functions may be located within the camera . For example, in some implementations, a camera  captures an IR image of an illuminated scene (e.g., using the illumination module  and the image capture module ), while a server system  stores the captured images (e.g., in the video storage database ) and creates a depth map  based on the captured images (e.g., performed by a depth mapping module  stored in the memory ). The server system , the client device , and the camera , shown in  are merely illustrative, and different configurations of the modules for implementing the functions described herein are possible in various implementations.",{"@attributes":{"id":"p-0174","num":"0219"},"figref":"FIG. 9","b":["900","900","508","900","900"]},"A scene understanding server  typically includes one or more processing units (CPUs)  for executing modules, programs, or instructions stored in the memory  and thereby performing processing operations; one or more network or other communications interfaces ; memory ; and one or more communication buses  for interconnecting these components. The communication buses  may include circuitry (sometimes called a chipset) that interconnects and controls communications between system components. In some implementations, the server  includes a user interface , which may include a display device  and one or more input devices , such as a keyboard and a mouse.","In some implementations, the memory  includes high-speed random access memory, such as DRAM, SRAM, DDR RAM or other random access solid state memory devices. In some implementations, the memory  includes non-volatile memory, such as one or more magnetic disk storage devices, optical disk storage devices, flash memory devices, or other non-volatile solid state storage devices. In some implementations, the memory  includes one or more storage devices remotely located from the CPU(s) . The memory , or alternately the non-volatile memory device(s) within the memory , comprises a non-transitory computer readable storage medium. In some implementations, the memory , or the computer readable storage medium of memory , stores the following programs, modules, and data structures, or a subset thereof:\n\n","Each of the above identified elements may be stored in one or more of the previously mentioned memory devices, and corresponds to a set of instructions for performing a function described above. The above identified modules or programs (i.e., sets of instructions) need not be implemented as separate software programs, procedures, or modules, and thus various subsets of these modules may be combined or otherwise re-arranged in various implementations. In some implementations, the memory  stores a subset of the modules and data structures identified above. In some implementations, the memory  stores additional modules and data structures not described above.","In some implementations, at least some of the functions of the scene understanding server  are performed by a client device , the camera , or other servers in the video server system . Similarly, in some implementations, at least some of the functions of the client device , the video server system , and the camera  are performed by the scene understanding server . For example, in some implementations, a camera  captures an IR image of an illuminated scene (e.g., using the illumination module  and the image capture module ), while a scene understanding server  stores the captured images  and creates one or more depth maps  based on the captured images (e.g., performed by a depth mapping module ).",{"@attributes":{"id":"p-0179","num":"0233"},"figref":"FIG. 10","b":["118","118","852","856","856","1","856","8","1010","852","852","118"]},"As described in greater detail below, the illuminators  are activated to illuminate a scene by emitting streams of light (e.g., infrared (IR) light). During illumination, light rays are scattered by and reflect off of object surfaces in the scene (e.g., walls, furniture, humans, etc.). Reflected light rays are then detected by the sensor array , which captures an image of the scene (e.g., and IR image or an RGB image). The captured image digitally measures the intensity of the reflected IR light for each of the pixels in the sensor array .","In some implementations, the illuminators  are light emitting diodes (LEDs). In some implementations, the illuminators  are semiconductor lasers or other semiconductor light sources. In some implementations, the illuminators  are configured to emit light spanning a broad range of the electromagnetic spectrum, including light in the IR range (e.g., 700 nm to 1 mm), the visible light range (e.g., 400 nm-700 nm), and\/or the ultraviolet range (e.g., 10 nm-400 nm). In some implementations, a portion of the illuminators  are configured to emit light in a first range (e.g., IR range), while other illuminators  are configured to emit light in a second range (e.g., visible light range). In some implementations, the illuminators  are configured to emit light in accordance with one or more predefined illumination patterns. For example, in some implementations, the illumination pattern is circular round-robin in a clockwise order. In some of these implementations, the round-robin pattern activates two illuminators at a time, as illustrated in  below. An illumination pattern may specify other parameters as well, such as the length of time each illuminator is activated, the output power (e.g., measured in watts), or other parameters.","The sensor array  converts an optical image (e.g., reflected light rays) into an electric signal. In some implementations, the sensor array  is a CCD image sensor, a CMOS sensor, or another type of light sensor device (e.g., a hybrid of CCD and CMOS). The sensor array  includes a plurality of individual light-sensitive sensors. In some implementations, the sensors of the sensor array  are arranged in a rectangular grid pattern as illustrated in . Upon exposure to light, each sensor of the sensor array  detects a measurable and proportional value corresponding to the light intensity. In some implementations, the sensor array  or other camera circuitry converts the measured value (e.g., current) into a digital value. In some implementations, the sensor array  or the enclosure  includes an IR filter to remove wavelengths of incident light that fall outside of a predefined range. For example, some implementations use an IR filter that passes only light having wavelengths in the range of 810 nm to 870 nm. In some implementations, the illuminators  emit light at a specified wavelength and the light reaching the sensor array is filtered to correspond to the specified wavelength of the illuminators.","In some implementations, the camera  includes additional camera components, such as one or more lenses, image processors, shutters, and\/or other components known to those skilled in the art of digital photography.","In some implementations, the camera  also includes camera circuitry for coordinating various image capture functionality of the camera . In some implementations, the camera circuitry is coupled to the illuminators , to the sensor array , and\/or to other camera components, and coordinates the operational timing of the various camera device components. In some implementations, when capturing an IR image of a scene, the camera circuitry activates a subset of the illuminators , activates the sensor array  to capture the image, and determines an appropriate shutter speed to manage the image exposure. In some implementations, the camera circuitry performs basic image processing of raw images captured by the sensor array  during the exposure. The image processing includes filtering and conversion of a produced voltage or current at the sensor array  into a digital value.",{"@attributes":{"id":"p-0185","num":"0239"},"figref":"FIG. 11","b":["852","118","1110","852","1110","1110","1110","1110","1110"],"sub":["1,1","1,2","i,j"]},{"@attributes":{"id":"p-0186","num":"0240"},"figref":["FIG. 12","FIG. 10","FIG. 12"],"b":["118","118","856","852","1010"]},"In some implementations, one or more illuminators  are angled relative to the planar axis of the sensor array, such as illuminator - in . By positioning the illuminators  at respective angles (e.g., angle ), portions of a scene will be illuminated at greater or lesser intensities depending on which of the illuminators  are activated and the angles at which the activated illuminators are positioned.  illustrate a sequence of IR images with different illuminators activated. For example,  is an image captured with the top two illuminators activated, whereas  is an image captured with the bottom two illuminators activated.",{"@attributes":{"id":"p-0188","num":"0242"},"figref":"FIGS. 13-15B","b":"852"},"To generate a lookup table for a pixel, the lookup table generation module  determines an expected reflected light intensity at the pixel based on the simulated surfaces  being at various fixed distances  from the pixel. This is illustrated in , with fixed distances d-, d-, d-, . . . , d-, and surfaces -, -, -, . . . , -. The number of distinct simulated distances  affects the accuracy of the subsequently estimated depths. In this example, all of the surfaces  are planar. In other implementations, the surfaces are spherical, parabolic, cubic, or other appropriate shape. Typically, however, all of the surfaces are of the same type (e.g., there would generally not be a mixture of planar and spherical surfaces). In the simulation, each virtual surface has a constant surface reflectivity.","For each depth , the illuminators  of the camera  are simulated to activate in accordance with a pre-defined illumination pattern. An illumination pattern specifies the grouping of illuminators  (if any), specifies the order the groups of illuminators are activated, and may specify other parameters related to the operation of the illuminators.  provides an example in which the illuminators  are grouped into consecutive pairs in a clockwise orientation and activated in that order. At a first time -, the top illumination group - is activated, at a second time - a second illumination group - is activated, at a third time - a third illumination group - is activated, and at a fourth time - a fourth illumination group - is activated. In the example illustrated in , there are four illumination groups  in the illumination pattern, so there are four distinct estimated light intensity values.","In some implementations, the estimated light intensity values are placed into an intensity matrix Y, as illustrated in . In this matrix, each column corresponds to one depth, and each row corresponds to an illumination group from the illumination pattern. For example, the first column - corresponds to a first distance d. The first light intensity estimate - corresponds to the first illumination group -, the second light intensity estimate - corresponds to the second illumination group -, the third light intensity estimate - corresponds to the third illumination group -, and the fourth light intensity estimate - corresponds to the fourth illumination group -.","The kth column -in the intensity matrix Y has four light intensity estimates -, -, -, and -, corresponding to the same four illumination groups in the illumination pattern. Finally, the mth column -has four list intensity estimates corresponding to the same four illumination groups in the illumination pattern. Note that the matrix Y is for a single pixel i,j (e.g., as downsampled from the sensor array ).","As currently computed, the entries in the intensity matrix Y depend on the reflectivity \u03c1 of the simulated surface. Because different actual surfaces have varying reflectivities, it would be useful to \u201cnormalize\u201d the matrix in a way that eliminates the reflectivity constant \u03c1. In some implementations, the columns of the intensity matrix Y are normalized by dividing the elements of each column by the length (e.g., Lnorm) of the column.",{"@attributes":{"id":"p-0194","num":"0248"},"figref":"FIG. 15B","sub":["i,j ","i,j","i,j","2","1k","2k","3k","4k","i,j ","i,j"],"b":["1508","1506","1510","1508","1506"],"sup":["2","2","2","2"]},"Note that after normalization, each column of the lookup table {tilde over (Y)}has the same normalized length, even though each column corresponds to a different distance from the sensor array. However, the distribution of values across the elements (corresponding to the illumination groups) are different for different depths (e.g., the normalized first column is different from the normalized kth column).","Some implementations take advantage of symmetry to reduce the number of lookup tables. For example, using the illumination pattern illustrated in , some implementations reduce the number of lookup tables by a factor of four (e.g., using rotational symmetry), or reduce the number of lookup tables by a factor of eight (e.g., using rotational symmetry and reflection symmetry).",{"@attributes":{"id":"p-0197","num":"0251"},"figref":"FIGS. 16A-16D","b":["17","17","118","852","852"]},{"@attributes":{"id":"p-0198","num":"0252"},"figref":["FIGS. 16A-16D","FIGS. 16A-16D","FIG. 16A","FIG. 16C","FIG. 16C","FIG. 16D","FIGS. 16A-16D"],"b":["1606","856","118","1602","1","1604","1","1602","3","1604","3","1602","2","1604","2","1602","4","1606","1","1606","2","1606","3","1606","4","1602"]},"As illustrated in , a vector {right arrow over (b)} is constructed for each pixel i,j. The four components of the vector  correspond to the four distinct illumination groups -, -, -, and -. The first component b-b is the light intensity bat the pixel when the first illumination group - is active minus the light intensity bat the pixel from the baseline image. Similarly, the second component b-b is the light intensity bat the pixel when the second illumination group - is active minus the light intensity bat the pixel from the baseline image. The third component b-b is the light intensity bat the pixel when the third illumination group - is active minus the baseline light intensity b, and the fourth component b-b is the light intensity bat the pixel when the fourth illumination group - is active minus the baseline light intensity b.","For each individual pixel there is a separate lookup table, which is generated as described above by simulating virtual surfaces at different depths. The actual depth in the scene at the pixel is determined by finding the closest matching record in the lookup table for the pixel. In this example, the vector {right arrow over (b)} and the records in the lookup table (e.g., column {tilde over (Y)}(k) ) are four dimensional vectors. In some implementations, the closest match is computed by finding the lookup table record whose \u201cdirection\u201d in Rmost closely aligns with the sample vector {right arrow over (b)}. This can be determined by computing the inner product (e.g., dot product) of the vector {right arrow over (b)} with each of the records in the lookup table. In some implementations, the inner product of the vector {right arrow over (b)} with the record {tilde over (Y)}(k)  is <{right arrow over (b)}, {tilde over (Y)}(k)>=y(b\u2212b)+y(b\u2212b)+y(b\u2212b)+y(b\u2212b). The record in the lookup table whose inner product with the sample vector  is the greatest has an associated depth (i.e., the simulated depth for which the lookup table record was created), and this is the estimated depth for the pixel. Typically, the inner product used is just the dot product, as illustrated in this example.","The process just described is shown concisely by the formula in . The lookup table index {circumflex over (k)} is estimated by computing the dot product of the normalized lookup table records {tilde over (Y)}(k)  with the sample vector {right arrow over (b)}, and selecting the index for which the dot product is maximal. The estimated depth corresponds to the index {circumflex over (k)}.","In the example illustrated in , A, and B, the eight illuminators are grouped into four illumination groups. However, many other illumination patterns are possible with the same set of eight illuminators. For example, in some implementations, the eight illuminators are activated individually, creating lookup tables with eight rows and vectors with eight components. Some implementations use other illumination patterns as well. For example, some implementations use two illuminators at a time, but use each illuminator in two groups (e.g., a first group consisting of illuminators  and , a second group consisting of illuminators  and , a third group consisting of illuminators  and , etc.).",{"@attributes":{"id":"p-0203","num":"0257"},"figref":["FIGS. 18A-18E","FIG. 18A"],"b":["118","118"]},"In some implementations, the camera  has infrared illuminators , which illuminate the scene (typically at night) and capture one of more IR images to form an IR intensity image , as illustrated in . In this example of an IR intensity image , black represents high intensity and white represents low intensity. Because windows are specular, the light emitted from the IR illuminators  mostly reflects off in other directions rather than back towards the image sensor array  of the camera , thus creating regions of low intensity. As seen in the IR intensity image , there are various areas , , , and  of low intensity. The low intensity pixels are clustered together to form contiguous regions. In addition to being specular, windows typically have a reasonable size (e.g., a house would not have a window that is one inch wide), and are generally rectangular. Because of the deformation of the images, a rectangular window appears as a quadrilateral, which may not be a rectangle.","Using size and\/or quadrilateral analysis of the low intensity regions in , the process determines that the lower regions  and  do not appear to be windows. However, the upper left low intensity region  is sufficiently large and fits in a quadrilateral  fairly well, as indicated in . Therefore, the region  is designated as a probable window. Similarly, the upper right low intensity region  is sufficiently large and fits well into a quadrilateral , so it is identified as a probable window as well.","The same techniques described with respect to windows can identify other types of objects as well. For example, the same analysis used for windows can be applied to identify mirrors or television screens. In some implementations, a sufficiently large quadrilateral region with low intensity of reflected IR light is identified as a television rather than a window based on other information, such as frequent movement within the region. Certain materials have reflectivities that are intermediate between a specular surface and a surface with highly diffused reflections. In some implementations, these materials are identified by a range of expected image intensity from reflecting the IR light.","In some implementations, quadrilateral fitting measures the absolute difference between the quadrilateral and the region, and determines that there is a good fit when the absolute difference is less than a threshold percentage of the area of the quadrilateral (e.g., less than 5%, less than 10%, or less than 20%). In some implementations, the process uses more general polygons rather than quadrilaterals.","Some implementations use motion discontinuity as a factor in determining whether a low intensity region is a window. For example, motion of an object on an opposite side of a window will show up as discontinuous both as the object enters the field of the window and when the object exits the field of the window. In some implementations, the presence of motion discontinuity within a region is used as evidence that the region is a window, but the absence of motion discontinuity is not used as evidence that the region is not a window.",{"@attributes":{"id":"p-0209","num":"0263"},"figref":["FIGS. 18D and 18E","FIG. 18D","FIG. 18B","FIG. 18D","FIG. 18E"],"b":["1822","1824","1822","1824","1832","1834","1832","1834","1822","1824"]},{"@attributes":{"id":"p-0210","num":"0264"},"figref":"FIG. 19A","b":["1980","118","1988","118","118","928"]},"Some implementations build () a depth map based on IR images captured while the camera  is in the first position . In some implementations, the IR images are captured temporally proximate to the time the zone is defined in order to ensure that the depth map is built based on the same field of vision. In some implementations, temporal proximity is defined to be within 12 hours or within 24 hours. At some point later, the camera moves (). For example, a person may bump the camera or a person may choose to move the camera slightly to get better coverage of a room. Later, some implementations build () a second depth map based on IR images captured while the camera  is in a second position . Note that the zone correction module  does not necessarily know the camera has moved. In some implementations, depth maps are created on a periodic basis (e.g., once each night, every two days, or once each week).","In some implementations, the zone correction module  computes point clouds  corresponding to each of the depth maps, where each point in a point cloud  is a three dimensional position in the scene monitored by the camera, as illustrated below in . In some implementations, a predetermined number of points are selected for each point cloud (e.g., 50, 100, or 1000 points), but in other implementations, the number of points varies based on the objects in the monitored scene. In some implementations, the points for each point cloud are selected based on designated positions within the image sensor array (e.g., the intersection of each tenth row with each tenth column). In some implementations, the points in the point cloud are selected by downsampling from the depth map (i.e., combine multiple points from the depth map to create an individual point for the point cloud). In some implementations, points for each point cloud are selected based on other characteristics, such as proximity to the camera (e.g., choose points from the depth map that are close to the camera).","The process of comparing two point clouds is sometimes referred to as \u201cregistration\u201d by those of skill in the art. A registration process determines how to transform one point cloud into another point cloud. Some implementations use one or more iterated closest point (ICP) methods to determine the transformation. When one of the point clouds can be transformed to match the other point cloud, the iterative process builds the transformation as a sequence of steps that converge on the final transformation. When the two point clouds are fundamentally different (e.g., from IR images captured from different scenes), the iterative process is generally unable to converge.","After the transformation is determined, the transformation is applied to the zone defined by the user, thereby creating an adjusted zone that corresponds to the defined zone. This is illustrated below in . In some implementations, the user is prompted to confirm the adjusted zone. The process of performing zone correction is also described below with respect to the flowchart  in .",{"@attributes":{"id":"p-0215","num":"0269"},"figref":["FIGS. 19B and 19C","FIG. 19B","FIGS. 23A-23C"],"b":["118","1900","118"]},"In the scene -B of , seven points have been identified: the points  and  that appear to be the left side corners of a picture frame or window; the point  at the left side of an apparent table; the point  that appears to be the bottom of a table leg, and three points , , and  that are at various locations on what appears to be a chair. Note that what the points represent is not relevant to the analysis. Here, the relative positions of the points (horizontally, vertically, and depth from the camera) identify points in 3 dimensional space. In , seven similar points - have been identified, and in this case the depths (not shown) are approximately the same as the corresponding points - in . However, the scene -B appears to have shifted to the left to create the modified scene -C. Rather than concluding that the whole scene has shifted to the left, the zone correction module  determines that the camera has moved a little to the right. In some implementations, the points - and the points - are stored as point clouds . Although the example of  has a one-to-one correspondence between the points in the two point clouds , the zone correction module  does not require such a perfect correspondence between the two point clouds .",{"@attributes":{"id":"p-0217","num":"0271"},"figref":["FIGS. 19D and 19E","FIG. 19D","FIG. 19B","FIG. 19E","FIG. 19D","FIG. 19E","FIG. 19D","FIG. 19D","FIG. 19E","FIG. 19E"],"b":["1901","1907","1908","1901","1902","1921","1927","1901","1907","1928","1921","1922","1908","1901","1907","1921","1927","1900","1900","928"]},"As illustrated in , the zone correction module uses two point clouds  that represent the field of vision of the camera, and determines whether the two point clouds correspond to slightly different views of the same scene. In some instances, the camera is moved to a completely different scene (e.g., a different room), so the two depth maps are quite different. The zone correction module  is generally able to determine that the point clouds  do not correspond.",{"@attributes":{"id":"p-0219","num":"0273"},"figref":["FIG. 19F","FIG. 19F","FIGS. 26A-26C"],"b":["1940","1950"]},"When the camera is at the first location , the field of vision of the camera is illustrated by the dotted lines  on the left and  on the right. When the camera is at the second location , the field of vision of the camera is illustrated by the dotted lines  on the left and  on the right. A first depth map is created based on images captured while the camera  is at the first position , and a second depth map is created based on images captured while the camera  is at the second position . For each of the depth maps, a point cloud is created that contains a plurality of points.","In this illustration, the points - and - are in the field of vision of the camera at the first position  but not in the field of vision from the second location . Conversely, the points -, -, and - are in the field of vision of the camera  at the second position  but not in the field of vision from the first location . The other points in this illustration are in the shared region .","A first point in this region is identified both as point - and as point -. The two labels for the same point are due to the presence of the point in both the first and second depth maps. With respect to the camera , the three dimensional coordinates of the point - are different from the 3-dimensional coordinates of the point -, even though the point has not moved. For example, the depth and horizontal position of the point - (as measured from the first camera location ) are different from the depth and horizontal position of the point - (as measured from the second camera location ). If the height of the camera above the floor at the first and second locations are the same, then the measured height of the point - is the same as the height of the point -. The same analysis applies to the second labeled point in the region , which is labeled as both - and -. They are the same physical point in the scene, but have different 3-dimensional coordinates based on the two views. The same analysis applies to the third labeled point in the region , which is labeled as both - (from the first depth map) and - (from the second depth map).","The first point cloud (containing the points ---) is correlated to the second point cloud (containing the points ---), based on points in the overlap region . In practice, the points are not literally identical as they are in this example. As indicated above, an iterative algorithm determines how to map one of the point clouds to the other.",{"@attributes":{"id":"p-0224","num":"0278"},"figref":["FIG. 19G","FIG. 19H","FIGS. 19G and 19H","FIGS. 19G and 19H","FIGS. 23A-23C","FIG. 19A","FIGS. 26A-26C"],"b":["1960","1960","118","878"]},"As shown in , the uncorrected zone  (using the same coordinates that were saved for the original zone ) no longer covers the entryway that was covered by the zone  previously. However, using the point clouds created from the depth maps, the zone correction module  determines the transformation required to correlate the two views, and applies the transformation to the first zone. The transformation constructs an adjusted zone , which again covers the entryway. Even if the adjusted zone  is not perfect (it should be a little wider to match the entryway), it is a much better zone for the camera in the new position than the uncorrected zone .",{"@attributes":{"id":"p-0226","num":"0280"},"figref":"FIG. 19I","b":"1970"},"When the camera has moved slightly, the process computes an output , which is an adjusted zone. The adjusted zone corresponds to the original zone, but accounts for the camera movement. This is illustrated above .","In some implementations, computing the adjusted zone includes: (1) converting () the original depth map to a point cloud with 3D coordinates. In some implementations, the constructed point cloud has at least 100 points. In some implementations, the point cloud has fewer or more points. For example, in some implementations, the point cloud has 50 points or 500 points. In some implementations, the points for the point cloud are randomly or pseudo-randomly selected from the depth map. In some implementations, the points in the point cloud are selected in a regular pattern, such as every tenth pixel horizontally and vertically. In some implementations, the points in the point cloud are selected based on specific characteristics, such as proximity to the camera or locations where there is significant depth discontinuity (see ).","The process builds () a second point cloud from the second map, which corresponds to the current location of the camera. The points in the second point cloud are generally selected in the same way as for the first point cloud.","The process then compares () the two point clouds. This process is sometimes referred to as point cloud registration. Some implementations use an iterative process to perform point cloud registration. In some implementations, the process uses an iterated closest point (\u201cICP\u201d) method. The registration process determines a transformation that maps the first point cloud to the second point cloud.","Finally, the process applies () the identified transformation to the user-selected zone to identify an adjusted zone based on the new camera location. In some implementations, the new zone is used immediately. In some implementations, the user is prompted to confirm the adjusted zone, and the user may tweak the adjusted zone further.",{"@attributes":{"id":"p-0232","num":"0286"},"figref":["FIGS. 20A-20K","FIG. 20A","FIG. 20G"],"b":"946"},"In some implementations, the floor\/wall\/ceiling module  uses a depth map  of the scene, which is constructed as illustrated in , A, B, and A-C. The floor\/wall\/ceiling module  uses the depth map  to identify depth discontinuities. In some implementations, the floor\/wall\/ceiling module  identifies the discontinuities using an x-direction gradient map G as illustrated in  and a y-direction gradient map G as illustrated in . As illustrated in , some implementations combine the two gradients G and G to form a binary depth edge map , as shown in . In some implementations, an edge is identified at a pixel when the total depth change exceeds a predefined threshold value.","Once the depth discontinuities are identified in the binary depth edge map , the floor\/wall\/ceiling module  identifies the closed components  in the image (i.e., regions that are enclosed by the edges). These closed components  represent the candidates for floors, walls, and ceilings.  shows the closed components  corresponding to the depth map  in . The two largest components - and - are good candidates. In some implementations, closed components  that are smaller than a threshold size are excluded from further analysis. For example, in some implementations, only the two largest closed components - and - are evaluated.",{"@attributes":{"id":"p-0235","num":"0289"},"figref":"FIG. 20G","b":["118","2020","1","2020","4","2020","5","2020","8"]},"For each of the closed components  that is evaluated, the floor\/wall\/ceiling module  fits a plane to the points in the component. In some implementations, the fitted plane has an equation of the form wx+wy+wz=1, where w, w, and ware constants to be determined, as illustrated in . For each closed component, a subset of points in the component are used to form a matrix C, as illustrated in . The matrix C has a row for each selected point in the component, and has three columns corresponding to the x, y, and z-coordinates of the points. A single closed component  may have a large number of points, so implementations typically take a sampling (e.g., a pseudo-random sample of 20 points or 50 points). The fitted plane  should closely match the data, so a \u201cbest fit\u201d can be determined by measuring the total error. Some implementations use least squares, and thus select the values for w, w, and wto minimize the expression \u03a3(wc+wc+wc\u22121), as illustrated in . Some implementations use alternative methods to identify a \u201cbest\u201d plane for a set of data points from a closed component.","Once a best plane  is identified for a component, the floor\/wall\/ceiling module  evaluates the plane in two ways. First, is the total error sufficiently small so that the plane is a good fit? Second, does the orientation of the plane correspond to floor, wall, or ceiling? Some implementations specify an error threshold, and designate a closed component as a probable floor, wall, or ceiling only when the actual error is less than the threshold. In some implementations, the total error is normalized based on the number of points in the sample.","As illustrated in , a floor should have z increasing as a function of y. Using the formula in , z=\u2212w\/w+(other terms), so the expression \u2212w\/wshould be positive for a floor. Similarly, for a ceiling, the expression \u2212w\/wshould be negative. Some implementations also evaluate the magnitude of the expression \u2212w\/wto determine whether it is consistent with data expected for a floor or ceiling. For walls, the expressions are similar, but use the x-dimension rather than the y-dimension.",{"@attributes":{"id":"p-0239","num":"0293"},"figref":"FIG. 20K","b":["946","2","2022","946","1"]},{"@attributes":{"id":"p-0240","num":"0294"},"figref":"FIGS. 21A-21E","b":["852","118"]},"In , the camera  is at a height h above the floor , and some of the floor  is in the field of vision of the camera , as illustrated by the dashed lines . In , the camera is facing straight forward, so the camera orientation  matches the plane  parallel to the floor . This produces a tilt angle \u03b8 of 0 degrees.",{"@attributes":{"id":"p-0242","num":"0296"},"figref":"FIG. 21B","b":["118","2110","2110","118","2130","118","2122","2110","2124","2128","2126"],"sub":["2 ","2 ","2 "]},"The illustrations in  illustrate both the process of building a dictionary (typically using simulation with varying heights and tilt angles) as well as determining the position of an actual camera .",{"@attributes":{"id":"p-0244","num":"0298"},"figref":"FIG. 21C","b":["2150","2152","118","2152","118","118"]},"The dictionary includes a height  and a tilt  for each entry, and includes data for one or more images captured based on different sets of IR illuminators emitting light. In some implementations, a single image is captured while all of the IR emitters are on. In some implementations, a separate image is captured for each individual IR emitter, taken while that IR emitter is on and the remaining IR emitters are off. In some implementations, the emitters are grouped into pairs, as illustrated above with respect to . In the example dictionary  in , there are four subsets (as in ), and separate images , , , and  are simulated or captured for each of the subsets. When built using simulation, the estimated intensity at each pixel depends on the location and orientation of the IR emitters relative to the image sensor array .","In this example dictionary , the second dictionary entry - corresponds to a height of 0.6 meters and a tilt angle of 10\u00b0. In some implementations, positive title angles indicate the camera is pointing downward. For this second entry -, the process simulates or captures four images I, I, I, and I, corresponding to each of the four subsets of IR illuminators. In some implementations, abbreviated images are stored. For example, some implementations store only pixels corresponding to the simulated floor. Note that the pixels in the images are typically downsampled from the image sensor array. For example, the image sensor array may include 4 million individual image sensors, whereas the saved images may include only 10,000 pixels.","In this example dictionary , there are 250 dictionary entries , corresponding to heights ranging from 0.6 meters to 3.0 meters (in 0.1 meter increments) and angles ranging from 0 degrees to 90 degrees (in 10 degree increments). In some implementations, there are fewer or more dictionary entries , depending on the desired granularity, available storage space, required processing speed, and\/or other considerations.","Whereas a dictionary  is typically creating one time for a given camera model, the dictionary  can be used many times to estimate the heights and tilt angles of many cameras at many different times.",{"@attributes":{"id":"p-0249","num":"0303"},"figref":["FIG. 21D","FIG. 21D"],"b":["118","2150","2160","2160","2162"]},"Using the adjusted intensity images, the process identifies () at least one possible floor region. In some implementations, identifying a possible floor region uses techniques illustrated in  and A-B. If no floor regions are identified, some implementations automatically switch to determining the position of the camera relative to the ceiling. When more than one floor region is identified, some implementations estimate a camera position relative to each of the identified regions, then select a best fit or compute an aggregated estimate. If there are two or more regions and the estimates are similar, some implementations compute an average or weighted average. If there are two or more regions and the estimates differ substantially, some implementations select the data for the larger height based on the statistical reasoning that the higher number is more likely to be correct (e.g., because the smaller number is from a table).","Some implementations use an iterative algorithm for identifying a floor region. In some of these implementations, the entire set of pixels is used as a starting point for the first iteration, and in each iteration some of the pixels are removed. In some implementations, the pixels identified for removal in each iteration are selected based on overall contribution to the computed distances between the adjusted IR intensity images and entries in the dictionary. In some implementations, the process combines floor selection () and classification () into an iterative loop.","Once a floor region is identified, a classifier estimates () the (height, tilt)  using the adjusted IR intensity images, the previously computed dictionary , and limiting the analysis to pixels in the identified floor region. The operation of the classifier is described in more detail in .","The classifier identifies a \u201cclosest\u201d dictionary entry  to the adjusted IR intensity images, and estimates the height and tilt of the camera based on that closest dictionary entry. When the number of dictionary entries is small (e.g., ), some implementations compare the adjusted IR intensity images to each of the dictionary entries to find the closest one. In some implementations, the process is able to prune some of the dictionary entries, thereby comparing the adjusted IR intensity images to a smaller list of dictionary entries.","To identify a closest dictionary entry , some implementations compute distances between vectors, as illustrated in . In this figure, the input is the set of four images I, I, I, and Ibased on the different subsets of illuminators, and the baseline image I. The baseline image Iis subtracted from the others to create the input , which can be viewed as a long feature vector . In this example, each image has n pixels, and the elements are arranged in order of the images. For example, the elements a, . . . , a, . . . , a, correspond to the pixels of the image I-I. In this example, the index r corresponds to one specific pixel in the identified floor region. Because there are four distinct images, there are four feature vector components a, a, a, a corresponding to the rth pixel.",{"@attributes":{"id":"p-0255","num":"0309"},"figref":"FIG. 21E","b":["2152","2","2150","2152","2","2172","2","2180","2178"],"sub":["2,1","2,2","2,3","2,4"]},"To compute the distance between the feature vector  and a dictionary entry vector , some implementations use Euclidean distance based on the relevant vector components. The relevant components are the ones associated with the pixels in the identified floor region. For example, in this case, the rth pixel is part of the identified floor region, so the four components corresponding to r are included in the calculation of the distance, as illustrated in formula -. If there are four illuminator subsets and 100 pixels in the identified floor region, then the distance calculation will use 400 components of the vectors. In some implementations, alternative distance metrics are used, such as the total absolute difference between vector components |a\u2212b|+ . . . or the maximum absolute difference between vector components.","In some implementations, the single closest dictionary entry is used to estimate the camera position. For example, if the second dictionary entry - above is determined to be closer than all of the other dictionary entries, then the camera is estimated to be at a height of 0.6 meters and at an angle of 10 degrees (see ). In some implementations, the k closest dictionary entries are identified for a predefined positive integer k. These k entries are then used to estimate the height and tilt angle for the camera. For example, some implementations compute a weighted average from the k nearest entries, and weight each entry inversely based on its calculated distance. Some implementations use alternative techniques, such as other regression algorithms.",{"@attributes":{"id":"p-0258","num":"0312"},"figref":"FIGS. 22A-22C","b":["2200","2202","2204","900","2204"]},"The process identifies () a plurality of distinct subsets of IR illuminators  of a camera system . One example is illustrated above in , where the camera's  illuminators  are grouped into four distinct subsets. One of skill in the art recognizes that many other alternatives are possible, such as having one illuminator in each subset, having some overlap between subsets, or having different subsets with different numbers of illuminators.","The camera also has () a 2-dimensional array  of image sensors. The 2 dimensional array  is typically laid out in a rectangular pattern, as illustrated above in , but the disclosed process  can be applied regardless of the pattern to lay out the image sensors in the array. In some implementations, the array of image sensors includes () more than 1,000,000 individual image sensors (e.g., 2sensors). The IR illuminators  are () in fixed locations relative to the array  of image sensors, as illustrated in  above.","The process partitions () the image sensors into a plurality of pixels. In some implementations, each pixel includes () a respective single image sensor. In some implementations, each pixel includes () a respective plurality of image sensors. In some implementations, each pixel includes () more than 50 respective image sensors. These are a few ways that implementations partition the individual image sensors into pixels. Typically the array of image sensors has a high resolution, but sensors are downsampled to create a more manageable number of pixels (e.g., 10,000 pixels).","A separate lookup table is constructed for each pixel. Each record in a lookup table corresponds to a depth in front of the pixel. The accuracy of subsequent depth estimation depends on the number of depths used to build each lookup table. For example, if depth data is created for each inch in front of the pixel, then subsequent depth estimation may be accurate within an inch. However, if there are only two depth data points, the accuracy for subsequent estimation will be limited.","For each pixel, and for each of m distinct depths from the pixel, the process performs () the following operations. The process simulates () a virtual surface at the respective depth. Implementations use various shapes for the virtual surfaces, such as planar (), spherical (), parabolic (), or cubic ().  illustrates the case of planar surfaces. Typically, an implementation uses the same surface shape for each of the pixels and depths, although potentially with different parameters. For example, when spherical surfaces are used, some implementations simulate a sphere whose radius is the given depth so that the surfaces at each of the depths create concentric spheres.","For each pixel and for each of the depths (), the process also determines () an expected IR light intensity at the respective pixel based on the respective depth, the shape of the virtual surface, and which subset of IR illuminators is emitting IR light. In some implementations, the expected IR light intensity at the respective pixel is () based on other characteristics of the IR illuminators of the camera system as well. For example, in some implementations, the characteristics include () the lux of the IR illuminators . In some implementations, the characteristics include () orientation of the IR illuminators relative to the sensor array. This is illustrated above in , with illuminator - oriented at an angle . In some implementations, the characteristics include () location of the IR illuminators relative to the sensor array.","For each pixel and for each of the depths (), the process also forms () an intensity vector using the expected IR light intensity for each of the distinct subsets. This is illustrated in  above. Typically a baseline value is subtracted from each of the values, where the baseline value is measured when none of the illuminators are emitting light. The process then normalizes () the intensity vector. In some implementations, the process normalizes each intensity vector by determining () a respective magnitude of the intensity vector and dividing each component of the intensity vector by the respective magnitude.","The process constructs () a lookup table for each pixel using the normalized vectors corresponding to the pixel. Each lookup table associates () each respective normalized vector in the table with the respective depth of the respective simulated surface. Some implementations use this lookup table as described below with respect to the process  illustrated in .",{"@attributes":{"id":"p-0267","num":"0321"},"figref":"FIGS. 23A-23C","b":["2300","2302","2304","900","2304","2306","2308"]},"In some implementations, the process  detects () a trigger event. In some implementations, creating the depth map of the first scene is () in response to detecting the trigger event. In some implementations, the first scene includes () a first object positioned at a first location within the first scene and the process  detects () the first object positioned at a second location within the first scene, where the second location is distinct from the first location. The movement of the first object triggers the building of the depth map. In some implementations, the trigger event is () a power outage (e.g., build or rebuild the depth map when the computing device reboots).","In some implementations, the process  switches () the mode of operation of the camera system when building the depth map. For example, some implementations switch () from a first mode of the camera system to a second mode of the camera system, including deactivating the first mode and activating the second mode. In some implementations, the array of image sensors has () an associated first pixel gain curve when the first mode is activated, and the array of image sensors has () an associated second pixel gain curve when the second mode is activated.","For each of a plurality of distinct subsets of IR illuminators of the camera system, the process  performs () a set of operations. In some implementations, one or more of the subsets of the IR illuminators consists () of a single IR illuminator. In some implementations, the plurality of IR illuminators are orientated () at a plurality of distinct angles relative to the array of image sensors. In some implementations, each of the distinct subsets of IR illuminators comprises () two adjacent IR illuminators, and the distinct subsets of IR illuminators are () non-overlapping. One of skill in the art recognizes that various groupings, arrangements, and\/or configurations may be used for the IR illuminators.","The process  receives () a captured IR image of a first scene taken by a 2-dimensional array of image sensors of the camera system while the respective subset of IR illuminators are emitting IR light and the IR illuminators not in the respective subset are not emitting IR light. This occurs for each distinct subset of IR illuminators. The image sensors are partitioned () into a plurality of pixels. As noted above with respect to the process  in , the partitioning of image sensors into pixels can occur in various ways depending on the implementation. In some implementations, the process  receives () a baseline IR image of the scene captured by the array of sensors while none of the IR illuminators are emitting IR light. Some implementations subtract the light intensity from this baseline image from the light intensity in each of the other captured IR images, as illustrated above in .","For each of the pixels, the process  performs () several operations, including using () the captured IR images to form a respective vector of light intensity at the respective pixel. In some implementations, the respective vector for each pixel has () a plurality of components. Each of the components corresponds () to a respective IR light intensity for the respective pixel for a respective captured IR image. This is illustrated above in , where the vector {right arrow over (b)} has four components, corresponding to the four illumination groups  illustrated in . In some implementations, forming each respective vector of light intensity at a respective pixel comprises () subtracting a light intensity at the pixel in the baseline IR image from the light intensity at the pixel in each of the captured IR images, as illustrated in . In this way, the vector measures the additional light that is received at the image sensor array  based on reflections of light emitted from each of the illumination groups.","For each pixel (), the process  then estimates () a depth in the first scene at the respective pixel by looking up the respective vector in a respective lookup table. In some implementations, the process looks up () the respective vector in the respective lookup table by computing () an inner product of the respective vector with records in the lookup table. One of skill in the art recognizes that in a vector space an inner product can be used to measure the extent to which a pair of vectors are pointing in the same direction. In some instances, the inner product is () an ordinary dot product. In some implementations, the process  computes () the inner product of the respective vector with each respective record in the respective lookup table. In some implementations, fewer than all of the inner products are computed for the lookup table (e.g., based on optimization techniques, such as recognizing that certain records in the lookup table would produce smaller inner products than some inner products that are already computed).","In some implementations, the process  determines () the depth in the first scene at the pixel as the depth corresponding to a record in the lookup table whose inner product with the respective vector is greatest among the computed inner products for the respective vector. This is illustrated above with respect to .","In some implementations, the respective lookup table is generated () during a calibration process at the camera . In some implementations, the calibration process includes () simulating a virtual planar surface at a plurality of respective depths in the first scene. In some implementations, the calibration process includes (), for each pixel and each respective depth, determining an expected reflected light intensity. In some implementations, each respective lookup table is downloaded () to the camera system  from a remote server during an initialization process prior to creating the depth map.","In some implementations, each respective lookup table includes () a plurality of normalized light intensity vectors, where each normalized light intensity vector corresponds to a respective depth in the first scene. This is illustrated above in , , A, and B.","Although lookup tables have been identified separately for each pixel, one of skill in the art recognizes that the separate logical lookup tables are not necessarily stored as separate files or databases. For example, some implementations store all of the lookup tables as a single physical table in a relational database or as a single physical file on a file server. In some implementations, the totality of lookup tables is stored as a small number of distinct files. As described above, implementations generate and use the lookup tables on various devices depending on the capabilities of the camera system , available network bandwidth, and other resources. For example, for camera systems with limited processing power and\/or storage, some implementations build and use the lookup tables at a scene understanding server . The camera system  captures the IR images (e.g., baseline image plus additional images with different sets of illuminators on), and transmits them to the server . The server then constructs the depth map. In some implementations, the lookup tables are constructed at the server  based on the depth simulations and knowledge of the camera configuration, and then downloaded to the camera. In some of these implementations, the camera  uses the lookup tables itself to build a depth map.",{"@attributes":{"id":"p-0278","num":"0332"},"figref":"FIGS. 24A-24C","b":["2400","2402","2404","900","2404","2406","2408"]},"The process receives () a captured IR image of a scene taken by a 2-dimensional image sensor array of a camera system while one or more IR illuminators of the camera system are emitting IR light, thereby forming an IR intensity map of the scene with a respective intensity value determined for each pixel of the IR image. Typically, the IR image is captured at night, so most of the intensity is based on reflection of the light from the IR illuminators. Typical surfaces disperse light in all directions, so some of the emitted light is reflected back to the image sensor array. For a specular surface, however, such as a window, mirror, or some television screens, the incoming light at a surface is reflected off primarily in one direction, with the angle of incidence equal to the angle of reflection. A specular region therefore typically has low intensity in the IR intensity map.","The pixels in the IR intensity map can correspond to the image sensors in the array  in various ways, as previously illustrated with respect to  (boxes -). In some implementations, each pixel of the IR image corresponds () to a unique respective image sensor in the image sensor array. In some implementations, the pixels of the IR image form () a partition of the image sensors in the image sensor array. In some of these implementations, at least one pixel corresponds () to a plurality of image sensors in the image sensor array.","Typically, the camera system  includes () a plurality of IR illuminators, as illustrated above in . In some implementations, the process  constructs the IR intensity map from multiple distinct IR images. For example, in some implementations, the process receives () a respective IR sub-image of the scene for each of a plurality of distinct subsets of IR illuminators of the camera system. Each sub-image is captured () while illuminators in a respective subset are emitting IR light and the IR illuminators not in the respective subset are not emitting IR light. The process  computes () an average of the intensity values at the pixel in each of the sub-images to determine the intensity value for the pixel.","The process uses () the IR intensity map to identify a plurality of pixels whose corresponding intensity values are within a predefined intensity range. In some implementations, the predefined intensity range is () all intensity values below a threshold value. This is the intensity range typically used when the goal is to identify windows. Some implementations use other ranges to identify other specific materials.","The process  clusters () the identified plurality of pixels (i.e., the pixels identified based on the intensity range) into one or more regions that are substantially contiguous. This is illustrated above with respect to . Some implementations use other factors in the clustering process as well. For example, some implementations set a threshold size for a region. Small regions of low intensity are either combined with other nearby regions or ignored. In some implementations, clustering the identified plurality of pixels into one or more regions uses () a depth map that was constructed using the image sensor array. For example, when trying to identify windows, a window should be continuous. A single region with two or more significantly disparate depths is not likely to be a window. In some implementations, clustering the identified plurality of pixels into one or more regions uses () an RGB image of the scene captured using the image sensor array. For example, evaluating the color distribution of a region can identify some regions that are unlikely to be windows (e.g., the presence of certain colors or the number of distinct colors).","The process  determines () that a first region of the one or more regions corresponds to a specific material based, at least in part, on the intensity values of the pixels in the first region. In some implementations, determining that a first region of the one or more regions corresponds to a specific material includes () determining that the first region is substantially a quadrilateral. This is illustrated by the quadrilaterals  and  in  above, and the quadrilaterals  and  in . In some implementations, the first region is () substantially a quadrilateral when a total absolute difference in area between the first region and the quadrilateral is less than a threshold percentage of the quadrilateral's area (e.g., less that 10% of the area of the quadrilateral). In some implementations, the specific material is () glass and the first region is determined to correspond to a window in the scene. In some implementations, the region is identified as a probable window candidate, which is subsequently confirmed either by a user or other independent criteria.","Once a region has been classified, the process  stores () information in the memory that identifies the region. The information can be stored in various ways. In some implementations, the process  stores coordinates for the region, such as coordinates of a centroid, or coordinates of a subset of points along the boundary. In some implementations, the process  creates a two-dimensional scene map corresponding to the pixels, and specifies a value (e.g., a number or a character) to identify the object\/material\/function for each pixel. For example, in some implementations, a value of 0 indicates no information, a value of 1 indicates a probable window, 2 indicates a probable floor, 3 indicates a probable wall, and 4 indicates a probable ceiling. Usage of a scene map is illustrated in  below. Identification of floors, walls, and ceilings is described above with respect to  and below with respect to . Some implementations use characters instead of numbers, such as a \u201cW\u201d to indicate a probable window, an \u201cF\u201d to indicate a probable floor, and a blank space if there is no information about a possible object at the pixel.","In some implementations, the process  receives () a video stream of the scene from the camera system and reviews () the video stream to detect movement in the scene. Movement in the scene can be used to identify possible intruders in a home or other potential problems. In some implementations, the first region is excluded () from movement detection. For example, if the first region is identified as a window, movement in the window region may be movement on the other side of the window (e.g., outside), and thus not suitable for a motion alert. In another example, the first region is a television set, and thus \u201cmotion\u201d in the region is typically based on displayed television images rather than real motion at the scene. In some implementations, the process  generates () a motion alert when there is motion detected at the scene outside of the first region.",{"@attributes":{"id":"p-0287","num":"0341"},"figref":"FIGS. 25A-25B","b":["2500","2502","2504","900","2504","2506","2508"]},"The process  receives () a plurality of captured IR images of a scene taken by a 2-dimensional array of image sensors of a camera system. Each IR image is captured () when illuminators in a distinct subset of IR illuminators of the camera system  are emitting light. In some implementations, the image sensors are partitioned () into a plurality of pixels. As described above with respect to  (e.g., boxes -), implementations group the image sensors into pixels in various ways.","The process  constructs () a depth map of a scene using the plurality of IR images. Some implementations use a process as described in  (process ) to construct the depth map. In some implementations, for each pixel the process  performs () a set of operations. In some implementations, the set of operations includes using () the captured IR images to form a respective vector of light intensity at the respective pixel. In some implementations, the set of operations includes estimating () a depth in the first scene at the respective pixel using the respective vector and a respective lookup table. In some implementations, lookup tables are constructed using a process as described in  (process ).","The process  uses () the depth map to compute a binary depth edge map  for the scene. The binary depth edge map  identifies () which points in the depth map comprise depth discontinuities. This is illustrated in  above. The process  then identifies () a plurality of contiguous components based on the binary depth edge map. This is illustrated in  above. Depth discontinuities create boundaries between components.","The process then determines () that a first component of the plurality of contiguous components represents a large planar surface in the scene. This determination involves a few steps. A first step is to fit () a plane to the points in the first component. In some implementations, the fitting uses least squares to find the best plane for the data in the component. Some implementations use other techniques to identify a \u201cbest\u201d plane for the data, such as minimizing the sum of absolute differences between a hypothetical plan and the points in the component. Implementations typically use a sampling of data points from a component to fit the best plane. For example, some implementations use 50 or 100 sample data points from a component.","In making the determination that the first component represents a large planar surface, the process also confirms that the \u201cbest\u201d plane is actually a good plane for the data. In some implementations, the process  determines () that the plane fitting residual error is less than a predefined threshold. In some implementations, the plane fitting residual error is the sum of the absolute differences between the plane and the sample points in the component. In some implementations, the plane fitting residual error is the sum of the squares of the differences between the sample points and the plane, or the square root of the sum of the squares. In some implementations, the plane fitting residual error is the maximum absolute difference between the sample points and the plane. Some implementations use two or more techniques to confirm that the residual error is small (e.g., the maximum absolute error is less than a first threshold and the sum of the absolute errors is less than a second threshold).","Once the plane is fitted and it is determined that the residual error is sufficiently small, the first component is identified as a large planar surface. The process  then analyzes the plane to determine whether the surface is likely to be a floor, a ceiling, or a wall. To make this determination, some implementations determine () the orientation of the plane. This is illustrated above with respect to . When the orientation of the plane is upwards, the process  determines () that the plane is probably a floor. When the orientation of the plane is downwards, the process  determines () that the plane is probably a ceiling. When the orientation of the plane is horizontal, the process  determines () that the plane is probably a wall.","Some implementations use other criteria as well in making the determination that a component represents a large planar surface. For example, some implementations require the component to have a minimum threshold area to be classified as a probable floor, wall, or ceiling.",{"@attributes":{"id":"p-0295","num":"0349"},"figref":"FIGS. 26A-26C","b":["2600","2602","2604","900","2604","2606","2608"]},"The process  receives () a first RGB image of a scene taken by a 2-dimensional array of image sensors of a camera system at a first time. The RGB image identifies what is in the field of vision of the camera. The process also receives () a first plurality of distinct IR images of the scene taken by the array of image sensors temporally proximate to the first time. In general, the temporal proximity ensures that the field of vision of the camera while capturing the IR images is substantially the same as the field of vision of the camera while capturing the RGB image. Commonly, the RGB image is captured during daylight hours, whereas the IR images are captured at night. In some implementations, temporal proximity means within 24 hours or 12 hours. Each of the IR images is taken () while a different subset of IR illuminators of the camera system is emitting light.","The process  uses () the first plurality of IR images to construct a first depth map of the scene, where the first depth map indicates a respective depth in the scene at a plurality of pixels. Some implementations use a process like the depth mapping process  described with respect to  to construct the first depth map. The pixels of the depth map correspond to the image sensors of the array. In some implementations, each pixel corresponds () to one or more image sensors. In some implementations, each pixel corresponds to a single image sensor. In some implementations, the process  partitions () the image sensors into a plurality of pixels. In some implementations, the process  forms () a respective vector of the received IR images for each pixel. For each pixel, the process  estimates () a depth in the scene at the respective pixel by looking up the respective vector in a respective lookup table. Some implementations use lookup tables constructed as described above with respect to the process  in .","A user designates () a zone within the RGB image. In some implementations, the designated zone is a region of interest, such as a region with special monitoring. In some implementations, the special monitoring consists of excluding the region from monitoring movement. In some implementations, an alert is triggered when there is movement in a designated zone. In some implementations, the zone corresponds () to a contiguous plurality of pixels. In some implementations, the zone is () a quadrilateral. In some implementations, the zone is a polygon. In alternative implementations, the user designates a zone within an IR image instead of within an RGB image.","The process  receives () a second plurality of distinct IR images of the scene taken by the array of image sensors at a second time that is after the first time. In some implementations, each of the IR images in the second plurality is captured () while a different subset of IR illuminators of the camera system is emitting light. Typically, the subsets of IR illuminators used to capture the second plurality of IR images are the same as the subsets of IR illuminators used to capture the first plurality of illuminators.","The process  then uses () the second plurality of IR images to construct a second depth map of the scene. The process  typically uses the same steps for building the second depth map as used for building the first depth map, which was described above with respect to boxes - in .","The process  then determines () physical movement of the camera system based on the first and second depth maps. In many cases, if there has been no movement of the camera, the second depth map is substantially the same as the first depth map. However, in some cases, objects in the scene itself change, such as placing a new item of furniture in the monitored area, placing new artwork on a wall, or even accumulated clutter on a floor.","In some instances, the determined physical movement is () an angular rotation. In some implementations, the determined physical movement is () a lateral displacement. For example, the camera may be bumped a little to the left or the right on a shelf. Note that lateral displacement can be a horizontal movement, a vertical movement, and\/or a movement forward or backward. In some implementations, a \u201clateral displacement\u201d is defined as any movement of the camera  in which the camera continues to point in the same direction (e.g., due east). In many cases, if the camera  is bumped or nudged, the physical movement includes () both an angular rotation and a lateral displacement.","In some implementations, the process  identifies () a plurality of points in the first depth map and a corresponding plurality of points in the second depth map. The process  then determines () a respective displacement for each of the identified points between the first and second depth maps. By combining the displacements for a plurality of distinct points, the process  determines the overall movement of the camera .","In some implementations, determining the movement of the camera uses point clouds. The process  forms () a first point cloud using a first plurality of points from the first depth map, and forms () a second point cloud using a second plurality of points from the second depth map. The process then computes () a minimal transformation that aligns the first point cloud with the second point cloud. One of skill in the art recognizes that correlating two point clouds can be performed in various ways. Based on the point cloud transformation, the process  identifies the motion of the camera  that would produce the point cloud transformation.","Based on the determined physical movement of the camera system , the process  translates () the zone in the first RGB image into an adjusted zone. When the zone originally designated by the user is a quadrilateral, the adjusted zone is () also a quadrilateral. However, because of the transformation, in some instances, a first edge of the quadrilateral has () a length that is different from a corresponding second edge of the second quadrilateral.","In some implementations, the process  receives () a second RGB image of the scene taken by the array of image sensors of the camera system temporally proximate to the second time. In some implementations, the process  correlates () the adjusted zone to a set of pixels from the second RGB image. This can be helpful to a user who wants to view the zones.",{"@attributes":{"id":"p-0307","num":"0361"},"figref":"FIGS. 27A-27D","b":["2700","2702","2704","900","2704","2706","2708"]},"The process  identifies () a plurality of distinct subsets of the IR illuminators. Subsequently, each of the distinct subsets of illuminators are activated one subset at a time, and the images captured with different illumination enables determination of the camera height and tilt angle. In some implementations, each of the distinct subsets of the IR illuminators comprises () two adjacent IR illuminators, and the distinct subsets of the IR illuminators are non-overlapping. In some implementations, each individual illuminator is one of the distinct subsets. For example, if a camera system has eight illuminators, some implementations have eight distinct subsets, consisting of each individual illuminator. In some implementations there is overlap between the distinct subsets. For example, in a camera system with eight illuminators, some implementations have eight distinct subsets corresponding to each possible pair of adjacent illuminators. One of skill in the art recognizes that many other selections of subsets of IR illuminators are possible.","The process  also partitions () the image sensors in the array into a plurality of pixels. In some implementations, each pixel comprises () a single image sensor. In other implementations, each pixel comprises () a plurality of image sensors. Typically, the image sensor array  has a large number of image sensors (e.g., a million or more). Implementations commonly downsample the images, combining multiple sensors into a single virtual pixel. In some implementations, each pixel includes about 100 image sensors (e.g., a 10\u00d710 contiguous square). In some implementations, each pixel corresponds to the same number of image sensors.","Before computing an actual camera position, implementations build a dictionary (also referred to as a training set). An example dictionary  is provided in  above. Typically, the dictionary is constructed once, and used many times. The dictionary is constructed based on characteristics of a specific camera, but there are generally many cameras that can use the same dictionary (e.g., a million instances of a single camera model can all use the same dictionary as long as the cameras are substantially identical). The dictionary consists of a plurality of entries, each corresponding to a (height, tilt angle) pair. The height and tilt angle represent the relationship of the camera (i.e., the image sensor array  of the camera) relative to a floor near where the camera is located. In some implementations, all of the (height, tilt angle) pairs are unique, but in other implementations, two or more dictionary entries have the same height and tilt angle. In some implementations, the dictionary entries are constructed based on simulation (e.g., simulating a specific height and tilt angle above a floor, and simulating illumination from the identified subsets of illuminators). In other implementations, the dictionary entries are constructed based on experimental data (e.g., placing the camera at various heights and tilts and capturing images based on activating the various identified subsets of illuminators).","For each of a plurality of heights and tilt angles, the process  constructs () a dictionary entry that corresponds to the camera system  having the respective height and tilt angle above a floor. The respective dictionary entry includes () respective IR light intensity values for pixels in images corresponding to activating individually each of the distinct subsets of the IR illuminators. For example, in some implementations with 15,000 pixels and four subsets of illuminators, each dictionary entry has a light intensity value for each of the 60,000 pixel\/subset combinations plus the height and tilt angle (e.g., a vector with 60,002 entries). In some implementations, the dictionary entries only include pixels that correspond to the simulated floor. For example, if there are 15,000 pixels for the entire sensor array, the simulated floor may occupy 3000 pixels, thus creating dictionary entries with 12,002 components (12,000 components corresponding to the pixel\/subset combinations, and two components for the height and tilt angle). Some implementations have about 100 dictionary entries (e.g., with height values of 0.0 meters, 0.3 m, 0.6 m, and tilt angles of \u221240\u00b0, \u221230\u00b0, \u221220\u00b0, . . . ). Some implementations include more entries to provide greater accuracy (e.g., height values every 0.1 meter and angles every 5 degrees).","In some implementations, the constructed dictionary entries are () based on simulating the camera, the floor, and the images, and computing expected IR light intensity values for pixels in the simulated images. In some implementations, each expected IR light intensity value is () based on characteristics of the IR illuminators. As noted previously, the characteristics may include () one or more of: lux, orientation of the IR illuminators relative to the array of image sensors, and location of the IR illuminators relative to the array of image sensors. In some implementations, a respective dictionary entry for a respective height and respective tilt angle is () based on measuring IR light intensity values of actual images captured by the camera having the respective height and respective tilt angle with respect to an actual floor.","In some implementations, the process  normalizes () each of the dictionary entries. In some implementations, this accounts for different surface reflectivity. In some implementations, the process  normalizes () each dictionary entry by determining () a respective total magnitude of the light intensity features in the respective dictionary entry and dividing () each component of the respective dictionary entry by the respective total magnitude. For example, with a dictionary entry having 12,002 elements, compute the total magnitude of the first 12,000 entries (corresponding to light intensity at pixels) and divide each of those 12,000 entries by the total magnitude. If the light intensity features are labeled x, x, . . . , x, then in some implementations the total magnitude is \u221a{square root over (\u03a3(x))}.","In some implementations, the dictionary entries are constructed at a computing device that is distinct from the camera system, then downloaded () to the camera system from the computing device during an initialization process. In some implementations, the subsequent determination of height and tilt angle is calculated at the camera system , even when the building of the dictionary is performed at a separate computing device (e.g., a scene understanding server ).","For each of the plurality of distinct subsets of the IR illuminators, the process  receives () a captured IR image of a scene taken by the array of image sensors while the respective subset of the IR illuminators are emitting IR light and the IR illuminators not in the respective subset are not emitting IR light. In some implementations, the process  receives () a baseline IR image of the scene captured by the array of image sensors while none of the IR illuminators are emitting IR light, and subtracts () a light intensity at each pixel of the baseline IR image from the light intensity at the corresponding pixel of each of the other captured IR images. This can provide a better estimate of the light intensity due to the IR illuminators.","The process uses () at least one of the captured IR images to identify a floor region corresponding to a floor in the scene. Some implementations use the techniques illustrated above in  and A-B to identify a floor region. For example, in some implementations the process  constructs () a depth map of the scene using the captured IR images. In some implementations, the process  then identifies () a region bounded by depth discontinuities. This is illustrated above in . In some implementations, the process  also determines () that the region is substantially planar and facing upwards.","The process  then forms () a feature vector including pixels from the captured IR images in the identified floor region. This is illustrated in . Typically, the components of the feature vector are arranged in the same order as the components of the dictionary entries.","The process then estimates () a camera height and camera tilt angle relative to the floor by comparing () the feature vector to the dictionary entries. In some implementations, the process  normalizes () the feature vector and the dictionary entries prior to computing the distances.","In some implementations, the process  computes () a respective distance between the feature vector and respective dictionary entries, and selects () a first dictionary entry whose corresponding computed distance is less than the other computed distances. In some implementations, computing the distance between a feature vector and respective dictionary entries comprises () computing a Euclidean distance that uses only vector components corresponding to pixels in the identified floor region. This is illustrated in . For example, the actual floor may have some objects on it, such as furniture or toys. The floor identification process typically excludes these objects because they are not part of the planar surface. Because the process  determines the height and angle relative to the floor, only pixels that correspond to the floor region are relevant. The process  estimates () the camera height and tilt angle to be the height and tilt angle associated with the first dictionary entry.","Some implementations expand or modify this basic process in various ways. In some implementations, the process  identifies a ceiling rather than a floor, and measures the \u201cheight\u201d and tilt angle relative to the ceiling. As noted above in  and A-B, the processes described with respect to a floor can be used for a ceiling as well. In this case, the dictionary entries are constructed relative to the ceiling. In some implementations, the position of the camera is computed both with respect to a floor and with respect to a ceiling. A side effect of this dual calculation is to estimate the height of the room where the camera is located.","As noted above, the data for the dictionary entries can be constructed by simulation or by experiments with an actual camera. When formed by experimentation, some implementations capture a baseline image for each camera position, and subtract the baseline from the other captured images with each of the subsets of illuminators activated. Alternatively, the experiments are performed in a room with no ambient light so that each captured image represents only light coming originally from the activated illuminators. The size of the dictionary can be selected based on the desired accuracy.","In some instances, multiple \u201cfloor\u201d regions are identified. In some of these instances, the multiple regions are different portions of the same floor. In other instances, one or more of the regions may be tables and one or more regions may be an actual floor. Some implementations estimate the height and tilt angle based on each of the identified regions, then compare the multiple results. If they are all approximately the same, some implementations estimate the height and tilt based on all of them (e.g., by averaging the values, taking the values associated with the largest region, or choosing the first one). When the heights are substantially different, some implementations take the larger estimate, guessing that the smaller height estimate is based on a table or other planar object above the floor. Note that the process is only an estimate. If the camera is sitting on a table and the floor is not in the field of vision of the camera, the estimated height will be the height above the table.","Some implementations use interpolation to provide a finer estimate. For example, in some instances the feature vector has equally small distances from two dictionary entries. In some implementations, the estimated height and tilt angle are based on averaging these two closest entries. In some implementations, finding the matching dictionary entry uses a nearest neighbor algorithm. In some implementations, only the single nearest neighbor is used. In some implementations, the k nearest neighbors are used for a fixed small positive integer k, and a weighted average of these neighbors is used to compute the height and tilt angle of the camera. For example, in some implementations, the k nearest entries are selected, and each is weighted based on the inverse of its distance from the feature vector.",{"@attributes":{"id":"p-0324","num":"0378"},"figref":["FIG. 28","FIGS. 13","FIGS. 21A-21C"],"b":["14","15","15","22","22","27","27"]},"In the data acquisition phase , the camera  captures () IR images while controlling which IR illuminators are on. In some implementations, the images are captured at night, and may occur multiple times each night (e.g., every hour). In some implementations, the camera  receives a command from the video server system  or scene understanding server  to collect the images. Before taking the images, the camera typically locks auto exposure so that all of the captured images are taken with the same parameter settings.  illustrates an example where the illuminators are grouped into adjacent pairs. In general, an additional IR image is taken with none of the illuminators active in order to determine the ambient light.","For cameras with substantial processing power and memory, subsequent processing may be performed at the camera. However, the data is commonly transmitted to a separate server for the data processing phase , which commonly occurs at a video server system  or a scene understanding server . In some implementations, the data is transmitted from the camera to an external computing device in a native format (e.g., five IR images). In some implementations, some processing occurs on the camera before it is transmitted. For example, in some implementations, the images are downsampled at the camera, which reduces the amount of data transmitted. In some implementations, the captured background image is subtracted from the other images, so the data transmitted corresponds to light from the IR illuminators, and the background light is already canceled out. In some implementations, the data is transmitted as a single long array of data, such as the feature vector  in . In some implementations, the components of the transmitted data are arranged differently, such as grouping together the data for each pixel (e.g., placing a, a, a, and afrom the feature vector  together).","In some implementations, the scene understanding server  includes a depth mapping module , which computes () a 3-D depth map of the scene in the field of vision of the camera. Constructing a depth map is described above with respect to , A, B, and A-C. The depth map information is passed on to various scene understanding processes , such as object classifiers , a camera pose estimator , or a zone correction module . These processes compute or determine various information about the scene. Both the depth information and the scene information are passed on to the computer vision engine . In some implementations, the computer vision engine  uses the information to provide better alerts. For example, the computer vision engine  can reduce the number of false security alerts by excluding certain regions or by performing automatic zone correction when a camera is moved slightly. In some implementations, this data facilitates motion tracking and detection of humans. The data processing phase is described in more detail with respect to .",{"@attributes":{"id":"p-0328","num":"0382"},"figref":["FIG. 29","FIGS. 16A-16D","FIGS. 20B"],"b":["2902","878","2902","878","2912","17","17","23","23","878","2914","20","20","878","2916","878","2918"]},"The second process  identifies large planar regions, such as floors, walls, and ceilings. This process is described above with respect to  and A-B. The floor\/wall\/ceiling module  is also referred to as the planar support detection engine. The floor\/wall\/ceiling module  uses as inputs the depth map  and the depth edge map , and identifies regions that likely correspond to floors, walls, or ceilings. In some implementations, the floor\/wall\/ceiling module  labels the pixels of a scene image (either an RGB image or an IR image) as probable floors, walls, or ceilings. This is illustrated below in .","The third process  performs zone correction, as described above with respect to  and A-C. The zone correction module  uses depth maps constructed at two different times, as well as a user-defined zone. When the zone has changed slightly, the zone correction module  recommends an updated zone, which is typically presented to the user for verification. In some implementations, if the camera has moved significantly (e.g., to another room), the zone correction module recommends removing the zone.","The fourth process  identifies specular regions in a scene, which generally correspond to windows, televisions, or sliding glass doors. This process is described above with respect to  and A-C. The window detection module  is sometimes referred to as a specular region identification engine. The window detection module  uses the depth map  and the active IR brightness image  to identify regions that are probable windows, and typically uses other information to make such a confirmation. For example, in some implementations, the window detection module  uses the size of the region (e.g., is it too big or too small to be a likely window). In some implementations, the window detection module  uses the shape of the region based on the empirical fact that most windows are rectangular. Based on some distorting effects, an object that is rectangular generally appears as a quadrilateral in an image, and thus some implementations do quadrilateral fitting for windows.","The information provided by the scene understanding server can be used in various ways to reduce false motion alerts. For example, an identified specular region (identified as a possible window), may be a television set. In some implementations, a rectangular specular region that includes lots of motion is identified as a probable television. When a television is identified, \u201cmovement\u201d within the television region that would otherwise create a false motion alert can be avoided. In some implementations, false motion alerts from ceilings can be avoided as well. Typically, \u201cmotion\u201d on a ceiling is caused by lights, such as headlights from cars, and should not trigger a motion alert.","Some implementations are able to identify other characteristics of the camera location as well. For example, some implementations determine whether the camera is inside or outside (e.g., based on the presence of a ceiling). When a camera is inside, some implementations determine whether the room is a small room or a large room. These characteristics can help determine when to create motion alerts. For example, when a camera is outside, there are many regions where motion would be expected (e.g., plants or trees flowing with the wind). Therefore, motion detection may be limited to very specific areas and\/or set at a high threshold for what triggers a motion event. In some implementations, the information about the camera environment (e.g., floors and windows) is used to make recommendations on where to place the camera and\/or to recommend zones for more detailed monitoring. For example, in , the camera appears to be sitting on or close to a floor. In some implementations, the system recommends placing the camera at a higher location.",{"@attributes":{"id":"p-0334","num":"0388"},"figref":["FIG. 30","FIG. 30"],"b":["900","3002","3002","3006","3006","1","3006","3004","852"]},"As illustrated in , some of the grid cells have information that identifies the type of object believed to be in the cell. For example, the upper right grid cell - is encoded with a \u201cC\u201d to indicate that it is believed to be part of a ceiling. In this example, there are several cells in a contiguous region  that are believed to be part of a ceiling. Although the region  is identified in the figure, implementations typically do not store a region definition with the grid . Instead, the encoded individual cells, such as the cell - provide the information.","Similarly, a group of cells including the cell - are encoded with a \u201cW,\u201d indicating that the cells are part of a probable window. The region  includes these cells. Also, on the left is a group  of cells that include the cell -, which is identified as a probable wall. In some implementations, an individual cell can be labeled with at most object type, but in other implementations, each cell can have two or more designations. For example, the dark region  in  appears to be a window, but it is also part of a door. In some implementations, the designations of \u201cdoor\u201d and \u201cwindow\u201d are compatible, so both are included. In some implementations, when there are two or more designations (which are potentially incompatible), each of the designations has an associated probability.","Although the grid  in  shows only probable designations of objects in the monitored scene, some implementations provide additional information with the grid cells. For example, in some implementations, each pixel has an associated IR and\/or RGB image value. In some implementations, each grid cell  includes the estimated depth from the computed depth map . In some implementations, the grid cells encode a computed depth edge map  as well, such as the depth map  in . In general, whatever features are computed for individual pixels are stored in the grid .","Some implementations provide zone correction, as illustrated in  and A-C above. Some implementations address camera movement more generally, recognizing that there are both small moves (e.g., a bump) and large moves (e.g., taking the camera to a different room). In a small move, the camera sees substantially the same field of vision, as illustrated in . In this case, an activity zone can generally be adjusted. In a large move, the camera sees a substantially different field of vision. The previously defined activity zone is now irrelevant to the current field of vision, so the zone should be discarded. Whether a small move or a large move, some implementations issue a camera move alert so that the user can take appropriate action. Some implementations use push notifications to alert the user of a camera move event, but other implementations use pull notifications, allowing the user to receive a camera move event only when requested. Some implementations support both push and pull notifications, and select the type based on the importance. For example, some implementations use push notifications when there is a detected motion event (e.g., a possible intruder), but use pull notifications for camera move events. Some implementations track the history of camera move events, and provide the user with access to that history. In some implementations, each camera move event has additional data that is stored. For example, some implementations store the model of the camera, the software or firmware version, the existing activity zones, an identifier for the camera when a household has more than one camera, one or more timestamps to indicate when the camera moved, the recommended action, and so on.",{"@attributes":{"id":"p-0339","num":"0393"},"figref":["FIGS. 31A and 31B","FIG. 31A","FIG. 31B","FIGS. 19A-19I","FIGS. 31C and 31D","FIG. 31C","FIG. 31D"],"b":["26","26","3102","3104"]},{"@attributes":{"id":"p-0340","num":"0394"},"figref":["FIG. 31E","FIGS. 19A-19I"],"b":["3120","928","3124","3124","3124","3122","3120"]},"In situations in which the systems discussed here collect personal information about users, or may make use of personal information, the users may be provided with an opportunity to control whether programs or features collect user information (e.g., information about a user's social network, social actions or activities, profession, a user's preferences, or a user's current location), or to control whether and\/or how to receive content from the content server that may be more relevant to the user. In addition, certain data may be treated in one or more ways before it is stored or used, so that Personally Identifiable Information (\u201cPII\u201d) is removed. For example, a user's identity may be treated so that no PII can be determined for the user, or a user's geographic location may be generalized where location information is obtained (such as to a city, ZIP code, or state level), so that a particular location of a user cannot be determined. Thus, the user may have control over how information is collected about the user and used by a content server.","It is to be appreciated that one or more implementations disclosed hereinabove is particularly advantageous for application in the home monitoring context, for which there are particular combinations of desirable goals including low cost hardware, very low device power (especially for battery-only devices), low device heating, nonintrusive device operation, ease of device installation and configuration, tolerance to intermittent network connectivity, low-maintenance or maintenance-free device operation, long device lifetimes, the ability to operate in a variety of different lighting conditions, and so forth, the home monitoring context further involving particular sets of expected target characteristics and\/or constraints for which the preferred implementations may be particularly effective, such as the statistically prominent presence of certain target types (humans, pets, houseplants, ceilings, floors, furniture, doors, windows, household fixtures, various household items, etc.), the fact that the monitoring device is usually stationary relative to the monitored space, the fact that certain target types have certain expected ranges of sizes and characteristics (e.g., humans and pets have certain sizes and any movement is usually parallel to a floor or stairway; floors-ceilings-walls are also usually of certain size or height ranges and are stationary; doors-windows rotate or slide within expected ranges; furniture is usually stationary and has certain expected sizes), and so forth. However, it is to be appreciated that the scope of the present teachings is not so limited, with other implementations being applicable for the monitoring of other types of structures (e.g., multi-unit apartment buildings, hotels, retail stores, office buildings, industrial buildings) and\/or to the monitoring of any other indoor or outdoor facility or space. It is to be still further appreciated that, while facility or space monitoring represents one particular advantageous application, the scope of the present teachings can further be applicable to any field in which automated machine characterizations of stationary or moving objects, facilities, environments, persons, animals, or vessels, are desired based on optical, ultraviolet, or infrared electromagnetic reflection or emission characteristics.","It will also be understood that, although the terms first, second, etc. are, in some instances, used herein to describe various elements, these elements should not be limited by these terms. These terms are only used to distinguish one element from another. For example, a first user interface could be termed a second user interface, and, similarly, a second user interface could be termed a first user interface, without departing from the scope of the various described implementations. The first user interface and the second user interface are both user interfaces, but they are not the same user interface.","The terminology used in the description of the various described implementations herein is for the purpose of describing particular implementations only and is not intended to be limiting. As used in the description of the various described implementations and the appended claims, the singular forms \u201ca,\u201d \u201can,\u201d and \u201cthe\u201d are intended to include the plural forms as well, unless the context clearly indicates otherwise. It will also be understood that the term \u201cand\/or\u201d as used herein refers to and encompasses any and all possible combinations of one or more of the associated listed items. It will be further understood that the terms \u201cincludes,\u201d \u201cincluding,\u201d \u201ccomprises,\u201d and\/or \u201ccomprising,\u201d when used in this specification, specify the presence of stated features, integers, steps, operations, elements, and\/or components, but do not preclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and\/or groups thereof.","Although some of various drawings illustrate a number of logical stages in a particular order, stages that are not order dependent may be reordered and other stages may be combined or broken out. While some reordering or other groupings are specifically mentioned, others will be obvious to those of ordinary skill in the art, so the ordering and groupings presented herein are not an exhaustive list of alternatives. Moreover, it should be recognized that the stages could be implemented in hardware, firmware, software or any combination thereof.","The foregoing description, for purpose of explanation, has been described with reference to specific implementations. However, the illustrative discussions above are not intended to be exhaustive or to limit the scope of the claims to the precise forms disclosed. Many modifications and variations are possible in view of the above teachings. The implementations were chosen in order to best explain the principles underlying the claims and their practical applications, to thereby enable others skilled in the art to best use the implementations with various modifications as are suited to the particular uses contemplated."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["For a better understanding of the various described implementations, reference should be made to the Description of Implementations below, in conjunction with the following drawings in which like reference numerals refer to corresponding parts throughout the figures.",{"@attributes":{"id":"p-0080","num":"0079"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0081","num":"0080"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0082","num":"0081"},"figref":["FIG. 3","FIG. 1"]},{"@attributes":{"id":"p-0083","num":"0082"},"figref":["FIG. 4","FIG. 3"]},{"@attributes":{"id":"p-0084","num":"0083"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0085","num":"0084"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0086","num":"0085"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0087","num":"0086"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0088","num":"0087"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0089","num":"0088"},"figref":"FIGS. 10-12"},{"@attributes":{"id":"p-0090","num":"0089"},"figref":"FIGS. 13","b":["14","15","15"]},{"@attributes":{"id":"p-0091","num":"0090"},"figref":"FIGS. 16A-16D","b":["17","17"]},{"@attributes":{"id":"p-0092","num":"0091"},"figref":"FIGS. 18A-18E"},{"@attributes":{"id":"p-0093","num":"0092"},"figref":"FIGS. 19A-19I"},{"@attributes":{"id":"p-0094","num":"0093"},"figref":"FIGS. 20A-20K"},{"@attributes":{"id":"p-0095","num":"0094"},"figref":"FIGS. 21A-21E"},{"@attributes":{"id":"p-0096","num":"0095"},"figref":"FIGS. 22A-22C"},{"@attributes":{"id":"p-0097","num":"0096"},"figref":"FIGS. 23A-23C"},{"@attributes":{"id":"p-0098","num":"0097"},"figref":"FIGS. 24A-24C"},{"@attributes":{"id":"p-0099","num":"0098"},"figref":"FIGS. 25A-25B"},{"@attributes":{"id":"p-0100","num":"0099"},"figref":"FIGS. 26A-26C"},{"@attributes":{"id":"p-0101","num":"0100"},"figref":"FIGS. 27A-27D"},{"@attributes":{"id":"p-0102","num":"0101"},"figref":"FIGS. 28-30"},{"@attributes":{"id":"p-0103","num":"0102"},"figref":"FIGS. 31A-31E"}]},"DETDESC":[{},{}]}
