---
title: Cognitive control framework for automatic control of application programs exposure a graphical user interface
abstract: A cognitive control framework system for automatically controlling execution of an application program having a graphical user interface includes a recording component, an execution scenario script, and a playback component. The recording component is adapted to capture user input data and images displayed by the graphical user interface during a recording phase of execution of the application program, and to analyze the captured user input data and displayed images to generate an execution scenario (script) during the recording phase. The execution scenario may be written in a selected high level language (e.g., XML). The playback component is adapted to generate simulated user input data based on the execution scenario during a playback phase of execution of the application program.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08401221&OS=08401221&RS=08401221
owner: Intel Corporation
number: 08401221
owner_city: Santa Clara
owner_country: US
publication_date: 20050610
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATION","BACKGROUND","DETAILED DESCRIPTION"],"p":["This application is a National Phase application of, and claims priority to, International Application No. PCT\/RU2005\/000325, filed Jun. 10, 2005, entitled \u201cA COGNITIVE CONTROL FRAMEWORK FOR AUTOMATIC CONTROL OF APPLICATION PROGRAMS EXPOSING A GRAPHICAL USER INTERFACE\u201d.","1. Field","The present invention relates generally to automatic control of software application programs and image analysis and, more specifically, to analyzing graphical user interface (GUI) images displayed by an application program for automatic control of subsequent execution of the application program.","2. Description","Typical application program analysis systems capture keyboard input data and mouse input data entered by a user. The captured input data may then be used to replay the application program. These systems rely on playback of the application program on the same computer system used to capture the input data, and thus are not portable.","Some existing application program analysis systems use image recognition techniques that are dependent on screen resolution and\/or drawing schemes, or have strong dependencies to the underlying operating system (OS) being used. Such systems typically rely on dependencies such as Windows32 or X-Windows application programming interfaces (APIs). This limits their portability and usefulness.","Hence, better techniques for analyzing the GUIs of application programs are desired.","An embodiment of the present invention is a cognitive control framework (CCF) for automatic control of software application programs that have a graphical user interface (GUI). Examples of such applications programs may be executed on current operating systems such as Microsoft Windows\u00ae and Linux, for example, as well as other operating systems. An embodiment of the present invention creates a system simulating a human user interacting with the GUI of the application program and using the GUI for automatic control of the application program without relying on dependencies such as specific graphical libraries, windowing systems, or visual controls interfaces or implementations. The CCF comprises an easy-to-use cross-platform tool useful for GUI testing based on pattern recognition. By being independent of any OS-specific controls and graphical libraries, the CCF may be used for interaction with non-standard graphical interfaces as well as with well known ones. The system provides for recording any kind of keyboard and mouse actions the user performs while working with the GUI of the application program and then providing playback of the recorded scenario. In the present invention, image analysis of captured display data (such as screen shots, for example) is performed to identify actions of the application program corresponding to user input data. These actions and input data may be stored for use in future playback of the same user scenario for automatically interacting with the application program.","Embodiments of the present invention comprise operating on two phases: a recording phase and a playback phase. During the recording phase, the system is \u201clearning\u201d how to control the application program. The system registers and captures input actions supplied by the user (such as a mouse click or entering of text via a keyboard, for example) and display data (e.g. screen shots) of images displayed by the application program in response to those actions. The user actions, the time interval between actions, resulting display data of the GUI of the application program, and possibly other data and\/or commands form an execution scenario. By following the execution scenario, during the playback phase the system provides the same but fully automatic execution of the application program (simulating the user control but without the real presence of the user). Automatic execution is made possible due to a plurality of image analysis and structural techniques applied correspondingly to images during the recording and playback phases.",{"@attributes":{"id":"p-0020","num":"0019"},"figref":["FIG. 1","FIG. 1"],"b":["100","102","104","106","108","110","106","108","112"]},"At a later point in time, during the playback phase the playback component  may be initiated. At block , simulated user activity may be generated based on the execution scenario. That is, saved inputs and commands from the execution scenario may be input to the application program for purposes of automatic control using the CCF system. While the application program processes this data, display data may be changed on the display as a result. At block , the CCF system performs image analysis on the playback display data currently being shown as a result of application program processing and the display data captured during the recording phase. At block , recorded time conditions may be checked to take into account possible variations in playback. For example, the time when an object appears may be within a time interval based on a recorded time. For example, in one embodiment a lower bound time (time to start the search) may be extracted from the saved data in the execution scenario and an upper bound time may be the lower bound time plus 10%, or some other appropriate value. Processing of blocks , , and  each result in data being stored in report . At block , the CCF system controls execution of the application program based on the results of the image analysis. Blocks ,  and  may be repeated for each in a sequence of user input data items from the execution scenario.","The time interval between sequential actions is a part of the captured execution scenario. However, while following the execution scenario in the playback phase, one should not expect that the time interval between any two actions at playback will be equal to the time interval between the same two actions during the recording phase. There are a number of objective reasons why this interval could be different on playback than during recording. For example, the application program during recording and playback may be executed on different computer systems having different processor speeds, or an application program could require different times for the same actions during playback due to accesses of external data or resources. This indicates a requirement in the CCF system to handle flexible time conditions, e.g. handle some tolerance for the time interval between actions during the playback phase. During that time interval at playback, the system checks the recorded display data to the playback display data several times to determine if the playback display data is substantially similar to the recorded display data. A finding that the two are substantially similar indicates that a previous user action has completed and the system can progress to the next action in the execution scenario. This activity may be similar to the situation where the user is interacting with the application program and pauses periodically to view the display to determine if the expected visible changes to the display have been made by the application program based on previous actions. If so, then a new action may be performed. If at the end of a higher bound of the time interval the application program has not produced an image on the display that the CCF system expected according to the execution scenario, then the CCF system may interrupt the playback of the execution scenario and generate an error report describing how the execution scenario has not been followed. In one embodiment, the scenario may be corrected and the CCF system may be required to use other branches to continue.","The cognitive control framework (CCF) system of embodiments of the present invention performs image analysis and object detection processing on display data from the GUI of the application program. The CCF system includes comparing an image captured during a recording phase (called IR) to the corresponding image captured during the playback phase (called IP). One task of the system is to detect an object in the IR to which the user applied an action, find the corresponding object in the IP, and continue progress on the execution path of the execution scenario by applying the action to the detected object. These steps may be repeated for multiple objects within an image, and may be repeated across multiple pairs of IRs and IPs over time. An object that the user has applied an action to may be called an \u201cobject of action.\u201d Absence in the IP of the object of action corresponding to the one found at IR means that one should capture the IP again at a later time and try to find the object of action again. Finally, either an object of action may be found in the IP or execution of the scenario may be halted and a report generated describing how the wrong state was achieved and the scenario may not be continued. In embodiments of the present invention, this detection of objects of action may be done in real time during the playback phase, progressing from one action to another. Thus, the image analysis process employed must have good performance so as to introduce only a minimal disturbance to the time conditions at playback.","The CCF system of embodiments of the present invention comprises an image analysis and detecting process. Such a process has at least two requirements. First, the process should be able to overcome some variations in the captured images such as different color scheme, fonts, and the layout and state of the visual elements. In one embodiment, comparison constraints for checking these items (color scheme, fonts, etc.) may be set to specified parameters in accordance with specific needs. Overcoming these variations is desirable because recording and playback might be executed in different operating environments such as different screen resolutions, different visual schemes, different window layouts, and so on. Additionally, there could be insignificant differences in corresponding IR (usually captured after an action was applied to an object of interest) and IP pairs (captured after a previous action was completed). Second, the implementation of the image analysis and object detection process should be fast enough to introduce only minimal disturbances and delay of application execution during playback.","By processing captured images, the system builds descriptions of the images in terms of the objects presented on them. Each display object may be represented by its contour and a plurality of properties. Table I enumerates some possible contour properties for use in the present invention. In other embodiments, other properties may also be used.",{"@attributes":{"id":"p-0026","num":"0025"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE I"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Contour properties"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"161pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Property","Description"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]},{"entry":[{},"Location","Coordinates (on the image) of the contour center."]},{"entry":[{},"Image size","Characteristic contour size. In case of rectangular"]},{"entry":[{},{},"contours they are just vertical and horizontal sizes."]},{"entry":[{},{},"For controls of more complicated shape, another"]},{"entry":[{},{},"format may be used."]},{"entry":[{},"Layout","Connection to other contours that lay in proximity"]},{"entry":[{},{},"to its boundaries\/layout pattern of this contour."]},{"entry":[{},"Content","Indicates what is inside of the contour: text,"]},{"entry":[{},"Type","image or a combination."]},{"entry":[{},"Content","If the content type is text, then a text string;"]},{"entry":[{},{},"if image (e.g. icon), then the image."]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}]}}},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 2","b":["220","102","200","202","204","200","202","204"]},"Next, during the playback phase  handled by playback component , at block  the CCF system determines the contours of objects in the IP. At block , the CCF system filters contours by size to determine contours that may become hypotheses for active objects and contours that connect them. At block , the CCF system filters the objects by basic space layout in the IP to determine subsets of hypotheses for active and additional objects. For example, filtering criteria for space layout may include tables, wizards, and menus. In one embodiment, the user (or CCF schema with a cascade search) could set both strict (e.g. \u201cas is\u201d) and fuzzy (e.g. \u201cobject could be near each other\u201d) conditions. At block , the CCF system filters the objects by content to produce further subsets of hypotheses for active and additional objects. For example, the filtering criteria by content may include images and text. Moreover, in one embodiment, the user (or CCF schema with cascade search) could set both strict (e.g. \u201cimage should have difference in a few points and text should have minimal differences on a base of Levenstein distance\u201d) and fuzzy (e.g. \u201cimage could be stable to highlighting and have insignificant structural changes and text could have noticeable differences on a base of Levenstein distance without consideration of digits\u201d) conditions. At block , the CCF system performs structural filtering of the objects to produce a best hypothesis for active objects.","Finally, at block , the CCF system recalculates old actions for a new object by applying the action according to the execution scenario. For example, suppose the user selected (via the mouse) the screen location at (X=70, Y=200), and that a button is displayed at the rectangle denoted (X=50, Y=150, X=100, Y=100). In the IP, the button may be represented as a rectangle denoted (X=250, Y=300, X=200, Y=100). For a general view, coordinates of the top left corner and the size of the rectangle may be changed. The mouse click (user selection) may be recalculated based on the position of the button and the scaled size (for X and Y coordinates). The calculation gives the new mouse click coordinates (e.g., X=290, Y=350).","Table II shows the input data and output of the image analysis process for .",{"@attributes":{"id":"p-0031","num":"0030"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"266pt","align":"center"}},"thead":{"row":{"entry":"TABLE II"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Image Analysis Processing"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"4","colwidth":"112pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},{},"Input parameters and"]},{"entry":["Step","Input Data","Result","Description"]},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"266pt","align":"center"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"Working with image from recording phase (IR) by recording component"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"4","colwidth":"112pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["1. Contouring","Image from","Contours","Thresholds, distances between"]},{"entry":[{},"recoding (IR)",{},"objects (with some tolerance)."]},{"entry":[{},{},{},"Intel\u2009\u00ae OpenCV library used in one"]},{"entry":[{},{},{},"embodiment."]},{"entry":["2. Detecting","Image IR and","Contour","Typical object size (with tolerance)"]},{"entry":["object of","contours","representing","for object of action."]},{"entry":["activity","from","object of","Optical character recognition"]},{"entry":[{},"previous","activity","(OCR) and fuzzy text comparison,"]},{"entry":[{},"step.",{},"e.g. with Levenshtein distance."]},{"entry":["3. Detecting","Image IR,","Additional","Typical object size (with tolerance)"]},{"entry":["additional","contours and","objects and","for additional objects."]},{"entry":["objects","active","their layout","Structural analysis, e.g. \u201ccriss-"]},{"entry":["around object","objects.","against object","cross\u201d rules."]},{"entry":["of activity",{},"of action"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"266pt","align":"center"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"Working with image from playback phase (IP) by playback component"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"4","colwidth":"112pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["4. Contouring","Image from","Contours","Thresholds, distances between"]},{"entry":[{},"playback",{},"objects (with some tolerance)."]},{"entry":[{},"(IP)",{},"Intel\u2009\u00ae OpenCV library used in one"]},{"entry":[{},{},{},"embodiment."]},{"entry":["5. Filtering by","Contours","Contours that","Mean object size (with tolerance)"]},{"entry":["size","from","become","based on active object"]},{"entry":[{},"previous","hypotheses for","characteristics evaluated at Step 2."]},{"entry":[{},"step","active object","Typical object size (with tolerance)"]},{"entry":[{},{},"and contours","for additional objects."]},{"entry":[{},{},"connected with","Filtering out contours that don't fit"]},{"entry":[{},{},"them","into input size limits."]},{"entry":["6. Filtering by","Subsets of","Decreased","Fuzzy distance filtration. Fuzzy"]},{"entry":["basic space","hypotheses","subsets of","filtration for directions."]},{"entry":["layout","for active","hypotheses for"]},{"entry":[{},"and","active and"]},{"entry":[{},"additional","additional"]},{"entry":[{},"objects","objects"]},{"entry":["7. Filtering by","Subsets of","Decreased","OCR and fuzzy text comparison,"]},{"entry":["content","hypotheses","subsets of","e.g. with Levenshtein distance."]},{"entry":[{},"for active","hypotheses for","Fuzzy image comparison. Using"]},{"entry":[{},"and","active and","\u201cfuzzy content type\u201d method for"]},{"entry":[{},"additional","additional","filtration."]},{"entry":[{},"objects","objects"]},{"entry":["8. Structural","Subsets of","The best","Method based on fuzzy triple links"]},{"entry":["filtering","hypotheses","hypothesis for","both between objects from IR and"]},{"entry":[{},"for active","active objects.","their hypotheses from IP. It's stable"]},{"entry":[{},"and",{},"to additional objects which don't"]},{"entry":[{},"additional",{},"have strong structural links with"]},{"entry":[{},"objects",{},"active object. Moreover, one can"]},{"entry":[{},{},{},"use the result of this method to"]},{"entry":[{},{},{},"choose the best hypotheses for"]},{"entry":[{},{},{},"active objects. Some other"]},{"entry":[{},{},{},"methods, e.g. Hough"]},{"entry":[{},{},{},"transformation may also be used"]},{"entry":[{},{},{},"here."]},{"entry":["9. Recalculating","Object of","Applied the","Recalculating action coordinates in"]},{"entry":["old actions for","action","action","IP (playback image) coordinate"]},{"entry":["new object",{},"according to","system"]},{"entry":[{},{},"the execution"]},{"entry":[{},{},"scenario"]},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}}]}}]}}},"During filtering at each step there is an evaluation of specific contour properties (as required for a specific filter). This filtering pipeline is designed in such a way that the most time consuming evaluation steps are shifted to later in the processing pipeline when the number of contours (hypotheses) is smaller. By using this approach, the overall computational cost may be decreased, thereby helping to ensure good performance of the system.","It is useful to maintain a compromise in order to make sure that the system does not filter out some contours in the early steps that may be later determined to be either a hypothesis of an object of activity or objects connected with an object of activity. In this regard, predefined input parameters may be set to broad limits that requires spending a little more time on processing of additional contours (hypotheses), but ensure that the system has not dropped important contours.","Example pseudo-code for one embodiment of the present invention is shown in Table III.",{"@attributes":{"id":"p-0035","num":"0034"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"301pt","align":"center"}},"thead":{"row":[{"entry":"TABLE III"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Pseudo Code Example"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"301pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"BEGIN CCF"},{"entry":"<<<<<<<< Recording >>>>>>>>"},{"entry":"LOOP \/*recording, e.g. till a special key combination *\/"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"287pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Wait on user action \/*mouse, keyboard, it's possible to set something else*\/"]},{"entry":[{},"Hook and save screenshot \/*e.g. <Screenshot fileName=\u201d1.png\u201d\/>*\/"]},{"entry":[{},"Save time interval from the previous action \/*e.g. <Sleep duration=\u201d2000\u201d\/>*\/"]},{"entry":[{},"Save information about user action"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"301pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"\/*e.g. <Mouse action=\u201dRightClick\u201d x=\u201d100\u201d y=\u201d200\u201d\/>*\/"},{"entry":"END LOOP \/*recording, e.g. till a special key combination*\/"},{"entry":"EXIT"},{"entry":"<<<<<<<< Post-processing >>>>>>>"},{"entry":"Process saved data into a more compact form. It's possible for the user to change it for"},{"entry":"his or her needs."},{"entry":"<<<<<<<< Playback >>>>>>"},{"entry":"LOOP \/*till the end of saved data*\/"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"287pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Load time interval and wait in accordance with it."]},{"entry":[{},"IF [actions depend on coordinates on the screen] \/*e.g. mouse click*\/ THEN"]},{"entry":[{},"Load saved screenshot"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"273pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"Detect object of action \/*e.g. button*\/, nearest structure-layout \/*e.g. menu items"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"301pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"around button*\/ and other useful info on saved screenshot"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"273pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"TimeConditions_label: Hook the current screenshot"]},{"entry":[{},"Use image processing to find the corresponding object on the current screenshot \/*it's"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"301pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"possible to require more information from saved screenshot during search*\/"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"273pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"IF [Object not found] THEN"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"259pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"IF [Check time condition] \/*e.g. it's possible to repeat search 3 times with 1000-"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"301pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"msec step, for example*\/ THEN"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"245pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"GOTO TimeConditions_label"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"259pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"ELSE"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"245pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"EXIT with error code \/*moreover, it's possible to send corresponding report to log-"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"301pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"file*\/"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"231pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"END IF"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"273pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"ELSE"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"259pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"Recalculate actions on a base of new found objects \/*e.g. recalculate new"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"301pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"coordinates for mouse click*\/"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"273pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"END IF"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"301pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"END IF"},{"entry":"Produce actions \/*it could be changed actions after image processing; moreover, it's"},{"entry":"possible to finish execution in case of wrong situations during actions*\/"},{"entry":"END LOOP \/*till the end of saved data*\/"},{"entry":"EXIT"},{"entry":"END CCF"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"Embodiments of the present invention including image analysis and object of activity detection on two images may be illustrated by the following examples using a performance analyzer application program. These figures show applying the process blocks of  to a first image from the recording phase (IR) and a corresponding image from the playback phase (IP).  is an example display of the GUI of an application program captured and saved during a recording phase. This IR screen shot shows that the item \u201cTuning Activity\u201d was selected by the user using a mouse.  is an example display of the GUI of an application program captured during a playback phase. Note there are some insignificant changes in the displayed windows in comparison to .  is an example image illustrating objects identified during contouring operations of the recording phase according to an embodiment of the present invention as performed on the image of .  shows the sample output from block  of .  is an example image illustrating objects of activity of the recording phase according to an embodiment of the present invention as performed on the image of . These contours were identified after performing blocks  and  of  on the image from . The contour with the text labeled \u201cTuning\u201d has been determined in this example to be the current object of activity.  is an example image illustrating objects identified during contouring operations of the playback phase according to an embodiment of the present invention. This image is output from performing block  of  on the sample image of . Finally,  is an example image illustrating a hypothesis during the playback phase according to an embodiment of the present invention.  shows hypotheses from  for the \u201cTuning Activity\u201d object of activity from . Size, space, content, and structural filtration of blocks - has been performed. The ellipse represents the contour which was selected as the best hypothesis from performing block  of . A new point for the mouse click is recalculated relative to the given object (i.e., the \u201ctuning\u201d display object).","Embodiments of the present invention provide at least several advantages. One benefit is that the present approach is applicable to any application program exposing a GUI on any platform and OS, and is not dependent on a specific application programming interface (API) or architecture of visual system implementation and OS specifics. In contrast, existing prior art systems are dependent on system APIs while working with visual elements. Another benefit is that the present approach uses a structured representation of images and objects so it makes the execution stable with respect to visual properties that could change (such as screen resolution, drawing schemes and changes in data values, shapes, and orders). This allows one execution scenario (set to a specific image resolution, visual scheme, OS, and computer system) to be run on many different computer systems that could have different visual configurations and operating systems. Existing systems that claim to use image recognition techniques are dependent on the screen resolution and\/or drawing schemes. Also, such systems have strong dependencies on the OS resident in those systems.","Another benefit is that the present system includes paramerization that supports flexibility and extensibility. A number of conditions and specific processes used may be easily changed to allow the CCF system to work for objects represented by contours of different shapes, more strict or flexible time conditions, or more strict or flexible object sizes and different layout requirements. Embodiments of the present invention allow the system to adapt to specific application program family and use cases. For example, while using this system as a test framework, on can ensure that each action completed within the required time limits and easily detect performance degradation. Thus, embodiments of the present invention may be used for capturing and simulating a user interacting with an application program, and thus may be useful for developing new GUIs.","Reference in the specification to \u201cone embodiment\u201d or \u201can embodiment\u201d of the present invention means that a particular feature, structure or characteristic described in connection with the embodiment is included in at least one embodiment of the present invention. Thus, the appearances of the phrase \u201cin one embodiment\u201d appearing in various places throughout the specification are not necessarily all referring to the same embodiment.","Although the operations detailed herein may be described as a sequential process, some of the operations may in fact be performed in parallel or concurrently. In addition, in some embodiments the order of the operations may be rearranged without departing from the scope of the invention.","The techniques described herein are not limited to any particular hardware or software configuration; they may find applicability in any computing or processing environment. The techniques may be implemented in hardware, software, or a combination of the two. The techniques may be implemented in programs executing on programmable machines such as mobile or stationary computers, personal digital assistants, set top boxes, cellular telephones and pagers, and other electronic devices, that each include a processor, a storage medium readable by the processor (including volatile and non-volatile memory and\/or storage elements), at least one input device, and one or more output devices. Program code is applied to the data entered using the input device to perform the functions described and to generate output information. The output information may be applied to one or more output devices. One of ordinary skill in the art may appreciate that the invention can be practiced with various computer system configurations, including multiprocessor systems, minicomputers, mainframe computers, and the like. The invention can also be practiced in distributed computing environments where tasks may be performed by remote processing devices that are linked through a communications network.","Each program may be implemented in a high level procedural or object oriented programming language to communicate with a processing system. However, programs may be implemented in assembly or machine language, if desired. In any case, the language may be compiled or interpreted.","Program instructions may be used to cause a general-purpose or special-purpose processing system that is programmed with the instructions to perform the operations described herein. Alternatively, the operations may be performed by specific hardware components that contain hardwired logic for performing the operations, or by any combination of programmed computer components and custom hardware components. The methods described herein may be provided as a computer program product that may include a machine accessible medium having stored thereon instructions that may be used to program a processing system or other electronic device to perform the methods. The term \u201cmachine accessible medium\u201d used herein shall include any medium that is capable of storing or encoding a sequence of instructions for execution by a machine and that cause the machine to perform any one of the methods described herein. The term \u201cmachine accessible medium\u201d shall accordingly include, but not be limited to, solid-state memories, and optical and magnetic disks. Furthermore, it is common in the art to speak of software, in one form or another (e.g., program, procedure, process, application, module, logic, and so on) as taking an action or causing a result. Such expressions are merely a shorthand way of stating the execution of the software by a processing system cause the processor to perform an action of produce a result."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The features and advantages of the present invention will become apparent from the following detailed description of the present invention in which:",{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 8"}]},"DETDESC":[{},{}]}
