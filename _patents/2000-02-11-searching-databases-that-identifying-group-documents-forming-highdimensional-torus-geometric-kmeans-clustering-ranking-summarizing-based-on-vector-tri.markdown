---
title: Searching databases that identifying group documents forming high-dimensional torus geometric k-means clustering, ranking, summarizing based on vector triplets
abstract: A method and structure for performing a database search includes searching a database using a query (searching producing result items), and ranking the result items based on one or more of a frequency of an occurrence of in-links and out-links in each of the result items.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=06862586&OS=06862586&RS=06862586
owner: International Business Machines Corporation
number: 06862586
owner_city: Armonk
owner_country: US
publication_date: 20000211
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF PREFERRED EMBODIMENTS OF THE INVENTION"],"p":["1. Field of the Invention","The present invention generally relates to searching and organizing a collection of hyperlinked, hypertext documents, such as those making up the World-wide-web. The present invention exploits the words (or terms) present in each of the documents and exploits the link structure (or link topology) between the hypertext documents to organize the document collection into various groups or clusters. The present invention also identifies typical documents in a group or a cluster. Finally, the present invention provides a way to rank documents in each of the groups. As an example, suppose a web search engine has returned a set of hyperlinked documents in response to a user search or query. The present invention provides a way to organize the set of returned documents into various groups or clusters.","2. Description of the Related Art","The explosive growth of the World-Wide-Web has created an abundance of hyperlinked document corpora. Prominent examples of such data are the IBM patent server, the Internet archive, and scientific literature. Mining the information present in such corpora represents a major contemporary scientific and technological challenge. Given a user query, today's Internet search engines may return a large number of relevant documents. Without effective summarization, it is a hopeless and enervating task to sort through all the returned documents in search of high-quality, representative information resources. In particular, what is needed is a technique that can aid in organizing, ranking, and effectively summarizing the gist or essence of the results returned by the Internet search engine.","It is, therefore, an object of the present invention to provide a structure, organization, and ranking method for viewing hyperlinked documents. The invention views hyperlinked documents as nodes in a directed graph whose edges are the hyperlinks between the documents, where each node contains some descriptive text. To systematically exploit the three-fold information: words, out-links, and in-links, each hyperlinked document is represented as a triplet of three unit vectors, respectively, of normalized word, out-link, and in-link frequencies. Out-links are links from a hypertext document, whereas in-links are links to a hypertext document. Geometrically, each document vector triplet can be thought of as a point on the surface of a high-dimensional torus. Such documents are clustered using a new clustering process, the toric k-means. The invention is illustrated by clustering the documents returned by an internet search engine for various queries. As a further illustration of the invention, each cluster is then summarized by seven informative nuggets of information. Finally, documents within each cluster are ranked in the order of typicality from the most typical to the least typical. The invention is quite effective in disambiguating the query words, in locating high-quality documents, and in concisely summarizing the retrieved documents.","One embodiment of the invention includes searching a database using a query (which produces result items), clustering the result items into logical categories, and ranking the result items within each category based on the frequency of the occurrence of words, the frequency of the occurrence of in-links, and the frequency of the occurrence of out-links in each of the result items. The frequency of the occurrence of words is, for a given result item, the number of unique words that are contained in the given result item. The frequency of the occurrence of out-links is, for a given result item, the number of other hypertext items (whether in the result set or not) that are linked to by or cited by or pointed to by the given result item. The frequency of the occurrence of in-links is, for a given result item, the number of other hypertext items (whether in the result set or not) that are linked to or cite or point to the result item.","In another embodiment, the invention comprises a method of performing a database search that includes, searching a database using a query, (the searching identifying a group of hyperlinked documents), forming a high-dimensional torus geometric representation of the hyperlinked documents, (wherein each hyperlinked document is represented by a vector triplet that includes a normalized word frequency, a normalized out-link frequency and a normalized in-link frequency), clustering the result items into clusters based on the high-dimensional torus geometric representation, ranking items within each of the clusters based on the high-dimensional torus geometric representation, and summarizing contents of the clusters based on the high-dimensional torus geometric representation.","The normalized out-link frequency includes the number of the hyperlinked documents linked to, cited, or pointed to by the hyperlinked document. The normalized in-link frequency includes the number of the hyperlinked documents linking to, citing, or pointing to the hyperlinked document. The normalized word frequency includes the number of unique words, terms, or n-grams contained in the hyperlinked document.","The clustering of the vector triplets on the high-dimensional torus geometric representation is performed using a toric k-means clustering process that uses a cosine-type similarity measure between document vector triplets, thereby producing clusters of vector triplets and producing a concept triplet for each of the clusters. The ranking includes a cosine-type similarity measure between document vector triplets.","The invention summarizes the clusters of vector triplets based on nuggets of information including, a closeness of the vector triplets in a cluster to the concept triplet for the cluster on the high-dimensional torus geometric representation, identifying the words with a highest normalized word frequency in the concept triplet for the cluster as the most frequent key-words for each of the clusters, identifying the out-links with a highest normalized out-link frequency in the concept triplet for the cluster as most frequent key out-links for each of the clusters, identifying the in-links with a highest normalized in-link frequency in the concept triplet for the cluster as most frequent important in-links for each cluster, identifying hypertext items relevant to the user's query by using a weighting of terms used in the query, identifying documents closest to the concept triplet as most typical documents in a cluster using a cosine-type textual content similarity measure between document vector triplets, identifying documents closest to the concept triplet as using a cosine-type out-link similarity measure between document vector triplets and most typical documents in a cluster, and identifying documents closest to the concept triplet as most typical documents in a cluster using a cosine-type in-link similarity measure between document vector triplets.","The toric k-means clustering process includes, forming initial clusters and computing concept vector triplets for each initial cluster formed, for each document vector triplet, finding the closest concept vector triplet using a cosine-similarity measure, assigning each document vector triplet to a closest concept vector triplet, recomputing new clusters as groups of document vectors assigned to a specific concept vector triplet, recomputing new concept vector triplets for the new clusters, repeating finding the closest concept vector triplet using a cosine-similarity measure, assigning each document vector triplet to a closest concept vector triplet, and recomputing new concept vector triplets for the new clusters. The clustering includes aggolomerative clustering, hierarchical clustering, EM algorithm, or mixture modeling. The ranking includes identifying a most typical vector triplet in each of the clusters of vector triplets.","The invention uses clustering to discover \u201clatent classes\u201d or \u201clatent categories\u201d in unstructured, unlabeled hyperlinked document copora. For a review of classical algorithms for clustering text documents, see E. Rasmussen, \u201cClustering Algorithms,\u201d in (W. B. Frakes and R. Baeza-Yates, eds.), pp. 419-442, Prentice Hall, Englewood Cliffs, N.J., 1992, and P. Willet, \u201cRecent Trends in Hierarchic Document Clustering: A Critical Review,\u201d , vol. 24, no. 5, pp. 577-597, 1988, incorporated herein by reference. For recent work on document clustering algorithms, see Cutting et al., \u201cScatter\/Gather: A Cluster-Based Approach to Browsing Large Document Collections,\u201d in , 1992; Dhillon et al., Hearst et al., \u201cReexamining the Cluster Hypothesis: Scatter\/Gather on Retrieval Results,\u201d in , 1996; Sahami et al., \u201cSONIA: A service for Organizing Networked Information Autonomously,\u201d in 98, New York, N.Y., pp. 200-209, ACM, 1999; Sch\u00fctze et al., \u201cProjections for Efficient Document Clustering,\u201d in , 1997; Silverstein et al., \u201cAlmost-Constant-Time Clustering of Arbitrary Corpus Subsets,\u201d in , 1997; Vaithyanathan and B. Dom et al., and Zamir et al., \u201cWeb Document Clustering: A Feasibility Demonstration,\u201d in , 1998, incorporated herein by reference.","The above papers focus on clustering using only the textual features of the documents. To the contrary, the invention includes a clustering invention that uses a link structure of the document corpora in addition to the commonly used textual features. Relevant documents tend to-be-more similar to each other than to non-relevant documents. The invention employs an enlarged notion of similarity that includes similarity in words as well as in link-topology.","A link structure has been used quite successfully to identify \u201chubs\u201d and \u201cauthorities\u201d in hyperlinked document corpora (see Chakrabarti et al., \u201cHypersearching the Web,\u201d , June 1999, incorporated herein by reference). Kleinberg's (HITS) algorithms (see Kleinberg, J., \u201cAuthoritative sources in a hyperlinked environment,\u201d ACM-SIAM Symposium on Discrete Algorithms, 1998) and its refinements use an algebraic eigenvalue based approach. In comparison, the invention uses a geometric clustering based approach. Furthermore, HITS only uses the link structure and not the textual features.","One novel aspect of the invention is a geometric representation of hyperlinked documents as points on a high-dimensional torus. Each hyperlinked document is represented as a triplet of three unit vectors, respectively, of normalized word, out-link, and in-link frequencies. This is a natural extension of the classical vector space models of text data (see Salton et al., , McGraw-Hill Book Company, 1983, incorporated herein by reference). Another novel aspect of the invention is a cosine-type similarity measure between document vector triplets. One more novel aspect of the invention is a geometric toric k-means clustering process that clusters points on the surface of a torus. An additional novel aspect of the invention is summarizing the content of each of the clusters using seven informative nuggets. A further novel aspect of the invention is ranking the documents within each cluster in the order of typicality.","The new clustering process, namely, the toric k-means, is a generalization of the classical, spherical k-means method (see Rasmussen, Supra, and Dhillon et al., Supra, respectively), in that the invention uses three different features: words, out-links, and in-link. Geometrically, each document vector triplet can be thought of as a point on the surface of a high-dimensional torus.","The invention is illustrated by the following example of clustering documents returned by an Internet search engine for the query \u201clatent semantic indexing\u201d. For this query, the top document returned by the algorithm is the LSI home page at Bellcore (e.g., superbook.bellcore.com\/\u02dcremde\/lsi\/LSI.html). The invention is quite effective in disambiguating the query words, in locating high-quality documents, and in concisely summarizing the retrieved documents (see Dhillon et al., Supra).","Each document x is represented as a triplet of unit vectors (D, F, B). These component vectors are to be thought of as column vectors. For each document x, the first vector D in the triplet is the unit vector of normalized word frequencies for each word that occurs in the document (e.g., words, terms, or n-grams). The basic idea is to eliminate stopwords, high-frequency words, and low-frequency words from the document collection. Suppose d unique words remain in the document collection after such elimination. For each document, the first vector D in the triplet will be a d-dimensional vector. For each column entry of D is the number of occurrences of the word corresponding to that column in the document x. Finally, the vector D is normalized to have a unit Euclidean norm. In the terminology of Salton et al., \u201cTerm-Weighting Approaches in Automatic Text Retrieval,\u201d , vol. 4, no. 5, p. 513:523, 1988, incorporated herein by reference, the invention uses a txn term-weighting scheme.","For each document x, the second vector F in the triplet is the unit vector of normalized out-link frequencies. The idea is to represent the second vector F as a f-dimensional vector, where f denotes the number of distinct documents that are pointed to by at least two other documents in the document collection. Thus, every entry in F has an in-degree at least 2. Thus, i-th column entry of F is the number of out-links from the document x to the document corresponding to the i-th column. Finally, the vector F is normalized to have a unit Euclidean norm.","For each document x, the third vector B in the triplet is the unit vector of normalized in-link frequencies. The idea is to represent the third vector B as a b-dimensional vector, where b denotes the number of distinct documents that point to at least two other documents in the document collection, again having an out-degree at least 2. Thus, i-th column entry of B is the number of in-links to the document x from the document corresponding to the i-th column. Finally, the vector B is normalized to have a unit Euclidean norm.","In one example, if there are n documents in the corpus, each document triplet is denoted as\n\n=(), 1\u2266\n","By construction, the component vectors D, F, and Ball have unit Euclidean norms, and, hence, can be considered points on the unit spheres in dimensions d, f, and b, respectively. Thus, the document triplet xlies on the product space of three spheres, which is generically a torus. Furthermore, by construction, the individual entries of the component vectors D, F, and Bare nonnegative. Hence, the component vectors are in fact in the nonnegative orthants of R, R, and R, respectively.","Given document triplets x=(D, F, B) and {tilde over (x)}=({tilde over (D)}, {tilde over (F)}, {tilde over (B)}), a measure of similarity between them is defined as a weighted sum of the inner products between the individual components. The similarity measure is completely novel and has not been considered in any prior art. Precisely,\n\n()=\u03b1\n\nwhere \u03b1, \u03b1, and \u03b1are nonnegative numbers such that\n\n\u03b1+\u03b1+\u03b1=1. \n","If \u03b1=1, \u03b1=0, and \u03b1=0, then the classical cosine similarity between document vectors that has been widely used in information retrieval is obtained (see Salton et al., 1983, Supra). These parameters are tunable with the invention to assign different weights to words, outlinks, and in-links as desired.","Given n document vector triplets x, x, . . . , x, let \u03c0, \u03c0, . . . , \u03c0denote a partitioning of the document vector triplets into k disjoint clusters. For each fixed 1, the concept triplet of the documents contained in the cluster \u03c0is\n\n=()\u2003\u2003(1) \n\nand \n\n\nwhere x=(D, F, B). The concept triplet is a triplet of the normalized mean vector or the normalized centroid vector of each of the three component vectors.\n","It is anticipated that k disjoint clusters \u03c0, \u03c0, . . . , \u03c0are found such that the following is maximized: \n\n\nIn other words, \u201ccoherent\u201d or \u201ccohesive\u201d clusters are sought such that within each cluster, the document vector triplets are as close as possible to the corresponding concept triplet.\n","Finding the optimal solution to the above maximization problem is NP-complete (e.g., see Theorem 3.1 of Kleinberg et al., \u201cA Microeconomic View of Data Mining,\u201d , vol. 2, pp. 311-324, December 1998, incorporated herein by reference).","The following is an approximation process, namely, the toric k-means process, which is an effective and efficient iterative heuristic. The first step is to start with an arbitrary partitioning of the document vectors, namely, \n\n\nLet \n\n\ndenote the concept triplets associated with the given partitioning. The index of iteration t is set to 0. The second step is, for each document vector triplet x, 1\u2266i\u2266n, to find the concept triplet that is closest to x. The new partitioning is computed \n\n\ninduced by the old concept triplets \n\n\n","In other words, \n\n\nis the set of all document vector triplets that are closest to the concept triplet c. If it happens that some document triplet is simultaneously closest to more than one concept triplet, then it is randomly assigned to one of the clusters. Clusters defined using equation (3) are known as Veronoi or Dirichlet partitions. The third step is to compute the new concept triplets corresponding to the partitioning computed in equation (3) by using equations (1)-(2) where instead of \u03c0, \n\n\nis used. If some \u201cstopping criterion\u201d is met, then \u03c0=\u03c0and fc=cor 1\u2266j\u2266k, and the process exits. Otherwise, t is incremented by 1, and processing returns to the second step. An example of a stopping criterion is if the change in the objective function, between two successive iterations, is less than some specified threshold.\n","Thus, the invention first arbitrarily partitions the document vectors and forms a concept triplet for each partition based on the document vectors in each arbitrary partition. Then, the invention creates clusters by assigning each document vector triplet to the cluster having the closest concept triplet. Therefore, each cluster has one concept triplet with document vector triplets grouped around it. New concept triplets are then calculated for each of the clusters. This process is repeated until the change between the old concept triplet and the new concept triplet is below a predetermined threshold.","The following example illustrates the operation of the invention. An Internet search engine may return a list of 200 documents in response to a user query. With the invention, each of these 200 documents are retrieved and, in this case, n=200. For each document, by using the text of these documents, the word-frequency component of the document vector triplet is computed by proceeding as mentioned above. For speed, instead of the full text of the documents, the summaries returned by the Internet search engine are sometimes used.","For each document, by parsing the hypertext of each document, the list of universal resource locators (URLs) pointed to by each document is obtained. Every URL that is not pointed to by at least 2 of the original  documents is deleted because the invention seeks only those documents which are similarly linked with other documents produced in response to the query. The out-link component of the document vector triplet is now computed by proceeding as mentioned above.","In one embodiment, using queries of the form \u201clink: URL\u201d on an Internet search engine, the URLs of the top  documents that point to each of the original  documents could be retrieved. Thus, in all, a total of roughly 2000 URLs that point to the original  documents are obtained. From this list of URLs, every URL that does not point to at least 2 of the original  documents is deleted. Finally, the in-link component of the document vector triplet is similarly computed, by proceeding as mentioned above.","At this point, the toric k-means algorithm is applied to the above document triplets to find k clusters.","For each cluster j, 1\u2266j\u2266k, the following seven nuggets of information are pertinent. First, the document that is closest to the concept triplet S (the document S has the most typical text, out-link, and in-link content in the cluster) is presented at the very top. The second nugget of information is that two or three most frequent words have the largest weight Dwhich provide a compact summary of the textual content of the documents in the cluster, and help disambiguate the query. Thirdly, the most frequent out-link has the largest weight Fwhich represents the document that is most frequently pointed to by the documents in the cluster. The fourth nugget is that the most frequent in-link has the largest weight Bwhich represents the document that points most frequently to the documents in the cluster. A fifth nugget is that the most typical document's D component that is closest to Din cosine similarity represents the document that has the most typical textual content in the cluster. The sixth information nugget is that the most typical document's F component that is closest to Fin cosine similarity represents the document that has the most typical out-link content in the cluster. The last nugget is that the most typical document's B component that is closest to Bin cosine similarity represents the document that is most typically pointed to in the cluster.","Results for one query are discussed below (e.g., the query \u201clatent semantic indexing\u201d). The 200 documents returned by the Internet search engine are clustered into k=5 clusters using the inventive toric k-means process discussed above. The parameters \u03b1=\u00bd, \u03b1=\u00bc, and \u03b1=\u00bc are used in this example. In Table 1, the five clusters are presented. The LSI home page at Bellcore was the very first document returned by the invention. It is clear from table one below that each cluster is neatly summarized by the seven nuggets of information. In addition, these nuggets are an extremely useful aid to navigation and to word-sense disambiguation.","Similarly, in response to the query \u201cstring theory\u201d the invention brings up \u201cThe Official String Theory home page\u201d as S and in response to the query \u201cInformation Retrieval\u201d the invention brings up \u201cThe SIGIR home page\u201d as S.",{"@attributes":{"id":"P-d0e3135","num":"00002"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":"TABLE 1"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Table 1 illustrates five clusters for"},{"entry":"the query (e.g., latent semantic indexing)."},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"182pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Cluster 0","Latent Semantic Indexing [Bellcore]"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"112pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"70pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Size: 59","frequent","typical"]},{"entry":["words","latent-indexing, computer-","dreamwave: Re: LSI"]},{"entry":[{},"science","(More Like This)"]},{"entry":["out-link","dreamwave by thread","dreamwave: Re: Excite"]},{"entry":[{},{},"NewsTracker"]},{"entry":["in-link","CS 384-Resources [U","Latent Semantic"]},{"entry":[{},"Chicago]","Indexing [Bellcore]"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"182pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Cluster 1","Information Retrieval References (Arnon Rungsawang)"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"112pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"70pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Size: 51","frequent","typical"]},{"entry":["words","information-retrieval, research","Topics in IR (history;"]},{"entry":[{},{},"1995) [U Mass]"]},{"entry":["out-link","www.cs.utk.edu\/ . . .","1. Introduction"]},{"entry":[{},{},"[Michael"]},{"entry":[{},{},"Berry's paper]"]},{"entry":["in-link","403 Forbidden","IR References (Arnon"]},{"entry":[{},{},"Rungsawang)"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"182pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Cluster","Inf\u2003Information Retrieval of"]},{"entry":["2","Imperfectly Recognized Handwriting"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"112pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"70pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Size: 43","frequent","typical"]},{"entry":["words","papers, utk","Using Relevance to"]},{"entry":[{},{},"Train . . . Experts"]},{"entry":["out-link","LSI Web Site [Berry and","No Title"]},{"entry":[{},"Dumais]"]},{"entry":["in-link","Information Retrieval Links","Recent papers by"]},{"entry":[{},{},"Peter Foltz"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"182pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Cluster 3","NERSC: Sparse Linear Algebra Algorithms for MPPs"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"112pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"70pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Size: 28","frequent","typical"]},{"entry":["words","publications, index","CSD [Berkeley Lab]"]},{"entry":["out-link","T. G. Kolda Redirect","cv.html [Horst Simon]"]},{"entry":["in-link","Berkeley Lab Computing","No Title [Horst"]},{"entry":[{},"Sciences Research","Simon's paper]]"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"182pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Cluster 4","TREC6 Cross-Language Track"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"112pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"70pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Size: 19","frequent","typical"]},{"entry":["words","language, umd-dlrg","Resources on Cross-"]},{"entry":[{},{},"Language Text"]},{"entry":[{},{},"Retrieval"]},{"entry":["out-link","www.glue.umd.edu\/ . . .","References"]},{"entry":["in-link","Cross-Language Text","TREC6 Cross-"]},{"entry":[{},"Retrieval Conferences","Language Track"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}]}}},{"@attributes":{"id":"P-00058","num":"00058"},"figref":"FIG. 1","b":["10","11","12"]},"While the overall methodology of the invention is described above, the invention can be embodied in any number of different types of systems and executed in any number of different ways, as would be known by one ordinarily skilled in the art. For example, as illustrated in , a typical hardware configuration of an information handling\/computer system in accordance with the invention preferably has at least one processor or central processing unit (CPU) 200. For example, the central processing unit  could include various image\/texture processing units, mapping units, weighting units, adders, subtractors, comparators, etc. Alternatively, as would be known by one ordinarily skilled in the art given this disclosure, multiple specialized CPU's (or other similar individual functional units) could perform the same processing, mapping, weighting, adding, subtracting, comparing, etc.","The CPU  is interconnected via a system bus  to a random access memory (RAM) , read-only memory (ROM) , input\/output (I\/O) adapter  (for connecting peripheral devices such as disk units  and tape drives  to the bus ), communication adapter  (for connecting an information handling system to a data processing network) user interface adapter  (for connecting a peripherals ,  such as a keyboard, mouse, microphone speaker and\/or other user interface device to the bus ), a printer , and display adapter  (for connecting the bus  to a display device ). The invention could be implemented using the structure shown in  by including the inventive method within a computer program stored on the storage device . Such a computer program would act on information (e.g., a query) supplied through the interface units ,  or through the network connection  (e.g., Internet connection). The system would then automatically produce the final desired product on the display , through the printer  or back to the network .",{"@attributes":{"id":"P-00061","num":"00061"},"figref":"FIG. 3","b":["30","31","32","33","34"]},{"@attributes":{"id":"P-00062","num":"00062"},"figref":"FIG. 4","b":["40","41","42","43","44","45","46","47"]},{"@attributes":{"id":"P-00063","num":"00063"},"figref":"FIG. 5","b":["50","51","52","53","54"]},"Therefore, as described above, in one embodiment, the invention increases the performance of a query search into a database by ranking the results not only by textural features, but also by link-topology. In addition, the invention produces a cosine-type similarity between document rankings (e.g., based on textural and link-topology). Further, the invention clusters the results into different conceptual categories prior to the inventive ranking to further increase the performance of the query process. The invention is transparent to the user in that no additional input, other than that required for the conventional query, is required.","The invention is very useful in organizing unlabeled, unorganized hypertext documents. There are many applications of this basic process: knowledge management, searching and retrieval, ontology creation (like Yahoo!), etc. Any environment which can be interpreted either logically or physically as a collection of hyperlinked documents can benefit from the use of the above invention. The most important application of the present invention is to design better quality Internet search engines for the World-Wide-Web.","While the invention has been described in terms of preferred embodiments, those skilled in the art will recognize that the invention can be practiced with modification within the spirit and scope of the appended claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The foregoing and other objects, aspects and advantages will be better understood from the following detailed description of a preferred embodiment of the invention with reference to the drawings, in which:",{"@attributes":{"id":"P-00014","num":"00014"},"figref":"FIG. 1"},{"@attributes":{"id":"P-00015","num":"00015"},"figref":"FIG. 2"},{"@attributes":{"id":"P-00016","num":"00016"},"figref":"FIG. 3"},{"@attributes":{"id":"P-00017","num":"00017"},"figref":"FIG. 4"},{"@attributes":{"id":"P-00018","num":"00018"},"figref":"FIG. 5"}]},"DETDESC":[{},{}]}
