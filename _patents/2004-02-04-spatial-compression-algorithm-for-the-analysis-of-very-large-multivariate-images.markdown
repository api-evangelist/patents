---
title: Spatial compression algorithm for the analysis of very large multivariate images
abstract: A method for spatially compressing data sets enables the efficient analysis of very large multivariate images. The spatial compression algorithms use a wavelet transformation to map an image into a compressed image containing a smaller number of pixels that retain the original image's information content. Image analysis can then be performed on a compressed data matrix consisting of a reduced number of significant wavelet coefficients. Furthermore, a block algorithm can be used for performing common operations more efficiently. The spatial compression algorithms can be combined with spectral compression algorithms to provide further computational efficiencies.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07400772&OS=07400772&RS=07400772
owner: Sandia Corporation
number: 07400772
owner_city: Albuquerque
owner_country: US
publication_date: 20040204
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","STATEMENT OF GOVERNMENT INTEREST","FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION","Multivariate Spectral Analysis","Spectral Compression Using a Block PCA Algorithm","Implementation of the Block PCA Algorithm","Spatial Compression Using Wavelets","Implementation of the Spatial Compression Algorithm"],"p":["This application claims the benefit of U.S. Provisional Application No. 60\/472,447, filed May 20, 2003, which is incorporated herein by reference. This application is related to U.S. patent application Ser. No. 10\/772,548, filed of even date with this application.","This invention was made with Government support under contract no. DE-AC04-94AL85000 awarded by the U.S. Department of Energy to Sandia Corporation. The Government has certain rights in the invention.","The present invention relates to multivariate image analysis and, in particular, to methods for compressing data and using block algorithms to enable the analysis of very large multivariate images.","Comprehensive chemical characterization of a complex microstructure is a daunting task. Recently, full spectrum imaging instruments have become available that can collect a complete spectrum at each point in a spatial array and which promise to provide the data needed to perform such a characterization. A remaining hurdle is the computational difficulty of reducing the very large quantities of raw spectral data to meaningful chemical information.","Analysis of full spectrum images having sizes that can be collected with current instrumentation can outstrip the computational resources typically found in the laboratory. For example, a typical commercial energy dispersive x-ray imaging system can produce a 1024\u00d71024 pixel (one megapixel) image with each pixel being represented by a complete 1024 channel x-ray spectrum. Four Gbytes of computer memory are required just to hold a single precision floating point representation of this data set. To put this number in context, 4 Gbytes equals the full physical address space of a modern 32-bit microprocessor. This data overload precludes the use of standard analysis algorithms, which require that all of the data to be in the computer's physical memory at one time.","A related issue is the computational complexity of the analysis algorithms. While the time required to fully read a given data set obviously scales linearly with the size of the data set, the analysis algorithms may scale up more quickly, leading to unacceptable computation times. To take one example, given a m\u00d7p matrix with m\u2267p, Singular Value Decomposition (SVD), a popular method for effecting Principal Components Analysis (PCA), requires computing time proportional to m\u00d7p.","Researchers have taken a number of different approaches in attempting to overcome the foregoing computational problems. Current approaches to analyzing very large spectrum images range from downsampling to simply omitting data to various data compression schemes. In the context of spectroscopic data, the general idea behind data compression is to represent a high dimensional spectrum in terms of a lower dimension basis. Convenient bases include B-splines and wavelets. A principal component basis derived from PCA can yield good compression, provided the PCA can be computed. While these approaches are effective at reducing the size of the problem to be solved, they suffer the potential drawback that they work with an approximation to the data rather than the data itself. Whether or not the approximation is acceptable depends on the details of the data and the problem at hand. See, e.g., J. Andrew and T. Hancewicz, \u201cRapid Analysis of Raman Image Data Using Two-Way Multivariate Curve Resolution, 52, 797 (1998); B. Alsberg and O. Kvalheim, \u201cSpeed improvement of multivariate algorithms by the method of postponed basis matrix multiplication Part I. Principal component analysis\u201d, 24, 31 (1994); H. Kiers and R. Harshman, \u201cRelating two proposed methods for speedup of algorithms for fitting two- and three-way principal component and related multilinear models\u201d, 36, 31 (1997); F. Vogt and M. Tacke, \u201cFast principal component analysis of large data sets\u201d, 59, 1 (2001); and F. Vogt and M. Tacke, \u201cFast principal component analysis of large data sets based on information extraction,\u201d 16, 562 (2002).","The current invention overcomes these limitations by combining compression strategies with image analysis algorithms that operate directly on the compressed data. Spectral compression is described herein using an exemplary PCA-factored representation of the data. Furthermore, a block algorithm can be used for performing common operations more efficiently. A key advantage of the block algorithm is that at no point is it required to have all of the data residing simultaneously in the computer's main memory, enabling the operations to be performed on larger-than-memory data sets. For example, the block algorithm can be used that is suitable for out-of-core kernel PCA and a Multivariate Curve Resolution\u2014Alternating Least Squares (MCR-ALS) procedure that directly employs a PCA representation of the data. The crossproduct matrix that is formed during the PCA compression using the block algorithm is mathematically identical to the one that would be computed if the entire data set could have been held in memory. Consequently, data sets that are larger than memory are readily analyzed. The performance of the algorithm is characterized for data sets ranging in size to upward of one billion individual data elements. For sufficiently large data sets, an out-of-core implementation of the algorithm extracts only a minor performance penalty over a fully in-core solution.","Multiresolution spatial processing, or spatial compression, can be used separately or in combination with the spectral compression algorithm and image analysis techniques to provide speed improvements without the loss of spatial or spectral resolution. Spatial compression is described herein using an exemplary Haar wavelet transform. Multiresolution techniques have been used in traditional image processing and, in fact, serve as the basis of the recently introduced JPEG2000 image compression standard. Similar ideas can be applied to spatially compress high-spectral-dimension images. The basic idea is to reduce the number of pixels analyzed to a smaller number of coefficients that capture all of the spatial information of interest. Using orthogonal wavelets to perform this compression allows standard multivariate curve resolution techniques to be applied to the coefficients themselves rather than the raw data. This can lead to a tremendous reduction in both the computational resources and time required to complete the analysis. In addition to improved computational performance, the multiresolution approach can also yield improved sensitivity to minor constituents for data having low signal to noise ratios. This is particularly true if the multiresolution spatial filters are matched to the features of interest in the sample.","The method of the present invention is directed to spatial compression algorithms for analyzing multivariate images, comprising providing a data matrix containing measured spectral data, transforming the data matrix, using a wavelet transform, to obtain a transformed data matrix, performing an image analysis on the transformed data matrix to obtain a transformed concentration matrix and a spectral shapes matrix, and computing a concentration matrix from the transformed concentration matrix. The wavelet transform preferably comprises a Haar wavelet. The wavelet coefficients of the transformed data matrix can be thresholded, to compress the important spatial information in a reduced number of significant wavelet coefficients that is small compared to the number of original pixels. The image analysis is preferably a MCR-ALS analysis, using a transformed non-negativity constraint. The concentration matrix can be computed from either an inverse transformation of the transformed concentration matrix or by projecting the data matrix onto the spectral shapes matrix to reconstruct the component maps with full spatial resolution. Furthermore, the spatial compression algorithm can use a block algorithm. The data block is suitably sized to fit in core memory and consists of full image planes for some number of spectral channels.","The spatial compression algorithms can be combined with a spectral compression algorithm to provide further computational efficiencies. Spectral compression uses a factored representation of the data. The factored representation preferably comprises a scores matrix and a loadings matrix derived from a principal components analysis of the data matrix. More preferably, the factored representation represents the most significant components of the principal components analysis.","As problem sizes begin to test the limits of available computing resources, it becomes necessary to pay close attention to the details an algorithm's implementation in order to achieve optimal or even acceptable performance. How an algorithm is structured with respect to the computer system's memory architecture is perhaps the foremost influence in this regard.","In the present invention, by paying careful attention to data locality, efficient block algorithms can be devised for performing common operations on data sets. The block algorithms enable the analysis of larger-than-memory data sets and the improved performance for data sets that are small enough to fit entirely within memory. Contrary to conventional wisdom, for linear algebra operations typical of chemometrics, movement between disk storage and main memory can be minimized such that disk access time can be a small part of the total computation time for the analysis of large data sets. Alternatively, performance can be improved for smaller data sets by minimizing the movement between main memory and cache memory. To demonstrate these ideas, a block algorithm has been designed and implemented that enables the more efficient use of a computer's memory to push back apparent memory limitations.","In  is shown the memory hierarchy of a typical personal computer, where memory is viewed expansively to include any stored data that could be made available to the processor. In general, speed goes up but capacity goes down as we move up the hierarchy toward the processor. As a problem size grows, we are forced to move down the hierarchy to gain access to sufficient memory, at the cost of losing performance. See K. Gallivan et al., \u201cImpact of Hierarchical Memory Systems on Linear Algebra Algorithm Design,\u201d 2, 12 (1988).","To make efficient use of the memory hierarchy, it is necessary to design an algorithm with data locality in mind. Two flavors of locality are generally considered. Temporal locality relates to the tendency for a given memory location to be accessed repeatedly over a short period of time. Utilizing spatial locality, on the other hand, means designing an algorithm to access nearby memory locations together. In terms of the memory hierarchy, exploiting locality and designing for performance means minimizing data flow between different levels of the hierarchy while trying to perform as many operations as possible in the higher levels. These ideas have been critical in obtaining high performance in distributed computing environments where inter-processor communication poses a bottleneck, but they have been paid relatively scant attention by users of stand-alone computers.","That locality is important in the latter environment, however, can be easily demonstrated by a simple example. Consider programming in the popular MATLAB\u00ae environment. See MATLAB Version 6.5, The Mathworks, Natick, Mass. MATLAB\u00ae stores matrices in column-major order, as depicted by matrix A in . Thus, matrix elements that are adjacent in a column are, in fact, adjacent in memory, whereas, elements that are adjacent in a row are separated in memory by the number of rows in the matrix. To make the most of spatial locality, in this case, algorithms should be designed to operate on columns of the matrix rather than on rows.",{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 3"},"The preceding example and discussion were concerned with performance in the case that a problem is too large to be contained fully in cache memory, but which did fit within the computer's main memory. As problems become too large to be contained in main memory, the next level in the memory hierarchy, the local disk storage, comes into play. Data analysts have some experience with using disk storage to mimic main memory in the virtual memory capability of modern operating systems. This experience has invariably been negative from the standpoint of numerically intensive computation. Once the operating system begins \u201cswapping to disk,\u201d the speed of numerical algorithms is brought to a crawl. However, it should be kept in mind that the disk usage, in this case, is being controlled by the operating system\u2014not by the algorithm. By explicitly considering disk usage when designing large-scale algorithms, rational choices about disk usage can be made up front and disk-based memory can provide an effective solution to the larger-than-memory problem. The design considerations when using disk storage are the same as the ones discussed previously\u2014minimize the flow of data between levels of the memory hierarchy and perform as many calculations as possible at the highest level possible.","A key to making efficient use of disk storage is the development of block algorithms for common linear algebra operations. See G. H. Golub and C. F. Van Loan, 3Ed., The John Hopkins University Press, Baltimore, Md. (1996). Block techniques have played an important role in designing high performance algorithms both on hierarchical uniprocessor systems and in parallel computing environments.","The block algorithm can be combined with data compression algorithms to enable the efficient analysis of very large multivariate images. This approach will be illustrated in the following description by the development of a block algorithm for performing PCA that is suitable for use in applications in which the entire data set cannot be contained simultaneously within main memory. When performing PCA with the block algorithm, the algorithm is capable of producing a mathematically exact factorization of a data matrix into scores and loadings. Substantial speed improvements can then be achieved by truncating the factorization to the number of chemically relevant factors either known a priori or estimated from the data. The algorithm can also accommodate several common preprocessing operations such as mean-centering, autoscaling, and weighting by a diagonal or block-diagonal matrix. The block algorithm for PCA can also be used with the multiresolution spatial processing either pre- or post-factorization.","Multivariate spectral analysis can be applied to spectral data produced by a variety of spectroscopic imaging techniques, including: Electron Probe Microanalysis (EPMA), Scanning Electron Microscopy (SEM) with attached Energy Dispersive X-Ray Spectrometer (EDX), X-Ray Fluorescence (XRF), Electron Energy Loss spectroscopy (EELS), Particle Induced X-ray Emission (PIXE), Auger Electron Spectroscopy (AES), gamma-ray spectroscopy, Secondary Ion Mass Spectroscopy (SIMS), X-Ray Photoelectron Spectroscopy (XPS), Infrared Spectroscopy (IR), Raman Spectroscopy, Magnetic Resonance Imaging (MRI) scans, Computerized Axial Tomography (CAT) scans, IR reflectometry, Mass Spectrometry (MS), multidimensional chromatographic\/spectroscopic techniques, hyperspectral remote imaging sensors, etc.","In general, an image can comprise any arbitrary, multidimensional array of points. The image can include a spatial dimension, such as lines, traditional 2D images, or 3D volumes; or a temporal dimension, such as a time series of images. In the case of a gas chromatography\/mass spectroscopy (GC\/MS) image, the dimension is a separation coordinate. The multivariate image analysis techniques will be described herein in reference to a spatial dimension. However, it is understood that the techniques can be applied also to non-spatial images, such as those comprising a time series or chromatographic coordinate.","In general, multivariate spectral analysis for chemical characterization of a sample can include: determining the number of chemical species (pure elements and chemical phases or alloys) that comprise an inhomogeneous mixture being imaged; extracting the spectra of these \u201cpure\u201d components (elements or phases); quantifying the amount or concentration of each component present in the sample; and mapping the spatial distribution of these components across the sample.","Multivariate spectral analysis can be performed on a full spectrum image that can be represented as a two-dimensional data matrix D. A two-dimensional data matrix D can be obtained by unfolding a measured multidimensional spectral data set . For example, the multidimensional spectra data set  can be a data cube that comprises a 1D spectrum at each pixel on a 2D spatial grid corresponding to the (X,Y) coordinates of the pixel's location. The 2D data matrix D enables the easy and efficient use of standard linear algebra and matrix operations.","The data matrix D can be factored into the product of two matrices, C and S, according to:\n\nD=CS\u2003\u2003(1)\n\nwhere D has dimensions of m\u00d7p, and m is the number of pixels and p is the number of spectral channels. The matrix C is a concentration matrix, which is related to the concentration of the chemical phases (e.g., a matrix representing a map of the component abundances) and has dimensions of m\u00d7q, where q is the number of pure components. The matrix S is a spectral shapes matrix, which contains information about the spectral shapes of the pure chemical components (e.g., a matrix of the pure component spectra). S has dimensions p\u00d7q.\n","The factorization of Eq. (1) can be accomplished by an image analysis of D. A number of prior image analysis methods are described in U.S. Pat. No. 6,675,106 to Keenan and Kotula and U.S. Pat. No. 6,584,413 to Keenan and Kotula, which are incorporated herein by reference. A preferred image analysis method comprises a constrained MCR-ALS analysis of Eq. (1). A variety of constraint conditions can be used. For example, the constraint condition can be a non-negativity constraint. Alternatively, the constraint condition can be chosen to track a physical attribute. The concentrations C can be constrained to be either monotonically increasing or decreasing over time, for example, when monitoring a chemical reaction over time. Alternatively, the concentrations C or spectral shapes S can be constrained to be unimodal. Alternatively, the spectral shapes S can be constrained to a fixed number of spectral peaks (e.g., a single peak for chromatography, or, to less than three peaks). Alternatively, the spectral shapes S can be constrained to match the shape of a Gaussian distribution. The factors can also be partially constrained. For example, the concentration of a particular species can be zero at a particular location and unconstrained elsewhere.","In an exemplary MCR-ALS analysis, non-negativity constraints can be applied to the chemical concentrations and spectral intensities. One overall procedure for applying non-negativity constraints is described by R. Bro and S. De Jong, \u201cA Fast Non-Negativity-Constrained Least Squares Algorithm\u201d, 11, 393 (1997), which is incorporated herein by reference.","In a non-negativity constrained MCR-ALS analysis, an initial feasible estimate can be made for S (Bro starts with all zeros as an initial feasible solution). Eq. (1) can then be solved for C under the non-negativity constraint:",{"@attributes":{"id":"p-0046","num":"0045"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"munder":{"mi":["min","C"]},"mo":"\u2062","msub":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"D","mo":"-","msup":{"mi":["CS","T"]}}},"mi":"F"}},{"mrow":{"mi":["subject","to","C"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]},"mo":"\u2265","mn":"0."}],"mo":","}},{"mrow":{"mo":["(",")"],"mn":"2"}}]}}}},"br":{}},{"@attributes":{"id":"p-0047","num":"0046"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"munder":{"mi":["min","S"]},"mo":"\u2062","msub":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"D","mo":"-","msup":{"mi":["CS","T"]}}},"mi":"F"}},{"mrow":{"mi":["subject","to","S"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]},"mo":"\u2265","mn":"0."}],"mo":","}},{"mrow":{"mo":["(",")"],"mn":"3"}}]}}}},"br":{}},{"@attributes":{"id":"p-0048","num":"0047"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"munder":{"mi":"min","mrow":{"mi":["C","S"],"mo":","}},"mo":"\u2062","msub":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"D","mo":"-","msup":{"mi":["CS","T"]}}},"mi":"F"}},{"mrow":[{"mi":["subject","to","C"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]},{"mn":"0","mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":["and","S"]}],"mo":["\u2265","\u2265"],"mn":"0."}],"mo":","}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}}}}},"The previous discussion assumed that the MCR-ALS analysis started by making an initial estimate for S and then solving Eq. (2) for C, and so on. However, the roles of C and S can be easily interchanged by making an initial feasible estimate for C and first solving Eq. (3) for S.","If desired, the measured data matrix D can be weighted, depending on the type of experiment being analyzed, and depending on the properties of the noise or background signal generated during data acquisition. Weighting is generally used whenever the properties of the noise are not uniform throughout the measurement space (i.e., heteroscedastic noise). This is particularly true in the case of \u201ccounting\u201d experiments in which the noise is characterized as following a Poisson probability distribution in which the magnitude of the uncertainty varies with the magnitude of the signal. For multi-channel data acquisition the noise is not characterized by a single probability distribution, but rather, by a distribution whose parameters will, in principle, differ from channel to channel and from pixel to pixel within a channel. Additionally, heteroscedasticity can also arise from other effects, e.g., non-uniform detector responses, or mathematical transformations applied to the data (e.g., taking logarithms).","Weighting is also useful when there is a large disparity between the total number of counts (e.g., observations) arising from different elements or phases (e.g., a sample comprising a small amount of contaminant located on top of a substrate made of a single material). Weighting, therefore, is useful for accurately identifying minor phases, trace elements, or subtle gradients in composition across a sample. By properly accounting for experimental noise characteristics, chemically relevant features having small number of counts can become more significant, in a least squares sense, than large magnitude noise associated with major spectroscopic features.","Weighting can also be used to account for data \u201coutliers\u201d. Data outliers can include malfunctioning energy channels or pixel elements in a detector array. For example, a dead (i.e., inoperative) energy channel (e.g., zero signal) can be effectively removed from the data matrix D by assigning a sufficiently small weight. Alternatively, for detectors that use a CCD pixel array, an occurrence of a \u201chot\u201d (i.e., saturated) or dead pixel can be weighted in a similar fashion, namely, by assigning a sufficiently small weight.","Data matrix D can be weighted to create a weighted data matrix {hacek over (D)} according to:\n\n{hacek over (D)}=GDH\u2003\u2003(5)\n\nwhere G is a pre-multiply weighting matrix having dimensions m\u00d7m; and H is a post-multiply weighting matrix having dimensions p\u00d7p. In general, G is used to weight the row-space of D, and H is used to weight the column-space of D. Obviously, the weighting matrices G and H can be the identity matrix if no weighting is desired.\n","The matrix G can be used to account for unequal variance in the observations from pixel-to-pixel, independent of channel number (i.e., energy or wavelength). For example, weighting by G could be used in the case where there are hot or dead pixels, or if there was an array detector where different detector elements had different noise properties. Weighting by G also could be used in the situation where there were much higher total counts in some pixels as compared to others, caused, for example, by unequal dwell times.","The matrix H can account for unequal variance in the observations from channel-to-channel, independent of pixel location. For example, weighting by H could be used when the noise level varies from one energy channel to the next. For X-ray detection in spectral analysis, the noise will be higher on average in channels that have greater signal on average due to the Poisson nature of a counting experiment.","If the data matrix D is weighted, the test for convergence of the constrained ALS solution is:\n\nmin\u2225=min\u2225\u2225\u2003\u2003(6)\n\nwhere {hacek over (D)} is the weighted data matrix, {hacek over (C)}=GC is the weighted concentration matrix, and {hacek over (S)}=SH is the weighted spectral shapes matrix. Eq. (6) can be solved with appropriate constraints applied to {hacek over (C)} and {hacek over (S)}, as described above. After estimating the weighted concentrations and spectra, the corresponding unweighted concentrations and spectra can be recovered as C=G{hacek over (C)} and S={hacek over (S)}H. For simplicity, the analysis of the unweighted data matrix D will be described hereinafter, although it will be understood that the method of the present invention can also be applied to the weighted data matrix {hacek over (D)}.\n","Below are described spectral compression algorithms, which are the subject of the related U.S. patent application Ser. No. 10\/772,548, \u201cSpectral Compression Algorithms for the Analysis of Very Large Multivariate Images\u201d. The spatial compression algorithms of the present invention can be combined with these spectral compression algorithms to provide additional computational efficiencies in the analysis of large, high resolution full-spectrum images.","The prior MCR-ALS image analysis method described above, using the full data matrix D, the concentration matrix C, and the spectral shapes matrix S, requires a large amount of memory and computation time. For example, the computation time of CD in Eq. (3) is proportional to m\u00d7p\u00d7q.","The computational complexity can be reduced through a spectral compression of the data matrix using PCA. PCA is one of the core techniques of multivariate statistical analysis and it has been employed in numerous and diverse applications including dimensional reduction, data compression, exploratory data analysis, factor analysis, pattern recognition, classification, and multivariate calibration. In particular, PCA can be used in the analysis of hyperspectral image data. See P. Geladi and H. Grahn, , Wiley, Chinchester, UK (1996), which is incorporated herein by reference.","The goal of PCA is to extract the useful information in a high-dimension data set into a lower dimension subspace. From a geometric point of view, PCA begins by finding that single direction in the p-dimensional space that best describes the location of the data. The vector describing that direction is the first principal component. Once found, a second direction, orthogonal to the first, is determined that best accounts for the variation in the data that is orthogonal to the first. This is the second principal component. The process continues with each new principal component maximally accounting for the variation in the data that is orthogonal to all preceding components. The first few principal components will contain the chemical information of interest. If there are r such components, the remaining p\u2212r components are assumed to describe experimental noise or error. Limiting further analysis to the r-dimensional subspace defined by the first r principal components provides the desired dimensional reduction and data compression.","In matrix terms, PCA is concerned with factoring a data matrix D into the product of two other matrices, a scores matrix T whose columns are mutually orthogonal and a matrix P of orthonormal loading vectors, according to:\n\nD=TP\u2003\u2003(7)\n\nTo take a spectroscopic example, the loading vectors P describe the spectral characteristics of the chemical constituents of a sample and the scores T are related to their concentrations.\n","PCA is closely related to the SVD, which is often used to compute the PCA. SVD performs the factorization:\n\nD=U\u03a3V\u2003\u2003(8)\n\nIf D is an m\u00d7p matrix, then U and V are m\u00d7m and p\u00d7p orthogonal matrices, respectively, and \u03a3 is an m\u00d7p diagonal matrix containing the singular values along the diagonal, ordered by decreasing size. The right singular vectors V provide abstract representations of the spectra of the individual chemical components (e.g., elements or phases). The min(m, p) singular values are related to the amount of variance in the data that is accounted for by the corresponding principal components. Specifically, the isingular value is equal to the square root of the variance accounted for by the iprincipal component. The diagonal form indicates that the transformed data are uncorrelated. By decomposing the data into a set of uncorrelated factors of decreasing statistical significance, data compression can be accomplished by selecting those factors having the greatest statistical significance and discarding the rest as noise or error.\n","SVD has the useful property that the space spanned by the first r columns of V represents, in a least squares sense, the best rank r approximation to the space spanned by the rows of D. The remaining p\u2212r columns of V represent experimental noise or error and can be discarded. Thus, the scores and loading matrices can be truncated, or compressed, to contain only those vectors corresponding to significant singular values. Letting Vbe the matrix whose columns are the first r columns of V, a r-component PCA model can then be computed, according to:\n\n{tilde over (P)}=V\u2003\u2003(9)\n\n=(\u03a3)\u2003\u2003(10)\n\nwhere Eq. (10) follows from the orthonormality of the columns of V. Therefore, {tilde over (T)} is the spectrally compressed scores matrix and {tilde over (P)} is the spectrally compressed loadings matrix.\n","In the case of large spectral images, SVD becomes a very inefficient way to compute the principal components. As described below, in the typical case that the number of pixels, m, is greater than the number of spectral channels, p, a better approach is to consider the p\u00d7p crossproduct, or kernel matrix DD. Those skilled in the art will appreciate that the methods can also be applied to cases where p>m, in which case the crossproduct matrix would be computed as DD. See W. Wu, D. Massart, and S. de Jong, \u201cThe kernel PCA algorithms for wide data. Part I: theory and algorithms\u201d, 36, 165 (1997), which is incorporated herein by reference.","Starting with Eq. (8), the crossproduct matrix can be written\n\n(\u03a3\u03a3)\u2003\u2003(11)\n\nEq. (11) has the form of a standard symmetric matrix eigenvalue problem, which is readily solved by standard methods. E, in this equation, is a p\u00d7p diagonal matrix of eigenvalues sorted in descending order. Given V, the PCA model can, once again, be computed according to Eqs. (9) and (10), where only the first r columns of V are used to compute the compressed loading matrix {tilde over (P)} and the compressed scores matrix {tilde over (T)}.\n","Alternatively, the data may be presented as an arbitrary f-component factor model according to:\n\nD=AB\u2003\u2003(12)\n\nwhere A and B are m\u00d7f and p\u00d7f data factor matrices, respectively. Therefore, the data factor matrix A comprises the spatial information and the data factor matrix B comprises the spectral information. The f most significant eigenvalues Eof DD can be obtained from the solution to the generalized symmetric eigenvalue problem\n\n()()\u2003\u2003(13)\n\nand the corresponding f most significant eigenvectors of DD can be computed, according to:\n\nV=BY\u2003\u2003(14)\n\nA r-component PCA model for D, given r\u2266f, can then be computed, according to:\n\n{tilde over (P)}=BY\u2003\u2003(15)\n\n()\u2003\u2003(16)\n\nwhere the Yis the matrix whose columns are the first r columns of Y.\n","While the eigenvectors V include all of the information contained in the pure spectral components, they do not do so in a chemically recognizable form. Therefore, a single principal component will not, in general, represent either a pure element or a multi-element phase, but rather, a linear combination of such elements or phases. In other words, there may not be a one-to-one correspondence between a selected principal component and a particular chemical phase or pure element. For example, physically admissible concentrations must be non-negative, and the pure spectral components must be greater than the background signal, whereas a general principal components analysis need not be thus constrained. The principal components produced by PCA often have negative values, which presents a spectrum that is difficult for the practicing analyst to understand. Additionally, a major disadvantage of PCA is that the principal components are constrained to be orthogonal, while any chemical phase that contains overlapping spectral peaks will necessarily be non-orthogonal. Therefore, subsequent post-processing of results obtained from PCA is useful for transforming the abstract components into physically meaningful factors.","The principal components can be transformed into more physically realizable spectra by performing an MCR-ALS analysis using the PCA-factored representation of the data. The PCA-factored representation, TP, can be used in place of the data matrix, D, in Eq. (1), according to:\n\nTP=CS\u2003\u2003(17)\n\nFactorization can then be accomplished by performing a constrained MCR-ALS analysis of Eq. (17). As described above, an initial feasible estimate can be made for S. C can then be computed, subject to constraints, according to:\n",{"@attributes":{"id":"p-0069","num":"0068"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"munder":{"mi":["min","C"]},"mo":"\u2062","msub":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msup":[{"mi":["TP","T"]},{"mi":["CS","T"]}],"mo":"-"}},"mi":"F"}}},{"mrow":{"mo":["(",")"],"mn":"18"}}]}}}},"br":{}},{"@attributes":{"id":"p-0070","num":"0069"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"munder":{"mi":["min","S"]},"mo":"\u2062","msub":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msup":[{"mi":["TP","T"]},{"mi":["CS","T"]}],"mo":"-"}},"mi":"F"}}},{"mrow":{"mo":["(",")"],"mn":"19"}}]}}}},"br":{}},{"@attributes":{"id":"p-0071","num":"0070"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"munder":{"mi":"min","mrow":{"mi":["C","S"],"mo":","}},"mo":"\u2062","mrow":{"msub":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msup":[{"mi":["TP","T"]},{"mi":["CS","T"]}],"mo":"-"}},"mi":"F"},"mo":"."}}},{"mrow":{"mo":["(",")"],"mn":"20"}}]}}}},"br":{}},"Preferably, the MCR-ALS analysis can be performed using the spectrally compressed, PCA-factored representation of the data, subject to constraints, according to:",{"@attributes":{"id":"p-0073","num":"0072"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"munder":{"mi":"min","mrow":{"mi":["C","S"],"mo":","}},"mo":"\u2062","msub":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mrow":{"mover":{"mi":"T","mo":"~"},"mo":"\u2062","msup":{"mover":{"mi":"P","mo":"~"},"mi":"T"}},"mo":"-","msup":{"mi":["CS","T"]}}},"mi":"F"}}},{"mrow":{"mo":["(",")"],"mn":"21"}}]}}}},"br":{}},"While this approach is numerically more efficient than direct computation via SVD, the equations, as written, may not make the most effective use of cache memory and may still require the entire data set D to reside in memory in order to form the requisite matrix products. The key to developing an efficient algorithm for PCA that is suitable for larger-than-memory data sets is the realization that both the formation of the crossproduct matrix in Eq. (11) and the projection of the data onto the loading vectors in Eq. (10) can be accomplished in a blockwise manner.","Considering the case, once again, that m>p, D and T (or {tilde over (T)}) can be written as conformable block matrices:",{"@attributes":{"id":"p-0076","num":"0075"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"D","mo":"=","mrow":{"mrow":[{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msub":{"mi":"D","mn":"1"}}},{"mtd":{"msub":{"mi":"D","mn":"2"}}},{"mtd":{"mi":"\u22ee"}},{"mtd":{"msub":{"mi":"D","mrow":{"mi":"j","mo":"-","mn":"1"}}}},{"mtd":{"msub":{"mi":["D","j"]}}}]}},"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":["and","T"]},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msub":{"mi":"T","mn":"1"}}},{"mtd":{"msub":{"mi":"T","mn":"2"}}},{"mtd":{"mi":"\u22ee"}},{"mtd":{"msub":{"mi":"T","mrow":{"mi":"j","mo":"-","mn":"1"}}}},{"mtd":{"msub":{"mi":["T","j"]}}}]}}],"mo":"="}},"mo":"\u2062","mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}},{"mrow":{"mo":["(",")"],"mn":"22"}}]}}}},"br":{}},{"@attributes":{"id":"p-0077","num":"0076"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msup":{"mi":["D","T"]},"mo":"\u2062","mi":"D"},{"mrow":[{"mrow":[{"mo":["[","]"],"mrow":{"msubsup":[{"mi":["D","T"],"mn":"1"},{"mi":["D","T"],"mn":"2"},{"mi":["D","T"],"mrow":{"mi":"j","mo":"-","mn":"1"}},{"mi":["D","j","T"]}],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"\u2026"}},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msub":{"mi":"D","mn":"1"}}},{"mtd":{"msub":{"mi":"D","mn":"2"}}},{"mtd":{"mi":"\u22ee"}},{"mtd":{"msub":{"mi":"D","mrow":{"mi":"j","mo":"-","mn":"1"}}}},{"mtd":{"msub":{"mi":["D","j"]}}}]}}],"mo":"\u2061"},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"j"},"mo":"\u2062","mrow":{"msubsup":{"mi":["D","i","T"]},"mo":"\u2062","msub":{"mi":["D","i"]}}}],"mo":["\u2062","="],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"23"}}]}}}},"br":{},"sup":["T","T","T","T"],"sub":["i","i ","i","i "]},"After computing V and P in the standard way, a scores block can be constructed by projecting the data block Donto P, according to:\n\nT=DP\u2003\u2003(24)\n\nPreferably, a compressed scores block {tilde over (T)}can be constructed using the block version of Eq. (10). It is at this point that the larger-than-main-memory algorithm pays a penalty, since the data blocks Dhave to be read from disk a second time. The magnitude of the penalty depends on the details of the disk subsystem, total system memory, operating system file caching strategy, etc. However, experience has shown that the penalty is small in comparison to the overall computation time for large spectral image data sets.\n","Those skilled in the art will appreciate that the factorization can be applied to the case that there are more spectral channels than pixels (i.e., p>m). In this later case, the data block Dwould consist of the full spatial data at some number of spectral channels. The crossproduct matrix would be computed as DD, and the roles of T and P would be reversed. That is, the eigenvectors would be associated with a single T matrix and Pwould be computed blockwise, according to:\n\nP=TD\u2003\u2003(25)\n\nThe T matrix would have orthonormal columns and the columns of P would be orthogonal. The commonly assumed case of orthogonal columns for T and orthonormal columns for P could be achieved by applying a simple normalization to P and the inverse normalization to T.\n","Data presented in a factored form according to Eq. (12) is also compatible with a block algorithm. A and {tilde over (T)} can be written as conformable block matrices:",{"@attributes":{"id":"p-0081","num":"0080"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"A","mo":"=","mrow":{"mrow":[{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msub":{"mi":"A","mn":"1"}}},{"mtd":{"msub":{"mi":"A","mn":"2"}}},{"mtd":{"mi":"\u22ee"}},{"mtd":{"msub":{"mi":"A","mrow":{"mi":"j","mo":"-","mn":"1"}}}},{"mtd":{"msub":{"mi":["A","j"]}}}]}},"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"and","mover":{"mi":"T","mo":"~"}},{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msub":{"mover":{"mi":"T","mo":"~"},"mn":"1"}}},{"mtd":{"msub":{"mover":{"mi":"T","mo":"~"},"mn":"2"}}},{"mtd":{"mi":"\u22ee"}},{"mtd":{"msub":{"mover":{"mi":"T","mo":"~"},"mrow":{"mi":"j","mo":"-","mn":"1"}}}},{"mtd":{"msub":{"mover":{"mi":"T","mo":"~"},"mi":"j"}}}]}},"mo":["\u2062","."],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}],"mo":"="}}},{"mrow":{"mo":["(",")"],"mn":"26"}}]}}}},"br":{},"sup":"T"},{"@attributes":{"id":"p-0082","num":"0081"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msup":{"mi":["A","T"]},"mo":"\u2062","mi":"A"},{"mrow":[{"mrow":[{"mo":["[","]"],"mrow":{"msubsup":[{"mi":["A","T"],"mn":"1"},{"mi":["A","T"],"mn":"2"},{"mi":["A","T"],"mrow":{"mi":"j","mo":"-","mn":"1"}},{"mi":["A","j","T"]}],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"\u2026"}},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msub":{"mi":"A","mn":"1"}}},{"mtd":{"msub":{"mi":"A","mn":"2"}}},{"mtd":{"mi":"\u22ee"}},{"mtd":{"msub":{"mi":"A","mrow":{"mi":"j","mo":"-","mn":"1"}}}},{"mtd":{"msub":{"mi":["A","j"]}}}]}}],"mo":"\u2061"},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"j"},"mo":"\u2062","mrow":{"msubsup":{"mi":["A","i","T"]},"mo":"\u2062","msub":{"mi":["A","i"]}}}],"mo":["\u2062","="],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"27"}}]}}}},"br":{},"sup":["T","T","T"],"sub":["i","i "]},"Likewise, B and {tilde over (P)} can be written as conformable block matrices:",{"@attributes":{"id":"p-0084","num":"0083"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"B","mo":"=","mrow":{"mrow":[{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msub":{"mi":"B","mn":"1"}}},{"mtd":{"msub":{"mi":"B","mn":"2"}}},{"mtd":{"mi":"\u22ee"}},{"mtd":{"msub":{"mi":"B","mrow":{"mi":"k","mo":"-","mn":"1"}}}},{"mtd":{"msub":{"mi":["B","k"]}}}]}},"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"and","mover":{"mi":"P","mo":"~"}},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msub":{"mover":{"mi":"P","mo":"~"},"mn":"1"}}},{"mtd":{"msub":{"mover":{"mi":"P","mo":"~"},"mn":"2"}}},{"mtd":{"mi":"\u22ee"}},{"mtd":{"msub":{"mover":{"mi":"P","mo":"~"},"mrow":{"mi":"k","mo":"-","mn":"1"}}}},{"mtd":{"msub":{"mover":{"mi":"P","mo":"~"},"mi":"k"}}}]}}],"mo":"="}},"mo":"\u2062","mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}},{"mrow":{"mo":["(",")"],"mn":"28"}}]}}}},"br":{},"sup":"T"},{"@attributes":{"id":"p-0085","num":"0084"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msup":{"mi":["B","T"]},"mo":"\u2062","mi":"B"},{"mrow":[{"mrow":[{"mo":["[","]"],"mrow":{"msubsup":[{"mi":["B","T"],"mn":"1"},{"mi":["B","T"],"mn":"2"},{"mi":["B","T"],"mrow":{"mi":"k","mo":"-","mn":"1"}},{"mi":["B","k","T"]}],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"\u2026"}},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msub":{"mi":"B","mn":"1"}}},{"mtd":{"msub":{"mi":"B","mn":"2"}}},{"mtd":{"mi":"\u22ee"}},{"mtd":{"msub":{"mi":"B","mrow":{"mi":"k","mo":"-","mn":"1"}}}},{"mtd":{"msub":{"mi":["B","k"]}}}]}}],"mo":"\u2061"},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"k"},"mo":"\u2062","mrow":{"msubsup":{"mi":["B","i","T"]},"mo":"\u2062","msub":{"mi":["B","i"]}}}],"mo":["\u2062","="],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"29"}}]}}}},"br":{},"sup":["T","T","T"],"sub":["i","i "]},"After computing Y in the standard way as the solution to the generalized symmetric eigenvalue problem in Eq. (13), a block of loading vectors can be constructed according to:\n\n{tilde over (P)}=BY\u2003\u2003(30)\n\nand a scores block can be constructed according to:\n\n()\u2003\u2003(31)\n","Furthermore, the factored representation set can be spatially compressed, as described below. If a spatially compressed data factor matrix \u00c3 is used, a spatially compressed scores block {tilde over ({tilde over (T)}will be provided by Eq. (31).","In many applications, it is necessary to preprocess the raw data in some manner prior to performing PCA. If it is desired to work with the data's covariance matrix or correlation matrix rather than its crossproduct matrix, for instance, the raw data must be mean-centered and\/or variance-scaled. It may also be necessary to weight the data to account for non-uniform noise characteristics. Spectroscopic techniques that rely on photon counting, for example, need to scale the data to reflect the fact that the estimated variance for a given data element is equal to the data element itself. In all of these cases, it is possible either to perform the operation blockwise as the data is read, or to construct a transformation of the crossproduct matrix that achieves the same result.","Several possible transformations of the crossproduct matrix are discussed in the book by Geladi and Grahn. For example, given a crossproduct matrix Z=DD, the covariance and correlation matrices, Zand Z, can be computed, respectively, by",{"@attributes":{"id":"p-0090","num":"0089"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["Z","cov"]},"mo":"=","mrow":{"mrow":[{"mo":["(",")"],"mfrac":{"mn":"1","mrow":{"mi":"m","mo":"-","mn":"1"}}},{"mo":["(",")"],"mrow":{"mi":"Z","mo":"-","mrow":{"mi":"m","mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mover":{"mi":["d","_"]},"msup":{"mover":{"mi":["d","_"]},"mi":"T"}}}}],"mo":"\u2062"}}},{"mrow":{"mo":["(",")"],"mn":"32"}}]}}}},"br":[{},{},{}],"in-line-formulae":[{},{}],"i":["Z","=S","Z","S"],"sub":["cor","cov"],"sup":["\u22121","\u22121"],"o":"d"},"Clearly, Zand Zcan be computed after the crossproduct matrix has been assembled if  and s are available. This is readily accomplished in the block algorithm by accumulating the appropriate sums as the raw data is read. If a row of D, d, represents a single multivariate observation (e.g., the spectrum at a single pixel), then  and s are computed according to",{"@attributes":{"id":"p-0092","num":"0091"},"maths":{"@attributes":{"id":"MATH-US-00015","num":"00015"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mover":{"mi":["d","_"]},"mo":"=","mrow":{"mrow":[{"mo":["(",")"],"mfrac":{"mn":"1","mi":"m"}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","msub":{"mi":["d","i"]}}],"mo":"\u2062"}}},{"mrow":{"mo":["(",")"],"mn":"34"}}]}}}},"br":{}},{"@attributes":{"id":"p-0093","num":"0092"},"maths":{"@attributes":{"id":"MATH-US-00016","num":"00016"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"s","mo":"=","msqrt":{"mfrac":{"mrow":[{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","msubsup":{"mi":["d","i"],"mn":"2"}},{"mi":"m","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mover":{"mi":["d","_"]},"mn":"2"}}],"mo":"-"},{"mi":"m","mo":"-","mn":"1"}]}}}},{"mrow":{"mo":["(",")"],"mn":"35"}}]}}}},"br":{},"sub":["cov ","cor "]},"When the data is weighted, equivalent weighting can be applied to the crossproduct, if the weighting matrices are block diagonal in a way that is conformable with the way D is blocked. Typically, weighting matrices are chosen to be diagonal so this restriction poses little practical difficulty. In the general case that both the row-space and the column-space of D are to be weighted, the weighted data matrix {hacek over (D)} is given by Eq. (5). For m\u2267p, the crossproduct of the weighted data is computed as\n\n=()()=()\u2003\u2003(36)\n\nAssuming G is blocked conformably with D:\n",{"@attributes":{"id":"p-0095","num":"0094"},"maths":{"@attributes":{"id":"MATH-US-00017","num":"00017"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"D","mo":"=","mrow":{"mrow":[{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msub":{"mi":"D","mn":"1"}}},{"mtd":{"msub":{"mi":"D","mn":"2"}}},{"mtd":{"mi":"\u22ee"}},{"mtd":{"msub":{"mi":"D","mrow":{"mi":"j","mo":"-","mn":"1"}}}},{"mtd":{"msub":{"mi":["D","j"]}}}]}},"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"1.4em","height":"1.4ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":["and","G"]},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"msub":{"mi":"G","mn":"1"}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}]},{"mtd":[{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"msub":{"mi":"G","mn":"2"}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mn":"0"},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}]},{"mtd":[{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mi":"\u22f0"},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}]},{"mtd":[{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mn":"0"},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"msub":{"mi":"G","mrow":{"mi":"j","mo":"-","mn":"1"}}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}]},{"mtd":[{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"msub":{"mi":["G","j"]}}]}]}}],"mo":"="}}},{"mrow":{"mo":["(",")"],"mn":"37"}}]}}}},"br":{}},{"@attributes":{"id":"p-0096","num":"0095"},"maths":{"@attributes":{"id":"MATH-US-00018","num":"00018"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msup":[{"mi":["D","T"]},{"mi":["G","T"]}],"mo":["\u2062","\u2062"],"mi":"GD"},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"j"},"mo":"\u2062","mrow":{"msubsup":[{"mi":["D","i","T"]},{"mi":["G","i","T"]}],"mo":["\u2062","\u2062","\u2062"],"msub":[{"mi":["G","i"]},{"mi":["D","i"]}]}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"38"}}]}}}},"br":{},"sub":["i ","i"]},"In the case that the data is presented in the factored form according to Eq. (12), the data factor matrices A and B can be preprocessed prior to generalized eigenanalysis to achieve results equivalent to those obtained through eigenanalysis of preprocessed raw data. For example, scores and loading vectors corresponding to an analysis of the data covariance matrix can be obtained from the solution to the generalized symmetric eigenvalue problem:",{"@attributes":{"id":"p-0098","num":"0097"},"maths":{"@attributes":{"id":"MATH-US-00019","num":"00019"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mfrac":{"mn":"1","mrow":{"mo":["(",")"],"mrow":{"mi":"m","mo":"-","mn":"1"}}},"mo":["\u2062","\u2062"],"mrow":[{"mo":["(",")"],"mrow":{"mrow":[{"msup":{"mi":["A","T"]},"mo":"\u2062","mi":"A"},{"mi":"m","mo":"\u2062","msup":{"mover":{"mi":["aa","_"]},"mi":"T"}}],"mo":"-"}},{"mo":["(",")"],"mrow":{"msup":{"mi":["B","T"]},"mo":"\u2062","mi":"B"}}]},"mo":"=","mi":"EY"}},{"mrow":{"mo":["(",")"],"mn":"39"}}]}}}},"br":{}},{"@attributes":{"id":"p-0099","num":"0098"},"maths":{"@attributes":{"id":"MATH-US-00020","num":"00020"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mover":{"mi":"P","mo":"~"},"mo":"=","msub":{"mi":["BY","r"]}},{"mover":{"mi":"T","mo":"~"},"mo":"=","mrow":{"msqrt":{"mfrac":{"mn":"1","mrow":{"mo":["(",")"],"mrow":{"mi":"m","mo":"-","mn":"1"}}}},"mo":["\u2062","\u2062","\u2062"],"mrow":[{"mo":["(",")"],"mrow":{"mi":"A","mo":"-","mrow":{"msub":{"mn":"1","mi":"m"},"mo":"\u2062","msup":{"mover":{"mi":["a","_"]},"mi":"T"}}}},{"mo":["(",")"],"mrow":{"msup":{"mi":["B","T"]},"mo":"\u2062","mi":"B"}}],"msub":{"mi":["Y","r"]}}}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mtext":{}},{"mtext":{}}],"mi":"and"}},{"mrow":{"mo":["(",")"],"mn":"40"}}]}}}},"br":{},"sub":"m "},"Clearly, Eqs. (39) and (40) are compatible with the block algorithm since \u0101 can be accumulated as the blocks of A are read. If ais a row of A, then",{"@attributes":{"id":"p-0101","num":"0100"},"maths":{"@attributes":{"id":"MATH-US-00021","num":"00021"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mover":{"mi":["a","_"]},"mo":"=","mrow":{"mrow":[{"mo":["(",")"],"mfrac":{"mn":"1","mi":"m"}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","msub":{"mi":["a","i"]}}],"mo":"\u2062"}}},{"mrow":{"mo":["(",")"],"mn":"41"}}]}}}},"br":{}},{"@attributes":{"id":"p-0102","num":"0101"},"maths":{"@attributes":{"id":"MATH-US-00022","num":"00022"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mfrac":{"mn":"1","mrow":{"mo":["(",")"],"mrow":{"mi":"m","mo":"-","mn":"1"}}},"mo":["\u2062","\u2062"],"mrow":[{"mo":["(",")"],"mrow":{"mrow":[{"msup":{"mi":["A","T"]},"mo":"\u2062","mi":"A"},{"mi":"m","mo":"\u2062","msup":{"mover":{"mi":["aa","_"]},"mi":"T"}}],"mo":"-"}},{"mo":["(",")"],"mrow":{"msup":[{"mi":["B","T"]},{"mi":"S","mrow":{"mo":"-","mn":"2"}}],"mo":["\u2062","\u2062"],"mi":"B"}}]},"mo":"=","mi":"EY"}},{"mrow":{"mo":["(",")"],"mn":"42"}}]}}}},"br":{}},{"@attributes":{"id":"p-0103","num":"0102"},"maths":{"@attributes":{"id":"MATH-US-00023","num":"00023"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mover":{"mi":"P","mo":"~"},"mo":"=","mrow":{"msup":{"mi":"S","mrow":{"mo":"-","mn":"1"}},"mo":"\u2062","msub":{"mi":["BY","r"]}}},{"msqrt":{"mfrac":{"mn":"1","mrow":{"mo":["(",")"],"mrow":{"mi":"m","mo":"-","mn":"1"}}}},"mo":["\u2062","\u2062","\u2062"],"mrow":[{"mo":["(",")"],"mrow":{"mi":"A","mo":"-","mrow":{"msub":{"mn":"1","mi":"m"},"mo":"\u2062","msup":{"mover":{"mi":["a","_"]},"mi":"T"}}}},{"mo":["(",")"],"mrow":{"msup":[{"mi":["B","T"]},{"mi":"B","mrow":{"mo":"-","mn":"2"}}],"mo":["\u2062","\u2062"],"mi":"B"}}],"msub":{"mi":["Y","r"]}}],"mo":["\u2062","\u2062","\u2062","\u2062","="],"mstyle":[{"mtext":{}},{"mtext":{}}],"mi":"and","mover":{"mi":"T","mo":"~"}}},{"mrow":{"mo":["(",")"],"mn":"43"}}]}}}},"br":{}},{"@attributes":{"id":"p-0104","num":"0103"},"maths":{"@attributes":{"id":"MATH-US-00024","num":"00024"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"B","mo":"=","mrow":{"mrow":[{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msub":{"mi":"B","mn":"1"}}},{"mtd":{"msub":{"mi":"B","mn":"2"}}},{"mtd":{"mi":"\u22ee"}},{"mtd":{"msub":{"mi":"B","mrow":{"mi":"k","mo":"-","mn":"1"}}}},{"mtd":{"msub":{"mi":["B","k"]}}}]}},"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":["and","S"]},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"msub":{"mi":"S","mn":"1"}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}]},{"mtd":[{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"msub":{"mi":"S","mn":"2"}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mn":"0"},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}]},{"mtd":[{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mi":"\u22f0"},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}]},{"mtd":[{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mn":"0"},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"msub":{"mi":"S","mrow":{"mi":"k","mo":"-","mn":"1"}}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}]},{"mtd":[{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"msub":{"mi":["S","k"]}}]}]}}],"mo":"="}}},{"mrow":{"mo":["(",")"],"mn":"44"}}]}}}},"br":{}},{"@attributes":{"id":"p-0105","num":"0104"},"maths":{"@attributes":{"id":"MATH-US-00025","num":"00025"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msup":[{"mi":["B","T"]},{"mi":"S","mrow":{"mo":"-","mn":"2"}}],"mo":["\u2062","\u2062"],"mi":"B"},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"k"},"mo":"\u2062","mrow":{"msubsup":[{"mi":["B","i","T"]},{"mi":["S","i"],"mrow":{"mo":"-","mn":"2"}}],"mo":["\u2062","\u2062"],"msub":{"mi":["B","i"]}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"45"}}]}}}},"br":{},"sup":["T","T "],"sub":"i"},{"@attributes":{"id":"p-0106","num":"0105"},"maths":{"@attributes":{"id":"MATH-US-00026","num":"00026"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["S","ii"]},"mo":"=","msqrt":{"mfrac":{"mrow":[{"mrow":{"msubsup":{"mi":["b","i","T"]},"mo":["\u2062","\u2062"],"msup":{"mi":["A","T"]},"msub":{"mi":["Ab","i"]}},"mo":"-","msup":{"mrow":{"mi":"m","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["b","i","T"]},"mo":"\u2062","mover":{"mi":["a","_"]}}}},"mn":"2"}},{"mi":"m","mo":"-","mn":"1"}]}}}},{"mrow":{"mo":["(",")"],"mn":"46"}}]}}}},"br":{},"sub":["i ","i "]},"If the data is to be weighted, weighted scores and loading vectors can be obtained from the solution to generalized symmetric eigenvalue problem:\n\n()()\u2003\u2003(47)\n\naccording to:\n",{"@attributes":{"id":"p-0108","num":"0107"},"maths":{"@attributes":{"id":"MATH-US-00027","num":"00027"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mover":{"mi":"P","mo":"~"},"mo":"=","msub":{"mi":["HBY","r"]}},{"mover":{"mi":"T","mo":"~"},"mo":"=","mrow":{"mrow":[{"mo":["(",")"],"mi":"GA"},{"mo":["(",")"],"mrow":{"msup":[{"mi":["B","T"]},{"mi":["H","T"]}],"mo":["\u2062","\u2062"],"mi":"HB"}},{"msub":{"mi":["Y","r"]},"mo":"."}],"mo":["\u2062","\u2062"]}}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mtext":{}},{"mtext":{}}],"mi":"and"}},{"mrow":{"mo":["(",")"],"mn":"48"}}]}}}},"br":{}},{"@attributes":{"id":"p-0109","num":"0108"},"maths":{"@attributes":{"id":"MATH-US-00028","num":"00028"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":[{"msup":[{"mi":["A","T"]},{"mi":["G","T"]}],"mo":["\u2062","\u2062"],"mi":"GA"},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"j"},"mo":"\u2062","mrow":{"msubsup":[{"mi":["A","i","T"]},{"mi":["G","i","T"]}],"mo":["\u2062","\u2062","\u2062"],"msub":[{"mi":["G","i"]},{"mi":["A","i"]}]}}],"mo":"="},{"mrow":[{"msup":[{"mi":["B","T"]},{"mi":["H","T"]}],"mo":["\u2062","\u2062"],"mi":"HB"},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"k"},"mo":"\u2062","mrow":{"msubsup":[{"mi":["B","i","T"]},{"mi":["H","i","T"]}],"mo":["\u2062","\u2062"],"msub":{"mi":["HB","i"]}}}],"mo":"="}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mtext":{}},{"mtext":{}}],"mi":"and"}},{"mrow":{"mo":["(",")"],"mn":"49"}}]}}}},"br":{},"sub":["i ","i "]},{"@attributes":{"id":"p-0110","num":"0109"},"figref":"FIG. 4"},"At step , the data matrix D, comprising measured spectral data, is provided in disk storage. The data matrix D can comprise j blocks of data D, according to Eq. (22). Alternatively, a spatially compressed data matrix {tilde over (D)} comprising blocks of compressed data {tilde over (D)}can be provided, as described below.","At step , a block of data Dis read from disk a first time into the computer memory. The block is suitably sized to fit in core memory and consists of the full spectral data at some number of pixels.","If a covariance matrix is to be computed, the rows of Dcan be added to an accumulation of the row vectors \u03a3d. Alternatively, if a correlation matrix is to be computed, the squares of the rows of Dcan be added to an accumulation of the squared row vectors \u03a3d. The data blocks can also be row weighted as shown in Eq. (38).","The block crossproduct matrix DDis computed. The block crossproduct matrix DDis added to an accumulation of the block crossproduct matrices \u03a3DD. If all j of the data blocks have not been read in, another data block is read in, another block crossproduct matrix is computed and added to the accumulation until all of the block crossproduct matrices have been accumulated, according to Eq. (23), to provide the crossproduct matrix DD.","If the crossproduct matrix is to be transformed, the column-wise means of the data matrix d can be computed from the accumulated rows \u03a3d, according to Eq. (34). The covariance matrix Zcan then be computed, according to Eq. (32). Alternatively, the column-wise standard deviations of the data matrix s can be computed from the accumulated squared rows \u03a3d, according to Eq. (35). The correlation matrix Zcan then be computed, according to Eq. (33). If the data is to be column-weighted, the weighted crossproduct matrix can be computed according to Eq. (36).","At step , an eigenanalysis of the crossproduct matrix DD is performed. The eigenvectors V and eigenvalues E of the crossproduct matrix DD are calculated, according to Eq. (11). Alternatively, the eigenvalues of the covariance matrix Z, or correlation matrix Zcan be computed and sorted.","If spectral compression of the PCA factors is desired, a best rank r can be determined from the eigenvalues E. A ranked loadings matrix {tilde over (P)} can be constructed from V based on the number of factors r desired in the model, according to Eq. (9).","At step , a scores block Tis computed by re-reading a data block Dand computing the product of Dand P, according to Eq. (24). When using the ranked loadings matrix {tilde over (P)}, a ranked block {tilde over (T)}of the scores matrix can be computed.","At step , a PCA-factored representation of the data is provided, comprising the conformable block scores matrix T, obtained by accumulating the scores blocks T, according to Eq. (22); the loadings matrix P; and the eigenvalues E. Preferably, the ranked scores and loading vectors matrices {tilde over (T)} and {tilde over (P)} are truncated to contain only those vectors corresponding to the r significant eigenvalues.","Alternatively, a data set may be provided wherein the data matrix D has already been factored into the product of two matrices, according to Eq. (12). Such factorization can be done on the full data matrix on a large computer or blockwise on a smaller computer using the algorithms presented herein. The factorization can also be effected in an adaptive manner, in which case the full data matrix may never be available, even in principle. Those skilled in the art will recognize that the spectral compression methods can be applied to any factored representation of the data, such as spectrally compressed scores and loading matrices, rotated PCA factors, MCR factors, etc.","In  is shown a method for obtaining the spectrally compressed scores matrix {tilde over (T)} and loading matrix {tilde over (P)} from the data factor matrices A and B. For example, the data factor matrices A and B can be an uncompressed scores matrix T and an uncompressed matrix P of loading vectors. The method comprises a series of sequential steps that can be performed on a computer.","At step , the data factor matrices A and B are provided. These matrices can be written as conformable block matrices, as in Eqs. (26) and (28). Therefore, the data factor matrix A can comprise j blocks of data factors Aand the data factor matrix B can comprise k blocks of data factors B.","At step , a block of data factors A, is read from a disk into the computer memory. The block is suitably sized to fit in core memory. The transpose of the data factor block Ais computed and the block crossproduct data factor matrix AAis computed. The block crossproduct data factor matrix AAis added to an accumulation of the block crossproduct data factor matrices \u03a3AA. If all j of the data factor blocks have not been read in, another data factor block is read in, another block crossproduct data factor matrix is computed and added to the accumulation until all of the block crossproduct data factor matrices have been accumulated, to provide the crossproduct data factor matrix AA, according to Eq. (27). Similarly, a block of data factors Bis read in, a block crossproduct data factor matrix BBis computed, and the block crossproduct data factor matrices are accumulated to provide the crossproduct data factor matrix BB, according to Eq. (29).","At step , an eigenanalysis of the crossproduct matrix AA\u00d7BB is performed, according to Eq. (13). The generalized eigenvectors Y and eigenvalues E of the crossproduct matrix are calculated. The most significant eigenvectors Vof DD can be determined, according to Eq. (14).","At step , a r-component PCA model for D, given r\u2266f, can then be computed according to Eqs. (15) and (16).","At step , an r-component, PCA-factored representation of the data is provided, comprising the ranked scores matrix {tilde over (T)}, the ranked blocked loadings matrix {tilde over (P)}, and the r significant eigenvalues E.","To obtain chemically recognizable solutions for the concentration and spectral shapes matrices, image analysis can be performed on the PCA-factored representation of the data. Spectral image analysis techniques suitable for the present invention include PCA, weighted SVD, MCR-ALS, or other multivariate analysis techniques known to those skilled in the art. See, e.g., Kotula et al., \u201cAutomated Analysis of SEM X-Ray Spectral Images: A Powerful New Microanalysis Tool,\u201d 9, 1 (2003).","A preferred spectral image analysis technique is MCR-ALS. In  is shown a method for the image analysis using MCR-ALS that can be applied to the PCA-factored representation TP(or the spectrally compressed, r-component PCA-factored representation {tilde over (T)}{tilde over (P)}).","At step , an initial spatial estimate is made for the concentration matrix C. This provides an initial feasible estimate for the spectral shapes matrix S at step .","At step , a least squares solution for C can be computed from Eq. (18), using the PCA-factored representation of the data TP(or r-component PCA-factored representation {tilde over (T)}{tilde over (P)}) and the initial feasible estimate for S, subject to constraints at step .","At step , a least squares solution for S can be computed from Eq. (19), using the PCA-factored representation of the data {tilde over (T)}{tilde over (P)}(or r-component PCA-factored representation {tilde over (T)}{tilde over (P)}) and the least squares solution for C from step , subject to constraints at step .","At step , a convergence metric is computed and compared to a convergence criterion. If the convergence metric is not less than the convergence criterion, the procedure is returned to step  with the updated least squares solutions for S and C and steps  and  are repeated.","If the convergence metric is less than the convergence criterion at step , an acceptable level of convergence is achieved and a least squares solution for S and C is provided at step .","Alternatively, the roles of C and S can be easily interchanged by providing an initial feasible estimate for C at step  and first solving for S at step .","The block PCA algorithm outlined in the previous section has been implemented and applied to several data sets ranging in size from 1024to 1024(\u02dc10to \u02dc10) individual data elements. A C-language implementation of the algorithm was constructed using the Microsoft Visual C++ compiler and it makes extensive use of the Intel Math Kernel and Performance Primitives libraries for performing the linear algebra operations. The Intel library contains versions of the BLAS and LAPACK routines that are optimized for Intel processors. All calculations were performed using single precision floating point arithmetic. In designing the implementation, care was taken to select block sizes that made efficient use of the cache and main memory available in the system. All of the example data sets represent energy dispersive x-ray spectrum images as acquired in a scanning electron microscope. The data are stored on disk in a compressed format and were read into the PCA program using a dynamic link library supplied by the manufacturer. The calculations were all performed on a Dell Precision 420 Workstation containing a Pentium III processor running at 1 GHz with 1 Gbyte of main memory.","In  is shown the first six principal components obtained from a PCA analysis of one of the spectrum images (data set \u201cB\u201d in Table 1). The analysis was performed on the unweighted data matrix. The sample consisted of series of wires having varying composition embedded in an epoxy block, which was then cross-sectioned and imaged with EDX. The first five components clearly represent the chemical information present in the data set while the sixth and subsequent components seemingly represent noise. Using the C-language implementation of the block PCA algorithm, all 1024 principal components of this data set were computed in 50 sec. By way of comparison, computation of PCA using SVD in MATLAB required 599 sec. for the same data. For the significant components, the block PCA algorithm and SVD yielded results that are the same within numerical precision.","In this data set, as is typical of full spectrum images, the number of principal components representing noise greatly outnumbers the chemically relevant principal components. This provides an opportunity to spectrally compress the data substantially by truncating the score and loading matrices to contain only those vectors corresponding to significant singular values (or eigenvalues). For purposes of compression it is not necessary to know exactly how many significant components are needed to represent the chemical information, but rather, to have an upper limit to that number. See R. Bro and C. Andersson, \u201cImproving the speed of multi-way algorithms: Part II: Compression,\u201d 42, 105 (1998). To evaluate the penalty exacted by computing more than the absolute minimum number of components, the time to compute truncated PCA models of varying size for data set \u201cB\u201d was measured. The results are shown in . These results indicate that computation time is relatively insensitive to the model size for small numbers of components. This is reasonable since the time required to read the full data set and to compute the crossproduct matrix is independent of model size and is the dominant calculation for small models.","Table 1 summarizes the results of timing the block PCA algorithm while computing 100-component PCA models for data sets varying in size by a factor of over 1000. The times required to complete each of the major subparts of the algorithm are also listed. These include reading the data from disk, forming the data crossproduct matrix, performing an eigenanalysis of the crossproduct matrix, reading the data a second time, if necessary, and projecting the data into the subspace defined by the loading vectors to compute the scores. It should be noted that the time required to read the data from disk a second time when a data set is larger than can be held in memory is the only cost that is unique to the out-of-main-memory algorithm. In other words, the times required for the other four steps will be the same irrespective of whether of not the full data set can be contained within main memory. In general, the computation times scale as expected for the case that the number of pixels is greater than or equal to the number of spectral channels. Reading the data and projecting the data onto the loading vectors takes time approximately proportional to the data size (m\u00d7p). Forming the crossproduct matrix, on the other hand, takes time proportional to the number of pixels and proportional to the square of the number of channels (m\u00d7p). Finally, eigenanalysis requires time proportional to the cube of the number of channels (p). Based on the computational complexities of the subtasks, it would be expected that for sufficiently large images, formation of the crossproduct matrix should become the dominant step in the calculation. This was observed, in practice, as shown in Table 1. It is also noteworthy that for all of the images analyzed, the time to compute the crossproduct matrix was substantially larger than the time required to read the data from disk. This belies our intuition that an operation requiring disk access will necessarily be the slow step in an algorithm.",{"@attributes":{"id":"p-0139","num":"0138"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 1"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Times required to compute 100-component PCA models for energy"},{"entry":"dispersive x-ray images of various sizes. The times are broken down by"},{"entry":"the major tasks accomplished by the block PCA algorithm."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"35pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Data Set","A","B","C","D","E","F"]},{"entry":{"@attributes":{"namest":"1","nameend":"7","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"7","colwidth":"35pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Number of","1024","16384","65536","65536","262144","1048576"]},{"entry":"Pixels"},{"entry":["Number of","1024","1024","1024","2048","1024","1024"]},{"entry":"Channels"},{"entry":["Data Size","4","64","256","512","1024","4096"]},{"entry":"(Mbyte)"},{"entry":["1Read","0.23","3","9.9","17.8","76","260"]},{"entry":"(sec)"},{"entry":["Form","0.66","10.7","42.7","165","171","682"]},{"entry":"Cross-"},{"entry":"product"},{"entry":"(sec)"},{"entry":["Eigen-","3.4","3.3","3.4","25.7","3.4","4"]},{"entry":"analysis"},{"entry":"(sec)"},{"entry":["2Read","\u2014","\u2014","\u2014","0.1","6","64"]},{"entry":"(sec)"},{"entry":["Compute","0.05","0.7","2.9","5.6","11","48"]},{"entry":"Scores"},{"entry":"(sec)"},{"entry":["Total","4.3","17.7","58.9","214","267","1058"]},{"entry":"Time (sec)"},{"entry":{"@attributes":{"namest":"1","nameend":"7","align":"center","rowsep":"1"}}}]}}]}}},"The remaining discussion of the block PCA algorithm concerns its performance when the data sets are larger than available main memory. As noted above, the only performance penalty incurred by such large data sets arises from the need to read the data from disk a second time. Based on computational complexity arguments similar to those presented above, it would be expected, once again, that as data sets become larger, reading the data becomes relatively less important to the overall computation time. Experimental verification of this expectation is a more difficult here, however, owing to the lack of sufficiently large data sets and the fact that the time required to read given data from disk a second time depends on operating system caching strategies, etc. In the worst case, the time it takes to read the data twice should be no longer than double the time it takes to read the data once. Allowing this, we find that for the two data sets that do require an out-of-main-memory algorithm, data set \u201cE\u201d which is approximately the size of main memory, and data set \u201cF\u201d, which is about 4 times larger, the expected trend is observed. Finally, for these two large data sets, the time penalty incurred by the block PCA algorithm amounts to only a few percent of the total analysis time, thus establishing the feasibility of applying such techniques to the solution of very large problems of real chemical interest.","The present invention is directed to spatial compression algorithms, described below. The spatial compression algorithms can be combined with the spectral compression algorithms, described above, that are the subject of the related U.S. patent application Ser. No. 10\/772,548, to provide additional computational efficiencies in the analysis of large, high resolution full-spectrum images.","The goal of spatial compression is to map an image into a compressed image containing a smaller number of pixels that retain the original image's information content. Wavelet transforms can provide a multiresolution representation of the image, by hierarchically decomposing it in terms of a coarse overall image and successively finer levels of detail. The efficacy of a multiresolution technique can be judged by the degree of compression achieved during the transformation and the preservation of the information contained in the image following reconstruction or decompression. High compression levels can generally be achieved due to the large amount of redundancy in a typical spectrum image. Additionally, spatial compression is a filtering operation that improves the signal-to-noise ratio of the data. Therefore, working with spatially compressed data can provide a better result while utilizing fewer computer resources.","Such multiresolution techniques are common in conventional image processing. For example, the JPEG2000 (Joint Photographers Expert Group) standard is a transform-based compression method used in digital photography applications. It uses smoothing wavelet filters and coefficient thresholding to separate spatial information according to frequency and orientation. The JPEG2000 wavelet basis is chosen to maximize compression while minimizing distortion as perceived by the human visual system.","However, the JPEG compression technique may be unacceptable for scientific images intended for measurement and analysis. Other sets of basis functions may provide a more efficient and accurate representation of a scientific image. In particular, a basis should be selected that is aligned with the underlying physical model. Most spectroscopic analyses follow Beer's Law, a linear additive model. That is, at each spatial location, the measured spectrum is a linear combination of pure-component spectra.","The Haar wavelet transform is the simplest wavelet basis, comprising a square step. The Haar wavelets provide an excellent basis for the multiresolution processing of spectral images for compositional analysis. Application of the Haar transform to an image results in an overall average approximation of the original image and detail coefficients in order of increasing resolution. Using the Haar basis, the approximation coefficients are obtained simply by co-adding spectra from adjacent pixels. These approximation pixels will also follow the linear additive model and, as long as there is sufficient variation in the compressed data to span the spectral space of the original data, there will be no loss of spectral information. Furthermore, since the Haar basis is orthonormal, the least squares problem can be solved using the wavelet coefficients themselves, without having to reconstruct the data from those coefficients. Although the Haar wavelets provide an excellent basis for chemical analysis, other general wavelet transformations can also be used in the spatial compression algorithm described herein.","In  is shown the EDX image of the cross-sectioned wires embedded in the epoxy block. In  is shown a one-level Haar wavelet transformation of the original image into four subimages by applying low pass (L) and high pass (H) filters to each of the two spatial dimensions. Therefore, the upper left quadrant (LL) of the transformed image provides an overall average, or approximation, of the original image; the lower left quadrant (LH) provides the vertical details; the upper right quadrant (HL) provides the horizontal details; and the lower right quadrant (HH) provides the diagonal details. By thresholding or throwing away (i.e., decimating) the detail coefficients and recursively decomposing the approximation subimages by repeating the filtering process on the low-low pass quadrant of the transformed image, a multiresolution pyramid comprising multiple levels of the approximation subimages can be constructed. Accordingly, very high levels of compression of the original image can be achieved.","As indicated previously, typically the goal of spectrum image analysis is to factor a multidimensional data set  into the product of the concentration array C and the transpose of the spectral shapes matrix S, according to:\n\n=S\u2003\u2003(50)\n\nFor a 3-way data cube, D has the dimensions of m\u00d7n\u00d7p, C has the dimensions m\u00d7n\u00d7q and S has dimensions p\u00d7q, where m\u00d7n is the number of pixels in the two spatial dimensions, p is the number of channels, and q is the number of pure components.\n","The first step in a typical multivariate image analysis algorithm is to unfold the data cube  and the concentration array  into the 2D data matrix D and the 2D concentration matrix C according to:\n\n()() . . . ()\u2518\u2003\u2003(51)\n\nand\n\n()() . . . ()\u2518\u2003\u2003(52)\n\nthen\n\nmin\u2225\u2225=min\u2225\u2225\u2003\u2003(53)\n\nwhich can be solved by normal linear algebra.\n","The Haar transform can be implemented as an orthogonal matrix as:",{"@attributes":{"id":"p-0150","num":"0149"},"maths":{"@attributes":{"id":"MATH-US-00029","num":"00029"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["W","l"]},"mo":"=","mrow":{"mfrac":{"mn":"1","msqrt":{"mn":"2"}},"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mrow":{"msub":{"mi":"I","mrow":{"mi":"l","mo":"\/","mn":"2"}},"mo":"\u2297","mrow":{"mo":["[","]"],"mrow":{"mn":["1","1"],"mo":","}}}}},{"mtd":{"mrow":{"msub":{"mi":"I","mrow":{"mi":"l","mo":"\/","mn":"2"}},"mo":"\u2297","mrow":{"mo":["[","]"],"mrow":{"mn":"1","mo":",","mrow":{"mo":"-","mn":"1"}}}}}}]}}}},{"mrow":{"mo":["(",")"],"mn":"54"}}]}}}},"br":[{},{},{},{},{},{}],"figref":"FIG. 10","u":["D","{tilde over (D)}"],"in-line-formulae":[{},{},{},{},{},{},{},{}],"i":["W","\u00d7","\u00d7W","=","W","{circle around (x)}W","vec","D","vec","W","{circle around (x)}W","D={tilde over (D)}"],"sub":["m",". . . k","n",". . . k","n","m",". . . k",". . . k","m ","n","n","m"],"sup":["T","th ","T "]},"The Haar transform combines and permutes the elements of the data matrix into wavelet coefficients of an approximation matrix {tilde over (D)}and detail matrices which can be adjoined to form a single detail matrix {tilde over (D)}. That is, the wavelet transformation partitions the matrix {tilde over (D)}, according to:",{"@attributes":{"id":"p-0152","num":"0151"},"maths":{"@attributes":{"id":"MATH-US-00030","num":"00030"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mover":{"mi":"D","mo":"~"},"mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msub":{"mover":{"mi":"D","mo":"~"},"mi":"a"}}},{"mtd":{"msub":{"mover":{"mi":"D","mo":"~"},"mi":"d"}}}]}}}},{"mrow":{"mo":["(",")"],"mn":"58"}}]}}}}},"Especially if there are not too many sharp edges in the spectral image, most of the information (i.e., energy) will be contained in the approximation coefficients {tilde over (D)}(i.e., the upper left quadrant in ). The Haar approximation coefficients {tilde over (D)}simply represent spectral mixing on a broader scale. If the linear additive model holds and there remains sufficient variation in these \u201csuper\u201d pixels to span the spectral space, there will be no loss of spectral information by setting {tilde over (D)}to zero for purposes of spectral image analysis. Of course, prior to setting them to zero, the details can be stored using standard data compression techniques to enable inverse wavelet transformation to recover the concentration maps at the original spatial resolution.","Because the wavelet coefficients for a particular pixel depend solely on the pixels in a small neighborhood surrounding that pixel, only that neighborhood of pixels needs to be in memory at any given time. In the case of the Haar transform, the mapping of the pixel locations in the original image to pixel locations in the compressed image is known a priori for any arbitrary level of compression. Consequently, as each individual spectrum is read, it can be accumulated immediately into the appropriate compressed pixel and then discarded. Therefore, the Haar wavelet transform can be computed on-the-fly, such that the entire data set or image never needs to reside simultaneously in memory. This allows even larger problems to be solved on a given computer and enables faster computations.","Because the wavelet transform is linear and the Haar basis is orthonormal, a least squares analysis can be performed on the wavelet coefficients rather than the image data. By recognizing that (W{circle around (x)}W) is an orthogonal matrix, the MCR-ALS analysis can be accomplished in terms of the transformed data, according to:\n\nmin\u2225\u2225=min\u2225()()\u2225=min\u2225\u2225\u2003\u2003(59)\n\nsubject to transformed constraints. For example, assuming a constraint of non-negativity of concentrations (i.e., C\u22670), the transformed non-negativity constraint becomes:\n\n()0\u2003\u2003(60)\n\nwhere {tilde over (C)} is a transformed concentration matrix.\n","If the detail coefficients are set to zero, the least squares problem in Eq. (59) becomes",{"@attributes":{"id":"p-0157","num":"0156"},"maths":{"@attributes":{"id":"MATH-US-00031","num":"00031"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"min","mo":"\u2062","msub":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mover":{"mi":"D","mo":"~"},"mo":"-","mrow":{"mover":{"mi":"C","mo":"~"},"mo":"\u2062","msup":{"mi":["S","T"]}}}},"mi":"F"}},{"mrow":[{"mi":"min","mo":"\u2062","msub":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mrow":[{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msub":{"mover":{"mi":"D","mo":"~"},"mi":"a"}}},{"mtd":{"mn":"0"}}]}},{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msub":{"mover":{"mi":"C","mo":"~"},"mi":"a"}}},{"mtd":{"msub":{"mover":{"mi":"C","mo":"~"},"mi":"d"}}}]}},"mo":"\u2062","msup":{"mi":["S","T"]}}],"mo":"-"}},"mi":"F"}},{"mi":"min","mo":"\u2062","msub":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msub":{"mover":{"mi":"D","mo":"~"},"mi":"a"},"mo":"-","mrow":{"msub":{"mover":{"mi":"C","mo":"~"},"mi":"a"},"mo":"\u2062","msup":{"mi":["S","T"]}}}},"mi":"F"}}],"mo":"="}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"61"}}]}}}},"br":{},"sub":["a ","d ","a"]},"Therefore, an equivalent solution to the MCR-ALS problem can be obtained working with either the original data or the wavelet coefficients. The advantage of working in the wavelet domain is that most of the spatial information of interest is contained in a few significant wavelet coefficients, much as most of the spectral information is contained in a few principal components in the PCA factoral representation. By thresholding the coefficients, the interesting spatial information can be concentrated in a reduced number of significant wavelet coefficients that is small compared to the number of original pixels. In particular, the MCR-ALS analysis can be performed on a compressed data matrix consisting of only the approximation coefficients. For example, using only the approximation coefficients at level two in both spatial dimensions and decimating the details enables the computation to be performed using only 1\/16 the number of coefficients as original pixels.","Finally, if the individual pixels follow Beer's law, the Haar approximation coefficients do also. Therefore, the approximation coefficients are given, within a normalization factor, as a sum of adjacent pixels. That is, a level one decomposition in both rows and columns results in a summing over a 2\u00d72 pixel neighborhood. From a physical viewpoint, the unmixing problem, or MCR, assumes that the signal arising from adjacent pixels is a linear combination of pure component spectra weighted by the amount of each component in the mixed pixel. As long as the linear model holds and a sufficient number of linearly independent mixtures are retained in the approximation, analysis of the approximation coefficients alone will provide a satisfactory curve resolution or spectral unmixing. A preferred level of compression maximizes the compression while still retaining sufficient variation in the compressed pixels to span the spectral space of the original data. Therefore, good results can generally be obtained as long as the number of compressed pixels remains substantially larger than the number of spectral pure components.","The spatial compression algorithm can also be applied to any arbitrary factored representation of the data, according to Eq. (12), where the data factor matrix A comprises the spatial information and the data factor matrix B comprises the spectral information. The data factor matrix A can be transformed, using a wavelet transform, to provide a transformed data factor matrix \u00c3. The wavelet coefficients of the transformed data factor matrix \u00c3 can be spatially compressed by thresholding the wavelet coefficients. An image analysis can be performed using the transformed data factor matrix \u00c3 and the data factor matrix B to provide a transformed concentration matrix {tilde over (C)} and a spectral shapes matrix S. A concentration matrix C can then be computed from the transformed concentration matrix {tilde over (C)}. Furthermore, the data factor matrices A and B can be blocked and the concentration matrix C can be accumulated blockwise.","Preferably, the spatial compression algorithm using wavelets can be combined with the spectral compression algorithm to provide even greater computational efficiencies. In summary, the spectrally compressed data can be spatially compressed, according to:\n\n=()\u00d7{tilde over ()}=()\u00d7\u2003\u2003(62)\n\nImage analysis, preferably using MCR-ALS, can then be applied to the spectrally and spatially compressed data representation {tilde over ({tilde over (T)}{tilde over (P)}, according to:\n\nmin\u2225\u2225=min\u2225()()\u2225=min\u2225\u2003\u2003(63)\n\nsubject to appropriate constraints. Again, the detail coefficients have been set to zero.\n","Image analysis, according to either Eq. (61) or Eq. (63), provides an estimated spectral shapes matrix S, comprising a set of pure component spectra, and a spatially compressed concentration matrix {tilde over (C)}. The goal of decompression or reconstruction is to estimate a components map at the full spatial resolution. Such a components map can be obtained from the concentration matrix C.","The concentration matrix C can be computed from the compressed concentration matrix {tilde over (C)}by inverse wavelet transformation. In this context, {tilde over (C)}represents the approximation coefficients as transformed into the spectral shapes basis defined by S. Inverse wavelet transformation involves rereading the detail coefficients back into memory and transforming them to the spectral basis defined by S, according to:\n\n{tilde over (C)}={tilde over (D)}S\u2003\u2003(64)\n\nThen, the component maps at full spatial resolution are obtained by performing the inverse wavelet transform:\n",{"@attributes":{"id":"p-0164","num":"0163"},"maths":{"@attributes":{"id":"MATH-US-00032","num":"00032"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"C","mo":"=","mrow":{"msup":{"mrow":[{"mo":["(",")"],"mrow":{"msub":[{"mi":["W","n"]},{"mi":["W","m"]}],"mo":"\u2297"}},{"mo":"-","mn":"1"}]},"mo":"\u00d7","mrow":{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msub":{"mover":{"mi":"C","mo":"~"},"mi":"a"}}},{"mtd":{"msub":{"mover":{"mi":"C","mo":"~"},"mi":"d"}}}]}},"mo":"."}}}},{"mrow":{"mo":["(",")"],"mn":"65"}}]}}}}},"Alternatively, because the spectral shapes matrix S in Eq. (53) is equivalent to the spectral shapes matrix S estimated from the compressed data in Eq. (61) or (63), C can also be computed directly from the original data D (or the PCA-factored representation of the data TP). Therefore, a preferred approach is to project the original data set D (or the product of T and P) and the estimated spectral shapes matrix S to solve the least squares problem",{"@attributes":{"id":"p-0166","num":"0165"},"maths":{"@attributes":{"id":"MATH-US-00033","num":"00033"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"munder":{"mi":["min","C"]},"mo":"\u2062","msub":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"D","mo":"-","msup":{"mi":["CS","T"]}}},"mi":"F"}},{"munder":{"mi":["min","C"]},"mo":"\u2062","msub":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msup":[{"mi":["TP","T"]},{"mi":["CS","T"]}],"mo":"-"}},"mi":"F"}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"66"}}]}}}},"br":{}},"Furthermore, the solution to Eq. (66) can be obtained by reading the data blocks D(or the scores blocks T) sequentially and solving the least squares problem blockwise. The concentration matrix C can then be accumulated as concentration blocks C, according to:\n\n]\u2003\u2003(67)\n\nTherefore, the full spatial and spectral resolution can be achieved in an acceptable MCR-ALS analysis, at the cost of reading the data twice.\n",{"@attributes":{"id":"p-0168","num":"0167"},"figref":"FIG. 11","sup":["T ","T"],"b":["42","46"]},"At step , the full 2D data matrix D (or a scores matrix T or {tilde over (T)}) is provided in disk storage. The matrix D can comprise block matrices.","At step  is shown conceptually a folded 3-way data cube , comprising an m-row\u00d7n-column\u00d7p-channel data set, that is represented by the data matrix D in step , according to Eq. (51).","The wavelet transform can be applied channel-wise to the folded data cube , according to Eq. (55), and as illustrated conceptually in . As implemented on a computer, the wavelet transformation would actually be applied to the unfolded data matrix D, according to Eq. (57).","At step  is shown an exemplary two-level wavelet transform of the data cube , comprising the second-level approximation in the upper left hand quadrant of the second-level transformed subimages (i.e., LL), the second-level details (i.e., HL, LH, and HH), and the first-level details (i.e., HL, LH, and HH). Those skilled in the art will realize that arbitrary levels of compression can be applied to the original image.","At step , the exemplary second-level transformed image can be decimated by throwing away the first- and second-level details to leave only the second-level approximation coefficients. Additionally, at step , the detail coefficients can be quantized, thresholded, and encoded and the retained detail coefficients can be stored.","At step , the approximation coefficients can be unfolded to provide the spatially compressed 2D data matrix {tilde over (D)}(or {tilde over (T)}).","The spatially compressed data matrix {tilde over (D)}can be analyzed by standard image analysis methods. Preferably, MCR-ALS is performed on the spatially compressed data set {tilde over (D)}, or the spatially compressed PCA-factored representation of the data {tilde over (T)}{tilde over (P)}, to provide a spectral shapes matrix S and a compressed concentration matrix {tilde over (C)}, according to Eq. (61) or (63).","In  is shown conceptually two methods for computing the concentration matrix C from the spatially compressed data and reconstructing the component maps at full spatial resolution.","In method , the compressed concentration matrix {tilde over (C)}is decompressed using an inverse wavelet transform to obtain the concentration matrix C. Since the mapping of pixels can be done on a computer, there is no need to actually do the folding and unfolding, as depicted conceptually in steps -, in the computer implementation.","At step , the compressed concentration matrix {tilde over (C)}is provided.","At step  is shown conceptually the folded second-level approximation subimages (i.e., LL) of the compressed concentration matrix {tilde over (C)}.","At step , the stored detail coefficients are decoded, unfolded, projected onto S, and refolded.","At step , the refolded concentration details (i.e., HL, LH, HH, HL, LH, and HH) are added to the second-level approximation subimages to provide the second-level images of the compressed concentration matrix.","At step , the inverse wavelet transformation is applied to the second-level compressed concentration matrix to provide the folded concentration array .","At step , the folded concentration array  is unfolded to provide the concentration matrix C, according to Eq. (52). A component map can be reconstructed from the concentration matrix C.","In method , the concentration matrix C is computed by projection of the data matrix D (or the PCA-factored representation of the data matrix TP) onto the estimated spectral shapes matrix S.","At step , if the data has been spectrally compressed and the analyst is working with a PCA-factored representation of the data TP, the analyst proceeds to step . If the analyst is working with the original data set D, the analyst proceeds to step .","At step , a scores block Tof a total of j scores blocks is read into memory.","At step , a least squares solution is obtained for a concentration block Cby projecting the product of the scores block Tand P onto the estimated spectral shapes matrix S, according to Eq. (66) and subject to constraints at step .","At step , the concentration blocks Care accumulated.","At step , the steps - are repeated until all of the j scores blocks have been read into memory and all j concentration blocks have been accumulated.","At step , the concentration matrix C is provided from the accumulation of concentration blocks, according to Eq. (67). A component map can be reconstructed from the concentration matrix C.","Alternatively, at step , if the analyst is working with the original data set D, the analyst proceeds to step .","At step , a data block Dof a total of j data blocks is read into memory.","At step , a least squares solution is obtained for a concentration block Cby projecting the data block Donto the estimated spectral shapes matrix S, according to Eq. (66) and subject to constraints at step .","At step , the concentration blocks Care accumulated.","At step , the steps - are repeated until all j data blocks have been read into memory and all j concentration blocks have been accumulated.","At step , the concentration matrix C is provided from the accumulation of concentration blocks, according to Eq. (67). A component map can be reconstructed from the concentration matrix C.",{"@attributes":{"id":"p-0197","num":"0196"},"figref":["FIGS. 13A-13D","FIG. 13A"]},"The EDX spectrum images were spatially compressed to the level four approximation in both spatial dimensions and analyzed at a 32\u00d732 resolution. The MCR-ALS analysis of the spatially compressed data was completed in about 3 minutes on a 1 GHZ computer having a single Gbyte of main memory. In  is shown the results of the MCR-ALS analysis of the compressed data. The analysis gave excellent, physically meaningful results consistent with the expert geological examination. Using both spatial and spectral compression, the complex geological sample was analyzed in less than two minutes.","The MCR-ALS analysis with spatial compression retains the fine spatial details of the original image. In  are shown the analysis of the silicate component in the geological sample.  shows the 32\u00d732 pixel compressed data image of the silicate component that was analyzed by MCR-ALS.  shows the reconstructed silicate image after projection of the original data onto the estimated spectral shapes matrix derived from the MCR-ALS analysis of the compressed data matrix. The reconstructed image of the silicate component map retains the full spatial resolution of the original data.","An additional benefit that can be obtained using wavelets is improved sensitivity to the pure spectral components. By co-adding pixels to provide the approximation coefficients, the signal-to-noise ratio can be improved. For a homogeneous region, the improvement will go as the square root of the number of pixels added. Therefore, high-resolution images collected at a low signal-to-noise (S\/N) ratio can achieve results equivalent to low spatial resolution, high S\/N images.",{"@attributes":{"id":"p-0201","num":"0200"},"figref":"FIG. 14"},"Furthermore, because the discrete wavelet transform is separable, the rows and columns can be treated separately. Therefore, the spatial filter can be matched to the spatial characteristics of the sample.  show partial results from MCR-ALS analysis of an EDX spectral image of an alumina\/braze interface.  shows two components, the alumina phase and an interspersed glass particulate phase, when analyzed at the original, uncompressed 128\u00d7128 resolution.  shows the two corresponding components obtained after spatially compressing the spectral image to level seven along the rows, but remaining uncompressed down the columns (i.e., a 128\u00d71 approximation image). Such an asymmetric filter improves sensitivity to features that look like horizontal lines. In the case of the braze interface, applying this asymmetric filter has enabled the detection of a real, differential self-absorption process occurring at the interface. At the same time, spectral contrast arising from the glass particulates has been reduced since the small globular shapes of the particulates are spatially mismatched to a horizontal line.","It will be understood that the above description is merely illustrative of the applications of the principles of the present invention, the scope of which is to be determined by the claims viewed in light of the specification. The invention has been described as an efficient spatial compression algorithm for the analysis of very large multivariate images. Other variants and modifications of the invention will be apparent to those of skill in the art."],"GOVINT":[{},{}],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The accompanying drawings, which are incorporated in and form part of the specification, illustrate the present invention and, together with the description, describe the invention. In the drawings, like elements are referred to by like numbers.",{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":["FIG. 9A","FIG. 9B"]},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":["FIGS. 13A-13D","FIG. 13A","FIG. 13B","FIG. 13C","FIG. 13D"]},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":["FIGS. 15A and 15B","FIG. 15A","FIG. 15B"]}]},"DETDESC":[{},{}]}
