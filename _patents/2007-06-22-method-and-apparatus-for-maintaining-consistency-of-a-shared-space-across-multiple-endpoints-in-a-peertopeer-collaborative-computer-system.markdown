---
title: Method and apparatus for maintaining consistency of a shared space across multiple endpoints in a peer-to-peer collaborative computer system
abstract: In a peer-to-peer collaboration system, deltas containing data change commands are organized in a persistent data structure called a delta log. The delta log is organized into blocks, which are the largest division in the delta log. In turn, blocks contain groups, groups contain chains and chains contain deltas. Delta blocks are used to implement priority deltas that are used to limit the collection of data change commands that must be transferred. Within a block the deltas are organized by groups, each of which is a set of deltas organized into chains. The delta group in used to determine which deltas to purge. The chains are ordered by increasing creator ID of the endpoint that created the chain. Organizing the delta log in this fashion allows the log to be “walked” to detect convergence problems. To achieve causality-preservation, each delta has a list of dependencies representing other deltas that must be executed before the current delta can be executed. The dynamics manager uses the ability to do (execute) and undo commands to perform roll back and roll forward operations on deltas in order to achieve convergence.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08073905&OS=08073905&RS=08073905
owner: Microsoft Corporation
number: 08073905
owner_city: Redmond
owner_country: US
publication_date: 20070622
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION"],"p":["This application claims the benefit as a divisional application of U.S. patent application Ser. No. 10\/279,785 filed on Oct. 24, 2002 and entitled \u201cMETHOD AND APPARATUS FOR MAINTAINING CONSISTENCY OF A SHARED SPACE ACROSS MULTIPLE ENDPOINTS IN A PEER-TO-PEER COLLABORATIVE COMPUTER SYSTEM,\u201d which is incorporated herein by reference in its entirety.","The invention relates to peer-to-peer collaborative computer systems that directly exchange command and data blocks in order to maintain consistency of a shared space among the collaborators.","Collaboration involves the ability for each member in a group of members, called \u201ccollaborators\u201d to automatically transmit information to, and receive information from, other collaborators in the group. In order to facilitate such collaboration, various systems have been developed that allow such information to be transmitted between personal computer systems, communication appliances or other communication devices, including handheld and wireless devices. Collectively, these devices will be referred to a \u201ccomputers\u201d in this description.","Computer-based collaboration may occur locally among users connected to, or operating with, one computer or server. Alternatively, collaboration may occur over a network, such as the Internet, wherein each of the users is located at a computer connected to the network. A server may also be connected to the network. Several collaboration models are currently being implemented as networked computer collaboration systems. One of these models is a client-server model in which all collaborators are connected, via the network, to a central server. Information generated by each collaborator is sent over the network to the server that then broadcasts the information back over the network to each other collaborator. Another model is a \u201cpeer-to-peer\u201d model in which direct connections are established over the network between each of the collaborator computers. Information generated by each collaborator is then sent directly to each other collaborator. In such a system, the collaborators communicate in a private \u201cvirtual\u201d shared space.","In both of these models, there are several methods by which information is transferred between collaborators. For example, in a client-server system, data that is being collaboratively modified may be stored on the server. Then, each collaborator that wants to modify the data sends a command to the server to effect a change in the server data. The server modifies its copy of the data and then sends information representing a \u201cview\u201d of the modified data to all collaborators, so that each collaborator can display the data locally.","A central data repository is not possible in a peer-to-peer collaboration system because no collaborator acts as a server. Thus, in such systems, each collaborator has a local copy of the data being collaboratively modified. In order to change the data, a collaborator generates a data change request that is forwarded to each other collaborator. The incoming data change requests are then used by each collaborator to modify its local data copy.","The latter type of collaboration system is described in detail in U.S. patent application Ser. No. 09\/357,007 entitled METHOD AND APPARATUS FOR ACTIVITY-BASED COLLABORATION BY A COMPUTER SYSTEM EQUIPPED WITH A COMMUNICATIONS MANAGER, filed Jul. 19, 1999 by Raymond E. Ozzie, Kenneth G. Moore, Robert H. Myhill and Brian M. Lambert; U.S. patent application Ser. No. 09\/356,930 entitled METHOD AND APPARATUS FOR ACTIVITY-BASED COLLABORATION BY A COMPUTER SYSTEM EQUIPPED WITH A DYNAMICS MANAGER, filed Jul. 19, 1999 by Raymond E. Ozzie and Jack E. Ozzie; U.S. patent application Ser. No. 09\/356,148 entitled METHOD AND APPARATUS FOR PRIORITIZING DATA CHANGE REQUESTS AND MAINTAINING DATA CONSISTENCY IN A DISTRIBUTED COMPUTER SYSTEM EQUIPPED FOR ACTIVITY-BASED COLLABORATION, filed Jul. 19, 1999 by Raymond E. Ozzie and Jack E. Ozzie and U.S. patent application Ser. No. 09\/588,195 entitled METHOD AND APPARATUS FOR EFFICIENT MANAGEMENT OF XML DOCUMENTS, filed Jun. 6, 2000 by Raymond E. Ozzie, Kenneth G. Moore, Ransom L. Richardson and Edward J. Fischer.","In this collaboration system, each collaborator has a program called an \u201cactivity\u201d, that is operable in his or her computer. The activity program contains a tool that responds to user interactions by generating data change commands. These data change commands are provided to a data-change engine that maintains the local data copy by performing the changes to the data requested by the data change commands. The commands are inserted into a container called a \u201cdelta\u201d and deltas are distributed from one collaborator to another by a program called a communications manager.","When a peer-to-peer collaboration system is used over a network, special considerations must be addressed. A major consideration is network latency. In particular, when a delta is transmitted over the network to a group of other collaborators, it may reach some collaborators sooner than others due to unequal transit times over the network. Since all collaborators send and receive deltas \u201casynchronously\u201d, the requests may be received by different collaborators in different orders. This can potentially create a problem because the correct execution of some commands may depend on other commands having been previously executed. In order to ensure that the local data copies remain consistent, the collaboration system must preserve causality. Specifically, causality demands that, when a current data change command received from a first collaborator is executed by a second collaborator, the second collaborator will have executed all previous data change commands that the first collaborator had executed when the current data change command was created.","Another condition that must be satisfied is convergence. Convergence ensures that when all collaborators have executed the same set of operations, the final execution order of data change commands by all collaborators is the same. In the collaboration system described in the aforementioned patent applications, a special program in each computer called a dynamics manager receives and interprets the deltas generated in that computer and received by that computer from other computers in order to preserve causality and to ensure convergence.","Another potential problem in a peer-to-peer system concerns collaborators entering and leaving the collaboration group during an on-going session by disconnecting their computers from the network or powering down the computers. Since the integrity of a collaborator's local data copy depends on receiving data change commands from other collaborators and correctly interpreting these commands, collaborators who leave a collaboration session will need either a complete current copy of the local data or a collection of data change commands that were transmitted by other collaborators during their absence in order to restart their system. In many cases, the copy of the local data and the collection of data change commands can be quite large resulting in a lengthy startup delay for a collaborator entering an ongoing collaboration session.","In accordance with the principles of the invention, deltas are organized in a persistent data structure called a delta log. The delta log is organized into blocks, which are the largest division in the delta log. In turn, blocks contain groups, groups contain chains and chains contain deltas. Delta blocks are used to implement priority deltas that are used to limit the collection of data change commands that must be transferred. Within a block the deltas are organized by groups, each of which is a set of deltas organized into chains. The delta group in used to determine which deltas to purge. The chains are ordered by increasing creator ID of the endpoint that created the chain. Organizing the delta log in this fashion keeps the deltas in a consistent order on all endpoints. This makes it possible to \u201cwalk\u201d the delta log to achieve convergence on all endpoints.","To achieve causality-preservation, each delta has a list of dependencies representing other deltas that must be executed before the current delta can be executed. The dynamics manager uses the ability to do (execute) and undo commands to perform roll back and roll forward operations on deltas in order to achieve convergence.","In order to prevent the delta log from growing too large, a purging technique in accordance with the principles of the invention uses endpoint pulses transmitted between the endpoints in a shared space. Each endpoint includes in an endpoint pulse information identifying a delta group that it is willing to purge (based on the highest group that each endpoint has acknowledged receiving). Then, all endpoints purge deltas in groups selected by comparing the groups that the endpoints have declared that they are willing to purge.","In accordance with another embodiment, special deltas called priority deltas are used to control the execution ordering of independent deltas. For example, a delta inviting an endpoint to a shared space can be a priority delta so that deltas independent with the invite delta do not cause a rollback necessary to achieve convergence and, therefore, it is not necessary to send the contents of the entire delta log to a new invitee.","In accordance with yet another embodiment, special deltas called asynchronous deltas are used to transmit large files without blocking the transmission of other deltas during the file transfer process. Asynchronous deltas are arranged so that they do not have other deltas that are dependent on them. Accordingly, endpoints do not need to wait until processing of an asynchronous delta is finished in order to transmit other deltas.","In accordance with yet another embodiment, persistent data is kept representing the state of all endpoints in the shared space. Deltas are only sent to active endpoints and the processing of inbound deltas depends on the state of the endpoint that sent the delta. These states support the implementation of membership changes, such as invite and uninvite, in a decentralized and secure fashion. In addition, it is possible to suspend an inactive endpoint in the shared space to allow portions of the delta log to be purged.",{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 1","b":["100","110","110","102","108","110","102","108"]},"Peer-to-peer communications can be made directly between peer units. For example, peer unit  may communicate directly with peer units ,  and , as indicated schematically by dotted links ,  and , respectively. In a similar manner, peer unit  can connect to units  and  via connections  and , respectively. Finally, units  and  can communicate over connection . A collaboration system such as that shown in  is available from Groove Networks, Inc., 100 Cummings Center, Suite 535Q, Beverly, Mass. 01915 and is described in detail in the Groove\u2122 Platform Development Kit which is available from Groove Networks, Inc. In the discussion below, the collaboration system will be assumed to be such a system. However, it will be apparent to those skilled in the art that other collaboration systems could also be used with the present invention.","In this collaboration system, a program called an \u201cactivity\u201d is resident in each collaborating computer system, communication appliance or other network-capable device. The activity allows a shared, focused task, such as, for example, a \u201cchat\u201d, gaming, or business application, to be performed in collaboration with other, remotely-located collaborators. This collaboration involves shared and mutual activities between individuals and small groups in private shared spaces. Each shared space is an instantiation of one or more activities operable on each of the collaborating computers of members of that shared space.","In the system, participants or members of a shared space access the system by opening \u201caccounts\u201d that are associated with \u201cendpoints.\u201d Since an individual collaborator may access the system via more than one device, an endpoint is defined as a unique combination of an individual and a device. Each endpoint stores an individual, local copy of the shared space data.","Each activity includes one or more tools, each of which interacts with a collaborator, for example, by receiving mouse and keyboard events, and initiates data change requests in response to the interactions. These data change requests are used locally and sent to other members of the shared space. Each activity also includes one or more data-change engines, separate from the tools, for maintaining the local copy of the shared space data pursuant to a common data model. The data model is, for example, activity-specific, and preferably the same over all members of the shared space. Each collaborating computer also includes a dynamics manager, that examines data change requests generated locally and received from other shared space members and coordinates the execution of the local and other data change requests and directs the data-change engine to make the requested changes to the local copy of data.",{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 2","b":["200","102","108","102","102","202","204","205","206"]},"The framework  can provide a platform for servicing a number of shared spaces, of which shared space  is shown. The framework  preferably is of modular construction, with an application programming interface (API) on which the activities run and through which they communicate with framework components. The framework  includes a user interface manager , an identity manager , a shared space manager , an activity manager , a storage manager , a dynamics manager , and a communications manager .","The user interface (UI) manager  is responsible for managing shared services for a number of user interface controllers (not separately shown). The UI manager  manages the graphical layout of activity screen displays within panes of a display window, and otherwise provides a desired \u201clook and feel\u201d for the user interface. The UI manager  also manages activity navigation (for example, go to, next, previous, etc.) and maintains a navigation history.","The identity manager  is responsible for maintaining an \u201cidentity\u201d for each shared space member. An identity is the name, and corresponding uniform resource locator (URL), by which each user is known by others. Individual users may have one or many identities. The identity manager  maintains a record or table, in the local storage of the identities. The identity manager  can also maintain a record or table of URLs for the shared space members and their corresponding device URLs. Alternatively, a separate member manager can be implemented.","The shared space manager  is responsible for managing each of the shared spaces  that may be opened on the peer unit . Each shared space  is an instantiation of one or more activities. Each shared space  has a corresponding activity manager .","Each activity manager  is responsible for (a) adding new activities to a shared space, (b) opening existing activities in a shared space, and (c) updating shared space activities. Each activity is defined by an activity \u201ctemplate\u201d that defines the initial activity configuration for a shared space and is a persistent representation of the tool and engine components comprising the activity. In order to create an activity template, a software developer may write a tool or adapt an existing tool and engine for use within the framework. For example, an activity template can be distributed as shrink-wrapped software or downloaded over the Internet to peer unit  from a remote server. Activity components can be regarded as Web documents and are represented persistently via URLs. The activity template itself preferably has a URL, which allows for tracking activity design changes. The activity template can be a single activity template or an activity collection template. A single activity template pertains to only one activity, such as \u201cchat\u201d. An activity collection template pertains to a collection of activities, such as \u201cchat and outline\u201d.","To add a new activity, the activity manager  is provided by the means described above with the URL of a template for the new activity. In order to open the new activity or an existing activity, the activity manager opens the template, extracts the template information (such as component URLs) and propagates the information into the shared space. A collaborator may add additional activities to the shared space  as needed. After being added, an activity is \u201cpart of\u201d the shared space and visible to all shared space members and each shared space member has an activity template for the shared space available on his or her peer unit.","Each shared space, such as shared space  has a tag to identify its corresponding activity manager  and to bind the activity manager with data associated with the activity. Preferably, the data is located in a document in the local memory and each document has a local registry linked to it with tag names maintained in the registry to express a mapping (reference pointers or associations) in an extensible, platform-independent way, between the document and its corresponding shared space.","Each activity, such as activity , includes a tool, such as tool  and an engine, such as engine . The tool , in conjunction with the user interface , allows an activity to interact with a collaborator. For example, the tool may receive user interface events, such as keyboard or mouse events, generated when the user interacts with the user interface . In response to such user interface events, the tool  may make data change requests to its corresponding engine . Tool  also implements APIs for interacting with background services.","The engine  is responsible for maintaining and changing the data that supports the shared space  and\/or results from user interaction obtained through the tool . It responds to data change requests from tool  by returning to the tool  commands necessary to implement the data change requests. Under the direction and control of the dynamics manager , the engine  can make changes to the shared space data copy that is stored locally under control of the storage manager . When these changes are made, the engine  asynchronously generates data change notifications. The tool  can subscribe to the engine  to receive these data change notifications so that the tool  can update the user interface asynchronously when the data changes occur.","The dynamics manager  receives local data change commands from the tool  and receives data change commands from other collaborating computers, via communication manager  from a network connection . These commands may be encrypted for transmission between collaborators and the dynamics manager  can use security manager  to decrypt the commands. Dynamics manager  makes decisions on which commands to implement in order to maintain synchronization among all collaborators and forwards these commands to engine  in order to cause engine  to make changes to the local data copy.","During operation the collaborative system  obtains a member's identity from the identity manager  and opens a shared space manager . The system  then requests that the shared space manager  open a shared space identified via a URL and create an activity manager . Once created, the activity manager  opens an activity, typically by using the activity's URL to identify the activity. Then, the collaboration system  is ready for members to use the shared space to perform the shared, focused tasks offered by the particular activity.","As previously mentioned, data change requests generated by the engine  are placed into a container called a \u201cdelta\u201d that is used to send to the data change requests to other collaborators. In the illustrative collaboration system, a delta is an atomic unit of change in a shared space and is the only way to make shared space changes that affect multiple endpoints. The dynamics manager  in each endpoint is responsible for making sure that the data change commands in each delta are correctly executed so that the ordering of the data change commands is consistent on multiple endpoints in a shared space. In order to do this, the dynamics manager  works with various engines  to execute and \u201cundo\u201d data change commands.","The dynamics manager provides a number of frameworks that handle various services necessary to ensure the aforementioned causality preservation and convergence. These frameworks include a Command Execution framework, an Advise framework, a Distribution framework and a Delta Log management framework.","To achieve causality-preservation each delta has a list of dependencies representing other deltas that must be executed before the current delta can be executed. The dynamics manager uses the ability to do (execute) and undo commands to perform roll back and roll forward operations on deltas in order to achieve convergence. If two endpoints A and B independently generate and execute deltas DA and DB, respectively, and send the deltas to the other endpoint, then one endpoint will have to \u201crollback\u201d its delta execution in order for both endpoints to converge. For example, on endpoint A, delta DA will be executed before delta DB is received. On endpoint B, delta DB will be executed before delta DA is received. In order to ensure convergence, one endpoint will need to rollback. If the correct execution order determined by the dynamics manager is Do(DA) Do(DB), then endpoint B will need to perform a rollback. The order of operations on B will be: Do(DB) Undo(DB) Do(DA) Do(DB). Thus, the final order of execution will be the same on both endpoints.","As part of the command execution framework, the dynamics manager  also provides handling for exceptions that occur when commands are executed. The dynamics manager  also provides APIs to allow tools to execute user level undo and redo operations. The dynamics manager  uses the same undo and redo implementation used by rollback and rollforward operations to support the user undo and redo operations.","The dynamics manager also places restrictions on operation that can be performed by engines as part of do and undo methods. In particular, the engines can not call any higher level components (such as views) during these methods. In order to help engines implement view notifications, the dynamics manager  provides an asynchronous notification service in connection with \u201cadvise\u201d elements. These elements are provided to the dynamics manager  and the dynamics manager  then asynchronously notifies any entity that is registered to receive those notifications. The notifications are also used for the notification of events other than the execution of a command.","The dynamics manager  is further responsible for guaranteeing that deltas are sent and received by all active endpoints in the shared space. In order to do this, the dynamics manager  maintains contact information for all endpoints in the space. Deltas that are executed locally are also disseminated to all members of the space to be executed on other endpoints. Typically, the communications network between endpoints is not fully reliable and, thus, the dynamics manager  is responsible for re-transmitting deltas that may have been lost during transmission. The dynamics manager  uses the dependencies on deltas to detect when deltas are missing. The dynamics manager  also periodically sends pulses when there have been changes in the shared space. Like deltas, the pulses contain dependencies that can be used to detect missing deltas.","Once the dynamics manager  detects that a delta is missing, it normally attempts to fetch the missing delta from one or more other endpoints in the shared space. The deltas are stored in a data structure called a delta log in each endpoint so that other endpoints can normally satisfy the request and resend the missing delta. The dynamics manager  also exposes an interface to allow others to disseminate information to the endpoints in a shared space.","The dynamics manager  maintains the aforementioned delta log that contains all of the deltas that have been executed in the space. The log is kept for three reasons: the deltas may need to be rolled back to ensure convergence, the deltas may need to be fetched by another endpoint and the deltas may be undone or redone by the user. The dynamics manager  detects when a delta no longer needs to be kept for any of these reasons. At that time, the dynamics manager  may remove the delta from the delta log in a process called purging. In order for the dynamics manager  to detect that it no longer needs a delta, it must receive pulses from all of the other endpoints. If an endpoint doesn't send a pulse for predetermined time period, the dynamics manager  may decide that the endpoint is a \u201claggard.\u201d A laggard endpoint is defined as an endpoint that appears to have lost connection with the shared space. Generally, it is desirable to temporarily remove such an endpoint from the space so that its deltas can be purged from the delta log in order to reduce the size of the delta log. If other endpoints also decide that an endpoint is a laggard, then the laggard endpoint is disconnected from the shared space. If the laggard endpoint becomes active in the shared space again, it will need to receive a new copy of the space from one of the active endpoints.","There are several steps involved in the process of creating and executing data change commands in deltas, and many components of the collaborative system are involved in this process. , when placed together, form a flowchart that illustrates the major steps involved in creating a delta and inserting the delta into a shared space. This flowchart is described in connection with  that illustrates the major components involved in the process, including components at the source endpoint from which the delta is generated and a destination endpoint to which the delta is sent.","The process starts in step  and proceeds to step  where a dynamics manager, such as manager , in a source endpoint  (the endpoint that creates the delta) creates a delta at the request of a tool . The dynamics manager  represents the delta using XML code. A newly created delta is an empty container for data change commands. The structure of a typical delta varies depending of the type of delta, the number of commands contained in the delta and whether \u201cundo\u201d information allowing the commands to be \u201cundone\u201d is present.  shows the basic XML code structure of a delta  with one command and no undo information and  shows the basic XML code structure of a delta  with two commands and undo information for both commands. In accordance with typical XML structure, this XML code consists of a plurality of hierarchical elements that are nested with each other. Each element may have attributes associated with it and specific content. All the XML code is in a namespace where \u201cg\u201d stands for the URL \u201curn:groove.net\u201d.","The LDel element ,  is a local delta element and has attributes that are relevant to the local endpoint only. For that reason this delta element is not sent to other endpoints when the deltas is sent to other endpoints. It has several attributes. Attributes are only included in an element if they are relevant so not all attributes are shown in . The LDel element ,  attributes include an AssPrevGenDel attribute that is set when the delta is associated with a previous delta for undo purposes (that is the deltas will be undone together). The BlkDel attribute this is set when this delta has the highest priority in the current block. The DeltaToRedoIP attribute is set on a redo delta. It is set to the insertion point of the delta that is being redone. The DoError attribute is set when there was an error the last time the delta was executed.","The Done attribute is set when the delta has been executed on the endpoint. It doesn't exist if the delta has not been executed. The Redone attribute is set when the delta has been undone and then redone. The ReKeys attribute contains key information for the delta that is used in encrypting the delta to send to other endpoints. It only exists on the endpoint that is the creator of the delta. The SenderUID attribute is set to the unique ID of the sender endpoint on deltas that were received as the result of a fetch request by the dynamics manager .","The Ses attribute is the delta session used in undo and redo operations. It only exists on the endpoint that generated the delta. The Undo attribute is set when the delta is an undo delta. The UndoError attribute is set when there was an error the last time an attempt was made to undo the delta. The Undone attribute is set when a delta has been undone. The URL attribute stores a URL if one was specified for the delta. This URL is used as the message URL and allows the dynamics manager to support delta progress notifications.","The content of the LDel element ,  include the Del element ,  and the SEC element , . The Del Element ,  is the delta element that is sent to other endpoints. Note that only the command element (or commands element) is disseminated. Any command undo information is not disseminated. The Sec element ,  is the security element where the security manager  can store delta-specific security information.","The attributes of the Del element ,  include an AddEpt attribute that is set when this delta adds an endpoint to the shared space. An AddNewEpt attribute is set when this delta adds a new endpoint to the shared space. This attribute is only used when an account space is imported on a new device. A CR attribute holds the conflict region of the delta that is set when the delta is created. Deltas that have different conflict regions do not require a consistent order of execution. Deltas with no conflict region conflict with all other deltas. The CR attribute contains a string and an empty string indicates that the delta will conflict with all other deltas. If two different regions are set on the same delta, then the conflict string is set to empty so that the delta will conflict with all other deltas. Typically, each tool has a unique conflict region that it sets on a delta. However, system tools, such as the security manager and member manager do not set a conflict region so that their deltas conflict with all other deltas. This is important because any tool may access the data in the system tools.","A DepSeq attribute is the set of dependency sequences of the delta. This may not exist if the only dependencies are implicit. Dependencies are described in more detail below.","A Gp attribute is the group number of the delta. A PurNot attribute is set when an engine that created one of the commands in the delta needs to be notified when the delta is purged. A Seq attribute holds a sequence number of the delta as described below. A Version attribute holds the version of the dynamics manager  that created the delta.","The content of the Del element ,  includes a command element , if there is only one command, or a commands element  if there is more than one command. The command element  is followed by a content section that includes the command content and the commands element  is followed by a content section that includes command elements ,  for each command in the delta. If the delta has only one command, the command element  will be a direct child of the delta element. If the delta has multiple commands, they will be contained in the commands element , which is a direct child of the delta. Cmd elements ,  and  each represent a command in the delta. The content and most of the attributes of these elements are set by the engine that created the command. The attributes include an EngineURL attribute that is set to the URL of the engine that created the command. A PurNot attribute is set when the engine that created the command requires notification when the command is purged.","If there is no undo information for the command an empty comment \u201c<!--->\u201d  is used as a placeholder. If there is undo information, a CmdU element ,  holds command undo information for one of the commands in the delta. The content and attributes on this element are set by the engine that executed the command.","When a delta is created, it is a unattached, or unparented, XML code fragment within a larger, persistent, XML document  that represents deltas in the dynamics manager . A typical XML representation of the document  is shown in . The XML document  includes an EngineData element  that is the root element for the dynamics manager document. The contents of this element include a number of other elements. These include a BlockedDels element  that is the parent for any deltas that are blocked. This element is used as a parent for any messages that are blocked. It contains an EptPul element that holds a pulse element from a different element and a LocalDel element that represents a delta including information specific to the local endpoint. A CmdU element  serves as a read-only empty command undo element. It is used as the undo element for all commands that do not have explicit undo information. The attributes and content of the CmdU element are specific to the engine that executes the command.","At times the dynamics manager  may create CmdU elements that are later determined to be superfluous. These CmdU elements are stored in CmdUParent elements for future use. For example, CmdUParent elements  and  are used as parents for extra CmdU elements. A DelHldr element  is used to represent a data structure called a \u201cdelta holder\u201d . This structure is described in more detail below. Similarly, the DelLog element  represents a data structure called a \u201cdelta log\u201d . It is also covered in detail below.","A DisconnectNotifications element  is used as a temporary parent for incoming disconnect notifications. A Disseminator element  is used to represent elements that are waiting to be sent to other endpoints or \u201cdisseminated.\u201d This structure is also described in detail below. An Epts element  is used to represent endpoints in the shared space. It is described in more detail below. An InboundDeltas element  is used as a temporary queue for incoming messages to the dynamics manager  and is described below. It contains a Del element representing the part of a delta common to all endpoints, a DisNtfy element that holds a disconnect notification from a different endpoint, a EptPul element that holds a pulse element from a different element and a LocalDel element that represents a delta including information specific to the local endpoint.","The attributes of the EngineData element  include a Blocked attribute that holds insertion point of a delta that caused the dynamics manager  to enter a blocked state. A BlockedCount attribute holds the count of the number of times dynamics manager  has been blocked. A Divergent attribute is set when the dynamics manager  for a shared space is divergent from other members of the shared space. The attribute is set to a string representing the reason for the divergence. A LastEptPulse attribute holds the time that the local endpoint last sent a pulse message. A PurgingDisabled attribute is set when purging is disabled. A UnackedDel attribute is set when there exists a delta that hasn't been acknowledged by a pulse. As will be discussed in detail below, these last three attributes are used when the delta log mentioned above is periodically purged. A PingSeq attribute holds a sequence number indicating the last time a relay server was contacted. An Uninvited attribute is set when the local endpoint has been uninvited from the space. A Version attribute holds the version of the dynamics manager data model. Finally, a Watermark attribute is used to keep the local cached state consistent with persistent state by detecting when transactions are aborted.","This persistent XML document  is maintained by a storage manager . A storage manager  that is suitable for use with the present invention is described in detail in the aforementioned U.S. patent application Ser. No. 09\/588,195. In addition to creating the persistent XML representation , the dynamics manager  creates a COM object representing the delta. The latter object implements a number of interfaces that the tool  can use to add commands to the delta and to set properties of the delta.","Creating a delta also automatically creates a transaction in a transaction database that is associated with the shared space (not shown). Transactions provide an exclusive method of accessing the shared space database and make it possible to completely undo any changes that were started as a result of a particular delta. A transaction will be aborted if the delta that created it is aborted or if any errors occur during delta execution. The transaction is committed if the delta is committed and successfully executed.","Next, in step , the tool  adds one or more data change commands to the delta container, either directly or through the use of \u201ccreation nested\u201d deltas, as described below. A command is a unit of change created and executed by an engine, such as engine , and a delta is a grouping of commands with the intention that all those commands are always executed together. Therefore, a delta is \u201catomic\u201d in the sense that all of the commands in the delta are executed or none are executed. There is one exception to this rule that occurs during the handling of exceptions during command execution and is discussed below.","The interfaces on the COM object that is created with, and represents, the delta allow commands to be added to the delta. A command is added by using the dynamics manager  to create an empty command XML element (a Cmd element) within the aforementioned XML fragment that represents the delta within the XML delta document. At the time that the delta is created, the dynamics manager  must be provided with a bindable URL of an engine, such as engine  that will later execute the command. Although only one engine  is shown in , it will be understood by those skilled in the art that a plurality of engines may be present. In the description that follows, engine  will be used to refer to all of these engines. This URL is a unique name of that engine so that the dynamics manager can find the engine later when it is needed. The aforementioned EngineURL attribute is set on the applicable XML command element. The creator of the delta can then add any attributes and content that will be required to execute the command. For example, an engine that implements a sketchpad will have commands to draw objects. If the user draws an object on the sketchpad, a command will be created with attributes that describe the shape, color and location of the object. When this command is executed, both on the source endpoint that created the delta and on remote destination endpoints, the changes in the command will be applied to the data model  associated with the engine .","An alternative way to add commands to a delta is by \u201ccreation nested\u201d deltas. Creation nested deltas provide an easy way to combine components to build more complex tools. For example, a tool could use a number of other components that could be other tools. When the user takes an action in the tool, that tool creates a delta container. The tool then calls the other components to add commands to the delta container. Instead of directly adding commands to the delta, a tool can create another delta and add this new delta to the first delta. Because the creation of this delta is nested within the creation of another delta, it is called a creation nested delta. For example, a set of steps during the creation of a delta could be:","Delta 1 Created","Command A added","Delta 2 Created (this is creation nested)","Command B Created","Delta 2 Committed","Command C added","Delta 1 Committed","The XML structure of a delta  that contains two creation nested deltas, both of which have undo information is shown in , when placed together. A creation nested delta has the same elements as a standard delta with some changes to the command element. For example, a Nested attribute in the Cmd element is set if the command was created as part of a creation nested delta. The Nested attribute is set to the ID of the creation nested delta. In addition, a NOrd attribute is set on the Cmd element to the ordinal of the command within the nested delta.","When a creation nested delta is committed as described below, any commands in the delta are executed, just as they would be for a non-nested delta. However, instead of storing the delta in a delta log  after the commands are executed, the dynamics manager  instead automatically adds the commands and undo information to the beginning of the containing delta. If the containing delta is aborted, the creation nested delta commands are also aborted. If the containing delta is committed, the creation nested commands have already been executed and are not executed again. Note that commands in the final containing delta are in the order that they are executed. Because they are not executed until the delta that contains them is executed, the execution order of the commands in Delta 1 above is B,A,C.","When a tool, such as tool , has completed adding commands to the delta, the tool  can either commit or abort the delta in step . If the delta is aborted, none of the commands is executed and the transaction that was started when the delta was created is also aborted. Because nothing further happens in that case, it will be assumed that the delta is committed in step . When a delta is committed, the remaining steps actually execute the commands in the local endpoint and disseminate the delta and its commands to the destination endpoints. The transaction that was started when the delta was created is not committed until after the delta has been executed and queued for dissemination.","Next, in step , the dynamics manager  determines where the newly committed delta will be inserted into a delta log . The delta log is an XML data structure that is actually part of the XML document  in the dynamics manager . The structure has a typical XML code structure such as that shown in , when placed together.","The delta log XML structure is comprised of several elements including a DelLog element  that represents the delta log. The DelLog element  has several attributes including a NeedToDelete attribute that is set when there are groups that have been purged but not deleted yet. A PenGrpNum attribute holds the number of the highest group that is pending for purge, that is that has been declared as eligible for purging. A PurGrpNum attribute holds the number of the highest group that has been purged.","The DelLog element  also includes other elements as its contents. These include one or more DelBlk elements  and , each of which represents a block of deltas. Delta blocks are used to implement priority deltas as described below. Within a block, the deltas are organized by group. Each block will have a content element for each group from the first group in the block to the highest group in the block, event if the group contains no deltas within the block.","Each DelBlk element, for example element , contains one or more DelGrp elements ,  and , each of which represents a group of deltas. A delta group is a set of deltas organized into chains. There is one XML element for each chain within a group. The chains are ordered by increasing creator ID of the endpoint that created the chain. A DelGrp element  can have two attributes: a DoneTime attribute that holds the time at which the next group was created and a Number attribute that holds the number of the group.","Each DelGrp element, for example, element , contains one or more DelChn elements  representing a chain of deltas. Each DelChn element  has a CreatorID attribute that holds the ID of the creator of the chain. Each DelChn element  contains one or more LDelChn elements , each of which represents a delta.","Every position in the delta log  has a unique address, called an \u201cinsertion point.\u201d A new delta is always inserted at the end of the delta log  and, consequently, in order to insert a new delta, the insertion point corresponding to the end of the delta log  must be determined. In one embodiment, each insertion point consists of the combination of a group number, a creator ID, and a sequence number. In turn, the creator ID consists of a combination of an endpoint ID and a random number creator ID. The endpoint ID is a combination (for example, a hash) of the device URL and the contact URL of the endpoint and is a unique identifier of the endpoint. Every time an endpoint opens a shared space, the endpoint is assigned a new random number creator ID. The creator ID is intended to prevent collisions from occurring during the insertion of a new delta into the delta log with deltas created by the same endpoint. These collisions might occur in cases of a system crash, or a re-invitation to a shared space, which causes the endpoint to lose its internal record of previous deltas it had created.","Once the creator ID has been determined, the group number can be determined. In the particular embodiment under discussion and as set forth above, the delta log  collects deltas into chains and chains into groups. A chain consists of a deltas created by the same creator up to a predetermined maximum number of deltas. In order to determine the group number, the dynamics manager  first determines whether the newly committed delta can be added to the last chain being formed, that is, whether the chain contains deltas from the creator and there are less than the maximum number of deltas already on the chain. If the delta can be added to this last chain, the group number is the number of the group that contains the last chain. If the delta cannot be added, a new chain must be created.","Next, a determination must be made whether the new chain can be created in the current group, or if a new group must be created. In accordance with the embodiment, within each group, the chains must be in increasing order by creator ID. Because the insertion point for the new delta must be at the end of the delta log , the creator ID for the new delta must be compared with the creator ID of the last chain in the delta log. If it is greater, then the new chain can be added to the last group and the number of that group is the group number for the delta.","Otherwise, a new group needs to be created. The new group, and the new delta, will have a number one greater than the previous group. Finally, the sequence number is determined. The first delta created by a creator has a sequence number of \u201c1\u201d. Each subsequent delta created will have a sequence number of one greater than the previous delta.","Taken together, the creation ID and the sequence number is a number called a \u201cdelta sequence.\u201d The group number is concatenated with the delta sequence to produce the insertion point. To make indexing easier, the group number and sequence are stored as separate attributes on the delta.","In step , the insertion point is \u201cstamped\u201d on the delta by including the calculated value in the delta attributes, but the delta is not actually inserted into the delta log  at this time. The insertion point must be included in the delta because certain special commands, such as adding a member to the shared space, use the delta insertion point. It would be theoretically possible to actually put the delta in the correct place in the delta log  at this point, but that would make the \u201cundo delta\u201d command much more difficult to implement.","After the insertion point is stamped, in step , the dynamics manager  executes the commands in the delta (this process is called \u201cexecuting the delta\u201d.) In order to perform this step, the dynamics manager  iterates over all of the command elements and opens the attribute containing the bindable URL of the engine that is to execute the command. The dynamics manager  then binds to that engine\u2014this process takes the URL and finds the engine that is associated with that URL. Once the dynamics manager  has located the engine (for example, engine ), it calls the engine  to execute the command. In addition to providing the engine  with the command element describing the command, the dynamics manager  also provides an empty command undo element to the engine . The engine  can use this element to store any information that may be needed subsequently to undo, or reverse the execution, of the command. If the engine  adds information to the command undo element, the element is stored in the delta. For performance reasons, if the engine  doesn't store any information in the undo element, then the dynamics manager  uses an empty comment as a place holder for the undo element in the delta.","The steps performed in executing a command depend on the engine executing the command and the command being executed. An engine  will execute the command and apply any changes to its persistent data model . Engine  may also create \u201cadvises\u201d that asynchronously notify it that the change has been made. Often it will add undo information to the command undo element, which undo information may be necessary if the command must be undone in the future. Each engine  can also perform an access control check to make sure that the creator of the delta has the permission to perform this command. This check is done on each endpoint before the command is executed. If the any endpoint does not have permission, then the engine  can throw an exception that is handled as described below.","In addition, the engine  can create execution nested deltas. An execution nested delta is created during the execution of commands in another delta. Execution nested deltas are similar to creation nested deltas, except that they are created while the commands in the containing delta is being executed instead of when the containing delta is being created. When creation nested deltas are executed, as discussed above, commands and undo information are added to the containing delta. However, when execution nested deltas are executed, the commands they contain and any undo information for those contained commands are stored in the undo information of the containing delta.","Creation nested deltas are only created on the endpoint  that creates the delta and the commands in them are then automatically executed on all endpoints that execute the containing delta. In contrast, execution nested deltas are typically created on all endpoints that execute the containing delta. The commands in them are not disseminated, so they may need to be generated on each endpoint that executes the containing delta. Executing the delta may have different effects on different endpoints, for example because of differences in the role of the member executing the delta. In these cases, the undo information may be different and execution nested deltas may not need to be created or may be different. Access control checks are not done in execution nested deltas since the check by the containing delta suffices.","When a delta is undone, the dynamics manager  will undo any execution nested deltas before it undoes the command that caused the execution nested deltas to be created.","Exceptions that occur during command execution are handled by the dynamics manager . Exception handling in the case when the delta is being executed on the endpoint  that created it is straightforward. The exception is thrown to the tool  that is trying to commit the delta. Care must be taken that any changes already made as part of this delta are undone. In order to make sure that these changes are undone, the dynamics manager  creates a \u201ctransaction\u201d around the execution of each command. If an exception occurs during the execution of the command, then the transaction around that command is aborted. This latter process will undo any changes that were made as a result of the partial execution of the command.","Thereafter, the dynamics manager  calls other appropriate engines to undo any other commands that had already been executed as part of the delta. These calls allow the engines to undo any information that is not part of the shared space and, consequently, won't be automatically undone when the dynamics manager  aborts the transaction containing the entire delta. In addition to the transaction around the execution of each command, the dynamics manager  executes each delta within a transaction. After the engines have been called to undo any successfully executed commands, the dynamics manager  aborts this latter transaction as well. This transaction ensures that any changes in the shared space that were made as part of the delta and are not correctly undone will be automatically undone. Once the dynamics manager  has ensured that the changes made as part of the delta that had the exception are undone, it throws the original error back to the tool  that was attempting to commit the delta.","After execution, in step , the dynamics manager  adds the new delta into the delta log . This step is straightforward since the dynamics manager  has already determined the correct insertion point for the delta in step . The process then proceeds, via off-page connectors  and , to step .","As previously described, an important property of a collaboration system is \u201ccausality\u201d which demands that, when a current data change command received from a first collaborator is executed by a second collaborator, the second collaborator will have executed all previous data change commands that the first collaborator had executed when the current data change command was created. One of the major properties provided by the dynamics manager  is \u201cprior execution.\u201d In particular, the dynamics manager  ensures that an endpoint will not execute a delta unless it has already executed all deltas that had been executed by the creator of the delta when the delta was created. In order ensure prior execution, each delta has dependencies stored in it in step .","Delta dependencies describe all previous deltas that have been executed on the endpoint that created the delta. For example, consider the following transaction that occurs in five steps:\n\n","In this situation, for each delta, such as delta A2, 2 is the sequence number and A is intended as a shorthand notation for the creation ID as described above. Thus, the notation A2 represents the delta sequence. \u201cSimultaneous\u201d creation means that endpoint A creates delta A3 before receiving delta B2 and endpoint B creates delta B2 before receiving delta A3. The resulting deltas are said to be \u201cindependent.\u201d","One possible way to indicate the dependencies would be to stamp each delta with the delta sequences of all deltas that had been previously executed. If this is done in the example given above, the dependency set on each delta would be as indicated in brackets in the following delta list:",{"@attributes":{"id":"p-0115","num":"0119"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"119pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"A1","{ }"]},{"entry":[{},"A2","{A1}"]},{"entry":[{},"B1","{A1, A2}"]},{"entry":[{},"A3","{A1, A2, B1}"]},{"entry":[{},"B2","{A1, A2, B1}"]},{"entry":[{},"C1","{A1, A2, B1, A3, B2}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}}},"As more and more deltas are created, the set of dependencies will continue to grow. In order to reduce the number of dependencies, implied dependencies are eliminated. For example, delta B1 depends on delta A2 and delta A2 depends on delta A1. Therefore, the dependency of delta B1 on delta A1 is already implicit in the A2 dependency of delta B1. The elimination of these implicit dependencies results in the following explicit dependencies on the deltas:",{"@attributes":{"id":"p-0117","num":"0121"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"91pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"A1","{ }"]},{"entry":[{},"A2","{A1}"]},{"entry":[{},"B1","{A2}"]},{"entry":[{},"A3","{B1}"]},{"entry":[{},"B2","{B1}"]},{"entry":[{},"C1","{A3, B2}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}}},"As a further optimization, an implicit dependency of a delta on a previous delta from the same creator can be assumed. For example, it can be assumed that delta A2 implicitly depends on delta A1. With this further optimization, the resulting dependencies for these deltas is:",{"@attributes":{"id":"p-0119","num":"0123"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"91pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"A1","{ }"]},{"entry":[{},"A2","{ }"]},{"entry":[{},"B1","{A2}"]},{"entry":[{},"A3","{B1}"]},{"entry":[{},"B2","{ }"]},{"entry":[{},"C1","{A3, B2}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}}},"Note that, it may be necessary for a delta to have multiple dependencies, such as the case of independent deltas. For example, in the latter dependency list, delta C1 depends on both deltas A3 and B2, but because there is no relationship between those two deltas, delta C1 must explicitly include the dependency on both of them.","Maintaining a list of dependencies for the next delta created by an endpoint is straightforward. The dynamics manager  keeps a set of new dependencies. Each time a new delta is committed, or assimilated, this set is updated by removing any dependencies of the new delta and then adding the new delta. For example, if another endpoint, D, exists in the example given above, its new dependency set would appear as follows after receiving each of the deltas generated by the other endpoints A, B and C above:",{"@attributes":{"id":"p-0122","num":"0126"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"91pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"A1","{A1}"]},{"entry":[{},"A2","{A2}"]},{"entry":[{},"B1","{B1}"]},{"entry":[{},"A3","{A3}"]},{"entry":[{},"B2","{A3, B2}"]},{"entry":[{},"C1","{C1}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}}},"If endpoint D created a new delta after receiving delta C1, then delta C1 would need to be the only explicit dependency of the new delta. At the time when dependencies are added to the new delta, a check is made for any dependency on a previous delta from the same creator, because that dependency is always implied.","Before each delta is sent to other collaborators, it must be \u201csecured.\u201d This involves encrypting and integrity protecting the delta. Because these operations can be a relatively complex and time-consuming, the dynamics manager  performs these operations asynchronously (not on the main line of execution) when there are no transactions on the shared space database. This prevents encryption from interfering with other operations in the shared space. In order to secure a delta asynchronously, it is necessary to store some security information in the delta at this time. This security information will be used later by the security manager  to secure the delta. In step , the dynamics manager  engages the security manager  to store any information that will later be needed to encrypt the delta.","The asynchronous encryption and forwarding of the delta to other collaborators (called \u201cdissemination\u201d) is actually implemented by a disseminator component  in the dynamics manager . This component  includes a disseminator queue element  in the XML document  where deltas and other messages that are to be disseminated are enqueued. An element with an attribute set to the sequence number of the new delta is enqueued to the disseminator element  in step . The enqueueing generates an event that will result in the dynamics manager  being called asynchronously on a different thread of execution to actually perform the dissemination. When the dynamics manager  is later is called back asynchronously, it, in turn, calls the disseminator component  to process any deltas in the queue . The disseminator  will dequeue the element, read the sequence number attribute, open the corresponding delta from the delta log and process it. After the delta is enqueued for asynchronous processing, the transaction on the shared space database that was started when the delta was created is committed.","Before being sent to other collaborators, each delta is secured by encryption and integrity protection. Encryption prevents anyone who intercepts the delta message from obtaining useful information. Integrity protection allows the dynamics manager to detect if another party modifies the delta. When processing a delta, the disseminator component  of the dynamics manager , in step , uses the security manager  to secure the delta with the previously stored information. The delta is secured in a two-phase process. The first phase is initiated with the shared space database locked. During this phase the relevant security information, such as keys, are read from the shared space database. During the second phase, which happens without the shared space database locked, the delta is actually secured.","Only part of the delta will be sent to other collaborators, and only that part is secured. Any information in the local delta element is not disseminated or secured. Also any command undo information that is relevant only to the local endpoint  and is not disseminated or secured. Securing a delta leaves the delta element itself, which contains the group number, the sequence number and the dependencies, unencrypted (but integrity protected). This information may be needed before the delta can be decrypted, so it must be sent unencrypted. The command elements are encrypted and sent as content of the security process. A local delta element also has a content element that the security manager  can use to store any local data that must stored for the delta. A secured delta looks like the following:",{"@attributes":{"id":"p-0128","num":"0132"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"273pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"<urn:groove.net:Del CR=\u201cYwyi5Fkw\u201d DepSeq=\u201c6C58D518236030D3BEA50001\u201d Gp"},{"entry":"=\u201c352\u201d Seq=\u201c1E3F58465D4741E945370003\u201d Version=\u201c1,0,0,0\u201d>"},{"entry":"\u2003<urn:groove.net:SE Version=\u201c3,0,0,0\u201d>"},{"entry":"\u2003\u2003<urn:groove.net:EC EC"},{"entry":"=\u201cmk1BJa1etosZfG0tnsUPsJN7Nud4Zlz86op+M0jcj8OjwQEVbTXrJBR8PnakL7icF0S4"},{"entry":"7qLMRwUj7yloV2C5lAjXGvP\/zM1XF2pUPYHJSh+PryjJxmQ9w+EltEn0ll1vyP9RzN7Ne"},{"entry":"N2v\/l\/mrhzeSv6Y9awd+qTWhf1luZ4TLV6mV9S6i1GVI9VnKoD0M2wpP+CsK9GEk0tK"},{"entry":"guhK4mTYspQgAvBGOdJoqf5J1qvGUWmly1C9eWjrCMP7+V9k2SqXgOzTffGM1EXsy"},{"entry":"tVI4kKin5eazhNI49JDPGuB5cgFRXzAi9s1dhlZWX1q4VA24UHfTpOL6cet7JhqMN04gB"},{"entry":"Dvu6tbUTIpcS9BtsMKHtv3EZVIaq80C\/tiAel5X+NKGSJaZher7rMJjPdxNjx9M4jur5jUrUK"},{"entry":"6xMRnnxcHPD1vJ1pVjP5UR2flC1hFeKPaZ13QyJcB10lSaGg1yvTDrLqw6jMzF6nSL7r"},{"entry":"LtLMz+iMvxmmQCk9aLB7UuudZ0ygp4eOjnOep+C0EipOyWGUCb8kNSKhn4PcGCSjd"},{"entry":"Pc2jJVXa9nw4BAkCeVkpESMUaXpGhcYtSBbATxCNDzrYYm0HVUAXhknQAAHwP0C"},{"entry":"2zjq5Tze71V4vpz+cbANOYs2zCcfndohCV+tTClbwPc+kzgbbWqmciJsd+QulwmdV4op"},{"entry":"ZD\/1STQaM9iC9hRRpvo4dg69M3xX1CUNDkOwGD0tcfU+k00NgU5ja7+qUp2UXa15K"},{"entry":"k0pJxEfsysd2QY2bak8aaEFR9AVcWIEwl2UBcr+y8aRJTJqMY9jmvz2ZIEdSYU7qmVp"},{"entry":"bT7ncNcxQ0oIXbD9s++PVC+nNSEQXH4WSABLDDpKcFgpqpQLWk1Ra+lKpfPtGmfm"},{"entry":"YnqveKM\/d1x\/oKwfMa4zxo9qhvJQd55KrTjl7knH08ZalKiNbanfVilylPn9HJxF6GSyn+cQ"},{"entry":"TgstmcN0hKHyAmAraOe54ydGChH9W3rGT8RVtJxbz+BhpTMIUCqP4ehFqtjsESGUV4"},{"entry":"iLFHNg4UpKXd4H8b61w4ZpHakVPgtaqRhrpVgzgeTu0QhRQ9Ug3HE6koNwQKcW8d"},{"entry":"hVCZbCorVW6mR3PhzGO3R358+zIXTwNfXxlnRhwA5L6kHnZJywjEjeZb2hGDUtyWV"},{"entry":"hc\u201d"},{"entry":"IV=\u201c0dERdZ3iJCWDlo1WeZbmEI+Lxv+PojBF\u201d KID=\u201c368BC06A9DB273B133A80002\u201d"},{"entry":"KV"},{"entry":"=\u201c2\u201d\/>"},{"entry":"\u2003\u2003<urn:groove.net:Auth PTSig"},{"entry":"=\u201ci1alU0dLC7OYcrmmdzelc+uTnAGv2Ak1RE9oVL4Zy9tylUL5ybT81uVXtGNViSwtZ57"},{"entry":"BwPrfrgnGQbQcVi\/N+duGYGAOc4zL4Mp0L4Vn6tzw6LZP8elU1TDDztpenzZG4zkJcLt"},{"entry":"vhjnEp82VIU923FnTotv4PeSQOFkpwvc5ZAkpwd8NsmctR41ULc7c8faf1MlqdYF4iQfW"},{"entry":"0iAlpz7Bo9jnO5f0TOHnlcCWE0tLp32lEIHjh2qMfnAIDbt5B4zW\u201d\/>"},{"entry":"\u2003<\/urn:groove.net:SE>"},{"entry":"<\/urn:groove.net:Del>"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}}}},"Here Del is the delta element, with attributes as explained below. The SE element is the secured element with a single attribute that is the version of security used for the delta. The EC element is the encrypted content that is the result of encrypting the commands in the delta. The EC element contains a base64 encoding of the encrypted commands (if the encrypted commands are very large, a binary document will be used instead of a binary attribute). The IV element is an initialization vector used in the encryption algorithm, the KID element is the security key ID and the KV element is the key version number. The Auth element contains the authenticator that is used to integrity protect the delta. The PTSig element is a base64-encoded signature for the delta.","After being secured, the delta is serialized in step . For example, in one embodiment, the data is serialized as WBXML, a standard binary encoding for XML. If the delta has any binary documents, they can be bundled together as a MIME message.","Finally, in step , the serialized delta is forwarded to an outbound delta queue  in the communications manager  that sends the delta to other endpoints in the shared space. The process then ends in step .",{"@attributes":{"id":"p-0132","num":"0136"},"figref":"FIGS. 10A and 10B","b":["402","400","1000","1002","402","410","402","410","422"]},"The dynamics manager  then de-serializes the delta in step . This process converts the delta data back into XML code that is stored in the XML document  in the dynamics manager . At this point, the dynamics manager  can recognize that this is a delta element. All new inbound deltas are placed in an inbound delta queue  in the dynamics manager XML document . This queue  serves as a temporary holding place until the dynamics manager  is ready to process them. Often the dynamics manager  will wait until it has received all deltas from other sources, such as a relay server, before beginning to process the deltas. Once the dynamics manager  is ready to process the data, the delta is removed from the inbound delta queue .","In step , a check is made whether the endpoint that sent the delta is in an \u201cactive\u201d state (endpoint states are discussed below.) The delta is \u201cpended\u201d in step  if the endpoint that sent it is not in an active state. This is required because the delta that added the endpoint to the shared space may not have been executed and received deltas cannot be processed until it has been executed. Thus, all deltas received from an endpoint are pended until the delta that created the endpoint is obtained. Similarly, if an endpoint that generated the deltas has been \u201cuninvited\u201d from the shared space, then the system will not process a delta from that endpoint unless another endpoint that is still active in the shared space has also executed that delta. Pending deltas are stored in a pending deltas element  under the appropriate creator, until a decision is made to execute the delta. The pending deltas element  is periodically purged. If the pending deltas are not executed, they are deleted when the group that contains them is purged from the pending deltas element .","If the delta is not pended as determined in step , the sending endpoint is known and the delta can be integrity checked in step  by the security manager . It is important to verify that the delta has not been tampered with at this stage, before continuing to read the dependencies and other information from the delta. If the delta fails an integrity check, it is discarded in step  and the process proceeds, via off-page connectors  and , to finish in step .","If the delta passes the integrity check in step , the process proceeds, via off-page connectors  and , to step  where an attempt is made to decrypt the delta. It is possible to decrypt the delta at step , if the correct key is available. Because the dependencies of the delta have not been checked, it is possible that certain keys for the delta are missing. If that is the case, the delta is decrypted later. The decryption step converts the SE element in the encrypted delta back into the commands in the delta.","Next, in step , the dependencies of the delta are retrieved by the dynamics manager  by examining the dependency attribute of the delta. Since the dependency attribute is not encrypted, the dependencies can still be retrieved even if decryption was not possible in step . The dependencies are then assimilated as shown in more detail in , which when placed together form a flowchart that illustrates the steps in the process.","Dependency processing begins in step  and proceeds to step  where the list of explicit dependencies is opened from the attribute in the delta. If this is not the first delta from a creator, the previous delta from the creator is added to the beginning of the dependency list. In step , a determination is made whether all of the deltas that are dependencies have been processed. If so, dependency processing proceeds, via off-page connectors  and , to finish in step .","Alternatively, if in step , it is determined that not all dependencies have been processed, then in step , the dynamics manager  retrieves the next dependency from the list of dependencies. For each of these dependency deltas, a check is made, in step  to determine whether that dependency delta has been assimilated as described below. If that dependency delta has already been assimilated, then the process returns to step  to check if any further dependencies remain.","If any of the deltas in the dependency set have not been assimilated, then, in step , a determination is made whether the next delta has been pended. If the delta has not been pended, then the process proceeds, via off-page connectors,  and  to finish in step  because the next delta is missing. Alternatively, if the next delta has been pended as determined in step , then the process proceeds, via off-page connectors  and , to step  where an attempt is made to assimilate the pended delta. If the attempts is successful as determined in step , then the process proceeds, via off-page connectors  and , back to step  to determine whether additional dependencies must be processed. Alternatively, if the pended delta cannot be assimilated as determined in step , then the process finishes in step  because the dependency is missing.","If the new delta cannot be assimilated because some of its dependencies cannot be processed, it is placed in a delta holder . The delta holder is an XML data structure in the XML document  associated with the dynamics manager . It has a structure  as shown in . The delta holder  includes a DelHldr element  representing the delta holder. This element has an ActiveBlockNum attribute that holds the number of the currently active block. The DelHldr element , in turn, includes one or more HldGrp elements, each of which represents a group of held deltas and endpoint pulses that are all waiting for the same dependency. Each of these elements has a Dep attribute that specifies the dependency for which all the held deltas and pulses are waiting. Each HldGrp element includes one or more LDel elements, each of which represents a held delta and one or more EptPul elements, each of which represents a held pulse. The new delta is stored in a HldGrp element of the delta holder  corresponding to the missing dependency delta.","If the missing dependency delta is not pended as determined in step , then the dynamics manager  attempts to fetch the dependency delta from other endpoints in the shared space. In particular, a fetch request is sent to another endpoint in an attempt to retrieve the delta. In order to increase efficiency, fetch requests may be batched for all deltas and requested with one request.","Since the number of deltas that must be fetched from another endpoint can be large and, therefore, occur over a significant time period, it is necessary to prevent an endpoint from making multiple fetch requests for the same deltas while it is still receiving the deltas from a previous request. Multiple re-fetching is prevented by associating a sequence number with each fetch request. If an incoming fetch request does not have a high enough sequence number, then an endpoint receiving the request will not send the requested deltas. A fetch response sequence number is stored for each endpoint and used when a fetch request is received by that endpoint. In addition, a fetch request sequence number is stored for each endpoint and used when that endpoint generates a fetch request to another endpoint. These sequence numbers both start at zero and are used in the following manner.","The fetch process is illustrated in , which begins in step  and proceeds to step  where a fetch request is generated in a first endpoint, for example, endpoint A and sent to another endpoint, for example endpoint B in step . When such a fetch request is generated, it includes the fetch request sequence number. When a fetch request is received, in step , the fetch request sequence in the request is compared with the stored fetch response sequence number in the receiving endpoint. If the two sequence numbers are equal, then the fetch request is valid and, in step , the requested deltas are forwarded back to endpoint A. The receiving endpoint then increments its stored value of the fetch response sequence as set forth in step . Next, the receiving endpoint sends a fetch sequence number update message containing the value of the fetch response sequence number to the requesting endpoint in step . However, if the fetch request was invalid as determined in step , then the requested deltas are not sent. Instead, only the update message is sent in step .","When the fetch sequence update message is received at the sending endpoint, the sending endpoint updates its fetch request sequence number as set forth in step . In particular, if the fetch response sequence number in the received message is larger than current fetch request sequence number being used by the sending endpoint, then the endpoint sets its fetch request sequence number to the sequence number in the update message. Otherwise, the sending endpoint does not change its fetch request sequence number. The process then finishes in step .","The following is an example of the use of these sequence numbers. Assume that there exist two endpoints A and B each with internal stored values of fetch request sequence numbers and fetch response sequence numbers with values of zero. Endpoint A realizes it is missing deltas - and sends a fetch request for these deltas to endpoint B. This fetch request has fetch request sequence number with a value of zero. Endpoint B receives the fetch request, compares the sequence number therein to its stored fetch response sequence number (with a value of zero) and determines that the fetch request is valid. Endpoint B then sends the requested deltas to endpoint A and increments its stored fetch response sequence for endpoint A to a value of one. Endpoint B also sends to endpoint A, a fetch sequence update message carrying a sequence number of one.","Assume that endpoint A receives deltas - but decides to request deltas - again even though these deltas and the fetch sequence update message are still in transit. Endpoint A sends another request with a fetch request sequence number with a value of zero. Endpoint B receives the new fetch request from endpoint A and realizes that the request is invalid since the fetch request sequence number in it (with a value of zero) is less then its current fetch response sequence number for endpoint A (with a value of one). Therefore, endpoint B does not re-send deltas -, but instead sends another fetch sequence update message carrying an update sequence number with a value of one.","Endpoint A then finishes receiving deltas -, and also receives the first fetch sequence update message. Accordingly, endpoint A updates its internal fetch request sequence number to a value of one. Endpoint A ignores the second fetch sequence update message since the update sequence number value carried by it is less than or equal to its stored fetch request sequence number. Therefore, the same deltas are not sent twice.","The fetch request and fetch response sequence numbers are stored in and endpoints data structure that is part of the XML document . A sample XML data structure for holding endpoint information is shown in , when placed together show an illustrative XML structure  for storing information regarding endpoints. The structure is comprised of a number of XML elements. The Epts element  represents the collection of endpoints. This element  has an NDeps attribute that holds the new dependencies that need to be set on the next delta that this endpoint creates. The Epts element contains one or more Ept elements , , each of which represents a single endpoint.","Each Ept element  has a number of attributes including a ContactURL attribute that holds the contact URL of the endpoint. A CreationId attribute holds the sequence of the delta that created the endpoint, or the notation \u201cFirst\u201d if this endpoint created the shared space. A DeviceURL attribute holds the device URL of the endpoint. A HGN attribute holds the highest group number for that endpoint has declared in a pulse or by creating a delta. The HIP attribute stores the highest insertion point for a delta created by the endpoint. An OldCreationIds holds sequences of deltas that had previously invited this endpoint into the shared space.","An OutOfDateTime attribute is set to the time at which the first local delta was created since the last message received from the endpoint. A PendingLaggard attribute is set when the endpoint is a laggard as described below. The PurGrp attribute holds the highest group that this endpoint has declared it is willing to purge. A Reqs attribute stores deltas that are needed to assimilate held deltas from the endpoint and a ReqSeq attribute holds the fetch request sequence number to be used when the next fetch request is received at the endpoint. A RespSeq attribute holds the last fetch response sequence number sent to the endpoint after responding to one of its fetches.","The SPG attribute stores a secondary purge group used to determine what to purge in edge conditions such as when an endpoint has not created deltas. A State attribute holds the state of the endpoint for use in purging decisions that are discussed below. The UID attribute holds the unique ID of the endpoint. In one embodiment, this is a hash of the values in the ContactURL and DeviceURL attributes.","Each of the Ept elements, for example element , contains one or more Creator elements -, each of which represents a creator and a PndgDels element  that is used as a parent for pending deltas. Each Creator element represents a creator. Each time an endpoint reopens a space it creates a new creator to use when creating deltas. This creator has a random id that is used to make sure delta sequences are unique in the case of a crash or other event that may cause a loss of information. Each Creator element has several attributes including an HIP attribute that holds the insertion point of the highest delta created by the creator. A LastUID attribute holds the last sequence used in generating a unique ID for this creator. A PurgeGrp attribute stores the number of a group that when purged, permits the creator to be deleted. A SubNum attribute holds the last sub-sequence number used on an identity targeted or asynchronous delta created by this creator as described below.","The PndgDels element (, ) is the parent of any pending deltas. As previously mentioned, pending deltas are deltas that the dynamics manager may, or may not, want to assimilate. Pending deltas only occur if deltas are received before an endpoint is added to the space or after an endpoint has been removed. The PndgDels element  contains one or more LDel elements , each representing a delta.","Returning to , after the dynamics manager has attempted to assimilate all dependencies in step , the process proceeds to step  where a determination is made whether all dependencies have been assimilated. If not, the process proceeds to step  where the original delta is held. The process then finishes in step .","Alternatively, if it is determined in step  that all dependencies have been assimilated, then the process proceeds to step  where the new delta is also assimilated. Assimilating a delta consists of creating a transaction in the shared space database and putting the delta in the delta log . As discussed above, every delta has been previously stamped with a group number and a sequence. Thus, the process of inserting a delta into the delta log  starts with a determination of the correct group element in the delta log . If the group does not exist, it is created at this point. Then, the group is examined for a chain corresponding to the creator ID in the delta sequence. Again, if the chain does not exist, then it is created. The delta to be assimilated is added to the end of the chain.","Once a delta has been assimilated, it will be executed before the dynamics manager  commits the current transaction on the shared space database. Errors that occur during delta execution are handled by rolling forward or rolling back any commands that had been executed. In addition, the transaction that was started around the delta execution is aborted. The delta execution errors are handled as each delta is executed and before the current transaction is committed. To limit roll back and roll forward, the dynamics manager  attempts to assimilate as many deltas as possible before executing them. Therefore, the delta log  keeps track of how many deltas have been assimilated but not executed. This count will be used to make sure that all of the deltas are executed.","After assimilating the delta, it may now be possible to assimilate additional deltas that depend on the assimilated delta. These deltas are called \u201cdependents.\u201d This latter assimilation is performed in step . Specifically, within the delta holder , deltas are grouped based on the delta that they are waiting for. So whenever a delta is assimilated, a check is made to determine if there are any deltas in the delta holder  that were waiting for that delta. If such deltas exist, then an attempt is made to assimilate those deltas. During this latter assimilation, a check is made on the dependencies on those deltas, and if they are now all assimilated, the dependent deltas can be assimilated as well. Delta processing then finishes in step .","The execution process is illustrated in more detail in . This process begins in step  and proceeds to step , where the dynamics manager  walks backward through the delta log , starting from the end of the delta log and proceeding to the beginning of the log. In particular, because the log is arranged into chains, groups and blocks, the end of the log is the last delta in the last chain in the last group in the last block. Each delta in the last chain is examined moving towards the beginning of the chain. When the beginning of the last chain is reached, the process continues with the last delta in the next-to-last chain and so on until all deltas in the first chain in the group has been examined. Then, the process continues with the last delta in the last chain in the previous group. When the first group in a block is reached, then the process continues with the last delta of the last chain of the last group in the previous block. Operation continues in this manner until all newly assimilated deltas are found.","If a delta under examination has not been executed, as determined in step , process returns to step  where the next delta is examined. However, if the delta being examined has been executed, as determined in step , the dynamics manager  checks, in step , to determine whether it will conflict with any of the other assimilated deltas. During the assimilation process described above, a set of the aforementioned conflict strings of all assimilated deltas is built. The conflict check is performed by comparing the conflict string of the delta under examination with all conflict strings in the conflict string set. If the strings in any pair of strings are the same or if either string being compared is empty, then the corresponding deltas conflict. If there is no conflict, then the process returns to step  where the next delta is examined.","If there is a conflict as determined in step , then, in step , the dynamics manager rolls back the delta. As previously mentioned, the dynamics manager  must ensure that different endpoints converge, by insuring that all of the deltas are executed in a consistent order in each endpoint. It is possible that one of the newly assimilated deltas was assimilated at a position in the delta log  before the position of an independent delta that has already been executed. If such a situation occurs, in order to ensure convergence, the already-executed delta must be \u201crolled back\u201d before the assimilated deltas are executed, and then \u201crolled forward\u201d after the assimilated deltas are executed.","Rolling back a delta consists of iterating over the commands in the delta in reverse order. When each command is encountered, the engine  is called to undo the effect of the command. The engine  has access to the command element and the command undo element in each delta that the engine previously generated when the delta was last executed. If there is an error during the rollback of a command in the delta, the dynamics manager  will attempt to redo any commands in that delta that had already been rolled back, so that the delta is left in the executed state. The process then proceeds to step  where a determination is made whether all new deltas have been examined. If not, the process proceeds back to step  to continue examining deltas.","If all new deltas have been examined as determined in step , the process proceeds, via off-page connectors  and , to step  where the roll forward procedure begins. In step , the delta log is walked and each delta is again examined. A determination is made in step  whether the delta being examined has been rolled back. If so, the process proceeds to step . Alternatively, if the delta was not rolled back, either because there was no conflict as determined in step  or because an error occurred during rollback, the dynamics manager  determines in step  whether the delta is a new delta. If the delta is not a new delta, the dynamics manager  does nothing and the process proceeds to step . However, if the dynamics manager determines in step , that the delta is a new delta, then in steps  and , the new delta is decrypted and executed. The process then proceeds to step .","If the delta was successfully rolled back, as determined in step , then, in step , the dynamics manager  begins a roll forward, or re-execution, of all the commands in the delta. This process is exactly like the process of executing the delta in the first place. During this roll forward operation, the dynamics manager  will find all the newly assimilated deltas, including the delta that initiated the processing. Although the dynamics manager has already integrity checked this latter delta, it was possible that it could not be decrypted at that time as explained above. If the delta has not previously been decrypted, then it is decrypted as part of the roll forward process. At this point, any dependency deltas will have been executed. Therefore, the key needed to decrypt the delta will be available and the decryption should succeed.","The re-execution of the commands in the delta is performed in the same manner as the execution of commands described above in order to execute a delta on the endpoint that created it. However, in the case of an error during the execution, it is not possible to throw an exception to the tool that created the delta, because the tool is on a different machine. Instead, if such errors occur, after rolling back any successfully executed commands and aborting the transactions, the delta is just marked as having had an exception on execution and the dynamics manager continues with the process.","Exceptions during the execution of deltas that add new endpoints to the space are treated differently. If one endpoint in the space successfully adds the new endpoint, in is necessary for the dynamics manager to add the endpoint on all other endpoints in the shared space. This can be a problem in certain situations, for example if the add member permission was removed from the inviter of the new endpoint independent of the invitation. To handle these cases, the dynamics manager will continue to execute other commands in an add endpoint delta after an exception.","In any case, the process then proceeds to step  where a determination is made whether the roll forward process is complete. If not, the process returns to walking the delta log in step . If the roll forward operation is complete, the process then ends in step .","The processes described above illustrate the creation, transmission and reception of normal deltas. However, in accordance with the principles of the invention, special deltas have been created in order to handle several special situations. These latter deltas include asynchronous deltas, identity disseminated deltas and priority deltas. Asynchronous deltas are not sent as part of the normal communications traffic within a shared space. Instead, they are used for transferring very large files to prevent the deltas that contain them from blocking the transmission of other deltas in the shared space for a long time period and for supporting the transfer of asymmetric files. Asynchronous deltas have certain differences from normal deltas. The most important of these differences is that no future deltas can depend on an asynchronous delta. Therefore, other deltas in the shared space can be executed before the asynchronous delta arrives and asynchronous deltas will not have to be executed to maintain consistency. In addition, the dynamics manager will not fetch missing asynchronous deltas from other endpoints and, thus, their delivery is not as reliable as normal delta delivery. Finally, because no deltas depend on an asynchronous delta, it is not necessary to send asynchronous deltas to all endpoints in the shared space. This property makes transmitting asynchronous deltas to specific endpoints much simpler than normal deltas and allows support for asymmetric files.","However, asynchronous deltas have dependencies that permit the dynamics manager to guarantee prior execution, that is, when an asynchronous delta is executed on a remote endpoint all non-asynchronous deltas that the creator had executed at the creation time of the asynchronous delta will also have been executed on the remote endpoint. Asynchronous deltas also have a well-defined insertion point in the delta log and can be rolled back before the execution of deltas with lower delta log insertion points. Other deltas with a higher delta log insertion point will be rolled back before execution of an asynchronous delta. Asynchronous deltas are not guaranteed to be executed in the same relative order on all endpoints and, therefore, there is no convergence in the case of asynchronous deltas.","Asynchronous deltas can be interleaved with other deltas in the shared space and may be sent at a lower priority. For example, a large file may be transmitted with a combination of normal and asynchronous deltas. Normal deltas will be used to send the file descriptor and asynchronous deltas will be used to send the file contents. A GUID will be used to match descriptors with contents at the destination. The asynchronous delta containing the file contents may be sent as soon as the file is added (symmetric files) or may be sent only when a remote endpoint requests the file (asymmetric files). The XML structure of an asynchronous delta is the same as a normal delta as illustrated in , except an Async attribute on the Del element is set when the delta is asynchronous.","Identity targeted deltas are designed to support unread marks and other information that only needs to be known by other instances of the shared space for an identity. Identity targeted deltas can only be targeted at the local identity. These deltas are only sent to a subset of the endpoints in the shared space where this subset consists of the endpoints of the member that created the delta. As with asynchronous deltas, no other deltas can depend on an identity-disseminated delta, so identity disseminated deltas do not support convergence or reliability.","The dynamics manager  handles asynchronous and identity disseminated deltas as \u201csubdeltas.\u201d Instead of being identified by a delta sequence, subdeltas have a delta \u201csub-sequence.\u201d A sub-sequence is the sequence of the last delta created by the creator followed by a sub-number. The sub-number is incremented each time a new sub delta is executed. For example, if creator A executes a delta, then two identity disseminated deltas, then another delta and finally another identity disseminated delta, these deltas and subdeltas would have the following sequences and subsequences:","A1, A1.1, A1.2, A2, A2.1","Like normal deltas, sub-deltas are always inserted at the end of the delta log and have a fixed group number and insertion point in the delta log. In the above example, A1.1 is always inserted after A1 and before A2. It could be in the same group as A1, in the same group as A2 or in a group between the two. Because sub-deltas may not be sent to all endpoints, there is a much greater chance for other endpoints to create deltas that are independent in relation to sub-deltas. This greatly increases the chance that a sub-delta will be rolled back. However, because there are a limited number of deltas per group, an endpoint generating independent deltas will advance the group number and then the independent deltas will come after the sub-deltas. This limits the number of times that sub-deltas are rolled back.","Dependencies are set on a sub-delta in a manner similar to a normal delta. Because the sub-delta is not sent to all endpoints, the dependencies are not removed from the new dependency set after setting the dependencies on a sub-delta. In the example above, the deltas would have the following dependencies.",{"@attributes":{"id":"p-0175","num":"0179"},"tables":{"@attributes":{"id":"TABLE-US-00006","num":"00006"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"77pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"84pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"A1","{ }"]},{"entry":[{},"A1.1","{A1}"]},{"entry":[{},"A1.2","{A1}"]},{"entry":[{},"A2","{A1}"]},{"entry":[{},"A2.1","{A2}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}}},"Note that because they are all on previous deltas from this endpoint, these dependencies are not explicitly set on the deltas. Because sub-deltas are not sent to all endpoints, they cannot contain new security keys for the space. When securing deltas, the dynamics manager does not send a rekey in a sub-delta.","An XML file of an identity-disseminated delta is shown in . This delta  includes one command and undo information for that command and an execution nested delta with one command and undo information for that command. The IdDiss attribute  of the Del element  is set when the delta is an identity disseminated delta. The SubSeq attribute  of the Del element  holds a sub-sequence of the delta.","Priority deltas are used to address two situations that occur in the delta dissemination system described above. The first situation is that when a new endpoint is invited to a shared space, it must be sent the contents of a delta log containing all the previous deltas generated in the shared space because deltas executed independently with deltas that the inviter had executed prior to the invite could cause the rollback of those prior deltas. For that reason, the newly invited endpoint requires all of those prior deltas. The contents of a delta log are often much larger than the actual data in the shared space and the transmission of such contents can result in a significant time delay in the startup of a new endpoint.","The second situation can be illustrated in a shared space in which there are ten endpoints. One endpoint goes offline and executes a delta. The remaining nine endpoints are all online and each execute one hundred deltas. When the offline endpoint comes back online, the other endpoints will all receive the delta executed when the endpoint was offline and the (now) online endpoint will receive the deltas generated when that endpoint was offline. To achieve convergence, there are two possibilities. If the delta from the endpoint that is offline comes after the other deltas, the endpoint that went offline would need to rollback that delta, do the nine hundred deltas from the other endpoints, and then roll forward that delta. None of the other endpoints would need to do rollback. However, if the delta from the offline endpoint came first in the delta log, the other nine endpoints would each have to rollback the nine hundred deltas, do the one delta, and then roll forward the nine hundred deltas. Obviously, the former situation involves much less work, however, because of the manner in which group numbers are assigned, the latter situation, or at least one close to it would be the most likely to happen.","Both situations are addressed by priority deltas that provide a way to control the ordering of independent deltas. These can be used to address the first situation by ordering independent deltas so that deltas independent with an invite delta do not cause a rollback and therefore it is not necessary to send the entire delta log to a new invitee. Similarly, such an ordering can be used to reduce the amount of rollback in order to address the second situation. Priority deltas are processed so that a delta independent of a priority delta will be ordered after any deltas that the priority delta depends on. For example, if independently A generates deltas A1 and A2 and A2 is a priority delta and B generates B1, then, because A2 is a priority delta, B1 will always be ordered after A1. B1 might come before or after A2.","To address the first situation mentioned above, any time a new endpoint is added to a shared space, the delta that adds it has the highest possible priority. Therefore, none of the prior deltas will need to be rolled back. To address the second situation, endpoints that are online and generating deltas will periodically make one of the deltas a priority delta. In the example given above, this means that the nine hundred deltas generated by the nine online endpoints will contain a number of priority deltas. The offline endpoint will not generate a priority delta, so very few of the nine hundred deltas will need to be rolled back and the amount of work will be greatly reduced.","In order to prevent interference between simultaneous priority deltas, the priority of a delta will be ignored if there is an independent delta with a higher priority (or the same priority but a lower insertion point). This operation is problematic in that, if there are two independent deltas that add a new endpoint to the space, one of the newly added endpoints will need to do a rollback of deltas that it may not have. This latter situation is addressed by marking one of the newly invited endpoints as \u201cout of sync\u201d and requiring it to be re-invited to the shared space.","Priority deltas are implemented by adding a layer of structure to the delta log called a delta \u201cblock.\u201d Delta blocks are the largest division in the delta log. Blocks contain groups, groups contain chains, and chains contain deltas. A group may actually be split among multiple blocks. Each block contains any number of groups and each block has one particular delta that is the priority delta that caused that block to be created. When a new priority delta arrives, it will cause a new block to be created as long as that new block does cause the re-assimilation of another delta with a higher priority. Any assimilated independent deltas are removed from the previous block and moved to the new block (this requires a roll back to the previous location of an independent delta and then a roll forward with the independent delta in the new position). These independent deltas can be detected if priority deltas contain a complete representation of the delta log state. The delta log state consists of a list representing the highest sequence received from each endpoint. Note that it will be possible for a group to be split between different blocks so that absolute delta ordering by group and sequence will no longer be true.","Referring to the XML implementation of the a delta log as illustrated in , delta blocks are implemented by including one of more DelBlk elements in the delta log. A block is defined as a set of deltas with a block delta, which is the highest priority delta in the block. All deltas that the block delta depends on are located in previous blocks. All deltas that are independent of the block delta are in its block. Deltas that depend on the block delta may be in its block or in subsequent blocks. This arrangement ensures that all deltas independent of the block delta are assimilated after all deltas on which the block delta depends.","A DelBlk element, such as element  has several attributes including a DelSeq attribute that holds a sequence number of the priority delta for this block. This sequence number is constructed as described above. A HighGrpNum attribute holds the number of the highest group in the block. A NoUndo attribute is set when deltas in this block cannot be undone by a user. Each DelBlk element  has one of more DelGrp elements contained within it. These DelGrp elements represent the delta groups.","With this implementation, the aforementioned problem that occurs with independent add endpoint operations can be detected when two add endpoint deltas have the same block number. In this case, one new endpoint is declared the \u201cwinner\u201d and the other endpoint is declared \u201cout of synch.\u201d","Priority deltas are processed with the steps illustrated in . When a priority delta is going to be assimilated, a decision must first be made whether its priority will be ignored. As described above, the priority will be ignored when there is an independent delta with a higher priority. Therefore, the process begins in step  and proceeds to step  where all of the independent deltas are located. Each priority delta has an attribute that describes the state of the delta log on the endpoint that created it. This latter state is compared to the state of the delta log on the endpoint that is assimilating the delta. From this information, the independent deltas can be located.","Then in step , the priorities of the independent deltas are compared. If, as determined in step , none of the located independent deltas has a higher priority than the priority delta being assimilated, then the priority of the new delta will be honored. In step , each priority delta will begin a new block in the delta log. Priority deltas divide the delta log into blocks such that all deltas that the priority delta depends on are assimilated into blocks earlier than deltas in the block of the priority delta. Then, in step , all deltas independent of the priority delta and deltas that depend on the priority deltas are assimilated in the same block or a later block.","In particular, after the new block is created, any delta independent of the priority delta must be re-assimilated into its new position in the delta log. During the rollback phase, it is necessary to roll these deltas back in their old position and then roll them forward in the new position to maintain convergence. The process then finishes in step .","Alternatively, if, in step , a delta with higher priority is located, then the priority of the priority delta being assimilated will not be honored. In this case, the process proceeds to step  where the delta is assimilated into the existing current block. The process then finishes in step .","In order to prevent delta logs in each endpoint from continually growing in size due to new deltas being added, the logs are periodically purged. However, to prevent the premature destruction of delta information, the purging process is a two-step process. First, each endpoint declares the group that it is willing to purge (based on the highest group in each endpoint). Then, all endpoints purge deltas in groups selected by comparing the groups that the endpoints have declared that they are willing to purge. This process is illustrated in .","The process starts in step  and proceeds to step . In particular, in step , all endpoints periodically broadcast purge information in transmissions called endpoint pulses. Each pulse contains information identifying the highest group number for that endpoint and information identifying a group for which a pending for purge decision has been made. Further, each endpoint pulse has dependencies in the same way that deltas have dependencies. If the dependencies are missing from the endpoint receiving the pulse, the pulse will be held just like a delta would be held. In step , the highest group numbers for the endpoints are compared. In step , one less than the minimum of the highest group numbers for all active endpoints is identified as the group that is pending for purge. Finally, in step , each endpoint then purges up to the minimum of the pending groups of all active endpoints. The process then ends in step .","In a similar manner, endpoints themselves may be purged from the endpoints XML structure depending on whether they are active or not. Each endpoint has a state that is stored in the State attribute of the EPT element in the endpoints XML structure discussed above. The State attribute can have any of the following values:",{"@attributes":{"id":"p-0194","num":"0198"},"tables":{"@attributes":{"id":"TABLE-US-00007","num":"00007"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"147pt","align":"left"}}],"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":["Value","State","Definition"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["0","Local","The current user on his current device. Each"]},{"entry":[{},{},"endpoint considers itself the only local endpoint."]},{"entry":[{},{},"The remaining states are in reference to this"]},{"entry":[{},{},"local endpoint."]},{"entry":["1","Active","Another endpoint with which a local endpoint has"]},{"entry":[{},{},"decided it can synchronize. Deltas are always"]},{"entry":[{},{},"assimilated and executed from active endpoints if"]},{"entry":[{},{},"the dependencies are available."]},{"entry":["2","New","A new endpoint is implicitly added to the shared"]},{"entry":[{},{},"space because an inbound delta has been received"]},{"entry":[{},{},"from it before it has been added to the shared"]},{"entry":[{},{},"space. These latter deltas are stored in the"]},{"entry":[{},{},"pending deltas element until the new endpoint"]},{"entry":[{},{},"is actually added to the shared space."]},{"entry":["6","Disconnecting","An endpoint with which the local endpoint has"]},{"entry":[{},{},"ceased synchronizing. It may be necessary"]},{"entry":[{},{},"to execute deltas received from a"]},{"entry":[{},{},"disconnecting endpoint if deltas received"]},{"entry":[{},{},"from active endpoints depend on these"]},{"entry":[{},{},"deltas."]},{"entry":["7","Disconnected","An endpoint with which the local endpoint has"]},{"entry":[{},{},"ceased synchronizing. An endpoint is"]},{"entry":[{},{},"disconnected when no deltas received from"]},{"entry":[{},{},"active endpoints can depend on deltas"]},{"entry":[{},{},"received from the disconnected endpoint."]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}}}},"Each endpoint moves between these defined states depending on selected conditions. A state diagram for the various states and the transitions between states is shown in . For example, an endpoint enters the Local state  as illustrated by arrow  upon the creation of a new shared space. An endpoint other than the local endpoint enters the Active state , as illustrated by arrow , when an AddMember delta (invitation) for that endpoint is executed or when an AddDeviceForMember delta for that endpoint (shared space fetch) is executed. Similarly, an endpoint enters the New state  as indicated by arrow  when a delta is received from that previously unknown endpoint.","An endpoint transitions from the Local state  to the Active state  as indicated by arrow  when serializing of a space for a different endpoint is begun. In this case, the local endpoint is changed to the Active state . Similarly, an endpoint transitions from the Active state  to a Local state , as indicated by arrow , when a space on an endpoint is de-serialized. In that case, that endpoint is transitioned to the local state . This latter transition can happen when an account is re-imported to a device on which it had previously been installed.","An endpoint can transition from the Active state  to the New state  as indicated by arrow  when an account is re-imported to a device on which it had previously been installed. An endpoint can transition from an Active state  to a Disconnecting state , as indicated by arrow , when the active endpoint is declared to be a \u201claggard\u201d by the local endpoint. A laggard endpoint is defined as an endpoint from which no deltas have been assimilated for a specified period of time. The time check is typically performed during the purge declaration process mentioned above. This transition can also happen when a delta from the active endpoint is executed informing the local endpoint that the active endpoint deleted the space. It can further happen when a delta uninviting the active endpoint from the space is executed. Finally, this transition can be produced when a disconnect notification is received by the local endpoint informing it that the active endpoint is now disconnected.","A New state  to Active state  transition occurs, as indicated by arrow , when an AddMember delta (invitation) for that endpoint is executed or when an AddDeviceForMember delta (shared space fetch) is executed. This transition can also happen when an AddEndpoint delta (account import) is executed.","A New state  to Disconnecting state  transition can occur, as indicated by arrow  when a delta is received from a new endpoint whose insertion point has been marked as pending for purge. It can also occur when the local endpoint declares the insertion point of a delta from the new endpoint as \u201cpending for purge\u201d. In addition, this transition can occur when a disconnect notification is received by the local endpoint informing it that the new endpoint is now disconnected.","A New state  to Disconnected state  transition can occur, as indicated by arrow  when a delta is received from a new endpoint whose insertion point has purged. It can also occur when the insertion point of a delta received from by the local endpoint from the new endpoint has been purged by the local endpoint.","A Disconnecting state  to Active state  transition, as indicated by arrow  can occur when an AddMember delta (invitation) is executed or when an AddDeviceForMember delta (shared space fetch) is executed. Such a transition can also occur when an AddEndpoint delta (account import) is executed.","A Disconnecting state  to New state  transition can occur, as indicated by arrow  when an account is re-imported to a device on which it had previously been installed. A Disconnecting state  to Disconnected state  transition can occur as indicated by arrow  when the insertion point of a delta received by the local endpoint from the disconnecting endpoint is purged by the local endpoint. A Disconnected state  to Active state  transition can occur as indicated by arrow  when the local endpoint executes an AddMember delta (invitation) or an AddDeviceForMember delta (shared space fetch) or an AddEndpoint delta (account import).","A Disconnected state  to a New state  transition can occur as indicated by arrow  when an account is re-imported to a device on which it had previously been installed.","A software implementation of the above-described embodiment may comprise a series of computer instructions either fixed on a tangible medium, such as a computer readable media, for example, a diskette, a CD-ROM, a ROM memory, or a fixed disk, or transmittable to a computer system, via a modem or other interface device over a medium. The medium either can be a tangible medium, including but not limited to optical or analog communications lines, or may be implemented with wireless techniques, including but not limited to microwave, infrared or other transmission techniques. It may also be the Internet. The series of computer instructions embodies all or part of the functionality previously described herein with respect to the invention. Those skilled in the art will appreciate that such computer instructions can be written in a number of programming languages for use with many computer architectures or operating systems. Further, such instructions may be stored using any memory technology, present or future, including, but not limited to, semiconductor, magnetic, optical or other memory devices, or transmitted using any communications technology, present or future, including but not limited to optical, infrared, microwave, or other transmission technologies. It is contemplated that such a computer program product may be distributed as a removable media with accompanying printed or electronic documentation, e.g., shrink wrapped software, pre-loaded with a computer system, e.g., on system ROM or fixed disk, or distributed from a server or electronic bulletin board over a network, e.g., the Internet or World Wide Web.","Although an exemplary embodiment of the invention has been disclosed, it will be apparent to those skilled in the art that various changes and modifications can be made which will achieve some of the advantages of the invention without departing from the spirit and scope of the invention."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The above and further advantages of the invention may be better understood by referring to the following description in conjunction with the accompanying drawings in which:",{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":["FIG. 2","FIG. 1"]},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIGS. 3A and 3B"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIGS. 8A and 8B"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIGS. 9A and 9B"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIGS. 10A and 10B"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIGS. 11A and 11B"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIGS. 14A and 14B"},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIGS. 15A and 15B"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 18"},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 19"}]},"DETDESC":[{},{}]}
