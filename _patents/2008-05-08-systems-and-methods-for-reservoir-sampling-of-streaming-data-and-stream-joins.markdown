---
title: Systems and methods for reservoir sampling of streaming data and stream joins
abstract: Algorithms and concepts for maintaining uniform random samples of streaming data and stream joins. These algorithms and concepts are used in systems and methods, such as wireless sensor networks and methods for implementing such networks, that generate and handle such streaming data and/or stream joins. The algorithms and concepts directed to streaming data allow one or more sample reservoirs to change size during sampling. When multiple reservoirs are maintained, some of the algorithms and concepts periodically reallocate memory among the multiple reservoirs to effectively utilize limited memory. The algorithms and concepts directed to stream joins allow reservoir sampling to proceed as a function of the probability of a join sampling operation. In memory limited situations wherein memory contains the sample reservoir and a join buffer, some of the stream join algorithms and concepts progressively increase the size of the sampling reservoir and reallocate memory from the join buffer to the reservoir.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08392381&OS=08392381&RS=08392381
owner: The University of Vermont and State Agricultural College
number: 08392381
owner_city: Burlington
owner_country: US
publication_date: 20080508
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["RELATED APPLICATION DATA","STATEMENT OF GOVERNMENT INTEREST","FIELD OF THE INVENTION","BACKGROUND","SUMMARY OF THE DISCLOSURE","DETAILED DESCRIPTION"],"p":["This application claims the benefit of priority of U.S. Provisional Patent Application Ser. No. 60\/916,669, filed May 8, 2007, and titled \u201cAdaptive\/Progressive Reservoir Sampling Of Streaming Data And Stream Joins,\u201d which is incorporated by reference herein in its entirety.","Subject matter of this disclosure was made with government support under National Science Foundation Grant No. IIS-0415023. The government may have certain rights in this subject matter.","The present invention generally relates to the field of statistical sampling. In particular, the present invention is directed to systems and methods for reservoir sampling of streaming data and stream joins.","Uniform random sampling has been known for its usefulness and efficiency for generating consistent and unbiased estimates of an underlying population. In this sampling scheme, every possible sample of a given size has the same chance to be selected. Uniform random sampling has been heavily used in a wide range of application domains such as statistical analysis, computational geometry, graph optimization, knowledge discovery, approximate query processing, and data stream processing.","When data subject to sampling come in the form of a data stream (e.g. stock price analysis, and sensor networks monitoring), sampling encounters two major challenges. First, the size of the stream is usually unknown a priori and, therefore, it is not possible to predetermine the sampling fraction (i.e., sampling probability) by the time sampling starts. Second, in most cases the data arriving in a stream cannot be stored and, therefore, have to be processed sequentially in a single pass. A technique commonly used in this scenario is reservoir sampling, which selects a uniform random sample of a given size from an input stream of an unknown size. Reservoir sampling has been used in many database applications including clustering, data warehousing, spatial data management, and approximate query processing.","Conventional reservoir sampling selects a uniform random sample of a fixed size, without replacement, from an input stream of an unknown size (see Algorithm 1, below). Initially, the algorithm places all tuples in the reservoir until the reservoir (of the size of r tuples) becomes full. After that, each ktuple is sampled with the probability r\/k. A sampled tuple replaces a randomly selected tuple in the reservoir. It is easy to verify that the reservoir always holds a uniform random sample of the k tuples seen so far. Conventional reservoir sampling assumes a fixed size reservoir (i.e., the size of a sample is fixed).",{"@attributes":{"id":"p-0008","num":"0007"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Algorithm 1: Conventional Reservoir Sampling"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Inputs: r {reservoir size}"]},{"entry":[{},"1: k = 0"]},{"entry":[{},"2: for each tuple arriving from the input stream do"]},{"entry":[{},"3: \u2003k = k + 1"]},{"entry":[{},"4: \u2003if k \u2266 r then"]},{"entry":[{},"5: \u2003\u2003add the tuple to the reservoir"]},{"entry":[{},"6: \u2003else"]},{"entry":[{},"7: \u2003\u2003sample the tuple with the probability r\/k and replace a"]},{"entry":[{},"\u2002\u2009 \u2003\u2003randomly selected tuple in the reservoir with the"]},{"entry":[{},"\u2002\u2009 \u2003\u2003sampled tuple"]},{"entry":[{},"8: \u2003end if"]},{"entry":[{},"9: end for"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}}},"In addition to its usefulness in sampling in the context of data streams, uniform random sampling has been extensively used in the database community for evaluating queries approximately. This approximate query evaluation may be necessary due to limited system resources like memory space or computation power. Two types of queries have been mainly considered: 1) aggregation queries and 2) join queries. As between the two types, it is far more challenging for join queries because uniform random sampling of join inputs does not guarantee a uniform random sample of the join output.","In the context of data stream processing, others have addressed that challenge with a focus on streaming out (without retaining) a uniform random sample of the result of a sliding-window join query with limited memory. There are, however, many data stream applications for which such a continuous streaming out is not practical. One example is applications that need a block of tuples (instead of a stream of tuples) to perform some statistical analysis like median, variance, etc. For these applications, there should be a way of retaining a uniform random sample of the join output stream.","Another example comes from the applications that collect results of join queries from wireless sensor networks using a mobile sink. Data collection applications have been extensively addressed in research literature. In these applications, a mobile sink traverses the network and collects data from sensors. Thus, each sensor needs to retain a uniform random sample of the join output, instead of streaming out the sample tuples toward the sink.","A natural solution to keep a uniform random sample of the join output stream is to use reservoir sampling. However, keeping a reservoir sample over stream joins is not trivial since streaming applications can be limited in memory size.","In one implementation, the present disclosure is directed to a method of maintaining a uniform random sample by a machine. The method includes: establishing in a machine memory a sampling reservoir having a size; receiving a data stream containing sequentially arriving tuples; sampling the data stream so as to store ones of the sequentially arriving tuples in the sampling reservoir so as to create stored tuples; while sampling, adjusting the size of the sampling reservoir in a controlled manner; and after adjusting the size, continuing sampling the data stream and storing ones of the sequentially arriving tuples in the sampling reservoir so as to maintain a sample of the data stream with a certain uniformity confidence.","In a particular example of the immediately foregoing implementation, the machine memory has a limited size and the method further includes: establishing in the machine memory a plurality of sampling reservoirs each having a size; receiving a plurality of data streams each containing a plurality of sequentially arriving tuples, the plurality of data streams corresponding respectively to the plurality of sampling reservoirs; checking whether the size of any one or more of the plurality of sampling reservoirs should be changed; and for each of the plurality of sampling for which the size should be changed, adjusting the size of that one of the plurality of sampling reservoirs as a function of the limited size of the machine memory.","In another implementation, the present disclosure is directed to a method of performing join sampling by a machine. The method includes: establishing in a machine memory a sampling reservoir having a sampling reservoir size, and a join buffer having a join buffer size; simultaneously receiving a plurality of data streams; join-sampling the plurality of data stream so as to create a plurality of join-sample tuples; storing the plurality of join-sample tuples in the join buffer; reservoir sampling the plurality of join-sample tuples so as to create a plurality of reservoir sample tuples; and storing the plurality of reservoir sample tuples in the sampling reservoir.","In a further implementation, the present disclosure is directed to a computer-readable medium containing computer-executable instructions for performing a method of maintaining a uniform random sample. The computer-executable instructions include: a first set of computer-executable instructions for receiving a data stream containing sequentially arriving tuples; a second set of computer-executable instructions for sampling the data stream so as to store ones of the sequentially arriving tuples in a sampling reservoir so as to create stored tuples; a third set of computer-executable instructions for adjusting the size of the sampling reservoir in a controlled manner while sampling; and a fourth set of computer-executable instructions for continuing sampling the data stream after the adjusting of the size and storing ones of the sequentially arriving tuples in the sampling reservoir so as to maintain a sample of the data stream with a certain uniformity confidence.","In a particular example of the immediately foregoing implementation, the machine memory has a limited size and the computer-executable instructions further include: computer-executable instructions for establishing in the machine memory a plurality of sampling reservoirs each having a size; computer-executable instructions for receiving a plurality of data streams each containing a plurality of sequentially arriving tuples, the plurality of data streams corresponding respectively to the plurality of sampling reservoirs; computer-executable instructions for checking whether the size of any one or more of the plurality of sampling reservoirs should be changed; and computer-executable instructions that, for each of the plurality of sampling reservoirs for which the size should be changed, adjusts the size of that one of the plurality of sampling reservoirs as a function of the limited size of the machine memory.","In yet another implementation, the present disclosure is directed to a computer-readable medium containing computer-executable instructions for performing a method of maintaining a uniform random sample. The computer-executable instructions include: a first set of computer-executable instructions for establishing in a machine memory a sampling reservoir, having a sampling reservoir size, and a join buffer having a join buffer size; a second set of computer-executable instructions for simultaneously receiving a plurality of data streams; a third set of computer-executable instructions for join-sampling the plurality of data stream so as to create a plurality of join-sample tuples; a fourth set of computer-executable instructions for storing the plurality of join-sample tuples in the join buffer; a fifth set of computer-executable instructions for reservoir sampling the plurality of join-sample tuples so as to create a plurality of reservoir sample tuples; and a sixth set of computer-executable instructions for storing the plurality of reservoir sample tuples in the sampling reservoir.","In still another implementation, the present disclosure is directed to a system that includes: at least one processor for processing computer-executable instructions; and memory functionally connected to the at least one processor, the memory containing computer-executable instructions for performing a method of maintaining a uniform random sample. The computer executable instructions include: a first set of computer-executable instructions for receiving a data stream containing sequentially arriving tuples; a second set of computer-executable instructions for sampling the data stream so as to store ones of the sequentially arriving tuples in a sampling reservoir so as to create stored tuples; a third set of computer-executable instructions for adjusting the size of the sampling reservoir in a controlled manner while sampling; and a fourth set of computer-executable instructions for continuing sampling the data stream after the adjusting of the size and storing ones of the sequentially arriving tuples in the sampling reservoir so as to maintain a sample of the data stream with a certain uniformity confidence.","In a particular example of the immediately foregoing implementation, the memory includes a portion having a limited size and further contains: computer-executable instructions for establishing in the portion of the memory a plurality of sampling reservoirs each having a size; computer-executable instructions for receiving a plurality of data streams each containing a plurality of sequentially arriving tuples, the plurality of data streams corresponding respectively to the plurality of sampling reservoirs computer-executable instructions for checking whether the size of any one or more of the plurality of sampling reservoirs should be changed; and computer-executable instructions that, for each of the plurality of sampling reservoirs for which the size should be changed, adjusts the size of that one of the plurality of sampling reservoirs as a function of the limited size of the portion of the memory.","In yet still another implementation, the present disclosure is directed to a system that includes: at least one processor for processing computer-executable instructions; and memory functionally connected to the at least one processor, the memory containing computer-executable instructions for performing a method of maintaining a uniform random sample. The computer executable instructions include: a first set of computer-executable instructions for establishing in a machine memory a sampling reservoir, having a sampling reservoir size, and a join buffer having a join buffer size; a second set of computer-executable instructions for simultaneously receiving a plurality of data streams; a third set of computer-executable instructions for join-sampling the plurality of data stream so as to create a plurality of join-sample tuples; a fourth set of computer-executable instructions for storing the plurality of join-sample tuples in the join buffer; a fifth set of computer-executable instructions for reservoir sampling the plurality of join-sample tuples so as to create a plurality of reservoir sample tuples; and a sixth set of computer-executable instructions for storing the plurality of reservoir sample tuples in the sampling reservoir.","The present invention relates to the development by the present inventors of novel algorithms for reservoir sampling of data streams and stream joins, as well as to systems and methods that implement these algorithms. The novel algorithms include adaptive reservoir sampling, for single and multiple reservoirs, in which the size of the reservoir(s) at issue is\/are dynamically increased and\/or decreased in size during the sampling. The novel algorithms also include fixed and progressive (increasing in size) reservoir join sampling. These algorithms, as well as examples of implementations of these algorithms in systems and methods, are described below in detail in the following sections.","Prior to proceeding, however, it is useful at this point to introduce the notion of \u201cuniformity confidence\u201d (UC) since a number of the specific algorithms presented utilize it in decision-making that addresses tradeoffs inherent in increasing the size of a sampling reservoir. Uniformity confidence is the probability that a sampling algorithm generates a uniform random sample. A sample is a uniform random sample if it is produced using a sampling scheme in which all statistically possible samples of the same size are equally likely to be selected. In this case, we say the uniformity confidence in the sampling algorithm equals 100%. In contrast, if some statistically possible samples cannot be selected using a certain sampling algorithm, then we say the uniformity confidence in the sampling algorithm is below 100%. Thus, we define uniformity confidence as follows:",{"@attributes":{"id":"p-0043","num":"0042"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mfrac":{"mtable":[{"mtr":[{"mtd":{"mrow":{"mi":["the","number","of","different","samples"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]}}},{"mtd":{"mrow":{"mi":["of","the","same","size","possible","with","the","algorithm"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]}}}]},{"mtr":[{"mtd":{"mrow":{"mi":["the","number","of","different","samples"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]}}},{"mtd":{"mrow":{"mi":["of","the","same","size","possible","statistically"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]}}}]}]},"mo":"\u00d7","mn":"100"}},{"mrow":{"mo":["{","}"],"mn":"1"}}]}}}}},"For reservoir sampling, the uniformity confidence in a reservoir sampling algorithm which produces a sample S of size r (denoted as S) is defined as the probability that S is a uniform random sample of all the tuples seen so far. That is, if k tuples have been seen so far, then the uniformity confidence is 100% if and only if every statistically possible S has an equal probability to be selected from the k tuples. As seen below, if the reservoir size is decreased from r to r\u2212\u03b4 (\u03b4>0), then there is a way to maintain the sample in the reduced reservoir such that every statistically possible S has an equal probability of being in the reduced reservoir, whereas if the reservoir size is increased from r to r+\u03b4 (\u03b4>0), then some statistically possible S's cannot be selected.","Adaptive Reservoir Sampling","There are situations where it would be desirable to change the size of one or more sampling reservoirs dynamically as sampling proceeds. A first example scenario is illustrated in , which shows a wireless sensor network  that includes a plurality of wireless sensors  distributed in a plurality of spatial clusters, here three clusters A-C. For the collection of readings from sensors , each cluster A-C has an associated proxy A-C that includes a memory  that stores sensor readings from the sensors of that cluster and acts as a data cache. A mobile sink  navigates network  to periodically collect data from proxies A-C. Memory  of each proxy A-C, however, is limited and, therefore, may store only samples of the readings. Each proxy A-C may very well keep multiple reservoir samples , one for each sensor  in the corresponding cluster A-C. A software application, for example, a monitoring and analysis application  aboard a computer , such as a general purpose computer (e.g., laptop, desktop, server, etc.), may demand that the size of a reservoir be in proportion to the number of readings generated so far by the corresponding sensor. If, for example, the sampling rates of sensors  change over time, the reservoir sizes should be adjusted dynamically as the sampling rates of the sensors change.","As those skilled in the art will readily appreciate, in the foregoing example each sensor  may include one or more of any type of transducer or other measurement device suitable for sensing the desired state(s) at the location of that sensor. Examples of such device types include, but are not limited to, temperature transducers, motion transducers (e.g. accelerometers), displacement transducers, flow transducers, speed transducers, pressure transducers, moisture transducers, chemical sensing transducers, photo transducers, voltage sensors, electrical current sensors, electrical power sensors, and radiation transducers, among many others. Wireless sensors, proxies, mobile sinks and computers suitable for use, respectively, as wireless sensors , proxies A-C, mobile sink  and computer , are well known in the art and therefore need not be described in any detail herein for those skilled in the art to implement concepts of the present invention to their fullest scope. The same holds true for actual software applications that correspond to software application .","As another example of a situation in which it could be beneficial to dynamically change the size of a sampling reservoir is approximate query processing. For instance, others have adapted reservoir sampling to sample the result of a join operation. Random sampling over joins requires information about the statistics of base relations. However, such information may not be available and, therefore, it may not be possible to pre-estimate the size of an intermediate join result and accordingly pre-allocate an appropriate reservoir size. Even if available, such statistics are often inaccurate at run time, and the size of an intermediate join result may be much larger than estimated. If, while the sampling is in progress, the reservoir size becomes too small to represent the intermediate join result adequately, then the reservoir size should be increased. Furthermore, if the total available memory for the query processor is limited, increasing the size of a reservoir would force the release of some memory elsewhere, possibly decreasing the size of another reservoir.","In a further example, periodic queries, a variation of continuous queries, are appropriate for many real-time streaming applications, such as security surveillance and health monitoring. In the periodic query model, once a query is registered to the system, query instances are instantiated periodically by the system. Upon instantiation, a query instance takes a snapshot of tuples that arrived since the last instantiation of the query. Consider a common situation in which, due to the nature of data streams and their potentially high arrival rates, a technique like random sampling is used to reduce the stream rate. As the query is instantiated periodically, the system may keep a reservoir sample of stream data arriving between the execution times of two consecutive query instances. If at some point the reservoir size has become too small to represent the stream adequately, the system should provide a way to increase the reservoir size for better representing the stream data at the execution time of the next query instance. Moreover, it is not uncommon that multiple queries run simultaneously in the system. In this case, each query may have its own reservoir sample maintained. Besides, at any point in time, one or more queries can be registered to or expired from the system. In order to adapt to this dynamic change of the query set, the system should be able to adaptively reallocate the memory among all reservoirs of the current query set.","Although in a variety of situations it is desirable to adjust the reservoir size while the sampling is in progress, this adjusting does not come for free. As described below, such an adjustment may have a negative impact on the statistical quality of the sample in terms of the probability of the sample being uniform. Motivated by this observation, the development of algorithms for adaptive-size reservoir sampling over data streams considered the following two main factors: 1) reservoir size (or equivalently sample size) and 2) sample uniformity. An appropriate sample size depends on data characteristics such as the size, mean, and variance of the population. Sample uniformity brings an unbiased representation of the population, and is especially desirable if it is not clear in advance how the sample will be used.","Development of adaptive-size reservoir sampling shows that, on one hand, if the reservoir size decreases, the sample in the reduced reservoir can be maintained with a 100% uniformity confidence. On the other hand, if the reservoir size increases, it is not possible to maintain the sample in the enlarged reservoir with a 100% uniformity confidence, and, in this case, there is a tradeoff between the size of the enlarged reservoir and the uniformity confidence.","The following section presents a novel algorithm (called \u201cadaptive reservoir sampling\u201d) for maintaining a reservoir sample for a single reservoir after the reservoir size is adjusted. If the size decreases, the algorithm maintains a sample in the reduced reservoir with a 100% uniformity confidence by randomly evicting tuples from the original reservoir. If the size increases, the algorithm finds the minimum number of incoming tuples that should be considered in the input stream to refill the enlarged reservoir such that the resulting uniformity confidence exceeds a given threshold. Then, the algorithm decides probabilistically on the number of tuples to retain in the enlarged reservoir and randomly evicts the remaining number of tuples. Eventually, the algorithm fills the available room in the enlarged reservoir using the incoming tuples.","Following the presentation of a single-reservoir algorithm, subsequent sections extend the single-reservoir algorithm to an \u201cadaptive multi-reservoir sampling\u201d and describe experiments using real sensor networks data sets. The experimental results demonstrate the adaptivity of the adaptive multi-reservoir sampling algorithm through two sets of experiments. The first set of experiments shows the sizes of multiple reservoirs changing adaptively to the change in the sampling rate of sensors, and the second set of experiments shows the effects of these changes on the samples' uniformity.","Adaptive Reservoir Sampling\u2014Single Reservoir","An algorithm for adaptive reservoir sampling for a single reservoir is as follows:",{"@attributes":{"id":"p-0054","num":"0053"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Algorithm 2: Adaptive Reservoir Sampling"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"Inputs:\u2003r {reservoir size}"},{"entry":"\u2002\u2009 \u2003k {number of tuples seen so far}"},{"entry":"\u2002\u2009 \u2003\u03b6 {uniformity confidence threshold}"},{"entry":"1: while true do"},{"entry":"2: \u2003while reservoir size does not change do"},{"entry":"3: \u2003\u2003conventional reservoir sampling (Algorithm 1 (Background"},{"entry":"\u2002\u2009 \u2003\u2003section, above)."},{"entry":"4: \u2003end while"},{"entry":"5: \u2003if reservoir size is decreased by \u03b4 then"},{"entry":"6: \u2003randomly evicts \u03b4 tuples from the reservoir."},{"entry":"7: \u2003else"},{"entry":"8: \u2003\u2003{i.e., reservoir size is increased by \u03b4}"},{"entry":"9: \u2003\u2003Find the minimum value of m (using Equation {3}, below, with"},{"entry":"\u2002\u2009 \u2003\u2003the current values of k, r, \u03b4) that causes the UC to exceed \u03b6."},{"entry":"10:\u2003\u2003flip a biased coin to decide on the number, x, of tuples to retain"},{"entry":"\u2002\u2009 \u2003\u2003among r tuples already in the reservoir (Equation {4}, below)."},{"entry":"11:\u2003\u2003randomly evict r \u2212 x tuples from the reservoir."},{"entry":"12:\u2003\u2003select r+\u03b4\u2212x tuples from the incoming m tuples using conventional"},{"entry":"\u2002\u2009 \u2003\u2003reservoir sampling (Algorithm 1 (Background section, above)."},{"entry":"13:\u2003end if"},{"entry":"14:end while"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"As mentioned above, using this algorithm to dynamically decrease the size of the reservoir during sampling allows the uniformity confidence to remain 100%, while using the algorithm to dynamically increase the size of the reservoir during sampling results in a uniformity confidence less than 100%, though the decrease in confidence can be controlled by controlling the size increase, and vice versa. Using Equation {1}, above, the effect of decreasing and increasing the size of the reservoir on the uniformity confidence can be demonstrated as follows.","Decreasing the Size of the Reservoir. Suppose the size of a reservoir is decreased from r to r\u2212\u03b4 (\u03b4>0) immediately after the ktuple arrives (see ). Then, the sample in the reduced reservoir can be maintained by randomly evicting \u03b4 tuples from the original reservoir. With this random eviction in place, there are",{"@attributes":{"id":"p-0057","num":"0056"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mo":"\u2003","mrow":{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mi":"r"}},{"mtd":{"mrow":{"mi":["r","\u03b4"],"mo":"-"}}}]}}}}},"br":{},"sub":"[r\u2212\u03b4]"},{"@attributes":{"id":"p-0058","num":"0057"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mo":"\u2003","mrow":{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mi":"k"}},{"mtd":{"mi":"r"}}]}}}}},"br":{},"sub":"[r]"},{"@attributes":{"id":"p-0059","num":"0058"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mo":"\u2003","mrow":{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mrow":{"mi":"k","mo":"-","mrow":{"mo":["(",")"],"mrow":{"mi":["r","\u03b4"],"mo":"-"}}}}},{"mtd":{"mrow":{"mi":"r","mo":"-","mrow":{"mo":["(",")"],"mrow":{"mi":["r","\u03b4"],"mo":"-"}}}}}]}}}}},"br":{},"sub":["[r\u2212\u03b4]","[r]"]},{"@attributes":{"id":"p-0060","num":"0059"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"mrow":[{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mi":"k"}},{"mtd":{"mi":"r"}}]}},{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mi":"r"}},{"mtd":{"mrow":{"mi":["r","\u03b4"],"mo":"-"}}}]}}],"mo":"\u2062"}},{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mrow":{"mi":"k","mo":"-","mrow":{"mo":["(",")"],"mrow":{"mi":["r","\u03b4"],"mo":"-"}}}}},{"mtd":{"mrow":{"mi":"r","mo":"-","mrow":{"mo":["(",")"],"mrow":{"mi":["r","\u03b4"],"mo":"-"}}}}}]}}],"mo":"\/"}}},"br":{},"sub":"[r\u2212\u03b4]"},{"@attributes":{"id":"p-0061","num":"0060"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mi":"k"}},{"mtd":{"mrow":{"mi":["r","\u03b4"],"mo":"-"}}}]}},"mo":"."}}},"br":{}},{"@attributes":{"id":"p-0062","num":"0061"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"UC","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["k","r","\u03b4"],"mo":[",",","]}}},{"mrow":[{"mfrac":{"mrow":[{"mrow":[{"mo":["(",")"],"mrow":{"mrow":[{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mi":"k"}},{"mtd":{"mi":"r"}}]}},{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mi":"r"}},{"mtd":{"mrow":{"mi":["r","\u03b4"],"mo":"-"}}}]}}],"mo":"\u2062"}},{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mrow":{"mi":"k","mo":"-","mrow":{"mo":["(",")"],"mrow":{"mi":["r","\u03b4"],"mo":"-"}}}}},{"mtd":{"mrow":{"mi":"r","mo":"-","mrow":{"mo":["(",")"],"mrow":{"mi":["r","\u03b4"],"mo":"-"}}}}}]}}],"mo":"\/"},{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mi":"k"}},{"mtd":{"mrow":{"mi":["r","\u03b4"],"mo":"-"}}}]}}]},"mo":"\u00d7","mn":"100"},{"mfrac":{"mrow":[{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mi":"k"}},{"mtd":{"mrow":{"mi":["r","\u03b4"],"mo":"-"}}}]}},{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mi":"k"}},{"mtd":{"mrow":{"mi":["r","\u03b4"],"mo":"-"}}}]}}]},"mo":"\u00d7","mn":"100"}],"mo":"="}],"mo":"="}},{"mrow":{"mo":["{","}"],"mn":"2"}}]}}}},"br":{}},"Increasing the Size of the Reservoir. Suppose the size of a reservoir is increased from r to r+\u03b4 (\u03b4>0) immediately after the ktuple arrives (see ). Then, the reservoir has room for 6 additional tuples. Clearly, there is no way to fill this room from sampling the k tuples as they have already passed by. Only incoming tuples can be used to fill the room. The number of incoming tuples used to fill the enlarged reservoir is denoted as m and is called the \u201cuniformity confidence recovery tuple count.\u201d","For the sake of better uniformity, some of the r existing tuples are allowed to be evicted probabilistically and replaced by some of the incoming m tuples. In Algorithm 2, the number of tuples evicted (or equivalently, the number of tuples retained) are randomly picked. Clearly, the number of tuples that are retained, x, can be no more than r. Besides, x should not be less than (r+\u03b4)\u2212m if m<r+\u03b4 (because otherwise, with all m incoming tuples the enlarged reservoir cannot be refilled), and no less than 0 otherwise. Hence, there can be x tuples, where x\u03b5[max {0, (r+\u03b4)\u2212m}, r], from the k tuples and the other r+\u03b4\u2212x tuples from the m tuples. This eviction scheme allows for",{"@attributes":{"id":"p-0065","num":"0064"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mi":"k"}},{"mtd":{"mi":"x"}}]}},{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mi":"m"}},{"mtd":{"mrow":{"mi":["r","\u03b4","x"],"mo":["+","-"]}}}]}}],"mo":"\u2062"}}},"br":{},"sub":"[r+\u03b4]"},{"@attributes":{"id":"p-0066","num":"0065"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mrow":{"mi":["k","m"],"mo":"+"}}},{"mtd":{"mrow":{"mi":["r","\u03b4"],"mo":"+"}}}]}},"mo":"."}}},"br":{}},{"@attributes":{"id":"p-0067","num":"0066"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"UC","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["k","r","\u03b4","m"],"mo":[",",",",","]}}},{"mfrac":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"x","mo":"=","mrow":{"mi":"max","mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mn":"0","mo":",","mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":["r","\u03b4"],"mo":"+"}},"mo":"-","mi":"m"}}}}},"mi":"r"},"mo":"\u2062","mrow":{"mrow":[{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mi":"k"}},{"mtd":{"mi":"x"}}]}},{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mi":"m"}},{"mtd":{"mrow":{"mi":["r","\u03b4","x"],"mo":["+","-"]}}}]}}],"mo":"\u2062"}},{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mrow":{"mi":["k","m"],"mo":"+"}}},{"mtd":{"mrow":{"mi":["r","\u03b4"],"mo":"+"}}}]}}]},"mo":"\u00d7","mn":"100"}],"mo":"="}},{"mrow":{"mo":["{","}"],"mn":"3"}}]}}}},"br":{}},"Examining this formula shows that the uniformity confidence increases monotonously and saturates as m increases.  shows this pattern for one setting of k, r, and \u03b4. Note that the uniformity confidence never reaches 100%, as exemplified by  which magnifies the uniformity confidence curve of  for m\u22679000.","Based on the foregoing, the exemplary adaptive reservoir sampling algorithm works as shown in Algorithm 2, above. As long as the size of the reservoir does not change, it uses conventional reservoir sampling to sample the input stream (Line 3). If the reservoir size decreases by \u03b4, the algorithm evicts \u03b4 tuples from the reservoir randomly (Line 6). After that, the algorithm continues sampling using the conventional reservoir sampling (Line 3). On the other hand, if the reservoir size increases by \u03b4, the algorithm computes the minimum value of m (using Equation {3}) that causes the uniformity confidence to exceed a given threshold (\u03b6) (Line 9). Then, the algorithm flips a biased coin to decide on the number of tuples (x) to retain among the r tuples already in the reservoir (Line 10). The probability of choosing the value x, where max {0, (r+\u03b4)\u2212m}\u2266x\u2266r, is defined as:",{"@attributes":{"id":"p-0070","num":"0069"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}},"mo":"=","mfrac":{"mrow":[{"mrow":[{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mi":"k"}},{"mtd":{"mi":"x"}}]}},{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mi":"m"}},{"mtd":{"mrow":{"mi":["r","\u03b4","x"],"mo":["+","-"]}}}]}}],"mo":"\u2062"},{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mrow":{"mi":["k","m"],"mo":"+"}}},{"mtd":{"mrow":{"mi":["r","\u03b4"],"mo":"+"}}}]}}]}}},{"mrow":{"mo":["{","}"],"mn":"4"}}]}}}}},"After that, the algorithm randomly evicts r\u2212x tuples from the reservoir (Line 11) and refills the remaining reservoir space with r+\u03b4\u2212x tuples from the arriving m tuples using conventional reservoir sampling (Line 12). Eventually, the algorithm continues sampling the input stream using the conventional reservoir sampling (Line 3) as if the sample in the enlarged reservoir were a uniform random sample of the k+m tuples.","Adaptive Reservoir Sampling\u2014Multiple Reservoirs","Notations used in this section appear in the following Table 1.",{"@attributes":{"id":"p-0073","num":"0072"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"161pt","align":"left"}}],"thead":{"row":[{"entry":[{},"TABLE 1"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]},{"entry":[{},"Symbol","Description"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\u03bd","number of streams (i.e., number of reservoirs)"]},{"entry":[{},"S","stream i"]},{"entry":[{},"R","the reservoir allocated to S"]},{"entry":[{},"M","total available memory for \u03bd reservoirs"]},{"entry":[{},"t","current time point"]},{"entry":[{},"r(t)","computed size of Rat t"]},{"entry":[{},"r(t)","computed size of Rat t with limited memory M"]},{"entry":[{},"r(t)","size of Radjusted at time point t(t< t)"]},{"entry":[{},"\u03b4i (t)","change in the size of Rat t"]},{"entry":[{},"k(t)","number of tuples seen up to t from S"]},{"entry":[{},"m(t)","number of tuples to be seen from S, starting from"]},{"entry":[{},{},"t, to fill an enlarged reservoir R"]},{"entry":[{},"\u03bb(t)","the average stream rate of S"]},{"entry":[{},"T","time period left until the next data collection time"]},{"entry":[{},"\u03b6","uniformity confidence threshold"]},{"entry":[{},"\u03c6","memory adjustment threshold (0 \u2266 \u03c6 \u2266 1)"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}}},"In this section, we extend the adaptive reservoir sampling algorithm for a practical application of multi-reservoir sampling in which samples are collected in memory-limited situations, such as wireless sensor networks using a mobile sink. See  and accompanying description as an example of such a network. Applications of data collection over wireless sensor networks using a mobile sink have recently received significant research attention. These applications take advantage of the mobility to improve the process of data gathering. Again, in such applications, at least one mobile sink roves the network and collects data from sensors in its proximity, thereby reducing the in-network communications and increasing the lifetime of the network. While a wireless sensor network is presented as an example, those skilled in the art will readily appreciate that adaptive multi-reservoir sampling of the present disclosure can be readily implemented in any of a variety of applications, such as health monitoring (for patient care, senior citizen safety, etc.), surveillance (for border security, building security, etc.), power grid monitoring (for grid protection, load distribution, etc.), environmental monitoring (for traffic control, habitat monitoring, etc.), structural diagnostics (for bridge or building monitoring and repair, etc.), and target tracking (for military operation, etc.), among others. As those skilled in the art will appreciate, each of these examples involves the collection of sensor data from multiple sensors that may be provided in a wired or wireless sensor network, or hybrid containing both wired and wireless sensors. Again,  is illustrative of a wireless sensor network that could be used for any of these applications. Those skilled in the art will readily understand the modifications needed to implement the present invention in a wired or hybrid type sensor network.","In this example it is assumed that the processing power of each proxy, such as each proxy A-C of , is sufficient to carry out the required computations. For this scenario, an adaptive multi-reservoir sampling algorithm is based on the following key ideas. First, an objective of the algorithm is to adaptively adjust the memory allocation in each proxy so that the size of each reservoir is allocated in proportion to the number of readings (i.e., tuples) generated so far by the corresponding sensor. More specifically, this objective is to allocate the memory of size M to the reservoirs (R, R, . . . , Rv) of v input streams (S, S, . . . , Sv) so that at the current time point t, the size r(t) of each reservoir Ris proportional to the total number of tuples, k(t), seen so far from S. The rationale behind this objective is explained below. Second, the algorithm adjusts the memory allocation only if the relative change in the size of at least one reservoir is above a given memory adjustment threshold and the resulting uniformity confidence for all reservoirs exceeds a given uniformity confidence threshold.","Three criteria can be used to determine a statistically appropriate sample size for a given population. These criteria are the confidence interval, the confidence level, and the degree of variability in the population. Confidence interval is the range in which the true value of the population is estimated to be. Confidence level is the probability value associated with a confidence interval. Degree of variability in the population is the degree in which the attributes being measured are distributed throughout the population. A more heterogeneous population requires a larger sample to achieve a given confidence interval. Based on these criteria, the following simplified formula for calculating a statistically appropriate sample size is provided, assuming 95% confidence level and 50% degree of variability (note that 50% indicates the maximum variability in a population):",{"@attributes":{"id":"p-0077","num":"0076"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"n","mo":"=","mfrac":{"mi":"N","mrow":{"mn":"1","mo":"+","mrow":{"mi":"N","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mi":"e","mn":"2"}}}}}},{"mrow":{"mo":["{","}"],"mn":"5"}}]}}}},"br":{}},"Adapting this formula to our problem, we compute the size r(t) of Ras:",{"@attributes":{"id":"p-0079","num":"0078"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msub":{"mi":["r","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},"mo":"=","mfrac":{"mrow":[{"msub":{"mi":["k","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"mn":"1","mo":"+","mrow":{"mrow":{"msub":{"mi":["k","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mi":"e","mn":"2"}}}]}}},{"mrow":{"mo":["{","}"],"mn":"6"}}]}}}},"br":{}},{"@attributes":{"id":"p-0080","num":"0079"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"v"},"mo":"\u2062","mrow":{"msub":{"mi":["r","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}},"mo":"\u2264","mi":"M"}},{"mrow":{"mo":["{","}"],"mn":"7"}}]}}}},"br":{},"sub":["i ","i"]},{"@attributes":{"id":"p-0081","num":"0080"},"maths":{"@attributes":{"id":"MATH-US-00015","num":"00015"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msubsup":{"mi":["r","i","M"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"mo":["\u230a","\u230b"],"mrow":{"mi":"M","mo":["(",")"],"mfrac":{"mrow":[{"msub":{"mi":["r","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"v"},"mo":"\u2062","mrow":{"msub":{"mi":["r","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}}]}}}],"mo":"="}},{"mrow":{"mo":["{","}"],"mn":"8"}}]}}}}},"At the current time point t, this computed reservoir size r(t) may be different from the reservoir size r(t) adjusted at time point t(t<t). Let \u03b4(t) denotes the difference. As concluded above, if \u03b4(t)<0, the uniformity confidence is 100%. In contrast, if \u03b4(t)>0, the uniformity confidence is below 100%; in this case, as in Algorithm 2, the sample is maintained in an enlarged reservoir Rusing incoming tuples from the input stream. In the present problem formulation, the number of incoming tuples m(t) used to fill an enlarged reservoir Ris computed as a product of the average stream rate, \u03bb(t), and the time period, T, left to the next data collection time as follows:\n\n()=\u03bb()\u00d7\u2003\u2003{9}\n\nFor an enlarged reservoir R, the uniformity confidence expressed in Equation {3} is refined here as follows:\n",{"@attributes":{"id":"p-0083","num":"0082"},"maths":{"@attributes":{"id":"MATH-US-00016","num":"00016"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["UC","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":["k","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"msub":{"mi":["r","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["t","u"]}}},{"msub":{"mi":["\u03b4","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"msub":{"mi":["m","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}],"mo":[",",",",","]}}},{"mfrac":{"mrow":[{"munderover":{"mo":"\u2211","mrow":[{"mi":"x","mo":"=","mrow":{"mi":"max","mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mn":"0","mo":[",",","],"mrow":[{"mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":["r","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["t","u"]}}},{"msub":{"mi":["\u03b4","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}],"mo":"+"}},"mo":"-","mi":"m"},{"mo":["(",")"],"mi":"t"}]}}}},{"msub":{"mi":["r","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["t","u"]}}}]},"mo":"\u2062","mrow":{"mrow":[{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"msub":{"mi":"k","mrow":{"mi":"i","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}}}},{"mtd":{"mi":"x"}}]}},{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mrow":{"msub":{"mi":["m","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}}},{"mtd":{"mrow":{"mrow":[{"msub":{"mi":["r","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["t","u"]}}},{"msub":{"mi":["\u03b4","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}],"mo":["+","-"],"mi":"x"}}}]}}],"mo":"\u2062"}},{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"msub":{"mi":["k","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"msub":{"mi":["m","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}],"mo":"+"}}},{"mtd":{"mrow":{"mrow":[{"msub":{"mi":["r","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["t","u"]}}},{"msub":{"mi":["\u03b4","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}],"mo":"+"}}}]}}]},"mo":"\u00d7","mn":"100"}],"mo":"="}},{"mrow":{"mo":["{","}"],"mn":"10"}}]}}}},"br":{},"sub":["i","i"]},"To control the frequency of memory allocation adjustment, we consider the adjustment only if the relative change in the computed size (Equation {8}) exceeds a given threshold (denoted as \u03c6) for some R, that is, the adjustment is considered if Equation {11} holds for some i\u03b5{1, 2, . . . v}.",{"@attributes":{"id":"p-0085","num":"0084"},"maths":{"@attributes":{"id":"MATH-US-00017","num":"00017"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mfrac":{"mrow":[{"mo":["\uf603","\uf604"],"mrow":{"mrow":[{"msubsup":{"mi":["r","i","M"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"msub":{"mi":["r","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["t","u"]}}}],"mo":"-"}},{"msub":{"mi":["r","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["t","u"]}}}]},"mo":">","mi":"\u03c6"}},{"mrow":{"mo":["{","}"],"mn":"11"}}]}}}},"br":{}},"In view of the foregoing, an exemplary adaptive multi-reservoir algorithm is as follows:",{"@attributes":{"id":"p-0087","num":"0086"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Algorithm 3: Adaptive Multi-Reservoir Sampling"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Inputs: \u03b6, \u03c6, M, T, {r(t), r(t), ..., r(t)}, { k(t), k(t), ..., k(t)},"]},{"entry":[{},"{\u03bb(t), \u03bb(t), ..., \u03bb(t)}"]},{"entry":[{},"1:\u2002 while true do"]},{"entry":[{},"2:\u2002 \u2003while there are no tuples arriving from any stream do"]},{"entry":[{},"3:\u2002 \u2003\u2003{do nothing.}"]},{"entry":[{},"4:\u2002 \u2003end while"]},{"entry":[{},"\u2002\u2009\u2002 \u2003{one or more tuples arrived from some streams}"]},{"entry":[{},"5:\u2002 \u2003compute r(t) (Equation {6}) for the streams from which tuples"]},{"entry":[{},"\u2002\u2009 \u2002 \u2003\u2002arrived."]},{"entry":[{},"6:\u2002 \u2003for each R\u03b5 { R, R, ..., R} do"]},{"entry":[{},"7:\u2002 \u2003\u2003compute r(t) (Equation {8})."]},{"entry":[{},"8:\u2002 \u2003\u2003compute \u03b4(t) = r(t) \u2212 r(t)."]},{"entry":[{},"9:\u2002 \u2003end for"]},{"entry":[{},"10: \u2003if Equation {11} holds for any R\u03b5 { R, R, ..., R} then"]},{"entry":[{},"11: \u2003\u2003L= set of all Rwhose \u03b4(t) < 0"]},{"entry":[{},"12: \u2003\u2003L= set of all Rwhose \u03b4(t) > 0"]},{"entry":[{},"13: \u2003\u2003compute m(t) (Equation {9}) for all R\u03b5L."]},{"entry":[{},"14: \u2003\u2003L\u2032= set of all R\u03b5Lwhose UC(k(t), r(t), \u03b4(t),"]},{"entry":[{},"\u2003\u2009 \u2003\u2003m(t)) \u2266 \u03b6"]},{"entry":[{},"15: \u2003\u2003if L\u2032is empty then"]},{"entry":[{},"16: \u2003\u2003\u2003for each R\u03b5 (L\u222a L) do"]},{"entry":[{},"17: \u2003\u2003\u2003\u2003if R\u03b5Lthen"]},{"entry":[{},"18: \u2003\u2003\u2003\u2003\u2003randomly evicts \u03b4(t) tuples from R."]},{"entry":[{},"19: \u2003\u2003\u2003\u2003else"]},{"entry":[{},"20: \u2003\u2003\u2003\u2003\u2003flip a biased coin to decide on the number of"]},{"entry":[{},"\u2003\u2009 \u2003\u2003\u2003\u2003\u2003tuples, x, to retain in R(using Equation {4} with"]},{"entry":[{},"\u2003\u2009 \u2003\u2003\u2003\u2003\u2003k(t), r(t), \u03b4(t), m(t) substituting k,r,\u03b4,m,"]},{"entry":[{},"\u2003\u2009 \u2003\u2003\u2003\u2003\u2003respectively)."]},{"entry":[{},"21: \u2003\u2003\u2003\u2003\u2003randomly evict r(t) \u2212 x tuples from R."]},{"entry":[{},"22: \u2003\u2003\u2003\u2003\u2003select r(t)+ \u03b4(t)\u2212x tuples from the incoming m(t)"]},{"entry":[{},"\u2003\u2009 \u2003\u2003\u2003\u2003\u2003tuples using Algorithm 1 (Background section,"]},{"entry":[{},"\u2003\u2009 \u2003\u2003\u2003\u2003\u2003above)."]},{"entry":[{},"23: \u2003\u2003\u2003\u2003end if"]},{"entry":[{},"24: \u2003\u2003\u2003\u2003r(t) = r(t)"]},{"entry":[{},"25: \u2003\u2003\u2003end for"]},{"entry":[{},"26: \u2003\u2003end if"]},{"entry":[{},"27: \u2003end if"]},{"entry":[{},"28: end while"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}}},"Based on the foregoing problem formulation, Algorithm 3 works as follows. As long as there are no tuples arriving from any stream, the algorithm stays idle (Lines 2-4). Upon the arrival of a new tuple from any stream, it computes r(t) for those streams from which tuples arrived (Line 5) and computes r(t) and \u03b4(t) for all streams (Lines 6-9). Then, it checks if the relative change in the size of any reservoir is larger than the memory adjustment threshold \u03c6 (using Equation {11}) (Line 10). If so, it computes m(t) for all of the enlarged reservoirs (Lines 12 and 13). Then, it checks if the uniformity confidence computed using Equation {10} exceeds the given threshold for every enlarged reservoir (Lines 14-15). If so, for each of the adjusted reservoirs, it applies the corresponding steps of the adaptive reservoir sampling algorithm (see Algorithm 2), and updates r(t) to the current computed reservoir size r(t) (Lines 16-25).","Adaptive Reservoir Sampling\u2014Experimental Evaluation","The purpose of this evaluation is to empirically examine the adaptivity of the exemplary multi-reservoir sampling algorithm (Algorithm 3, above) with regard to reservoir size and sample uniformity. Two sets of experiments were conducted. The objective of the first set of experiments was to observe how the reservoirs sizes change as data arrive. The objective of the second set of experiments was to observe the uniformity of the reservoir samples as the reservoirs sizes change.","Data Setup. We use a real data set collected from sensors deployed in a research lab over a period of about five weeks. Sensors mounted with weather boards collected timestamped topology information, along with humidity, temperature, light and voltage values once every 31 seconds. Collection of data was done using the TinyDB in-network query processing system, built on a TinyOS platform. (TinyDB and TinyOS are research components that evolved out of laboratories of Intel-Research Berkeley and University of California Berkeley.)","The resulting data file includes a log of about 2.3 million readings collected from these sensors. The schema of records was (date: yyyy-mm-dd, time: hh:mm:ss.xxx, epoch:int, moteid: int, temperature: real, humidity: real, light:real, voltage:real). In this schema, epoch was a monotonically increasing sequence number unique for each mote. Moteid ranged from 1 to 58. Data from three motes (of ID=5, ID=28, and ID=57) had incomplete readings, and thus were discarded. This left readings from 55 motes used in the experiments. ( reports the total number of readings from each mote.) Temperature is in degrees Celsius. Humidity is temperature-corrected relative humidity, ranging from 0 to 100%. Light is in Lux. Voltage is expressed in volts, ranging from 2.0 to 3.0 volts.","Algorithm setup. In Algorithm 3, the uniformity confidence threshold \u03b6 was set to 0.90. It is believed that this value is adequately large to constrain the frequency of adjusting the memory allocation. To check the effect of the total available memory size on the frequency of change in reservoir sizes, we ranged the value of M from 1000 (tuples) to 5000 (tuples) and range the memory adjustment threshold \u03c6 from 0.1 to 0.5. Readings acquired for the whole first day of the experiment were used in the experiments. Data collection was done every 1 hour and, accordingly, report results on the change in reservoir size and sample uniformity every hour.","Change in reservoir size.  shows the changes in the sizes of the 55 reservoirs. For better visibility,  shows the changes for 5 selected reservoirs. In the beginning (i.e., by the end of the 2nd hour), the total available memory is enough to store all reading from all motes and, therefore, the reservoir sizes increase linearly. Then, the reservoir sizes started fluctuating. The fluctuations were smooth and small in the first stage (from the 2nd to the 4th hour), larger in the second stage (from the 4th to the 21st hour), and eventually diminished in the last stage (after the 21st hour). This pattern of changes is attributed to the characteristics of data sets used in the experiments. In the first stage, there was no tangible difference between the numbers of readings acquired by different motes. Therefore, reservoir sizes stayed almost constant. In the second stage, the differences started increasing and, therefore, the changes in reservoir sizes became more frequent and more tangible. The saturations in reservoir sizes in the last stage indicate that the number of readings acquired by each mote were large enough and, therefore, did not cause a change in the computed reservoir size (see Equation {6}).","With a larger value of the memory adjustment threshold \u03c6 (=0.5),  shows a similar pattern except that the changes in reservoir sizes happened less frequently, and saturated earlier. The reason for these observations can be easily seen from Equations {6} and {11}. Results obtained for varying other parameters (M and \u03b6) show similar patterns, and are omitted due to space constraint.","Sample Uniformity. We used \u03c72 statistics as a metric of the sample uniformity. Higher \u03c72 indicates lower uniformity and vice versa. For each value v in a domain D, \u03c72 statistics measures the relative difference between the observed number of tuples (o(v)) and the expected number of tuples (e(v)) that contain the value v. That is:",{"@attributes":{"id":"p-0096","num":"0095"},"maths":{"@attributes":{"id":"MATH-US-00018","num":"00018"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"x","mn":"2"},"mo":"=","mrow":{"munder":{"mo":"\u2211","mrow":{"mo":"\u2200","mrow":{"mi":["v","D"],"mo":"\u2208"}}},"mo":"\u2062","mfrac":{"msup":{"mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"e","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"v"}},{"mi":"o","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"v"}}],"mo":"-"}},"mn":"2"},"mrow":{"mi":"e","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"v"}}}}}},{"mrow":{"mo":["{","}"],"mn":"12"}}]}}}},"br":{}},{"@attributes":{"id":"p-0097","num":"0096"},"figref":"FIG. 10"},"Join Sampling","As mentioned above, this disclosure also addresses the problem of reservoir sampling over memory-limited stream joins. Novel concepts directed to this problem and two algorithms for performing reservoir sampling on the join result are presented below. These algorithms are referred to herein as the \u201creservoir join-sampling\u201d (RJS) algorithm and the \u201cprogressive reservoir join-sampling\u201d (PRJS) algorithm. In the RJS algorithm, the reservoir size is fixed. As a result, the sample in the reservoir is always a uniform random sample of the join result. Therefore, RJS fits those applications that may use the sample in the reservoir at any time (e.g., continuous queries). This algorithm, however, may not accommodate a memory-limited situation in which the available memory may be too small even for storing tuples in the join buffer. In such a situation, it may be infeasible to allocate the already limited memory to a reservoir with an adequately large size.","The PRJS algorithm, on the other hand, is designed to alleviate this problem by increasing the reservoir size during the sampling process. For this, the conventional reservoir sampling technique of RJS is replaced with what is referred to herein as \u201cprogressive reservoir sampling.\u201d (As will be seen below, progressive reservoir sampling is the case of adaptive reservoir sampling (see Algorithm 2, above) in which the sampling reservoir size is increased during sampling.) A key idea of PRJS is to exploit the property of reservoir sampling that the sampling probability keeps decreasing for each subsequent tuple. Based on this property, the memory required by the join buffer keeps decreasing during the join-sampling. Therefore, PRJS releases the join buffer memory not needed anymore and allocates it to the reservoir.","Evidently, a larger reservoir sample represents the original join result more closely. It, however, comes at a cost in terms of the uniformity of the sample. Once the reservoir size is increased, the sample's uniformity is damaged. Besides, even after the enlarged reservoir is filled again with new tuples, the sample's uniformity is still not guaranteed, i.e., the sample's uniformity confidence stays below 100%. (See Equation {3},  and accompanying description.) There is thus a tradeoff that a larger increase of reservoir size leads to lower uniformity confidence after the reservoir is filled again. Therefore, PRJS is suitable for those applications that can be tolerant in terms of the uniformity of the sample. Specifically, it fits those applications that use the sample at a predetermined time (such as applications of data collection over wireless sensor networks, such as wireless sensor network  of ). Given such a tradeoff, PRJS is designed so that it determines how much the reservoir can be increased given a sample-use time and a uniformity confidence threshold.","The present inventors have performed extensive experiments to evaluate the RJS and PRJS algorithms with respect to the two competing factors (size and uniformity of sample). The inventors have also compared the two algorithms in terms of the aggregation error resulting from applying AVG on the join result. The experimental results confirm understanding of the tradeoffs. The RJS and PRJS algorithms, as well as a description of the experiments, are presented and described below.","Prior to describing the algorithms and experiments, conventional reservoir sampling and progressive reservoir sampling are briefly reviewed and uniform join-sampling will be discussed, as these are used in formulating the RJS and PRJS algorithms presented below. The conventional reservoir sampling algorithm is presented in the Background section above as Algorithm 1. Initially, the algorithm places all tuples in the reservoir until the reservoir (of size r tuples) becomes full. After that, each ktuple is sampled with the probability r\/k. A sampled tuple replaces a randomly selected tuple in the reservoir. This way, the reservoir always holds a uniform random sample of all the tuples seen from the beginning.","As described above relative to Equation {3} and  in connection with adaptive reservoir sampling, when the size of the sample reservoir is increased (i.e., the reservoir size is \u201cprogressively\u201d increased), the uniformity confidence UC (Equation {3}) will be less than 100%, increases monotonously and saturates as the uniformity confidence recovery tuple count m increases. As mentioned above, progressive reservoir sampling is one case of adaptive reservoir sampling (Algorithm 2, above) wherein the size of the reservoir is only increased. A progressive reservoir sampling algorithm is as follows:",{"@attributes":{"id":"p-0104","num":"0103"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Algorithm 4: Progressive Reservoir Sampling"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Inputs:","r {reservoir size}"]},{"entry":[{},"k {number of tuples seen so far}"]},{"entry":[{},"\u03b6 {uniformity confidence threshold}"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"1: while true do"},{"entry":"2: \u2003while reservoir size does not increase do"},{"entry":"3: \u2003\u2003conventional reservoir sampling (Algorithm 1, Background"},{"entry":"\u2002\u2009 \u2003\u2003Section, above)."},{"entry":"4: \u2003end while"},{"entry":"5: \u2003Find the minimum value of m (using Equation {3} with the current"},{"entry":"\u2002\u2009 \u2003values of k, r, \u03b4) that causes the UC to exceed \u03b6."},{"entry":"6: \u2003flip a biased coin to decide on the number, x, of tuples to retain"},{"entry":"\u2002\u2009 \u2003among r tuples already in the reservoir (Equation {4})."},{"entry":"7: \u2003randomly evict r \u2212 x tuples from the reservoir."},{"entry":"8: \u2003select r + \u03b4 \u2212 x tuples from the incoming m tuples using conventional"},{"entry":"\u2002\u2009 \u2003reservoir sampling (Algorithm 1, Background section, above)."},{"entry":"9: end while"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"Based on the above discussion, the progressive reservoir sampling works as shown in Algorithm 4. As long as the size of the reservoir does not increase, it uses the conventional reservoir sampling to sample the input stream (Line 3). Once the reservoir size increases by \u03b4, the algorithm computes the minimum value of m (using Equation {3}) that causes the UC to exceed a given threshold (\u03b6) (Line 5). Then, the algorithm flips a biased coin to decide on the number of tuples (x) to retain among the r tuples already in the reservoir (Line 6). The probability of choosing the value x is defined in Equation {4}, above.","After that, the algorithm randomly evicts r\u2212x tuples from the reservoir (Line 7) and refills the remaining reservoir space with r+\u03b4\u2212x tuples from the arriving m tuples using the conventional reservoir sampling (Line 8). Eventually, the algorithm continues sampling the input stream using the conventional reservoir sampling (Line 3) as if the sample in the enlarged reservoir were a uniform random sample of the k+m tuples.",{"@attributes":{"id":"p-0107","num":"0106"},"figref":"FIG. 11","b":"1100","sub":["1 ","2","1 ","2","1","S",{"sub2":"1"},"\u00b7A=S",{"sub2":"2\u00b7A"},"2","i ","i ","i","i","i","i ","i","i ","1","2 ","1 ","2","1 ","1","2 ","2","1","2"],"img":{"@attributes":{"id":"CUSTOM-CHARACTER-00001","he":"3.13mm","wi":"4.91mm","file":"US08392381-20130305-P00001.TIF","alt":"custom character","img-content":"character","img-format":"tif"}}},"Every join-result tuple may be classified as either an S-probe join tuple or an S-probe join tuple. When a new tuple sarrives on Sand is joined with a tuple s\u03b5W, sis said to produce an S-probe join tuple. An S-probe join tuple is defined symmetrically. A tuple s\u03b5Smay first produce S-probe join tuples when it arrives. Then, before it expires from W, it may produce S-probe join tuples with tuples newly arriving on S. n(s) is a function which returns the number of S-probe join tuples produced by a tuple s\u03b5Sbefore it expires from W. n(s) is defined symmetrically.","Tuples arrive in a data stream in a monotonically increasing order of the timestamp. In other words, there is no out of order arrival. The available memory M is limited, and insufficient for the join buffer to hold all tuples of the current sliding windows. It is assumed the initial reservoir size, r, is given. Under this join-sampling processing model, the present inventors have observed that as time passes memory requirement on the join buffer can be lowered and memory from the join buffer can be transferred to the reservoir. This makes the results of progressive reservoir sampling applicable to this processing model.","As will be seen in the following sections, each of the new RJS and PRJS algorithms may be considered to have two phases: 1) a join sampling phase and 2) a reservoir sampling phase. The sampling probabilities used in the first phase are denoted as pand the sampling probability used in the second phase are denoted as p. In the specific RJS and PRJS algorithms presented below, the join sampling phase utilizes a particular uniform join-sampling algorithm known as the \u201cUNIFORM algorithm.\u201d The UNIFORM algorithm (Algorithm 5) appears immediately below.",{"@attributes":{"id":"p-0111","num":"0110"},"tables":{"@attributes":{"id":"TABLE-US-00006","num":"00006"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Algorithm 5: Uniform Join-Sampling (UNIFORM)"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"182pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"1:\u2002 for each sin Wwhere s.A = s.A do"]},{"entry":[{},"2:\u2002 \u2003s.num = s.num + 1"]},{"entry":[{},"3:\u2002 \u2003if s.num = s.next then"]},{"entry":[{},"4:\u2002 \u2003\u2003output s|| s"]},{"entry":[{},"5:\u2002 \u2003\u2003decide on the next sto join with s"]},{"entry":[{},"6:\u2002 \u2003end if"]},{"entry":[{},"7:\u2002 end for"]},{"entry":[{},"8:\u2002 pick X ~ G | (p) {geometric distribution}"]},{"entry":[{},"9:\u2002 s.next = s.num + X"]},{"entry":[{},"10: if s.next > n(s) then"]},{"entry":[{},"11: \u2003discard s"]},{"entry":[{},"12: end if"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}}},"The UNIFORM algorithm streams out a uniform random sample of the result of a sliding-window join query in a memory limited stream environment. Algorithm 5 outlines the steps of the algorithm for one-way join from Sto S. (Join in the opposite, from Sto S, is symmetric.) The basic assumption of the algorithm is that n(s) (i=1, 2) (i.e., the number of S-probe join tuples produced by a tuple s\u03b5Sbefore it expires from W(see Table 2, below)) is known. The algorithm works with two prediction models that provide n(s): 1) a frequency-based model and 2) an age-based model. The frequency-based model assumes that, given a domain D of the join attribute A, for each value v\u03b5D a fixed fraction f(v) of the tuples arriving on Sand a fixed fraction f(v) of the tuples arriving on Shave value v of the attribute A. The age-based model assumes that for a tuple s\u03b5Sthe S-probe join tuples produced by ssatisfies the conditions that 1) the number of S-probe join tuples produced by sis a constant independent of sand 2) out of the n(s) S-probe join tuples of s, a certain number of tuples is produced when sis between the age g\u22121 and g. These definitions are symmetric for a tuple s\u03b5S. The choice of a predictive model is not important to the novelty of concepts disclosed herein; thus, without loss of generality, the frequency-based model is used in the rest of this disclosure.","For the frequency-based model, n(s)=\u03bb\u00d7W\u00d7f(S\u00b7A), the join sampling probability pis computed by first obtaining the expected memory usage (i.e., the expected number of tuples retained in the join buffer) in terms of pand, then, equate this to the amount of memory available for performing the join and solving it for p. The expected memory usage of Wthus obtained as:",{"@attributes":{"id":"p-0114","num":"0113"},"maths":{"@attributes":{"id":"MATH-US-00019","num":"00019"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":[{"mi":"\u03bb","mn":"1"},{"mi":"W","mn":"1"}],"mo":["\u2062","\u2062"],"mrow":{"munder":{"mo":"\u2211","mrow":{"mi":["v","D"],"mo":"\u2208"}},"mo":"\u2062","mrow":{"mrow":[{"msub":{"mi":"f","mn":"1"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"v"}},{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","mfrac":{"mrow":[{"mrow":[{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","msub":{"mi":"p","mn":"1"}}},{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","msup":{"mrow":[{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","msub":{"mi":"p","mn":"1"}}},{"msub":[{"mi":"\u03bb","mn":"2"},{"mi":"W","mn":"1"}],"mo":["\u2062","\u2062"],"mrow":{"msub":{"mi":"f","mn":"2"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"v"}}}]}}}],"mo":"\u2062"},{"msub":[{"mi":"p","mn":"1"},{"mi":"\u03bb","mn":"2"},{"mi":"W","mn":"1"}],"mo":["\u2062","\u2062","\u2062"],"mrow":{"msub":{"mi":"f","mn":"2"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"v"}}}]}}}],"mo":"\u2062"}}}},{"mrow":{"mo":["{","}"],"mn":"13"}}]}}}},"br":{},"sub":["2","1 ","2"]},{"@attributes":{"id":"p-0115","num":"0114"},"maths":{"@attributes":{"id":"MATH-US-00020","num":"00020"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":[{"mi":"\u03bb","mn":"2"},{"mi":"W","mn":"2"}],"mo":["\u2062","\u2062"],"mrow":{"munder":{"mo":"\u2211","mrow":{"mi":["v","D"],"mo":"\u2208"}},"mo":"\u2062","mrow":{"mrow":[{"msub":{"mi":"f","mn":"2"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"v"}},{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","mfrac":{"mrow":[{"mrow":[{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","msub":{"mi":"p","mn":"1"}}},{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","msup":{"mrow":[{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","msub":{"mi":"p","mn":"1"}}},{"msub":[{"mi":"\u03bb","mn":"1"},{"mi":"W","mn":"2"}],"mo":["\u2062","\u2062"],"mrow":{"msub":{"mi":"f","mn":"1"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"v"}}}]}}}],"mo":"\u2062"},{"msub":[{"mi":"p","mn":"1"},{"mi":"\u03bb","mn":"1"},{"mi":"W","mn":"2"}],"mo":["\u2062","\u2062","\u2062"],"mrow":{"msub":{"mi":"f","mn":"1"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"v"}}}]}}}],"mo":"\u2062"}}}},{"mrow":{"mo":["{","}"],"mn":"14"}}]}}}},"br":{},"sub":["1","2"],"img":{"@attributes":{"id":"CUSTOM-CHARACTER-00002","he":"3.13mm","wi":"4.91mm","file":"US08392381-20130305-P00001.TIF","alt":"custom character","img-content":"character","img-format":"tif"}}},"Given p, the algorithm proceeds as shown in Algorithm 5. When a tuple sarrives on S, the UNIFORM algorithm looks for every s\u03b5Wsuch that s\u00b7A=s\u00b7A (Line 1). Then, it outputs s\u2225sif this sis the tuple sis waiting for the output of the next sample tuple (Line 4), and then decides on the next sfor s(Line 5). Moreover, once sarrives on S, the UNIFORM algorithm flips a coin with bias pto decide the next S-probe join tuple of s(Lines 8-9). To do that, the UNIFORM algorithm picks X at random from the geometric distribution with parameter p, G(p). If all remaining S-probe join tuples of sare rejected in the coin flips, sis discarded (Lines 10-12).","Notations used in this section appear in the following Table 2.",{"@attributes":{"id":"p-0118","num":"0117"},"tables":{"@attributes":{"id":"TABLE-US-00007","num":"00007"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"168pt","align":"left"}}],"thead":{"row":[{"entry":[{},"TABLE 2"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]},{"entry":[{},"Symbol","Description"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"S","Data stream i (i = 1,2)"]},{"entry":[{},"\u03bb","Rate of stream S"]},{"entry":[{},"s","Tuple arriving in stream S"]},{"entry":[{},"W","Sliding window on stream S"]},{"entry":[{},"A","Join attribute (common to Sand S)"]},{"entry":[{},"S-probe","Join tuple produced by s\u2208 W"]},{"entry":[{},"n(s)","Number of S-probe join tuples produced by a"]},{"entry":[{},{},"tuple s\u2208 Sbefore it expires from W"]},{"entry":[{},"S","Sample in a reservoir"]},{"entry":[{},"r","Initial reservoir size"]},{"entry":[{},"\u03b4","Increment of a reservoir size"]},{"entry":[{},"k","Number of tuples seen so far in an input stream"]},{"entry":[{},"l","Number of tuples that would be generated without"]},{"entry":[{},{},"join-sampling by the time the reservoir sample will"]},{"entry":[{},{},"be used (or collected)"]},{"entry":[{},"RC","Reservoir refill confidence"]},{"entry":[{},"\u03be","Reservoir refill confidence threshold"]},{"entry":[{},"UC","Uniformity confidence in a reservoir sample"]},{"entry":[{},"\u03b6","Uniformity confidence threshold"]},{"entry":[{},"m","Uniformity confidence recovery tuple count, i.e.,"]},{"entry":[{},{},"number of tuples to be seen in an input stream of"]},{"entry":[{},{},"the progressive reservoir sampling until UC for the"]},{"entry":[{},{},"enlarged reservoir reaches \u03b6"]},{"entry":[{},"x","Number of tuples to be selected from k after"]},{"entry":[{},{},"increasing the reservoir size"]},{"entry":[{},"y","Number of tuples to be selected from m after"]},{"entry":[{},{},"increasing the reservoir size"]},{"entry":[{},"p","Join sampling probability in the first phase of the"]},{"entry":[{},{},"algorithms RJS and PRJS"]},{"entry":[{},"p","Reservoir sampling probability in the second"]},{"entry":[{},{},"phase of the algorithms RJS and PRJS"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}},"br":{}},"Based on the foregoing, immediately following is a specific example of an RJS algorithm (Algorithm 6).",{"@attributes":{"id":"p-0120","num":"0119"},"tables":{"@attributes":{"id":"TABLE-US-00008","num":"00008"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Algorithm 6: Reservoir Join-Sampling (RJS)"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"1: k = 0"]},{"entry":[{},"2: for each tuple output by UNIFORM do"]},{"entry":[{},"3: \u2003if k \u2266 r then"]},{"entry":[{},"4: \u2003\u2003add the tuple to the reservoir"]},{"entry":[{},"5: \u2003else"]},{"entry":[{},"6: \u2003\u2003sample the tuple with the probability p= (r\/(k + 1))\/p"]},{"entry":[{},"7: \u2003end if"]},{"entry":[{},"8: \u2003k = k + (1\/p)"]},{"entry":[{},"9: end for"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}}},"Algorithm 6 applies reservoir sampling on the output of the UNIFORM algorithm. Thus, it uses a fixed size reservoir and always holds a uniform random sample in the reservoir. Algorithm 6 outlines the steps of RJS. Given a fixed reservoir of size r, the first r join-sample tuples produced by the UNIFORM algorithm are directly placed in the reservoir (Lines 3-4). After that, each join-sample tuple is re-sampled using reservoir sampling with a probability pso that a p\u00d7p=r\/(k+1), that is, p=(r\/(k+1)\/p(Line 6).","k is an index of the original join output tuples that would be generated from the join. Since join-sampling selects only a portion of them, the value of k should be estimated. This estimation may be done as follows. When a tuple sproduces an S-probe join tuple, 1\/ptuples would be generated on average from the exact join since the algorithm samples a join result tuple with probability p. Therefore, k=k+(1\/p) (Line 8). This estimation process is symmetric for S-probe join tuples.","Join Sampling\u2014Progressive Reservoir Join Sampling","Also based on the foregoing, immediately following is a specific example of a PRJS algorithm (Algorithm 7).",{"@attributes":{"id":"p-0124","num":"0123"},"tables":{"@attributes":{"id":"TABLE-US-00009","num":"00009"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Algorithm 7: Progressive Reservoir Join-Sampling (PRJS)"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"1:\u2002 k = 0"},{"entry":"\u2003\u2009 {Initially, the memory utilization of the join buffer is 100%.}"},{"entry":"2:\u2002 while the memory utilization of the join buffer does not decrease do"},{"entry":"3:\u2002 \u2003for each tuple output by the UNIFORM algorithm do"},{"entry":"4:\u2002 \u2003\u2003if k \u2266 r then"},{"entry":"5:\u2002 \u2003\u2003\u2003add the tuple to the reservoir"},{"entry":"6:\u2002 \u2003\u2003else"},{"entry":"7:\u2002 \u2003\u2003\u2003sample the tuple with a probability p= (r\/(k + 1))\/p1"},{"entry":"8:\u2002 \u2003\u2003end if"},{"entry":"9:\u2002 \u2003\u2003k = k + (1\/p)"},{"entry":"10: \u2003\u2003set p= r\/(k + 1) {for the next incoming tuple}"},{"entry":"11: \u2003\u2003re-compute the memory utilization of the join buffer using"},{"entry":"\u2003\u2009 \u2003\u2003Equations {13} and {14}"},{"entry":"12: \u2003end for"},{"entry":"13: end while"},{"entry":"14: while (RC(m) \u2267 \u03be)"},{"entry":"\u2003\u2009 \u2003and (UC(Sr+\u03b4) \u2267 \u03b6)"},{"entry":"\u2003\u2009 \u2003and (m \u2267 (x + y) \u2212 (p(k + 1))) do"},{"entry":"15: \u2003decrease pby a specified constant value"},{"entry":"16: \u2003re-compute the memory utilization of the join buffer using"},{"entry":"\u2003\u2009 \u2003Equations {13} and {14}"},{"entry":"17: \u2003increase \u03b4 by the amount of unused memory"},{"entry":"18: end while"},{"entry":"19: while (RC(m) < \u03be)"},{"entry":"\u2003\u2009 \u2003or (UC(Sr+\u03b4) < \u03b6)"},{"entry":"\u2003\u2009 \u2003or (m < (x + y) \u2212 (p(k + 1))) do"},{"entry":"20: \u2003\u03b4 = \u03b4 \u2212 1"},{"entry":"21: \u2003if \u03b4 = 0 then"},{"entry":"22: \u2003\u2003return"},{"entry":"23: \u2003end if"},{"entry":"24: end while"},{"entry":"25: release \u03b4 memory units from the join buffer and allocate the released"},{"entry":"\u2003\u2009 memory to the reservoir."},{"entry":"26: flip a biased coin to decide on x and y (Equation {4})"},{"entry":"27: randomly evict r \u2212 x sample tuples from the reservoir"},{"entry":"28: get y sample tuples out of m using Algorithm 1 (Background section,"},{"entry":"\u2003\u2009 above)"},{"entry":"29: continue sampling the input stream using Algorithm 1 (Background"},{"entry":"\u2003\u2009 section, above)"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"A key idea behind PRJS and Algorithm 7 is to utilize the property of reservoir sampling that the sampling probability keeps decreasing for each subsequent tuple (see Algorithm 1, Background section, above). This property allows the algorithm to release memory from the join buffer and transfer it to the reservoir. However, as mentioned above the benefit of increasing a reservoir size comes at a cost on the uniformity of the sample. PRJS needs to know the values of m (uniformity confidence recovery tuple count) and \u03b6 (uniformity confidence threshold). Given the time left until the sample-use (or collection) time (denoted as T), the number of tuples (denoted as l) that would be generated during T if there were no join sampling is computed as follows:",{"@attributes":{"id":"p-0126","num":"0125"},"maths":{"@attributes":{"id":"MATH-US-00021","num":"00021"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"l","mo":"=","mrow":{"mi":"T","mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":"\u03bb","mn":"1"},"mrow":[{"msub":{"mi":"\u03bb","mn":"2"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"W","mn":"1"},{"mi":"W","mn":"2"}],"mo":"+"}}},{"munder":{"mo":"\u2211","mrow":{"mi":["v","D"],"mo":"\u2208"}},"mo":"\u2062","mrow":{"mrow":[{"msub":{"mi":"f","mn":"1"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"v"}},{"msub":{"mi":"f","mn":"2"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"v"}}],"mo":"\u2062"}}]}}},{"mrow":{"mo":["{","}"],"mn":"15"}}]}}}},"br":[{},{},{}],"sub":["1","1","i ","i","1 ","1 ","i","i ","i","i ","1","1 "],"in-line-formulae":[{},{}],"i":"m=lp","ul":{"@attributes":{"id":"ul0001","list-style":"none"},"li":{"@attributes":{"id":"ul0001-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0002","list-style":"none"},"li":["Refill confidence: The refill confidence, RC, is defined as the probability that m is at least the same as the enlarged reservoir size. That is given r and \u03b4:\n\nRC()=probability(+\u03b4)\u2003\u2003{17}\n","\u2003Unlike progressive reservoir sampling (see Algorithm 4), PRJS cannot guarantee that the enlarged reservoir will be filled out of m tuples since m is only an expected number of tuples on the outcome of the join-sampling phase (see Equation {16}). That is, the value of m is an expected value rather than an exact value. This means that actual value of m may be less than r+\u03b4, and this implies that \u03b4\u2266y\u2266min(m, r+\u03b4). (y is the number of tuples to be selected from the m tuples). Therefore, the algorithm has to make sure that y falls in that range with a confidence no less than a given threshold \u03be.","Uniformity confidence: UC\u2267\u03b6. (See Equation {3}) That is, the uniformity confidence should be no less than \u03b6 after the enlarged reservoir is filled.","Uniformity-recovery tuple count: m\u2267(x+y)\u2212(p(k+1)). The rationale for this constraint is as follows. PRJS assumes the reservoir sample (of x+y tuples) will be used (or collected) after it will have seen m tuples. But if the sample-use does not happen, then it will have to continue with the conventional reservoir sampling on the join-sample tuples as if the sample in the reservoir were a uniform random sample of all join result tuples seen so far. In this case, (x+y)\/((k+(m\/p))+1)\u2266p. Hence, m\u2267(x+y)\u2212(p(k+1)).\n\nIf all these three constraints are satisfied, then in the second step PRJS keeps decreasing pand increasing \u03b4 until one or more of them are not satisfied anymore. The more pis decreased, the larger \u03b4 can be. Therefore, PRJS finds the smallest possible pthat makes the three constraints satisfied. This ensures to find the largest possible memory (\u03b4) to be transferred to the reservoir.\n\nWhen PRJS enters the third step, \u03b4 has been set too large to satisfy one or more of the three constraint. So, PRJS decreases \u03b4 until the constraints are satisfied or \u03b4 becomes 0. The latter case means that the reservoir size cannot be increased. Once \u03b4 (>0) is determined, in the fourth step (Line 25-29) PRJS releases \u03b4 memory units from the join buffer and allocates the released memory to the reservoir. Then, PRJS works in the same way as in the progressive reservoir sampling (see Lines 6-8 of Algorithm 4) to refill the reservoir.\n\nJoin Sampling\u2014Experimental Examples\n"]}}}},"As mentioned several times above, there is a tradeoff between the presented RJS and PRJS algorithms, i.e., Algorithms 6 and 7, respectively. Thus, in one experiment the aim was to compare these two algorithms in terms of the two traded factors: 1) the achieved reservoir sample size and 2) the achieved (recovered) uniformity of the sample. In addition, another set of experiments was performed to put the evaluations in the database context. Specifically, an aggregation (AVG) was performed on the reservoir sample, and comparison made on the aggregation errors between the two algorithms.","The experimental results confirm the following:\n\n","Algorithm setup: Both window sizes (Wand W) were set to 500 time units, and the two stream rates (\u03bband \u03bb) were set to 1 tuple per time unit and 5 tuples per time unit, respectively. Memory allocated to join buffer was 50% of the memory required for an exact result. The initial size of reservoir was 100 (i.e., r=100 tuples) which represented 6% of the total available memory. Both the uniformity confidence threshold \u03b6 and the refill confidence threshold \u03be were set to 0.90. It is believed this value is sufficiently large to constrain the increase of reservoir size in PRJS. Unless stated otherwise, the results reported were obtained as an average of the results of 50 runs.","Data streams setup: Stream data sets each containing tuples amounting to 10000 time units were generated. Values of join attribute in the input stream tuples were generated assuming the frequency-based model as indicated above. The values were drawn from a normal distribution with mean \u03bc=1000 and variance \u03c3=1000. Values of aggregate attribute were drawn from a normal distribution with mean \u03bc=1000 and variance \u03c3=10000.","Reservoir Sample Size","An objective of this experiment was to observe how the size of a sample in the reservoir changes size over time.  shows the average sample size over time, at the interval of 10 time units, for both PRJS and RJS. For PRJS, the sample size increased linearly until the enlarged reservoir was filled, and then the increase saturated. The same happened for RJS, but sample size did not ever exceed the initial reservoir size.",{"@attributes":{"id":"p-0132","num":"0138"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0133","num":"0139"},"figref":"FIG. 14"},"Reservoir Sample Uniformity","The purpose of this set of experiments was to test the uniformity of the sample in the reservoir. The chi-squared (\u03c72} statistic was used as a metric of the sample uniformity. Higher \u03c72 indicates lower uniformity and vice versa. The \u03c72 statistic measures, for each value v in a domain D, the relative difference between the observed number of tuples (o(v)) and the expected number of tuples (e(v)) that contain the value. That is:",{"@attributes":{"id":"p-0135","num":"0141"},"maths":{"@attributes":{"id":"MATH-US-00022","num":"00022"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"x","mn":"2"},"mo":"=","mrow":{"munder":{"mo":"\u2211","mrow":{"mo":"\u2200","mrow":{"mi":["v","D"],"mo":"\u2208"}}},"mo":"\u2062","mfrac":{"msup":{"mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"e","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"v"}},{"mi":"o","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"v"}}],"mo":"-"}},"mn":"2"},"mrow":{"mi":"e","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"v"}}}}}},{"mrow":{"mo":["{","}"],"mn":"18"}}]}}}}},{"@attributes":{"id":"p-0136","num":"0142"},"figref":"FIG. 15"},"Since PRJS evicts some tuples from the reservoir in order to refill the reservoir with the incoming tuples, the uniformity can be damaged more if there is some sort of dependence in the arrival of join attribute values on the input streams. Therefore, an experiment was conducted to test the effect of the ordering of tuples in the input streams by the join attribute. For this, partially sorted streams were generated. This was done by configuring the values in the domain of the attribute into a tree structure. In the tree, the value in a parent node had a precedence in appearing in the input stream over the values in the children nodes. Between siblings there was no precedence conditions. The number of children of each node was fixed and was parameterized as fanout. As the value of fanout decreased, the stream became more sorted. That is, when fanout=1, the stream was totally sorted. The value of fanout was set to 2, 3, and 4 as shown in .  shows that, for PRJS, there was more damage on the uniformity when the degree of the input stream ordering was higher. On the other hand, RJS is not sensitive for any kind of ordering in the input stream. This is evident for RJS and, thus, the graph is omitted.","Aggregation on the Reservoir Samples","In this set of experiments, RJS and PRJS were compared in terms of the accuracy of aggregation (AVG) query results. The average absolute error (AE) at the interval of 500 time units for each algorithm is reported. Absolute error is defined as follows:",{"@attributes":{"id":"p-0139","num":"0145"},"maths":{"@attributes":{"id":"MATH-US-00023","num":"00023"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"AE","mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":"\u2062","mfrac":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"msub":[{"mover":{"mi":"A","mo":"^"},"mi":"i"},{"mi":["A","i"]}],"mo":"-"}},"mi":"n"}}}},{"mrow":{"mo":["{","}"],"mn":"19"}}]}}}},"br":{},"sub":["i ","i "]},"The results shown in  demonstrate that right after the reservoir size increased, PRJS gave a larger aggregation error but, after that, as the sample size increased the aggregation errors decreased. The curve of PRJS crosses over the curve of RJS even before reaching the sample-use time (marked as a circle on the PRJS curve). This happens because the benefit of the enlarged reservoir size dominates over the damage in the uniformity. As the uniformity recovers more, the aggregation error decreases more.","It is noted that the algorithms, aspects, and embodiments described herein, for example, any one or more of Algorithms 2-7, above, may be conveniently implemented using one or more machines (e.g., general-purpose computing devices, devices incorporating application-specific integrated circuits, devices incorporating systems-on-chip, etc.) programmed according to the teachings of the present specification, as will be apparent to those of ordinary skill in the computer arts. Appropriate software coding can readily be prepared by skilled programmers based on the teachings of the present disclosure, as will be apparent to those of ordinary skill in the software art.","Such software may be a computer program product that employs one or more machine-readable media and\/or one or more machine-readable signals. A machine-readable medium may be any medium that is capable of storing and\/or encoding a sequence of instructions for execution by a machine (e.g., a general purpose computing device) and that causes the machine to perform any one of the methodologies and\/or embodiments described herein. Examples of a machine-readable medium in the form of a non-volatile machine-readable medium include, but are not limited to, a magnetic disk (e.g., a conventional floppy disk, a hard drive disk), an optical disk (e.g., a compact disk \u201cCD\u201d, such as a readable, writeable, and\/or re-writable CD; a digital video disk \u201cDVD\u201d, such as a readable, writeable, and\/or rewritable DVD), a magneto-optical disk, a read-only memory \u201cROM\u201d device, a random access memory \u201cRAM\u201d device, a magnetic card, an optical card, a solid-state memory device (e.g., a flash memory), an EPROM, an EEPROM, and any combination thereof. A machine-readable medium, as used herein, is intended to include a single medium as well as a collection of physically separate media, such as, for example, a collection of compact disks or one or more hard disk drives in combination with a computer memory. As those skilled in the art will readily appreciate, the term \u201cnon-volatile\u201d as used above and in the amended claims excludes encoded signals that propagate via electromagnetic energy, pressure energy, or other form of energy.","Examples of a computing device include, but are not limited to, a computer workstation, a terminal computer, a server computer, a handheld device (e.g., tablet computer, a personal digital assistant \u201cPDA\u201d, a mobile telephone, etc.), a web appliance, a network router, a network switch, a network bridge, a computerized device, such as a wireless sensor or dedicated proxy device, any machine capable of executing a sequence of instructions that specify an action to be taken by that machine, and any combination thereof.",{"@attributes":{"id":"p-0144","num":"0150"},"figref":"FIG. 18","b":["1800","1800","1804","1808","1812","1812"]},"Memory  may include various components including, but not limited to, a random access read\/write memory component (e.g, a static RAM (SRAM), a dynamic RAM (DRAM), etc.), a read only component, and any combination thereof. In one example, a basic input\/output system  (BIOS), including basic routines that help to transfer information between elements within computer system , such as during start-up, may be stored in memory . Memory  may also include (e.g., stored on one or more machine-readable media) instructions (e.g., software)  embodying any one or more of the aspects and\/or methodologies of the present disclosure. In another example, memory  may further include any number of instruction sets including, but not limited to, an operating system, one or more application programs, other program modules, program data, and any combination thereof.","Computer system  may also include one or more storage devices . Examples of storage devices suitable for use as any one of the storage devices  include, but are not limited to, a hard disk drive device that reads from and\/or writes to a hard disk, a magnetic disk drive device that reads from and\/or writes to a removable magnetic disk, an optical disk drive device that reads from and\/or writes to an optical media (e.g., a CD, a DVD, etc.), a solid-state memory device, and any combination thereof. Each storage device  may be connected to bus  by an appropriate interface (not shown). Example interfaces include, but are not limited to, Small Computer Systems Interface (SCSI), advanced technology attachment (ATA), serial ATA, universal serial bus (USB), IEEE 13144 (FIREWIRE), and any combination thereof. In one example, storage device  may be removably interfaced with computer system  (e.g., via an external port connector (not shown)). Particularly, storage device  and an associated machine-readable medium  may provide nonvolatile and\/or volatile storage of machine-readable instructions, data structures, program modules, and\/or other data and\/or data storage for computer system . In one example, software  may reside, completely or partially, within machine-readable medium . In another example, software  may reside, completely or partially, within processor .","In some embodiments, such as a general purpose computer, computer system  may also include one or more input devices . In one example, a user of computer system  may enter commands and\/or other information into the computer system via one or more of the input devices . Examples of input devices that can be used as any one of input devices  include, but are not limited to, an alpha-numeric input device (e.g., a keyboard), a pointing device, a joystick, an audio input device (e.g., a microphone, a voice response system, etc.), a cursor control device (e.g., a mouse), a touchpad, an optical scanner, a video capture device (e.g., a still camera, a video camera), touchscreen, a digitizer pad, and any combination thereof. Each input device  may be interfaced to bus  via any of a variety of interfaces (not shown) including, but not limited to, a serial interface, a parallel interface, a game port, a Universal Serial Bus (USB) interface, a FIREWIRE interface, a direct interface to the bus, a wireless interface (e.g., a Bluetooth\u00ae connection) and any combination thereof.","Commands and\/or other information may be input to computer system  via storage device  (e.g., a removable disk drive, a flash drive, etc.) and\/or one or more network interface devices . A network interface device, such as network interface device , may be utilized for connecting computer system  to one or more of a variety of networks, such as network , and one or more remote devices  connected thereto. Examples of a network interface device include, but are not limited to, a network interface card, a modem, a wireless transceiver (e.g., a Bluetooth\u00ae transceiver) and any combination thereof. Examples of a network include, but are not limited to, a wide area network (e.g., the Internet, an enterprise network), a local area network (e.g., a network associated with an office, a building, a campus, a group of wireless sensors or other group of data streaming devices, or other relatively small geographic space), a telephone network, a direct connection between two computing devices, and any combination thereof. A network, such as network , may employ a wired and\/or a wireless mode of communication. In general, any network topology may be used. Information (e.g., data, software , etc.) may be communicated to and\/or from computer system  via the one or more network interface devices .","In some embodiments, such as a general purpose computer, computer system  may further include a video display adapter  for communicating a displayable image to a display device, such as display device . Examples of a display device include, but are not limited to, a liquid crystal display (LCD), a cathode ray tube (CRT), a plasma display, and any combination thereof. In addition to a display device, a computer system  may include one or more other peripheral output devices including, but not limited to, an audio speaker, a printer, and any combination thereof. Such peripheral output devices may be connected to bus  via a peripheral interface . Examples of a peripheral interface include, but are not limited to, a serial port, a USB connection, a FIREWIRE connection, a parallel connection, and any combination thereof.","A digitizer (not shown) and an accompanying pen\/stylus, if needed, may be included in order to digitally capture freehand input. A pen digitizer may be separately configured or coextensive with a display area of display device . Accordingly, a digitizer may be integrated with display device , or may exist as a separate device overlaying or otherwise appended to the display device.","Exemplary embodiments have been disclosed above and illustrated in the accompanying drawings. It will be understood by those skilled in the art that various changes, omissions and additions may be made to that which is specifically disclosed herein without departing from the spirit and scope of the present invention."],"GOVINT":[{},{}],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["For the purpose of illustrating the invention, the drawings show aspects of one or more embodiments of the invention. However, it should be understood that the present invention is not limited to the precise arrangements and instrumentalities shown in the drawings, wherein:",{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":["FIG. 5","FIG. 4"]},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":["FIG. 7","FIG. 6"]},{"@attributes":{"id":"p-0030","num":"0029"},"figref":["FIG. 8","FIG. 6"],"b":["2","15","31","49","54"]},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 9","b":["2","15","31","49","54"]},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 10","b":"49"},{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 18"}]},"DETDESC":[{},{}]}
