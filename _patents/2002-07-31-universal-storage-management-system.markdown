---
title: Universal storage management system
abstract: A universal storage management system which facilitates storage of data from a client computer and computer network is disclosed. The universal storage management system functions as an interface between the client computer and at least one storage device, and facilitates reading and writing of data by handling I/O operations. I/O operation overhead in the client computer is reduced by translating I/O commands from the client computer into high level commands which are employed by the storage management system to carry out I/O operations. The storage management system also enables interconnection of a normally incompatible storage device and client computer by translating I/O requests into an intermediate common format which is employed to generate commands which are compatible with the storage device receiving the request. Files, error messages and other information from the storage device are similarly translated and provided to the client computer.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=RE042860&OS=RE042860&RS=RE042860
owner: 
number: RE042860
owner_city: 
owner_country: 
publication_date: 20020731
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["PRIORITY","FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION"],"p":[{"@attributes":{"id":"p-0002","num":"0001"},"insert-start":{},"insert-end":{}},"The present invention is generally related to data storage systems, and more particularly to cross-platform data storage systems and RAID systems.","One problem facing the computer industry is lack of standardization in file subsystems. This problem is exacerbated by I\/O addressing limitations in existing operating systems and the growing number of non-standard storage devices. A computer and software application can sometimes be modified to communicate with normally incompatible storage devices. However, in most cases such communication can only be achieved in a manner which adversely affects I\/O throughput, and thus compromises performance. As a result, many computers in use today are \u201cI\/O bound.\u201d More particularly, the processing capability of the computer is faster than the I\/O response of the computer, and performance is thereby limited. A solution to the standardization problem would thus be of interest to both the computer industry and computer users.","In theory it would be possible to standardize operating systems, file subsystems, communications and other systems to resolve the problem. However, such a solution is hardly feasible for reasons of practicality. Computer users often exhibit strong allegiance to particular operating systems and architectures for reasons having to do with what the individual user requires from the computer and what the user is accustomed to working with. Further, those who design operating systems and associated computer and network architectures show little propensity toward cooperation and standardization with competitors. As a result, performance and ease of use suffer.","Disclosed is a universal storage management system which facilitates storage of data from a client computer. The storage management system functions as an interface between the client computer and at least one storage device and facilitates reading and writing of data by handling I\/O operations. More particularly, I\/O operation overhead in the client computer is reduced by translating I\/O commands from the client computer to high level I\/O commands which are employed by the storage management system to carry out I\/O operations. The storage management system also enables interconnection of a normally incompatible storage device and client computer by translating I\/O requests into an intermediate common format which is employed to generate commands which are compatible with the storage device receiving the request. Files, error messages and other information from the storage device are similarly translated and provided to the client computer.","The universal storage management system provides improved performance since client computers attached thereto are not burdened with directly controlling I\/O operations. Software applications in the client computers generate I\/O commands which are translated into high level commands which are sent by each client computer to the storage system, The storage management system controls I\/O operations for each client computer based on the high level commands. Overall network throughput is improved since the client computers are relieved of the burden of processing slow I\/O requests.","The universal storage management system can provide a variety of storage options which are normally unavailable to the client computer. The storage management system is preferably capable of controlling multiple types of storage devices such as disk drives, tape drives, CD-ROMS, magneto optical drives etc., and making those storage devices available to all of the client computers connected to the storage management system. Further, the storage management system can determine which particular storage media any given unit of data should be stored upon or retrieved from. Each client computer connected to the storage system thus gains data storage options because operating system limitations and restrictions on storage capacity are removed along with limitations associated with support of separate storage media. For example, the universal storage management system can read information from a CD-ROM and then pass that information on to a particular client computer, even though the operating system of that particular client computer has no support for or direct connection to the CD-ROM.","By providing a common interface between a plurality of client computers and a plurality of shared storage devices, network updating overhead is reduced. More particularly, the storage management system allows addition of drives to a computer network without reconfiguration of the individual client computers in the network. The storage management system thus saves installation time and removes limitations associated with various network operating systems to which the storage management system may be connected.","The universal storage management system reduces wasteful duplicative storage of data. Since the storage management system interfaces incompatible client computers and storage devices, the storage management system can share files across multiple heterogeneous platforms. Such file sharing can be employed to reduce the overall amount of data stored in a network. For example, a single copy of a given database can be shared by several incompatible computers, where multiple database copies were previously required. Thus, in addition to reducing total storage media requirements, data maintenance is facilitated.","The universal storage management system also provides improved protection of data. The storage management system isolates regular backups from user intervention, thereby addressing problems associated with forgetful or recalcitrant employees who fail to execute backups regularly.","Referring to , the universal storage management system includes electronic hardware and software which together provide a cross platform interface between at least one client computer  in a client network  and at least one storage device . The universal storage management system is implemented in a host computer  and can include a host board , a four channel board , a five channel board  for controlling the storage devices . It should be noted, however, that the software could be implemented on standard hardware. The system is optimized to handle I\/O requests from the client computer and provide universal storage support with any of a variety of client computers and storage devices. I\/O commands from the client computer are translated into high level commands, which in turn are employed to control the storage devices.","Referring to , a,  & a, the software portion of the universal storage management system includes a file management system  and a storage management architecture (\u201cSMA\u201d) kernel . The file management system manages the conversion and movement of files between the client computer  and the SMA Kernel . The SMA kernel manages the flow of data and commands between the client computer, device level applications and actual physical devices.","The file management system includes four modules: a file device driver , a transport driver a, b, a file system supervisor , and a device handler . The file device driver provides an interface between the client operating system  and the transport driver. More particularly, the file device driver resides in the client computer and redirects files to the transport driver. Interfacing functions performed by the file device driver include receiving data and commands from the client operating system, converting the data and commands to a universal storage management system file format, and adding record options, such as lock, read-only and script.","The transport driver a, b facilitates transfer of files and other information between the file device driver  and the file system supervisor . The transport driver is specifically configured for the link between the client computers and the storage management system. Some possible links include: SCSI-2, SCSI-3, fiber link, 802.3, 802.5, synchronous and a synchronous RS232, wireless RF, and wireless IR. The transport driver includes two components: a first component a which resides in the client computer and a second component b which resides in the storage management system computer. The first component receives data and commands from the file device driver. The second component relays data and commands to the file system supervisor. Files, data, commands and error messages can be relayed from the file system supervisor to the client computer operating system through the transport driver and file device driver.","The file system supervisor  operates to determine appropriate file-level applications for receipt of the files received from the client computer . The file system supervisor implements file specific routines on a common format file system. Calls made to the file system supervisor are high level, such as Open, Close, Read, Write, Lock, and Copy. The file system supervisor also determines where files should be stored, including determining on what type of storage media the files should be stored. The file system supervisor also breaks each file down into blocks and then passes those blocks to the device handler. Similarly, the file system supervisor can receive data from the device handler.","The device handler  provides an interface between the file system supervisor  and the SMA kernel  to provide storage device selection for each operation. A plurality of device handlers are employed to accommodate a plurality of storage devices. More particularly, each device handler is a driver which is used by the file system supervisor to control a particular storage device, and allow the file system supervisor to select the type of storage device to be used for a specific operation. The device handlers reside between the file system supervisor and the SMA kernel and the storage devices. The device handler thus isolates the file system supervisor from the storage devices such that the file system supervisor configuration is not dependent upon the configuration of the specific storage devices employed in the system.","The SMA Kernel  includes three independent modules: a front end interface , a scheduler , and a back-end interface . The front end interface is in communication with the client network and the scheduler. The scheduler is in communication with the back-end interface, device level applications, redundant array of independent disks (\u201cRAID\u201d) applications and the file management system. The back-end interface is in communication with various storage devices.","The front-end interface  handles communication between the client network  and resource scheduler , running on a storage management system based host controller which is connected to the client network and interfaced to the resource scheduler. A plurality of scripts are loaded at start up for on-demand execution of communication tasks. More particularly, if the client computer and storage management system both utilize the same operating system, the SMA kernel can be utilized to execute I\/O commands from software applications in the client computer without first translating the I\/O commands to high level commands as is done in the file management system.","The resource scheduler  supervises the flow of data through the universal storage management system. More particularly, the resource scheduler determines whether individual data units can be passed directly to the back-end interface  or whether the data unit must first be processed by one of the device level applications  or RAID applications . Block level data units are passed to the resource scheduler from either the front-end interface or the file management system.","The back-end interface  manages the storage devices . The storage devices are connected to the back-end interface by one or more SCSI type controllers through which the storage devices are connected to the storage management system computer. In order to control non-standard SCSI devices, the back-end interface includes pre-loaded scripts and may also include device specific drivers.",{"@attributes":{"id":"p-0043","num":"0042"},"figref":["FIG. 2a","FIG. 2"],"b":["14","46","14"]},"The storage management system employs high level commands to access the storage devices. The high level commands include array commands and volume commands, as follows:\n\n","The acreate command creates a new array by associating a group of storage devices in the same rank and assigning them a RAID level.","Syntax:\n\n",{"@attributes":{"id":"p-0047","num":"0049"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"154pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"rank_id","Id of rank on which the array will be created."]},{"entry":[{},"level","RAID level to use for the array being created"]},{"entry":[{},"aname","Unique name to be given to array. if NULL, one"]},{"entry":[{},{},"will be assigned by the system."]},{"entry":[{},"ch_use","bitmap indicating which channels to use in this set"]},{"entry":[{},{},"of drives."]},{"entry":[{},"Return","0"]},{"entry":[{},"ERANK","Given rank does not exist or it is not available"]},{"entry":[{},{},"to create more arrays."]},{"entry":[{},"ELEVEL","Illegal RAID level"]},{"entry":[{},"ECHANNEL","No drives exist in given bitmap or drives are"]},{"entry":[{},{},"already in use by another array."]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}},"ul":{"@attributes":{"id":"ul0005","list-style":"none"},"li":{"@attributes":{"id":"ul0005-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0006","list-style":"none"},"li":"\u201caremove\u201d"}}}},"The aremove command removes the definition of a given array name and makes the associated storage devices available for the creation of other arrays.\n\n","The vopen command creates and\/or opens a volume, and brings the specified volume on-line and readies that volume for reading and\/or writing.\n\n",{"@attributes":{"id":"p-0050","num":"0060"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"154pt","align":"left"}}],"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["arrayname","Name of the army on which to create\/open the"]},{"entry":[{},"volume."]},{"entry":["volname","Name of an existing volume or the name to be given"]},{"entry":[{},"to the volume to create. If left NULL, and the"]},{"entry":[{},"O_CREAT flag is given, one will be assigned by the"]},{"entry":[{},"system and this argument will contain the new"]},{"entry":[{},"name."]},{"entry":["vh","When creating a volume, this contains a pointer to"]},{"entry":[{},"parameters to be used in the creation of requested"]},{"entry":[{},"volume name. If opening an existing volume, these"]},{"entry":[{},"parameters will be returned by the system."]},{"entry":["flags","A constant with one or more of the following"]},{"entry":[{},"values."]},{"entry":["O_CREAT","The system will attempt create the volume using the"]},{"entry":[{},"parameters give in vh. If the volume already"]},{"entry":[{},"exists, this flag will be ignored."]},{"entry":["O_DENYRD","Denies reading privileges to any other tasks on"]},{"entry":[{},"this volume anytime after this call is made."]},{"entry":["O_DENYWR","Deny writing privileges to any other tasks that"]},{"entry":[{},"open this volume anytime after this call is made."]},{"entry":["O_EXCLUSIVE","Deny any access to this volume anytime after"]},{"entry":[{},"this call is made."]},{"entry":["Return 0","Successful open\/creation of volume"]},{"entry":["EARRAY","Given array does not exist"]},{"entry":["EFULL","Given array is full"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}},"ul":{"@attributes":{"id":"ul0012","list-style":"none"},"li":{"@attributes":{"id":"ul0012-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0013","list-style":"none"},"li":"\u201cvclose\u201d"}}}},"The vclose command closes a volume, brings the specified volume off-line, and removes all access restrictions imposed on the volume by the task that opened it.\n\n",{"@attributes":{"id":"p-0052","num":"0065"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"175pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"vh","Volume handle, returned by the system when the volume"]},{"entry":[{},{},"was opened\/created"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}},"ul":{"@attributes":{"id":"ul0017","list-style":"none"},"li":{"@attributes":{"id":"ul0017-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0018","list-style":"none"},"li":"\u201cvread\u201d"}}}},"The vread command reads a specified number of blocks into a given buffer from an open volume given by \u201cvh\u201d.\n\n",{"@attributes":{"id":"p-0054","num":"0070"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"175pt","align":"left"}}],"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["vh","Handle of the volume to read from"]},{"entry":["bufptr","Pointer to the address in memory where the data is to be read "]},{"entry":[{},"into"]},{"entry":["Iba","Logical block address to read from"]},{"entry":["count","Number of blocks to read from given volume"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"161pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Return","0","Successful read"]},{"entry":["EACCESS",{},"Insufficient rights to read from this"]},{"entry":[{},{},"volume"]},{"entry":["EHANDLE",{},"Invalid volume handle"]},{"entry":["EADDR",{},"Illegal logical block address"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}]}},"ul":{"@attributes":{"id":"ul0022","list-style":"none"},"li":{"@attributes":{"id":"ul0022-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0023","list-style":"none"},"li":"\u201cvwrite\u201d"}}}},"The vwrite command writes a specified number of blocks from the given buffer to an open volume given by \u201cvh.\u201d\n\n",{"@attributes":{"id":"p-0056","num":"0075"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"175pt","align":"left"}}],"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["vh","Handle of the volume to write to"]},{"entry":["bufptr","Pointer to the address in memory where the data to be written "]},{"entry":[{},"to the device resides"]},{"entry":["Iba","Volume Logical block address to write to"]},{"entry":["count","Number of blocks to write to given volume"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"161pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Return","0","Successful read"]},{"entry":["EACCESS",{},"Insufficient rights to write to this volume"]},{"entry":["EHANDLE",{},"Invalid volume handle"]},{"entry":["EADDR",{},"Illegal logical block address"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}]}},"ul":{"@attributes":{"id":"ul0027","list-style":"none"},"li":{"@attributes":{"id":"ul0027-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0028","list-style":"none"},"li":"\u201cvolcpy\u201d"}}}},"The volcpy command copies \u201ccount\u201d number of blocks from the location given by src_addr in src_vol to the logical block address given by dest_addr in dest_vol. Significantly, the command is executed without interaction with the client computer.\n\n",{"@attributes":{"id":"p-0058","num":"0080"},"tables":{"@attributes":{"id":"TABLE-US-00006","num":"00006"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"175pt","align":"left"}}],"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["dest_vol","handle of the volume to be written to"]},{"entry":["dest_Iba","destination logical block address"]},{"entry":["src_vol","handle of the volume to be read from"]},{"entry":["src_Iba","Source logical block address"]},{"entry":["count","Number of blocks to write to given volume"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"140pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":["Return","0","Successful read"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"175pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["EACCW","Insufficient rights to write to this destination volume"]},{"entry":["EACCR","Insufficient rights to read from source volume"]},{"entry":["EDESTH","Invalid destination volume handle"]},{"entry":["ESRCH","Invalid source volume handle"]},{"entry":["EDESTA","Illegal logical block address for destination volume"]},{"entry":["ESRCA","Illegal logical block address for source volume"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}]}}},"The modular design of the storage management system software provides some advantages. The SMA Kernel and file management system are independent program groups which do not have interdependency limitations. However, both program groups share a common application programming interface (API). Further, each internal software module (transport driver, file system supervisor, device handler, front-end interface, back-end interface and scheduler) interacts through a common protocol. Development of new modules or changes to an existing module thus do not require changes to other SMA modules, provided compliance with the protocol is maintained. Additionally, software applications in the client computer are isolated from the storage devices and their associated limitations. As such, the complexity of application development and integration is reduced, and reduced complexity allows faster development cycles. The architecture also offers high maintainability, which translates into simpler testing and quality assurance processes and the ability to implement projects in parallel results in a faster time to market.",{"@attributes":{"id":"p-0060","num":"0082"},"figref":"FIGS. 3 & 4","b":"10"},"The universal storage management system utilizes a standard file format which is selected based upon the cross platform client network for ease of file management system implementation. The file format may be based on UNIX, Microsoft-NT or other file formats. In order to facilitate operation and enhance performance, the storage management system may utilize the same file format and operating system utilized by the majority of client computers connected thereto, however this is not required. Regardless of the file format selected, the file management system includes at least one file device driver, at least one transport driver, a file system supervisor and a device handler to translate I\/O commands from the client computer.","Referring to ,  and b, the storage management system is preferably capable of simultaneously servicing multiple client computer I\/O requests at a performance level which is equal to or better than that of individual local drives. In order to provide prompt execution of I\/O operations for a group of client computers the universal storage management system computer employs a powerful microprocessor or multiple microprocessors  capable of handling associated overhead for the file system supervisor, device handler, and I\/O cache. Available memory  is relatively large in order to accommodate the multi-tasking storage management system operating system running multiple device utilities such as backups and juke box handlers. A significant architectural advance of the RAID is the use of multiple SCSI processors with dedicated memory pools . Each processor  can READ or WRITE devices totally in parallel. This provides the RAID implementation with true parallel architecture. Front end memory  could also be used as a first level of I\/O caching for the different client I\/O's. A double 32 bit wide dedicated I\/O bus  is employed for I\/O operations between the storage management system and the storage device modules . The I\/O bus is capable of transmission at 200 MB\/sec, and independent 32 bit wide caches are dedicated to each I\/O interface.","Referring to ,  and , a redundant power supply array is employed to maintain power to the storage devices when a power supply fails. The distributed redundant low voltage power supply array includes a global power supply  and a plurality of local power supplies  interconnected with power cables throughout a disk array chassis. Each local power supply provides sufficient power for a rack  of storage devices . In the event of a failure of a local power supply , the global power supply  provides power to the storage devices associated with the failed local power supply. In order to provide sufficient power, the global power supply therefore should have a power capacity rating at least equal to the largest capacity local power supply.","Preferably both horizontal and vertical power sharing are employed. In horizontal power sharing the power supplies  for each rack of storage devices includes one redundant power supply  which is utilized when a local power supply  in the associated rack fails. In vertical power sharing a redundant power supply  is shared between a plurality of racks  of local storage devices .","Referring now to , a redundant array of independent disks (\u201cRAID\u201d) is provided as a storage option. For implementation of the RAID, the storage management system has multiple SCSI-2 and SCSI-3 channels having from 2 to 11 independent channels capable of handling up to 1080 storage devices. The RAID reduces the write overhead penalty of known RAIDS which require execution of Read-modify-Write commands from the data and parity drives when a write is issued to the RAID. The parity calculation procedure is an XOR operation between old parity data and the old logical data. The resulting data is then XORed with the new logical data. The XOR operations are done by dedicated XOR hardware  in an XOR router  to provide faster write cycles. This hardware is dedicated for RAID-4 or RAID-5 implementations. Further, for RAID-3 implementation, parity generation and data striping have been implemented by hardware . As such, there is no time overhead cost for this parity calculation which is done \u201con the fly,\u201d and the RAID-3 implementation is as fast as a RAID-0 implementation.","Referring now to , at least one surface  of each of the drives is dedicated for parity. As such, a RAID-3 may be implemented in every individual disk of the array with the data from all other drives (See  specifically). The parity information may be sent to any other parity drive surface (See  specifically). In essence, RAID-3 is implemented within each drive of the array, and the generated parity is transmitted to the appointed parity drive for RAID-4 implementation, or striped across all of the drives for RAID-5 implementation. The result is a combination of RAID-3 and RAID-4 or RAID-5, but without the write overhead penalties. Alternatively, if there is no internal control over disk drives, as shown in , using standard double ported disk drives, the assigned parity drive  has a dedicated controller board  associated therewith for accessing other drives in the RAID via the dedicated bus , to calculate the new parity data without the intervention of the storage management system computer microprocessor.","Referring to , b and , the storage management system optimizes disk mirroring for RAID-1 implementation. Standard RAID-1 implementations execute duplicate WRITE commands for each of two drives simultaneously. To obtain improved performance the present RAID divides a logical disk , such as a logical disk containing a master disk  and a mirror disk , into two halves , . This is possible because the majority of the operations in a standard system are Read operations, and since the information is contained in both drives. The respective drive heads ,  of the master and mirror disks are then positioned at a halfway point in the first half  and second half , respectively. If the Read request goes to the first half  of the logical drive , then this command is serviced by the master disk . If the Read goes to the second half  of the logical drive , then it is serviced by the mirror disk . Since each drive head only travels one half of the total possible distance, average seek time is reduced by a factor of two. Additionally, the number of storage devices required for mirroring can be reduced by compressing  mirrored data and thereby decreasing the requisite number of mirror disks. By compressing the mirrored data \u201con the fly\u201d overall performance is maintained.","File storage routines may be implemented to automatically select the type of media upon which to store data. Decision criteria for determining which type of media to store a file into can be determined from a data file with predetermined attributes. Thus, the file device driver can direct data to particular media in an intelligent manner. To further automate data storage, the storage management system includes routines for automatically selecting an appropriate RAID level for storage of each file. When the storage management system is used in conjunction with a computer network it is envisioned that a plurality of RAID storage options of different RAID levels will be provided. In order to provide efficient and reliable storage, software routines are employed to automatically select the appropriate RAID level for storage of each file based on file size. For example, in a system with RAID levels 3 and 5, large files might be assigned to RAID-3, while small files would be assigned to RAID-5. Alternatively, the RAID level may be determined based on block size, as predefined by the user.","Referring now to , the RAID disks  are arrayed in a protective chassis . The chassis includes the global and local power supplies, and includes an automatic disk eject feature which facilitates identification and replacement of failed disks. Each disk  is disposed in a disk shuttle  which partially ejects from the chassis in response to a solenoid . A system controller  controls securing and releasing of the disk drive mounting shuttle  by actuating the solenoid . When the storage system detects a failed disk in the array, or when a user requests release of a disk, the system controller actuates the solenoid associated with the location of that disk and releases the disk for ejection.","An automatic storage device ejection method is illustrated in . In an initial step  a logical drive to physical drive conversion is made to isolate and identify the physical drive being worked upon. Then, if a drive failure is detected in step , the drive is powered down . If a drive failure is not detected, the cache is flushed  and new commands are disallowed  prior to powering the drive down . After powering down the drive, a delay  is imposed to wait for drive spin-down and the storage device ejection solenoid is energized  and the drive failure indicator is turned off .","Referring to , an automatic configuration routine can be executed by the backplane with the dedicated microprocessor thereon for facilitating configuring and replacement of failed storage devices. The backplane microprocessor allows control over power supplied to individual storage devices  within the pool of storage devices. Such individual control allows automated updating of the storage device IDs. When a storage device fails, it is typically removed and a replacement storage device is inserted in place of the failed storage device. The drive will be automatically set to the ID of the failed drive, as this information is saved in SRAM on the backplane when the automatic configuration routine was executed at system initialization (). When initializing the system for the first time, any device could be in conflict with another storage device in the storage device pool, the system will not be able to properly address the storage devices. Therefore, when a new system is initialized the automatic configuration routine is executed, to assure that the device Ids are not in conflict. As part of the automatic ID configuration routine all devices are reset , storage device identifying variables are set , and each of the storage devices  in the pool is powered down . Each individual storage device is then powered up  to determine if that device has the proper device ID . If the storage device has the proper ID, then the device is powered down and the next storage device is tested. If the device does not have the proper ID, then the device ID is reset  and the storage device is powercycled. The pseudocode for the automatic ID configuration routine includes the following steps:",{"@attributes":{"id":"p-0072","num":"0094"},"tables":{"@attributes":{"id":"TABLE-US-00007","num":"00007"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"196pt","align":"left"}}],"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["1.","Reset all disks in all channels"]},{"entry":["2.","Go through every channel in every cabinet:"]},{"entry":["3.","channel n = 0"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"182pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},"cabinet j = 0"]},{"entry":[{},{},"drive k = 0"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"196pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["4.","Remove power to all disks in channel n"]},{"entry":["5.","With first disk in channel n"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"182pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"a.","turn drive on via back plane"]},{"entry":[{},"b.","if its id conflicts with previously turned on drive, change its id "]},{"entry":[{},{},"via back plane then turn drive off"]},{"entry":[{},"c.","turn drive off"]},{"entry":[{},"d.","goto next drive until all drives in channel n have"]},{"entry":[{},{},"been checked."]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"196pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Use next channel until all channels in cabinet j have"]},{"entry":[{},"been checked."]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}]}}},"Automatic media selection is employed to facilitate defining volumes and arrays for use in the system. As a practical matter, it is preferable for a single volume or array to be made up of a single type of storage media. However, it is also preferable that the user not be required to memorize the location and type of each storage device in the pool, i.e., where each device is. The automatic media selection feature provides a record of each storage device in the pool, and when a volume or array is defined, the location of different types of storage devices are brought to the attention of the user. This and other features are preferably implemented with a graphic user interface (\u201cGUI\u201d)  () which is driven by the storage management system and displayed on a screen mounted in the chassis.","Further media selection routines may be employed to provide reduced data access time. Users generally prefer to employ storage media with a fast access time for storage of files which are being created or edited. For example, it is much faster to work from a hard disk than from a CD-ROM drive. However, fast access storage media is usually more costly than slow access storage media. In order to accommodate both cost and ease of use considerations, the storage management system can automatically relocate files within the system based upon the frequency at which each file is accessed. Files which are frequently accessed are relocated to and maintained on fast access storage media. Files which are less frequently accessed are relocated to and maintained on slower storage media.","The method executed by the microprocessor controlled backplane is illustrated in . In a series of initialization steps the backplane powers up , executes power up diagnostics , activates an AC control relay , reads the ID bitmap , sets the drive IDs , sequentially powers up the drives , reads the fan status  and then sets fan airflow  based upon the fan status. Temperature sensors located within the chassis are then polled  to determine  if the operating temperature is within a predetermined acceptable operating range. If not, airflow is increased  by resetting fan airflow. The backplane then reads  the 12V and 5V power supplies and averages  the readings to determine  whether power is within a predetermined operating range. If not, the alarm and indicators are activated . If the power reading is within the specified range, the AC power is read  to determine  whether AC power is available. If not, DC power is supplied  to the controller and IDE drives and an interrupt  is issued. If AC power exists, the state of the power off switch is determined  to detect  a power down condition. If power down is active, the cache is flushed  (to IDE for power failure and to SCSI for shutdown) and the unit is turned off . If power down is not active, application status is read  for any change in alarms and indicators. Light and audible alarms are employed  if required. Fan status is then rechecked . When no problem is detected this routine is executed in a loop, constantly monitoring events.","A READ cycle is illustrated in . In a first step  a cache entry is retrieved. If the entry is in the cache as determined in step , the data is sent  to the host and the cycle ends. If the entry is not in the cache, a partitioning address is calculated  and a determination  is made as to whether the data lies on the first half of the disk. If not, the source device is set  to be the master. If the data lies on the first half of the disk, mirror availability is determined . If no mirror is available, the source device is set  to be the master. If a mirror is available, the source device is set  to be the mirror. In either case, it is next determined  whether the entry is cacheable, i.e., whether the entry fits in the cache. If not, the destination is set  to be temporary memory. If the entry is cacheable, the destination is set  to be cache memory. A read is then performed  and, if successful as determined in step , the data is sent  to the host. If the read is not successful, the storage device is replaced  with the mirror and the read operation is retried  on the new drive. If the read retry is successful as determined in step , the data is sent  to the host. If the read is unsuccessful, the volume is taken off-line .","A WRITE cycle is illustrated in . In an initial step  an attempt is made to retrieve the entry from the cache. If the entry is in the cache as determined in step , the destination is set  to be the cache memory and the data is received  from the host. If the entry is not in the cache, a partitioning address is calculated , the destination is set  to cache memory, and the data is received  from the host. A determination  is then made as to whether write-back is enabled. If write back is not enabled, a write  is made to the disk. If write-back is enabled, send status is first set  to OK, and then a write  is made to the disk. A status check is then executed  and, if status is not OK, the user is notified  and a mirror availability check  is done. If no mirror is available, an ERROR message is produced . If a mirror is available, a write  is executed to the mirror disk and a further status check is executed . If the status check  is negative (not OK), the user is notified . If the status check  is positive, send status is set to OK . If status is OK in status check , send status is set to OK  and a mirror availability check is executed . If no mirror is available, flow ends. If a mirror is available, a mirror status check is executed , and the user is notified  if the result of the status check is negative.","Other modifications and alternative embodiments of the present invention will become apparent to those skilled in the art in light of the information provided herein. Consequently, the invention is not to be viewed as limited to the specific embodiments disclosed herein."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWING","p":["These and other features of the present invention will become apparent in light of the following detailed description thereof, in which:",{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 1a"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":["FIG. 2a","FIG. 2"]},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIGS. 3 and 4"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIGS. 8-11"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIGS. 12a-13"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIGS. 15 and 15a"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIGS. 18 & 19"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 20"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 21"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 22"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIGS. 23-25"},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIGS. 26-29"}]},"DETDESC":[{},{}]}
