---
title: Visual-symbolic control of remote devices having display-based user interfaces
abstract: Traditional, programmatic automated remote control of computerized devices requires extensive tailoring for each device type and operating system. A visual-symbolic control method enables largely device-agnostic control of any target device with access to display and a means of user input (keyboard, mouse, touchpad, touch-screen, etc). An image-processing daemon analyzes the displayed image and recognizes its component visual entities (windows, icons, buttons, etc.), creates symbolic entities from extracted attributes of the visual entities, and organizes the symbolic entities into a symbolic object model instance. The functional relationships and hierarchies of the visual entities are captured in the arrangement of symbolic entities in the symbolic object model instance. Visual-symbolic control commands act on the symbolic entities, and, where appropriate, the commands are transmitted to the target device as user-like target-input.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09274742&OS=09274742&RS=09274742
owner: PixiOnCloud, Inc.
number: 09274742
owner_city: Menlo Park
owner_country: US
publication_date: 20121025
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["RELATED APPLICATIONS","FEDERALLY SPONSORED RESEARCH AND DEVELOPMENT","APPENDICES","BACKGROUND","SUMMARY","DETAILED DESCRIPTION","DEFINITIONS","FURTHER DETAILS OF ALTERNATE EMBODIMENTS","Target-Device Multiplicity and Variety"],"p":["none","none","(1) Examples of symbolic entity types, (2) examples of commands, (3) sample script, and (4) part of symbolic object model were submitted in text form via EFS-Web and are incorporated herein by reference.","Related fields include electronic digital data processing and, in particular, control of a slave computer by a master computer, remote data access, and remote detection and correction of software faults.","Occasions to control one microprocessor-containing device from another are plentiful and increasing. Enterprise information technology (IT) groups in any industry can greatly increase their efficiency and reduce their companies' overhead if they can install, update, and troubleshoot software on servers without needing to physically visit each server. Technical support for field-installed software and hardware becomes more cost-effective when problems can be diagnosed and fixed remotely, computer-to-computer. Insufficient network connectivity is no longer a primary obstacle to efficient control of remote devices, because both public and private networks have become increasingly ubiquitous, especially in urban areas. Instead, the diversity of operating systems and device technology has become a dominant obstacle.","One approach to the device-diversity problem is to provide some mechanism to configure the remote devices. Such mechanisms have included standardized network protocols such as NETCONF\u00ae, software development kits for the various platforms such as Amazon's AWS SDK\u00ae, and graphical desktop-sharing systems such as VNC\u00ae that enable control of a computer with one OS by another computer with a different OS. Enterprise IT departments typically exercise central control over remote devices through platform-specific application programming interfaces (APIs) provided by the device vendors. Each new device platform, needing to be addressed through its own API, adds cost to the central management system. As new devices and platforms enter the market at an increasing rate, the problem becomes exacerbated. Because of the labor and risk involved in adding another platform that IT will need to support, many enterprises are reluctant to adopt new devices or programs that could potentially boost productivity, Instead, they strive to limit the diversity of hardware and software in use.","Software bugs cost the U.S. economy $60 billion per year, according to a 2003 study by the U.S. National Institute of Standards and Technology (NIST), despite software companies' typically allocating 50% of development costs to testing. The most common approach to software testing is to run a large number of focused test cases. These test cases could be executed faster and more consistently, and cover more of the program under test, if they were automated. Often, however, the automation itself takes a prohibitively long time; time to create tests that thoroughly cover the scope of operation, and time to maintain the tests as new versions of the software are created.","As diverse as the underlying programs for different device operating systems may be, the graphical user interfaces (GUIs) have been converging on at least some degree of similarity. Manufacturers have found that many customers are more willing to use a new device if the interface looks and feels at least somewhat familiar. Among computers, and increasingly among smaller devices their processors and screens have become able to support it, the \u201cdesktop\u201d GUI has gained wide acceptance. The device's display simulates the surface of a desk with various objects on it: folders, books, calendars, clocks, and graphic icons identifying programs, functions, or peripheral devices such as connected printers. By entering an input (typing, clicking touching, gesturing, speaking, etc.) directed at one of these objects, a \u201cfolder\u201d can be opened to show its contents; an appointment can be entered on the \u201ccalendar\u201d; a calculation can be done on the \u201ccalculator\u201d; a program can be started by selecting its icon, and so on. Among simpler devices such as microprocessor-enhanced household or public appliances, \u201ccontrol-board\u201d GUIs are still seen; the user \u201cpresses\u201d text-labeled or pictorial buttons to perform an action or expose a new selection of options. In some cases, simple text or numeric values can be entered. As the GUI interface becomes more ubiquitous, software manufactures take pains to ensure that their new devices support a GUI, and further that the GUI is not too dissimilar from established GUIs, so that users adopt the new devices easily.","This increasing ubiquity of the GUI as the device interface offers an opportunity for controlling remote devices without the costly traditional API programming approach by interacting with devices the way a human user would: analyzing the on-screen display and interacting with the visual entities shown. If the complexity and consequent slowness of analyzing and manipulating the image data could be overcome, automated operations such as control of remote devices and software testing could be carried out on multiple platforms with identical, or nearly identical, scripts. Therefore, a need exists to interact with different display-based user interfaces (graphic user interfaces, text \u201cconsole\u201d interfaces, etc.) in a unified way, largely agnostic to the operating system or other programmatic nuances of the target device being controlled.","Visual-symbolic control (VSC) software enables a controlling device to automatically and remotely control virtually any type of target device, as long as the target device has a readable display that changes in response to user input actions (e.g. a monitor screen) and some way to receive user input actions (e.g. a keyboard, mouse, touch-screen, touch-pad, or microphone for spoken command input). The software analyzes the readable display, renders its displayed components as data structures, performs actions on the data structures, and transforms the actions into user-like target input (e.g. keystrokes for target devices with keyboards, mouse-clicks for target devices with mice, etc.). Security measures on the target device designed to exclude unauthorized users, however, will exclude the software's \u201cuser-like\u201d scripted interactions to the same extent. For example, remote access to a password-protected file via visual-symbolic control still requires the correct password.","The data-intensive displayed image (e.g. desktop) is quickly dissected into its component visual entities (VEs); windows, icons, buttons, and the like. Compact, abstract symbolic entities (data structures) are generated from the visual entities. The symbolic entities (SEs) are organized into a symbolic object model instance that reflects the relationships, connections, and hierarchies of the corresponding VEs on the display. All the software's interactions with the target device manipulate this compact symbolic object model instance (SOM-I), rather than the much larger displayed image or compressions or sections thereof.","VSC is adaptable to a broad range of devices and tasks. Some embodiments are equipped to disambiguate the complex and sometimes confusing images that can appear on the displays of systems designed for multi-tasking. Some embodiments need no special hardware or software on the target device. Target devices can include tablets, smartphones, vehicle and kiosk displays, and display-equipped appliances and machines as well as desktop and laptop computers.","VSC facilitates efficient management of distributed systems. One controlling device can execute tasks on multiple target devices, in some cases simultaneously. The software can act on a target device's virtual and remote desktops in the same manner as on its primary desktop. The software can control a target device whose operating system is not running, as long as the bootloader, diagnostic, or other underlying utility responds to target-input and the response is expressed through the readable display. A new device introduced in the distributed system can be handled with simple edits to existing scripts, or sometimes with unaltered existing scripts, rather than rebuilding and re-testing an entire management layer.","VSC reduces the time and expense required for software testing. Scripts for the same tasks on different operating systems can be very similar or even identical, depending on the similarities between the user interfaces. Results of GUI actions can be verified via text-based actions on back-end databases.","The visual-symbolic control software includes an application programming interface (API) that enables a wide range of control tasks to be automated easily. Scripts can be written to \u201clearn\u201d a new target device with little or no prior knowledge of its operating system. A single script can be used to perform the same task on target devices with different operating systems, as long as the user inputs and displayed reactions to those inputs are sufficiently similar (e.g., double-clicking a program icon starts running the program on a variety of different computer operating systems, and the icon for a given program has the same appearance for any operating-system version). Alternatively, scripts can recognize a target device's OS from cues in the readable display, then access stored OS profiles to immediately choose the correct target-input (e.g., clicking the \u201cx\u201d button to close a window on a PC or the \u201cred light\u201d button to close a window on a Mac). These scripts can run faster and perform a wider variety of functions than visual processors that do not operate in a symbolic domain.","In some scripting embodiments, visual-symbolic control commands can be embedded in a variety of existing scripting languages, or written as a series of naturalistic text commands to be interpreted or compiled. The API is easy to adopt because its abstraction level is not based on esoteric concepts in the underlying code, understood only by programmers fluent in that particular code. Rather, the abstraction level is based on visual entities appearing in the displayed image.","Other ways to use the visual-symbolic control software, besides embodiments of the API, include graphical user interfaces, HTTP and other transfer protocols, NETCONF and other network-management protocols, ReST and other networked-application design architectures, infrared remote controls, motion-captured gestures, or voice commands. These interaction modes also benefit from ease of adoption because the visually-based abstraction is easier for many new users to grasp than many forms of programmatically-based abstraction.","This Description presents some definitions of terms as used in this document, provides an overview of some central concepts, describes some simple embodiments in more depth, provides a step-by-step example of a scripted process, and discusses examples of alternate and enhanced embodiments. The Appendix includes samples of symbolic-entity types, scripting commands, a script for the example process, and part of a symbolic object model.","As used herein,","\u201cControlling device\u201d means the computer or other device from which VSC action is initiated by a user. For scripted actions, the script may or may not have been composed on the controlling device, and may or may not be stored on the controlling device or run on the controlling device (i.e., a controlling device may access a development environment, editor, or runtime remotely over a network).","\u201cFramebuffer\u201d means a portion of target-device memory reserved for holding the complete displayed image. On many devices, a video output or similar port allows access to the content of the framebuffer.","\u201cHover-text\u201d means text that appears when a cursor hovers over a displayed visual entity such as an icon or button. Usually, hover-text includes information on what the visual entity is or does. Typically, hover-text disappears when the cursor moves away.","\u201cLabel text\u201d is text associated with a visual entity that is visible whenever the associated visual entity is visible (though it may be obscured by another visual entity \u201cin front\u201d of it).","\u201cMouse\u201d includes optical and mechanical computer mice, and any other input device or group of devices that can move a cursor around a display screen in a continuous path at an arbitrary angle and manipulate visual entities (select, move, deselect, re-size, etc.) Non-limiting examples include touch-pads, touch-screens, trackballs, joysticks, track-pads, and game gloves.","\u201cTarget adaptor\u201d means the functions of (1) capturing a displayed image from a target device for processing and (2) translating commands for \u201cuser-like\u201d target-input (keystrokes, clicks, touches, etc.) into signals understood by the target device.","\u201cTarget device\u201d means a device that is controlled by VSC. At a minimum, a target device needs a visible display of some aspect of system status (or something that could be captured as a visible display, e.g. a spatially varying tactile or thermal output) and a way for a user to interact with the device and change the displayed aspect (e.g. a keyboard, mouse, button(s), touch-screen, gesture scanner, speech analyzer, biofeedback sensor, or the like).","\u201cTarget-input\u201d means the type of signal that acts on the target device when a user interacts with it (e.g., the signals corresponding to keystrokes, mouse-clicks, button-pushes, etc.).","\u201cWidget\u201d means, generically, a feature displayed by the target device that can be affected directly or indirectly by target-input.","\u201cWindow\u201d means, generically, a widget that can be caused by particular target-input to \u201copen\u201d and reveal additional widgets or other contents, or \u201cclose\u201d and relinquish space on the target device's display. Some types of window may also be moved or resized, or their contents may be moved within the window, for example by scrolling. In particular, the meaning extends to widgets with this type of behavior on any operating system, including but not limited to Microsoft Windows\u00ae.","Overview and Introductory Embodiments:","VSC scripts can automate a wide variety of tasks. Script functionalities include, without limitation, opening application by name (e.g. \u201copen Firefox\u201d); moving a cursor to a particular visual entity (e.g., \u201cgo to first text-entry field of in-focus window\u201d or \u201cgo to \u2018My Account\u2019 menu); jumping to the \u201cnext\u201d or \u201cprevious\u201d text-entry field; selecting (e.g. left-click, right-click, double-click, key-shortcut) a particular visual entity; typing characters; selecting or clearing text where the text-selection \u201ccaret\u201d is located. Program, browser, or \u201cconsole\u201d (command-shell) windows can be opened, moved, maximized, or minimized, and have text typed into and read from them. Visual entities within a window can be searched for, scrolled through, selected, typed into, and otherwise manipulated. The scripts are resilient to variable response times on the target devices, which is a common challenge in automating tasks; the scripts can wait an allotted time for one or more visual entities to appear, signifying that the target device is ready for the next command.",{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 1","b":["1","2","2","2","3","4","4","8"]},"SOM  includes knowledge about the display domain, such as fonts and the appearances of common VEs  such as windows and taskbars. SOM  may include image libraries for recognizing, and instantly creating the correct SEs for, particular VEs such as application icons. SOM  may also include mapping structures relating the alternate appearances of polymorphous VEs, such as cursors that change shape depending on their proximity to other VEs such as resizing handles. This mapping structure enables the SOM-I to preserve an SE, albeit with changed attributes, when the corresponding polymorphous VE changes its appearance. This is a situation in which many other display-analysis algorithms often encounter problems: If a VE changes its appearance, the tendency is for the software to erroneously determine that the VE it was tracking has disappeared and a new one has appeared. The domain knowledge in the SOM may be organized by operating system (OS), by display type or device type, or any other logical taxonomy. Tagging or other labeling strategies may enable the SOM to be reorganized to suit a particular situation.","The communication path  between the SOM and SOM-I is shown as a double-headed arrow because, in many embodiments, the information flows both ways. Domain knowledge flows from the SOM to the SOM-I, populating the SOM-I with SEs corresponding to recognized VEs. If an unfamiliar VE is detected in the displayed image, some embodiments include training modes or training sequences for capturing the appearance and behavior of the new VE and cataloguing it as part of the domain knowledge of the SOM. New VEs and other domain knowledge may also be added to the SOM manually.","Each SE  in SOM-I  contains information about the attributes and relationships of the corresponding VE . However, where a VE  is typically a fairly large block of data (for instance, hundreds of pixels, each with a red, blue, and green level), an SE  is a far smaller and simpler data structure that can be processed much faster and stored in far less space. In some embodiments, an SE  may be expressed as a textual list of identifiers, attributes, and relationships. In embodiments of the runtime, queries  and commands  are addressed directly to SOM-I  and its component SEs . The structure of SOM-I  adds functionality that would not be possible with an unstructured collection of entities. For example, if an application icon appears on a desktop, and a picture of the same icon is visible in a browser or document window, the SOM-I \u201cknows\u201d which one will open the application when clicked (the one directly subordinate to the desktop).","Some of the commands  are translated into target-input , i.e. signals that the target device understands as user input such as keystrokes and mouse motions. This allows the VSC software, like a user, to explore and manipulate functions of the target device without needing to know any programmatic details of the underlying code. Because of the efficiency of accessing information, making decisions, and executing commands in the SE space, however, these processes can take place very quickly. In many cases, scripts for diagnostics and maintenance can run on alternate desktops of target devices that are in use, without disrupting human users' simultaneous use of the devices.",{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIG. 2","b":["201","211","211","202","1","1","204","203","204","2","1","3","3","4","8","8","4"]},"Periodically, or when triggered by a change in displayed image , IPD  sends SOM-I update information  to script interpreter . As a script or other sequence runs, script interpreter  sends queries  and commands  to IPD . If a command  calls for a target-input action, IPD  transmits target-input indicator  to target adaptor . Target adaptor  converts target-input indicator  to target device 's native target-input  and transmits target-input  to target device  such that target device  perceives native target-input  as a user input.","Script development environment  on controlling device  allows a user to develop new scripts. In adaptive embodiments, script interpreter  can interact with script development environment  to \u201clearn\u201d about the target device and refine scripts automatically. To operate VSC, a controlling device needs (a) a way for the programmer to write and edit scripts, (b) an interface for loading the scripts, (c) a mechanism for running the scripts (e.g. an interpreter or compiler and a runtime environment), and (d) a way to capture and log the results. Some preferred embodiments also include (e) a way to build higher-level functionality by combining scripts, and (f) in the context of an enterprise device-management system, integration points with other relevant management systems such as customer, inventory, order, and fault management systems. In some embodiments, a single controlling device may manage thousands of target devices, for instance by routing the data-intensive image-processing tasks to specialized hardware or multiple CPUs.","Distributed computing allows for many variations on where the various processes take place. Target adaptor  as shown in this conceptual diagram is an abstraction of the set of target-adaptor functions. These functions can be performed alternatively by hardware, software, or a combination of both, which can be located on the target device, the controlling device, or somewhere on a network connecting the two. Likewise, IPD  may be distributed in a number of ways between the controlling device and the target adaptor, target device, and intermediate resources on the network. Script development environment  may partially reside on a network connected to the controlling device (for instance, a mobile computer the programmer can use offsite) instead of on the controlling device itself. Distributed versions are discussed in more detail in a later section.",{"@attributes":{"id":"p-0053","num":"0052"},"figref":["FIGS. 3A-3E","FIG. 3A"],"b":["301","302","303","301","304","301"]},"In , the IPD has detected the edges in the displayed image of  by a suitable edge-detection algorithm, which may include binarizing the image. Each pixel (or other minimum unit of display-image area) is now identified as either an \u201cedge\u201d point or a \u201cnon-edge\u201d point. The IPD then organizes all the edge points into geometric borders. In this example, the geometric borders are straight lines, which are efficient for window and panel outlines such as side borders , , , and  of focus window . Curved lines such as the outer perimeter of circular icon  are fit with a series of straight lines . In other embodiments, the geometric borders fit to curved lines may be equations describing arcs of circles or ellipses, sinusoids, polynomials, or other suitable expressions.","In additional embodiments, the geometric borders are arbitrary contours formed from any chain of edge points in contact with each other, regardless of the shape of the chain. In further embodiments, icon  and other graphically complex widgets are selectively processed by a method including capture of its image; selective processing will be further discussed in a later section. In some embodiments, each VE type is extracted by an algorithm tailored to quickly and accurately recognize the particular type and extract its attributes. Many embodiments include a disambiguation process (discussed further in a later section) for situations where multiple VEs are juxtaposed such that their relationship is unclear.","Already, the extracted geometric borders are much more compact than even a compressed version of the displayed-image bitmap. In some embodiments, the IPD stores the geometric borders or their attributes in persistent memory for use in debugging or adaptive processing. A next step may be to recognize instances where the extracted geometric borders define geometric shapes.","In , some geometric borders from  are recognized as forming rectangular geometric shapes. This approach is efficient for rectangle-dominated displays such as window-based desktops, but other geometric shapes may also be detected. Pairs of parallel lines that visibly intersect at 90\u00b0 are identified as rectangles , , , , , and . In addition, partially hidden rectangles  and  are extrapolated from their visible portions. Some of the detected rectangles, such as the eyes  and  of Smiley icon , may be discarded as too small to be independent VEs. In some embodiments, the IPD stores the geometric borders or their attributes in persistent memory for use in debugging or adaptive processing.","One scenario benefiting from storage of intermediate entities such as geometric borders and geometric shapes is when some edge points are missing, extra, or ambiguous. This can occur, for example, where the background is highly patterned (extra edges that belong to the background image rather than any separate VE) or where a VE is nearly the same color as whatever is behind it (gaps where part of the VE outline is not detected as an edge). The IPD can iterate the edge detection at higher and lower thresholds and evaluate the resulting changes in the extracted geometric borders and geometric shapes.","The IPD then calculates a minimal-size untilted bounding box for each geometric shape. In this example, both the position and the size of the bounding box of rectangle  are captured by four values representing horizontal or vertical distances from a reference point : vertical distance  from reference point  to the uppermost edge of rectangle , vertical distance  from reference point  to the lowermost edge of rectangle , horizontal distance  from reference point  to the leftmost edge of rectangle , and horizontal distance  from reference point  to the rightmost edge of rectangle . A convention in computer graphics libraries uses the top left corner of the screen as the reference point , and measures all distances from there in pixels. However, any other choice of reference point and measurement units, and any other way of expressing the position and size of the bounding box, and even the use of non-rectangular bounding shapes, could be used in different embodiments.","Once all the geometric shapes have bounding boxes, the IPD uses the domain knowledge in the SOM to determine which geometric shapes or associated sets of geometric shapes correspond to VE types in a predetermined set, and derive correct SE types for those VEs. Here, the predetermined set includes at least a \u201cwindow\u201d VE type and an \u201cicon\u201d VE type. In , two of the rectangles from  are identified as windows  and . Various criteria can distinguish windows from other rectangular VEs: not being contained in a larger rectangle (other than the displayed background); being larger than a threshold size; having a characteristic border width, top banner, menu bar, or handles for opening, closing, and resizing. Several criteria can be applied in the alternative if no one criterion necessarily applies to all possible windows. Window symbolic entities (SEs) corresponding to windows ,  will be added to the SOM-I. In some embodiments, the SEs may retain associations to their stored constituent geometric shapes or geometric borders. Debugging, disambiguating, and regenerating original images are some of the processes that may make use of the retained associations.","In , the icon with detected perimeter  is given a minimum untilted rectangular bounding box defined by y-coordinates  and , and x-coordinates  and . Text from label  is recognized and extracted as plain text (e.g. ASCII or Unicode) . The bounding box, text, and image are placed in the SOM-I. In the illustrated embodiment, the icon with detected perimeter  is compared to contents of a stored image library  associated with the SOM, and flagged with an identifier if a close match  is found. In other embodiments, windows and other SEs, chosen from a finite set of SE types, are derived from the VEs (see, e.g. Appendix section ) based on their extracted characteristics, without reference to an image library. In the selective-processing embodiments described in a later section, the original VE , or a compressed version, or associated textual metadata is matched to entries in the image library. In many embodiments, this type of processing is limited to a subset of VE types, usually those with a complex or individualized appearance such as icons.","In some embodiments, if no close match to icon  were found, a copy of the unmatched icon would be given an identifier and stored in the image library in case of being encountered again. The image-library copy could include the label text and other textual meta-data, or not. In some embodiments, both the image and the textual metadata are present but may be searched for independently. The comparison and identification may alternatively be done with raw pixel images of the visual entities rather than vectorized versions. In other embodiments, some widgets are processed differently from others; those will be discussed in later sections.","Through this process, the bulky raw pixel data of the displayed image is broken up into visual entities (e.g., windows and icons), and the visual entities are further reduced to symbolic entities that are sets of variables in a programmatic expression, e.g. UniqueID={EntityType; BoundingBox(Xmin, Xmax, Ymin, Ymax); Foreground Y\/N; \u201cLabelText;\u201d MatchingImageID}. These highly compact symbolic entities can be manipulated more quickly even than vector images, and much more quickly than pixel-based images.","Visual entities displayed on a device screen are connected (at the underlying programmatic level) by relationships and hierarchies. Actions on one VE (and thereby on the underlying program) can also affect one or more other VEs in the displayed image. For example, opening a \u201cControl Panel\u201d or \u201cSystem Preferences\u201d window and clicking the \u201cDisplay\u201d or \u201cDesktop & Screen Saver\u201d icon inside the resulting open window allows access to an interface for changing the desktop background. The SOM-I keeps track of relationships and hierarchies of the SEs corresponding to displayed VEs. Each time a VE in a displayed image changes, the corresponding SE in the SOM-I reflects that change.",{"@attributes":{"id":"p-0065","num":"0064"},"figref":"FIGS. 4A-B","b":["401","451","402","452","403","453","408","411","458","461","412","462","413","463","452","453","458","461","462","463","451","402","403","408","411","412","413","401"]},"Panel  contains panel icons , , and . Taskbar\/dock divider  separates the group of the heart  and crescent  icons from the star icon . In the SOM-I, divider  is represented as separator SE . Icons ,  and  are represented as icon SEs ,  and , respectively. Because divider  and panel icons , , and  are contained in panel , their corresponding SEs , ,  and  are subordinate to panel SE . Separator SE 's relationship to icon SEs ,  and  can also be stored, illustrated in this diagram by position flag . Window  contains icons  and . Icons  and  are represented by icon SEs  and , subordinate to the corresponding window SE .","In some embodiments, position flag  shown schematically here may not be a distinct position-flag element of the SE data structure, but may be built into the SE or SOM-I in some other way. For instance, in some embodiments each SE has a stored position relative to a reference point on the desktop (e.g. via its bounding-box coordinates) and the position of the separator relative to other elements of the panel can be readily derived from there.","The SEs in the SOM-I are expressed as data structures. SOM-Is have been successfully rendered in general-purpose languages such as C and, as is sometimes advantageous for expressing a variety of data-manipulation functions, database languages such as SQLite. Stored attributes of an SE can include some of the corresponding visual entity's visual properties (position, size, shape, color, reference to an image-library entry); its relationships to other entities; its status (e.g. visible\/hidden, active\/inactive, has focus\/does not have focus); and associated text (e.g. labels, hover-text) converted into a plain-text format such as ASCII or Unicode.","Commands act on the SEs in the current SOM-I. In various embodiments, scripts can be written to search for an SE or set of SEs in the SOM-I based on various criteria. Criteria can include SE type (e.g., \u201call open windows,\u201d), location (\u201call SEs subordinate to the Preferences window SE\u201d), other properties (\u201call SEs with label text that includes the word \u2018Save\u2019), or a combination (\u201call icon SEs in the Taskbar panel SE\u201d). These and other lookup functions are implemented in runtime by the IPD. Other commands can, for example, \u201cexpect\u201d an SE to appear, disappear, or change within a predetermined time, extract text from an SE, and tag an SE for later processing. Script commands that result in target-input to the target device include, but are not limited to, moving a cursor to an identified SE, typing a text string, combinations of \u201ckeydowns\u201d and \u201ckeyups\u201d of identified keys, families of mouse-clicks (single, double, left, right), touches and multi-touches, and spoken commands. A non-limiting list of example commands can be found in the Appendix.","Step-by-Step Process Example",{"@attributes":{"id":"p-0071","num":"0070"},"figref":"FIG. 5"},"Before shutting down the physical server, the preferred practice is to shut down all its virtual machines first, using their internal mechanisms. This ensures that each guest OS's journals and logs are saved, and that no undesirable states are created by shutting down the VM in the midst of a write operation or communication with another device. A programmatic automated control script for this type of mixed-OS environment would be long, complex, and necessarily loaded with contingencies because of the variations in technologies and interfaces between one OS and another. A much simpler VSC script can handle the multi-OS scenario and be incorporated in the provider's provisioning framework. FIG. 's process begins after the \u201cleast-utilized\u201d servers have been identified and the applications migrated.","One approach to managing servers running virtual machines is use of a \u201cvirtual machine manager\u201d (VMM) program. VMMs present a user-interface on a display, and therefore can be operated through VSC. In initial sequence , the script starts the VMM (e.g. by searching for its icon and double-clicking it) and logs in with a valid username and password. In various embodiments, the username and password may be in the script, or the script may include instructions to retrieve the username and password from stored data. Once the login is verified, the VMM displays a list of the VMs running on the server.","The script then performs loop  to shut down each VM. A VM is selected from the displayed list (e.g. by clicking or keystroke tabbing). Its IP address is determined (e.g. by recognizing it in list text, drop-down text, or hover-text if it is shown there; or by any other means a user would undertake to get the address from the VMM, such as by clicking the \u201cNetworking\u201d tab on Citrix\u00ae XenCenter\u00ae). The script then connects to the VM (e.g. through VNC\u00ae) and displays the VM's desktop.","Each guest-OS type requires different shutdown steps, so the script determines the VM's OS. Once the VM desktop is visible, its OS can be determined by searching for OS-identifying widgets such as the Windows\u00ae \u201cStart\u201d button or the Ubuntu\u00ae Linux\u00ae \u201cSystem\u201d menu item on the taskbar. (In some embodiments, this step may not be necessary if the VMM's VM list expressly identifies each VM's OS. In that case, the script could search for text saying, e.g., \u201cWindows\u201d or \u201cLinux\u201d in the VM's listing on the VMM screen, before opening the VM desktop).","Once the guest OS is determined, the script executes the shutdown procedure for that particular OS. For example, in a Windows VM, the target-input may be \u201cleft-click \u2018Start button\u2019 on desktop\/wait for dialog box to open\/search for text \u2018Shut Down\u2019 in dialog box\/left-click \u2018Shut Down\u2019 option in dialog box.\u201d In a Ubuntu Linux VM, the target-input may be \u201cleft-click \u2018System\u2019 menu item on taskbar\/wait for drop-down menu to appear\/search for text \u2018Quit\u2019 in drop-down menu\/left-click \u2018Quit\u2019 in drop-down menu.\u201d Because the SOM-I has the entity-type and relationship information for each of the SEs, the script will not be misled by other text saying \u201cstart\u201d or \u201csystem\u201d that happens to be elsewhere on the screen.","If the sought-after VEs cannot be found, the script logs an error and notifies the user that the VM could not be shut down. In some embodiments, the script exits the loop immediately on an error. In others, the script attempts to shut down the other VMs and, if they could not all be shut down, exits the loop before shutting down the physical machine.","The system repeats loop  until no more VMs are running. Then, in final sequence , it selects the physical server (from, e.g., the XenCenter\u00ae tree view) and clicks \u201cShut Down,\u201d then quits the VMM.","A single controlling device can run a VSC script on multiple target devices simultaneously. Some types of software (e.g. messaging applications, multi-player games, streaming-video libraries) have hidden weaknesses that only affect performance when the number of users, or the number of messages or transactions per unit time, becomes excessive. The logistics of testing for these \u201chigh-use failures\u201d can be daunting. With VSC, a single controlling device can direct multiple target devices to generate increasing traffic in the application under test, and observe and log any errors that occur.","Computers are not the only devices that can be controlled by VSC.  is an example of a simple button control-board display, showing several screens and the corresponding SEs in the SOM-I. When a human user uses the device, the buttons may be \u201cpushed\u201d by touching directly if the display is a touch-screen, or by buttons on the device, or by buttons on a remote control, or by speech commands. Interfaces like this are found in some vehicles, information kiosks in public venues such as airports and shopping malls, programmable appliances such as televisions and embroidery machines, and some phones and electronic readers. On these and other devices designed for one user task at a time, many selections produce a new full-screen \u201cdesktop\u201d with a new set of visual entities. Here, the start-screen displayed image  has four menu buttons. In this example, if a command \u201cpushes\u201d (clicks, touches, or otherwise selects) bottom-left button SE  or bottom-right button SE  in SOM-I, start-screen displayed image  is completely replaced by alpha-keypad displayed image , just as if a user had pushed bottom-left button  or bottom-right button  on screen .","Alpha-keypad displayed image  is processed by the IPD and its VEs become the bases for SEs in the SOM-I subordinate to a new desktop SE  (some embodiments may alternatively treat displayed image  as a maximized window subordinate to initial desktop ). From here, a VSC command could push back-button SE , corresponding to back-button , to return the device to start-menu screen . The script can also push any of the alpha-key SEs  generated from alpha-keys  to enter text in text-entry field . To erase text from text-entry field , a command could push delete-button  via its SE  (or, in some embodiments, use a specialized \u201cclear entryfield\u201d command as in the Appendix). As text is entered in or erased from text-entry field , the corresponding text in text-entry-field SE is similarly changed when the SOM-I is refreshed.","Selecting the \u201c123\u201d button SE in SOM-I  produces numeric-keypad displayed image , and its SEs are added to SOM-I , subordinate to a new desktop (or window) SE . This type of keypad-switching is common on touch-screens where buttons need to be large enough for a human finger to touch just one key at a time (or on devices where the keys must be readable from some distance, as when using a remote control to select keys displayed on a TV screen across a room). On the screen, the text already typed into text-entry field  persists when the user switches keypads between alpha-keypad  and numeric keypad . In embodiments where the SOM-I only retains SEs for currently-visible VEs, text-entry-field SE in the alpha-keypad desktop or window and its counterpart in the numeric-keypad desktop or window may have no links with each other at all, but the text entries will remain correct due to being re-read. In embodiments that keep SEs in the SOM-I even when they are not being displayed, text-entry field SE instances and can be expressed as a single persistent SE.","While information is being entered in either of the keypads, the device's software may sense an information threshold and change the keypad's displayed image to an autocomplete-list displayed image , which is processed to create another new desktop (or window) SE . Autocomplete-list displayed image  has several visible options , along with a slider  that can hide visible options  to reveal other options. A VSC command can now select any of the visible-option SEs , or operate slider SE  to reveal other options. In embodiments where the SOM-I only retains SEs for currently-visible VEs, visible-option SEs  can be compared to a desired string, the matching SE  can be selected if visible, or slider  activated to reveal other options if there is no match. In embodiments that keep SEs in the SOM-I even when not visible, option SEs  can be collected for every position of slider  and compared with a desired string to find the closest match, in case none is a perfect match.","Software for many devices other than computers, such as smartphones and global positioning system (GPS) units, is often tested on simulators rather than the actual devices because interfacing with the actual devices is inconvenient. As long as a device has a display and some way to receive target-input, it can be tested via visual-symbolic control. One advantage of testing on the actual device is that conditions that affect device hardware, which in turn can affect software performance, can be realistically introduced into the test. Such conditions include variations in amount of memory, ambient temperature, humidity, communication signal strength, and electromagnetic interference.","Control of Virtual Desktops","Some embodiments can handle target devices that generate more than one displayed image. Different parts of a local desktop, for example, may be displayed on multiple monitors. Part of a virtual desktop may be visible on a local monitor while the rest is hidden until the hidden space is displayed. A desktop may be transmitted from a remote device and presented on a local monitor.","In many operating systems, a user can expand the virtual space of a computer's desktop environment beyond the physical dimensions of the screen, allocating sets of documents and applications to various regions of a larger virtual desktop. This allows the user to keep his or her place in multiple concurrent tasks without confusingly cluttering a single desktop. Different regions of enlarged virtual desktops can be accessed via a window, via a screen-switching icon, or moving the cursor to the edge of the current screen.","In multi-monitor systems, different regions of enlarged virtual desktops can be routed to different monitor screens. By contrast, in switching-KVM (keyboard-video-mouse) systems, a switch determines which of several devices displays its interface on a single monitor and responds to target-input of the associated keyboard and mouse.","Another type of virtual desktop is a guest-OS desktop, displayed when a device with one \u201chost\u201d OS is enabled to run a second \u201cguest\u201d OS via a virtual machine. A virtual machine can provide a complete system platform for running the guest OS without requiring extra hardware on the local device.","Remote desktops are another type of virtual desktop; a program such as pcAnywhere\u00ae, Splashtop\u00ae, Microsoft's Remote Desktop Connection\u00ae or VNC\u00ae may enable one computer to access and manipulate a remote computer by displaying the desktop of that computer in a window or on a switchable screen.  is an example of a remote desktop display. Facilitated by a network connection and appropriate software, window  on local desktop  is displaying remote desktop  from remote device . If the software enables control of the VEs on remote desktop  by target-input to local desktop , then a VSC script addressing local desktop  as a target device can also \u201creach through\u201d window  and operate remote device  as a target device.","Variations on Device Connection and Function Distribution",{"@attributes":{"id":"p-0092","num":"0091"},"figref":"FIGS. 8A-D"},{"@attributes":{"id":"p-0093","num":"0092"},"figref":"FIG. 8A","b":["821","201","203","1","10","202","203","201","821"]},{"@attributes":{"id":"p-0094","num":"0093"},"figref":"FIG. 8B","b":["201","203","831","831","832","833","202","831","4","205","206","205","5","6","832","4","5","6","201","1","10","833","833","832","832"]},"The illustrated embodiment is not the most bandwidth-conservative, because entire display-images (in compressed or uncompressed form) are passed across the network every time the SOM-I is refreshed. However, it is one of the most flexible embodiments because the target device only needs some kind of suitable network connection and the controlling device only needs a way to send commands to, and receive data from, a network-based development environment or runtime. In other distributed embodiments, script interpreter (or compiler)  or script development environment  may reside on a server  in distributed network , e.g. they may be accessed via a Website on the server. Although the components of the VSC software can be distributed in a wide variety of ways on a network, including using the network as a passive conduit like cable  in , distributed operation presents opportunities to greatly reduce the technical demands on controlling device .",{"@attributes":{"id":"p-0096","num":"0095"},"figref":"FIG. 8C","b":["203","841","1","201","842","10","201","203","832","833","831","201","201","843"],"i":["m ","m "]},{"@attributes":{"id":"p-0097","num":"0096"},"figref":"FIG. 8D","b":["851","201","203","202","4","201","851"]},"Here, scripts running on controlling device  interact with SOM-I  over wireless link ; other embodiments could include wired links and a larger intervening network. This embodiment conserves bandwidth on link  by carrying out most of the complex processing close to target device . Other embodiments of target-adaptor modules might execute only part of the target-adaptor function , with the rest occurring on controlling device  or elsewhere on a network to which controlling device  and target-adaptor module  are connected. For example, target adaptor module  could periodically (or after a triggering event) receive a copy of the SOM-I and verify that it still accurately represents the current displayed image on the target device. If a mismatch is detected, target adaptor module  can re-transmit the current displayed image (or only those portions found to have changed).","Alternatively, target adaptor module  could be instructed to ignore changes in certain regions of the desktop (e.g., a background slide-show, a timer, or a console window where logging information is rapidly appearing or scrolling). The instructions describing types of display changes to ignore may be part of the script, stored settings called by the script, or resident in firmware or software in target adaptor module . Some embodiments could include a camera to capture the screen display and mechanical actuators to operate hardware keys, buttons, knobs, or sliders on devices without accessible input\/output ports.","Selective Processing","Enhanced embodiments may use a variety of processing algorithms, each optimized for a specific VE type. For example, a text-processing algorithm might look for a collection of neighboring edge-pixel chains that are within a certain size range and aligned on a horizontal axis. The collection of chains can then be given a bounding box and sent to an OCR algorithm for further processing to extract the ASCII or Unicode text. A desktop-icon processing algorithm might look for edge-pixel chains of a certain size, with text labels positioned below them, arranged on the desktop in a grid pattern. A \u201cbutton\u201d extraction algorithm can look for a rectangle of a certain size and with rounded corners, and then determine whether the interior edge-pixel chains form a text label or an icon within the rectangle.","Icons and similar graphically-detailed VEs (MacOS\u00ae bookmarklets and other \u201cmini-icons\u201d, some graphic buttons) can include some form of captured image in their SEs along with other attributes. However, there are some differences between VSC techniques that include an image in the SE and other approaches that match entities in the pixel domain rather than the symbolic domain.","The IPD can be programmed to recognize and select graphically-detailed VEs using a number of different criteria: size, position, (e.g. in a taskbar or dock), reactions to clicking or hovering by a cursor, geometric-border characteristics, similarity to images in libraries, and any other characteristic common to the selected VEs but uncommon among other VEs. Once recognized, the selected VEs can be subject to a different process than the other VEs.  conceptually illustrate one embodiment of selective processing for icons.","In , the IPD determines the coordinates , , ,  of the bounding box for icon  as presently displayed, relative to reference point  of the displayed image. In some embodiments, each character of the label text is also given its own bounding box (omitted from the illustration for legibility) prior to being routed to OCR. In , the IPD recognizes \u201cSmiley\u201d icon  in window  as an icon by some recognition process . One possible recognition process  is based on the size of the bounding box and the characteristics of nearby VEs. If bounding box (,,,) is a within a range of typical icon sizes, and the bounding boxes nearby and below are sized and arranged typically for text, Smiley  is classified as an icon. In , icon image  is extracted. This may be an original image, a compressed image, or in some embodiments the binarized edges, extracted contours, or geometric borders. Label text  is extracted along with the image.","In , the IPD optionally generates target-input to reveal the displayed icon 's reactions, including appearance alterations and associated \u201crevealable\u201d text that can be added to the domain knowledge of the SOM. In the illustration, the IPD moved cursor  over icon , revealing hover-text , then entered a \u201cright-click,\u201d revealing pop-up contextual menu .","In another embodiment, hidden-but-revealable text ,  is not collected routinely during initial analysis; that process is deferred until a script includes a query or command identifying an icon by its revealable text. For example, if a script command requests a widget identified by its hover-text, the IPD hovers the cursor over each widget in the displayed image, revealing and extracting hover-text from each one, until the request is matched or until all the widgets in the displayed image have been hovered over. The IPD adds the extracted hover-text for each widget to the widget's SE in the SOM-I. The resulting SOM-I includes information on whether hover-text has been found for each widget, and if so, what it is. If a subsequent command requests another widget by hover-text, and the hover-text does not match any that is already cached in the SOM-I, the IPD need only extract hover-text from any widgets that were not previously hovered over.","In , captured information  about icon  goes into its SE  in SOM-I . The expression \u201cImage={Pix(1,1) . . . Pix(N,M)}\u201d represents the encoded form of captured icon image , which in this illustrated example is an array of N\u00d7M pixels. As well as the icon's graphic and textual content, the SOM-I needs the icon's context: the bounding box information \u201cBbox\u201d that keeps track of the icon's position and size on this particular displayed image, and its visibility (as illustrated, 100% because no part of it is hidden behind another VE). Optionally, a core subset  of this information, comprising the information that could help identify this icon if the IPD encounters it again, is added to the associated domain knowledge, libraries, and mapping structures  in SOM . If the IPD is not equipped to add the metadata required for the mapping structure, core subset  can be preserved in a log or other file and the metadata can be added later.","Capturing and cataloguing a widget's image as part of a selective-processing algorithm provides another way to identify particular widgets in VSC scripts. Besides identifying them by textual metadata such as identifiers, labels, tags and the like, or by location relative to another VE or reactions to target-input, image-captured widgets can be identified by including their images directly in the script. The IPD responds by looking for a match for the scripted image among the images in the SOM-I. The images do increase the size of the SE data structure. However, the ability to recognize icons and similar graphically complex widgets quickly and accurately whenever they appear, and to retrieve the relevant domain knowledge from the SOM instead of needing to repeatedly \u201crediscover\u201d what the icon is or does, improves efficiency and can produce a net increase in speed over groups of successive operations. Judicious selection of VE types for which to retain images is part of overall process optimization.","This differs from approaches that act on all VEs as images, without creating SEs or a SOM-I. Here, only selected (and generally small) parts of the displayed image, rather than the entire displayed image, are addressed by VSC queries and commands. Another difference is that because the SOM-I reflects the relationships and hierarchies of the SEs, a particular image-containing SE often only needs to be searched for in the SE's expected parent structure (e.g., a Control Panel window or a taskbar) rather than the entire SOM-I. This SE searched for under its parent structure is found just as quickly no matter where the parent structure's corresponding VE is located in the displayed image. When a target device's docks and taskbars can be anchored on any edge of a displayed image, and its windows can open in arbitrary locations, this can be especially helpful.","Learning Modes","Another advantage of VSC's user-like interaction with target devices is the ability to \u201clearn\u201d a new or changed application or system just as the (more empirically-inclined variety of) user would. The information acquired by learning is added to the domain knowledge of the SOM: its image and character libraries, mapping structures, and so forth. Both manual and automatic learning modes are possible.","Manual learning depends on explicit action by a VSC user. In one embodiment, the script development environment includes a \u201ccapture-and-catalog\u201d tool. With the capture-and-catalog tool, a user accesses a displayed image and manually selects a new widget (e.g., an icon that starts a newly installed application). Optionally, the IPD may initially query the SOM to make sure the widget has not already been added. The tool enables the user to capture all the necessary attributes of an SE corresponding to the new widget. The process may include fitting a bounding box around the new widget; recognizing and storing text associated with the new widget (e.g. label, hover-text, contextual menu); assigning the new widget a name and type identifier; in selective-processing embodiments, storing the image if appropriate for the SE type; and cataloguing all the new widget's SE attributes in the SOM.","In another embodiment, a VSC user can capture and catalog a new widget by importing a file (screen-shot, other bitmap, other image type, set of image files). In some embodiments, the library can notify the user if an identical or substantially similar widget already exists in the library. In another aspect of manual training, software developers can catalog new widgets from their own products and contribute them to shared libraries. This generates publicity and goodwill for the developers among the user-base, and also removes the risk that a supplier's widget might be miscatalogued.","In one automatic learning mode, SEs corresponding to new widgets can be incorporated into the runtime during a training session, prior to running a script. In one embodiment, during the training session, the software compares all the SEs generated from VEs detected in the displayed image with stored information on known SEs. This might be done from the images, from the extracted lines, from associated text such as labels and hover-text, or from a combination. Any new widget (a VE not expressible by any known SE in the stored set) would be captured and catalogued: i.e., its attributes, which may include its image, would be placed in the library and given a name. Names can be arbitrary or based on associated text, and in some embodiments may be edited for easier recognition. For example, an automated training mode might find a new widget in a taskbar and place it in the image library, associated with its hover-text \u201cBrowse and run installed applications,\u201d assign it an auto-generated name such as \u201cABCD44,\u201d and notify the VSC user. The user, recognizing the image as the Ubuntu \u201cCircle of Friends\u201d\u00ae start button, could rename the new library entry \u201cUbuntu Circle of Friends.\u201d Now the user can write scripts to search for or act on this widget on any target device by identifying it in the script as \u201cUbuntu Circle of Friends.\u201d In another embodiment, a training session may simply populate an image library with all the found icon images and their associated metadata.","An alternative approach identifies new widgets \u201con the fly\u201d while a script is running. The technique relies on knowing a target-input that will cause the widget to appear or disappear. Suppose a new version of an application places a mini-icon in a taskbar when the application is running. The script would cause the IPD to (1) extract all the widgets from the taskbar without the application running and temporarily store them as \u201cSet A,\u201d (2) start the application, (3) extract all the widgets from the taskbar again and temporarily store them as \u201cSet B,\u201d (4) subtract Set A from Set B, and (5) identify the remaining widget(s) as belonging to the application just started. The identified widget could then be tagged with metadata to be used for future queries and commands in the same script. (Some embodiments might do a \u201cdouble-check\u201d in case something else appeared at the same time for an unrelated reason, e.g. a low-battery warning: quitting the application, collecting a \u201cSet C\u201d of taskbar widgets, and making sure Set C\u2212Set A=null).","Consider the example process of . A learning process involving the VMM screens could streamline the script to go straight to information such as the IP address and OS of each virtual machine, without needing to hunt for it and build in contingencies in case the information is not readily found.","Device-Aware Enhancements","Although VSC enables many tasks to be performed device-agnostically\u2014that is, with no \u201cknowledge\u201d of the operating system or communication protocol at the program level\u2014some scripts can be made more efficient by recognizing the OS (or a running application, or a device type). For example, the process in  began by searching the SOM-I for a \u201cStart\u201d button in the bottom left corner of the desktop (indicating a Microsoft Windows\u00ae OS) or a \u201cSystem\u201d menu item on the taskbar (indicating a Ubuntu Linux\u00ae OS). Once the OS is recognized, the process can be tailored for faster or more accurate performance on that OS. For example, a script for Windows XP\u00ae would only look for the in-focus application's main menu bar at the top of the in-focus window, while one for Mac OS X\u00ae, would only look for it in the panel at the top of the desktop. The process could also filter the image libraries it uses to look up icons (or, in some embodiments, other widgets); if the desktop is recognized as Ubuntu Linux\u00ae, the icon-matching process will consume less time if only Ubuntu Linux\u00ae icons are searched.","In some embodiments, the user may start a script with an OS-recognition process and then provide a branch point, where a subsequent process is chosen based on the OS recognition result. In tasks where the target-input to perform a particular action is particularly OS-dependent (for instance, \u201cCtrl+s\u201d to save a file on Windows\u00ae vs. \u201cCmd+s\u201d to save a file on Mac OS\u00ae), the correctly chosen branch could complete the task in less time than a completely agnostic script that tried every option for every possible OS.","In some embodiments, library entries corresponding to OS-identifying or device-identifying widgets can be built into a distributed product. Images, associated text and metadata, and other attributes for top-level \u201cStart\u201d buttons, window-manipulation handles, and access icons for system settings and preferences could be pre-catalogued and available to all scripts. Scripts could then call these frequently-used widgets by name or other shorthand identifier, accessing the device-identifying functionality from the scripted commands without needing to embed their images in the script.","Retention of Symbolic-Entity History","Embodiments of the SOM-I generating logic within the IPD can range from \u201cforgetful\u201d (re-creating the SOM-I each time the displayed image is analyzed) to \u201cretentive\u201d (keeping some kind of historical information about past states of the SOM-I or individual SEs within the SOM-I). Forgetful embodiments are simplest, but retentive embodiments add some useful capabilities.","Some embodiments overwrite the SOM-I only when something in the displayed image actually changes, rather than every time the framebuffer outputs a displayed image. This can reduce the bandwidth and processing time. The IPD can retain a stored displayed image, compare it to the current displayed image, and only overwrite the SOM-I (and store the current displayed image as the new comparison image) if the comparison detects a change.","Alternatively, the IPD can generate a new \u201ccandidate\u201d SOM-I at intervals, compares the candidate SOM-I to the current SOM-I, and only overwrites the current SOM-I if the comparison detects a change. In one embodiment, the SOM-I takes \u201csnapshots\u201d of the displayed image periodically at a default frequency (every second, every 100 ms, or any other suitable interval). If mouse, keyboard, or other target-input activity is detected by the IPD, the snapshot interval is increased because the displayed image is more likely to change. The IPD compares consecutive (or otherwise subsequent) snapshots to look for differences. The changed areas, once identified, can be assigned bounding boxes or other types of boundaries. Feature extraction and other image analysis could then be confined to the assigned boundaries to account for the changes, rather than unnecessarily repeating those processes for unchanged parts of the displayed image.","In some embodiments, past copies of the SOM-I per se are not retained, but any VE that becomes visible at any point in a VSC task retains an SE in the SOM-I even while the VE is hidden. In one such embodiment, if a window opens during the script and hides some other widgets, those widget SEs are not removed from the SOM-I. Instead, they stay in the SOM-I and their visibility variables are changed to reflect their \u201chidden\u201d status. Some SEs may be removed from the SOM-I under certain conditions. For example, a window SE may remain in the SOM-I with a visibility variable denoting whether it is fully visible, partially hidden, completely hidden, or minimized\u2014but if the window is closed, the window SE is removed from the SOM-I. Some embodiments of the SOM keep track of the speed and direction of window movement.","Disambiguation","Another challenge frequently arising in machine analysis of displayed images in multitasking systems (where multiple windows or other application interfaces can be open simultaneously) is disambiguation. VEs that are programmatically independent can become juxtaposed in the displayed image to appear connected. Embodiments of the VSC IPD can resolve visual ambiguities from the results of disambiguating target-input, and sometimes successfully ignore them.","If the actions that juxtaposed the VEs (for instance, commands to open two overlapping windows) occurred earlier in the presently running VSC script, or in a previous period for which a log is available, the IPD will experience no ambiguity. As each event juxtaposing the VEs occurs (e.g. \u201cOpen Window 1,\u201d \u201cOpen Window 2\u201d), the accurate SE relationships were captured (\u201cWindow 1 was opened from a desktop icon; its SE is subordinate to the desktop SE. Window 2 was also opened from a desktop icon; its SE is also subordinate to the desktop SE\u201d). However, there may be situations where the history is not available. A target device may be in an arbitrary state when a script starts running. The SOM-I may become corrupted. Multiple applications (or multiple windows within an application) may open more rapidly than the SOM-I is being refreshed. Without access to the history that juxtaposed overlapping VEs, disambiguation may be needed before the IPD can sort out the relationships between the VEs and arrange the corresponding SEs in the SOM-I accordingly.",{"@attributes":{"id":"p-0129","num":"0128"},"figref":"FIG. 10A","b":["1002","1001","1003","1004","1002"]},"(a) Is small window  actually open inside large window  (i.e., are they programmatically linked), rather than arbitrarily placed \u201con top\u201d of large window ?","(b) Is text  the label for icon graphic , or a button associated with something else that just happens to be in a \u201clabel-like\u201d position?","(c) Are large window , small window , and icon  the interactive widgets they appear to be, or might any or all of them be a non-interactive picture (e.g. an illustration or screenshot visible in a document window)?","(d) In systems that support transparency, might large window  even be in front of all the other VEs rather than behind it?","Embodiments of VSC begin by creating a tentative SOM-I based on assumptions. In some embodiments, the assumptions may be customizable for the known behaviors of a particular device or OS. In the illustrated example, the assumptions are:","(1) Any VE completely within a larger VE is subordinate to the larger VE, and","(2) Any text near, below, and about the same size as an icon graphic is the icon's label.","Thus the illustrated tentative SOM-I has text SE  subordinate to icon SE , which is subordinate to small window SE , which is subordinate to large window SE , which is subordinate to desktop SE .","An optional preliminary step, for systems supporting transparent VEs, is to open a global settings utility (e.g., Preferences or Control Panel) via target-input and ensure that transparency is disabled. This can reduce the number of ambiguities; if all VEs are opaque, any VE visible within the border of a window is either inside it or in front of it.","In , the IPD generated a \u201cMove\u201d command (e.g. a mouse click-and-drag) to small-window SE . Small window  was able to move outside the border of large window , which a subordinate window would not have been able to do. Therefore, the SOM-I is revised to make small window  subordinate only to the desktop.","When small window  was moved, icon graphic  and text  moved with it. Therefore, icon SE  and text SE  are subordinate, directly or indirectly, to small-window SE . Their relationship between icon SE  and text SE  is still ambiguous, so for now the tentative assumption remains in place. A \u201cMove\u201d command to icon graphic  could resolve the remaining ambiguity. If text  moves with it, text  is icon 's label and text SE  is verified as directly subordinate to icon SE . However, if text  did not move with icon , the next SOM-I revision would make text SE  directly subordinate to small-window SE .","Note that a \u201cMove\u201d command for disambiguation need not be as large in amplitude as the one in . Even a shift of one pixel could allow the VSC software to ascertain which other visual entities moved the same amount as a result. A move that small, especially if reversed immediately after detecting the effects, would not be detected by a human viewing the screen.","Other target-input commands could alternatively have disambiguated the display of :","(a) Selecting large-window SE  to have focus (e.g., by clicking or Alt+Tab) would cause large window  to appear empty. Small window , icon graphic , and text  would disappear because they were not actually subordinate to large window .","(b) A \u201cCloseWindow\u201d or \u201cMinimize Window\u201d command to large-window SE  would have caused large window  to disappear while small window , icon graphic , and text  remained visible.","(c) Some windows will change a cursor's shape when the window is in focus and the cursor enters or exits the window; these can also be used for disambiguation.","Another assumption that can be used in disambiguation is that each application currently running on a target device contributes exactly one top-level window (and perhaps one or more subordinate windows) to the SOM-I.","(a) Target-input commands can check which applications are running (e.g., opening the Windows\u00ae Task Manager, looking for \u201cspotlights\u201d under icons in the Mac OS X\u00ae Dock, hovering the cursor over taskbar icons, and using a keyboard shortcut such as \u201cAlt+Tab\u201d or \u201cCmd+Tab\u201d to change which application has focus until the first-found application returns).","(b) If the SOM-I has the wrong number of top-level windows, a \u201cShow Desktop\u201d or \u201cMinimize All Windows\u201d command can be executed, then one window at a time can be restored to its regular size.","Some applications provide a way to see a list of their open windows, e.g. in a \u201cWindow\u201d drop-down menu, a contextual menu, or hover-text; the number of these open files is usually related to the number of subordinate SEs that should be under the application's top-level window in the SOM-I.","VSC can deal with other types of ambiguity. For example, the borders of windows can sometimes be difficult to detect against a background. One approach is to adjust the IPD's edge detection threshold, as described in an earlier section. Alternative approaches use target-input to resolve ambiguity:","(a) Where the background color is close to that of the window, a command to set the display to High Contrast can increase the difference in appearance and make the edge easier to detect.","(b) Where the background is patterned, a command to Show Desktop or Minimize All Windows will reveal which lines were actually window borders.","(c) In some operating systems, the cursor changes shape when it crosses a window boundary. In one of these systems, a command to move the cursor across the boundary of a suspected window, and analysis of the cursor's appearance before and after the crossing, can determine whether a rectangle is a window.","Another source of ambiguity arises when programmatically identical or analogous widgets can appear visually different (\u201cpolymorphous VEs\u201d as discussed in an earlier section). Cursors may be the most polymorphous. Users can customize a cursor's default size, shape, color, and motion behavior (e.g. blinking, trails). Windows, their on-screen controls (e.g. sizing handles) and their contents (e.g., editable documents and drawings, games) can also change a proximate cursor's appearance. Other polymorphous VEs include icons for the same application or file-type (e.g., an icon that opens a folder or document) that present somewhat different appearances when installed under different operating systems.","One way to keep track of alternate appearances of a VE is by reference to stored data, such as an image library, which may be part of the SOM. Device-dependent or OS-dependent subgroups or tags may be included, to narrow the search range in device-aware or OS-aware scripts. Within a library, alternative appearances of the same widget could be organized as a set of images using an identifier (e.g. \u201ccursor\/blackarrow, cursor\/whitearrow, cursor\/hand, cursor\/crosshair\u201d) and the SE could include a reference to this set identifier.","In some embodiments for handling polymorphous VEs, the SOM includes a mapping data structure (MDS). The MDS links all the alternative appearances of the polymorphous VE with associated information useful for generating the corresponding SE if any version of the VE is detected in a displayed image. In some embodiments, all the alternative-appearance linking is done within the MDS.  conceptually illustrates an MDS  as part of SOM . The information can include (1) the set of possible versions of the VE , ,  (as images or contours; as embedded data in the MDS or as references to an image library); associated text (e.g. label text , hover-text , contextual menu ); the corresponding SE type (e.g. icon SE, cursor SE, caret SE). When the IPD recognizes any mapped version of a polymorphous VE, the MDS ensures that the correct corresponding SE is generated, and that an appearance change in the polymorphous VE triggers a change in the corresponding SE rather than the generation of a separate \u201cnew\u201d SE. In various embodiments, the MDS for a particular SE may be built into a distributed product, or may be a result of an automatic or manual learning mode.","Programming Languages","The script development environment may include an API. In some embodiments, the VSC scripts can be written in a custom language. In other embodiments, the API structures the VSC commands as one or more language bindings to existing \u201cbase\u201d programming languages. Examples of compatible base languages include Lua, TCL\/Expect, Python, and C. The language-binding structure makes the interface accessible to multiple languages with no need to translate or revise the underlying material. One approach to language-binding is to implement VSC commands as a library function call where the function takes a string argument. The string argument may include familiar colloquial computer terms such as the verbs \u201cselect\u201d, \u201copen\u201d, \u201ctype\u201d, \u201cclick\u201d, \u201cright-click\u201d, \u201cdouble-click\u201d and the nouns \u201ccursor\u201d, \u201cmouse\u201d, \u201cwindow\u201d, \u201cmenu\u201d, \u201cdock\u201d, \u201ctitle\u201d, \u201cfolder\u201d, \u201cicon\u201d, \u201cbutton\u201d, and \u201ctext entry field.\u201d Some sample commands embedded in Lua appear in the Appendix.","This specification and the accompanying drawings are intended to explain general concepts and offer specific examples, but not to limit the scope of legal protection for the subject matter. That protection is limited only by the scope of the claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF DRAWINGS","p":[{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIGS. 3A-3E"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIGS. 4A-B"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIGS. 8A-D"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIGS. 9A-E"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIGS. 10A-B"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 11"}]},"DETDESC":[{},{}]}
