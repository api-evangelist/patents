---
title: Adaptation of exponential models
abstract: A method and apparatus are provided for adapting an exponential probability model. In a first stage, a general-purpose background model is built from background data by determining a set of model parameters for the probability model based on a set of background data. The background model parameters are then used to define a prior model for the parameters of an adapted probability model that is adapted and more specific to an adaptation data set of interest. The adaptation data set is generally of much smaller size than the background data set. A second set of model parameters are then determined for the adapted probability model based on the set of adaptation data and the prior model.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07860314&OS=07860314&RS=07860314
owner: Microsoft Corporation
number: 07860314
owner_city: Redmond
owner_country: US
publication_date: 20041029
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"p":["The present application claims priority from U.S. provisional application 60\/590,041 filed on Jul. 21, 2004.","The present invention relates to exponential models. In particular, the present invention relates to adapting exponential models to specific data.","Exponential probability models include models such as Maximum Entropy models and Conditional Random Field (CRF) models. In Maximum Entropy models, it is common to have a set of features, which are indicator functions that have a value of one when the feature is present in a set of data and a value of zero when the feature is not present. A weighted sum of the features is exponentiated and normalized to form the maximum entropy probability.","Typically, the weights for the Maximum Entropy model are trained on a large set of training data. To avoid overtraining the weights (model), at least one technique of the prior art applies smoothing to preserve probability mass for unseen data.","Although using a large set of training data makes the Maximum Entropy model useful across a large set of input data, it also produces a Maximum Entropy model that is not optimized for specific types of input data.","Thus, it would be desirable to be able to adapt Maximum Entropy models that have been trained on a large set of training data to specific sets of expected data so that they may perform better with the expected data.","A method and apparatus are provided for adapting an exponential probability model. In a first stage, a general-purpose background model is built from background data by determining a set of model parameters for the probability model based on a set of background data. The background model parameters are then used to define a prior model for the parameters of an adapted probability model that is adapted, and more specific, to an adaptation data set of interest. The adaptation data set is generally of much smaller size than the background data set. A second set of model parameters are then determined for the adapted probability model based on the set of adaptation data and the prior model.",{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 1","b":["100","100","100","100"]},"The invention is operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of well-known computing systems, environments, and\/or configurations that may be suitable for use with the invention include, but are not limited to, personal computers, server computers, hand-held or laptop devices, multiprocessor systems, microprocessor-based systems, set top boxes, programmable consumer electronics, network PCs, minicomputers, mainframe computers, telephony systems, distributed computing environments that include any of the above systems or devices, and the like.","The invention may be described in the general context of computer-executable instructions, such as program modules, being executed by a computer. Generally, program modules include routines, programs, objects, components, data structures, etc. that perform particular tasks or implement particular abstract data types. The invention is designed to be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment, program modules are located in both local and remote computer storage media including memory storage devices.","With reference to , an exemplary system for implementing the invention includes a general-purpose computing device in the form of a computer . Components of computer  may include, but are not limited to, a processing unit , a system memory , and a system bus  that couples various system components including the system memory to the processing unit . The system bus  may be any of several types of bus structures including a memory bus or memory controller, a peripheral bus, and a local bus using any of a variety of bus architectures. By way of example, and not limitation, such architectures include Industry Standard Architecture (ISA) bus, Micro Channel Architecture (MCA) bus, Enhanced ISA (EISA) bus, Video Electronics Standards Association (VESA) local bus, and Peripheral Component Interconnect (PCI) bus also known as Mezzanine bus.","Computer  typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by computer  and includes both volatile and nonvolatile media, removable and non-removable media. By way of example, and not limitation, computer readable media may comprise computer storage media and communication media. Computer storage media includes both volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information such as computer readable instructions, data structures, program modules or other data. Computer storage media includes RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other storage medium which can be used to store the desired information and which can be accessed by computer . Communication media typically embodies computer readable instructions, data structures, program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term \u201cmodulated data signal\u201d means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media includes wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, RF, infrared and other wireless media. Combinations of any of the above should also be included within the scope of computer readable media.","The system memory  includes computer storage media in the form of volatile and\/or nonvolatile memory such as read only memory (ROM)  and random access memory (RAM) . A basic input\/output system  (BIOS), containing the basic routines that help to transfer information between elements within computer , such as during start-up, is typically stored in ROM . RAM  typically contains data and\/or program modules that are immediately accessible to and\/or presently being operated on by processing unit . By way of example, and not limitation,  illustrates operating system , application programs , other program modules , and program data .","The computer  may also include other removable\/non-removable volatile\/nonvolatile computer storage media. By way of example only,  illustrates a hard disk drive  that reads from or writes to non-removable, nonvolatile magnetic media, a magnetic disk drive  that reads from or writes to a removable, nonvolatile magnetic disk , and an optical disk drive  that reads from or writes to a removable, nonvolatile optical disk  such as a CD ROM or other optical media. Other removable\/non-removable, volatile\/nonvolatile computer storage media that can be used in the exemplary operating environment include, but are not limited to, magnetic tape cassettes, flash memory cards, digital versatile disks, digital video tape, solid state RAM, solid state ROM, and the like. The hard disk drive  is typically connected to the system bus  through a non-removable memory interface such as interface , and magnetic disk drive  and optical disk drive  are typically connected to the system bus  by a removable memory interface, such as interface .","The drives and their associated computer storage media discussed above and illustrated in , provide storage of computer readable instructions, data structures, program modules and other data for the computer . In , for example, hard disk drive  is illustrated as storing operating system , application programs , other program modules , and program data . Note that these components can either be the same as or different from operating system , application programs , other program modules , and program data . Operating system , application programs , other program modules , and program data  are given different numbers here to illustrate that, at a minimum, they are different copies.","A user may enter commands and information into the computer  through input devices such as a keyboard , a microphone , and a pointing device , such as a mouse, trackball or touch pad. Other input devices (not shown) may include a joystick, game pad, satellite dish, scanner, or the like. These and other input devices are often connected to the processing unit  through a user input interface  that is coupled to the system bus, but may be connected by other interface and bus structures, such as a parallel port, game port or a universal serial bus (USB). A monitor  or other type of display device is also connected to the system bus  via an interface, such as a video interface . In addition to the monitor, computers may also include other peripheral output devices such as speakers  and printer , which may be connected through an output peripheral interface .","The computer  is operated in a networked environment using logical connections to one or more remote computers, such as a remote computer . The remote computer  may be a personal computer, a hand-held device, a server, a router, a network PC, a peer device or other common network node, and typically includes many or all of the elements described above relative to the computer . The logical connections depicted in  include a local area network (LAN)  and a wide area network (WAN) , but may also include other networks. Such networking environments are commonplace in offices, enterprise-wide computer networks, intranets and the Internet.","When used in a LAN networking environment, the computer  is connected to the LAN  through a network interface or adapter . When used in a WAN networking environment, the computer  typically includes a modem  or other means for establishing communications over the WAN , such as the Internet. The modem , which may be internal or external, may be connected to the system bus  via the user input interface , or other appropriate mechanism. In a networked environment, program modules depicted relative to the computer , or portions thereof, may be stored in the remote memory storage device. By way of example, and not limitation,  illustrates remote application programs  as residing on remote computer . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.",{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 2","b":["200","200","202","204","206","208","210"]},"Memory  is implemented as non-volatile electronic memory such as random access memory (RAM) with a battery back-up module (not shown) such that information stored in memory  is not lost when the general power to mobile device  is shut down. A portion of memory  is preferably allocated as addressable memory for program execution, while another portion of memory  is preferably used for storage, such as to simulate storage on a disk drive.","Memory  includes an operating system , application programs  as well as an object store . During operation, operating system  is preferably executed by processor  from memory . Operating system , in one preferred embodiment, is a WINDOWS\u00ae CE brand operating system commercially available from Microsoft Corporation. Operating system  is preferably designed for mobile devices, and implements database features that can be utilized by applications  through a set of exposed application programming interfaces and methods. The objects in object store  are maintained by applications  and operating system , at least partially in response to calls to the exposed application programming interfaces and methods.","Communication interface  represents numerous devices and technologies that allow mobile device  to send and receive information. The devices include wired and wireless modems, satellite receivers and broadcast tuners to name a few. Mobile device  can also be directly connected to a computer to exchange data therewith. In such cases, communication interface  can be an infrared transceiver or a serial or parallel communication connection, all of which are capable of transmitting streaming information.","Input\/output components  include a variety of input devices such as a touch-sensitive screen, buttons, rollers, and a microphone as well as a variety of output devices including an audio generator, a vibrating device, and a display. The devices listed above are by way of example and need not all be present on mobile device . In addition, other input\/output devices may be attached to or found with mobile device  within the scope of the present invention.","The present invention approaches the problem of identifying capitalization for a sentence as a sequence labeling problem in which a sequence of words is assigned a sequence of capitalization tags that indicate the type or form of capitalization to be applied to the words. Under one embodiment, the possible capitalization tags include:\n\n","Based on this approach, one embodiment of the present invention constructs a Markov Model that assigns a probability p(T|W) to any possible tag sequence T=t. . . t=Tfor a given word sequence W=w. . . w. Under one embodiment, this probability is determined as:",{"@attributes":{"id":"p-0032","num":"0036"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["T","W"],"mo":"\u2758"}}},{"mover":{"munder":{"mo":"\u220f","mrow":{"mi":"i","mo":"-","mn":"1"}},"mi":"n"},"mo":"\u2062","mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["t","i"]},"mo":"\u2758","mrow":{"msub":{"munder":{"mi":["x","_"]},"mi":"i"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"W","mo":",","msubsup":{"mi":"T","mn":"1","mrow":{"mi":"i","mo":"-","mn":"1"}}}}}}}}}],"mo":"="}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"1"}}]}}}},"br":{},"sub":["i ","i","1"],"sup":"i\u22121"},"Under one embodiment, the context information is information that can be determined from the preceding word, the current word, and the next word in the word sequence as well as the preceding two capitalization tags. The information provided by these values includes not only the words and tags themselves, but portions of each of the words, and bigrams and trigrams formed from the words and bigrams formed from the tags.","Under one embodiment of the invention, the probability P(T|x(W,T)) is modeled using a Maximum Entropy model. This model uses features, which are indicator functions of the type:",{"@attributes":{"id":"p-0035","num":"0039"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"y","mo":",","munder":{"mi":["x","_"]}}}},{"mo":"{","mtable":{"mtr":[{"mtd":[{"mrow":{"mn":"1","mo":","}},{"mrow":{"mrow":{"mrow":{"mrow":{"mrow":{"mrow":{"mrow":{"mi":["if","y"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},"mo":"=","mi":"feature"},"mo":"\u2019"},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mi":["s","tag","and"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"munder":{"mi":["x","_"]}},"mo":"=","mi":"feature"},"mo":"\u2019"},"mo":["\u2062","\u2062","\u2062"],"mi":["s","context"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}}]},{"mtd":[{"mrow":{"mn":"0","mo":","}},{"mi":"otherwise"}]}]}}],"mo":"="}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"2"}}]}}}},"br":{},"sub":["i","i","1"],"sup":"i\u22121"},"Assuming a set of features  whose cardinality is F the probability assignment is made according to:",{"@attributes":{"id":"p-0037","num":"0041"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["p","\u039b"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"y","mo":"\u2758","munder":{"mi":["x","_"]}}}},{"mrow":[{"msup":{"mi":"Z","mrow":{"mo":"-","mn":"1"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"munder":{"mi":["x","_"]},"mo":",","mi":"\u039b"}}},{"mi":"exp","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"F"},"mo":"\u2062","mrow":{"msub":{"mi":["\u03bb","i"]},"mo":"\u2062","mrow":{"msub":{"mi":["f","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"munder":{"mi":["x","_"]},"mo":",","mi":"y"}}}}}}}],"mo":"\u00b7"}],"mo":"="}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"3"}}]},{"mtd":[{"mrow":{"mrow":[{"mi":"Z","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"munder":{"mi":["x","_"]},"mo":",","mi":"\u039b"}}},{"munder":{"mo":"\u2211","mi":"y"},"mo":"\u2062","mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"F"},"mo":"\u2062","mrow":{"msub":{"mi":["\u03bb","i"]},"mo":"\u2062","mrow":{"msub":{"mi":["f","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"munder":{"mi":["x","_"]},"mo":",","mi":"y"}}}}}}}}],"mo":"="}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"4"}}]}]}}},"br":{},"sub":["1 ","F"],"sup":"F "},{"@attributes":{"id":"p-0038","num":"0042"},"figref":"FIG. 3","b":"300"},"At step , the weights for the Maximum Entropy model are estimated. Under one embodiment, the model parameters \u039b={\u03bb. . . \u03bb}\u03b5Rare estimated such that the model assigns maximum log-likelihood to a set of training data subject to a Gaussian prior centered at zero that ensures smoothing. In other embodiments, different prior distributions can be used for smoothing, such as an exponential prior. Under one embodiment that uses Improved Iterative Scaling to determine the model parameters, this results in an update equation for each \u03bb of:\n\n\u03bb=\u03bb+\u03b4\u2003\u2003EQ. 5\n\nwhere \u03b4satisfies:\n",{"@attributes":{"id":"p-0040","num":"0044"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":{"munder":{"mo":"\u2211","mrow":{"munder":{"mi":["x","_"]},"mo":",","mi":"y"}},"mo":"\u2062","mrow":{"mrow":[{"mover":{"mi":"p","mo":"~"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"munder":{"mi":["x","_"]},"mo":",","mi":"y"}}},{"msub":{"mi":["f","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"munder":{"mi":["x","_"]},"mo":",","mi":"y"}}}],"mo":"\u2062"}},"mo":"-","mfrac":{"msub":{"mi":["\u03bb","i"]},"msubsup":{"mi":["\u03c3","i"],"mn":"2"}}},{"mfrac":{"msub":{"mi":["\u03b4","i"]},"msubsup":{"mi":["\u03c3","i"],"mn":"2"}},"mo":"+","mrow":{"munder":{"mo":"\u2211","mrow":{"munder":{"mi":["x","_"]},"mo":",","mi":"y"}},"mo":"\u2062","mrow":{"mrow":[{"mover":{"mi":"p","mo":"~"},"mo":"\u2061","mrow":{"mo":["(",")"],"munder":{"mi":["x","_"]}}},{"msub":{"mi":["p","\u039b"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"y","mo":"\u2758","munder":{"mi":["x","_"]}}}},{"msub":{"mi":["f","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"munder":{"mi":["x","_"]},"mo":",","mi":"y"}}},{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["\u03b4","i"]},"mo":"\u2062","mrow":{"msup":{"mi":["f","#"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"munder":{"mi":["x","_"]},"mo":",","mi":"y"}}}}}}],"mo":["\u2062","\u2062","\u2062"]}}}],"mo":"="}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"6"}}]}}}},"br":{},"sup":["#","2 "],"sub":["i","\u039b","i"]},"Although the update equations are shown for the Improved Iterative Scaling estimation technique, other techniques may be used to estimate the model parameters by maximizing the log-likelihood such as Generalized Iterative Scaling, Fast Iterative Scaling, Gradient Ascent variants, or any other known estimation technique.","Once the weights of the Maximum Entropy model have been trained, strings of text that are to be capitalized are received at step . At step , the trained maximum entropy weights are used to find a sequence of capitalization forms for the sequence of words in a string of text that maximizes the conditional probability P(T|W). The sequence of capitalization that maximizes this probability is selected as the capitalization for the string of text.","The search for the sequence of tags that maximizes the conditional probability may be performed using any acceptable searching technique. For example, a Viterbi search may be performed by representing the possible capitalization forms for each word in a string as a trellis. At each word, a score is determined for each possible path into each capitalization form from the capitalization forms of the preceding word. When calculating these scores, the past capitalization forms used in the maximum entropy features are taken from the capitalization forms found along the path. The path that provides the highest score into a capitalization form is selected as the path for that capitalization form. The score for the path is then updated using the probability determined for that capitalization form of the current word. At the last word, the path with the highest score is selected, and the sequence of capitalization forms along that path is used as the capitalization forms for the sequence of words.","Although a Maximum Entropy model is used above, other models that use an exponential probability may be used to determine the conditional probability under other embodiments of the present invention. For example, Conditional Random Fields (CRF) may be used.","Under some embodiments of the present invention, a Maximum Entropy model is trained on a large set of background data and then adapted to a smaller set of specific data so that the model performs well with data of the type found in the smaller set of specific data.  provides a flow diagram of a method for adapting a Maximum Entropy model under the present invention and  provides a block diagram of elements used in adapting a Maximum Entropy model.","In step , a feature threshold count is selected. At step , this threshold count is used by a trainer  to select a set of features  based on the background training data . Under one embodiment, this involves counting the number of times each of a set of predefined features  occurs in background training data  and selecting only those features that occur more than the number of times represented by the threshold count.","At step , a variance for a prior Gaussian model is selected for each weight from a set of possible variances . At step , trainer  trains the weights of the maximum entropy model trained based on background training data  while using smoothing and the selected variances through Equations 5 and 6 identified above.","Note that in equations 5 and 6 above, an Improved Iterative Scaling technique was used to estimate the weights that maximize the log-likelihood. Step  is not limited to this estimation technique and other estimation techniques such as Generalized Iterative Scaling, Fast Iterative Scaling, Gradient Ascent, or any other estimation technique may be used to identify the weights.","At step , trainer  determines if there are more variances in the set of variances  that should be evaluated. Under the present invention, multiple sets of weights are trained using a different set of variances for each set of weights. If there are more sets of variances that need to be evaluated at step , the process returns to step  and a new set of variances is selected before a set of weights is trained for that set of variances at step . Steps ,  and  are repeated until there are no more sets of variances to be evaluated.","When there are no further sets of variances to be evaluated at step , the process determines if there are more threshold counts to be evaluated at step . If there are more threshold counts, a new threshold count is selected at step  and steps , , , and  are repeated for the new threshold count. By using different threshold counts, different features sets are used to construct different maximum entropy models.","When there are no further threshold counts to be evaluated at step , a set of possible models  has been produced, each with its own set of weights. A selection unit  then selects the model that provides the best capitalization accuracy on background development data  at step . The selected model forms an initial background model .","At step , feature threshold count is again selected and at step , the feature selection process is repeated for a set of adaptation training data  to produce adaptation features . This can result in the same set, although generally it will produce a super-set of features from those selected at step .","At step , a set of variances for a prior model is once again selected from the collection of variances . Using the selected set of variances, adaptation training data , and the weights of initial background model , an adaptation unit  trains a set of adapted weights at step . Under one embodiment, a prior distribution for the weights is modeled as a Gaussian distribution such that the log-likelihood of the adaptation training data becomes:",{"@attributes":{"id":"p-0054","num":"0058"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u039b"}},{"mrow":[{"munder":{"mo":"\u2211","mrow":{"munder":{"mi":["x","_"]},"mo":",","mi":"y"}},"mo":"\u2062","mrow":{"mrow":[{"mover":{"mi":"p","mo":"~"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"munder":{"mi":["x","_"]},"mo":",","mi":"y"}}},{"msub":{"mi":["p","\u039b"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"y","mo":"\u2758","munder":{"mi":["x","_"]}}}}],"mo":["\u2062","\u2062","\u2062"],"mi":"log","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},{"mover":{"munder":{"mo":"\u2211","mrow":{"mi":"i","mo":"-","mn":"1"}},"mi":"F"},"mo":"\u2062","mfrac":{"msup":{"mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["\u03bb","i"]},"mo":"-","msubsup":{"mi":["\u03bb","i"],"mn":"0"}}},"mn":"2"},"mrow":{"mn":"2","mo":"\u2062","msubsup":{"mi":["\u03c3","i"],"mn":"2"}}}},{"mi":"const","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u039b"}}],"mo":["-","+"]}],"mo":"="}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"7"}}]}}}},"br":{}},{"@attributes":{"id":"p-0055","num":"0059"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mo":["(",")"],"mrow":{"mover":{"munder":{"mo":"\u2211","mrow":{"mi":"i","mo":"-","mn":"1"}},"mi":"F"},"mo":"\u2062","mfrac":{"msup":{"mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["\u03bb","i"]},"mo":"-","msubsup":{"mi":["\u03bb","i"],"mn":"0"}}},"mn":"2"},"mrow":{"mn":"2","mo":"\u2062","msubsup":{"mi":["\u03c3","i"],"mn":"2"}}}}},"mo":","}}},"br":{},"b":["516","412","500","400","520","410","409","410"]},"Using this prior model and an Improved Iterative Scaling technique, the update equations for training the adapted weights at step  become:\n\n\u03bb=\u03bb+\u03b4\u2003\u2003EQ. 8\n\nwhere \u03b4satisfies:\n",{"@attributes":{"id":"p-0057","num":"0061"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":{"munder":{"mo":"\u2211","mrow":{"munder":{"mi":["x","_"]},"mo":",","mi":"y"}},"mo":"\u2062","mrow":{"mrow":[{"mover":{"mi":"p","mo":"~"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"munder":{"mi":["x","_"]},"mo":",","mi":"y"}}},{"msub":{"mi":["f","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"munder":{"mi":["x","_"]},"mo":",","mi":"y"}}}],"mo":"\u2062"}},"mo":"-","mfrac":{"mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["\u03bb","i"]},"mo":"-","msubsup":{"mi":["\u03bb","i"],"mn":"0"}}},"msubsup":{"mi":["\u03c3","i"],"mn":"2"}}},{"mfrac":{"msub":{"mi":["\u03b4","i"]},"msubsup":{"mi":["\u03c3","i"],"mn":"2"}},"mo":"+","mrow":{"munder":{"mo":"\u2211","mrow":{"munder":{"mi":["x","_"]},"mo":",","mi":"y"}},"mo":"\u2062","mrow":{"mrow":[{"mover":{"mi":"p","mo":"~"},"mo":"\u2061","mrow":{"mo":["(",")"],"munder":{"mi":["x","_"]}}},{"msub":{"mi":["p","\u039b"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"y","mo":"\u2758","munder":{"mi":["x","_"]}}}},{"msub":{"mi":["f","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"munder":{"mi":["x","_"]},"mo":",","mi":"y"}}},{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["\u03b4","i"]},"mo":"\u2062","mrow":{"msup":{"mi":["f","#"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"munder":{"mi":["x","_"]},"mo":",","mi":"y"}}}}}}],"mo":["\u2062","\u2062","\u2062"]}}}],"mo":"="}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"9"}}]}}}},"br":{},"b":["518","518"],"sub":["i","\u039b"]},"The effect of the prior probability is to keep the model parameters \u03bbclose to the model parameters generated from the background data. The cost of moving away from the initial model parameters is specified by the magnitude of the variance \u03c3, such that a small variance will keep the model parameters close to the initial model parameters and a large variance will make the regularized log-likelihood insensitive to the initial model parameters, allowing the model parameters to better conform to the adaptation data.","In situations where a feature is not present in adaptation training data  but is present in background training data , the weight for the feature is still updated during step .","At step , the method determines if there are more sets of variances to be evaluated. If there are more sets of variances to be evaluated, the process returns to step  and a new set of variances is selected. Another set of weights is then adapted at step  using the new sets of variances and the weights of initial background model . Steps , , and  are repeated until there are no further variances to be evaluated.","When there are no further sets of variances to be evaluated at step , the process determines if there are further feature threshold counts to be evaluated at step . If there are further feature counts, a new feature count is selected at step  and steps , ,  and  are repeated for the new threshold count.","Steps , , and  produce a set of possible adapted models . At step  the adapted model that provides the highest log-likelihood for a set of adaptation development data  using Equation 7, is selected by a selection unit  as the final adapted model .","Although in the description above, a Gaussian prior distribution was used in the log likelihood determinations of Equation 7, those skilled in the art will recognize that other forms of prior distributions may be used. In particular, an exponential prior probability may be used in place of the Gaussian prior.","Although the adaptation algorithm has been discussed above with reference to capitalization, it may be applied to any classification problem that utilizes a Maximum Entropy model, such as text classification for spam filtering and language modeling.","By allowing the model weights to be adapted to a small set of adaptation data, it is possible to train initial model parameters for the Maximum Entropy model and place those model parameters in a product that is shipped or transmitted to a customer. The customer can then adapt the Maximum Entropy model on specific data that is in the customer's system. For example, the customer may have examples of specific types of text such as scientific journal articles. Using these articles in the present adaptation algorithm, the customer is able to adapt the Maximum Entropy model parameters so they operate better with scientific journal articles.","Although the present invention has been described with reference to particular embodiments, workers skilled in the art will recognize that changes may be made in form and detail without departing from the spirit and scope of the invention."],"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF ILLUSTRATIVE EMBODIMENTS"],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 5"}]},"DETDESC":[{},{}]}
