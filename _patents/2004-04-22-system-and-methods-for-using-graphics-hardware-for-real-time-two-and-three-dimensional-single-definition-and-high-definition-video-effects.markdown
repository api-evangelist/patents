---
title: System and methods for using graphics hardware for real time two and three dimensional, single definition, and high definition video effects
abstract: A method, system, and computer readable medium including instructions for processing single definition or high definition video data to produce an two dimensional and three dimensional effects to occur at a future time. The effects are created in a video processing system using multiple threads.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07286132&OS=07286132&RS=07286132
owner: Pinnacle Systems, Inc.
number: 07286132
owner_city: Mountain View
owner_country: US
publication_date: 20040422
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["DESCRIPTION","SUMMARY","DETAILED DESCRIPTION"],"p":["1. Field","The present invention generally relates to video and graphics processing systems, and more particularly, to methods and systems for using graphics hardware for real time two and three dimensional, single definition, and high definition video effects.","2. Background","Today's reduced prices for electronic equipment and various technological advances make high tech electronic equipment available to a wide majority of consumers. This is especially true in the area of video processing, recording, capturing, and editing. Since personal computers are in widespread use, consumers can easily view and record video data on personal computer and capture video data from the video devices. Further, with the increase in the processing speed of personal computers, consumers can edit video data, such as adding effects or graphics, and view video which has effects or graphics on personal computers.","Most personal computers have both a central processing unit (\u201cCPU\u201d) and a graphics card. Modern graphics cards include a graphics processing unit (\u201cGPU\u201d) and video memory separate from the CPU and system memory. Conventionally, video editing comprises receiving multiple input streams from video or still images on a timeline and combining these streams with effects (e.g., transitions or clip effects) in order to create a single video output file. This process utilizes the CPU or GPU to perform several tasks.",{"@attributes":{"id":"p-0007","num":"0006"},"figref":"FIG. 1","b":["100","102"]},"Next, the computer sequences the effects that are involved in one frame result to determine the most desirable setup (stage ). During this step, in order to avoid mixing GPU\/CPU effects, the computer may replace GPU effects with CPU effects, or vice versa, replacing compatible effects if possible. Then, if the sequence of effects will overly tax resources, such as memory, the computer divides the task to sequence the effects and reduce complexity (stage ).","Next, the computer transfers the decoded video data to a graphics card for processing by the GPU (stage ). Then, from the decoded video and sequenced effects, the graphics card creates intermediate data, for example, polygon models and timeline information, needed to render a frame (stage ).","Next, the GPU renders the intermediate result of the decompressed video data and effects (stage ). Then, the computer determines if CPU\/GPU mixing is needed (stage ). If mixing is required, the CPU will read back the rendered intermediate result for processing. If this is the case, the process returns to stage . However, if read back is required, video editing in real time will not be possible.","After all the processing is performed, the finalized video data including the effects is displayed (stage ). For example, the video including the effects may be displayed on a computer monitor.","Sometimes the edited data with the effects may need to be saved (stage ). For example, the edited data may be needed as background rendered content. If the edited video data needs to be saved, the data is read back to the CPU (stage ). If read back is required, video editing in real time will not be possible. Then, the computer compresses the read back video data which is CPU intensive (stage ). The compressed data may be stored for later use.","In the above-mentioned video editing process, the video data received in stage  may be obtained in several ways. One method for obtaining the data is by capturing video data with a computer from a video device, such as a digital video or still camera.  illustrates a process for capturing video from a video device. First, the computer reads the video data from the device (stage ). The computer can read the data via a conventional data port, such as IEEE1394 (Firewire) or USB2. The video data may include video footage from a video camera, still images from a camera, or a video signal from a cable or satellite TV system.","Then, the computer stores the data on a storage device, such as a magnetic hard drive (stage ). Next, since most video data captured from video devices is encoded according to conventional format, such as M.P.E.G., the computer decodes the video data so that it is capable of being displayed (stage ). Next, the computer transfers the decoded video data to the graphics card (stage ). Finally, the graphics card processes the decoded data and the data is displayed (stage ).","In the above method for video editing, editing of video in real time may be prevented because of a need to read back data. Further, there exist several drawbacks that tax the resources of the computer. For example, the CPU may be using nearly all available processing time for decompression and the GPU is spending processing time in creating the effects. Further, every call into a graphics application programming interface (e.g., direct X) may have highest priority from the operating system (\u201cOS\u201d). Since a stall in the video device driver may be a \u201cRing 0\u201d stall, the interface call creates \u201cdead CPU time.\u201d In other words, as the GPU is processing and creating the effect, the video device driver awaits the results for display. Since the video device driver has highest priority, all other computer processes are delayed while the video device driver waits for the GPU to finish processing. Thus, when the driver is waiting for a result from the GPU, the computer loses CPU cycles, which reduces the availability of the CPU for decoding. Additionally, read backs from the graphics card may be asynchronous read backs, which may stall the CPU until the read back is finished.","In the past, most video signals were standard definition (\u201cSD\u201d) video, having a rate of 720\u00d7480\u00d72\u00d730=41 MB\/s per stream. But with the advent of high definition (\u201cHD\u201d) video, having a resolution of 1080 p, the incoming data rate may be 1920\u00d71080\u00d72\u00d760=250 MB\/s per stream. This introduces much more data into the video capture and editing process. Accordingly, the above mentioned drawbacks are amplified.","In response to these problems, several specialized products have been created to deal with video editing in real time. For example, the Pinnacle ProONE system creates real time effects using separate hardware. The Matrox Flex3D system creates real time effects with specialized graphics and decoder boards. The Silicon Graphics Octane system creates real time effects using specialized graphics hardware. The Softimage DS system edits video in 3D, but is unable to play in real time. The Avid Real Vision HD10 utilizes specialized hardware to create real time effects. However, in all these products, specialized hardware decoders, graphics boards, or specialized computers are required for real time video editing. Thus, conventional personal computers and other general computing devices are incapable of performing real time video without specialized hardware.","Accordingly, the present invention is directed to systems and methods for using graphics hardware for real time two and three dimensional, single definition, and high definition video effects which obviates one or more of the limitations and disadvantages of the related art.","In accordance with aspects consistent with the present invention, methods, systems, and computer readable media including instructions for performing a method for processing video data to produce an effect to occur at a future time, comprising: implementing an application thread for creating the effect to be added to the video data, generating pre-decompressed video data from the video data, and determining parameters which describe the effect; implementing an upload thread for uploading the pre-decompressed video data into video hardware; implementing a decoding thread for decoding the pre-decompressed video data to produce decoded video data; implementing a render thread rendering the effect in the decoded video data to produce output video data; and implementing a presenter thread presenting the output video data.","In accordance with aspects consistent with the present invention, methods, systems, and computer readable media including instructions for performing a method for processing video data to produce an effect to occur at a future time, comprising the steps of: receiving the video data; creating the effect; generating pre-decompressed video data from the video data; uploading the pre-decompressed video data into video hardware; decoding the pre-decompressed video data to produce decoded video data; determining parameters which describe the effect; rendering the effect in the decoded video data to produce output video data; and presenting the output video data.","Additional aspects related to the invention will be set forth in part in the description which follows, and in part will be obvious from the description, or may be learned by practice of the invention. Aspects of the invention may be realized and attained by means of the elements and combinations particularly pointed out in the appended claims.","It is to be understood that both the foregoing and the following descriptions are exemplary and explanatory only and are not intended to limit the claimed invention in any manner whatsoever.","Reference will now be made in detail to exemplary embodiments of the present invention, examples of which are illustrated in the accompanying drawings. Wherever possible, the same reference numbers will be used throughout the figures to refer to the same or like elements. The accompanying figures illustrate exemplary embodiments and implementations consistent with the present invention, which are described in sufficient detail to enable those skilled in the art to practice the invention. The description of the exemplary embodiments does not indicate or imply that other embodiments or implementations do not fall within the scope of the present invention. It is to be understood that other implementations may be utilized and that structural and method changes may be made without departing from the scope of the present invention.","Overview","A process consistent with the present invention relates to generating a video \u201ceffect\u201d, supplying the effect's parameters, uploading samples and bitmaps, decoding samples, rendering the effect, outputting the effect, and releasing resources. The process is tuned to utilize the CPU and the GPU in a manner that allows enhanced computing efficiency and allows the introduction and editing of effects in real time.","Enhanced computing efficiency is achieved by avoiding serialization of processing. This may be achieved by utilizing multiple threads to maximize computing efficiency. Each of the processes from uploading, decoding, rendering, and presenting are performed by an independent thread, which allows parallel processing in the effect generation. Further, computing efficiency is increased by avoiding GPU related driver stalls by accessing edited video only when the edited video is completed. This may be achieved by using a query process or a non stalling lock instead of calling the driver blind or delaying commands to avoid conflicts between graphics hardware and the CPU. Additionally, instead of using the CPU to copy the data into the graphics hardware, Bus Mastering may be utilized to avoid direct access delays.","Several terms are utilized throughout the description. The following outline of the terms provides a general overview of the meaning of each term. However, the overview of the terms is not intended to limit the terms to the examples provided, but are intended to cover all equivalents recognized in the art.\n\n",{"@attributes":{"id":"p-0037","num":"0051"},"figref":"FIG. 3","b":["300","300"]},"Process  begin by implementing each thread, application, upload, decoder, render, presenter, and release, which will be used by process  (stage ). Then, the processing system passes the sample to the application thread which creates the effects requested for the video (stage ). Then, the application thread generates \u201cpre-decomposed video.\u201d Pre-decomposed video is video data which is compressed by a format such as MPEG and may include video data packets and other static images. Next, the processing system passes the sample to the upload thread which uploads the pre-decomposed video to the graphics card (stage ). To reduce the work in the processing system, the upload thread may utilize a Bus Mastering processes to upload the video data into the graphics card.","Then, the processing system passes the sample to the decoder thread which performs final decoding of the video data (stage ). That is, the video data which may be compressed in a format such as MPEG is decoded into raw video data.","Subsequently, the processing system passes the sample to a render thread which generates the effect data and renders the effects (stage ). Once the effect is rendered, the processing system passes the rendered sample presenter thread which outputs the rendered video sample (stage ). Then, once the video has been output, the processing system passes any used resources to the release thread which releases the resources used in editing the video (stage ).","In process , in any of the above stages, the threads may be performed in parallel utilizing multiple processing units in the processing system. For example, the decoder thread (stage ) and render thread (stage ) may be performed by different processing units on the processing system.","In process , since each of the processes from uploading, decoding, rendering, and presenting are performed by an independent thread, the processing system may perform processing in parallel in order to generate the effect. Moreover, after each thread completes its respective processing, the thread may perform a snooping command to indicate that the process is completed. Further, since the completion of each process is noted, computing efficiency may be increased by avoiding GPU related driver stalls by accessing edited video only when the edited video is completed.","Exemplary Environment and Process",{"@attributes":{"id":"p-0044","num":"0058"},"figref":"FIG. 4","b":["400","400","401","422","424","402","418"]},"Video processing unit  may be a personal computer, mobile computing device (e.g., a PDA), mobile communications device (e.g., a cell phone), set top box (e.g., cable or satellite box), video game console, smart appliance, or any other structure that enables a user to receive and process video data. In one exemplary configuration, video processing unit  may include data ports , a storage module , a CPU , a memory , a graphics module , and a network interface , interconnected by at least one bus .","In environment , input device  is coupled to data ports . Input device  may include at least one user-actuated input mode to input commands and thereby select from a plurality of processor operating modes. Input device  may include components such as a keyboard, a mouse, and\/or a touch screen. Additionally, as mentioned above, input device  includes one or more audio capture devices. For example, input device  may include a microphone to which a user can input audible utterances. Accordingly, input device  may include or be coupled to voice recognition software for recognizing and parsing inputted utterances. The voice recognition software could reside in memory . Input device  may additionally or alternatively include a data reading device and\/or an input port.","Video input device  is also coupled to data ports . Video input device  may include at least a video camera, a still camera, or any other appropriate video production device. Additionally, video input device  could include one or more video capture devices (e.g., scanners), video recorders, or any device capable of supplying video data.","Storage module  may provide mass storage for video processing unit . Storage module  may be implemented with a variety of components or subsystems including, for example, a hard drive, an optical drive, CD ROM drive, DVD drive, a general-purpose storage device, a removable storage device, and\/or other devices capable of storing information. Further, although storage module  is shown within video processing unit , storage module  may be implemented external to video processing unit .","Storage module  may include program code and information for video processing unit  to communicate with network , input device , and video input device . Storage module  may include, for example, program code for various client applications and an Operating System (OS), such as the Windows Operation System provided by Microsoft Corporation. In addition, storage module  may include other program network communications, kernel and device drivers, configuration information, video display and editing, and other applications that might be installed on video processing unit .","CPU  in video processing unit  may be operatively configured to execute instructions. CPU  may be configured for routing information among components and devices and for executing instructions from memory . Although  illustrates a single CPU, video processing unit  may include a plurality of general purpose processors and\/or special purpose processors (e.g., ASICS). CPU  may also include, for example, one or more of the following: a co-processor, memory, registers, and other processing devices and systems as appropriate. CPU  may be implemented, for example, using a Pentium\u2122 processor provided from Intel Corporation.","Memory  may include any system and\/or mechanism capable of storing information. Memory  may be embodied with a variety of components and\/or subsystems, including a random access memory (\u201cRAM\u201d), a read-only memory (\u201cROM\u201d), magnetic and optical storage elements, organic storage elements, audio disks, and video disks. Memory  may provide a primary memory for CPU , such as for program code. Memory  may, for example, include program code for an Operating System (\u201cOS\u201d), such as the Windows Operation System provided by Microsoft Corporation, network communications, kernel and device drivers, configuration information, video display and editing, and other applications that might be installed on video processing unit .","Although a single memory is shown, any number of memory devices may be included in video processing unit , and each may be configured for performing distinct functions. When video processing unit  executes an application installed in storage module , CPU  may download at least a portion of program code from storage module  into memory . As CPU  executes the program code, CPU  may also retrieve additional portions of program code from storage module .","Graphics module  may include any system and\/or mechanism capable of processing video and graphics data and outputting video and graphics data. Graphics module  may be embodied with a variety of components and\/or subsystems, including a GPU  and a video memory . Graphics module  may be implemented, for example, using any appropriate graphics accelerator card compliant with various standards, such as AGP, PCI, or PCI Express.","GPU  may be operatively configured to execute instructions related to video and graphics. GPU  may be configured for routing information among components and devices and for executing instructions from CPU  and memory . GPU  may include one or a plurality of general purpose processors and\/or special purpose processors. GPU  may also include, for example, one or more of the following: a co-processor, memory, registers, and other processing devices and systems as appropriate.","Video memory  may include any system and\/or mechanism capable of storing information. Video memory  may be embodied with a variety of components and\/or subsystems, including a RAM and\/or a read-only memory ROM. Video memory  may provide a primary memory for GPU . Although a single memory is shown, any number of memory devices may comprise video memory , and each may be configured for performing distinct functions.","Video processing unit  may be connected to network  via network interface  which may be operatively connected via a wired and\/or wireless communications link. Network interface  may be any appropriate mechanism for sending information to and receiving information from network , such as a network card and an Ethernet port, or to any other network, such as an attached Ethernet LAN, serial line, etc. In one configuration, network interface  may allow video processing unit  to interact with processing units as well as the Internet.","Network  may be the Internet, a virtual private network, a local area network, a wide area network, a broadband digital network, or any other structure for enabling communication between two or more nodes or locations. Network  may include a shared, public, or private data network and encompass a wide area or local area. Network  may include one or more wired and\/or wireless connections. Network  may employ communication protocols, such as Transmission Control and Internet Protocol (TCP\/IP), Asynchronous Transfer Mode (ATM), Ethernet, or any other compilation of procedures for controlling communications among network locations. In certain embodiments, network  may also include and\/or provide telephone services. In such embodiments, network  may be included and\/or leverage a Public Switched Telephone Network (\u201cPSTN\u201d). Alternatively, network  may leverage voice-over Internet Protocol (\u201cVoIP\u201d) technology. In certain implementations, network  may include and\/or leverage PSTN and VoIP technology.","Output device  may be configured to visually display text, images, or any other type of information output by graphics module  by way of a cathode ray tube, liquid crystal, light-emitting diode, gas plasma, or other type of display mechanism. For example, output device  may be a computer monitor. Output device  may additionally or alternatively be configured to audibly present information. For example, output device  could include an audio output device, such as a speaker, for outputting audible sounds to a user. Accordingly, output device  may include or be coupled to audio software configured to generate synthesized or pre-recorded human utterances. Such software could reside in memory  and be configured to interact. Output device  may be used in conjunction with input device  for allowing user interaction.","As mentioned above, video processing unit  may comprise additional and\/or fewer components than what is shown in , and one or more of the components implanted in video processing unit  may be scalable in order to accommodate additional services, data, and\/or users.","Although  depicts the various components residing entirely in video processing unit , it should be understood that one or more of the components of video processing unit  may exist in or be distributed among one or more other processing units (not shown), or other locations, coupled to network . For example, applications could reside in other processing units and storage module  may reside external to video processing unit  and may be coupled to video processing unit  via network . It should also be understood that, as mentioned above, any number of processing units may be included in environment .","In alternative implementations of the instant invention, each of the plurality of other processing units (not shown) may contain a replica or version of all or part of video processing unit  respectively. In such implementations, each version may operate exclusively from or collaboratively with each other.",{"@attributes":{"id":"p-0062","num":"0076"},"figref":["FIGS. 5A and 5B","FIG. 4"],"b":["500","500","400"]},"The effects and video editing occur in response to an application executing on video processing unit . The application may be initiated by the user of video processing unit  through input device . Additionally, the application may constantly be running on video processing unit  and perform video editing for any received video. Also, the application may be initiated by the input or receipt of video data. Exemplary applications may include any appropriate type of program designed to perform a specific function for one or more users or other devices utilizing video or graphics data and a 3D-Server. The application may comprise, but is not limited to, one or more of a word processor; a database program; an internet, extranet, and\/or intranet browser or website; a development tool; a scheduling tool; a routing tool; a communication tool; a menu interface; a video display program; a game program including video or 2D\/3D effects; a video recording program; and an audio and\/or video editing program. The application may be a compilation of instructions for manipulating data written in any structural, procedural, object-oriented, or other type of programming language. As illustrated, the application may comprise a user interface, such as a GUI for facilitating using interaction with the application.","The application may be stored in storage module  and\/or memory . Additionally, the application may be received over network  or from input device . When an application is installed in storage module , CPU  may download at least a portion of program code from storage module  into memory . As CPU  executes the program code, CPU  may also retrieve additional portions of program code from storage module .","Video data to be processed may be obtained or received by various components of environment . Video data may be obtained or received from the video input device  or network . Additionally, video data may be stored in storage module  and\/or memory  and accessed when the application is initiated or created by the application when the application is initiated.","After the application has been initiated, process  begins. In process , a set of multiple threads are utilized to perform the various editing processing stages. First, the application creates the effect (the effect may be a single effect or multiple effects) ahead in play time (i.e., before the effect is scheduled to be displayed or utilized) in a manner known in the art (stage ). The application may create the effect using single and\/or multiple application threads depending on the complexity or number of the effects. An example of an effect would be PIP to combine separate video data or a menu in a Digital Versatile Disk (DVD). Then, the application uploads a textual description of the effect into the 3D-Server (stage ). The application instantiates, interprets, and prepares the effect (stage ). The 3D-Server may be an application extension which is part of a video editing program such as Pinnacle Studio or Liquid.","Next, the application generates or identifies pre-decompressed video data which is part of a timeline. The timeline may be video data of any length such as a full length moving video or a single still picture. The application may generate or identify the video data using a single and\/or multiple application threads. The pre-decompressed video data may include video packets and\/or other static images, such as titles. The timeline may be divided into different tracks. For example, in a full length moving video, each track may be a single frame of video data. For every track in the timeline, the application will read or generate all samples of video data needed to prepare the effect, determine if the samples are compressed, possibly partially decode the samples, and pass the samples to the 3D Server\/upload thread. For example, the application may read the sample of video data from storage module  or video input device . Additionally, video data, such as textual data, may be generated by the application.","The application allocates a sample buffer for the first sample read or generated (stage ). The sample buffer may be contained in memory  or the memory of storage module . Then, the application determines if the sample is compressed (stage ). If the sample is compressed, the application may partially decode the compressed sample.","If the sample is compressed, the application reads the sample into the decoding buffer (stage ). The decoding buffer may be contained in memory  or the memory of storage module . Then, the application reads the partially decoded sample into the sample buffer (stage ). Next, the application passes the sample buffer to the upload thread (stage ).","If the sample is not compressed, the application reads the sample into the sample buffer (stage ). Next, the application passes the sample buffer to the upload thread (stage ).","The effect created may have more than one sample. The application determines if all the samples have been read (stage ). If all the samples have not been read, the application repeats allocating the sample buffer and determining whether the sample is compressed (stages -). After all the samples have been read and passed to the upload thread, the upload thread proceeds with upload processing.","Next, the sample object is uploaded into video memory  of graphics hardware . The sample object initially includes the AGP memory surface. A significant amount of time may be required for allocation of the sample object, depending on the availability and status of the surface. Availability and status of the surface may be checked using the surface's associated snooping command. A snooping command may be a dummy command, which is supported by a GPU and 3D application program interfaces (\u201cAPI\u201d), which could be inquired to determine if a process has been executed by the GPU. Alternatively, a snooping command may be emulated by either using an API's \u201cin use\u201d inquire or simply waiting some time (e.g., up to 5 ms, depending on when the surface was used).","The upload process may be performed by an upload thread of the 3D-Server. The 3D server\/upload thread waits for a command from the application (stage ). As soon as the upload thread gets the sample object, command packet, and sample buffers, the upload thread will wait until a video memory surface required by the upload thread's specification becomes available (stage ). For example, the sample object may require a certain amount of memory or a color space type. If the required size or color space type is not available in video memory , the upload thread will wait until the memory becomes available. Next, the upload thread will check the video memory surface for pending operations (stage ). The surface may be checked by using an outstanding snooping command or a 3D API \u201cin use\u201d inquiry. Then, if the surface is ready, the upload thread issues the upload command (stage ). Since the effect is being prepared ahead in time, graphics module  is not awaiting the upload of the sample object. For example, the upload thread may use a Bus Mastering process to copy the data to the video memory surface. Further, the upload thread will note the upload command execution time and a snooping command will be issued and noted on both memory surfaces (stage ).","Then, the upload thread determines if the sample contained in the sample object is compressed (stage ). For example, the video data may be compressed using a standard format such as MPEG. If the video data contained in the sample object does not require decompression, the upload will signal the sample object as uploaded (stage ). Then, a new video memory surface containing the pre-decompressed video data will be attached to the sample object and the sample object and a command packet are passed to a render thread (stage ). Then, the upload thread places the AGP memory surface, if not used for other purposes, at the end of the free surface list, thereby releasing any waiting process (stage ).","If the sample requires decoding, the decoder thread is used to decompress the data. The upload thread places the sample object and a command packet in the decoder thread queue (stage ). Then, the upload thread places the AGP memory surface, if not used for other purposes, at the end of the free surface list, thereby releasing any waiting process (stage ).","Then, the upload thread will return to stage  to begin processing over for the next sample object which requires uploading.",{"@attributes":{"id":"p-0077","num":"0091"},"figref":"FIG. 6","b":["602","604","416"]},"Then, since the surface must be free of outstanding action, the decoder thread may determine whether the new video memory surface contains any outstanding actions by issuing a snooping command (stage ). Then, the decoder thread issues a command and performs the decoding process (stage ). The decoding process may include either \u201ciDCT\u201d (Inverse Discrete Cosine Transform) or \u201ciDCT+Motion Comp.\u201d iDCT\/Motion Comp is a separate hardware unit on most GPU and may be included in GPU , which allows the decode process to be executed in parallel to 3D commands. If necessary, after these commands have been issued, the command time and a snooping command will be issued and noted in both surfaces (stage ). Then, the decoder thread will signal the sample object as uploaded (stage ).","Finally, the decoder thread will attach the new video memory surface to the sample object and the decoder thread places the video memory surface containing the pre-decompressed video into the end of the free buffers list (which will release processes waiting for this kind of surface) and the new video memory surface will be attached to the sample object (stage ). Additionally, the sample object and a command packet are passed to a render thread (stage ). The decoder thread will then check the queue and begin decoding processing on a new sample object.","While the uploading and decoding are executed, the application thread may continue determining the effect parameters for a certain point in time. The application thread's determination may include collecting all required effects, calculating the time dependent data, and determining the used sample objects for the effects (stage , ). The application then passes these effect parameters to a render thread, and, in response, the render thread returns an output sample object which can then be used as reference to determine when the sample object with the effect is completed (stage ). Since the 3D-Server has not defined which physical target surface will be used for the output sample object, the output sample object is only a proxy.","Next, the render thread renders the effect.  illustrates a process for rendering the effect consistent with an aspect of the present invention. The rendering process may be performed by a render thread of the 3D-Server. The sample object and the command packet are passed to the render thread and placed in the render queue. The render queue will check the render queue for a sample object and command packet (stage ). If a sample object and a command packet are in the queue, the render thread will proceed with the rendering process. If not, the render thread will await a sample object and command packet. Additionally, the render thread may await several sample objects and command packets before performing rendering depending on the type of effect being rendered.","The render thread will prepare each used effect according to the effect parameters provided by the application (stage ). Next, the render thread will then assign a target memory surface to the output sample object (stage ). The target memory surface may, for example, be included in video memory . The target memory surface may also be included in memory . Then, the render thread waits until the sample object for the effect is ready for rendering (stage ). Then, the render thread will render the effect according to the effect parameters (stage ). The render thread will issue a snooping command and attach it to the sample object (stage ). Then, the render thread passes a command packet to a release thread in order to allow a process for releasing surfaces and resources (stage ).","Once the video needs to be displayed, the snooping command is executed and the 3D-Server passes the output sample object and a command to a presenter thread where they are placed in a queue for presentation (stage ). Since snooping commands have been executed by each thread, video processing unit  knows that each process thread has completed its designated task. The render thread will then check the queue and begin render processing on a new sample object.",{"@attributes":{"id":"p-0084","num":"0098"},"figref":"FIG. 8","b":["802","804","806","412","418","808"]},{"@attributes":{"id":"p-0085","num":"0099"},"figref":"FIG. 9","b":["902","500"]},"Process  was described with respect to a video frame with one effect. However, process  may be executed for multiple video frames with multiple effects. Furthermore, since the effect is created ahead in play time, the application and 3D-Server simultaneously perform over threads for processing and outputting video data for the current video play time.","Other embodiments of the invention will be apparent to those skilled in the art from consideration of the specification and practice of the invention disclosed herein. It is intended that the specification and examples be considered as exemplary only, with a true scope and spirit of the invention being indicated by the following claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The accompanying drawings, which are incorporated in and constitute a part of this specification exemplify certain aspects of the present invention and, together with the description, serve to explain some of the principles associated with the invention.",{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIGS. 5A and 5B"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":["FIG. 6","FIG. 5"]},{"@attributes":{"id":"p-0030","num":"0029"},"figref":["FIG. 7","FIG. 5"]},{"@attributes":{"id":"p-0031","num":"0030"},"figref":["FIG. 8","FIG. 5"]},{"@attributes":{"id":"p-0032","num":"0031"},"figref":["FIG. 9","FIG. 5"]}]},"DETDESC":[{},{}]}
