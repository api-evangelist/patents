---
title: Sentiment classification using out of domain data
abstract: Providing sentiment classification of out of domain data are disclosed herein. In some aspects, a source domain having a trained classifier is matched to a target domain having a target classifier. The trained classifier may include identifiers that may be used to predict the sentiment of opinion data for the source domain. The target classifier may use the identifiers of the trained classifier to determine the sentiment of opinion data for the target domain.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08942470&OS=08942470&RS=08942470
owner: Microsoft Corporation
number: 08942470
owner_city: Redwood
owner_country: US
publication_date: 20131206
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS REFERENCE To RELATED APPLICATION","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["This application is a continuation of co-pending, commonly owned U.S. patent application Ser. No. 12\/336,137, filed Dec. 16, 2008, now U.S. Pat. No. 8,605,966 entitled \u201cSENTIMENT CLASSIFIACATION USING OUT OF DOMAIN DATA,\u201d the entirety of which is herein incorporated by reference.","There are many ways for people to express opinions, submit feedback, or otherwise comment on various products, services, events, or other happenings. For example, with the popularity of Web 2.0 applications, an increasing number of people provide opinion data for others to consume. In addition to traditional forms of communication, many people contribute such opinion data via online forums, web blogs, and customer reviews, which provide a great wealth of information. Customer opinions are particularly valuable because they are often perceived to provide practical information about a subject, thus often being absent of puffery that may be used by companies when discussing their products and services. In addition, customer opinions typically include both positive and negative aspects of products and services which result in diverse opinion data.","It is often difficult to navigate through the vast amount of opinion data made available to customers, such as via the Internet. For example, a particular product may have hundreds of reviews, of which only a summary may be of particular interest to another customer. It is important to be able to sort through this information to organize people's opinions and perform analysis on the opinion data. This may result in better understanding of the wealth of information that makes up the opinion data.","One way to sort through the opinion data is to have humans manually categorize each entry into a number of logical categories that helps customers navigate though the data. For example, opinion data may be categorized as being a positive or negative opinion data. In addition, the opinion may be related to a particular product or service category and possibly further linked to subcategories. However, human categorization is very time consuming and expensive. Another way to categorize information is by requiring the author of the review to provide category information when the opinion data is generated. Although this may work very well in some instances, such as by asking a customer to rank the product (e.g., via stars, a numerical value, etc.) when submitting a customer review, this may not be available for all sources opinion data, such as forums, blogs, etc.","Techniques of sentiment classification of out of domain data are disclosed herein. Classifiers may be used to categorize data based on predetermined criteria and are tailored to operate for a particular domain of data. In some aspects, a source domain having a trained classifier is matched to a target domain having a target classifier. The trained classifier includes identifiers that may be used to predict the sentiment of opinion data for the source domain. The target classifier may use the identifiers of the trained classifier to determine the sentiment of opinion data for the target domain.","In other embodiments, the target classifier may use the identifiers to analyze opinion data and locate secondary identifiers, which have a strong correlation to the sentiment of the opinion data of the target domain. The secondary identifiers may be assigned a weight that is different than a weight assigned to the identifiers from the source domain.","In still further embodiments, the target classifier may include opinion data with a predetermined (known) sentiment. The target classifier may be used to evaluate the opinion data with the predetermined sentiment to determine whether the identifiers have sentiment prediction capabilities within the target domain. Some identifiers may be removed when they are determined to be inaccurate at predicting the predetermined sentiment of opinion data with the known sentiment.","This summary is provided to introduce simplified concepts of sentiment classification using out of domain data, which is further described below in the Detailed Description. This summary is not intended to identify essential features of the claimed subject matter, nor is it intended for use in determining the scope of the claimed subject matter.","Overview","Classifying large amounts of data is important to enable customers to quickly extract useful information from large quantities of textual information. A vast quantity of textual information that contains people's opinions (i.e., opinion data) is currently available, while additional textual information is relatively inexpensive and easy to obtain. It is desirable to automate classification of the opinion data before it becomes obsolete while minimizing costs and use of resources.","Trained domain classifiers enable automatic classification of opinion data for a particular domain. To maintain a high level of accuracy in classification, classifiers have to be developed for each domain, including new domains and\/or sub-domains. Techniques to develop trained classifiers for new domains using out of domain data are disclosed herein.","Illustrative Environment",{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 1","b":["100","102","104","104","1","102","1","104","2","104","102","2","102"]},"The opinion data  may be compiled from the sources  as a domain of unlabeled data . A domain  may include a grouping of the opinion data  for similar items, services, etc. For example, a first domain () may be limited to opinion data associated with \u201ccomputers, \u201d a second domain () may be used for \u201cbooks\u201d and another domain (M) may be used for video games, among the many possibilities of grouping opinion data by a domain. Unlabeled data is opinion data that is not classified. The domain of unlabeled data  may include opinion data that is collected from the sources . In some embodiments, the opinion data  may be obtained from the Internet (or other networks of computers). For example, Internet robots (bots) may be used to collect the opinion data  for a particular domain to populate the domain of unlabeled data . The bots may be implemented to collect data from e-commerce sites, blogs, forums, or other locations, which often include categories for data that may link to a domain, such as a forum for cameras or a customer review of a camera that link to a domain for cameras.","The opinion data of the domain of unlabeled data  may be categorized at a sorting operation . Although the disclosure may be used for any type of classification, sentiment classification is used as an illustrative example of one of many possible types of classifications. As such, sentiment classification is used to determine whether opinion data is positive or negative. For example, a sentence of text that describes a customer's opinion of an item may be analyzed to determine if the person liked the item (positive) or disliked the item (negative). In some embodiments, the opinion data may be sorted into one of multiple categories (), . . . , (P). Once the opinion data from the domain of unlabeled data is sorted, it becomes labeled data and is stored in a domain of labeled data .","As shown in the environment , the process of collecting opinion data and then categorizing it is conducted for each domain  of the many possible domains ()-(M). As further described below, automation of this process for each domain is desirable to more efficiently categorize the opinion data  from the sources  for each of the domains .",{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 2","b":["200","202","204","202","204"]},"Some domains may include identifiers that have specific meanings when used within the context of the domain, which are designated as domain identifiers . For example, the term \u201chooked\u201d may be a domain identifier that is a strong predictor of a positive sentiment for a domain of video game (e.g., \u201cI'm hooked on this video game and can't stop playing it.\u201d). However, \u201chooked\u201d may not be a predictor in another domain, such as a domain for computers. In contrast, common identifiers  are identifiers that are a consistent predictor of sentiment across many domains. The common identifiers  include generic terms which have the same meaning in many domains. For example, the terms \u201cgood\u201d and \u201cbad\u201d are common identifiers .","As shown in , words from the opinion data  may be selected and then designated as either the domain identifier  of the common identifier . For example, the first identifier  \u201cgreat\u201d and the second identifier  \u201cintuitive\u201d may be included in the common identifiers  because they have a consistent meaning across multiple domains. The third identifier  \u201clong lasting\u201d may be one of the domain identifiers  because this term may not be a consistent sentiment predictor in other domains. Finally, cues  may be selected that modify (e.g., negate, etc.) the identifiers. For example, the cue  \u201cnot\u201d is a negating term that makes the domain identifier \u201clong lasting\u201d a negative sentiment.",{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 3","b":["300","302","304","300","300"]},"Development of the domain classifiers  typically includes multiple processes. Initially, opinion data with a known sentiment (labeled data) is analyzed to determine identifiers. The trained classifiers (), (), . . . , (X) may include labeled data and the identifiers . Each of the trained classifiers is associated with a specific domain. For example, the trained classifier () may be for the domain \u201ccomputers,\u201d while the trained classifier () may be for the domain \u201cbooks.\u201d The identifiers include domain identifiers  and common identifiers .","Next, one of the trained classifiers  of a source domain is matched to one of the target classifiers  of a target domain. More specifically, a source domain  having one of the trained classifiers  is selected to train a target domain  having one of the target classifiers . The selection may compare attributes of the source domain  and the target domain  to ensure that the trained classifier of the source domain has enough common identifiers that may be used by the target classifier  to create accurate sentiment classification of opinion data. The selection may be performed by human selection, machine selection, or a combination of human and machine selection.","In some embodiments, the trained classifier may be carefully selected to match attributes of the target classifier, such that the identifiers associated with the trained classifier are relevant in the target classifier. For example, the trained classifier () for the domain \u201ccomputers\u201d may be matched with the target classifier () for the domain \u201cvideo games.\u201d Similarly, the trained classifier () for the domain \u201cbooks\u201d may be matched with the target classifier () for the domain \u201cmagazines.\u201d The common identifiers  are imported from the trained classifier  to the target classifier  as shown in .","In accordance with some embodiments, additional processing may occur when the target domain, such as the target domain (X), includes labeled data. For example, the target domain (X) may include a small sample of labeled data, which may be less than the amount of labeled data that was used to train previous classifiers such as the trained classifiers . The target classifier (X) may use the common identifiers () to predict the sentiment of additional unused labeled data during an evaluation process. This enables calibration of the classifier (e.g., addition or reduction of identifiers) to improve designation accuracy. For example, additional domain identifiers  may be identified for the target domain (X).","Finally, the classifier is designated as a trained classifier upon achieving satisfactory accuracy when determining the sentiment of the evaluation opinion data. Thus, the target domain (X), having the labeled data that may be used to calibrate the target domain may provide a more accurate sentiment classifier than the target domain () which does not include labeled data for evaluation and calibration.","Illustrative Operation",{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 4","b":["400","400","400"]},"At , the source domain  and the target domain  may be selected (or matched) as described with reference to . The matching may be performed by human selection, machine selection, or a combination of human and machine selection. For example, attributes of the source domain and the target domain may be compared to each other to determine if the source domain is an appropriate source of common identifiers for the target domain.","At , the common identifiers are copied from the trained classifier of the trained (source) domain to the target (new) classifier of the target domain. For example, the target classifier may import the common identifiers from the trained classifier using an import utility.","At , a decision may be performed to determine whether labeled data is available for the target classifier of the target domain, which may be used to evaluate the target classifier.","If labeled data is available (\u201cyes\u201d) at , then at  the labeled data is used to evaluate the accuracy of the common identifiers used by the target classifier in the target domain.","At , common identifiers may be removed from the target classifier to improve accuracy of the sentiment classification.","At , additional identifiers may be added to the target classifier to improve accuracy of the sentiment classification. The additional identifiers may be created using auxiliary categorization, as further discussed below.","Finally, at , the classifier (now trained) of the target domain may be used to categorize opinion data. For example, as sources (e.g., people, customers, etc.) generate opinion data for the target domain, this classifier may determine the sentiment of the opinion data as positive or negative. In addition, the target domain may be used as a source domain to train another domain, as appropriate when the domain is selected during a matching process as described above. When no labeled data is available at , the process may proceed immediately to the operation at  to categorize the opinion data.",{"@attributes":{"id":"p-0043","num":"0042"},"figref":"FIG. 5","b":["500","400","500","500","402","404","400"]},"At , opinion data is sorted into two or more categories using the control identifiers. For example, a control identifier of \u201cgood\u201d may be used to place opinion data in one of two categories: (1) opinion data that includes the common identifier (inclusive category) and (2) opinion data that does not include the common identifier (exclusive category).","At , auxiliary categorization tasks are created to predict the occurrence of control identifiers for previous unseen opinion data. For example, an auxiliary classifier can be trained for predicting the existence of the common identifier \u201cgood\u201d using the opinion data of inclusive category and exclusive category.","At , the additional identifiers are created using auxiliary categorization. For example, all of the opinion data in the inclusive category may be assumed to have the same sentiment as the control identifier (e.g., \u201cgood\u201d=positive sentiment). Next, terms that are frequently included in the inclusive category may be analyzed to determine whether they are good for predicting the occurrence of the common identifier (e.g. \u201cgood\u201d). For example, this opinion data may include many instances of the term \u201cfast,\u201d which, in context of the target domain (such as computers), may be a positive sentiment. For exemplary opinion data of \u201cThe CPU is fast!\u201d where no common identifier exists, the common identifier \u201cgood\u201d may be predicted to be in existence by auxiliary categorization. Then a pseudo-common identifier corresponding to \u201cgood\u201d is created. Therefore, the additional identifier may enable classification of the data as positive.","At , weights may be applied to the common identifiers and the additional identifiers. For example the common identifiers may have a higher weight than the additional identifiers, which creates a greater reliance on the common identifiers to predict the sentiment of opinion data. For example, if a piece of opinion data includes a positive common identifier with a higher weight and a negative additional identifier with a lower weight, the opinion data may be classified as positive because the common identifier has a higher weight, and thus may be more trusted that the additional identifier.",{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIG. 6","b":["600","400","500","600"]},"In some embodiments, the common identifiers may be created using labeled data , unlabeled data , or a combination of both from the target domain. There are two application scenarios: The first scenario does not include labeled data of the target domain, which is described with reference to the process  where the decision  follows the \u201cno\u201d route to the operation . The second scenario includes labeled data of the target domain and may include one or more of the operations after the decision  when the \u201cyes\u201d route is used. For the second scenario, the labeled data may be combined and analyzed to determine the terms which have good category prediction abilities, which are selected as the identifiers. Next, the terms which have good domain prediction abilities (the domain identifiers) are disregarded, leaving the common identifiers , which are selected for use (imported) in the target domain.","In an example, information gain may be employed as a term-goodness criterion. For category prediction, the information gain of a term t is G(t), as shown in Equation 1.\n\n()=\u2212\u03a3() log ()+()\u03a3() log ()+()\u03a3() log ()\u2003\u2003Equation (1)\n\nwhere c denotes a category which can be positive or negative. The higher the value of G(t) is, the better the category prediction ability of t. Similarly, the discriminative ability among domains of term t is written as G(t) in Equation 2.\n\n()=\u2212\u03a3() log ()+()\u03a3() log ()+()\u03a3() log ()\u2003\u2003Equation (2)\n\nwhere d denotes a domain which can be a source domain or the target domain. G(\u2022) and G(\u2022) are calculated for all terms that occur in the labeled data. Next, the terms that have a relatively higher value of G(\u2022) but lower value of G(\u2022) (according to predefined thresholds) are selected as the common identifiers .\n","As shown in , auxiliary tasks  may be used to further refine classification ability of the target domain. If auxiliary tasks are employed, then an auxiliary classification  may be performed using the common identifiers as described with reference to the process .","In an example implementation of the auxiliary classification , the common identifiers  are used to sort opinion data. The auxiliary classification  may be trained to predict the existence of the common identifier  in a given piece of text content. In this operation, the unlabeled data from target domain is utilized for training. For the common identifier \u201cgood,\u201d for instance, the opinion data in which the term \u201cgood\u201d occurs as positive training samples (inclusive) and the others as negative samples (exclusive). Next, the auxiliary classification may be trained to predict the occurrence of the term \u201cgood\u201d given previous unseen content.","The target domain having the target classifier may input, or otherwise receive, generate, or replicate training data . For example, the training data may include imported common identifiers , additional identifiers created by the auxiliary classification , and so forth.","In addition, weights  may be applied. For example, the common identifiers may be assigned a higher weight than other identifiers to adjust the influence of the identifiers when implementing the classifier. As an example, given an instance (a piece of textual content), one way is to increase the weights of common identifiers (such as the term frequency weight). Generally, the identifiers are re-weighted for such an instance. Let {right arrow over (x)} represent the instance with {right arrow over (x)}=<w, w, . . . , w> where wdenotes the frequency of term t, for example \u201cgood\u201d, occurring in this instance. {right arrow over (x)} is replaced by {circumflex over ({right arrow over (x)}, {circumflex over ({right arrow over (x)}=\u03b1\u00b7\u03c8({right arrow over (x)})+(1\u2212\u03b1)\u00b7{right arrow over (x)}, where \u03c8({right arrow over (x)}) denotes the transformation of {right arrow over (x)} whose components except the ones corresponding to common identifiers are weighted 0. Here, \u03b1 is a real number that may tune the weights of all identifiers. It can be set to be a number in the range of [0, 1]. The higher the value of \u03b1 is, the more weight is given to each of the common identifiers .","In other embodiments, additional identifiers are created when the auxiliary tasks are used after decision operation . This approach may first predict the existence of each common identifier at , and then expand the identifier vector, now {circumflex over ({right arrow over (x)}, by adding N elements (additional identifers), where N is the number of common identifiers. The weights of new elements are set to be 0 or 1 corresponding to non-existence or existence in terms of the corresponding common identifiers. For example, assuming there are two common identifiers, \u201cgood\u201d and \u201cbad\u201d and for instance {right arrow over (x)}, \u201cgood\u201d is predicted to \u201cexistence\u201d category and \u201cbad\u201d is predicted to \u201cnon-existence\u201d category, then this instance will be represented as <{circumflex over ({right arrow over (x)}, 1,0>, with two additional identifiers weighted as 1 and 0 respectively.","In additional embodiments, when labeled opinion data from the target domain is available, the labeled data may be used to further increase accuracy of the sentiment classification. Even a limited amount of labeled data may be beneficial and improve the target classifier. The labeled data may be used by refining the objective function of the target classifier.","Using a Na\u00efve Bayes classifier as an example, a Dirichlet prior may be used for parameters, and then the expectation of the parameters may be observed with respect to their posteriors. Particularly in Equation 1, in a category c the frequency of a term t in the labeled data of the source domain can be used as the parameter of the corresponding Dirichlet prior, referred as f\u2032. The term frequency of t for posterior distribution can be represented as f+f\u2032, where fdenotes the frequency of term t in category c in the labeled data of the target domain.","To balance the impact of labeled data of the target domain and the source domain for parameter estimation, this function may be used to calculate the frequency of term t in category c: \u03bbf+(1\u2212\u03bb)f\u2032. \u03bb is a real number with value in range [0,1] used for tuning the impact of labeled data of the target domain and the source domain. The higher the value of \u03bb is, the greater an impact of target labeled data. In practice, it is may be better to set \u03bb to be larger than 0.5, although the values in the range [0,1] are appropriate.","Finally, the target classifier (now trained) may be used to classify opinion data in the target domain at the operation .","Additional Embodiments",{"@attributes":{"id":"p-0060","num":"0059"},"figref":"FIG. 7","b":["700","702","704","702","702","1","702","2","702","3","702","702"]},"In accordance with embodiments, the common identifiers of the multiple domains  may be combined to create a pool of common identifiers . The pool of common identifiers  may be collectively larger than the common identifiers that are available from a single domain. The pool of common identifiers  may be imported to the target classifier , similar to the operation  of the process . Thus, the pool of common identifiers  may enable use of multiple out-of-domain common identifiers to increase the sentiment classification ability of the target classifier.",{"@attributes":{"id":"p-0062","num":"0061"},"figref":"FIG. 8","b":["800","800","802","804","802","806","802"]},"The second section  may include an additional break out  of information (e.g., sub-domain data, etc.). The break out  may include sentiment classification at a more granular level than the sentiment summary . In additional embodiments, more of fewer break outs  may be included, and may vary depending on the domain. For example, a domain for computers may include a different break  out than a domain for books.","Illustrative Computing Device",{"@attributes":{"id":"p-0064","num":"0063"},"figref":["FIG. 9","FIG. 9"],"b":["900","900","900"]},"In a very basic configuration, the computing device  typically includes at least one processing unit  and system memory . Depending on the exact configuration and type of computing device, the system memory  may be volatile (such as RAM), non-volatile (such as ROM, flash memory, etc.) or some combination of the two. The system memory  typically includes an operating system , one or more program modules , and may include program data . The operating system  includes a component-based framework  that supports components (including properties and events), objects, inheritance, polymorphism, reflection, and provides an object-oriented component-based application programming interface (API). The computing device  is of a very basic configuration demarcated by a dashed line . Again, a terminal may have fewer components but will interact with a computing device that may have such a basic configuration.","The computing device  may have additional features or functionality. For example, the computing device  may also include additional data storage devices (removable and\/or non-removable) such as, for example, magnetic disks, optical disks, or tape. Such additional storage is illustrated in  by removable storage  and non-removable storage . Computer storage media may include volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information, such as computer readable instructions, data structures, program modules, or other data. The system memory , the removable storage , and the non-removable storage  are all examples of computer storage media. The computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by the computing device . Any such computer storage media may be part of the computing device . The computing device  may also have input device(s)  such as keyboard, mouse, pen, voice input device, touch input device, etc. Output device(s)  such as a display, speakers, printer, etc. may also be included. These devices are well known in the art and are not discussed at length here.","The computing device  may also contain communication connections  that allow the device to communicate with other computing devices , such as over a network. These networks may include wired networks as well as wireless networks. The communication connections  are one example of communication media. The communication media may typically be embodied by computer readable instructions, data structures, program modules, etc.","It is appreciated that the illustrated computing device  is only one example of a suitable device and is not intended to suggest any limitation as to the scope of use or functionality of the various embodiments described. Other well-known computing devices, systems, environments and\/or configurations that may be suitable for use with the embodiments include, but are not limited to personal computers, server computers, hand-held or laptop devices, multiprocessor systems, microprocessor-base systems, set top boxes, game consoles, programmable consumer electronics, network PCs, minicomputers, mainframe computers, distributed computing environments that include any of the above systems or devices, and\/or the like. For example, some or all of the components of the computing device  may be implemented in a cloud computing environment, such that resources and\/or services are made available via a computer network for selective use by client devices.","Conclusion","The above-described techniques pertain to sentiment classification of out of domain data. Although the techniques have been described in language specific to structural features and\/or methodological acts, it is to be understood that the appended claims are not necessarily limited to the specific features or acts described. Rather, the specific features and acts are disclosed as exemplary forms of implementing such techniques."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The Detailed Description is described with reference to the accompanying figures. In the figures, the left-most digit(s) of a reference number identifies the figure in which the reference number first appears. The same reference number in different figures refers to similar or identical items.",{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":["FIG. 9","FIG. 6"]}]},"DETDESC":[{},{}]}
