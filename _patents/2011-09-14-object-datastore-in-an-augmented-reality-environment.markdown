---
title: Object datastore in an augmented reality environment
abstract: An augmented reality environment allows interaction between virtual and real objects and enhances an unstructured real-world environment. An object datastore comprising attributes of an object within the environment may be built and/or maintained from sources including manufacturers, retailers, shippers, and users. This object datastore may be local, cloud based, or a combination thereof. Applications may interrogate the object datastore to provide user functionality.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08953889&OS=08953889&RS=08953889
owner: Rawles LLC
number: 08953889
owner_city: Wilmington
owner_country: US
publication_date: 20110914
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","DETAILED DESCRIPTION","CONCLUSION"],"p":["Augmented reality environments allow interaction among users and real-world objects and virtual or computer-generated objects and information. This merger between the real and virtual worlds paves the way for new interaction opportunities.","An augmented reality system may be configured to interact with objects within a scene and generate an augmented reality environment. The augmented reality environment allows for virtual objects and information to merge and interact with tangible real-world objects, and vice versa. Disclosed herein are techniques and devices suitable for storing attributes about those objects in an object datastore and accessing that datastore within the augmented reality environment.","The augmented reality system includes a computing device configured to support the augmented reality environment. Objects and attributes associated with those objects may be stored in an object datastore. The object datastore may be queried to allow for functionality within the augmented reality environment. For example, a user may request an inventory of items within a room for insurance purposes or query the object datastore to look for a misplaced pair of sunglasses.","The object datastore may be local to the computing device, accessible to the computing device via a local area network, a cloud or remote resource accessible via a wide area network such as the Internet, or a combination thereof. For example, a local datastore may contain objects which are currently or likely to be in the user's environment while the cloud object datastore may contain aggregated data for many more objects.","The object datastore may be populated by data transfers or by input of information from sensors in the augmented reality environment. Data transfer for attributes of an object may be provided to the object datastore by entities handling goods. For example, upon purchase of an object the merchant may provide a model number and device serial number for inclusion in the object datastore. A shipping company transporting the object may provide a date of acquisition, mass, and overall volume of the object for inclusion in the object datastore. In another example, a manufacturer may provide data about functional attributes.","The object datastore may also be populated by detecting the object in the augmented reality environment. Using sensors accessible to the computing device which maintains the augmented reality environment, attributes of the detected objects are determined. These determined attributes may be used to query the object datastore in an attempt to identify the object. When a confidence value of the identification is below a pre-determined confidence threshold, the environment may query a user for assistance in identifying the object or for refined identification. For example, the environment may project an image of an illuminated circle around a detected but unidentified object and ask the user to identify this thing, such as via a speaker and speech synthesis module. The user may provide a refined identification such as \u201ca can of diet Dr. Salt flavored soda\u201d or at least a partial identification or classification such as \u201ca can of soda\u201d to assist the system in further identification. Once the user has made the identification, either refined or partial, the identification may be stored within the object datastore for later use. This later use may include further processing to improved recognition of other objects within the environment, as well as for disambiguation of objects such as those objects which have a partial identification.","The object datastore may include several categories of attributes including, but not limited to, specific object attributes, physical attributes, functional attributes, ownership attributes, and location attributes. Each of these categories, in turn, may store several attributes and sub-attributes. The specific object attributes comprise attributes unique to a particular object. For example, a unique serial number. The physical attributes comprise physical attributes inherent in and presented by the object. For example, the size and shape of the object. The functional attributes comprise functional and operational constraints associated with the object, such as those which affect use of the object. For example, intended uses and safety parameters. The ownership attributes comprise property rights associated with the object. For example, user Alice owns the object and may freely sell or lease it. The location attributes comprise location data of the object. For example, the current location of the object in the augmented reality environment. The object datastore and details about the various attributes are discussed in detail below.","As described herein for illustration and not by way of limitation, the augmented reality environment may be provided at least in part by a combination of a structured light source, such as an image projector or other light source configured to generate structured light patterns and a camera to image those patterns. The projector and camera may further be incorporated into a single unit and designated as an augmented reality functional node (ARFN). In other implementations, other combinations of elements such as projectors, video displays, computer screens, cameras, microphones, ultrasound transducers, depth-sensing devices, weight sensors, touch sensors, tactile output devices, and so forth may be also be present within or coupled to the ARFN. For convenience, and not by way of limitation, the examples in this disclosure refer to the use of structured light for the characterization of the physical environment of the scene including objects therein. However, in addition to or in place of structured light, other techniques may be used such as light detection and ranging (LIDAR), optical time-of-flight, ultrasonic ranging, stereoscopic imaging, radar, infrared scanning, and so forth either alone or in combination with one another.","Within the augmented reality environment, the ARFN includes an augmented reality module. This module is configured to identify and track objects within the scene, maintain a user interface, and facilitate queries of the object datastore, as discussed in detail below.","Illustrative Environment",{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 1","b":["100","102","1","102","2","102","102","1","102","1","102","102","1"]},"Each of the ARFNs ()-(N) couples to or includes a computing device . This computing device  may be within the ARFN , or disposed at another location and connected to the ARFN . The computing device  comprises a processor , an input\/output interface , and a memory . The processor  may comprise one or more processors configured to execute instructions. The instructions may be stored in memory , or in other memory accessible to the processor .","The input\/output interface  may be configured to couple the computing device  to other components such as projector, cameras, microphones, other ARFNs , other computing devices, other devices within the augmented reality environment, and so forth. For example, the input\/output interface  may be configured to exchange data with computing devices, cleaning robots, home automation devices, and so forth in the environment. The coupling between the computing device  and the devices may be via wire, fiber optic cable, or wireless connection including but not limited to radio frequency, optical, or acoustic signals.","The memory  may include computer-readable storage media (\u201cCRSM\u201d). The CRSM may be any available physical media accessible by a computing device to implement the instructions stored thereon. CRSM may include, but is not limited to, random access memory (\u201cRAM\u201d), read-only memory (\u201cROM\u201d), electrically erasable programmable read-only memory (\u201cEEPROM\u201d), flash memory or other memory technology, compact disk read-only memory (\u201cCD-ROM\u201d), digital versatile disks (\u201cDVD\u201d) or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by a computing device.","Several modules such as instructions, datastores, and so forth may be stored within the memory  and configured to execute on a processor, such as the processor . An operating system module  is configured to manage hardware and services within and coupled to the computing device  for the benefit of other modules. Modules may be stored in the memory of the ARFN , storage devices accessible on the local network, cloud storage accessible via a wide area network, or distributed across a combination thereof.","An object datastore  is configured to maintain information about objects within or accessible to the augmented reality environment or users. These objects may be tangible real world objects or virtual objects. Tangible objects include items such as tables, chairs, animals, plants, food containers, printed books, and so forth. Virtual objects include media content such as songs, movies, electronic books, computer generated scenes, media files, and so forth. Virtual objects may include stored copies of those objects or access rights thereto. The object datastore  may include a library of pre-loaded reference objects, as well as objects which are temporally persistent within a particular environment, such as a wall, a specific table, a user and so forth. The object datastore  or the other datastores described below may be stored on one or more of the memory of the ARFN , storage devices accessible on the local network, cloud storage accessible via a wide area network, or distributed across one or more of these. Data within the object datastore  may be stored in hierarchical, non-hierarchical, or hybrid arrangements. In a hierarchical structure objects may be classified in a structure which may be taxonomic in nature. For example, an object may be defined in the datastore as inanimate, metal exterior, food, ready-to-eat, beverage, soda, grape flavor. Non-hierarchical systems may comprise systems where apparently arbitrary or variable connections are established between objects. These arbitrary associations may be between physical and virtual objects, such as associating a grouping of digital photos with a small statute of the Eiffel tower. The object datastore  is discussed in more detail below in regards to .","A user interaction datastore  is configured to maintain information about interactions between one or more users and one or more objects. The user interaction data within the datastore  may be used in some implementations to facilitate additional functions such as to receive indications of user selections of advertisements, providing usage data, and so forth. The collection and use of the user interaction data may be limited based upon the preferences of the user. Changes in objects attributes as a result of interactions may be stored in the object datastore . For example, when the user moves a pair of sunglasses from one part of the augmented reality environment to another, the previous and current location attributes may be stored in the object datastore .","A transaction datastore  is configured to maintain information about changes in property rights associated with objects. For example, the datastore  may be configured to track the purchase, rental, sale, lease, and so forth of objects. Changes to the ownership rights may update the ownership attributes in the object datastore .","An advertisement datastore  is configured to maintain information about advertisements available to the augmented reality environment. These advertisements may be designated for presentation based on one or more of the attributes maintained within the object datastore . For example, a user bringing an apple into the augmented reality environment may be presented with an advertisement for a prepackaged apple pie the user has previously purchased.","An augmented reality module  is configured to generate augmented reality output in concert with the physical environment. The module  may access one or more of the datastores described herein. The augmented reality module  may include a tracking and control module  is configured to identify objects including users. This identification may include the use of a camera, structured light, radio frequency identification equipment, communication with the object, and so forth within the ARFN . Objects may be identified as described below by comparing attributes in the object datastore  such as shape, text thereon, universal product code (UPC), object behaviors (including an ability to move independently), optical barcode, radio frequency identification tag (RFID), and so forth. Where the object is capable of communication, it may exchange information with the ARFN , and such information may be stored within the object datastore .","The term object encompasses several categories including unitary, package, group, container, and so forth. As described above, objects may be hierarchically or arbitrarily associated with one another. Some objects may be unitary in that they are complete in and of themselves, such as a baseball bat or wrench. Some objects may be packages containing a plurality of items which may or may not be identical, such as a twelve-can case of cola or set of different sized wrenches. An object may also be a group of closely interrelated sub-objects, which operate together to form a whole, such as a jigsaw puzzle. An object may also be a container holding other objects, such as a filled tote with the significance being the contents of the tote and not necessarily the container itself. Different attributes may be stored for different categories of objects.","The category of an object may be determined at least in part by factors including context, prior use, pre-determined data, and so forth. For example in the context of home use and purchase, wine may be considered a unitary object and designated by the bottle not by the case. Similarly, pre-determined data may indicate that totes are used for storage, and as such a user pointing to a tote filled with objects may be determined as a default to be designating the objects within the tote, and not the tote itself.","A user interface module  which may be in the augmented reality module  is configured to accept and interpret input and generate output for the user. The ARFN  may use a camera, structured light, stereoscopic vision, and so forth to read the input from the user. The user interface module  may also include an object datastore query module configured to allow applications to access information in the object datastore . For example, a user may query the object datastore  to locate his misplaced sunglasses. The augmented reality module  may also include a transaction module  is configured to associate objects with advertisements, facilitate transactions involving third parties such as buying and selling objects, and perform other functions.",{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 2","b":["200","102","102","202","102"]},"A chassis  holds the components of the ARFN . Within the chassis  may be disposed a projector  that generates and projects images into the scene . These images may be visible light images perceptible to the user, visible light images imperceptible to the user, images with non-visible light, or a combination thereof. This projector  may be implemented with any number of technologies capable of generating an image and projecting that image onto a surface within the environment. Suitable technologies include a digital micromirror device (DMD), liquid crystal on silicon display (LCOS), liquid crystal display, 3LCD, and so forth. The projector  has a projector field of view  which describes a particular solid angle. The projector field of view  may vary according to changes in the configuration of the projector. For example, the projector field of view  may narrow upon application of an optical zoom to the projector. In some implementations, a plurality of projectors  or other displays such as televisions, monitors, and so forth may be used.","A camera  may also be disposed within the chassis . The camera  is configured to image the scene in visible light wavelengths, non-visible light wavelengths, or both. For example, in one implementation the camera  may be configured to generate a thermal image as well as a visible light image. The camera  has a camera field of view  which describes a particular solid angle. The camera field of view  may vary according to changes in the configuration of the camera . For example, an optical zoom of the camera may narrow the camera field of view . In some implementations, a plurality of cameras  may be used.","The chassis  may be mounted with a fixed orientation, or be coupled via an actuator to a fixture such that the chassis  may move. Actuators may include piezoelectric actuators, motors, linear actuators, and other devices configured to displace or move the chassis  or components therein such as the projector  and\/or the camera . For example, in one implementation the actuator may comprise a pan motor , tilt motor , and so forth. The pan motor  is configured to rotate the chassis  in a yawing motion. The tilt motor  is configured to change the pitch of the chassis . By panning and\/or tilting the chassis , different views of the scene may be acquired. The spatial analysis module  may use the different views to monitor objects within the environment.","One or more microphones  may be disposed within the chassis , or elsewhere within the scene. These microphones  may be used to acquire input from the user, for echolocation, location determination of a sound, or to otherwise aid in the characterization of and receipt of input from the scene. For example, the user may make a particular noise, such as a tap on a wall or snap of the fingers, which are pre-designated as attention command inputs. The user may alternatively use voice commands. Such audio inputs may be located within the scene using time-of-arrival or other techniques among the microphones.","One or more speakers  may also be present to provide for audible output. For example, the speakers  may be used to provide output from a text-to-speech module or to playback pre-recorded audio.","A transducer  may be present within the ARFN , or elsewhere within the environment, and configured to detect and\/or generate inaudible signals, such as infrasound or ultrasound. For example, the transducer  may be configured to detect a characteristic ultrasonic sound signature produced by keys on a keyring. Inaudible signals may also be used to provide for signaling between accessory devices and the ARFN .","A ranging system  may also be provided in the ARFN . The ranging system  is configured to provide distance information from the ARFN  to a scanned object or set of objects. The ranging system  may comprise radar, light detection and ranging (LIDAR), ultrasonic ranging, stereoscopic ranging, and so forth. In some implementations the transducer , the microphones , the speaker , or a combination thereof may be configured to use echolocation or echo-ranging to determine distance and spatial characteristics.","In this illustration, the computing device  is shown within the chassis . However, in other implementations all or a portion of the computing device  may be disposed in another location and coupled to the ARFN . This coupling may occur via wire, fiber optic cable, wirelessly, or a combination thereof. Furthermore, additional resources external to the ARFN  may be accessed, such as resources in another ARFN  accessible via a local area network, cloud resources accessible via a wide area network connection, or a combination thereof.","Also shown in this illustration is a projector\/camera linear offset designated \u201cO\u201d. This is a linear distance between the projector  and the camera . Placement of the projector  and the camera  at distance \u201cO\u201d from one another aids in the recovery of structured light data from the scene. The known projector\/camera linear offset \u201cO\u201d may also be used to calculate distances, dimensioning, and otherwise aid in the characterization of objects within the scene . In other implementations the relative angle and size of the projector field of view  and camera field of view  may vary. Also, the angle of the projector  and the camera  relative to the chassis  may vary.","In other implementations, the components of the ARFN  may be distributed in one or more locations within the environment . As mentioned above, microphones  and speakers  may be distributed throughout the scene. The projector  and the camera  may also be located in separate chassis . The ARFN  may also include discrete portable signaling devices used by users to issue command attention inputs. For example, these may be acoustic clickers (audible or ultrasonic), electronic signaling devices such as infrared emitters, radio transmitters, and so forth.",{"@attributes":{"id":"p-0051","num":"0050"},"figref":"FIG. 3","b":["300","302","1","302","2","302","104","304","304"]},"Resources available via the network  may include cloud object datastore servers . The cloud or remote object datastore may comprise an extended object datastore. Transfers of data such as received from other entities as shipping manifests, inventory data, manufacturing data, and so forth may build this object datastore as described below. The cloud object datastore may also be populated by aggregating at least a subset of the local object datastores from augmented reality environments. Such aggregation may take place after user approval and may include providing data comprising one or more object attributes in the local object datastore to the cloud object datastore. The cloud object datastore servers  receive the data and may integrate that data into the datastore. This integration may involve de-duplication, confirmation of identification, analysis of attributes to determine a range of possible values for a given object or class of object, and so forth. For example, the range of possible shapes and colors of apples may be determined from a sample of the attributes of thousands of individual apples across many augmented reality environments.","The cloud object datastore servers  are configured to provide data in the cloud object datastore to applications. In some implementations, the providing may comprise maintaining an application programming interface or other service configured to respond to queries against the datastore. For example, an application executing on a given computing device  may attempt to identify a detected object and query the cloud object datastore servers  to search the larger datastore contained therein","Object attributes (), (), . . . , (A) may be transferred via the network  between devices. For example, the computing device  may transfer object attributes () for the detected but unidentified object to the cloud object datastore servers  for refined identification. In other implementations the cloud object datastore servers  or other servers may exchange object attributes . For example, merchant servers  may provide object attributes () associated with purchased objects to the object datastore  of the computing device , cloud object datastore servers , and so forth. The merchant servers  may include bricks-and-mortar as well as online stores.","Likewise, the merchant servers  may request data from the cloud object datastore servers  via the network  to determine actual usage statistics for an object. For example, the merchant servers  may wish to determine how long a toy dump truck is played with before being discarded or having ownership transferred to another user. Such usage information may be useful in product development, marketing, and so forth.","Shipper servers  may be configured to couple to the network  and exchange object attributes  or other data with other devices. A common carrier, freight company, and so forth may maintain or provide logistics-related data to the shipper servers  relating to objects. For example, the shipper servers  may contain a manifest which lists objects in a given shipment, delivery information, weight, volumetric data, and so forth. The computing device  may populate the object datastore  by receiving object attributes () including this manifest data, such that when the user is unpacking the box, the augmented reality environment is able to identify the objects from the set of objects which were packed in the box per the manifest, rather than all objects existing in the aggregated cloud object datastore.","Marketing servers  may also access object attributes  via the network . The marketing servers  may be configured to query the object datastore to determine usage patterns, purchasing patterns, and so forth. Such information may then be used to develop marketing strategies. For example, where the augmented reality identifies that the user has a collection of porcelain figurines in his augmented reality environment, advertisements specific to that user's apparent interest in figurines may be provided.",{"@attributes":{"id":"p-0058","num":"0057"},"figref":"FIG. 4","b":"114"},"The object datastore  may store several categories of attributes including, but not limited to, specific object attributes , physical attributes , functional attributes , ownership attributes , and location attributes . Each of these categories, in turn, may include several attributes and sub-attributes. These categories are provided by way of example, and in some implementations attributes or sub-attributes may be present in a plurality of categories. The categories are discussed next with regards to .",{"@attributes":{"id":"p-0060","num":"0059"},"figref":"FIG. 5","b":["402","114","402"]},"A unique identifier  may be stored. This unique identifier may include a serial number, global unique identifier, and so forth. The unique identifier  may be considered to distinguish this object from all other similar or otherwise identical objects. The unique identifier  may be embedded within the device, affixed thereto with a label or tag, or otherwise associated therewith. For example, a portable game console may have a unique serial number embedded within its firmware. The unique identifier  may be stored on a tag such as an optical barcode or radio frequency identification chip.","An expiration date  may be associated with the object. The expiration date  is a calendar date or interval after which the object is considered to be degraded or at which time the object becomes unusable. For example, a printer cartridge may be configured to have an operational lifetime and thus expiration date of 120 days after installation in a printer.","A unique flag  indicates whether this specific object is considered unique. For example, the unique flag  for a custom-made diamond necklace may be \u201cyes\u201d while the unique flag  for a pencil is set to \u201cno.\u201d","A preliminary\/refined identification (flag)  may also be present in the object datastore . This flag  may be used to indicate when the specific object has received a preliminary identification which may be subject to change, or a refined identification which is considered to be absolute.","An identification confidence threshold  may also be maintained. The identification confidence threshold  indicates a level at which a calculated confidence value for an object is deemed sufficient to warrant the identification. The confidence value may be based on the comparison of one or more attributes of the object with data in the object datastore  for other objects, reliability of a user providing the identification, and so forth. An object may also have different confidence thresholds . For example, an identification provided by an authorized adult user may be given a 95% confidence threshold while an identification by an unauthorized child user may be given a 20% confidence threshold. The environment may be configured such that objects having a confidence value above a pre-determined threshold confidence threshold are deemed to have a refined identification. Likewise, objects with a calculated confidence value below a pre-determined confidence threshold may be assigned a preliminary identification which may be subject to change as additional data becomes available to the augmented reality environment.","Specific object attributes  may also include a hardware address  or other data networking parameter. This hardware address  may comprise a hardware or media-access control address, and so forth.",{"@attributes":{"id":"p-0067","num":"0066"},"figref":"FIG. 6","b":["404","114","404","404","102"]},"Size of the object  may be stored in the object datastore . For example, the size may include a volumetric measurement such as the object fits within a cuboid of dimensions 32 cm by 16 cm by 51 cm.","Mass  of the object may be stored. In one implementation a pressure sensitive touch sensor or scale in the augmented reality environment may provide an indication of the object's mass. In another implementation, the mass may be acquired from a third-party, such as from a shipper or merchant who is transporting or selling the object.","Topology  is a two- or three-dimensional map or representation of a configuration or shape of at least a portion of the object's exterior. For example, the overall topology of a beverage can is that of a cylinder, while the overall topology of a dog is more complex.","Color  of the object may also be stored in the object datastore . The color  may be determined from ambient light or from the interaction of different spectral bands of light as generated at particular times by a wavelength modulated light source. The color  may also include sub-attributes such as reflectance values or spectral responses for different spectral bands. In some implementations, the spectral data may be used to characterize the composition of the object.","Texture  of the object may acquired and stored as well. The texture of the object is the disposition or arrangement of relatively small scale surface features. For example, the texture of a tennis ball is fibrous while texture of a golf ball is smooth with dimples. Texture may be determined by imaging, a scan such as with structured light or LIDAR, tactile sensor, and so forth. As with any of the attributes in the object datastore , many textures  may be stored and associated with a given object or set of objects. For example, the texture of a user's face may change as his beard grows.","Baseline sound signatures  may also be stored within an object datastore . The sound signatures described herein may include sounds audible to the human ear, as well as inaudible sounds such as infrasound or ultrasound. Some objects produce a baseline of specific waveforms. Objects which are electrical or mechanical may produce particular sounds while operating. For example, a particular laptop may produce a distinctive sound due to cooling fans and the particular airflow of that device. Objects which are organic may produce particular sounds such as breathing.","Comparison of the baseline sound signature  or waveform may aid in identification a detected object. Returning to the laptop example, a portable computer device which is detected and has an operating fan could not be a portable device with no fan. As a result, the augmented reality environment may limit the search in the object datastore  for objects having cooling fans or the baseline sound signature  associated therewith.","An interacting sound signature  of the object interacting with the environment or users may also be stored. This sound signature  results during the object interacting with another object in the augmented reality environment rather than passively sitting there. For example, a key ring full of keys sitting on a table may have no operating sound signature . However, a user picking up those keys may cause them to \u201cjingle\u201d and emit a particular sound signature . This sound signature  may include audible and ultrasonic aspects. Similarly, the user placing the keys on a table may also generate a particular interacting sound signature .","Physical attributes  may also include a thermal signature . This thermal signature  may include data which indicates the particular range of temperatures the object is typically expected to have. For example, the thermal signature  for a human would be between about 96\u00b0 Fahrenheit (F) and 105\u00b0 F. The thermal signature  may also be used to distinguish and in some implementations aid in the identification of an object. For example, the augmented reality environment may be able to distinguish a bowl of frozen yogurt from a bowl of pudding based at least in part upon the temperature difference.","The thermal signature  attribute may comprise coarse temperature or \u201cspot\u201d measurements such as determined by an infrared photodetector or thermal image data such as acquired from an infrared camera. Where the thermal signature  comprises thermal image data, this image data may further be used to characterize an object. For example, thermal images of faces may be used to distinguish one user from another. Or the thermal image of a laptop may serve to distinguish one model from another based on the different relative position of relatively hot internal components such as the processor.","The physical attributes  may also include power requirements  for the object. Objects which consume electrical power may have their power requirements  stored in the object datastore . These power requirements may include the operating power requirements, actual power consumption, low power or sleep mode power requirements, and so forth. In some implementations electrical supplies such as a residential or commercial power system may provide for the exchange of data with the augmented reality environment. For example, a \u201csmart grid\u201d electric meter may be configured to provide data to the augmented reality environment. Where an individual object is so equipped, such as a personal computer or laptop, the object may provide data regarding electrical power consumption, requirements, and so forth to the augmented reality environment. In some implementations, a sub-attribute may include thermal power requirements, such as thermal power output during operation.","User contact points  may also be maintained. Many objects have a particular shape which is defined by how a user is intended to interact with the object. The user contact points  are where a user typically touches or otherwise interacts with the device. By monitoring the interaction of the user with the object in the environment, the user contact points  may aid in the identification of an object. For example, a crochet hook may be differentiated from a pencil based on the different contact points  resulting from differences in how the user grasps each.","Other physical attributes  may be stored as well in the augmented reality environment. For example, smells such as determined by machine olfaction devices may be stored. In another implementation chemical composition may be stored. Or characteristics of magnetic fields generated or distorted by the object as determined by magnetic field sensors may be stored.",{"@attributes":{"id":"p-0081","num":"0080"},"figref":"FIG. 7","b":["406","114","406"]},"One or more intended uses  may be stored. These are operations for which the object is intended during normal use to be used for. For example, scissors have as intended uses cutting sheets of material including paper, fabric, and so forth.","One or more alternate uses  are those operations for which the object may be used, but is not intended to be used for. For example, the scissors may be used to puncture rather than cut.","One or more intended users  may be specified. These intended users may be specific individuals or groups of users. For example, the scissors may have as intended users the group of users over age three.","Objects in the augmented reality environment may be used in conjunction with other objects. Objects used in conjunction with  are items which may, but need not always, be used together. For example, the scissors may have objects used in conjunction  with them of paper, glue, and glitter.","Operating or use cost  may be stored in the object datastore . This cost may be measured in terms of time, money, resource utilization, and so forth. For example, the operating cost  of an electronic game may be 350 watts of electrical power per hour.","A designated operating environment  may also be stored. The designated operating environment  indicates how and where the object typically is used. The object may be considered more likely to be present in the designated operating environment  than in other environments. For example, a laptop object has a designated operating environment  which differs from a scrubbing brush object  which resides in the kitchen sink.","Safety parameters  designate boundaries of a safe operating regime associated with the object. These safety parameters  may include safety to users, safety to non-users, safe operation to prevent damage to the object, and so forth. For example, the safety parameters  for a particular blender object may indicate that the blender should not operate continuously for more than fifteen minutes to minimize the risk of electrical fire from overheating.","Other  functional attributes  may also be stored in the object datastore . For example, details about consumable components or inputs for the object may be maintained, object behavior, object operation, and so forth. An object category may also be maintained to indicate whether the object is unitary, a package, a group, or a container for other objects.",{"@attributes":{"id":"p-0090","num":"0089"},"figref":"FIG. 8","b":["408","114"]},"The ownership attributes  may include a date acquired . The date acquired  may be determined by the sensors in the augmented reality environment, transfer of data from merchants or shippers, and so forth.","One or more ownership categories  may be specified. These categories may be used to designate if the object was received as a gift, purchased, leased, rented, and so forth.","Ownership rights  indicate what property rights are held by a particular user or group of users. These may include particular rights of alienability, such as whether the objects may be sold, rented, leased, performed publicly, and so forth. For example, the ownership rights  may include possession but not ownership if the object is being leased or rented from another.","An ownership rights expiration date  may be stored which indicates a point in time where the ownership rights will change. For example, access rights to a digital object such as a movie may be temporary and expire after seven days. Or the date when a rental period for a piece of furniture concludes. In some implementations an ownership rights expiration event attribute may also be stored. For example, ownership rights continue until a rental payment is missed.","A purchase price  of the object may be maintained. The purchase price  may be input manually or received from a shipper, merchant, or other party providing or interacting with the delivery of the object.","A current value  of the object may also be stored in the object datastore . The current value  may be configured to reflect a current market value of the object, the owner's current valuation, and so forth. For example, the market value of the object may be determined by searching for current used sale prices of the same or a similar object in one or more marketplaces.","Additional ownership attributes  may also be stored. For example, a list of authorized transferees may be stored which designate to whom an object may be transferred.",{"@attributes":{"id":"p-0098","num":"0097"},"figref":"FIG. 9","b":["410","114"]},"A current location  may be stored. For example, the sunglasses at coordinates 3400 mm, \u2212975 mm, 65 mm or on the \u201ckitchen counter\u201d in the augmented reality environment.","A default location  may be stored which indicates where the object is typically found or should be stowed. This default location  may be pre-loaded, determined via statistical analysis from data in other object datastores , defined by a manufacturer, based at least in part on a location history of the object, and so forth. For example, the default location  for a medical thermometer is in a medicine cabinet in a master bathroom based on where it is typically stored in that household. In another household, the default location  for the medical thermometer may be a medicine cabinet in a bathroom proximate to a child's room.","One or more associated locations  may be stored. The associated locations  are places where the object is likely to reside. For example, the associated locations  for the sunglasses may be in the car, on a table next to the patio, and so forth.","A quantity at a location  may be maintained. For example, where the objects are paper towels in a pantry, the quantity of paper towels at that location may be tracked.","A quantity threshold  may be designated for a given location, or overall throughout the environment. The quantity threshold  indicates what pre-determined quantity has been set to be maintained at that location. For example, a pre-determined minimum of two rolls of paper towels may be set for the pantry.","A location history  indicating the location of the object over a period of time may be maintained in the object datastore . This location history  may have variable levels of resolution in time, space, or both. For example, the augmented reality environment may be configured to maintain the history in ten minute intervals of what room the sunglasses were in, but not necessarily the exact location within the room. The location history  data may be used to determine usage patterns of objects.",{"@attributes":{"id":"p-0105","num":"0104"},"figref":"FIG. 10","b":["126","126","126","1002","210"]},"In some implementations, the gesture may occur at least in part free from contact with other objects. For example, the gesture may comprise the user extending a pointer finger and touching an object, or vice versa. The gesture need not, but may, call for contact. For example, the gesture may include the user waving a hand, pointing at an object, arranging their fingers in a particular configuration, and so forth.","Some gestures may also include audible elements, such as a user raising a hand and generating a finger snap from the raised hand, or rapping on a wall. Input may also include speech accepted by a speech recognition module . Other forms of input may also be accepted from input devices including keyboards, buttons, pointers, touch sensors, and so forth.","A user prompt module  in the user interface module  is configured to generate user prompts such as highlights, pointers, menus, icons, and so forth. For example, the user prompt module  may be configured to present a menu of options to the user. The user prompt module  may generate audible, visual, tactile, or other outputs.","An object datastore query module  is configured to accept user inputs and generate a query of the object datastore  including local, cloud, or both. For example, the user may make a hand gesture indicating a query. These queries may be used to determine presence or absence of an object in the environment, in another augmented reality environment, request data about an object, and so forth. For example, a user may wish to know a location of his sunglasses in the environment.","Illustrative Processes","The processes described in this disclosure may be implemented by the architectures described herein, or by other architectures. These processes are illustrated as a collection of blocks in a logical flow graph. Some of the blocks represent operations that can be implemented in hardware, software, or a combination thereof. In the context of software, the blocks represent computer-executable instructions stored on one or more computer-readable storage media that, when executed by one or more processors, perform the recited operations. Generally, computer-executable instructions include routines, programs, objects, components, data structures, and the like that perform particular functions or implement particular abstract data types. The order in which the operations are described is not intended to be construed as a limitation, and any number of the described blocks can be combined in any order or in parallel to implement the processes. It is understood that the following processes may be implemented on other architectures as well.",{"@attributes":{"id":"p-0111","num":"0110"},"figref":"FIG. 11","b":["1100","1102","102","218"]},"At , one or more attributes are determined about the object via the one or more sensors within the augmented reality environment. For example, the texture  of the object may be determined via data obtained by the structured light.","At , a local object datastore  coupled to the augmented reality environment is accessed to generate a preliminary identification of the object based at least in part on the one or more attributes. The local object datastore  may be stored within the memory  in the computing device , or accessible via a local area network. The preliminary identification may be made by comparing one or more of the attributes of the detected object with previously stored attributes in the object datastore .","At , a cloud object datastore  coupled to the augmented reality environment may be accessed to generate a refined identification. This refined identification may be based at least in part on the preliminary identification, the one or more attributes, or both. For example, the preliminary identification may indicate the object is a beverage can, but of an unidentified variety. This preliminary identification may be used to narrow the search in the cloud object datastore  to search only beverage cans, reducing the computational requirements for the search.","The cloud object datastore  may be stored within the cloud object datastore servers . Because of the increased storage and processing capability available within the cloud resource, the cloud object datastore may contain many more attributes for many more items, with those attributes at one or more different resolutions.","As mentioned above, the local and cloud object datastores may be populated with data from several sources. Individual users may identify objects, object attributes may be received from merchants, shippers, manufacturers, and so forth, or the augmented reality environment may be configured to try and determine a preliminary or refined identity. As described below with regards to , in some implementations object attributes may be exchanged between object datastores.",{"@attributes":{"id":"p-0117","num":"0116"},"figref":"FIG. 12","b":["1200","114"]},"At , an object is detected in an augmented reality environment, such as by one or more of the sensors in the ARFN . At , one or more attributes are determined about the object via the one or more sensors within the augmented reality environment.","At , an object datastore  coupled to the augmented reality environment is accessed to generate a preliminary identification of the object based at least in part on the one or more attributes. As above, the preliminary identification may be made by comparing one or more of the attributes of the detected object with previously stored attributes in the object datastore .","At , when a confidence value of the preliminary identification is below a pre-determined identification confidence threshold  the user is queried for identification data. The identification confidence threshold may be calculated by using one or more weights associated with object characteristics. For example, when the input is ambiguous as to the object characteristics, the associated weights with that data would indicate a level of ambiguity as to the measurement. The interrogation may comprise an audible prompt, visual prompt, tactile prompt, and so forth. For example, a selection indicator designating the detected but unidentified object may be generated. This selection indicator may comprise a visual indicator proximate to or overlapping the unidentified object. For example, a ring, arrow, overlay, audible prompt and so forth may be used to indicate the unidentified object.","In other implementations, audible prompts may also be used in conjunction with the visible prompts. These audible indicators may be configured to have an apparent source proximate to the associated object. For example, an audible prompt may be perceived by the user to come from (or nearby) the object. Or the audible prompt may comprise speech asking about the object such as \u201cwhat is the object on the table?\u201d A combination of prompts may be used. For example, the selection indicator may highlight the unidentified object while synthesized speech provides audio output asking the user \u201cwhat is this?\u201d","At , the identification data is accepted from the user resulting from the interrogation for identification of the object. For example, the user may respond \u201cthat is a garlic press\u201d. In some implementations the environment may further query the user to establish or confirm other attributes such as functional attributes . For example, the environment may ask \u201cwhat do you use a garlic press for?\u201d and the user may respond \u201cwhen cooking\u201d.","The augmented reality environment may be configured to accept the identification data in a variety of ways. In one implementation the identification data may comprise human speech. In another implementation the identification data may comprise a recognized human gesture. For example, the gesture may comprise a letter, word, or phrase in American Sign Language. Or the user may be presented with a list of potential identifications and may be asked to point to the correct identification.","At , the accepted identification of the object is stored in the object datastore . This accepted identification as well as object attributes may in turn be provided to other object datastores . For example, once that particular garlic press has been identified in an augmented reality user environment, data about that object's attributes may be shared to the cloud object datastore and thus accessible to other users in other augmented reality environments or portions thereof.",{"@attributes":{"id":"p-0125","num":"0124"},"figref":"FIG. 13","b":["1300","114","114","114"]},"At , a query of the object datastore  is received in an augmented reality environment. The query may include one or more attributes of an object. For example, the user may query the augmented reality environment to provide an inventory of the items located in a small closet.","At , one or more conditions of the augmented reality environment are applied to the query. These conditions may include limitations based on sensor capabilities, available data, environmental attributes, and so forth. In some implementations, these conditions may be considered to filter the query such that objects which conform to physical realities of the environment are returned by the query, while those which do not are omitted. Continuing the example above, the one or more conditions may include the dimensions of the small closet, object attributes such as size , and sensor limitations such as resolution of image data acquired by the camera  for an ARFN in view of the small closet.","At , the object datastore  is accessed to generate query results based at least in part upon the query and the one or more associated conditions. Continuing the example, the object datastore  may be queried to inventory and identify objects detected within the closet. Due to the size limitations of the closet, large pieces of furniture such as sofas, tables, chairs, and so forth may be excluded from the search. The search for identifiable objects may be configured to be limited to objects with an overall volume less than that of the closet and dimensionality such that the object would fit within the closet. By comparing these conditions with the attributes in the object datastore  the identification of the objects may be simplified.","In some implementations a tiered or weighted set of conditions may also be applied. For example, the functional attributes  of items in the small closet may apply a weighting factor to the query results such that household objects such as linens, towels are searched first and results involving these objects are more likely to be accurate than objects associated with outdoor use such as gardening tools.","At , output based at least in part on the query results may be generated. This generation may comprise compiling, exporting, presenting, and so forth. For example, the inventory of the closet may be projected via the projector  onto a wall in the hallway or presented on a handheld computing device.",{"@attributes":{"id":"p-0131","num":"0130"},"figref":"FIG. 14","b":"1400"},"At , an event of interest associated with an object is received in an augmented reality environment. For example, the transducer  and microphone  may detect the interacting sound signature  of keys on a key ring jingling. This interacting sound signature  may be configured as an event of interest.","At , at least partly in response to the event of interest, a scan is initiated within the augmented reality environment. This scan involves the acquisition of data by one or more sensors, configured to survey or view the augmented reality environment. For example, in response to the jingling, the structured light scan may be initiated to track the motion of the keys in the environment.","In some implementations the scan may comprise an increase in the frequency, resolution or both of a scan previously in progress. For example, the structured light scan may increase from a scan interval of five scans per second to sixty scans per second.","During times in the augmented reality environment where little or no activity is taking place, the set of sensors and their associated scan rates, resolution, and so forth may be reduced or discontinued. This may be done to reduce power consumption, minimize computational load, and so forth. For example, in an empty room the ARFN  may discontinue use of structured light and rely on audio signals received by the microphones  to determine an event of interest, such as a user's footsteps, breathing, opening of a door, and so forth.","At , a change in one or more of the attributes associated with the event of interest is determined. For example, a change in the current location  of the keys is determined as a result of the user picking the keys up, causing them to jingle.","At , the object datastore is updated to reflect the changes in the one or more attributes. For example, the current location  may now reflect that the keys are now in the garage with the user, and the location history  may be updated to show the path of the keys through the house.","As described above with regards to  and the discussion of the object datastore query module , the user may query the object datastore. For example, the user may have gotten distracted while heading to the car and inadvertently misplaced his keys. The user may ask the augmented reality environment via the query module  for the current location of his keys. By searching the object datastore  for keys with ownership attributes  associated with the querying user and their current location, the environment is able to provide this information.",{"@attributes":{"id":"p-0139","num":"0138"},"figref":"FIG. 15","b":["1500","506","606","114"]},"At , user approval for aggregation of one or more object attributes of an object in an augmented reality environment is accepted. The user approval may comprise an \u201copt in\u201d to allow anonymous or anonymized data to be collected into the cloud object datastore. In some implementations this user approval may be restricted to users with particular user permissions, such as a designated administrator of the augmented reality environment. In some implementations the collected object attributes relating to particular users, objects, or other criteria may be omitted or prevented from participating in aggregation. For example, a user may choose to not provide identified data about a rare collection of slide rules.","At , data comprising at least a subset of a local object datastore from the augmented reality environment is received. For example, the cloud object datastore servers  may receive a set of attributes describing an object which was identified by the user  in that user's augmented reality environment, or that user's portion of the augmented reality environment. In some implementations the entire local object datastore may be received.","At , the received data is aggregated into the cloud object datastore. This aggregation may include filtering, checking, verifying, weighing, or otherwise modifying the data. For example, data may be preliminarily aggregated into the cloud object datastore pending confirmation by a trusted user or by cross checking with object attributes of similar identified objects.","At , the cloud object datastore is provided for query. In one implementation, an application programming interface may be maintained allowing applications to access the datastore. As described above, the cloud object datastore may be accessed by a variety of users or entities to address questions relating to the objects and interactions of users at least partly within an augmented reality environment.","Although the subject matter has been described in language specific to structural features, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features described. Rather, the specific features are disclosed as illustrative forms of implementing the claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The detailed description is described with reference to the accompanying figures. In the figures, the left-most digit(s) of a reference number identifies the figure in which the reference number first appears. The use of the same reference numbers in different figures indicates similar or identical components or features.",{"@attributes":{"id":"p-0004","num":"0003"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0005","num":"0004"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0006","num":"0005"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0007","num":"0006"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 15"}]},"DETDESC":[{},{}]}
