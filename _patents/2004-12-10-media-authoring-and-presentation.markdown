---
title: Media authoring and presentation
abstract: A system and method for facilitating non-linear viewing of media is provided. The system facilitates non-linear viewing of media by providing a scene selector that scans a digitized media and selects a scene in the digitized media and a metadata generator that produces metadata associated with the scenes and relates the metadata to the selected scene. With the scenes annotated with metadata, a playlist generator can generate a playlist of related scenes based on user inputs like queries and a playlist updater can adapt the playlist based on user reaction to the displayed scenes. The scenes can be displayed on a variety of devices exhibiting various levels of intelligence. The displays can be distributed as can the system.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07757171&OS=07757171&RS=07757171
owner: Microsoft Corporation
number: 07757171
owner_city: Redmond
owner_country: US
publication_date: 20041210
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["REFERENCE TO RELATED APPLICATIONS","TECHNICAL FIELD","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION"],"p":["This application is a divisional of U.S. patent application Ser. No. 10\/055,538, entitled \u201cMEDIA AUTHORING AND PRESENTATION,\u201d filed Jan. 23, 2002. This application is also related to co-pending U.S. patent application Ser. No. 11\/009,115 entitled, \u201cMEDIA AUTHORING AND PRESENTATION\u201d filed on Dec. 10, 2004. The entireties of the above-noted applications are incorporated herein by reference.","The present invention relates generally to viewing annotated digital media and more particularly to non-linear viewing of related scenes that are annotated with metadata.","Conventional home video viewing systems have been underutilized due to problems including, but not limited to, the difficulty of digitizing video and\/or still images, complicated user interfaces and time consuming editing. For example, if a home videographer takes a two hour video, upon subsequent review there may only be ten minutes of interesting video (that are distributed between four shorter clips) that the person wants to watch or to have available to watch later. However, selecting the scenes in which the ten minutes appear, digitizing that ten minutes, editing the desired ten minutes, and arranging the shorter clips into an enjoyable, accessible presentation has conventionally been difficult. With the appearance of more and more digital cameras, both still and video, a system that facilitates simpler, faster and more widely available enjoyment of home video is desired.","The following presents a simplified summary of the invention in order to provide a basic understanding of some aspects of the invention. This summary is not an extensive overview of the invention. It is not intended to identify key or critical elements of the invention or to delineate the scope of the invention. Its sole purpose is to present some concepts of the invention in a simplified form as a prelude to the more detailed description that is presented later.","The present invention relates to a media authoring and presentation system that delivers media clip highlights (e.g., pictures, video and audio clips) to selected local and remote, active and passive connected display devices. The short media clips play randomly and each media clip is a point of entry to full length versions of those media clips stored in a media data store. The media clips serve as points of entry to facilitate non-linear viewing of additional related media from a media data store. The media is stored in a data store that facilitates accommodating multiple media types. The media is annotated with metadata that facilitates non-linear retrieval and viewing of the media. Thus, the system can continuously analyze the media to facilitate intelligent search and retrieval of related content from the data store.","Digitizing, storing and retrieving related scenes is facilitated by a user interface that simplifies selecting scenes, navigating within a media store of scenes and creating a playlist of scenes. Additionally, an application programming interface is provided that simplifies programmatic control of and access to the unannotated media and\/or metadata annotated media, which facilitates automating scene selection, scene organization and scene retrieval. Information concerning the annotated scenes can be transmitted between two or more computer components in a system and thus data packets adapted to transmit such data are provided. Such data packets can be related to a data structure that stores the media and the annotating metadata associated with the media.","The present invention is now described with reference to the drawings, where like reference numerals are used to refer to like elements throughout. In the following description, for purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of the present invention. It may be evident, however, to one skilled in the art, that the present invention may be practiced without these specific details. In other instances, well-known structures and devices are shown in block diagram form in order to facilitate description of the present invention.","As used in this application, the term \u201ccomputer component\u201d is intended to refer to a computer-related entity, either hardware, a combination of hardware and software, software, or software in execution. For example, a computer component may be, but is not limited to being, a process running on a processor, a processor, an object, an executable, a thread of execution, a program and a computer. By way of illustration, both an application running on a server and the server can be computer components. One or more computer components can reside within a process and\/or thread of execution and a computer component can be localized on one computer and\/or distributed between two or more computers.",{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 1","b":["100","100","110","120","130","140","140"]},"The digitized media can be produced by a home videographer in various forms including, but not limited to, video and still images, with and\/or without audio. Such digitized media can be stored, for example, on disks, memory sticks, other memories, compact disks (CDs), digital versatile disks (DVDs) and the like.","When the media includes more than one scene, (e.g., a home video with ten scenes), the scene selector  receives a digitized media and processes through the available scenes on the digitized media. The scene selector  can select a scene to annotate in various ways. By way of illustration and not limitation, the scene selector  may scan an input digitized media and select a scene to annotate based on recognizing a face in the scene. When face recognition is employed to select a scene, the metadata generator  can then annotate the scene with data associated with the recognized face. For example, a face identification number can be stored in the metadata, as can various data evaluated by the face recognizer (e.g., distance between eyes, distance between other facial reference points). By way of further illustration, the scene selector  can select a scene based on recognizing an item in the scene. For example, a videographer may be interested in viewing only the scenes in which a white car appears. Thus, the scene selector  scans the digitized media and select scenes in which a white car appears. Then, the metadata generator  annotates the scene with metadata concerning the white car. In both the face recognition example, and the white car recognition example, standard metadata can be generated. Such standard metadata can include, but is not limited to, the date of the scene, the time of the scene, the videographer, the length of the scene, the longer media from which the scene was retrieved, and so on. Similarly, the scene selector  can also select scenes based on methods including, but not limited to, voice recognition, color recognition, mood recognition and theme recognition. When a scene is selected through such methods, both method specific metadata and standard metadata are generated and associated with the scene.","The methods by which a scene can be selected can be adapted over time to respond to inputs from a user concerning whether the identified scene is one in which the user is actually interested. For example, in the white car example, while the scene selector may identify a scene in which a white SUV appears and a scene in which a white sedan appears, the user may only be interested in scenes including the white SUV. Thus, the user can provide an input to the scene selector  that adapts the item matching method and\/or the scene selector  to make it more likely that scenes including a white SUV will be selected and to make it less likely that scenes including a white sedan will be selected. The effect of such adaptations can be temporary or more permanent, based, for example, on user configuration of the system  and\/or any of its components.","The metadata produced by the metadata generator  can include, but is not limited to a date, a time, a length, a subject, a mood, a theme, a color, a person name, a set of person names, an item name and a set of item names associated with the scene. One or more pieces of such metadata can be generated for each scene. For example, a first scene may include both a face that is interesting to a user and a white car that is interesting to a user while a second scene may only include the white car. Thus, the metadata generator  can produce more metadata for the first scene than for the second scene. Therefore, the data structure employed to store the metadata associated with the scenes can vary in length based on the amount and\/or type of metadata associated with a scene. It is to be appreciated that the scene selector , the metadata generator  and the organizer  can be computer components, as that term is defined herein.","In view of the exemplary systems shown and described herein, methodologies, which can be implemented in accordance with the present invention will be better appreciated with reference to the flow diagrams of , , , , and . While for purposes of simplicity of explanation, the illustrated methodologies are shown and described as a series of blocks, it is to be understood and appreciated that the present invention is not limited by the order of the blocks, as some blocks may, in accordance with the present invention, occur in different orders and\/or concurrently with other blocks from that shown and described herein. Moreover, not all illustrated blocks may be required to implement a methodology in accordance with the present invention. Furthermore, additional and\/or alternative methodologies may employ additional blocks, not illustrated herein. In one example of the present invention, such methodologies can be implemented as computer executable instructions that can be stored on computer readable media including, but not limited to, disks, memories and carrier waves.",{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 2","b":["200","200","210","200","200","220","220"]},"At , the annotated scene and the metadata are stored in a manner that facilitates non-linear retrieval of the annotated scene. Non-linear retrieval relates to locating scenes not in the order in which they appeared in a longer digitized media or the order in which they are stored, but in a manner, for example, where a first scene from the end of a longer first digitized media may be viewed first followed by a second scene from the start of the first digitized media followed by a third scene from the middle of a second digitized media and a fourth scene from the middle of the first digitized media. Thus, rather than starting a first home video at its beginning and watching it linearly, (e.g., from start to finish) then starting a second home video and watching it from start to finish, the present invention facilitates retrieving scenes from various locations from various digitized media. Such non-linear viewing is facilitated by the storage method employed at , which can include, but is not limited to, storing the selected scene and the annotating metadata in at least one of a database and a datacube, where the scene and\/or the annotating metadata include references to each other.",{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 3","b":["300","300","310","310","300","320","310","320","320","310","320","310"]},"The system  includes a metadata analyzer  that analyzes annotating metadata. The result of the analysis performed by the metadata analyzer  is an identification of one or more relationships involving the annotating metadata. Such relationships can occur between process specific metadata (e.g., facial recognition metadata) and between generic metadata (e.g., video date). By way of illustration, the metadata analyzer  may analyze metadata for substantially all scenes that include facial recognition metadata and determine which scenes share a common face. By way of further illustration, the metadata analyzer  can analyze metadata for substantially all scenes shot by a common videographer and determine which scenes share a common and\/or related theme (e.g., outdoor scenes, action scenes). While two relationships are identified, it is to be appreciated that the metadata analyzer  can identify a variety of relationships.","The system  also includes a playlist generator  that evaluates the relationships identified by the metadata analyzer  and produces a playlist of related scenes. For example, the playlist generator  may produce a playlist that includes a variety of scenes that substantially all include a common face, or that substantially all include at least one face from an identified set of faces. Whether to include a scene in a playlist can be determined by analyzing, for example, a similarity value produced by the metadata analyzer  for a relationship that it identified. By way of illustration, voice recognition may not produce a digital (e.g., yes\/no) identification of whether a voice that appears in a first scene is the same voice that appears in a second scene. The voice recognition may instead produce a confidence value concerning the likelihood that two voices belong to the same speaker. Thus, the metadata analyzer  can compute a similarity score between two scenes based, at least in part, on the confidence value from the voice recognition. Then, the playlist generator  can include scenes that score above a pre-determined, configurable threshold. In one example of the present invention, the playlist generator  can present options to a user concerning whether a scene should be included in a playlist. Then, based on the user accepting or rejecting the offered scene, the playlist generator  can be adapted via machine learning techniques to make it more or less likely that a similar scene will be offered for inclusion in the playlist. It is to be appreciated that the scene retriever , the metadata analyzer  and the playlist generator  can be computer components.","The system  can produce one or more playlists. For example, a first user may configure the scene retriever  to retrieve a first set of scenes from the media store  (e.g., scenes including fast motion) while a second user may configure the scene retriever  to retrieve a second set of scenes from the media store  (e.g., scenes in which a dog appears). Given the two different sets of scenes retrieved by the scene retriever , the metadata analyzer  can perform different analyses and identify different relationships. Thus, the playlist generator  can produce different playlists based on the different processing performed by the scene retriever  and the metadata analyzer .",{"@attributes":{"id":"p-0040","num":"0039"},"figref":["FIG. 4","FIG. 3","FIG. 3","FIG. 3","FIG. 3"],"b":["400","400","450","400","460","450","460","470","470","450","460","470","400","410","310","420","320","430","330","440","340"]},"The viewer  can be, for example, an active device that can not only identify and\/or receive the scene to be displayed from the playlist, but which can also manipulate the scene (e.g., crop scene, rotate scene, slow action, speed action). The viewer  can also be, for example, a passive device that simply displays the scene it receives without any additional processing. Furthermore, the viewer  can be an intelligent device that performs processing on the received scene (e.g., color correction, digital data reconstruction, decompression, color conversions, voice translation). The viewer  can also be, for example, a non-intelligent device that simply displays the data it receives. While four examples of viewers  are described, it is to be appreciated that such examples are merely illustrative and are not intended to limit the present invention.","Given the rich variety of viewers , a correspondingly rich set of user feedbacks can be provided to the feedback receiver . By way of illustration, the feedback can include, but is not limited to, a touch input, a typed input, a mouse input, a voice input and\/or a facial expression input concerning the viewed scene. For example, if the viewer  is a touch screen, then a user can interact with a touch screen oriented user interface to indicate feedback concerning a current scene that is being viewed. Such a feedback can include, but is not limited to, a command to skip ahead in the playlist, a command to skip back in the playlist, a command to generate a new playlist, a command to find scenes similar to the current scene and a command to play a longer scene related to the current scene. Thus, the non-linear viewing of scenes is facilitated and the ease of use of home video viewing is improved.","By way of illustration, after the operation of the scene retriever , the metadata analyzer , and the playlist generator , the viewer  may display scenes identified in a playlist. The user watching the viewer  may watch several related scenes and then decide that a certain scene should not be included in the list. Thus, the user may interact with the viewer  and\/or the feedback receiver  and indicate that the scene should be removed from the playlist. For example, the user may click on a \u201cremove scene\u201d button on a graphical user interface. By way of further illustration, the user watching the viewer  may watch several related scenes in a playlist and may have a strong emotional (e.g., joy) reaction to three of the scenes. An intelligent, active viewer  and\/or feedback receiver  may recognize the emotional reaction via, for example, facial expressions, and determine that the playlist should be updated to include similar scenes and to remove scenes that do not generate a joyous reaction. Thus, the playlist updater  can receive inputs from the feedback receiver  to add scenes to the playlist, to remove scenes from the playlist and so on. When an input to add a scene to the playlist is encountered, the playlist updater  can invoke processing in the playlist generator , the metadata analyzer  and the scene retriever , for example, to find, retrieve, analyze and include scenes in the playlist. By way of further illustration, a user watching a scene on the viewer  may decide that a certain scene is interesting and that the user would like to see the entire available digitized media from which the scene was retrieved. Such entire available digitized media may not be the original digitized media. For example, the original digitized media may have been edited to remove certain content, to slow down certain portions, to correct colors, to increase\/decrease the volume of audio in certain portions and\/or to add effects, and so on. The metadata associated with a scene included in a playlist can include references to the longer digitized media from which the scene was retrieved, which facilitates non-linear viewing of media. The metadata associated with a scene can also include references to other similar scenes (e.g., same faces, same voices, same items) and\/or other longer digitized media from which such scenes were taken.","Thus, a user can watch a scene, see a face of a friend, and decide to watch other scenes in which that friend's face appears. Conventionally, such viewing would be difficult, if not impossible to perform, requiring the user to linearly watch a series of tapes from start to finish, which would include an abundance of material not related to the face of the friend in whom the user is interested. By employing the metadata annotated scenes provided by the present invention, a user can interact with a scene (e.g., frame a face and click on the face) and be presented with a new playlist of scenes that include that face. As the user watches the new playlist of scenes, the user can further interact with a scene and request a longer presentation associated with a scene. While watching the longer presentation, the user may hear a long-forgotten but familiar voice and decide to watch scenes that include that voice. The present invention facilitates retrieving such scenes and presenting a new playlist for user perusal. This sample session illustrates one example of the non-linear viewing of home video that is facilitated by the present invention.","While a playlist can be updated directly by user inputs, the present invention is not so limited. In one example, the playlist updater  can update the playlist based on factors including, but not limited to, a usage data, a feedback command and a time stamp. For example, if a certain scene has been viewed more than a pre-determined, configurable number of times, the playlist updater  can begin to \u201cage out\u201d the scene (e.g., progressively show the scene less frequently) so that the scene does not become stale to the user. Furthermore, the playlist updater  can monitor the calendar and manipulate the playlist based on the calendar and date metadata associated with a scene. By way of illustration, as Thanksgiving approaches, the playlist updater  can cause more scenes that were shot around previous Thanksgiving days to be included, while removing scenes that were filmed in May, June and July. By way of further illustration, as the birthday of a loved one approaches, the playlist updater  can cause more scenes that include the loved one to be included in the playlist, and can remove scenes that are unrelated to the loved one.",{"@attributes":{"id":"p-0046","num":"0045"},"figref":"FIG. 5","b":["500","500","510","520","530"]},"At , selecting a stored scene can be based, for example, on the presence of a face in the scene, on the absence of a face in the scene, on the presence of an item in the scene, on the absence of an item in the scene, on the presence of a voice in the scene, on the absence of a voice in the scene, on a mood of the scene, on the theme of the scene, and the like. While face, item, voice, theme and mood are provided as examples at , it is to be appreciated that other selection criteria can be employed by the present invention.","At , analyzing metadata associated with a scene involves computing a similarity score for metadata that hold information concerning, for example, a present face, a present item, a present voice, a mood and a theme. By way of illustration, a first scene may be annotated with first metadata items that identify a first set of faces appearing in the scene and a second scene may be annotated with second metadata items that identify a second set of faces appearing in the second scene. At , the number of matching faces and the confidence value for such possible matches can be computed into a scene similarity score that is then employed to determine whether to include a scene in a playlist.","At , a playlist is generated. In one example of the present invention, the playlist includes the scene and a reference to metadata associated with the scene. In another example of the present invention, the playlist includes references to scenes and related metadata. In yet another example of the present invention, the playlist includes both a reference to a scene and a reference to metadata associated with the scene. Thus, it is to be appreciated that the playlist is not limited to containing only one type of data.",{"@attributes":{"id":"p-0050","num":"0049"},"figref":"FIG. 6","b":["600","610","620","620"]},"At , the method  can take an action based on the user feedback. For example, the method  can take actions including, but not limited to, moving forward in the playlist, moving backward in the playlist, searching for related media, and displaying a media item related to the scene. In some cases, the user feedback received at  and responded to at  may require the playlist to be updated. For example, if the user feedback indicates that the user would like to see more scenes that include a voice heard in a displayed scene, then this warrants an update to the playlist. Thus, at  a determination is made concerning whether the playlist is to be updated. If the determination at  is YES, then at  the playlist is updated based, at least in part, on the user feedback, otherwise processing returns to . Updating the playlist can include, by way of illustration, and not limitation, adding a scene to the playlist, removing a scene from the playlist, reordering scenes in a playlist and altering the frequency with which scenes in the playlist are displayed.",{"@attributes":{"id":"p-0052","num":"0051"},"figref":["FIG. 7","FIG. 6"],"b":["700","700","710","720","730","770"]},"At , a determination is made concerning whether the user feedback commands skipping ahead in the playlist. If the determination is YES, then at , the next scene in the playlist is presented. At , a determination is made concerning whether the user feedback commands skipping back in the playlist. If the determination is YES, then at , the previous scene in the playlist is presented. At , a determination is made concerning whether the feedback commands finding scenes related to the displayed scene. If the determination at  is YES, then at  a search for related scenes is undertaken. Such a search can be performed on parameters including, but not limited to, faces, items, voices, colors, moods, themes and the like. Such parameters can be retrieved, for example, from metadata associated with the viewed scene and can be searched for in metadata associated with other scenes.","At , a determination is made concerning whether the user feedback commands creating a new playlist. If the determination is YES, then at , the existing playlist is cleared and a method to create a new playlist is invoked. At , a determination is made concerning whether the user feedback commands navigating within a playlist. If the determination at  is YES, then at , a next desired scene is selected as the scene to display next from the playlist. At , a determination is made concerning whether there is any more user feedback. If the determination at  is YES, then processing returns to , otherwise processing can conclude. While  illustrates five possible user feedbacks, it is to be appreciated that the present invention can employ a greater and\/or lesser number of such feedbacks.",{"@attributes":{"id":"p-0055","num":"0054"},"figref":"FIG. 8","b":["800","800","850","830","840","820","820","820","810","820","840","830","810"]},"In one example of the system , the annotating metadata  can include, but is not limited to, a date identifier that identifies the date when the video was taken, a time identifier that identifies a time when the video was taken, a videographer identifier that identifies who took the video, a face identifier that identifies one or more faces in the video, an item identifier that identifies one or more items in the video, a voice identifier that identifies one or more voices in the video, a mood identifier that identifies one or more moods associated with the video, and a theme identifier that identifies one or more themes associated with the video.","In one example of the present invention, the annotating metadata  is generated manually by a user. For example, the user can create the annotating metadata topic (e.g., mood, theme) and then assign a value for the metadata topic (e.g., happy, Veteran's day). In another example of the present invention, the annotating metadata  is generated automatically by a computer component. Such a computer component generates the annotating metadata  based, at least in part, on face recognition processing, item recognition processing, voice recognition processing, mood recognition processing and theme recognition processing.","In yet another example of the system, the annotating tool  is adapted by a machine learning technique based, at least in part, on a user input concerning the annotating metadata  generated by the annotating tool . For example, the annotating tool  can generate a value for a metadata topic (e.g., happy for mood), yet the user may determine that the mood is actually \u201cecstatic\u201d. Thus, the user can reconfigure one or more configurable parameters associated with the annotating tool  to make it more likely that the annotating tool  would identify the scene and similar scenes as \u201cecstatic\u201d rather than \u201chappy\u201d. Such configurable parameters may be threshold values for a neural network, a count of scene emotion identifiers (e.g., number of smiles per minute, number of different smiling people), and the like. It is to be appreciated that the annotating tool  can be a computer component.",{"@attributes":{"id":"p-0059","num":"0058"},"figref":"FIG. 9","b":["900","900","920","920","910","920","920","920"]},"The system  also includes a presenter  that presents displayable items retrieved from the media store . The presenter  can be an active and\/or passive display that does or does not have local intelligence. In one example of the present invention, the presenter  presents a first displayable item from the media store  and then the system  accepts feedback concerning which displayable item should be displayed next. The feedback can be, for example, a spoken word, a keystroke, a mouse click, and a facial expression. Such feedback facilitates viewing scenes in an order desired by the user rather than linearly from start to finish in a pre-determined order, providing advantages over conventional systems.","To facilitate such non-linear viewing, the system  also includes a selector  that selects a second displayable item from the media store  based, at least in part, on a relationship between a metadata associated with the first displayed item and a metadata associated with the second displayable item. Thus, the order in which video segments are viewed can depend on user reaction to displayed video segments. It is to be appreciated that the presenter  and the selector  can be computer components.",{"@attributes":{"id":"p-0062","num":"0061"},"figref":"FIG. 10","b":["1000","1000","1020","1010","1010","1030"]},"The system  also includes a playlist generator  that generates a playlist  of annotated media items . Such annotated media items  may be related, for example, by a first metadata retrieved in response to a first query. By way of illustration, a user may have generated an SQL query to an SQL database to retrieve scenes in which a favorite dog appears. Thus, the playlist  can contain media items related by the metadata associated with scenes that include metadata indicating the presence of the favorite dog.","The system  also includes a presenter  for presenting annotated media items  identified in the playlist . The presenter  can be, for example, an intelligent device (e.g., personal computer) or a dumb device (e.g., standard television). While a user watches media items identified in the playlist , the user may decide to watch different videos and\/or to update the playlist . In deciding to watch different videos, and\/or to watch complete versions of displayed scenes, the user may generate a second query that retrieves a second set of metadata responsive to the second query. Thus, the system  includes a playlist updater  that updates the playlist  based, for example, on the second metadata retrieved in response to a second query. Since the second metadata was responsive to a query, and the query was generated as a response to viewing a first scene that had related first metadata, the second metadata can be related to the first metadata, which facilitates faster retrieval of related scenes by limiting the amount of metadata that is searched. In one example of the present invention, the annotator , playlist generator , playlist updater  and presenter  are computer components.",{"@attributes":{"id":"p-0065","num":"0064"},"figref":"FIG. 11","b":["1100","1100","1110","1100","1120","1100","1130","1100"]},"At , the method  generates a playlist of media items related by one or more metadata items in the first metadata. For example, while a first number of scenes may include an identified voice, at , the method  may select the scenes where the identified voice appears at least ten percent of the time. Thus, the playlist will contain voice identification related metadata items. After the playlist has been generated, at , media items listed in the playlist will be displayed. A user watching the media items in the playlist can generate a feedback concerning which media item they wish to view next. Thus, at , the method  receives a second query related to identifying a media item by a relationship between the media item and a metadata. For example, while viewing a scene in which the desired voice appears, the user may see an item (e.g., a snowman) that prompts the user to desire to view different videos (e.g., snow covered, holiday theme videos in which the desired voice appears). Thus, the user can generate a second query to retrieve such videos. The query can cause an examination of metadata associated with video scenes and retrieval, at , of a second metadata responsive to the second query. When the second metadata responsive to the second query has been retrieved, then at , the playlist can be updated based, at least in part, on that second metadata. While a linear flow is depicted in , it is to be appreciated that the method  may loop.",{"@attributes":{"id":"p-0067","num":"0066"},"figref":"FIG. 12","b":["1200","1200","1210","1210","1200","1220"]},"The data structure  is merely exemplary and it is to be appreciated that numerous other structures are contemplated that provide for organizing and\/or storing a plurality of data types conducive to facilitating the non-linear viewing of related media scenes in connection with the subject invention. Any such data structure suitable for employment in connection with the present invention is intended to fall within the scope of the appended claims. Such data structures can be stored in computer readable media including, but not limited to, memories, disks and carrier waves.",{"@attributes":{"id":"p-0069","num":"0068"},"figref":"FIG. 13","b":["1300","1300","1300","1300","1300","1300","1300","1300"]},"Thus, in one example of the present invention, a computer system that facilitates non-linear viewing of media includes a graphical user interface that has a display and a selection device. The display may be, for example, an active or passive, intelligent or non-intelligent device. The graphical user interface supports a method of providing and selecting from a set of graphic user interface elements on the display. The graphical user interface elements can include, but are not limited to, buttons, menus, sliders, drop down boxes, frames, halos, radio buttons, check boxes, and the like. The graphical user interface can retrieve a set of graphic user interface elements where the interface elements represent one or more actions action associated with facilitating the non-linear display of media items. For example, an element can be associated with skipping ahead or skipping back in a playlist. The graphical user interface displays the set of interface elements on the display and receives an interface element selection signal that indicates which of the set of interface elements has been chosen by a user. In response to the interface element selection signal, the graphical user interface initiates processing to facilitate non-linear viewing of media.",{"@attributes":{"id":"p-0071","num":"0070"},"figref":"FIG. 14","b":["1400","1400","1410","1410","1400","1420","1400","1430"]},"The data packet  also includes a metadata key  that facilitates retrieving metadata associated with the clip identified in the clip identifier field . For example, while the clip identifier  may be a primary key accessible in an SQL database, the metadata key  may be a secondary key similarly accessible in the SQL database or a separate primary key in a second database. The data packet  includes a data field  in the packet  that includes various information that is intended to be communicated to the receiving computer component. The data packet  ends with a cyclical redundancy check (CRC) field  that serves as an error detecting field whereby a receiving device can determine if it has properly received a packet . While six fields are illustrated in data packet , it is to be appreciated that a greater and\/or lesser number of fields can be employed in packets utilized by the present invention.","Another example data packet may be transmitted between a computer component implementing an annotating tool and a media store. Such a data packet (not illustrated) may include, for example, a first field that stores a clip identifier that identifies a portion of a media. The identifier may be, for example, a globally unique identifier that facilitates locating the clip regardless of storage location. The example data packet can also include a second field that stores a metadata key that identifies an annotating metadata associated with the clip identified by the clip identifier. Again, such metadata key may be a globally unique identifier that facilitates retrieving the metadata from various distributed media stores. The data packet can also include a third field that stores data associated with the clip identified by the clip identifier. Such data may be, for example, the clip, or a longer work from which the clip was derived.","Yet another data packet adapted to be transmitted between a user interface and a playlist updater to facilitate the non-linear viewing of a media can include a first field that stores a clip identifier that identifies a portion of a media, the clip identifier substantially similar to the clip identifier described in the other data packets. The data packet can also include a second field that stores a requested user action concerning the portion identified by the clip identifier. For example, the second field can store commands to add a scene to a playlist, remove a scene from a playlist and to search for related scenes. The data packet can also include a third field that stores metadata associated with the portion identified by the clip identifier. Such metadata can be employed to retrieve one or more clips according to the requested user action.",{"@attributes":{"id":"p-0075","num":"0074"},"figref":["FIG. 15","FIG. 14"],"b":["1500","1450","1500"]},"The sub-fields  include a date field  that can hold information concerning the date when a media item was filmed. A time field  can hold information relating to the time when the media item was filmed, and\/or, for example, the length, in periods of time, of a media item. A person field  can hold information concerning people who are recognized in the scene. Such recognition may have occurred, by, for example, voice recognition and\/or face recognition. Similarly, an item field  can hold information concerning items that are recognized in the scene. Other fields include, but are not limited to, a mood field , a theme field , and a security field . The security field  can hold, for example, identifiers associated with users who are permitted to view the media related to the data packet .","Referring now to , an application programming interface (API)  is illustrated providing access to a system  that includes an annotating tool  and a user interface . The API  may be employed, for example, by programmers  and\/or processes  to gain access to processing performed by the system . Similarly, the API  may be employed to provide data values to the system  and\/or retrieve data values from the system . Thus, in one example of the present invention, a set of application program interfaces can be embodied on a computer-readable medium. The interfaces can be executed by a computer component to gain access to an annotating tool that is employed to annotate media in a manner that facilitates non-linear retrieval of the media. Such interfaces can include, but are not limited to, a first interface that receives media information, a second interface that receives annotation information associated with the media, and a third interface that receives user interface information associated with the order in which media will be displayed.","In order to provide additional context for various aspects of the present invention,  and the following discussion are intended to provide a brief, general description of a suitable operating environment  in which various aspects of the present invention may be implemented.  provides an additional and\/or alternative operating environment in which the present invention can operate. While the invention is described in the general context of computer-executable instructions, such as program modules, executed by one or more computers or other devices, those skilled in the art will recognize that the invention can also be implemented in combination with other program modules and\/or as a combination of hardware and software. Generally, however, program modules include routines, programs, objects, components, data structures, etc. that perform particular tasks or implement particular data types. The operating environment  is only one example of a suitable operating environment and is not intended to suggest any limitation as to the scope of use or functionality of the invention. Other well known computer systems, environments, and\/or configurations that may be suitable for use with the invention include but are not limited to, personal computers, hand-held or laptop devices, multiprocessor systems, microprocessor-based systems, programmable consumer electronics, network PCs, minicomputers, mainframe computers, distributed computing environments that include the above systems or devices, and the like.","With reference to , an exemplary environment  for implementing various aspects of the invention includes a computer . The computer  includes a processing unit , a system memory , and a system bus . The system bus  couples system components including, but not limited to, the system memory  to the processing unit . The processing unit  can be any of various available processors. Dual microprocessors and other multiprocessor architectures also can be employed as the processing unit .","The system bus  can be any of several types of bus structure including the memory bus or memory controller, a peripheral bus or external bus, and\/or a local bus using any variety of available bus architectures including but not limited to 8-bit bus, Industrial Standard Architecture (ISA), Micro-Channel Architecture (MSA), Extended ISA (EISA), Intelligent Drive Electronics (IDE), VESA Local Bus (VLB), Peripheral Component Interconnect (PCI), Universal Serial Bus (USB), Advanced Graphics Port (AGP), Personal Computer Memory Card International Association bus (PCMCIA), and Small Computer Systems Interface (SCSI).","The system memory  includes volatile memory  and nonvolatile memory . The basic input\/output system (BIOS), containing the basic routines to transfer information between elements within the computer , such as during start-up, is stored in nonvolatile memory . By way of illustration, and not limitation, nonvolatile memory  can include read only memory (ROM), programmable ROM (PROM), electrically programmable ROM (EPROM), electrically erasable ROM (EEPROM), or flash memory. Volatile memory  includes random access memory (RAM), which acts as external cache memory. By way of illustration and not limitation, RAM is available in many forms such as synchronous RAM (SRAM), dynamic RAM (DRAM), synchronous DRAM (SDRAM), double data rate SDRAM (DDR SDRAM), enhanced SDRAM (ESDRAM), Synchlink DRAM (SLDRAM), and direct Rambus RAM (DRRAM).","Computer  also includes removable\/nonremovable, volatile\/nonvolatile computer storage media.  illustrates, for example a disk storage . Disk storage  includes, but is not limited to, devices like a magnetic disk drive, floppy disk drive, tape drive, Jazz drive, Zip drive, LS-100 drive, flash memory card, or memory stick. In addition, disk storage  can include storage media separately or in combination with other storage media including but not limited to an optical disk drive such as a compact disk ROM device (CD-ROM), CD recordable drive (CD-R Drive), CD rewritable drive (CD-RW Drive) or a digital versatile disk ROM drive (DVD-ROM). To facilitate connection of the disk storage devices  to the system bus , a removable or non-removable interface is typically used such as interface .","It is to be appreciated that  describes software that acts as an intermediary between users and the basic computer resources described in suitable operating environment . Such software includes an operating system . Operating system , which can be stored on disk storage , acts to control and allocate resources of the computer system . System applications  take advantage of the management of resources by operating system  through program modules  and program data  stored either in system memory  or on disk storage . It is to be appreciated that the present invention can be implemented with various operating systems or combinations of operating systems.","A user enters commands or information into the computer  through input device(s) . Input devices  include, but are not limited to a pointing device such as a mouse, trackball, stylus, touch pad, keyboard, microphone, joystick, game pad, satellite dish, scanner, TV tuner card, digital camera, digital video camera, web camera, and the like. These and other input devices connect to the possessing unit  through the system bus  via interface port(s) . Interface port(s)  include, for example, a serial port, a parallel port, a game port, and a universal serial bus (USB). Output device(s)  use some of the same type of ports as input device(s) . Thus, for example, a USB port may be used to provide input to computer , and to output information from computer  to an output device . Output adapter  is provided to illustrate that there are some output devices  like monitors, speakers, and printers among other output devices  that require special adapters. The output adapters  include, by way of illustration and not limitation, video and sound cards that provide a means of connection between the output device  and the system bus . It should be noted that other devices and\/or systems of devices provide both input and output capabilities such as remote computer(s) .","Computer  can operate in a networked environment using logical connections to one or more remote computers, such as remote computer . The remote computer  can be a personal computer, a server, a router, a network PC, a workstation, a microprocessor based appliance, a peer device or other common network node and the like, and typically includes many or all of the elements described relative to computer . For purposes of brevity, only a memory storage device  is illustrated with remote computer . Remote computer  is logically connected to computer  through a network interface  and then physically connected via communication connection . Network interface  encompasses communication networks such as local-area networks (LAN) and wide-area networks (WAN). LAN technologies include Fiber Distributed Data Interface (FDDI), Copper Distributed Data Interface (CDDI), Ethernet\/IEEE 802.3, Token Ring\/IEEE 802.5 and the like. WAN technologies include, but are not limited to, point-to-point links, circuit switching networks like Integrated Services Digital Networks (ISDN) and variations thereon, packet switching networks, and Digital Subscriber Lines (DSL).","Communication connection(s)  refers to the hardware\/software employed to connect the network interface  to the bus . While communication connection  is shown for illustrative clarity inside computer , it can also be external to computer . The hardware\/software necessary for connection to the network interface  includes, for exemplary purposes only, internal and external technologies such as, modems including regular telephone grade modems, cable modems and DSL modems, ISDN adapters, and Ethernet cards.",{"@attributes":{"id":"p-0087","num":"0086"},"figref":"FIG. 18","b":["1800","1800","1800","1810","1820","1810","1820"]},"The displays  and  display media items identified in a playlist . The playlist  is generated by an annotating and authoring system  and refers to media items that are stored in a media store . While a single annotating and authoring system  is illustrated, it is to be appreciated that cooperating computer components can be employed to implement the annotating and authoring system . Thus, the computer components can be distributed between various processors, processes, threads, and locations. Similarly, while a single media store  is illustrated, it is to be appreciated that a distributed data storage can be employed with the present invention. Thus, in one example of the present invention, media items identified in the playlist  can be stored at different locations, in different formats and retrieved by different methods.","What has been described above includes examples of the present invention. It is of course, not possible to describe every conceivable combination of components or methodologies for purposes of describing the present invention, but one of ordinary skill in the art will recognize that many further combinations and permutations of the present invention are possible. Accordingly, the present invention is intended to embrace all such alterations, modifications and variations that fall within the spirit and scope of the appended claims. Furthermore, to the extent that the term \u201cincludes\u201d is used in either the detailed description or the claims, such term is intended to be inclusive in a manner similar to the term \u201ccomprising\u201d, as comprising is interpreted as a transitional word in a claim."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 18"}]},"DETDESC":[{},{}]}
