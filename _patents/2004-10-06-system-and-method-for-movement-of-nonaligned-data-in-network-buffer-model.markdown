---
title: System and method for movement of non-aligned data in network buffer model
abstract: A method is provided for transferring data between first and second nodes of a network. Such method includes requesting first data to be transferred by a first upper layer protocol (ULP) operating on the first node of the network; and buffering second data for transfer to the second node by a lower protocol layer lower than the first ULP, the second data including an integral number of standard size units of data including the first data. The method further includes posting the second data to the network for delivery to the second node; receiving the second data at the second node; and from the received data, delivering the first data to a second ULP operating on the second node. The method is of particular application when transferring the data in unit size is faster than transferring the data in other than unit size.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07840643&OS=07840643&RS=07840643
owner: International Business Machines Corporation
number: 07840643
owner_city: Armonk
owner_country: US
publication_date: 20041006
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION"],"p":["The present invention is directed to the communication of data within a network, especially a network of computing devices.","An important factor that affects the speed of processing within a computer network is the speed at which data is communicated between computers. Data communication speed is especially important in more complex networks in which clusters of smaller networks of computers are linked to each other and share certain common resources. The speed at which data is communicated in such networks impacts the availability of common resources, in turn affecting processing speed and potentially the integrity of the data available to the computers in the network.","Conventionally, data is transferred between the nodes of a network in whatever quantity is requested by a requesting agent, an upper layer protocol being an example of such agent. In some networks, when the requested quantity is a unit size quantity of data, the data is transferred to the network in a relatively quick manner because of predetermined procedures for handling such transfers in unit size quantities. However, when the requested quantity is not a unit size, the available procedures for handling such transfers may cause the data to be transferred more slowly to the network. Non-unit size data transfers contribute to latency, which can be considered idle time or wasted time for the system, when processing at a receiving node awaits receipt of data from another node. In a network, latency is the amount of time it takes data to transmit and receive a user message having zero length. A zero length message has a protocol header, so the actual number of bytes transferred is greater than zero but the header still has a smaller number of bytes than a cache line size, which is typically 128 bytes. On the other hand, bandwidth is the amount of data that can be transmitted and received per unit of time. Bandwidth is particularly important for the transfer of data between devices, e.g., nodes of a network.","Data transfers in a network can be categorized as either a memory transfer or an input\/output (I\/O) transfer. In either case, a common system or network bus may be used to route the data. Since a bus is a shared resource, latency affects performance.",{"@attributes":{"id":"p-0006","num":"0005"},"figref":"FIG. 1","b":["210","220","210","220","231","232","241","242","210","241","210","242","220","251","231","210","252","232","220","220","242","220","241","220","252","232","220","251","231","210","251","261","271","252","220","262","272"]},"When a particular amount of data smaller than a standard size is to be transferred from node  to node , for example, when the data is smaller than the size of a cache line, the upper level protocol (ULP) operating on node  requests that the particular amount of data be transferred to node . For example, assume that the particular amount of data to be transferred is 109 bytes, while the cache-line size is 128 bytes. As part of the transfer process, the requested amount (109 bytes) of data is copied from the user buffer  to the SEND FIFO buffer . The network adapter  then copies the requested amount of data from the SEND FIFO buffer  into a memory  of its own. The network adapter  may begin copying the data to its memory  before all of the data has been copied from the user buffer  to the SEND FIFO . Once some of the data is available in the adapter memory , the adapter  then transfers the data to the adapter at the receiving end in any of several available ways for sending data having length smaller than a cache line. Unfortunately, such methods of transferring data can actually take longer to transmit the data than is true when the data is transferred in other than an integral number of units of the data. This is especially so if the data transfer operation is interrupted in progress.","Latency within the network is impacted when the time required for transferring data between nodes is increased, as here. To the node that awaits the transferred data, latency unnecessarily causes delays in processing, since the node awaiting the transferred data cannot either begin processing or continue processing until the transferred data arrives. In addition, the bandwidth for transferring data across the network appears lower when the total amount of time it takes to transfer the data is higher than it is for transferring an integral number of units of the data. Consequently, a need exists for an improved system and method having improved efficiency for transferring data of non-standard size, e.g., non-cache line aligned data, between nodes of a network, to permit a reduction in latency and an increase in bandwidth for such transfers.","According to an embodiment of the invention, a method is provided for transferring data between first and second nodes of a network. Such method includes requesting, by a first upper layer protocol (ULP) operating on the first node of the network, first data to be transferred, the first data having a length not equal to an integral number of standard units of data; and buffering, by a lower protocol layer of the first node, the lower protocol layer being lower than the first ULP, second data in a buffer for transfer to the second node, the second data including an integral number of standard size units of data including the first data the second data being buffered by copying data from a memory at the first node into a buffer, the data being copied in units aligned to cache line boundaries of the memory. The method further includes posting, by the first node, the second data to the network for delivery to the second node; receiving the second data at the second node; and from the received data, delivering the first data to a second ULP operating on the second node, the length of the first data delivered being equal to the length of the first data and not the length of the second data such that remaining data of the second data is not delivered. The method is of particular application when transferring the data in unit size is faster than transferring the data in other than unit size.","According to another aspect of the invention, a machine-readable recording medium having instructions recorded thereon is provided for performing a method of transferring data between first and second nodes of a network, wherein the method includes requesting, by a first upper layer protocol (ULP) operating on the first node of the network, first data to be transferred, the first data having a length not equal to an integral number of standard units of data; and buffering, by a lower protocol layer of the first node, the lower protocol layer being lower than the first ULP, second data in a buffer for transfer to the second node, the second data including an integral number of standard size units of data including the first data, the standard size being a size of a cache line used in a cache of the first node. The method further includes posting, by the first node, the second data to the network for delivery to the second node, the posting including posting each of the units of the second data to the network by a network adapter layer protocol; receiving the second data at the second node, and from the received data, delivering the first data to a second ULP operating on the second node, the length of the first data delivered being equal to the length of the first data and not the length of the second data, such that remaining data of the second data is not delivered.","According to yet another aspect of the invention, a data communication apparatus operable to transfer data to and receive data from a network is provided, which includes a first node, the first node comprising an upper layer protocol (ULP), the ULP operable to request first data to be transferred to a device through the network, the first data having a length not equal to an integral number of standard units of data; and a buffer on the first node to buffer second data for transfer, the second data being buffered by a lower protocol layer, responsive to a command issued by the ULP to buffer data for transfer to the network, the second data including an integral number of standard size units of data, the second data including the first data the second data being buffered by copying data from a memory at the first node into a buffer, the data being copied in units aligned to cache line boundaries of the memory, the first node to post the second data to the network as a first message for delivery to the device, the device being operable to receive the second data. A lower protocol layer on the device is to deliver the first data contained in the transferred second data to a ULP operating on the device, the length of the first data delivered being equal to the length of the first data and not the length of the second data such that remaining data of the second data is not delivered.","The recitation herein of a list of desirable objects which are met by various embodiments of the present invention is not meant to imply or suggest that any or all of these objects are present as essential features, either individually or collectively, in the most general embodiment of the present invention or in any of its more specific embodiments.","As described above, dividing the data to be transferred into multiple smaller portions causes the data to take longer to be transferred from one node of the network to another, and thus negatively impacts latency and bandwidth. Embodiments of the invention overcome this problem by providing a system and method for transferring data between nodes of a network in standard size units of data, e.g. in units of a cache line, instead of attempting to transfer smaller amounts of data or even smaller divided portions thereof. Such standard size units are transferred more quickly across the network, thus reducing latency and increasing bandwidth.",{"@attributes":{"id":"p-0020","num":"0019"},"figref":["FIG. 2","FIG. 2"],"b":["300","300","304","304","304","315","330","315","330","330"]},"Through their network adapters, nodes  communicate with each other to transfer data to and from one or more storage units . The storage units may include storage disks or other storage devices, such as tape drives, semiconductor memories, and the like. Storage units can also be networked directly to permit direct communication between them.","In complex computing environments, each cluster  shown in  may have its own platform and sub-environment, which may be very different than that of the next cluster. To provide transparent processing across these environments, tools are provided for making such processing seamless and platform independent. However, in the description which follows, certain common platform elements of the nodes including a lower layer adapter protocol and upper layer protocol are assumed.","Embodiments of the invention are implemented with reference to a communication protocol stack that is active on a processor of a node and a network adapter of the node.  illustrates an exemplary communication protocol stack for a node. The lower layer of the protocol stack is a hardware abstraction layer (HAL), as illustrated in  at . HAL is a protocol layer that translates higher level commands, such as from a computer operating system, into instructions for interacting with a hardware device at a detailed hardware level. HAL is called from either the operating system kernel or from a device driver running at the node. In either case, such calling program interacts with the network adapter on a more general level than it would otherwise need to interact.","The operating system of a node  transfers data between that node and other nodes of the network through its network adapter by commands passed to and from the HAL. In addition, HAL provides the capability of monitoring, and detecting the activities of multiple types of input devices and those of data transfers across the system environment. In a preferred embodiment, both a low level application programming interface (LAPI) and a message passing interface (MPI) are provided as upper layer protocols on the nodes of the network.  schematically illustrates the relationship between the LAPI , HAL  and MPI  communication protocol layers.","In , for ease of reference for the description of data transfer operations to follow, two nodes out of the many that may exist in a cluster of computer systems  () are illustrated, with the understanding that the data transfers described herein can be conducted between more than the two nodes. Alternatively, the nodes can be part of a less sophisticated computing network, such as two PCs connected simply by a direct link, or over the Internet, or otherwise connected together in processing communication over a more complex network including one or more local area networks (LAN), and\/or one or more wide area networks (WANs) or the like.","In either case, in the schematic illustration of , a first node and a second node are referenced at  and , respectively. Each node has a network adapter associated with it, as illustrated respectively at  and . A user buffer  is provided at the first node , which is controlled by an upper level protocol of the first node , and a user buffer  is provided at the second node , which is controlled by an upper level protocol of the second node ","The upper level protocol of the node  is typically represented by a LAPI protocol layer  or MPI protocol layer , as described above with reference to . LAPI is based on a remote memory copy (RMC) model. RMC is a one-sided programming paradigm similar to the load\/store model of shared memory programming. The one-sided model eases some of the difficulties of the send\/receive model, which uses two-sided protocols.","MPI allows for the transfer of data in a transparent way, which is protocol independent. MPI was developed by a group of industry, academic, and government representatives with experience in developing and using message-passing libraries on a variety of computer systems. MPI was developed to serve as a common standard, bringing together years of research and experience with message passing. Programs expressed this way using MPI may run on distributed-memory multiprocessors, shared-memory multiprocessors, networks of workstations, and combinations of all of these which make it highly suitable for complex computing environments. An important feature of MPI is that it can be used easily regardless of the processing speed, so as the processing speed improves the paradigm will not be made obsolete. MPI can be implemented on a great variety of computing environments including those consisting of collections of other sub-environments, parallel or not, connected by a communication network or a communication means. In addition, MPI provides many features intended to improve performance on scalable parallel computers having specialized interprocessor communication hardware.","A data staging buffer , having a plurality of data storage locations including a send FIFO (SEND FIFO) and a receive FIFO (RECEIVE FIFO) is provided at node , as controlled by an adapter . A like data staging buffer  is provided at node . The data staging buffers  and  can be implemented in contiguous areas of a memory or multiple non-contiguous areas or units.","In the embodiment of the invention shown in , data packets are transferred using the SEND FIFO buffer  of node  at the sender side and the RECEIVE FIFO buffer  of node  at the receiver side of the transfer. Data to be transferred from node  to node  are placed in the SEND FIFO buffer  of node . Such data is then sent by the network adapter  over network  to the RECEIVE FIFO buffer  at the receiving node .","Steps in the transfer of data are now described. Initially, an upper level protocol (ULP) such as MPI or LAPI requests that a chunk of data, for example, 109 bytes, be moved from node  to node . A lower level protocol, e.g., HAL, responding to the request, then copies the data to be transferred into the SEND FIFO buffer  in units of a cache line. In such operation, the data is transferred along cache line boundaries into the SEND FIFO buffer  to match one or more whole cache lines of data. In one example, the size of the cache line is 128 bytes. In such case, the lower level protocol moves data in cache-line size units of 128 bytes from the user buffer  into the SEND FIFO buffer  for transfer to node .","Note that the invention does not restrict the size of the units of data to be transferred, e.g., the cache line size, with which the invention is designed to operate. Different size units of data having different lengths can be utilized. Moreover, transfer sizes that are based upon other storage sizes, e.g. memory block size, page size, buffer FIFO line size, etc. can be utilized instead of the cache line size as the basic size of the unit to be moved. When the cache line size is the basic unit to be moved as here, the number of cache lines needed to transfer the requested amount of data is determined by dividing the requested transfer amount by the cache line size and then rounding up to the next higher number of cache lines.","Transferring the data in units of one or more whole cache lines allows the entire 109 bytes to be transferred during one atomic operation. The method described herein is of particular benefit to systems where it is faster to transmit data as an integral number of standard size units than to transmit data in greater or less than such standard size units. In systems having SEND FIFO buffers and RECEIVE FIFO buffers for moving data between nodes such as here, a lower level protocol layer, e.g., HAL, and the network adapter, move the data in units of a cache line without direct control being asserted over the data movement by the upper level protocols on the respective nodes.","During or after the data is copied from user buffer  to the SEND FIFO , the network adapter  copies the 128 bytes of data from the SEND FIFO buffer  to adapter memory  and transfers the 128 bytes of data in one atomic operation over the network  to the RECEIVE FIFO  of node . At node  at the receiving end, the network adapter  receives the 128 bytes of data into adapter memory  and copies the 128 bytes of data into a RECEIVE FIFO buffer . The lower level protocol on the node  at the receiving end takes only the 109 bytes of data that was requested to be transferred and presents that data to the upper level protocol, indicating the exact location in the RECEIVE FIFO buffer  in which the 109 bytes of data are located. In other words, only the 109 bytes (out of the 128 bytes of the cache line size unit that was transferred) are presented to the upper level protocol.","In cases where LAPI or MPI are used in conjunction with HAL, the LAPI or the MPI directs HAL to transfer the non cache line-aligned data to the requested target node as described above. However, although both MPI and LAPI operate in conjunction with the above-described embodiments of the invention, they are only examples of some of the many upper layer protocols for which embodiments of the invention are intended to benefit.",{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 5","b":["610","620"]},"In step , the network adapter on the sending node constructs second data containing the one or more integral numbers of cache line-size units of data and sends the second data into the network for delivery to the receiving node. Thereafter, in step , a network adapter at the second node receives the second data and buffers the data in an incoming receive FIFO buffer (RECEIVE FIFO), the second data again being buffered in units of data which are aligned to cache lines, each unit having the same size as the cache line size defined for the nodes of the network. Thereafter, in step , the lower level protocol then notifies the ULP of the arrival of the transferred data and provides sufficient details to the ULP for it to collect only the data that was originally requested to be transferred thereto by the first node. Accordingly, by such method, the ULP on a first node of the network transfers data which is not aligned to cache line boundaries from the first node to the second node for receipt of the ULP operating on the second node. However, the data transfer is actually performed in an efficient manner by the lower level protocol and the network adapter transferring an equivalent amount of data between the two nodes that are aligned to cache lines and which have size corresponding to an integral number of cache lines.","According to another aspect of the invention, a machine-readable recording medium having instructions recorded thereon is provided for performing a method of transferring data between first and second nodes of a network.","Accordingly, while the invention has been described in detail herein in accord with certain preferred embodiments thereof, still other modifications and changes therein may be effected by those skilled in the art. Accordingly, it is intended by the appended claims to cover all such modifications and changes as fall within the true spirit and scope of the invention."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"DESCRIPTION OF THE DRAWINGS","p":["The subject matter which is regarded as the invention is particularly pointed out and distinctly claimed in the concluding portion of the specification. The invention, however, both as to organization and method of practice, together with further objects and advantages thereof, may best be understood by reference to the following description taken in connection with the accompanying drawings in which:",{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 5"}]},"DETDESC":[{},{}]}
