---
title: Method and system for generating interpolations of captured video content
abstract: An approach for enabling users to generate an interpolated view of content based on footage captured by multiple cameras is described. An interpolation platform receives a plurality of images from a plurality of video cameras providing overlapping fields of view. A camera angle that is different from angles provided by the plurality of cameras is selected. The interpolation platform then generates an interpolated image corresponding to the selected camera angle using a portion or all of the plurality of images from the plurality of cameras.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09277101&OS=09277101&RS=09277101
owner: VERIZON PATENT AND LICENSING INC.
number: 09277101
owner_city: Basking Ridge
owner_country: US
publication_date: 20111222
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND INFORMATION","DESCRIPTION OF THE PREFERRED EMBODIMENT"],"p":["Service providers are continually challenged to deliver value and convenience to consumers by providing compelling network services and advancing the underlying technologies. One area of interest has been the development of services and technologies for enabling users to view content from multiple perspectives, i.e., image content corresponding to captured video footage. In a typical multi-camera system, each camera is positioned at different angles to capture a recording of a common subject from a unique perspective. The cameras are moved however in order for the multi-camera system to capture any other perspectives of the subject. Unfortunately, moving the cameras is not always feasible depending on the recording environment, venue type, etc. Furthermore, the size of the equipment can make it difficult to move and active recording concurrently.","Based on the foregoing, there is a need for enabling users to generate an interpolated view of content based on footage captured by multiple cameras.","An apparatus, method and software for enabling users to generate an interpolated view of content based on footage captured by multiple cameras is described. In the following description, for the purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of the present invention. It is apparent, however, to one skilled in the art that the present invention may be practiced without these specific details or with an equivalent arrangement. In other instances, well-known structures and devices are shown in block diagram form in order to avoid unnecessarily obscuring the present invention.","Although the various exemplary embodiments are described with respect to video content, it is contemplated that these embodiments have applicability to any data protocols, methodologies or systems for processing audio, multimedia, images and the like.",{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 1","b":["100","103","109","109","109","109","103"],"i":["a","n","a","n "]},"As noted, in a multi-camera system, cameras are employed to simultaneously record a subject at varying angles, and hence, from various different perspectives. For example, a first camera can video record a subject or scene from a first angle, a second camera from a second angle, and a third camera from a third angle, etc. Each angle provides to a different viewpoint of the subject and furthermore, a relative distance is maintained between respective cameras for recording content. In addition to the cameras -, the system can include a multi-camera hub  that serves as a signal processor, computer, server or other system for coordinating the actions of the cameras -, receiving and\/or processing video data, etc. Multi-camera systems may be used for video recording, image capture, or the like.","Unfortunately, a camera is physically moved from a current angular position relative to the subject to another in order to capture data from a different perspective. As mentioned, this is not always feasible, however, depending on the recording environment, venue type, etc. For example, a concert or sporting event may feature many people and physical obstacles that make it difficult to move the equipment. Furthermore, the size of certain equipment does not lend it to easily being moved. Still further, those users having access to the recorded content have limited ability to view any other perspectives of the content beyond that as originally captured.","To address this issue, system  presents an interpolation platform  that is configured to allow viewers of the content to render the captured data to their devices -from multiple different perspectives; including those perspectives not originally captured by the plurality of cameras -. Hence, an interpolated view of the content is generated on demand at a user specified angle based on the data captured originally by one or more of the cameras -. The content  as captured, including video and\/or image content, is maintained by the interpolation platform  and processed based on one or more vector analysis models . The models  may be used to determine differences between content in order to formulate an interpolation based on the user selection. It is also noted that the models  may be integrated for use with one or more video decomposition, rendering, signal processing, segmentation, and other processing and\/or content generation techniques.","In certain embodiments, the interpolation platform  is configured to perform one or more of the following processing techniques for rendering an interpolation of the content : (1) determine vector differences between the content as captured by different cameras -at a common time frame based on the different angles at which the cameras -are positioned; (2) storing the content and corresponding vector differences for subsequent analysis; (3) receiving a request for a rendering of the content from an alternate angle, i.e., an angle that is different than that of the different cameras -; and (4) generating the interpolated convent (image) that corresponds to the request based on processing of the vector differences as determined relative to the alternate angle. The platform  therefore renders and interpolation of the content, i.e., an image, that is representative of the content from the perspective of the alternate angle. For illustration purposes, a subject  as depicted within the image appears to a display of a user device -at the angle corresponding to the selection. It is noted that the platform  may also calculate one or more vector magnitudes, scalars, etc., in executing a requisite computation for generating an interpolated view.","Of note, the interpolation platform  combines the multiple determined camera deltas into a difference vector between the cameras that can then be subsequently interpolated to any arbitrary spot between the physical devices, i.e., in a manner similar to inter-frame video compression of the like. Consequently, the cameras -need not be in a regularly ordered array as long as they know their positional relationship to each other. Information regarding the relative location, position and\/or distance of the cameras -can be maintained by the multi-camera hub , conveyed to the interpolation platform  for enabling vector computation, or a combination thereof. Optimal placement of the cameras, such as relative to a common 2D plane, may be determined by the multi-camera hub  accordingly.","In certain embodiments, the interpolation platform  presents a user interface for enabling a user to indicate the specific perspective, view and\/or angle they want content to be rendered from. For example, the user may be presented with a default image, such as that captured by one of the cameras , for representing a particular frame of content to view. In another example, the user may also be presented with the other images corresponding to the same timeframe as captured by different cameras. The images may be presented as one or more thumbnails, which upon selection result in an expanded view.","In addition, the user interface may feature various controls for enabling a user to generate a custom interpolated view based on the one or more images presented for access to the user as mentioned above. Hence, instead of viewing the one or more original images, the user can select an interpolated view or sweep between the two endpoint views as captured via cameras and ; thus directing the interpolation platform  to vary the magnitude of the difference vector relative to the angle between the two endpoint cameras.","Under this scenario, the selection is any angle or perspective that lies between opposing cameras of the multi-camera system positioned for capturing data regarding a subject  or scene of common interest (e.g., a sporting event, political rally, wedding ceremony, object or group thereof) during a relative common time frame. For example, a camera may record the subject  based on a line of view  correspond to a first angle while camera records the subject  based on a line of view  corresponding to a second angle. Each camera captures a multitude of images for providing an overlapping field of view of the subject . Consequently, the interpolated content as rendered by the platform  pursuant to a user selection of a desired line of view corresponds to or is representative of a view from anywhere between the endpoint cameras, i.e., and , as they record\/image the subject . As noted previously, the content rendered by the platform  can be an image conforming to a particular file format suitable for viewing by respective user devices -","In certain embodiments, the user may also select any multiple interpolated positions between cameras -for rendering of a particular view. For example, a first second and third view\/angle between endpoint cameras and can be selected by the user via the user interface. Hence, the interpolations generated with respect to the first, second and third angles are processed further by the interpolation platform  to generate a final three dimensional interpolated view. Still further, a four dimensional (holographic) view may also be constructed based on the number of selected data points corresponding to the various interpolated views. Under this scenario, the parallax\u2014i.e., amount of displacement or difference in the apparent position of the subject  as viewed along different lines of sight\u2014is adjusted dynamically by the user via the interface. Thus, the interpolation platform  enables the rending of standard two-dimensional as well as multi-dimensional content. More regarding the user interface rendered by the interpolation platform  for enabling access to various interpolations of content is discussed more fully later on with respect to A-C.","In another embodiment, the interpolation platform  enables the user to automatically trigger the calculation of vector differences of all the other cameras , e.g., relative to the camera that captured the default image, relative to each other, etc. For example, a camera (not shown) may be the default camera, while the other cameras and are likewise positioned for viewing the subject . Based on this calculation, the interpolation platform  maintains the optimal vectors (i.e., those of minimal difference) between the many cameras to one another. Under this approach, the minimal vector is the vector to its closest neighbors in every direction, wherein the minimal vector results in a more accurate interpolation.","Subsequently, a datagram (e.g., data packet) of information includes the primary camera; and the difference vectors between the primary camera and the other cameras are generated and transmitted to the user via the communication network . The datagram may include frame synchronization data, i.e. a sequence of bits or symbols, corresponding to respective captured time frames of content by the different cameras -. The frame synchronization data enables a receiver of the datagram to detect the beginning and end of the datagram in the stream of symbols or bits. As such, the datagram is received by a content rendering module  of a requesting user device, such as for enabling rendering of an interpolated image via a user interface upon demand.","In certain embodiments, the content rendering module  of respective devices  is an executable for receiving the content and subsequently decoding the stream of received content based on the bit information. The content rending module  is also configured to process and present the corresponding content, based on the requisite sequence, as video content, image content and the like. Presentment is performed in connection with the various application programming interface (API) executions of the requesting user device . Of note, the interpolation platform  interacts with a content rendering module  of a device using various known and still developing data transmission and frame synchronization protocols and techniques. This can include, for example, those techniques for enabling the rendering to a user interface\u2014i.e., a browser application\u2014as it is streamed via the communication network . More regarding generation of the datagram for conveyance of interpolated content is discussed more fully later on with respect to .","It is noted that user devices -may be any type of mobile terminal, fixed terminal, or portable terminal including a mobile handset, station, unit, device, multimedia computer, multimedia tablet, Internet node, communicator, desktop computer, laptop computer, Personal Digital Assistants (PDAs), smartphone or any combination thereof. It is also contemplated that the user devices -can support any type of interface for supporting the presentment or exchanging of data. In addition, user devices -may facilitate various input means for receiving and generating information, including touch screen capability, keyboard and keypad data entry, voice-based input mechanisms and the like. Any known and future implementations of user devices  are applicable.","In certain embodiments, user devices -, the interpolation platform  and other elements of system  may be configured to communicate via a communication network . The communication network  may employ various technologies for enabling wireless communication including, for example, code division multiple access (CDMA), long term evolution (LTE), enhanced data rates for global evolution (EDGE), general packet radio service (GPRS), mobile ad hoc network (MANET), global system for mobile communications (GSM), Internet protocol multimedia subsystem (IMS), universal mobile telecommunications system (UMTS), etc., as well as any other suitable wireless medium, e.g., microwave access (WiMAX), wireless fidelity (WiFi), satellite, and the like. Meanwhile, the data network may be any local area network (LAN), metropolitan area network (MAN), wide area network (WAN), the Internet, or any other suitable packet-switched network, such as a commercially owned, proprietary packet-switched network, such as a proprietary cable or fiber-optic network.","Still further, the communication network  may embody circuit-switched and\/or packet-switched networks that include facilities to provide for transport of circuit-switched and\/or packet-based communications. It is further contemplated that network  may include components and facilities to provide for signaling and\/or bearer communications between the various components or facilities of system . In this manner, the communication network  may embody or include portions of a signaling system 7 (SS7) network, Internet protocol multimedia subsystem (IMS), or other suitable infrastructure to support control and signaling functions.","It is noted, though not shown in the figure, that in certain embodiments user devices -may be configured to establish peer-to-peer communication sessions with each other using a variety of technologies\u2014near field communication (NFC), Bluetooth, ZigBee, infrared, etc. Also, connectivity can be provided via a wireless local area network (LAN). By way of example, a group of user devices -may be configured to a common LAN so that each device can be uniquely identified via any suitable network addressing scheme. For example, the LAN may utilize the dynamic host configuration protocol (DHCP) to dynamically assign \u201cprivate\u201d DHCP internet protocol (IP) addresses to each user device , i.e., IP addresses that are accessible to devices connected to the service provider network  as facilitated via a router.","For the purposes of explanation, the devices -are configured to communicate with the interpolation platform  to enable on-demand access to and rendering of varying interpolations of captured content. In one embodiment, the interpolation platform  is offered by a service provider as a managed or hosted solution. Under this approach, the interpolation platform  enables any one of the user devices -to generate an interpolated view of select content, based on a specification of a desired angle or perspective, using wireless communications. Alternatively, the interpolation platform  is directly integrated for execution by the user device\u2014i.e., as a combination of one or more modules that interact to provide the above described features.  is a diagram of an interpolation platform utilized in the system of , according to one embodiment.","For the purpose of illustration, the interpolation platform  includes various executable modules for performing one or more computing, data processing and network based instructions that in combination provide a means of enabling users to generate an interpolated view of content based on footage captured by multiple cameras. Such modules can be implemented in hardware, firmware, software, or a combination thereof. By way of example, the interpolation platform  may include an authentication module , context processing module , comparison module , image processing module , image selector module , a user interface module  and a communication interface .","In addition, the interpolation platform  also maintains content (e.g., video, images) in a content database , one or more models (e.g., for performing vector analysis) in a models database  and profile data regarding one or more users subscribed to the interpolation platform  in a profile database .","In one embodiment, an authentication module  authenticates users and user devices -for interaction with the interpolation platform . By way of example, the authentication module  receives a request to subscribe to the interpolation platform  for enabling access content as captured and\/or interpolations thereof. The subscription process may include enabling a preferred perspective, activation of one or more signal processing settings, etc. These settings are appropriately established by the authentication module . Preferences and settings information may be referenced to a specific user, user device, or combination thereof, and maintained as profile data .","The authentication process performed by the module  may also include receiving and validating a login name and\/or user identification value as provided or established for a particular user during a subscription or registration process. The login name and\/or user identification value may be received as input provided by the user from the user device  or other device via a graphical user interface to the platform  (e.g., as enabled by user interface module ). Registration data for respective subscribers, which contains pertinent user or device profile data, may be cross referenced as part of the login process. Alternatively, the login process may be performed through automated association of profile settings maintained as profile data  with an IP address, a carrier detection signal of a user device, mobile directory number (MDN), subscriber identity module (SIM) (e.g., of a SIM card), radio frequency identifier (RFID) tag or other identifier. Still further, the authentication module  is also configured to receive requests from various devices for accessing interpolated content, such as by way of the user interface module . This may include, for example, receipt of an input for specifying a desired perspective and\/or angle for rendering of an interpolated view. By way of this approach, having authenticated the device, the authentication module  alerts the other modules, such as the image processing module  and comparison module  of such requests.","In addition, the authentication module  operates in connection with the communication interface  for receiving image and\/or video data (e.g., content ) as recorded by a plurality of cameras, a multi-camera hub, or a combination thereof. As such, the authentication module  facilitates storing of the content to the database . It is noted that content related to a common subject (e.g., event, scene, object), such as received via a single multi-camera system, may be tagged in common for storage purposes.","In one embodiment, the comparison module  determines a delta (variance\/difference) between the data captured by a plurality of cameras. This may include, for example, performing of a vector analysis comparison between respective frames. In addition, the comparison module may determine optimal (minimal) vector differences between the pluralities of cameras. By way of illustration, the comparison module  determines the differences between the images and stores one of the original images (per one of the cameras) plus the delta to the content database . This image, corresponding to a default and\/or primary image, may also be rendered to a requesting device for review by a user.","Image selection, in certain embodiments, is performed by way of an image selector . The image selector  operates in connection with the user interface module  for rendering of images relative to user provided input. For example, the image selector  responds to user input for rendering of originally captured content, i.e., as captured via a perspective of one of the cameras, plus the corresponding delta information. Also, the image selector  operates in connection with an image processing module , which enables the rendering of an interpolated view based on the available content .","For the purpose of illustration, the image processing module  generates the interpolated view based on user selection of an interpolated view or sweep between two endpoint views. In certain embodiments, the image processing module  processes the input provided per the selection, such as a specified angle input, perspective input, view, etc., then varies the magnitude of the difference vector associated with the required images. In the case of a selected interpolation that is representative of an angle\/perspective between two camera views, the magnitude of the difference vectors of the first and second camera view is adjusted on the basis of the relative distance between the interpolated view and the first and second cameras.","In addition, the image processing module  also facilitates the rendering of multi-dimensional content, including three and four dimensional data. For example, the image processing module  is configured to control a parallax of a particular interpolated view, i.e., the amount of displacement or difference in the apparent position of the subject  as viewed along different lines of sight. The image processing module  adjust the parallax dynamically in response to a determined adjustment by the user via the user interface. Also, the image processing module  varies the magnitude of determined difference vectors\u2014i.e., as generated via the comparison module \u2014for generation of an interpolated view.","The image processing module  also operates in connection with the communication interface  to facilitate the transmission of interpolated content to one or more requesting user devices. This includes, for example, generation of a datagram includes information related to the primary camera and the difference vectors between it and the other cameras. By way of illustration, in the case of video content, MPEG video frames are encoded such that each line represents a moment in time, as shown in Table 1 below:",{"@attributes":{"id":"p-0042","num":"0041"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"thead":{"row":[{"entry":[{},"TABLE 1"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Intra frame (I-frame)-rendering of a full frame of video"]},{"entry":[{},"Prediction frame (P-frame)-this is a frame of content representing a"]},{"entry":[{},"difference between \u201cnow\u201d (the frame to be interpolated) and the"]},{"entry":[{},"original frame"]},{"entry":[{},"Prediction frame (predicted - this is a frame of content representing"]},{"entry":[{},"more difference between \u201cnow\u201d and the last frame P-frame"]},{"entry":[{},"Bi-directional frame (B-frame)-this is a frame of content that is"]},{"entry":[{},"predicted from both the previous and next fill frame. It is required"]},{"entry":[{},"for buffering to \u201csee the future,\u201d i.e., generate the interpolation"]},{"entry":[{},"corresponding to the desired frame position."]},{"entry":[{},"I-frame"]},{"entry":[{},". . . etc"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"Table 1 corresponds to I-P-B frame types that may be encoded in datagrams used for MPEG video compression. In contrast, the multi-camera analog at any instant may be generated by the module  as shown in Table 2 below:",{"@attributes":{"id":"p-0044","num":"0043"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"thead":{"row":[{"entry":[{},"TABLE 2"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"[MainCam + VectorToCam2 + VectorToCam3 . . . etc] is a single"]},{"entry":[{},"datagram"]},{"entry":[{},"Over Time this yields:"]},{"entry":[{},"[MainCam + VectorToCam2 + VectorToCam3 . . . etc]Time1"]},{"entry":[{},"[MainCam + VectorToCam2 + VectorToCam3 . . . etc]Time2"]},{"entry":[{},"[MainCam + VectorToCam2 + VectorToCam3 . . . etc]Time3"]},{"entry":[{},". . ."]},{"entry":[{},"[MainCam + VectorToCam2 + VectorToCam3 . . . etc]TimeN"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"By way of illustration, MainCam corresponds to the vector associated with the primary camera. VectorToCam-VectorToCamn corresponds to the vector from the primary camera to another of the cameras. In certain instances, the datagram may specify location and\/or position information as well. The datagram for enabling conveyance of interpolated content as shown in Table 2 is similar to the I-P-B frames. The exception, however, is that wherein an MPEG datagram is generated based on time events of a single data feed, the datagram for conveyance of multi-camera content and interpolated content is based on the differences across multiple sources (e.g., cameras). It is noted that vector differences become larger in magnitude as cameras are separated. Hence, it is more efficient to determine a double (or more) delta vector from the main camera to the neighboring cameras to the endpoint cameras\u2014i.e., corresponding to the optimal vector differences.","The datagram generated via the image processing module  enables proper sequencing of received content frames per the encoding presented with respect to Table 2. Thus, the content rendering module of a requesting device decodes the datagram accordingly for enabling proper rendering of the content.","In one embodiment, the context processing module  optionally processes context information associated with one or more cameras of a multi-camera system for capturing context information. By way of example, the context processing module  interprets time stamp information, location information, position information and other data captured in association with an image to support the comparing of images by the comparison module  for determining of delta information. By way of example, the coordinate data for a first camera and a second camera is determined based on a tag, label or metadata associated with captured video data. As such, the comparison module  can determine the relative distance of the cameras to one another based on the received content. The context information received is provided by one or more sensors of the camera devices. Of note, this optional execution may be coordinated by a multi-camera hub to which a plurality of cameras is connected.","In another embodiment, the context information may be interpreted to determine a common event, occurrence, venue, activity or other relationship between images. Under this scenario, images associated with the same subject of the captured content (e.g., video) are correlated despite being received from disparate data capture sources (e.g., cameras). Alternatively, the context processing module  may determine a common labeling or tagging of the images for enabling immediate determining of a relationship between the content capturing sources\u2014i.e., each may be stamped with a related device and\/or group identifier.","In one embodiment the user interface module  enables presentment of a graphical user interface for presenting content. By way of example, the user interface module  generates the interface in response to application programming interfaces (APIs) or other function calls corresponding to the browser application or web portal application of the user devices -; thus enabling the display of graphics primitives. Of note, the user interface module  may operate in connection with the image processing module  to enable the rendering of interpolated content, i.e., an image.","In one embodiment, a communication interface  enables formation of a session over a network  between the interpolation platform  and the user device. By way of example, the communication interface  executes various protocols and data sharing techniques for enabling collaborative execution between a subscriber's user device -(e.g., mobile devices, laptops, smartphones, tablet computers, desktop computers) and the interpolation platform  over the network . It is noted that the communication interface  is also configured to support a browser session\u2014i.e., the retrieval of content as referenced by a resource identifier during a specific period of time or usage of the browser. The browser session may support retrieval of captured content, generation of interpolations of captured content, etc.",{"@attributes":{"id":"p-0051","num":"0050"},"figref":["FIGS. 3A-3E","FIG. 7","FIG. 1"],"b":["103","300","306","310","312","320"]},"In step  of process , the interpolation platform  receives a plurality of images from a plurality of video cameras providing overlapping fields of view. As noted, the overlapping fields of view are produced as a result of the cameras being directed towards a common subject or scene, i.e., subject . In another step , the interpolation platform  selects a camera angle that is different from angles provided by the plurality of cameras. This different angle is selected based on a user request for a unique perspective of the content that was not captured originally by the plurality of cameras. In another step , an interpolated image corresponding to the selected camera angle using a portion or all of the plurality of images from the plurality of cameras is generated.","In step  of process  (), the interpolation platform  receives a request for the interpolated data corresponding to a time frame, the request indicating the camera angle that is different from angles provided by the plurality of video cameras. The platform  also causes a rendering of the interpolation, the plurality of images, or a combination thereof at a device based on the request, corresponding to step . By way of example, the interpolation platform  provides a user interface that enables the user to provide input for specifying the different angle. In addition, the interface may present the interpolation and\/or the content corresponding to the overlapping field of view at a given point in time.","In step  of process  (), the interpolation platform  determines vector differences between the plurality of images at respective time frames based on the angles provided by the plurality of cameras. Per step , the platform  also determines to increase or decrease the scale of the vector differences based on the different angle. As noted previously, a minimum scale of the vector differences between the plurality of cameras is preferably maintained for optimal rendering of an interpolated view. The scale of the vector differences is proportional to a difference in degree between the angle that is different and one of the angles provided by the plurality of cameras, a distance between the plurality of cameras, or a combination thereof. Of note, the angle that is different (i.e., as specified by a user) is between the angles provided by the plurality of cameras.","In steps  and  of process  (), the platform  determines a distance between the plurality of cameras and calculates multiple vector differences between the data at respective time frames of each of the plurality of cameras determined to exceed a predetermined distance threshold. It is noted that because differences become larger as cameras as separated, it is more efficient to determine a double (or more) delta\/vector difference from the main camera to the neighboring cameras and the endpoint cameras.","In step , the time frame corresponding to the request for an angle that is different than the plurality of angles provided is determined by the interpolation platform . As noted previously, the request for generation of interpolated content is facilitated by way of specification of the different angle as well as specification of the common moment of capture of the content for which to base the interpolation. It is noted that the video cameras may record overlapping fields of view over time. Consequently, the interpolation platform  determines the correct time frame for which to generate an interpolated image from among the plurality of video frames captured.","Per step , the platform  causes retrieval of the vector differences, the plurality of images, or a combination thereof based on the determination. This retrieval is based on the storing of the vector differences and the image data by the interpolation platform . For example, upon determining the differences between the images, the interpolation platform  can subsequently store one of the original images as captured by a primary\/default camera plus the vector difference information associated with that camera. Once stored, this information may also be retrieved for viewing by a user having access to the platform  via a user interface.","In step  of process , the interpolation platform  generates a datagram related to the interpolated image. Per step , the datagram is transmitted to the device. The datagram is generated to include one of the plurality of images, a vector difference between the one image and the plurality of images, or a combination thereof. This corresponds to the exemplary encoding scheme of Table 2 as described above.",{"@attributes":{"id":"p-0059","num":"0058"},"figref":"FIG. 4","b":["401","405","401","405","409","1"]},"By way of example, the first camera  captures a view of the subject  from a first angle\/position, the second camera  from a second angle\/position and the third camera  from a third angle\/position. Under this scenario, the second camera  acts as a primary camera that serves as the source of capture of a primary image. Also, the relative distances of the cameras is as follows: a distance D between the first device  and the second device , a distance D between the second device  and the third device  and a distance D between the first device  and the third device .","Each of the cameras captures footage of the subject  from different points of view. For example, due to the position of camera , a frame of content corresponding to an instant of time T is shown to present a different perspective than a frame of content  as captured by the third camera  during the same instance. Assuming the cameras stay in a fixed position, therefore, the relative distances between them corresponds to numerous vantage points\/angles during the same time frame that are not captured by the cameras.","The interpolation platform  calculates the vector differences between the cameras - based on the relative distances D-D, the angles of the cameras -, etc. As noted previously, the results are stored by the interpolation platform  along with the captured images corresponding to a given time frame. This information is then made available for view by a user from a device, as shown in . In addition, the user may generate one or more interpolated views of the recorded subject  as image data accordingly.",{"@attributes":{"id":"p-0063","num":"0062"},"figref":["FIGS. 5A-5C","FIGS. 3A-3E"],"b":"103"},"Continuing with the example of , the subject is captured via the cameras - from various vantage points, and user device  can present, via display , the corresponding thumbnail images , , and  at various time frames. Under the scenario of , default image , as captured via the primary camera  is shown. The thumbnail image  corresponds to a more forward facing view of the subject  given the orientation of the primary camera  during time T. The user can expand this image by activating a SELECT action button , resulting in expansion of the full image rather than the thumbnail view.","Alternatively, the user can toggle between all captured views for the corresponding time period T by selecting back or forward scroll buttons  and  respectively. Upon selection, the various thumbnails representative of the images (e.g., Image  and Image ) for all of the camera views is shown. The number of thumbnails presented therefore corresponds to the number of cameras actively recording content at the time. A time frame adjustment field  is made available along with a time value increase\/decrease selector for enabling the user to adjust the specific time frames to view, which in this example, corresponds to T seconds.","By way of example, thumbnail  corresponds to a thumbnail representative of a view (Image ) corresponding to frame  as captured by the third camera . Thumbnail  corresponds to an view (Image ) captured by the first camera . It is noted that the different images convey additional details related to the same scene\/subject at the time. For example, Image  corresponding to thumbnail  presents additional open field and less of the defender  attempting to thwart the receiver  of the football . Conversely, Image , corresponding to thumbnail , shows less of the receiver , none of the football  and more of the defender  and another defender  not shown in any other view. The interpolation platform  is able to determine the differences between the images based on the calculated delta information.",{"@attributes":{"id":"p-0067","num":"0066"},"figref":"FIG. 5B","b":["507","507","503","505","401","403","510","510","517"],"i":["a ","a"]},"When the user selects the INTERPOLATED action button , another interface for enabling rendering of interpolated views is presented, as shown in . Under this scenario, the user is presented with a toggle button  as part of a slide bar  for enabling a user to sweep between the various captured views. For example, placement of the toggle button  at points ,  and  along the slide bar correspond to the views represented by Image , the default image and Image  respectively. Hence, the corresponding thumbnails - for these images are presented along the slide bar  to the user for reference.","Alternatively, when the user places the toggle button  at a point between at least two of the captured images, the corresponding interpolated image is generated and subsequently presented. For example, placement of the toggle button  between Image  corresponding to thumbnail  and the default image corresponding to thumbnail  is correlated with a specific angular position relative to the subject by the interpolation platform. Consequently, an interpolated view  is rendered based on the angular position as selected by the user relative to the reference images (i.e., Image  and the default image) upon which the interpolation is based. The view is changed dynamically as the user adjusts the position of the toggle button  along the slide bar . This process may be repeated for different time frames accordingly, such as by adjusting the time increase\/decrease selector.","It is noted that the interpolated view , in this example, is generated based on the various vector processing and image processing capabilities of the interpolation platform . As such, the interpolated view  represents an angle that offers the user a different vantage point of the overlapping subject matter . The exemplary techniques and systems presented herein enable users to generate an interpolated view on demand based on an original set of data captured by a plurality of cameras. The interpolation platform  accounts for the relative position, orientation, offset and angular displacement of the cameras relative to one another and the subject  of interest for rendering the interpolated content.","The processes described herein for enabling users to generate an interpolation view of content based on footage captured by multiple cameras may be implemented via software, hardware (e.g., general processor, Digital Signal Processing (DSP) chip, an Application Specific Integrated Circuit (ASIC), Field Programmable Gate Arrays (FPGAs), etc.), firmware or a combination thereof. Such exemplary hardware for performing the described functions is detailed below.",{"@attributes":{"id":"p-0072","num":"0071"},"figref":"FIG. 6","b":["600","601","603","601","600","605","601","603","605","603","600","607","601","603","609","601"]},"The computer system  may be coupled via the bus  to a display , such as a cathode ray tube (CRT), liquid crystal display, active matrix display, or plasma display, for displaying information to a computer user. An input device , such as a keyboard including alphanumeric and other keys, is coupled to the bus  for communicating information and command selections to the processor . Another type of user input device is a cursor control , such as a mouse, a trackball, or cursor direction keys, for communicating direction information and command selections to the processor  and for adjusting cursor movement on the display .","According to an embodiment of the invention, the processes described herein are performed by the computer system , in response to the processor  executing an arrangement of instructions contained in main memory . Such instructions can be read into main memory  from another computer-readable medium, such as the storage device . Execution of the arrangement of instructions contained in main memory  causes the processor  to perform the process steps described herein. One or more processors in a multi-processing arrangement may also be employed to execute the instructions contained in main memory . In alternative embodiments, hard-wired circuitry may be used in place of or in combination with software instructions to implement the embodiment of the invention. Thus, embodiments of the invention are not limited to any specific combination of hardware circuitry and software.","The computer system  also includes a communication interface  coupled to bus . The communication interface  provides a two-way data communication coupling to a network link  connected to a local network . For example, the communication interface  may be a digital subscriber line (DSL) card or modem, an integrated services digital network (ISDN) card, a cable modem, a telephone modem, or any other communication interface to provide a data communication connection to a corresponding type of communication line. As another example, communication interface  may be a local area network (LAN) card (e.g. for Ethernet\u2122 or an Asynchronous Transfer Mode (ATM) network) to provide a data communication connection to a compatible LAN. Wireless links can also be implemented. In any such implementation, communication interface  sends and receives electrical, electromagnetic, or optical signals that carry digital data streams representing various types of information. Further, the communication interface  can include peripheral interface devices, such as a Universal Serial Bus (USB) interface, a PCMCIA (Personal Computer Memory Card International Association) interface, etc. Although a single communication interface  is depicted in , multiple communication interfaces can also be employed.","The network link  typically provides data communication through one or more networks to other data devices. For example, the network link  may provide a connection through local network  to a host computer , which has connectivity to a network  (e.g. a wide area network (WAN) or the global packet data communication network now commonly referred to as the \u201cInternet\u201d) or to data equipment operated by a service provider. The local network  and the network  both use electrical, electromagnetic, or optical signals to convey information and instructions. The signals through the various networks and the signals on the network link  and through the communication interface , which communicate digital data with the computer system , are exemplary forms of carrier waves bearing the information and instructions.","The computer system  can send messages and receive data, including program code, through the network(s), the network link , and the communication interface . In the Internet example, a server (not shown) might transmit requested code belonging to an application program for implementing an embodiment of the invention through the network , the local network  and the communication interface . The processor  may execute the transmitted code while being received and\/or store the code in the storage device , or other non-volatile storage for later execution. In this manner, the computer system  may obtain application code in the form of a carrier wave.","The term \u201ccomputer-readable medium\u201d as used herein refers to any medium that participates in providing instructions to the processor  for execution. Such a medium may take many forms, including but not limited to computer-readable storage medium ((or non-transitory)\u2014i.e., non-volatile media and volatile media), and transmission media. Non-volatile media include, for example, optical or magnetic disks, such as the storage device . Volatile media include dynamic memory, such as main memory . Transmission media include coaxial cables, copper wire and fiber optics, including the wires that comprise the bus . Transmission media can also take the form of acoustic, optical, or electromagnetic waves, such as those generated during radio frequency (RF) and infrared (IR) data communications. Common forms of computer-readable media include, for example, a floppy disk, a flexible disk, hard disk, magnetic tape, any other magnetic medium, a CD-ROM, CDRW, DVD, any other optical medium, punch cards, paper tape, optical mark sheets, any other physical medium with patterns of holes or other optically recognizable indicia, a RAM, a PROM, and EPROM, a FLASH-EPROM, any other memory chip or cartridge, a carrier wave, or any other medium from which a computer can read.","Various forms of computer-readable media may be involved in providing instructions to a processor for execution. For example, the instructions for carrying out at least part of the embodiments of the invention may initially be borne on a magnetic disk of a remote computer. In such a scenario, the remote computer loads the instructions into main memory and sends the instructions over a telephone line using a modem. A modem of a local computer system receives the data on the telephone line and uses an infrared transmitter to convert the data to an infrared signal and transmit the infrared signal to a portable computing device, such as a personal digital assistant (PDA) or a laptop. An infrared detector on the portable computing device receives the information and instructions borne by the infrared signal and places the data on a bus. The bus conveys the data to main memory, from which a processor retrieves and executes the instructions. The instructions received by main memory can optionally be stored on storage device either before or after execution by processor.",{"@attributes":{"id":"p-0080","num":"0079"},"figref":["FIG. 7","FIG. 6"],"b":["700","700","700","700","700"]},"In one embodiment, the chip set or chip  includes a communication mechanism such as a bus  for passing information among the components of the chip set . A processor  has connectivity to the bus  to execute instructions and process information stored in, for example, a memory . The processor  may include one or more processing cores with each core configured to perform independently. A multi-core processor enables multiprocessing within a single physical package. Examples of a multi-core processor include two, four, eight, or greater numbers of processing cores. Alternatively or in addition, the processor  may include one or more microprocessors configured in tandem via the bus  to enable independent execution of instructions, pipelining, and multithreading. The processor  may also be accompanied with one or more specialized components to perform certain processing functions and tasks such as one or more digital signal processors (DSP) , or one or more application-specific integrated circuits (ASIC) . A DSP  typically is configured to process real-world signals (e.g., sound) in real time independently of the processor . Similarly, an ASIC  can be configured to performed specialized functions not easily performed by a more general purpose processor. Other specialized components to aid in performing the inventive functions described herein may include one or more field programmable gate arrays (FPGA) (not shown), one or more controllers (not shown), or one or more other special-purpose computer chips.","In one embodiment, the chip set or chip  includes merely one or more processors and some software and\/or firmware supporting and\/or relating to and\/or for the one or more processors.","The processor  and accompanying components have connectivity to the memory  via the bus . The memory  includes both dynamic memory (e.g., RAM, magnetic disk, writable optical disk, etc.) and static memory (e.g., ROM, CD-ROM, etc.) for storing executable instructions that when executed perform the inventive steps described herein to enable users to generate an interpolation view of content based on footage captured by multiple cameras. The memory  also stores the data associated with or generated by the execution of the inventive steps.","While certain exemplary embodiments and implementations have been described herein, other embodiments and modifications will be apparent from this description. Accordingly, the invention is not limited to such embodiments, but rather to the broader scope of the presented claims and various obvious modifications and equivalent arrangements."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["Various exemplary embodiments are illustrated by way of example, and not by way of limitation, in the figures of the accompanying drawings in which like reference numerals refer to similar elements and in which:",{"@attributes":{"id":"p-0005","num":"0004"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0006","num":"0005"},"figref":["FIG. 2","FIG. 1"]},{"@attributes":{"id":"p-0007","num":"0006"},"figref":"FIGS. 3A-3E"},{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":["FIGS. 5A-5C","FIGS. 3A-3E"]},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 7"}]},"DETDESC":[{},{}]}
