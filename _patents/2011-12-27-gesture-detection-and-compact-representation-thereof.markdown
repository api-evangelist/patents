---
title: Gesture detection and compact representation thereof
abstract: Techniques are described that may be implemented with an electronic device to detect a gesture within a field of view of a sensor and generate a compact data representation of the detected gesture. In implementations, a sensor is configured to detect a gesture and provide a signal in response thereto. An estimator, which is in communication with the sensor, is configured to generate an elliptical representation of the gesture. Multiple coefficients for the compact representation of the gesture can be used to define the ellipse representing the gesture.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09075443&OS=09075443&RS=09075443
owner: Maxim Integrated Products, Inc.
number: 09075443
owner_city: San Jose
owner_country: US
publication_date: 20111227
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION","CONCLUSION"],"p":["Gesture detection and recognition can be used to provide new and more intuitive interfaces to electronic devices. The goal of gesture recognition is to interpret human gestures via mathematical algorithms. Generally speaking, gestures can originate from any bodily motion or state, but most often originate from the face or hand of a human user, e.g., in the manner of hand gestures. Gesture recognition is often looked to as a way for computers to begin to understand human body language, in order to provide a more convenient and\/or intuitive interface between machines and humans than text-based interfaces and Graphical User Interfaces (GUIs), which typically limit the majority of electronic device input to a keyboard, a mouse, and possibly a touchpad. Thus, gesture detection and recognition can enable humans to interact more naturally with machines without requiring the use of mechanical input devices.","Techniques are described that may be implemented with an electronic device to detect a gesture within a field of view of a sensor and generate a compact data representation of the detected gesture. In implementations, a sensor is configured to detect a gesture and provide a signal in response thereto. An estimator, which is in communication with the sensor, is configured to generate an elliptical representation of the gesture. Multiple coefficients for the compact representation of the gesture can be used to define the ellipse representing the gesture.","This Summary is provided solely to introduce subject matter that is fully described in the Detailed Description and Drawings. Accordingly, the Summary should not be considered to describe essential features nor be used to determine scope of the claims.","Overview","Increasingly, gesture detection is being employed by electronic devices to detect input for various applications associated with the electronic device. However, such electronic devices typically employ a large number of photodetectors to improve range and operation (e.g., noise reduction) of gesture detection.","Accordingly, techniques are described that may be implemented with an electronic device to detect a gesture within a field of view of a sensor (e.g., a photodetector) and generate a compact data representation of the detected gesture. In implementations, a photodetector of an electronic device is configured to detect light corresponding to a gesture and provide a signal in response thereto. For example, the photodetector may comprise a segmented photodetector that includes an array of individual photodetectors (e.g., an array of two-by-two (2\u00d72) photodetectors). An estimator, which is in communication with the sensor\/photodetector, is configured to generate one or more estimated values of the signal corresponding to an elliptical representation of the gesture. For example, the estimator may be a Kalman estimator configured to estimate velocity vectors based upon the signals generated by the segmented photodetector.","Multiple coefficients associated with the estimated values can be determined based upon an elliptical representation of the gesture. These coefficients can then be used to represent the gesture. In an implementation, five (5) coefficients can be used to represent various characteristics of an ellipse. For example, representative coefficients may include the center coordinates (centroid) of the ellipse within a geographic plane, radii of the ellipse (e.g., a semi-major radius and a semi-minor radius) within the geographic plane, and an orientation of the ellipse within the geographic plane (e.g., an angular measurement with respect to an axis of the geographic plane). In implementations, the orientation of the ellipse can be used to represent the direction of the gesture with respect to the orientation of the photodetector, while the semi-major radius of the ellipse can be used to represent the speed\/velocity of the gesture, and the area of the ellipse can be used to represent the size and height of the object.","Thus, the electronic device is configured to detect a gesture and determine a lossless and compact elliptical representation of the gesture (e.g., using five coefficients), allowing for greater gesture detection robustness. In implementations, a direct least squares fit of an ellipse can make full use of the measured gesture data, translating to an increase in the effective range of operation for a particular sensor size. Through the use of stochastic estimation techniques and least squared identification, gesture detection robustness can be increased, while false positives can be reduced. This stochastic estimation may provide compensation for imperfections in, for example, optical and\/or electrical paths. This improved performance is manifested as an extended range of operation. Further, cost of equipment associated with gesture detection may be reduced, e.g., by reducing the area required for the associated detection equipment, such as photodiodes, while still maintaining adequate performance.","Example Techniques","Referring now to , a sensor is described that is configured to sense a gesture and provide one or more electronic signals representing the gesture. For example, with reference to , a sensor may be implemented using a photodiode array  comprising a number of photodiodes (e.g., four photodiodes , , , and ). However, it should be noted that photodiode array  is provided by way of example only and is not meant to be restrictive of the present disclosure. Thus, other sensors may be employed. For example, the photodiode array  may comprise a four by four (4\u00d74) array of photodiodes, and so forth. In implementations, the photodiode array  may be implemented for gesture detection and\/or recognition with a device such as a tablet computer, a mobile phone, a smart phone, a Personal Computer (PC), a laptop computer, a netbook computer, a hand-held portable computer, a Personal Digital Assistant (PDA), a multimedia device, a game device, an e-book reader device (eReader), a Smart TV device, a surface computing device (e.g., a table top computer), and so forth.","As an object (e.g., a hand) traverses the field of view of the photodiode array  from left to right, the generated array response may be represented by the graph shown in , where the pair of photodiodes  and  exhibit similar responses, as does the pair of photodiodes  and . In the context of the present example, these responses indicate that the object entered the field of view of the photodiode array  from the left and exited to the right (e.g., in the manner of a left-to-right swipe gesture). Similarly, as shown in , when an object enters the field of view of the photodiode array  from the top and exits at the bottom (e.g., in the manner of a top-to-bottom swipe gesture), the pair of photodiodes  and  may exhibit similar responses, as does the pair of photodiodes  and . One technique to determine the direction of the gesture would be to time stamp a zero-crossing or threshold, and then determine a direction based upon a single sample point (e.g., with reference to the zero-crossing\/threshold). However, this technique is susceptible to noise.","Referring now to , a differential response may be computed for the four photodiodes , , , and  of the photodiode array  for gesture recognition. For example, differential pairs may be defined such that the response of photodiode  minus the response of photodiode  is used to represent a Northeast-to-Southwest (NESW) gradient, where the cardinal directions North (N), South (S), East (E), and West (W) correspond to orientations with respect to the photodiode array  of top, bottom, right, and left, respectively. Similarly, the response of photodiode  minus photodiode  is used to represent a Northwest-to-Southeast (NWSE) directional gradient. Additionally, the response of the four photodiodes , , , and  can be summed to provide an absolute magnitude\/depth for the photodiode array .","Referring to , differential pairs (e.g., as described above) can be combined to form a direct measurement within a Cartesian reference frame (e.g., using x and y coordinates). For instance, a coordinate system can be defined where an x-coordinate is determined based upon adding the response of photodiodes  and , and then subtracting the responses of photodiodes  and  from the resulting sum. Further, a y-coordinate can be determined based upon adding the responses of photodiodes  and  and subtracting the responses of photodiodes  and . The response of the photodiode array  calculated in this manner is shown in , where it can be seen that there is a clearly defined stimulus in the x-dimension and a slight stimulus in the y-dimension. This response implies a left-to-right gesture. It should be noted that a right-to-left gesture may be depicted in a similar manner, but with a change in sign.","In implementations, an elliptical representation of a gesture may be generated using a Kalman Estimator for velocity vector estimation and sensor calibration, and a direct form least squares estimation to fit the data to an ellipse. In the present example, the Kalman Estimator comprises seven states: x, dxdt, y, dydt, z, xoffset, and yoffset. In this example, (x,y) correspond to coordinates in the Cartesian reference frame derived above; (dxdt, dydt) are the dimensionless velocity vectors of the object within the reference frame; (z) corresponds to a magnitude vector, which is conceptually proportional to the depth\/height\/side of the object; and the (xoffset, yoffset) states track bias offset within the optical\/electrical paths. For example, dust on the lens will manifest as a bias in the measurement of (z1,z2).","The present example uses the linear form of a Kalman Estimator. A similar Extended Kalman Estimator can be used with a polar coordinate system, and can provide estimations of phase information. The techniques described herein can be used with either form of a Kalman Estimator. For these states, the following equations are defined:\n\n(1)=()+\n\nwhere A represents the state transition matrix, Q=G*v represents model variance, and\n",{"@attributes":{"id":"p-0033","num":"0032"},"maths":[{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"A","mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mi":"a"},{"mi":"T"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mn":"1"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mi":"a"},{"mi":"T"},{"mn":"0"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"1"},{"mn":"0"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"}]}]}}}}},{"@attributes":{"id":"MATH-US-00001-2","num":"00001.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"Q","mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mi":"q"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mi":"q"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mi":"q"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mi":"q"},{"mn":"0"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mi":"q"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mi":"q"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mi":"q"}]}]}}}}}]},"For the measurements, the following measurement equation is defined:\n\n()=()+\n\nwhere H is the measurement matrix, R=W*rr\u2032 is the noise variance, and\n",{"@attributes":{"id":"p-0035","num":"0034"},"maths":[{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"R","mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mi":"r"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mi":"r"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mi":"r"}]}]}}}}},{"@attributes":{"id":"MATH-US-00002-2","num":"00002.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"H","mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mrow":{"mo":"-","mn":"1"}},{"mn":"0"},{"mn":"1"},{"mn":"0"},{"mn":"0"},{"mn":"1"},{"mn":"0"}]},{"mtd":[{"mn":"1"},{"mn":"0"},{"mn":"1"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"1"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"1"},{"mn":"0"},{"mn":"0"}]}]}}}}}]},"The Kalman Estimator code segment below iterates on each measurement and estimates the states:\n\n","Referring now to , position states (x,y) are shown that correspond to filtered measurements shown in . It should be noted that the filter bandwidth is derived from the error covariance and measurement parameters, which depend on the noise characteristics of, for instance, photodiodes , , , and . Referring to , estimated pseudo velocity states (dxdt,dydt) are depicted. In , estimated depth is depicted. Referring now to , it can be seen that dxdt plotted against dydt is elliptical in nature.  depicts this for a left-to-right swipe gesture, and  depicts this for a top-to-bottom swipe gesture. It can be seen that the orientation of the gesture is implicit in this representation. Elliptical representations of the gestures of  are shown in , respectively. In some implementations, when the number of samples between two extremities of an elliptical representation small (e.g., less than four (4)), the gesture may be deemed invalid\/undefined. However, the number four (4) is provided by way of example only and is not meant to be restrictive of the present disclosure. Thus, other numbers of samples may be required to detect a valid gesture.","In the following discussion, an example electronic device is described. Example procedures are then described that may be employed by the device.","Example Environment",{"@attributes":{"id":"p-0040","num":"0051"},"figref":"FIG. 13","b":["1300","1300","1300","1300"]},"In , the electronic device  is illustrated as including a processor  and a memory . The processor  provides processing functionality for the electronic device  and may include any number of processors, micro-controllers, or other processing systems and resident or external memory for storing data and other information accessed or generated by the electronic device . The processor  may execute one or more software programs which implement the techniques and modules described herein. The processor  is not limited by the materials from which it is formed or the processing mechanisms employed therein and, as such, may be implemented via semiconductor(s) and\/or transistors (e.g., electronic Integrated Circuits (ICs)), and so forth.","The memory  is an example of device-readable storage media that provides storage functionality to store various data associated with the operation of the electronic device , such as the software program and code segments mentioned above, or other data to instruct the processor  and other elements of the electronic device  to perform the techniques described herein. Although a single memory  is shown, a wide variety of types and combinations of memory may be employed. The memory  may be integral with the processor , stand-alone memory, or a combination of both. The memory may include, for example, removable and non-removable memory elements such as Random Access Memory (RAM), Read Only Memory (ROM), Flash memory (e.g., a Secure Digital (SD) card, a mini-SD card, a micro-SD card), magnetic memory, optical memory, Universal Serial Bus (USB) memory devices, and so forth. In embodiments of the electronic device , the memory  may include removable Integrated Circuit Card (ICC) memory, such as memory provided by Subscriber Identity Module (SIM) cards, Universal Subscriber Identity Module (USIM) cards, Universal Integrated Circuit Cards (UICC), and so on.","As shown in , the electronic device  includes a sensor, such as a photosensor\/photodetector  (e.g., an Ambient Light Sensor (ALS)). The photodetector  may be configured in a variety of ways. For example, the photodetector  may comprise one or more photosensor diodes, phototransistors, and so forth (e.g., as described with reference to ). In implementations, the photodetector  is capable of detecting light and providing a signal in response thereto. Thus, the photodetector  may provide a signal by converting light into current and\/or voltage based upon the intensity of the detected light. For example, when photodetector  is exposed to light, multiple free electrons may be generated to create a signal comprised of electrical current. The signal may correspond to one or more characteristics of the detected light. For example, the characteristics may correspond to, but are not necessarily limited to: the position of the detected light with respect to the photodetector , the intensity (e.g., irradiance, etc.) of the light incident upon the photodetector , how long the light is incident on the photodetector , an orientation of the light incident upon the photodetector , and so forth.","The photodetector  can be configured to detect light in both the visible light spectrum and the near infrared light spectrum. As used herein, the term \u201clight\u201d is used to refer to electromagnetic radiation occurring in the visible light spectrum and\/or the near infrared light spectrum. For instance, as referenced herein, the visible light spectrum (visible light) includes electromagnetic radiation occurring in the range of wavelengths from about three hundred ninety nanometers (390 nm) to approximately seven hundred fifty nanometers (750 nm). Similarly, as referenced herein, the near infrared light spectrum (infrared light) includes electromagnetic radiation that ranges in wavelength from about seven hundred nanometers (700 nm) to three microns (3 \u03bcm). In implementations, Complementary Metal-Oxide-Semiconductor (CMOS) fabrication techniques may be used to form the photodetector .","In implementations, the photodetector  comprises an ALS configured as a segmented photodetector . The segmented photodetector  may include an array of individual photodetectors provided in a single package. For example, a quad segmented photodetector can be used that is functionally equivalent to four (4) individual photodetectors arranged in a quad (e.g., two-by-two (2\u00d72)) layout array. Thus, the photodetector  may be configured to detect gestures in multiple orientations with respect to the orientation of the photodetector  (e.g., right-to-left, left-to-right, top-to-bottom, bottom-to-top, diagonally across the photodetector, etc.). For example, as an object (e.g., a hand) passes through the field of view of the segmented photodetector , each individual photodetector may provide a signal that is out of phase with the other photodetectors of the segmented photodetector  as the object passes over the respective individual photodetectors.","While photodetector  has been described with some specificity as comprising a number of photodiodes arranged in an array (e.g., as shown in ) and\/or as a segmented photodetector , these configurations are provided by way of example only and are not meant to be restrictive of the present disclosure. Thus, the photodetector  may include, but is not necessarily limited to: an active pixel sensor (e.g., an image sensor including an array of pixel sensors, where each pixel sensor is comprised of a light sensor and an active amplifier); a Charge-Coupled Device (CCD); a Light-Emitting Diodes (LED) reverse-biased to act as a photodiode; an optical detector that responds to the heating effect of incoming radiation, such as a pyroelectric detector, a Golay cell, a thermocouple, and\/or a thermistor; a photoresistor\/Light Dependent Resistor (LDR); a photovoltaic cell; a photodiode (e.g., operating in photovoltaic mode or photoconductive mode); a photomultiplier tube; a phototube; a phototransistor; and so forth. Further, photodetector  is provided by way of example only and other sensors can be used to detect gestural motions, including a proximity sensor that emits a beam of electromagnetic radiation (e.g., infrared light), a touchpad, a camera, and so forth. For instance, one or more cameras can be used to detect gestures, such as depth-aware cameras, stereo cameras, and so forth.","The electronic device  may include an illumination source  configured to generate light (e.g., near infrared light and\/or visible light) within a limited spectrum of wavelengths. The illumination source  may be used to illuminate an object proximal to the electronic device , such as the hand of an operator, allowing the photodetector  to more easily and\/or accurately detect the object. In an implementation, the photodetector  may be configured to detect light (e.g., light reflected from an object proximate to the device ) generated and emitted from the illumination source . Thus, the photodetector  may be configured to detect light within a limited spectrum of wavelengths. For example, the illumination source  may generate a light occurring in a first spectrum of wavelengths, and the photodetector  may be configured to detect light only occurring within the first spectrum of wavelengths. In implementations, the illumination source  may comprise a light emitting diode, a laser diode, or another type of light source.","As shown in , the electronic device  includes an estimator  configured to provide estimated values based upon the signals received from the photodetector  (e.g., via a data bus, or the like). In an implementation, the estimated values correspond to characteristics of the detected light (e.g., positional data of the detected light within the photodetector  field of view, the intensity of the light incident upon the photodetector  for deriving depth data relating to the object within the field of view, the orientation of the light incident upon the photodetector  for deriving directional data relating to the object, the time for which light was incident upon the photodetector  for deriving velocity data relating to the object, etc.) For example, the estimator  is configured to receive signals representing characteristics of the light detected by the photodetector  (e.g., the segmented photodetector ) and produce estimated values based upon these characteristics. The estimator  may be implemented in hardware, software, firmware, combinations thereof, or the like.","The estimator  may use any suitable stochastic technique to derive the estimated values. For example, the estimator  may be a Kalman estimator, or the like. In a specific example, the estimator  may be a Kalman estimator configured to generate linear coordinate information representing the detected light. For example, the estimator  may be configured to derive estimated values, such as velocity estimates (e.g., velocity vectors), of the measured values of the signals and\/or calculated values associated with the signals by predicting an estimated value corresponding to characteristics of the light (e.g., as measured by the photodetector ), estimating the uncertainty of the predicted estimated value, and computing a weighted average of a predicted estimated value and a measured value. In an implementation, the estimator  may derive velocity vectors as a function of the amount of time light is incident upon the photodetector .","The estimator  may also determine the direction of a gesture based upon which individual photodetecting elements of the photodetector  receive reflected light for a given amount of time. For example, a first photodetecting element within a segmented photodetector  may detect light reflected from a gesture before a second photodetecting element detects the light, e.g., as an object is moved within the field of view of the segmented photodetector . Thus, the estimator  may be configured to generate velocity vectors for a detected object based upon positional changes of detected light within a field of view of the photodetector  as a function of time (e.g., as function of the capture rate of the photodetector ). In another specific example, the estimator  may be a Kalman estimator configured to derive polar coordinate information of the detected light (e.g., deriving phase information of the detected light, etc.).","While the device  is operational, the estimator  can be configured to continuously sample signals from the photodetector . For instance, the estimator  may be configured to continuously sample signals generated by the photodetector  at or during predetermined time intervals (e.g., sampling about every microsecond, about every millisecond, about every second, etc.). Further, the estimator  may be configured to account for biases and\/or offsets within signals received from the photodetector . For instance, the estimator  may be configured to account for an obscurity (e.g., a liquid drop, a dust particle, etc.) within the field of view of the photodetector  and generate estimated values (e.g., velocity estimates) corresponding to characteristics of the light incident upon the photodetector  while compensating for the obscurity with respect to the detected light characteristics. For example, the estimator  may derive offset information pertaining to detected light.","As shown in , the electronic device  may include an ellipse estimation module  in communication with the estimator  (e.g., via a data bus, etc.). The ellipse estimation module  represents functionality to generate coefficients that correspond to (e.g., represent) the estimated values generated by the estimator . For example, five (5) coefficients can be generated for an elliptical representation of a gesture direction and a gesture magnitude, as measured with respect to a geographic plane. For instance, the elliptical representation may be a description of a ellipse in a general parametric representation, a canonical representation, a polar representation, or the like. In implementations, the ellipse estimation module  is configured to generate coefficients relating to the detected gesture (e.g., a finger swipe, etc.) based upon velocity vectors generated by the estimator . The ellipse estimation module  may use a suitable ellipse estimation model to derive a coefficient dataset, such as a Least Squares model, and so forth. For example, the ellipse estimation module  may use the Least Squares model as described in \u201cDirect Least Squares Fitting of Ellipses\u201d (Fitzgibbon, Andrew W.; Pilu, Maurizio; & Fisher, Robert B. (1999). Direct Least Square Fitting of Ellipses. 21(5): 476-480.), which is herein incorporated by reference in its entirety. In implementations, the ellipse estimation module  uses the Least Squares model to generate five (5) coefficients, as described herein.","In a specific instance, the coefficients derived using a Least Squares model, may comprise a general parametric representation of an ellipse, with two (2) center coefficients that represent the center coordinates of the ellipse within a geographic (e.g., Cartesian) plane (where the center coefficients are denoted herein as Cx, Cy), two (2) radii coefficients that represent radii values (e.g., semi-major and semi-minor radii) of the ellipse within the geographic plane (denoted herein as Rx, Ry), and one (1) coefficient that represents the orientation (e.g., angle) of the ellipse within the geographic plane (denoted herein as theta). Thus, the coordinate system embodies a reference frame for the geometrical representation of the gesture, and the orientation and speed of the gesture are represented with respect to this reference frame. It should be noted that the geographic plane used to map the generated ellipse may correlate to spatial positions of the gesturer with respect to the photodetector . For example, center coefficients for ellipses representing spatially separated gestures may be separated by a finite distance within the coordinate system, where the distance between the center coefficients corresponds to a spatial distance between the gestures provided by the gesturer with respect to, for example, photodetector . While this example has been provided with reference to a general parametric representation, it should be noted that other elliptical representations may be used as well, including, but not necessarily limited to: a canonical representation and a polar representation.","In implementations, the theta coefficient corresponds to the direction of the detected gesture with respect to the orientation of the photodetector  (e.g., comprising an angle representing left-to-right, right-to-left, up-to-down, down-to-up, or diagonal orientations of the gesture, etc.). In implementations, one or more of the radii values may correspond to a velocity of the detected gesture. For instance, the longer the light is incident upon the photodetector , the smaller a radii value may be, as compared to a radii value associated with a shorter amount of time that light is incident upon the photodetector  (e.g., for a slower gesture performed over the photodetector  versus a quicker gesture performed over the photodetector ).","The electronic device  may be configured to interpret estimated values of a gesture based upon an analysis of two or more gestures. For example, once the device  is transitioned from a non-operational state to an operational state, when a different user begins to operate the electronic device, and so forth, the device  may request a baseline gesture to more accurately interpret relative velocity values for subsequently performed gestures. For example, the electronic device  may initiate a request for a user to perform a gesture at an intermediate speed (e.g., between what would be a fast speed and slow speed for that particular user). Thus, a detected gesture that is quicker (e.g., where less light is incident upon the photodetector ) than the baseline may be represented using radii coefficients that are greater than radii coefficients used to represent a baseline gesture. Conversely, a detected gesture that is slower (e.g., where a greater amount of light is incident upon the photodetector ) than the baseline may be represented using radii coefficients that are less than baseline radii coefficients for that user. Further, the electronic device  may store baseline gesture information in the form of coefficients (e.g., using memory , and so forth) in order to interpret subsequently detected gestures for a particular user.","The device  may be configured to distinguish between distinct gestures. For the purposes of the present disclosure, a distinct gesture may be defined as occurring when some amount of measurable light incident upon the photodetector  transitions to at least substantially less measurable light incident upon the photodetector . In some instances (e.g., where light reflected by an object is used to measure a gesture), a transition from less detected light to substantially more detected light and again to less detected light may comprise a distinct gesture. In other instances (e.g., where light blocked by an object is used to measure a gesture, such as for a backlit object), a transition from more detected light to substantially less detected light and again to more detected light may comprise a distinct gesture. For example, the photodetector  may be configured to generate signals corresponding to characteristics of the light (e.g., light emitted from the illumination source ) incident upon the photodetector . Thus, once the photodetector  is no longer is providing signals for a predetermined amount of time (e.g., a nanosecond, a millisecond, a second, and so forth), the ellipse estimation module  may determine that the associated gesture has been completed and generate the coefficients corresponding to the signals representing the distinct gesture.","It should be noted that, for the purposes of the present disclosure, the term \u201clight,\u201d when used with \u201cdetect,\u201d \u201csense,\u201d \u201cconvert,\u201d and so forth, should not be construed as limited to the detection or conversion of the presence or absence of light (e.g., above or below a particular threshold), or to detecting or converting a spectrum of wavelengths to a single measurement representative of overall light intensity (e.g., irradiance) within the spectrum. Thus, the detection and\/or conversion of the presence of light, within the context of the present disclosure, may be used to refer to detecting and\/or converting the presence or absence of light (e.g., above or below a particular threshold), detecting and\/or converting a spectrum of wavelengths to a single measurement representative of overall light intensity within the spectrum, as well as to detecting and\/or converting multiple frequencies within a range of possible frequencies, such as detecting and\/or converting intensities of radiation separately in two or more subsets of wavelengths within a spectrum, as well as for individual frequencies, such as colors of light, and so forth.","Accordingly, phrases such as \u201cmore detected light\u201d and \u201cless detected light\u201d may refer to both representations of light within a broad range of wavelengths and representations of light within a limited range of wavelengths (e.g., for a particular color within a color spectrum, etc.). For example, the phrase \u201ca transition from more detected light to substantially less detected light and again to more detected light\u201d may be used to refer to measurements of light within a spectrum of wavelengths (e.g., for visible light), as well as to measurements of light at one or more specific wavelengths and\/or within multiple wavelength ranges (e.g., for a particular color). Thus, techniques described with reference to an array of photodiodes may also be applied with an image capture device (e.g., a camera), where an object (e.g., a hand) may be detected by differentiating its color from a different color indicative of the surrounding environment.","The electronic device  includes a display  to display information to a user of the electronic device . In embodiments, the display  may comprise an LCD (Liquid Crystal Diode) display, a TFT (Thin Film Transistor) LCD display, an LEP (Light Emitting Polymer) or PLED (Polymer Light Emitting Diode) display, an Organic Light Emitting Diode (OLED) display, and so forth, which may be configured to display text and\/or graphical information, such as a graphical user interface, and so forth. The electronic device  may further include one or more Input\/Output (I\/O) devices  (e.g., a keypad, buttons, a wireless input device, a thumbwheel input device, a trackstick input device, and so on). In an implementation, the photodetector  may be configured as an I\/O device . For example, the photodetector  may detect light representing gestures corresponding to a desired operation associated with the electronic device . Additionally, the I\/O devices  may comprise one or more audio I\/O devices, such as a microphone, speakers, and so on.","The electronic device  may include a communication module , representative of communication functionality to permit electronic device  to send\/receive data between different devices (e.g., components\/peripherals) and\/or over one or more networks . Communication module  may be representative of a variety of communication components and functionality including, but not necessarily limited to: an antenna; a browser; a transmitter and\/or a receiver; a wireless radio; a data port; a software interface and\/or a driver; a networking interface; a data processing component; and so forth. The one or more networks  are representative of a variety of different communication pathways and network connections which may be employed, individually or in combination, to communicate among the components of the environment . Thus, the one or more networks  may be representative of communication pathways achieved using a single network or multiple networks. Further, the one or more networks  are representative of a variety of different types of networks and connections that are contemplated, including, but not necessarily limited to: the Internet; an intranet; a satellite network; a cellular network; a mobile data network; wired and\/or wireless connections; and so forth.","Examples of wireless networks include, but are not necessarily limited to: networks configured for communications according to: one or more standard of the Institute of Electrical and Electronics Engineers (IEEE), such as 802.11 or 802.16 (Wi-Max) standards; Wi-Fi standards promulgated by the Wi-Fi Alliance; Bluetooth standards promulgated by the Bluetooth Special Interest Group; a 3G network; a 4G network; and so on. Wired communications are also contemplated such as through USB, Ethernet, serial connections, and so forth. The electronic device , through functionality represented by the communication module , may be configured to communicate via one or more networks  to receive various content  from one or more content repositories  (e.g., an Internet provider, a cellular data provider, etc.). Content  may represent a variety of different content, examples of which include, but are not necessarily limited to: web pages; services, music, photographs, video, email service, instant messaging, device drivers, instruction updates, and so forth.","The electronic device  may include a user interface , which is storable in memory  and executable by the processor . The user interface  is representative of functionality to control the display of information and data to the user of the electronic device  via the display . In some implementations, the display  may not be included as a part of the electronic device  and may instead be connected externally using USB, Ethernet, serial connections, and so forth. The user interface  may provide functionality to allow the user to interact with one or more applications  of the electronic device  by providing inputs via the I\/O devices . For example, the user interface  may cause an Application Programming Interface (API) to be generated to expose functionality to an application  to configure the application for display by the display , or in combination with another display. In embodiments, the API may further expose functionality to configure the application  to allow a user to interact with an application by providing inputs via the I\/O devices . For example, a user may provide hand gestures proximate to the photodetector  corresponding to a desired operation associated with an application . For instance, a user may perform a finger swipe proximate to the photodetector  to transition between various display pages showing various applications  within the display .","The electronic device  may include applications , which may comprise software storable in memory  and executable by the processor , e.g., to perform a specific operation or group of operations to furnish functionality to the electronic device . Example applications include cellular telephone applications, instant messaging applications, email applications, gaming applications, address book applications, and so forth. In implementations, the user interface  may include a browser . The browser  enables the electronic device  to display and interact with content , such as a webpage within the World Wide Web, a webpage provided by a web server in a private network, and so forth. The browser  may be configured in a variety of ways. For example, the browser  may be configured as an application  accessed by the user interface . The browser  may be a web browser suitable for use by a full resource device with substantial memory and processor resources (e.g., a smart phone, a PDA, etc.). The browser  may be a mobile browser suitable for use by a low-resource device with limited memory and\/or processing resources (e.g., a mobile telephone, a portable music device, a transportable entertainment device, etc.).","The electronic device  is configured to detect gestures via the photodetector  and generate a compact representation of the detected gestures. As described above, the estimator  is configured to generate estimated values relating to the light incident upon the photodetector . The estimated values are generated based upon the characteristics of the detected light. Thus, the estimated values may depend upon the intensity of light incident upon the photodetector , the amount of time the light is incident upon the photodetector , an orientation (e.g., direction) of the light incident upon the photodetector , and so forth. In implementations, the ellipse estimation module  receives the estimated values, such as the velocity values, from the estimator  and generates the five (5) coefficients (Cx, Cy, Rx, Ry, theta) based upon the estimated values. The five coefficients may be used to form an ellipse in a geographic plane to represent the detected gesture. In implementations, the semi-major radius of the ellipse is proportional to the speed\/dimensionless velocity vector of the gesture (which can be measured with respect to a baseline gesture), and the orientation of the ellipse corresponds to the direction of the gesture (e.g., with respect to the orientation of the photodetector ). Further, the area of the ellipse may convey the size (e.g., height) of an object performing the gesture.","In implementations, an elliptical representation of a gesture can be described (e.g., stored, transmitted, interpreted, and so forth) in a variety of ways. For example, an elliptical representation can be described using coefficients to represent a mathematical definition of the ellipse (e.g., as previously described). Further, an elliptical representation can be described as an image (e.g., a bitmap, etc.). In still further implementations, an elliptical representation of a gesture can be described using magnitude and angle measures (e.g., pseudo velocity and degrees notation, respectively). For example, a slow left-to-right gesture can be denoted as [097,0.1], while a faster right to left gesture may be denoted as [271,0.4] (where degree measurements are described in compass rose notation). It should be noted that a discrete event interface (e.g., where a right-to-left gesture is denoted as a text string, like \u201cRight2Left,\u201d a left-to-right gesture is denoted as \u201cLeft2Right,\u201d and so forth) may be provided within the context of a gaming interface.","In implementations, the elliptical representation of a gesture (e.g., an image of an elliptical representation, coefficients defining an elliptical representation, magnitude and angle information derived from an elliptical representation, a string of text, and so forth) comprises an intermediate representation of a gesture and may be used to derive one or more discrete events, including, but not necessarily limited to: a left-to-right swipe, a right-to-left swipe, a top-to-bottom swipe, a bottom-to-top swipe, a stop-pause-select motion, a two-finger pinch, a two-finger zoom, a two-finger rotate, and so forth. In implementations, the elliptical representation may be used by the electronic device  in various applications. For example, an ellipse and\/or the coefficients of the ellipse may be used as input commands to the electronic device  and\/or to another device connected to the electronic device  (e.g., when the electronic device  is implemented as an interface device). For example, an elliptical representation having a small magnitude (e.g., with reference to a baseline gesture having a larger magnitude) may be used to define a zoom command.","A user may transition through an electronic book (e.g., perform finger swipes to \u201cturn\u201d the pages of the electronic book) displayed within the display . In another implementation, the coefficients may be provided as parameters for an application . For example, the coefficients may represent a desired action within a gaming sequence. For instance, the coefficients may be provided to a gaming application , such as a golfing game, and the coefficients can represent the power and orientation of an input to the gaming application  (e.g., the coefficients can represent the power and orientation of a golf swing within the golfing game). Additionally, the coefficients may be used as parameters to operate the user interface . Further, the coefficients may be used by the electronic device  within multiple applications that require input commands from a user. For example, an operating system or application may respond to a discrete command derived from an elliptical representation of a gesture by advancing a display at a rate proportional to a derived speed, a derived velocity vector, or a derived linear (e.g., horizontal or vertical) component vector derived from a velocity vector.","Using this type of approach, a user may navigate (e.g., flick) through menus controlling speed, direction, and\/or selection. For example, a user may navigate through a cascading series of graphical representations of cover-flow artwork with quick right-to-left swipes followed by slower right-to-left swipes as the user gets closer to a desired track. Then, a stop-pause-select event may be used to complete a selection. A bottom-to-top swipe may constitute a cancel event. The ability to provide compact and lossless representations of such gestures can provide an intuitive and touch-free user interface. In another example implementation, left-to-right swipes can be used to change channels on a smart TV, while a top-to-bottom swipe can be used to lower the volume of the TV. This type of interface can be implemented using, for example, a photodetector  positioned in the bezel of a TV frame, and may supplement or replace the buttons that would otherwise be provided for enabling control of the TV functions. In a further example, horizontal and\/or vertical swipes can be used to advance the pages of a buttonless eReader.","As shown in , the electronic device  may be configured to render a display of an ellipse representing a gesture as visual feedback to allow a user to visualize and\/or refine the user's gesture. In an implementation, the ellipse estimation module  is configured to provide instructions to the display  to display the generated ellipse (and possibly subsequently generated ellipses) based upon a user's input gestures. Thus, the ellipse estimation module  can generate the five coefficients based upon a gesture performed within the field of view of the photodetector . Once the ellipse estimation module  generates the coefficients, the module  (or another module) may provide instructions to display an ellipse based upon the coefficients using, for instance, the display . The user may then perform subsequent gestures to further refine a gesture (e.g., when establishing a baseline gesture, as previously described). In some implementations, the ellipse estimation module  and\/or an application  that uses the coefficients for various parameters, may provide feedback to the user (e.g., displaying a predetermined ellipse superimposed with the generated ellipse). This feedback can allow the user to adjust gesture motions according to the feedback.","Generally, any of the functions described herein can be implemented using software, firmware, hardware (e.g., fixed logic circuitry), manual processing, or a combination of these implementations. For example, as implemented with a smart phone and\/or a tablet computing device, an algorithm for determining an elliptical representation of a gesture can exist on an application processor and\/or within a co-processor\/subsystem. The terms \u201cmodule\u201d and \u201cfunctionality\u201d as used herein generally represent software, firmware, hardware, or a combination thereof. The communication between modules in the electronic device  of  can be wired, wireless, or some combination thereof. In the case of a software implementation, for instance, the module represents executable instructions that perform specified tasks when executed on a processor, such as the processor  with the electronic device  of . The program code can be stored in one or more device-readable storage media, an example of which is the memory  associated with the electronic device  of .","Example Procedures","The following discussion describes procedures that may be implemented in an electronic device for detecting gestures. Aspects of the procedures may be implemented in hardware, firmware, or software, or a combination thereof. The procedures are shown as a set of blocks that specify operations performed by one or more devices and are not necessarily limited to the orders shown for performing the operations by the respective blocks. In portions of the following discussion, reference may be made to the environment  of . The features of techniques described below are platform-independent, meaning that the techniques may be implemented on a variety of commercial electronic device platforms having a variety of processors.",{"@attributes":{"id":"p-0073","num":"0084"},"figref":["FIG. 15","FIG. 15","FIG. 13"],"b":["1500","1502","1306","1300","1306","1307"]},"As shown in , one or more values are estimated based upon the signals generated by the sensor (Block ). With continuing reference to , once signals are generated by the photodetector , the estimator  receives the signals and is configured to estimate one or more values based upon the characteristics of the light represented by the signals. In implementations, the estimator estimates velocity vectors (e.g., a speed and an orientation of the gesture with respect to the photodetector) via suitable stochastic techniques of the detected gesture (e.g., using a Kalman estimator). The signals may represent a distinct gesture as detected by the photodetector . For example, a single gesture may be defined for a time period from when the photodetector  initially detects light reflected within the specific wavelength until the photodetector  does not at least substantially detect light reflected within the specific wavelength (e.g., a hand passes over the photodetector  and reflects light for a period of time).","The estimator may determine whether a gesture has been detected (Decision Block ). If the gesture is not complete (NO from Decision Block ), the estimator continues to receive signals from the photodetector and generates estimated values based upon the signals. When a completed gesture is detected (YES from Decision Block ), the estimator furnishes the estimated values, such as the velocity vectors, to the ellipse estimation module.","Then, an elliptical representation of the gesture is determined (Block ). For example, coefficients are derived from the estimated values (Block ). In implementations, five (5) coefficients are derived by the ellipse estimation module through a suitable ellipse estimation model, such as a Least Squares model (e.g., as previously described). The coefficients can comprise two (2) center coefficients that represent the center coordinates of the ellipse within a geographic (e.g., Cartesian) plane (Cx, Cy), two (2) radii coefficients that represent the radii values of the ellipse within the geographic plane (Rx, Ry), and one (1) coefficient that represents the orientation of the ellipse within the geographic plane (theta). Once the ellipse estimation module derives the coefficients, the coefficients may be used as parameters in various applications. For example, the coefficients may be used to transition between pages within an electronic book. In another implementation, the coefficients may be used to generate an ellipse for display via the electronic device.","Although the subject matter has been described in language specific to structural features and\/or process operations, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather, the specific features and acts described above are disclosed as example forms of implementing the claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The detailed description is described with reference to the accompanying figures. In the figures, the left-most digit(s) of a reference number identifies the figure in which the reference number first appears. The use of the same reference numbers in different instances in the description and the figures may indicate similar or identical items.",{"@attributes":{"id":"p-0006","num":"0005"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0007","num":"0006"},"figref":["FIG. 2","FIG. 1"]},{"@attributes":{"id":"p-0008","num":"0007"},"figref":["FIG. 3","FIG. 1"]},{"@attributes":{"id":"p-0009","num":"0008"},"figref":["FIG. 4","FIG. 1"]},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":["FIG. 6","FIG. 5"]},{"@attributes":{"id":"p-0012","num":"0011"},"figref":["FIG. 7","FIG. 5"]},{"@attributes":{"id":"p-0013","num":"0012"},"figref":["FIG. 8","FIG. 5"]},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":["FIG. 11","FIG. 9"]},{"@attributes":{"id":"p-0017","num":"0016"},"figref":["FIG. 12","FIG. 10"]},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":["FIGS. 14A through 14D","FIG. 13"]},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 15"}]},"DETDESC":[{},{}]}
