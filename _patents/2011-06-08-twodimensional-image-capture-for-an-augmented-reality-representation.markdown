---
title: Two-dimensional image capture for an augmented reality representation
abstract: Technologies and implementations for capturing two-dimensional images for an augmented reality representation are generally disclosed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09058687&OS=09058687&RS=09058687
owner: Empire Technology Development LLC
number: 09058687
owner_city: Wilmington
owner_country: US
publication_date: 20110608
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["This application is a national stage filing under 35 U.S.C. \u00a7371 of PCT Application No. PCT\/US2011\/39639, filed on Jun. 8, 2011.","Unless otherwise indicated herein, the approaches described in this section are not prior art to the claims in this application and are not admitted to be prior art by inclusion in this section.","Some image processing services may merge several images into a virtual tour. Such virtual tours may attempt to simulate as much of a three-dimensional (3D) an experience as possible. Meanwhile other systems may convert a series of two-dimensional (2D) images to 3D.","The quality of the 3D model or synthesis produced may depend on the positioning and resolution of photos. When using photo-to-3D conversion, for example, it is possible to wind up with some combination of missing surfaces and\/or images that lack enough shared context or edges to allow the integration of a photo into such a 3D model.","As augmented reality representations (e.g., 3D images) may be used for more and more purposes, average home users may want to be able to take photos of objects and places and generate good quality 3D models of them.","Some example methods, apparatus, and systems described herein may relate to capturing two-dimensional images for an augmented reality representation.","Some example methods, apparatus, and systems related to capturing two-dimensional images for an augmented reality representation may be implemented in an electronic image capturing device. Such methods may include determining, by an electronic image capturing device, an augmented reality representation of an object based at least in part on an analysis of one or more two-dimensional images of the object. Such an augmented reality representation may include a plurality of surface hypotheses. One or more reliability values associated with individual surface hypotheses may be determined. The one or more reliability values may be compared with one or more threshold reliability criteria to identify one or more surface areas of interest from the plurality of surface hypotheses. Guidance regarding capturing one or more additional two-dimensional images of the object may be displayed via a display of the electronic image capturing device. Such guidance may be based at least in part on the identified surface areas of interest.","Some example methods, apparatus, and systems related to capturing two-dimensional images for an augmented reality representation might be implemented in an electronic image capturing device. Such an electronic image capturing device may include an optical sensor and a control unit. Such an optical sensor may be configured to capture two-dimensional images of an object and capture real-time video data. Such a control unit may be configured to determine an augmented reality representation of the object based at least in part on an analysis of one or more two-dimensional images of the object. Such an augmented reality representation may include a plurality of surface hypotheses. One or more reliability values associated with individual surface hypotheses may be determined by the control unit. The one or more reliability values may be compared with one or more threshold reliability criteria to identify one or more surface areas of interest from the plurality of surface hypotheses. Guidance may be determined regarding capturing one or more additional two-dimensional images of the object via a display of the electronic image capturing device. Such guidance may be based at least in part on the identified surface areas of interest.","The foregoing summary is illustrative only and is not intended to be in any way limiting. In addition to the illustrative aspects, embodiments, and features described above, further aspects, embodiments, and features will become apparent by reference to the drawings and the following detailed description.","The following description sets forth various examples along with specific details to provide a thorough understanding of claimed subject matter. It will be understood by those skilled in the art, however, that claimed subject matter may be practiced without some or more of the specific details disclosed herein. Further, in some circumstances, well-known methods, procedures, systems, components and\/or circuits have not been described in detail in order to avoid unnecessarily obscuring claimed subject matter.","In the following detailed description, reference is made to the accompanying drawings, which form a part hereof. In the drawings, similar symbols typically identify similar components, unless context dictates otherwise. The illustrative embodiments described in the detailed description, drawings, and claims are not meant to be limiting. Other embodiments may be utilized, and other changes may be made, without departing from the spirit or scope of the subject matter presented here. It will be readily understood that the aspects of the present disclosure, as generally described herein, and illustrated in the Figures, can be arranged, substituted, combined, and designed in a wide variety of different configurations, all of which are explicitly contemplated and make part of this disclosure.","This disclosure is drawn, inter alia, to methods, apparatus, and systems related to capturing two-dimensional images for an augmented reality representation.","Methods, apparatus, and systems are described below for computing advantageous viewpoints for photography in order to build augmented reality representations (e.g., 3D models) and showing those viewpoints to a user on their electronic image capturing device. For example, a smart phone-type electronic image capturing device may display a preferred vantage point showing a target view that is suggested to be photographed by the user. Such a preferred vantage point may have been calculated to provide additional image data for portions of the augmented reality representation that have low quality data or portions of the augmented reality representation that were not directly viewed but have been theorized to exist.",{"@attributes":{"id":"p-0028","num":"0027"},"figref":["FIG. 1","FIG. 3","FIG. 4"],"b":["100","100","100"]},"In the illustrated example, electronic image capturing device  may include an optical sensor , a control unit , and\/or a display . Such an optical sensor  may be configured to capture two-dimensional images of an object (not illustrated) and\/or capture real-time video data.","Control unit  may be configured to determine an augmented reality representation of such an object captured by optical sensor . For example, control unit  may be configured to determine an augmented reality representation based at least in part on an analysis of one or more two-dimensional images of the object. As used herein the term \u201caugmented reality representation\u201d may refer to a real-time direct view or an indirect view of a physical, real-world object, such as a three-dimensional image of an object or the like. Guidance regarding capturing one or more additional two-dimensional images of the object may be provided to a user, such as by visual guidance delivered via display . Control unit  may utilize such additional two-dimensional images of the object to update the augmented reality representation of such an object.","Further, electronic image capturing device  may also include additional items such as memory, a processor, network interface logic, etc. that have not been shown in  for the sake of clarity. For example, electronic image capturing device  may also include additional items, such as those described with respect to  below. In some examples, display  may be housed within electronic image capturing device  or display  may be an unattached display (e.g., such as a head-up display or secondary display device).",{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 2","b":["100","200","100","200","202","100","200","202"]},"Guidance regarding capturing one or more additional two-dimensional images of the object may be provided to a user  via electronic image capturing device . For example, electronic image capturing device  may guide user  to capture another two-dimensional image of object  taken from a second position . Electronic image capturing device  may be configured to update the augmented reality representation based at least in part on an analysis of the two-dimensional image of object  taken from second position .",{"@attributes":{"id":"p-0034","num":"0033"},"figref":["FIG. 3","FIG. 3","FIG. 3","FIG. 3","FIG. 3","FIG. 3"],"b":["300","300","300","300","302","304","306","308"]},"As illustrated, process  may be implemented for capturing two-dimensional images for an augmented reality representation that may be implemented in an electronic image capturing device (see, e.g., ). In some examples, one or more two-dimensional images of the object may be analyzed to form surface hypotheses for an augmented reality representation of an object (e.g., 3D modeling). Such surface hypotheses may be used to determine surface areas of interest (e.g., missing surfaces or surfaces with image quality below a threshold reliability criteria). Such surface areas of interest may be used to determine guidance regarding capturing one or more additional two-dimensional images, such as viewing locations that will provide needed in-fill of features to provide a full model at a target quality, for example. Such guidance (e.g., viewing locations and\/or directions) may be provided to a user so that additional two-dimensional images may be captured and the augmented reality representation may be updated.","Processing may begin at operation , \u201cDETERMINE AN AUGMENTED REALITY REPRESENTATION INCLUDING MULTIPLE SURFACE HYPOTHESES\u201d, where an augmented reality representation including a multiple number of surface hypotheses may be determined. For example, such an augmented reality representation of an object may be determined based at least in part on an analysis of one or more two-dimensional images of the object. Such an augmented reality representation may include multiple surface hypotheses. In some examples, \u201cpose matching\u201d (or the like) may be used to determine how two-dimensional images fit into an augmented reality representation (e.g., a 3D scene).","In some examples, the surface hypotheses may be computed employing a Random Sample Consensus Surface Extraction (RANSAC) algorithm, or the like, or combinations thereof. For example, RANSAC may take a point cloud and fit surface hypotheses to the point cloud. The overall approach can be described as generating a surface that minimizes the least squares difference from the surface to a point cloud while satisfying various assumptions. See, for example, Martin A. Fischler and Robert C. Bolles, \u201cRandom sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography,\u201d 24(6): 381-395, 1981. The assumptions can include things like an expectation of planarity and linear features (common for architecture), and assumptions that recognized symmetry may continue into undetected areas when not precluded by the data. RANSAC may be amenable to a variety of inputs such as assumptions of symmetry or suggested geometry from possible image recognition hits. RANSAC (or other suitable general \u201cerror estimation\u201d techniques) may be used in conjunction with lines in an image or edge conditions based on 3D extraction from multiple two-dimensional images or a video stream. For example, edges can be roughly extracted from video in the electronic image capturing device and then RANSAC may use tracked points to finalize shapes, textures, and\/or locations of edges and planes.","In some examples, the augmented reality representation may have a group of surface hypotheses that do not yet have images of their surface or surfaces with poor image quality. Such surface hypotheses may be unobserved, but some type of geometry may be developed for the augmented reality representation based at least in part on knowledge of where such surfaces can't be (e.g., where such surfaces can't be or it would have been seen) and\/or assumptions based on other elements of the augmented reality representation, such as symmetry or surface continuation, for example.","Processing may continue from operation  to operation , \u201cDETERMINE ONE OR MORE RELIABILITY VALUES ASSOCIATED WITH INDIVIDUAL SURFACE HYPOTHESES\u201d, where one or more reliability values associated with individual surface hypotheses may be determined. For example, such reliability values may quantify a quality of surface hypotheses (e.g., in pixels per area units, or the like). In such an example, such reliability values may be determined based at least in part on the number of pixels available per area units (e.g., in square inches or the like) associated with an individual surface hypothesis. Such reliability values may be utilized to determine which surface hypotheses might not meet quality standards (e.g., because they were imaged at a high incident angle).","Processing may continue from operation  to operation , \u201cIDENTIFY ONE OR MORE SURFACE AREAS OF INTEREST\u201d, where one or more surface areas of interest may be identified. For example, the one or more reliability values may be compared with one or more threshold reliability criteria to identify one or more surface areas of interest from the surface hypotheses. For example, such threshold reliability criteria may include one or more of the following criteria: a pixel per area threshold, an incomplete image data threshold, an absent image data threshold, images not matching at overlaps, image blur level, gamma correction (e.g., matching luminance or tristimulus values in video or still image systems), or the like, and\/or combinations thereof. For example, a pixel per area-type reliability value associated with an individual surface hypothesis may be compared with a pixel per area-type threshold reliability criteria to determine if that individual surface hypothesis should be identified as a surface area of interest.","To evaluate quality of the surface hypotheses, an orthogonal transform may be performed to get an image (which need not be displayed, just in computation, for example) of what the surface looks like as seen from the perpendicular, then an image quality evaluation may be applied to this image. The quality evaluation may be tested against a metric to classify each surface as sufficient or not.","In some examples, the surface hypotheses of the augmented reality representation may include higher quality surface hypotheses that were extracted based on existing image data and lower quality polygon \u201choles\u201d where there may be no existing image data. Such \u201choles\u201d (e.g., where there is no existing image data) would fail any metric but zero automatically, and be identified as surface areas of interest.","Processing may continue from operation  to operation , \u201cDETERMINE GUIDANCE REGARDING CAPTURING ONE OR MORE ADDITIONAL TWO-DIMENSIONAL IMAGES\u201d, where guidance regarding capturing one or more additional two-dimensional images may be determined. For example, guidance regarding capturing one or more additional two-dimensional images of the object may be displayed via a display of the electronic image capturing device. Such guidance may be based at least in part on the identified surface areas of interest, for example. Process  may utilize such additional two-dimensional images of the object to update the augmented reality representation.","As will be discussed in greater detail below with respect to , in some examples, such guidance may indicate one or more vantage points for capturing the one or more additional two-dimensional images of the object. Additionally or alternatively, such guidance may indicate the reliability values associated with individual surface hypotheses.","Some additional and\/or alternative details related to process  may be illustrated in one or more examples of implementations discussed in greater detail below with regard to .",{"@attributes":{"id":"p-0046","num":"0045"},"figref":"FIG. 4","b":["400","400","402","404","406","408","410","412","414","416"]},"Processing may begin at operation , \u201cDETERMINE AN AUGMENTED REALITY REPRESENTATION INCLUDING MULTIPLE SURFACE HYPOTHESES\u201d, where an augmented reality representation including a multiple number of surface hypotheses may be determined. For example, such an augmented reality representation of an object may be determined based at least in part on an analysis of one or more two-dimensional images of the object. Such an augmented reality representation may include multiple surface hypotheses.","Additionally or alternatively, prior to operation , one or more two-dimensional images of the object may be selected for determining the augmented reality representation of the object. For example a user may indicate an already taken image or may activate a camera mode to establish that the current viewport is framing something the user wishes to model.","Processing may continue from operation  to operation , \u201cDETERMINE ONE OR MORE RELIABILITY VALUES ASSOCIATED WITH INDIVIDUAL SURFACE HYPOTHESES\u201d, where one or more reliability values associated with individual surface hypotheses may be determined.","Processing may continue from operation  to operation , \u201cIDENTIFY ONE OR MORE SURFACE AREAS OF INTEREST\u201d, where one or more surface areas of interest may be identified. For example, the one or more reliability values may be compared with one or more threshold reliability criteria to identify one or more surface areas of interest from the multiple number of surface hypotheses.","Processing may continue from operation  to operation , \u201cDETERMINE ONE OR MORE VIEWING CONES\u201d, where one or more viewing cones may be determined. For example, one or more viewing cones associated with individual surface areas of interest may be determined. As used herein, the term \u201cviewing cone\u201d may refer to a defined viewing area bounded by a defined viewing angle and centered by a surface normal vector.","For example, a surface normal vector and viewing angle (e.g., a viewing cone) can be developed from individual surface hypotheses. Such a viewing cone may provide a set of locations and view angles that can guide the user to occupy to observe surface areas of interest. In cases where the user reaches this location and view angle and does not observe the surface area of interest, an additional two-dimensional image may be captured, updating the augmented reality representation and the surface areas of interest. Alternatively, in cases where the user reaches this location and view angle and does not observe the surface area of interest, the user may select another location and view angle associated with another surface area of interest.","In some examples, small areas of low quality may generate excessive (e.g., dozens or thousands) surface areas of interest and the associated viewing cones. In such examples, viewing cones of such excessive surface areas of interest may be merged into a minimal intersection to choose one or a series of fewer additional two-dimensional images that can be used to provide some or all of the missing information. In some examples, the merging of the viewing cones into groups for establishing a few discrete two-dimensional images to take, may be implemented as a version of the programming Knapsack problem, solutions to which may include the Greedy algorithm, linear programming (e.g., linear programming may be appropriate in some cases due to geometric restrictions), or the like, and\/or combinations thereof.","Additionally or alternatively, a filter may also be applied to require a minimum augmented reality representation impact before justifying capturing additional two-dimensional images. Such a filter may prevent tiny transition surfaces or surface glare from generating large numbers of surface areas of interest.","Processing may continue from operation  to operation , \u201cDETERMINE A VIEW RANGE AND ORIENTATION\u201d, where a view range and orientation may be determined. For example, a view range and orientation associated with one or more of the viewing cones may be determined based at least in part on a quality level input.","In some examples, such a quality level input may have preset default settings and\/or settings that may be adjusted based on user input. For example, a user may indicate an overall image quality desired (e.g., high-quality, medium-quality, or low-quality, or the like). Additionally or alternatively, in some examples, it may be possible to obtain the quality level input from another source (e.g., like a game or publication or program that may take a 3D model as an input) in order to allow such a source to require augmented reality representations of a certain quality. Additionally or alternatively, such quality level input may vary depending on the location of the associated individual surface hypotheses and\/or the type of object being represented. For example, the object may be a vehicle (e.g., which may be independently recognized by the electronic image capturing device or may be indicated by user input). The data regarding the underside of such a vehicle-type object may not be needed at all, or may have a relatively lower associated quality level input as compared to other portions of the vehicle-type object. Accordingly, such quality level input may vary depending on the location of the associated individual surface hypotheses and\/or the type of object being represented. In some examples, such quality level input may compared with or adjust the threshold reliability criteria.","Referring back to , for example, a view range  between electronic image capturing device  and object  may be determined that is at an optimal distance while also reaching a desired quality. Such a desired quality may be based at least in part on a pixel per area-based quality level input (e.g., such as pixels spread across area ), for example. Such an optimal distance that also reaches a desired quality may be based at least in part on a lens angle  and resolution of electronic image capturing device . Such lens angle  may come from lens setting extents and\/or a subtended angle of object  in frame. For example, view range  may be inversely proportional to the quality level input, inversely proportional to lens angle , and proportional to resolution of electronic image capturing device .","Referring back to , accordingly, such a determination of the view range and orientation may be influenced by the resolution of the electronic image capturing device, lens angle, and\/or on the quality level input (e.g., the desired quality). If only a rough outline is desired, the resultant view range and\/or orientation may be different as the view range and orientation angles may work out much larger, as the pixel density need not be as great (and there may be more overlaps of view cones\u2014potentially requiring fewer images). For example, such a resultant view range and\/or orientation may be based at least in part on a comparison of the quality level input to the size of a pixel at particular distance and\/or lens setting. Accordingly, adjusting the view range and orientation associated with one or more of the viewing cones based at least in part on a quality level input and\/or on resolution may increase or decrease the number of projected additional two-dimensional images (e.g., to achieve the fewest number of projected additional two-dimensional images needed).","Processing may continue from operation  to operation , \u201cMONITOR REAL-TIME VIDEO DATA\u201d, where real-time video data may be monitored. For example, real-time video data may be monitored by the electronic image capturing device in order to track the relative position and orientation between the electronic image capturing device and the object.","Processing may continue from operation  to operation , \u201cDETERMINE A REAL-TIME RELATIVE POSITION AND ORIENTATION\u201d, where a real-time relative position and orientation may be determined. For example, a real-time relative position and orientation between the electronic image capturing device and the object may be determined based at least in part on the real-time video data. For example, the real-time video data may be compared with a current augmented reality representation to confirm that the object is currently being monitored and\/or to determine the relative position and orientation between the electronic image capturing device and the object. For example, the real-time video data may be compared with a current augmented reality representation via \u201cpose matching\u201d (or the like); where such pose matching may be used to determine how two-dimensional images compare to an augmented reality representation (e.g., a 3D scene). See, for example, Lingyun Liu., Stamos I., A systematic approach for 2D-image to 3D-range registration in urban environments. IEEE ICCV, 1-8, 2007.","Processing may continue from operation  to operation , \u201cDETERMINE GUIDANCE REGARDING CAPTURING ONE OR MORE ADDITIONAL TWO-DIMENSIONAL IMAGES\u201d, where guidance regarding capturing one or more additional two-dimensional images may be determined. For example, guidance regarding capturing one or more additional two-dimensional images of the object may be displayed via a display of the electronic image capturing device. Such guidance may be based at least in part on the identified surface areas of interest, for example. In some examples, the guidance may be based at least in part on the view range and orientation associated with one or more of the viewing cones.","In some examples, such guidance may include the user being shown the view cones associated with individual surface areas of interest and asked to fill the view cones in. In some examples, such guidance may include the user being shown a set of locations and view angles that can guide the user to occupy to observe surface areas of interest (e.g., based at least in part on a corresponding viewing cone). In such an example, there might be a large number of overlapping possible viewing cones, (e.g., see ), and a particular guided location and view angle may be determined as being most beneficial (e.g., as perhaps that particular guided location and view angle is within several view cones). Additionally or alternatively, such guidance may include the user being shown some sort of overlay indicating the reliability values (or an indication of a comparison of the reliability values of with the threshold reliability criteria) associated with individual surface areas of interest or the reliability values associated with individual view cones (e.g., each potential photography location) so that they the user may decide where to take additional two-dimensional images to fill in the augmented reality representation.","This guidance may then evolve with each two-dimensional image taken, as the images associated with particular surface areas of interest or particular view cones are recognized as having been captured. In such an example, such guidance may then evolve with each two-dimensional image taken, as the augmented reality representation may be updated upon capturing additional two-dimensional images so that such particular surface areas of interest or particular view cones may be removed from indication when a corresponding two-dimensional image is captured. Additionally or alternatively, such guidance may evolve based at least in part on images captured from real-time video data captured by the electronic image capturing device.","In some examples, such guidance may only display a top one, two, or three view cones, which might be updated upon capturing an image from one of such view cones. Additionally or alternatively, such guidance may then evolve with each two-dimensional image taken, as the augmented reality representation may be updated upon capturing additional two-dimensional images, which may result in a new set of surface areas of interest and\/or view cones.","As will be discussed in greater detail below with respect to , in some examples, the guidance may be based at least in part on the view range and orientation associated with one or more of the viewing cones, as discussed above with respect to operations  and . Additionally or alternatively, the guidance may be based at least in part on the determined real-time relative position and orientation between the electronic image capturing device and the object, as discussed above with respect to operations  and .","Referring back to , as described with reference to processes  and\/or  of , an augmented reality representation may include a multiple number of surface hypotheses. One or more reliability values may be associated with individual surface hypotheses, which may be determined by control unit . The one or more reliability values may be compared with one or more threshold reliability criteria to identify one or more surface areas of interest from the multiple number of surface hypotheses. Guidance regarding capturing one or more additional two-dimensional images of the object may be provided to a user, such as by visual guidance delivered via display . Such guidance may be determined based at least in part on the identified surface areas of interest. For example, such guidance may prompt a user to capture one or more additional two-dimensional images of the object so as to obtain additional data regarding the identified surface areas of interest.",{"@attributes":{"id":"p-0067","num":"0066"},"figref":"FIG. 5","b":["500","500","502","504","506"]},"A number of different approaches may be employed to determine desired locations to capture one or more additional two-dimensional images in order to cover surface areas of interest (e.g., unobserved surfaces or low quality surfaces). This may be done based on information from observed surfaces. For example, RANSAC is the abbreviation for \u201cRandom Sample Consensus\u201d, which may be used for taking a point cloud and fitting surface hypotheses to the point cloud. The overall approach may be described as generating a surface that minimizes the least squares difference from the surface to a point cloud while satisfying various assumptions. The assumptions may include an expectation of planarity and linear features (common for architecture), recognition of symmetry (as in ), and comparable ones, which are likely to continue into unobserved areas when not precluded by the data. In order to determine desired locations to capture one or more additional two-dimensional images, surfaces that are in some way deficient may be identified such that more data on those surfaces can be collected.","The three dimensional figures in augmented reality representation  illustrate varying reliability values associated with individual surface hypothesis by varying the presentation of the individual surface hypothesis. According to an example scenario, such reliability values may be computed as the sum of the area proportion with point density below average and the ratio of the surface mean error to the surface mean error on surfaces closest to normal with the measured view. Thus, both the quality of the points and the proportion of the surface that has not been observed may be used in determining such reliability values.","For example, an augmented reality representation  generated based on a two-dimensional images similar to first view  may have a relatively high reliability value associated with surface , a relatively low reliability value associated with surface  and , and a relatively intermediate reliability value associated with surface . For example, surface  is easily seen from first view , while surface  can only be partially viewed or viewed from an extreme angle and surface  can only be predicted as it is not directly viewable, limiting the quality of data associated with surface  and surface .","Views , , and  of augmented reality representation  may be obtained as an output of a RANSAC surface extractor program. In views , , and , surfaces of the same architectural structure are hypothesized (referred to herein as surface hypothesis) and the structure is shown in various rotated positions to illustrate surface areas of interest (e.g., hypothesized but unobserved surfaces where surface image data has not been taken or low quality surfaces).","In addition to computing a suitable geometry, a reliability value may be generated for individual surface hypothesis segments (shown in different shades of grey). Instead of only using a least squares matching for confidence (which can actually be quite high for a presumed surface with few points), an appropriate reliability value (e.g., based on image pixels per area) may be used. In such an example, both the quality of the points (which is low here for the surface , even though it has been directly observed) and the proportion of the surface that has not been observed (which is high for surface  and surface ) may be evaluated.","In the illustrated example, reliability values may be illustrated by varying shades of grey for individual surface hypothesis segments. A relatively high reliability value associated with surface  may be illustrated with a light shade of grey, a relatively low reliability value associated with surface  and  might be illustrated with a dark shades of grey, and a relatively intermediate reliability value associated with surface  might be illustrated with an intermediate shade of grey. In some embodiments, such reliability values may be illustrated by varying shades of color (e.g., red, orange, yellow, and green). Additionally or alternatively, such reliability values may be illustrated by numerical or percentage values representing the reliability value appearing on or near the associated surface hypothesis segments or associated with an indicated cone or photo viewpoint.",{"@attributes":{"id":"p-0074","num":"0073"},"figref":"FIG. 6","b":["600","602","606","604","512","602","512","606","604","604","610","512","612","612","512","612","610"]},{"@attributes":{"id":"p-0075","num":"0074"},"figref":["FIG. 7","FIG. 5"],"b":["700","702","704","706","708","512","514","516"]},"Individual views cones  may be drawn using the surface normal vector associated with each surface area of interest (e.g., unobserved and low quality surfaces (, , \u2014of , for example). For non-planar surfaces this may be done by breaking down the surfaces that were previously identified as low quality from a single score (above) into a Voronoi tessellation (or the like) and giving each region a similar score and then using the largest area in the Voronoi diagram with a score below the threshold as the surface for normal generation, for example (e.g., generation of the cone from the left half of surface  in ). The segments of the Voronoi diagram may be the points in the plane that are equidistant to the two nearest sites. Then, each surface normal vector may be extended and the corresponding view cone  defined based on the imaging metrics discussed above.",{"@attributes":{"id":"p-0077","num":"0076"},"figref":["FIG. 8","FIG. 5"],"b":["800","802","708","512","514","516"]},"Some or all of the multiple number of view cones  may overlap at an intersection . The selection of desired locations for capturing images may be based at least in part on finding locations that generate as much data on as many surface areas of interest as possible (e.g., such as via an intersection ). In algorithmic terms, if filling in one of view cones  is allotted as having a certain \u201cscore\u201d then each available view cone  may have an overall compound score that may represent how much data that view cone  contributes to the augmented reality representation. The possible locations may also have other limitation on them such as height of the photographer and exclusion of taking two-dimensional images from inside other objects. Taking the highest scoring locations into mind may result in filling in the augmented reality representation with the least images, such as where some or all of the multiple number of view cones  may overlap at intersection , for example.",{"@attributes":{"id":"p-0079","num":"0078"},"figref":"FIG. 9"},"As discussed above (with respect to ), a reliability value may be generated for individual surface hypothesis segments (shown in different shades of grey). In the illustrated example, reliability values may be illustrated by varying shades of grey for individual surface hypothesis segments. A relatively low reliability value associated with surface  and  might be illustrated with a dark shades of grey, and a relatively intermediate reliability value associated with surface  might be illustrated with an intermediate shade of grey. In some embodiments, such reliability values may be illustrated by varying shades of color (e.g., red, orange, yellow, and green). In such an example, the color red might be associated with an unobserved surface , the color orange might illustrate a relatively low reliability value associated with surface , the color yellow might illustrate a relatively intermediate reliability value associated with surface , and the color green might illustrate a relatively high reliability value. Additionally or alternatively, such reliability values may be illustrated by numerical or percentage values representing the reliability value appearing on or near the associated surface hypothesis segments.","In some examples, such guidance may include a display of a current augmented reality representation . Such a current augmented reality representation  may include illustrated reliability values associated with surfaces , , and\/or , for example. Current augmented reality representation  may be rotated by user interaction with the electronic image capturing device (e.g., via touch screen or the like), so that a user might see visually from which angles individual surface areas of interest (e.g., surfaces  and\/or ) might best be viewed.","Additionally or alternatively, in some examples, such guidance may include a display of current augmented reality representation  that may be oriented to represent a suggested vantage point to capture one or more additional two-dimensional images. In the illustrated example, current augmented reality representation  may be oriented so that individual surface areas of interest (e.g., surfaces , , and\/or ) might best be viewed.","Additionally or alternatively, in some examples, such guidance may include a display of object  (e.g., a live display). View cone  may be superimposed on object  to represent a suggested vantage point to capture one or more additional two-dimensional images. Additionally or alternatively, other visual guidance may include the user being shown a location and view angle that can guide the user to occupy to observe surface areas of interest (e.g., based at least in part on a corresponding viewing cone). In such examples, the guidance may be based at least in part on the determined real-time relative position and orientation between the electronic image capturing device and the object. For example, the electronic image capturing device may instruct a user to keep the electronic image capturing device focused on the object as the user changes location, so that the display of object  and view cone  may be dynamically altered as relative position and orientation between the electronic image capturing device and the object changes.","Additionally or alternatively, view cone  may be superimposed on current augmented reality representation  and\/or  to represent a suggested vantage point to capture one or more additional two-dimensional images. In some examples, a multiple number of view cones  may be presented. In such an example, a visual indication may be given when an image is captured that satisfies each of the multiple number of view cones .","In some examples, the user can be guided to satisfy each two-dimensional image by showing arrows  indicating to shift or rotate the electronic image capturing device, hold it higher or lower, or to approach or move away from the object. The desired distance from an object may be a trade off between a wider view and higher resolution, and distance may be calculated by comparing the required metric (e.g., in pixels per linear distance) to the electronic image capturing device resolution and the angle of view to surfaces in the frame. Such a resolution metric may set a maximum distance while minimum distance influences range of view and thus how many images may need to be acquired. In such an example, the guidance may be based at least in part on the determined real-time relative position and orientation between the electronic image capturing device and the object. For example, the electronic image capturing device may instruct a user to keep the electronic image capturing device focused on the object as the user changes location, so that the display of arrows  (indicating to shift or rotate the electronic image capturing device, hold it higher or lower, or to approach or move away from the object) may be dynamically altered as relative position and orientation between the electronic image capturing device and the object changes.","In operation, display  may be utilized to provide a dynamic feedback that may give the user better results and less frustration. Additionally or alternatively, real-time updating of the augmented reality representations (e.g., 3D models) within the electronic image capturing device  (), may allow augmented reality representations to be viewed or shared immediately after generation, and\/or permit an iterative approach to improving an augmented reality representation as each additional two-dimensional image is captured.","In operation, providing real-time user feedback to help a user generate an augmented reality representation using two-dimensional images may reduce user result frustration and a may allow for a higher level of quality from \u201ccrowd-sourced\u201d 3D objects. Such augmented reality representations may be used with electronic games, where users might create augmented reality representations of gear, places, or people and use them in a game; general photography, where users might generate 3D location and feature imagery for sharing with friends; and\/or documentation, where users may record positions after a car accident or to record a location for annotation or later comparison to \u201cbefore.\u201d",{"@attributes":{"id":"p-0088","num":"0087"},"figref":["FIG. 10","FIG. 3","FIG. 4","FIG. 1","FIG. 3","FIG. 4"],"b":["1000","1000","1002","1002","1004","100","1004","1002"]},"In some implementations, signal bearing medium  may encompass a non-transitory computer-readable medium , such as, but not limited to, a hard disk drive, a Compact Disc (CD), a Digital Versatile Disk (DVD), a digital tape, memory, etc. In some implementations, signal bearing medium  may encompass a recordable medium , such as, but not limited to, memory, read\/write (R\/W) CDs, R\/W DVDs, etc. In some implementations, signal bearing medium  may encompass communications medium , such as, but not limited to, a digital and\/or an analog communication medium (e.g., a fiber optic cable, a waveguide, a wired communications link, a wireless communication link, etc.).",{"@attributes":{"id":"p-0090","num":"0089"},"figref":"FIG. 11","b":["1100","1101","1100","1110","1120","1130","1110","1120"]},"Depending on the desired configuration, processor  may be of any type including but not limited to a microprocessor (\u03bcP), a microcontroller (\u03bcC), a digital signal processor (DSP), or any combination thereof. Processor  may include one or more levels of caching, such as a level one cache  and a level two cache , a processor core , and registers . The processor core  may include an arithmetic logic unit (ALU), a floating point unit (FPU), a digital signal processing core (DSP Core), or any combination thereof. A memory controller  may also be used with the processor , or in some implementations the memory controller  may be an internal part of the processor .","Depending on the desired configuration, the system memory  may be of any type including but not limited to volatile memory (such as RAM), non-volatile memory (such as ROM, flash memory, etc.) or any combination thereof. System memory  may include an operating system , one or more applications , and program data . Application  may include an image guidance algorithm  that is arranged to perform the functions as described herein including the functional blocks and\/or actions described with respect to process  of  and\/or process  of . Program Data  may include image data  for use with image guidance algorithm . In some example embodiments, application  may be arranged to operate with program data  on an operating system  such that implementations of providing guidance to capture two-dimensional images for an augmented reality representation may be provided as described herein. For example, electronic image capturing device  (see, e.g., ) may comprise all or a portion of computing device  and be capable of performing all or a portion of application  such that implementations of providing guidance to capture two-dimensional images for an augmented reality representation may be provided as described herein. This described basic configuration is illustrated in  by those components within dashed line .","Computing device  may have additional features or functionality, and additional interfaces to facilitate communications between the basic configuration  and any required devices and interfaces. For example, a bus\/interface controller  may be used to facilitate communications between the basic configuration  and one or more data storage devices  via a storage interface bus . The data storage devices  may be removable storage devices , non-removable storage devices , or a combination thereof. Examples of removable storage and non-removable storage devices include magnetic disk devices such as flexible disk drives and hard-disk drives (HDD), optical disk drives such as compact disk (CD) drives or digital versatile disk (DVD) drives, solid state drives (SSD), and tape drives to name a few. Example computer storage media may include volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information, such as computer readable instructions, data structures, program modules, or other data.","System memory , removable storage  and non-removable storage  are all examples of computer storage media. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which may be used to store the desired information and which may be accessed by computing device . Any such computer storage media may be part of device .","Computing device  may also include an interface bus  for facilitating communication from various interface devices (e.g., output interfaces, peripheral interfaces, and communication interfaces) to the basic configuration  via the bus\/interface controller . Example output interfaces  may include a graphics processing unit  and an audio processing unit , which may be configured to communicate to various external devices such as a display or speakers via one or more NV ports . Example peripheral interfaces  may include a serial interface controller  or a parallel interface controller , which may be configured to communicate with external devices such as input devices (e.g., keyboard, mouse, pen, voice input device, touch input device, etc.) or other peripheral devices (e.g., printer, scanner, etc.) via one or more I\/O ports . An example communication interface  includes a network controller , which may be arranged to facilitate communications with one or more other computing devices  over a network communication via one or more communication ports . A communication connection is one example of a communication media. Communication media may typically be embodied by computer readable instructions, data structures, program modules, or other data in a modulated data signal, such as a carrier wave or other transport mechanism, and may include any information delivery media. A \u201cmodulated data signal\u201d may be a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media may include wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, radio frequency (RF), infrared (IR) and other wireless media. The term computer readable media as used herein may include both storage media and communication media.","Computing device  may be implemented as a portion of a small-form factor portable (or mobile) electronic device such as a cell phone, a personal data assistant (PDA), a personal media player device, a wireless web-watch device, a personal headset device, an application specific device, or a hybrid device that includes any of the above functions. Computing device  may also be implemented as a personal computer including both laptop computer and non-laptop computer configurations. In addition, computing device  may be implemented as part of a wireless base station or other wireless system or device.","Some portions of the foregoing detailed description are presented in terms of algorithms or symbolic representations of operations on data bits or binary digital signals stored within a computing system memory, such as a computer memory. These algorithmic descriptions or representations are examples of techniques used by those of ordinary skill in the data processing arts to convey the substance of their work to others skilled in the art. An algorithm is here, and generally, is considered to be a self-consistent sequence of operations or similar processing leading to a desired result. In this context, operations or processing involve physical manipulation of physical quantities. Typically, although not necessarily, such quantities may take the form of electrical or magnetic signals capable of being stored, transferred, combined, compared or otherwise manipulated. It has proven convenient at times, principally for reasons of common usage, to refer to such signals as bits, data, values, elements, symbols, characters, terms, numbers, numerals or the like. It should be understood, however, that all of these and similar terms are to be associated with appropriate physical quantities and are merely convenient labels. Unless specifically stated otherwise, as apparent from the following discussion, it is appreciated that throughout this specification discussions utilizing terms such as \u201cprocessing,\u201d \u201ccomputing,\u201d \u201ccalculating,\u201d \u201cdetermining\u201d or the like refer to actions or processes of a computing device, that manipulates or transforms data represented as physical electronic or magnetic quantities within memories, registers, or other information storage devices, transmission devices, or display devices of the computing device.","Claimed subject matter is not limited in scope to the particular implementations described herein. For example, some implementations may be in hardware, such as employed to operate on a device or combination of devices, for example, whereas other implementations may be in software and\/or firmware. Likewise, although claimed subject matter is not limited in scope in this respect, some implementations may include one or more articles, such as a signal bearing medium, a storage medium and\/or storage media. This storage media, such as CD-ROMs, computer disks, flash memory, or the like, for example, may have instructions stored thereon, that, when executed by a computing device, such as a computing system, computing platform, or other system, for example, may result in execution of a processor in accordance with claimed subject matter, such as one of the implementations previously described, for example. As one possibility, a computing device may include one or more processing units or processors, one or more input\/output devices, such as a display, a keyboard and\/or a mouse, and one or more memories, such as static random access memory, dynamic random access memory, flash memory, and\/or a hard drive.","There is little distinction left between hardware and software implementations of aspects of systems; the use of hardware or software is generally (but not always, in that in certain contexts the choice between hardware and software can become significant) a design choice representing cost vs. efficiency tradeoffs. There are various vehicles by which processes and\/or systems and\/or other technologies described herein can be effected (e.g., hardware, software, and\/or firmware), and that the preferred vehicle will vary with the context in which the processes and\/or systems and\/or other technologies are deployed. For example, if an implementer determines that speed and accuracy are paramount, the implementer may opt for a mainly hardware and\/or firmware vehicle; if flexibility is paramount, the implementer may opt for a mainly software implementation; or, yet again alternatively, the implementer may opt for some combination of hardware, software, and\/or firmware.","The foregoing detailed description has set forth various embodiments of the devices and\/or processes via the use of block diagrams, flowcharts, and\/or examples. Insofar as such block diagrams, flowcharts, and\/or examples contain one or more functions and\/or operations, it will be understood by those within the art that each function and\/or operation within such block diagrams, flowcharts, or examples can be implemented, individually and\/or collectively, by a wide range of hardware, software, firmware, or virtually any combination thereof. In one embodiment, several portions of the subject matter described herein may be implemented via Application Specific Integrated Circuits (ASICs), Field Programmable Gate Arrays (FPGAs), digital signal processors (DSPs), or other integrated formats. However, those skilled in the art will recognize that some aspects of the embodiments disclosed herein, in whole or in part, can be equivalently implemented in integrated circuits, as one or more computer programs running on one or more computers (e.g., as one or more programs running on one or more computer systems), as one or more programs running on one or more processors (e.g., as one or more programs running on one or more microprocessors), as firmware, or as virtually any combination thereof, and that designing the circuitry and\/or writing the code for the software and or firmware would be well within the skill of one of skill in the art in light of this disclosure. In addition, those skilled in the art will appreciate that the mechanisms of the subject matter described herein are capable of being distributed as a program product in a variety of forms, and that an illustrative embodiment of the subject matter described herein applies regardless of the particular type of signal bearing medium used to actually carry out the distribution. Examples of a signal bearing medium include, but are not limited to, the following: a recordable type medium such as a flexible disk, a hard disk drive (HDD), a Compact Disc (CD), a Digital Versatile Disk (DVD), a digital tape, a computer memory, etc.; and a transmission type medium such as a digital and\/or an analog communication medium (e.g., a fiber optic cable, a waveguide, a wired communications link, a wireless communication link, etc.).","Those skilled in the art will recognize that it is common within the art to describe devices and\/or processes in the fashion set forth herein, and thereafter use engineering practices to integrate such described devices and\/or processes into data processing systems. That is, at least a portion of the devices and\/or processes described herein can be integrated into a data processing system via a reasonable amount of experimentation. Those having skill in the art will recognize that a typical data processing system generally includes one or more of a system unit housing, a video display device, a memory such as volatile and non-volatile memory, processors such as microprocessors and digital signal processors, computational entities such as operating systems, drivers, graphical user interfaces, and applications programs, one or more interaction devices, such as a touch pad or screen, and\/or control systems including feedback loops and control motors (e.g., feedback for sensing position and\/or velocity; control motors for moving and\/or adjusting components and\/or quantities). A typical data processing system may be implemented utilizing any suitable commercially available components, such as those typically found in data computing\/communication and\/or network computing\/communication systems.","The herein described subject matter sometimes illustrates different components contained within, or connected with, different other components. It is to be understood that such depicted architectures are merely exemplary, and that in fact many other architectures can be implemented which achieve the same functionality. In a conceptual sense, any arrangement of components to achieve the same functionality is effectively \u201cassociated\u201d such that the desired functionality is achieved. Hence, any two components herein combined to achieve a particular functionality can be seen as \u201cassociated with\u201d each other such that the desired functionality is achieved, irrespective of architectures or intermedial components. Likewise, any two components so associated can also be viewed as being \u201coperably connected\u201d, or \u201coperably coupled\u201d, to each other to achieve the desired functionality, and any two components capable of being so associated can also be viewed as being \u201coperably couplable\u201d, to each other to achieve the desired functionality. Specific examples of operably couplable include but are not limited to physically mateable and\/or physically interacting components and\/or wirelessly interactable and\/or wirelessly interacting components and\/or logically interacting and\/or logically interactable components.","With respect to the use of substantially any plural and\/or singular terms herein, those having skill in the art can translate from the plural to the singular and\/or from the singular to the plural as is appropriate to the context and\/or application. The various singular\/plural permutations may be expressly set forth herein for sake of clarity.","It will be understood by those within the art that, in general, terms used herein, and especially in the appended claims (e.g., bodies of the appended claims) are generally intended as \u201copen\u201d terms (e.g., the term \u201cincluding\u201d should be interpreted as \u201cincluding but not limited to,\u201d the term \u201chaving\u201d should be interpreted as \u201chaving at least,\u201d the term \u201cincludes\u201d should be interpreted as \u201cincludes but is not limited to,\u201d etc.). It will be further understood by those within the art that if a specific number of an introduced claim recitation is intended, such an intent will be explicitly recited in the claim, and in the absence of such recitation no such intent is present. For example, as an aid to understanding, the following appended claims may contain usage of the introductory phrases \u201cat least one\u201d and \u201cone or more\u201d to introduce claim recitations. However, the use of such phrases should not be construed to imply that the introduction of a claim recitation by the indefinite articles \u201ca\u201d or \u201can\u201d limits any particular claim containing such introduced claim recitation to inventions containing only one such recitation, even when the same claim includes the introductory phrases \u201cone or more\u201d or \u201cat least one\u201d and indefinite articles such as \u201ca\u201d or \u201can\u201d (e.g., \u201ca\u201d and\/or \u201can\u201d should typically be interpreted to mean \u201cat least one\u201d or \u201cone or more\u201d); the same holds true for the use of definite articles used to introduce claim recitations. In addition, even if a specific number of an introduced claim recitation is explicitly recited, those skilled in the art will recognize that such recitation should typically be interpreted to mean at least the recited number (e.g., the bare recitation of \u201ctwo recitations,\u201d without other modifiers, typically means at least two recitations, or two or more recitations). Furthermore, in those instances where a convention analogous to \u201cat least one of A, B, and C, etc.\u201d is used, in general such a construction is intended in the sense one having skill in the art would understand the convention (e.g., \u201ca system having at least one of A, B, and C\u201d would include but not be limited to systems that have A alone, B alone, C alone, A and B together, A and C together, B and C together, and\/or A, B, and C together, etc.). In those instances where a convention analogous to \u201cat least one of A, B, or C, etc.\u201d is used, in general such a construction is intended in the sense one having skill in the art would understand the convention (e.g., \u201ca system having at least one of A, B, or C\u201d would include but not be limited to systems that have A alone, B alone, C alone, A and B together, A and C together, B and C together, and\/or A, B, and C together, etc.). It will be further understood by those within the art that virtually any disjunctive word and\/or phrase presenting two or more alternative terms, whether in the description, claims, or drawings, should be understood to contemplate the possibilities of including one of the terms, either of the terms, or both terms. For example, the phrase \u201cA or B\u201d will be understood to include the possibilities of \u201cA\u201d or \u201cB\u201d or \u201cA and B.\u201d","Reference in the specification to \u201can implementation,\u201d \u201cone implementation,\u201d \u201csome implementations,\u201d or \u201cother implementations\u201d may mean that a particular feature, structure, or characteristic described in connection with one or more implementations may be included in at least some implementations, but not necessarily in all implementations. The various appearances of \u201can implementation,\u201d \u201cone implementation,\u201d or \u201csome implementations\u201d in the preceding description are not necessarily all referring to the same implementations.","While certain exemplary techniques have been described and shown herein using various methods and systems, it should be understood by those skilled in the art that various other modifications may be made, and equivalents may be substituted, without departing from claimed subject matter. Additionally, many modifications may be made to adapt a particular situation to the teachings of claimed subject matter without departing from the central concept described herein. Therefore, it is intended that claimed subject matter not be limited to the particular examples disclosed, but that such claimed subject matter also may include all implementations falling within the scope of the appended claims, and equivalents thereof."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["Subject matter is particularly pointed out and distinctly claimed in the concluding portion of the specification. The foregoing and other features of the present disclosure will become more fully apparent from the following description and appended claims, taken in conjunction with the accompanying drawings. Understanding that these drawings depict only several embodiments in accordance with the disclosure and are, therefore, not to be considered limiting of its scope, the disclosure will be described with additional specificity and detail through use of the accompanying drawings.","In the drawings:",{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 11"}]},"DETDESC":[{},{}]}
