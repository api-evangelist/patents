---
title: Method and apparatus for implementing a speech interface for a GUI
abstract: A method and apparatus for providing speech control to a graphical user interface (GUI) divide a GUI into a plurality of screen areas; assign the screen areas priorities; receive a first audio input relating to the selection of one of the objects in the interface; determine the one of the screen areas having the highest priority and including a first object matching the first audio input; and select the first object in the determined screen area if the determined screen area only contains one object matching the first audio input. The method and apparatus also select one of the objects that matches the first audio input in the determined screen area if the determined screen area contains more than one object that matches the first audio input.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07036080&OS=07036080&RS=07036080
owner: SAP Labs, Inc.
number: 07036080
owner_city: Palo Alto
owner_country: US
publication_date: 20011130
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION"],"p":["This invention relates to the field of graphical user interfaces (GUIs) for PC systems, and, more specifically, to a speech interface for a GUI and a method for interfacing with a GUI using the same.","Many business applications use GUIs to allow users to interface with the applications for performing a number of operations. Typically, GUIs are mouse- and keyboard-intensive, which can be problematic or even impossible to use for many people, including those with physical disabilities. One type of interface that avoids a mouse or keyboard is a speech interface. A speech interface allows audio input of commands to communicate with applications, and can be used by anyone who wishes to speak to their system, such as mobile users with inadequately-sized keyboards and pointing devices.","One of the main challenges for a speech interface is specifying the desired target of an audio input, especially in a GUI where multiple selectable objects such as windows, text fields and icons, can have the same label or name. In these situations, it is important for both the computer system and the user to know the current focus when an audio input is issued, to help the system resolve possible ambiguities and to help the user keep track of what he is doing.","One type of ambiguity is called \u201ctarget ambiguity,\u201d where the target of a user's action is ambiguous and must be resolved. In a physical interaction involving a mouse or other pointing device, users specify the target of their actions directly by clicking on the selectable object of interest. Target ambiguities caused by selectable objects that have the same name are resolved through the physical interaction. With audio input, users do not have a way to resolve target ambiguities physically; therefore, the target ambiguity must be handled in some other way.","Traditional speech interfaces for GUIs typically emulate the keyboard and mouse directly using spoken equivalents; however, they are slow to operate and often take quite a bit longer to select an object than conventional mouse or keyboard techniques. Conventional speech interfaces lack public acceptance due to inaccurate control of the interfaces.","Other traditional speech interfaces for GUIs combine audio input with alternative pointing devices such as head- or eye-tracking technologies. However, conventional alternative pointing devices require calibration and expensive equipment, making them difficult to set up and use on computers shared by multiple people.","Still other traditional speech interfaces provide object selection solely by audio input. These speech interfaces explore a current window or current screen area to find selectable objects that match the audio input. One limitation of these speech interfaces is that they only explore the current screen area to find selectable objects matching the audio input. They do not explore the other screen areas for a match to the audio input. Instead, additional audio inputs are required to look for matches to the audio input in screen areas other than the current screen area. These additional required audio inputs decrease the efficiency of conventional speech interfaces.","Another limitation of these traditional speech interfaces involves their capability to resolve target ambiguity. These interfaces mark selectable objects matching the audio input with opaque icons for subsequent selection. However, placing an opaque icon adjacent to the selectable object often pushes screen elements out of place and distorts the screen layout. Furthermore, overlaying the selectable object with an opaque icon often obscures the underlying text and graphics. Finally, displaying opaque icons for all objects that match each other often clutters the screen. Thus, traditional speech interfaces often fail to maintain the integrity of screen layout and the view of the text and graphics of the selectable objects when resolving target ambiguities.","Methods and apparatus consistent with the present invention provide a speech interface for a GUI via a computer system.","Consistent with the present invention, a method for providing speech control to a GUI containing objects to be selected includes dividing the GUI into a plurality of screen areas; assigning priorities to the screen areas; and receiving a first audio input relating to the selection of one of the objects in the GUI. The method further includes determining one of the screen areas that has the highest priority and includes a first object matching the first audio input; selecting the first object, if the determined screen area only contains one object matching the first audio input; and using a second input to select one of the objects that matches the first audio input in the determined screen area, if the determined screen area contains more than one object that matches the first audio input.","Reference will now be made in detail to exemplary embodiments of the invention illustrated in the accompanying drawings. Wherever possible, the same reference numbers in different drawings refer to the same or like parts.","A. Overview","Methods and apparatus consistent with the invention provide a speech interface for a GUI. One embodiment includes dividing a GUI for a system into screen areas containing one or more selectable objects, and assigning priorities to the screen areas based on some criteria, for example, usage. The screen area with the highest priority is indicated, for example, by visual output such as highlighting. Next, an audio input is received, and its receipt confirmed, for example, by an audio output. In response to the received audio input, a system determines the screen area having (1) the highest priority and (2) a selectable object matching the audio input. If that screen area only contains one such object, then the system selects that object. However, if that screen area contains more than one matching object, then the system uses a second input to select one of the objects matching the audio input from that screen area.","Using a second input to select one of the objects that match an audio input includes marking the objects that matched the audio input, such as with icons; receiving a second audio input relating to the selection of one of the marked objects; and selecting the marked object that best matches the second audio input. The icons may be semi-transparent and may be laid over the objects. In addition, the icons may include numbered labels and may be removed from the screen area upon object selection.","B. Architecture",{"@attributes":{"id":"p-0022","num":"0021"},"figref":["FIG. 1","FIG. 1"],"b":["100","130","110","120","120","110","105","100"]},{"@attributes":{"id":"p-0023","num":"0022"},"figref":["FIG. 2","FIG. 1"],"b":["100","200","230","200","120","220"]},"Computer  contains a processor  connected to a memory . Processor  may be a microprocessor or any other micro-, mini-, or mainframe computer. Memory  may comprise a RAM, a ROM, a video memory, or mass storage. The mass storage may include both fixed and removable media (e.g., magnetic, optical, or magnetic optical storage systems or other available mass storage technology). Memory  may contain a program, such as a web browser, and an application programming interface.","A user typically provides information to computer  via input devices , such as a conventional microphone. In return, information is conveyed to the user via output devices , such as a conventional visual display device and an audio display device. Output devices  and input devices  connect to computer  via appropriate I\/O ports. The input and output devices may be physically connected to system bus , or could communicate via some other connection, such as by use of an infrared or other wireless signal.","The client computer  may run a \u201cbrowser\u201d , which is a software tool used to access a Web server . A Web server  operates a web site which supports files in the form of documents and pages. Browser  sends out requests to Web server  via network , and Web server  returns a responses to browser  via network . Browser  acts upon those responses such as by displaying content to the user. The content portion of the responses can be a \u201cWeb page\u201d expressed in hypertext markup language (HTML) or other data generated by the Web server .","One exemplary computer system  that can implement the present invention is the SAP R\/3 \u2122 system. In that system, a backend server , such as an SAP R\/3\u2122 server, contains program applications that can be performed on client computer . Web server , such as an Internet Transaction Server (ITS) from SAP, reads the application content from the backend server  and translates the application content into HTML.","An extension  in the Web server  structures the HTML content. The extension  is an application program run on Web server  that structures HTML content by, for example, defining control groupings and data types within the HTML content. Extension  adds data to the HTML content via the form of traditional meta-data tags or XML type structured data. The extension  may be written in programming languages such as Javascript\u2122, Visual Basic\u2122, C++ and C.","On the client side, in the exemplary system, the client computer runs Windows\u2122 O\/S to create a multi-framed environment. Browser  receives the structured HTML content from the Web server  via network . An extension module  within browser  then voice-enables the received HTML content. Extension module  consists of program applications to be performed on the client's processor  for voice-enabling the HTML content. In the exemplary system, the program applications include a parser , a prioritization module ; a speech recognition engine ; an event handler ; and a change detector . These applications may be written in languages such as Javascript\u2122, Visual Basic\u2122, C++ and C.","Extension module  reads in the structured HTML content and identifies elements in the content and their location within the content structure. Parser  then parses each element using standard parsing techniques, such as DOM-parsing in Javascript\u2122. For instance, upon identification of a group of two items, such as a label tag and text data, parser  extracts text data out of the label. The extracted text data can include identifying text, an object identification, an event and an action associated with the element (e.g. an action associated with an on-click event). Extension module  then assigns a voice command, such as the identifying text, for the element.","Extension module  then loads the element data into a prioritization module . Prioritization module  stores element data, such as the associated voice command, the object identification, the event, and the action in a data structure consisting of categorized areas. The data structure and categories correspond to the structure and groups of the originally read content. By storing the element data in categories, or grammars, the prioritization module  can associate more than one element.","As an illustrative example,  shows an exemplary GUI  displaying content generated by a server to the user. GUI  consists of five screen areas. On the left, GUI  shows the \u201cIn Use\u201d area , the \u201cWorkplace Favorites\u201d area , and the \u201cRoles\u201d area . On the right, GUI  shows two areas, the \u201cDeparting\u201d area  and the \u201cArriving\u201d area .  depicts an example of HTML content  associated with \u201cIn Use\u201d area  of . After extension module  reads through the HTML elements associated with the objects contained within \u201cIn Use\u201d area , it loads the parsed element data into the \u201cIn Use\u201d category, or grammar, in prioritization module .  shows an example of grammar structure  in prioritization module  having categorized element data.","After storing the element data into grammars, prioritization module  assigns a priority value to each of the grammars. This effectively assigns a priority value to each of the elements contained within the grammars. For example, in , assigning the highest priority to the \u201cIn Use\u201d grammar effectively assigns priority to each element contained within the \u201cIn Use\u201d grammar. Client computer  uses this prioritized element data to respond to an audio input relating to a selection of an object contained within the GUI.","Client computer  also contains a standard speech recognition engine (SR engine) , such as a Microsoft\u2122 SAPI-compliant SR engine. In one embodiment, extension module  contains SR engine . SR engine  produces a text stream from an audio input using dictionaries or phonetic recognition patterns. In one embodiment, client computer  loads or registers the stored voice commands against a standard SR engine . SR engine  then produces a text stream that matches a registered voice command. Extension module  uses the text stream to determine the selected object as enumerated below.","The selectable objects may have actions or functions associated with them that are performed upon their selection, such as the opening or closing of an application, or an action within the screen area. In one embodiment, extension module  contains an event handler  to enable the performance of an operation related to the speech interface prior to performing the action associated with an object selection. Event handler  replaces the function, or action, with an extension module function for performing an operation related to the speech interface. The extension module function is then set to call the previously replaced function upon completion of the operation related to the speech interface. For example, upon selection of a button, extension module  could produce highlighting prior to performing the action associated with the button selection. In another embodiment, event handler  also enables the performance of an operation related to the speech interface after performing the action associated with an object selection. For instance, upon selection of a button, extension module  could adjust screen area priorities after performing the action associated with the button selection.","In another embodiment, extension module  also contains a change detector  to monitor a screen area's content and to detect changes so that prioritization module  can account for the elements associated with a GUI. For instance, upon the selection of a button, change detector  would detect changes made to the content of a screen area. Upon detecting changes, parser  would re-parse the content and prioritization module  would update data structures for that screen area.",{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 5","b":["500","130","505","510","515","520","525","545","530","535","540","545","520"]},"As explained above, the system divides the GUI  into five screen areas: \u201cDeparting\u201d area , \u201cArriving\u201d area , \u201cIn Use\u201d area , \u201cWorkplace Favorites\u201d area , and \u201cRoles\u201d area . Prioritization module  assigns the screen areas a priority. For instance, \u201cDeparting\u201d area  could have the highest priority, \u201cArriving\u201d area  could have second priority, \u201cIn Use\u201d area  third priority and so on. There are two objects marked \u201cFriday,\u201d one within \u201cDeparting\u201d area  and one within \u201cArriving\u201d area , and three objects marked \u201cAirlines,\u201d two within \u201cIn Use\u201d area  and one within \u201cRoles\u201d area .","If, for example, the user says \u201cFriday,\u201d the system first looks for a matching object in the highest priority area, \u201cDeparting\u201d area . Since there is an object matching \u201cFriday\u201d here, the object is selected and the corresponding action is performed for that object. Although an object matching \u201cFriday\u201d also resides within the \u201cArriving\u201d area , the \u201cDeparting\u201d area  is the screen area from which the object is selected because \u201cDeparting\u201d area  has a higher priority than \u201cArriving\u201d area .","If, on the other hand, the user says \u201cAirlines,\u201d the system first looks for a matching object in the highest priority area, \u201cDeparting\u201d area . Since no object matches \u201cAirlines\u201d in that area, the system looks for a match in the next highest priority area, \u201cArriving\u201d area . Again, no object in \u201cArriving\u201d area  matches \u201cAirlines,\u201d so the system continues to look for a match in the other areas in order of their priority. The system finally finds a matching object in \u201cIn Use\u201d area . Although an object matching \u201cAirlines\u201d lies within \u201cRoles\u201d area , the system selects the object from \u201cIn Use\u201d area  because the \u201cIn Use\u201d area  has a higher priority than \u201cRoles\u201d area .","There are two instances of \u201cAirlines\u201d in \u201cIn Use\u201d area , however, so the system presents markers, such as semi-transparent, numbered icons, to allow the user to select one of the instances. These types of semi-transparent, numbered icons may be referred to as \u2018Representational Enumerated Semi-transparent Overlaid Labels for Voice\u2019 (RESOLV) icons from SAP. The user selects one of the \u201cAirlines\u201d objects by saying the number on the associated marker.","As described in detail above, methods and apparatus consistent with the invention provide a speech interface for a GUI. The foregoing description of an implementation of the invention has been presented for purposes of illustration and description. Modifications and variations are possible in light of the above teachings or may be acquired from practicing the invention. For example, the foregoing description is based on a client-server architecture, but those skilled in the art will recognize that a peer-to-peer architecture may be used consistent with the invention. Moreover, although the described implementation includes software, the invention may be implemented as a combination of hardware and software or in hardware alone. Additionally, although aspects of the present invention are described as being stored in memory, one skilled in the art will appreciate that these aspects can also be stored on other types of computer-readable media, such as secondary storage devices, like hard disks, floppy disks, or CD-ROM; a carrier wave from the Internet; or other forms of RAM or ROM.","The scope of the invention is therefore defined by the following claims and their equivalents."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate an embodiment of the invention and together with the description, serve to explain the principles of the invention.",{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":["FIG. 2","FIG. 1"]},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 4A"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 4B"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 5"}]},"DETDESC":[{},{}]}
